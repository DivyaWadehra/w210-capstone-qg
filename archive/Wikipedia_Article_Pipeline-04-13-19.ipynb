{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoQ: Improving reading comprehension through automatic question generation\n",
    "W210.5\n",
    "\n",
    "Julia Buffinton, Saurav Datta, Joanna Huang, Kathryn Plath\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import ast\n",
    "import string\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import nltk\n",
    "import time\n",
    "import torch\n",
    "#import spacy\n",
    "#from spacy.tokenizer import Tokenizer\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Uncomment if you need to download these!\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "File structure:\n",
    "\n",
    "w210-literacy/\n",
    "├── GenerationQ/\n",
    "│   ├── flask/\n",
    "│   ├── model/\n",
    "│   └── README.md\n",
    "├── DrQA/\n",
    "│   ├── TODO\n",
    "│   └── TODO\n",
    "├── wikipedia_data/\n",
    "│   ├── wikipedia_dump/\n",
    "|       ├── AA/\n",
    "|           ├── wiki_00\n",
    "|           ├── wiki_01\n",
    "|           └── ... etc.\n",
    "|       └── ... etc.\n",
    "│   ├── wikipedia_squad/\n",
    "|       ├── AA/\n",
    "|           ├── wiki_00\n",
    "|           ├── wiki_01\n",
    "|           └── ... etc.\n",
    "|       └── ... etc.\n",
    "|   ├── labeled/\n",
    "|       ├── AA/\n",
    "|       └── ... etc.\n",
    "|   ├── unlabeled/\n",
    "|       ├── AA/\n",
    "|       └── ... etc.\n",
    "|   ├── questions/\n",
    "|       ├── AA/\n",
    "|       └── ... etc.\n",
    "|   ├── answers/\n",
    "|       ├── AA/\n",
    "|       └── ... etc.\n",
    "└── Wikipedia Article Sentence Selection.ipynb [this file!!]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data File Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming these directories are already created! If not, need to create them. \n",
    "data_files = './wikipedia_data'\n",
    "\n",
    "# This should contain folders of files with lists of Wikipedia articles\n",
    "wiki_dump = data_files + '/wikipedia_dump/'\n",
    "\n",
    "# These should all be empty\n",
    "wiki_squad = data_files + '/wikipedia_squad/'\n",
    "labeled_sents = data_files + '/labeled_sentences/'\n",
    "unlabeled_sents = data_files + '/unlabeled_sentences/'\n",
    "questions = data_files + '/questions/'\n",
    "answers = data_files + '/answers/'\n",
    "\n",
    "# We'll want to reference these later, so we can manipulate corresponding files across all directors\n",
    "directories = [wiki_squad, labeled_sents, unlabeled_sents, questions, answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Make sure data file directories (except wiki dump) are empty to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should remove any existing files from the folders, before generating new data\n",
    "for i,d in enumerate(directories):\n",
    "    directory = d + '*'\n",
    "    !rm -rf $directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wikipedia Article Pre-Processing\n",
    "To move our data through the pipeline and ensure that it is suitable for our app, we will format it like the SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files in AB folder...\n",
      "Skipping article: Oxycodone\n",
      "Skipping article: Kyoto Protocol\n",
      "Skipping article: Index of philosophy articles (A–C)\n",
      "Skipping article: History of Christianity\n",
      "Skipping article: Klaus Barbie\n",
      "Completing files in AB folder... 5022 articles processed, 5 skipped.\n",
      "\n",
      "Processing files in AA folder...\n",
      "Skipping article: Floccinaucinihilipilification\n",
      "Skipping article: List of German proverbs\n",
      "Completing files in AA folder... 5280 articles processed, 2 skipped.\n",
      "\n",
      "Reformatting complete. Total 10302 articles processed for question generation, 7 skipped.\n"
     ]
    }
   ],
   "source": [
    "# iterate through dump of wikipedia articles\n",
    "total_articles = 0\n",
    "total_skipped = 0\n",
    "\n",
    "for foldername in os.listdir(wiki_dump):\n",
    "    \n",
    "    input_subfolder = wiki_dump + foldername\n",
    "    output_subfolder = wiki_squad + foldername\n",
    "    \n",
    "    if not os.path.exists(output_subfolder):\n",
    "        os.mkdir(output_subfolder)\n",
    "\n",
    "    # these are not files, just folders\n",
    "    print(\"Processing files in {} folder...\".format(foldername))\n",
    "    num_articles = 0\n",
    "    num_skipped = 0\n",
    "    \n",
    "    # each file represents several (variable #) wikipedia articles\n",
    "    for filename in os.listdir(input_subfolder):\n",
    "        #print(filename)\n",
    "        f = open(input_subfolder + '/' + filename)\n",
    "        \n",
    "        # each file of articles will become a separate .json of articles\n",
    "        # this helps if we run into issues, we can just discard a whole file and move on\n",
    "        \n",
    "        # set up json format for squad-like listing of articles\n",
    "        wikipedia_data_dict = {\"data\": [], \"version\" : 1.0}\n",
    "        \n",
    "        # save this to the 'wikipedia_squad' folder of correctly-formatted dicts of wikipedia articles\n",
    "        output_file = output_subfolder + '/' + filename\n",
    "       \n",
    "        # each line represents a different wikipedia article\n",
    "        # we will ignore the id and url for now, not needed\n",
    "        \n",
    "        for line in f:\n",
    "            line_dict = ast.literal_eval(line)\n",
    "            title = line_dict['title']\n",
    "                              \n",
    "            # for some reason, empty articles are included. They should be disregarded\n",
    "            try:\n",
    "                text = line_dict['text'].split(\"\\n\\n\",1)[1] # title is duplicated within text as well\n",
    "            except:\n",
    "                num_skipped += 1\n",
    "                print(\"Skipping article:\",title)\n",
    "            else:\n",
    "                # arbitrary length, should eliminate articles like \"disambiguation\" articles, etc. \n",
    "                if len(text) > 1000: \n",
    "                    num_articles += 1\n",
    "                    # Break text up into paragraphs\n",
    "                    paras = text.split(\"\\n\\n\")\n",
    "\n",
    "                    context = [{'context': para.rstrip(), 'qas' : []} for para in paras]\n",
    "\n",
    "                    wikipedia_data_dict['data'].append({'title' : title, 'paragraphs' : context})\n",
    "                    \n",
    "        # in case we don't have any articles in the file to add\n",
    "        if (wikipedia_data_dict['data']): \n",
    "            with open(output_file, 'w') as outfile:  \n",
    "                json.dump(wikipedia_data_dict, outfile)\n",
    "            \n",
    "    total_articles += num_articles\n",
    "    total_skipped += num_skipped\n",
    "    \n",
    "    print(\"Completing files in {} folder... {} articles processed, {} skipped.\\n\".format(foldername,num_articles,num_skipped))\n",
    "\n",
    "print(\"Reformatting complete. Total {} articles processed for question generation, {} skipped.\".format(total_articles,total_skipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentence Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_and_weights(d,paragraphs):\n",
    "    \"\"\"Clean sentences, remove digit, punctuation, upper case to lower\n",
    "    Args: d,paragraphs = d is article number within file, paragraphs is just paragraphs\n",
    "    Return: \n",
    "        labeled sentences: \n",
    "        stemmed sentences:\n",
    "        word distribution: \n",
    "    \"\"\"   \n",
    "    # Create dictionaries for labeled & stemmed sentences to populate\n",
    "    labeled_sentences = {}\n",
    "    stemmed_sentences = {}\n",
    "\n",
    "    # Create word distribution dictionaries\n",
    "    # these needs to be a default dict so it returns 0 if word not found \n",
    "    word_cts = defaultdict(int)\n",
    "    word_distr = defaultdict(int)\n",
    "\n",
    "    # initialize for stemming\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokenize = nltk.word_tokenize\n",
    "    sent_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # helper function for tracking stemmed words\n",
    "    def stem_and_add(wd):\n",
    "        word_cts[wd] += 1\n",
    "        word_cts['total_count'] += 1\n",
    "        return stemmer.stem(wd)\n",
    "\n",
    "    # need to go through each 'paragraph' (context) to gather info about sentences\n",
    "    for i,context in enumerate(paragraphs):\n",
    "\n",
    "        paragraph = context['context']\n",
    "        # split paragraph into sentences, make sure to keep digits, etc. together\n",
    "        #sentences = context['context'].split('. ') #last one still has a period at the end\n",
    "\n",
    "        #print(len(paragraph))\n",
    "\n",
    "        if len(paragraph) > 75:\n",
    "            sentences = sent_splitter.tokenize(context['context'].strip())\n",
    "        else: \n",
    "            break\n",
    "\n",
    "        # iterate through sentences to tokenize, calculate overall word distribution\n",
    "        for j,original_sentence in enumerate(sentences):\n",
    "\n",
    "            # Remove all digits\n",
    "            sentence = ''.join([x for x in original_sentence if not x.isdigit()])\n",
    "            # Remove all punctuation (OK to include periods since it's been split)\n",
    "            sentence = ''.join([x for x in sentence if x not in string.punctuation])\n",
    "\n",
    "            # Lowercase everything\n",
    "            sentence = sentence.strip()\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "            # Split into words & rejoin (remove extra spaces)\n",
    "            sentence = ' '.join(sentence.split())\n",
    "\n",
    "            tokenized_stemmed_sent = [stem_and_add(word) for word in nltk.tokenize.word_tokenize(sentence) \n",
    "                                      if not word in stop_words]\n",
    "\n",
    "            # keep track of tokenized for calculating sentence weight in next step\n",
    "            # save list of unprocessed sentences for later\n",
    "            # but we're only selecting from the first and last sentences in the paragraphs\n",
    "            if (original_sentence == sentences[0]) | (original_sentence == sentences[-1]):\n",
    "                if not original_sentence.startswith('[[File:'):\n",
    "                    labeled_sentences[(d,i,j)] = original_sentence.replace('\\n', ' ')\n",
    "                    stemmed_sentences[(d,i,j)] = tokenized_stemmed_sent\n",
    "\n",
    "    # update our word dictionary to be relative frequencies rather than absolute values\n",
    "    for word, ct in word_cts.items():\n",
    "        # but keep our total count, we may want that later (not sure)\n",
    "        if not word == 'total_count':\n",
    "            word_distr[word] = word_cts[word] / word_cts['total_count']\n",
    "            \n",
    "    #print(sorted(word_distr.items(), key=lambda k: k[1], reverse=True))\n",
    "    return labeled_sentences,stemmed_sentences,word_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sent_weight(word_dist, stemmed_sents):\n",
    "        \"\"\"Compute weight with respect to sentences\n",
    "        Args:\n",
    "                word_distribution: dict with word: weight\n",
    "                stemmed_sents: list with \n",
    "        Return:\n",
    "                sentence_weight: dict of weight of each sentence. key = sentence #, value = weight\n",
    "        \"\"\"\n",
    "        sentences_weight = {}\n",
    "        # Iterate through each word in each sentence, if word distribution and sentence id are in dictionary, \n",
    "        # add to existing word distribution. Else, sentence weight for given sentence equals current word distribution\n",
    "          \n",
    "        for key, words in stemmed_sents.items():\n",
    "            #print(words)\n",
    "            # Sentence weight equals sum of word distributions divided by length of cleaned sentence\n",
    "            if len(words) == 0:\n",
    "                weight = 0\n",
    "            else:\n",
    "                weight = sum([word_dist[word] for word in words]) / len(words)\n",
    "            \n",
    "            sentences_weight[key] = weight\n",
    "            \n",
    "        sentences_weight = sorted(sentences_weight.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        #print('sentence weight: ',sentences_weight)\n",
    "\n",
    "        return sentences_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topically_important_sentence(sentences_weight, labeled_sentences):\n",
    "        \"\"\"Select topically import sentences\n",
    "        Args:\n",
    "                sentence_weight: list of tuples, (sentence_num, sentence_weight) computed in sentence_weight\n",
    "                paragraph: set of sentences\n",
    "        Return:\n",
    "                sentences_selected: dict, topically important sentences selected\n",
    "        \"\"\"\n",
    "        final_sentences = {}\n",
    "        \n",
    "        total_sentences = len(sentences_weight)\n",
    "        # how many sentences to retain\n",
    "        num_sentences_selected = math.ceil(float(0.20) * total_sentences)\n",
    "        #print('num sentences for this passage:',num_sentences_selected)\n",
    "        \n",
    "        # key of selected sentences (# order of sentence in paragraph)\n",
    "        #sentences_selected_key = []\n",
    "        \n",
    "        # dictionary of all sentences \n",
    "        sentences_dict = {}\n",
    "        flag = 0\n",
    "        \n",
    "        # select num_sentences_selected # of sentences from list of sentence weights\n",
    "        #print(sentence_weight[0])\n",
    "        selected_keys = [k for k,v in sentence_weight[0:num_sentences_selected]]\n",
    "        \n",
    "        #print(\"selected sentence(s):\",selected_keys)\n",
    "\n",
    "\n",
    "        for sent_key in selected_keys:\n",
    "            pre_processed_sentence = labeled_sentences[sent_key]\n",
    "            \n",
    "            processed_sentence = pre_processed_sentence.lower() #lowercase\n",
    "            processed_sentence = processed_sentence.replace('[[','')\n",
    "            processed_sentence = processed_sentence.replace(']]','')\n",
    "            processed_sentence = processed_sentence.replace(']','')\n",
    "            processed_sentence = processed_sentence.replace('[','')\n",
    "            processed_sentence = re.sub('(?<!\\d)([.,!?()])(?<!\\d)', r' \\1 ', processed_sentence)\n",
    "            processed_sentence = re.sub(r'\\(','-lrb- ',processed_sentence) # replace left parens, add space after\n",
    "            processed_sentence = re.sub(r'\\)',' -rrb-',processed_sentence) # replace left parens, add space after\n",
    "            processed_sentence = re.sub(r'\\([^)]*\\)', '',processed_sentence) #replace brackets in links\n",
    "            processed_sentence = re.sub('(?<=\\s)\\\"','`` ',processed_sentence) # replace first double quotes with ``\n",
    "            processed_sentence = re.sub(r'\\\"', \" ''\", processed_sentence) # replace second double quote with two single quotes ''\n",
    "\n",
    "            #print(processed_sentence)\n",
    "\n",
    "            final_sentences[sent_key] = processed_sentence\n",
    "            \n",
    "        return final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting topical sentences for files in AB folder...\n",
      "Completing files in AB folder... 5022 articles processed, 0 articles skipped.\n",
      "\n",
      "Selecting topical sentences for files in AA folder...\n",
      "Completing files in AA folder... 5280 articles processed, 0 articles skipped.\n",
      "\n",
      "Sentence selection complete. Total 10302 articles processed, 0 skipped for question generation.\n"
     ]
    }
   ],
   "source": [
    "total_skipped = 0\n",
    "total_selected = 0\n",
    "\n",
    "for foldername in os.listdir(wiki_squad):\n",
    "    \n",
    "    input_subfolder = wiki_squad + foldername\n",
    "    output_subfolder_labeled = labeled_sents + foldername\n",
    "    output_subfolder_unlabeled = unlabeled_sents + foldername\n",
    "\n",
    "    if not os.path.exists(output_subfolder_labeled):\n",
    "        os.mkdir(output_subfolder_labeled)\n",
    "    \n",
    "    if not os.path.exists(output_subfolder_unlabeled):\n",
    "        os.mkdir(output_subfolder_unlabeled)\n",
    "\n",
    "    # these are not files, just folders\n",
    "    print(\"Selecting topical sentences for files in {} folder...\".format(foldername))\n",
    "    \n",
    "    num_skipped = 0\n",
    "    num_selected = 0\n",
    "    \n",
    "    # each file represents several (variable #) wikipedia articles\n",
    "    for filename in os.listdir(input_subfolder):\n",
    "        #print(filename)\n",
    "        input_file = input_subfolder + '/' + filename\n",
    "              \n",
    "        # save these to different directories of labeled and unlabeled sentences\n",
    "        output_file_labeled = open(output_subfolder_labeled + '/' + filename, \"w\")\n",
    "        output_file_unlabeled = open(output_subfolder_unlabeled + '/' + filename, \"w\")\n",
    "\n",
    "        with open(input_file) as json_file:  \n",
    "            data = json.load(json_file)\n",
    "\n",
    "        data = pd.DataFrame.from_dict(data)\n",
    "        df = data['data']\n",
    "\n",
    "        # for each article in the file\n",
    "        for row,value in df.iteritems():\n",
    "            #print(\"Article #{}, {}\".format(row,value['title']))\n",
    "            \n",
    "            # here is where we clean and stem words, build word distribution\n",
    "            try:\n",
    "                labeled_sentences, stemmed_sentences, word_distribution = sents_and_weights(row,value['paragraphs'])\n",
    "\n",
    "                # use this word distribution to get weights for each sentence and calculate most important sentences \n",
    "                sentence_weight = calc_sent_weight(word_distribution,stemmed_sentences)\n",
    "\n",
    "                # pull out most important sentences\n",
    "                # and keep track of where they came from: (doc #, context #, sentence #)\n",
    "                chosen_sentences = topically_important_sentence(sentence_weight,labeled_sentences)\n",
    "            except: \n",
    "                num_skipped += 1\n",
    "                print(\"Skipping article:\",value['title'])\n",
    "            else:\n",
    "                num_selected += 1\n",
    "                for sents in chosen_sentences.items():\n",
    "\n",
    "                    #save selected sentences directly to file, for onmt model\n",
    "                    output_file_unlabeled.write(str(sents[1])+'\\n')\n",
    "                    # keep track of their locations, though\n",
    "                    output_file_labeled.write(str(sents)+'\\n') \n",
    "                    \n",
    "    total_skipped += num_skipped\n",
    "    total_selected += num_selected\n",
    "    print(\"Completing files in {} folder... {} articles processed, {} articles skipped.\\n\".format(foldername,num_selected,num_skipped))\n",
    "\n",
    "print(\"Sentence selection complete. Total {} articles processed, {} skipped for question generation.\".format(total_selected, total_skipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference: https://github.com/drewserles/GenerationQ. Assuming training has already been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning question generation for files in AB folder...\n",
      "PRED AVG SCORE: -0.6979, PRED PPL: 2.0095\n",
      "PRED AVG SCORE: -0.6220, PRED PPL: 1.8626\n",
      "PRED AVG SCORE: -0.6757, PRED PPL: 1.9654\n",
      "PRED AVG SCORE: -0.6667, PRED PPL: 1.9479\n",
      "PRED AVG SCORE: -0.6571, PRED PPL: 1.9291\n",
      "PRED AVG SCORE: -0.6660, PRED PPL: 1.9464\n",
      "PRED AVG SCORE: -0.6595, PRED PPL: 1.9338\n",
      "PRED AVG SCORE: -0.7150, PRED PPL: 2.0443\n",
      "PRED AVG SCORE: -0.6685, PRED PPL: 1.9512\n",
      "PRED AVG SCORE: -0.6558, PRED PPL: 1.9266\n",
      "PRED AVG SCORE: -0.6985, PRED PPL: 2.0108\n",
      "PRED AVG SCORE: -0.6686, PRED PPL: 1.9515\n",
      "PRED AVG SCORE: -0.6498, PRED PPL: 1.9152\n",
      "PRED AVG SCORE: -0.6578, PRED PPL: 1.9306\n",
      "PRED AVG SCORE: -0.6954, PRED PPL: 2.0046\n",
      "PRED AVG SCORE: -0.6402, PRED PPL: 1.8968\n",
      "PRED AVG SCORE: -0.6048, PRED PPL: 1.8309\n",
      "PRED AVG SCORE: -0.6965, PRED PPL: 2.0067\n",
      "PRED AVG SCORE: -0.6773, PRED PPL: 1.9685\n",
      "PRED AVG SCORE: -0.6568, PRED PPL: 1.9285\n",
      "Progress: processed 20 files in 9.92 minutes\n",
      "PRED AVG SCORE: -0.6857, PRED PPL: 1.9852\n",
      "PRED AVG SCORE: -0.6650, PRED PPL: 1.9444\n",
      "PRED AVG SCORE: -0.6680, PRED PPL: 1.9504\n",
      "PRED AVG SCORE: -0.6656, PRED PPL: 1.9456\n",
      "PRED AVG SCORE: -0.6888, PRED PPL: 1.9914\n",
      "PRED AVG SCORE: -0.6603, PRED PPL: 1.9354\n",
      "PRED AVG SCORE: -0.6826, PRED PPL: 1.9791\n",
      "PRED AVG SCORE: -0.6484, PRED PPL: 1.9125\n",
      "PRED AVG SCORE: -0.7144, PRED PPL: 2.0429\n",
      "PRED AVG SCORE: -0.6510, PRED PPL: 1.9174\n",
      "PRED AVG SCORE: -0.6791, PRED PPL: 1.9720\n",
      "PRED AVG SCORE: -0.6367, PRED PPL: 1.8903\n",
      "PRED AVG SCORE: -0.6209, PRED PPL: 1.8607\n",
      "PRED AVG SCORE: -0.6231, PRED PPL: 1.8647\n",
      "PRED AVG SCORE: -0.6881, PRED PPL: 1.9899\n",
      "PRED AVG SCORE: -0.6662, PRED PPL: 1.9468\n",
      "PRED AVG SCORE: -0.6503, PRED PPL: 1.9161\n",
      "PRED AVG SCORE: -0.6669, PRED PPL: 1.9481\n",
      "PRED AVG SCORE: -0.6573, PRED PPL: 1.9295\n",
      "PRED AVG SCORE: -0.6735, PRED PPL: 1.9610\n",
      "Progress: processed 40 files in 9.69 minutes\n",
      "PRED AVG SCORE: -0.6253, PRED PPL: 1.8688\n",
      "PRED AVG SCORE: -0.6685, PRED PPL: 1.9514\n",
      "PRED AVG SCORE: -0.6857, PRED PPL: 1.9851\n",
      "PRED AVG SCORE: -0.6926, PRED PPL: 1.9988\n",
      "PRED AVG SCORE: -0.6218, PRED PPL: 1.8623\n",
      "PRED AVG SCORE: -0.6665, PRED PPL: 1.9474\n",
      "PRED AVG SCORE: -0.6768, PRED PPL: 1.9675\n",
      "PRED AVG SCORE: -0.6919, PRED PPL: 1.9975\n",
      "PRED AVG SCORE: -0.6535, PRED PPL: 1.9222\n",
      "PRED AVG SCORE: -0.6683, PRED PPL: 1.9508\n",
      "PRED AVG SCORE: -0.5973, PRED PPL: 1.8173\n",
      "PRED AVG SCORE: -0.6995, PRED PPL: 2.0127\n",
      "PRED AVG SCORE: -0.6658, PRED PPL: 1.9461\n",
      "PRED AVG SCORE: -0.6745, PRED PPL: 1.9630\n",
      "PRED AVG SCORE: -0.7037, PRED PPL: 2.0212\n",
      "PRED AVG SCORE: -0.6228, PRED PPL: 1.8641\n",
      "PRED AVG SCORE: -0.6541, PRED PPL: 1.9235\n",
      "PRED AVG SCORE: -0.6571, PRED PPL: 1.9292\n",
      "PRED AVG SCORE: -0.6342, PRED PPL: 1.8856\n",
      "PRED AVG SCORE: -0.6684, PRED PPL: 1.9512\n",
      "Progress: processed 60 files in 10.60 minutes\n",
      "PRED AVG SCORE: -0.6930, PRED PPL: 1.9997\n",
      "PRED AVG SCORE: -0.6955, PRED PPL: 2.0046\n",
      "PRED AVG SCORE: -0.6785, PRED PPL: 1.9708\n",
      "PRED AVG SCORE: -0.6848, PRED PPL: 1.9833\n",
      "PRED AVG SCORE: -0.6634, PRED PPL: 1.9414\n",
      "PRED AVG SCORE: -0.6760, PRED PPL: 1.9661\n",
      "PRED AVG SCORE: -0.7230, PRED PPL: 2.0605\n",
      "PRED AVG SCORE: -0.7039, PRED PPL: 2.0216\n",
      "PRED AVG SCORE: -0.6884, PRED PPL: 1.9906\n",
      "PRED AVG SCORE: -0.6020, PRED PPL: 1.8257\n",
      "PRED AVG SCORE: -0.6892, PRED PPL: 1.9921\n",
      "PRED AVG SCORE: -0.6697, PRED PPL: 1.9537\n",
      "PRED AVG SCORE: -0.6954, PRED PPL: 2.0044\n",
      "PRED AVG SCORE: -0.6315, PRED PPL: 1.8804\n",
      "PRED AVG SCORE: -0.6733, PRED PPL: 1.9608\n",
      "PRED AVG SCORE: -0.6780, PRED PPL: 1.9700\n",
      "PRED AVG SCORE: -0.6573, PRED PPL: 1.9297\n",
      "PRED AVG SCORE: -0.6401, PRED PPL: 1.8967\n",
      "PRED AVG SCORE: -0.6440, PRED PPL: 1.9041\n",
      "PRED AVG SCORE: -0.6653, PRED PPL: 1.9452\n",
      "Progress: processed 80 files in 10.54 minutes\n",
      "PRED AVG SCORE: -0.6794, PRED PPL: 1.9727\n",
      "PRED AVG SCORE: -0.7132, PRED PPL: 2.0406\n",
      "PRED AVG SCORE: -0.6422, PRED PPL: 1.9006\n",
      "PRED AVG SCORE: -0.6525, PRED PPL: 1.9204\n",
      "PRED AVG SCORE: -0.6481, PRED PPL: 1.9119\n",
      "PRED AVG SCORE: -0.6896, PRED PPL: 1.9929\n",
      "PRED AVG SCORE: -0.6866, PRED PPL: 1.9870\n",
      "PRED AVG SCORE: -0.6556, PRED PPL: 1.9263\n",
      "PRED AVG SCORE: -0.7016, PRED PPL: 2.0169\n",
      "PRED AVG SCORE: -0.6469, PRED PPL: 1.9095\n",
      "PRED AVG SCORE: -0.6441, PRED PPL: 1.9042\n",
      "PRED AVG SCORE: -0.6552, PRED PPL: 1.9256\n",
      "PRED AVG SCORE: -0.7097, PRED PPL: 2.0334\n",
      "PRED AVG SCORE: -0.6497, PRED PPL: 1.9151\n",
      "PRED AVG SCORE: -0.6564, PRED PPL: 1.9278\n",
      "PRED AVG SCORE: -0.6686, PRED PPL: 1.9515\n",
      "PRED AVG SCORE: -0.7086, PRED PPL: 2.0312\n",
      "PRED AVG SCORE: -0.6984, PRED PPL: 2.0105\n",
      "PRED AVG SCORE: -0.6167, PRED PPL: 1.8528\n",
      "PRED AVG SCORE: -0.6619, PRED PPL: 1.9385\n",
      "Progress: processed 100 files in 11.16 minutes\n",
      "Beginning question generation for files in AA folder...\n",
      "PRED AVG SCORE: -0.6078, PRED PPL: 1.8364\n",
      "PRED AVG SCORE: -0.6601, PRED PPL: 1.9351\n",
      "PRED AVG SCORE: -0.6860, PRED PPL: 1.9857\n",
      "PRED AVG SCORE: -0.6558, PRED PPL: 1.9267\n",
      "PRED AVG SCORE: -0.6735, PRED PPL: 1.9610\n",
      "PRED AVG SCORE: -0.6944, PRED PPL: 2.0026\n",
      "PRED AVG SCORE: -0.6980, PRED PPL: 2.0097\n",
      "PRED AVG SCORE: -0.6701, PRED PPL: 1.9544\n",
      "PRED AVG SCORE: -0.6565, PRED PPL: 1.9280\n",
      "PRED AVG SCORE: -0.6831, PRED PPL: 1.9800\n",
      "PRED AVG SCORE: -0.6701, PRED PPL: 1.9543\n",
      "PRED AVG SCORE: -0.6855, PRED PPL: 1.9847\n",
      "PRED AVG SCORE: -0.7012, PRED PPL: 2.0161\n",
      "PRED AVG SCORE: -0.6605, PRED PPL: 1.9358\n",
      "PRED AVG SCORE: -0.6697, PRED PPL: 1.9536\n",
      "PRED AVG SCORE: -0.7178, PRED PPL: 2.0498\n",
      "PRED AVG SCORE: -0.6864, PRED PPL: 1.9865\n",
      "PRED AVG SCORE: -0.6363, PRED PPL: 1.8895\n",
      "PRED AVG SCORE: -0.6725, PRED PPL: 1.9591\n",
      "PRED AVG SCORE: -0.6558, PRED PPL: 1.9266\n",
      "Progress: processed 120 files in 12.50 minutes\n",
      "PRED AVG SCORE: -0.6321, PRED PPL: 1.8816\n",
      "PRED AVG SCORE: -0.6920, PRED PPL: 1.9976\n",
      "PRED AVG SCORE: -0.6444, PRED PPL: 1.9049\n",
      "PRED AVG SCORE: -0.6531, PRED PPL: 1.9214\n",
      "PRED AVG SCORE: -0.6720, PRED PPL: 1.9582\n",
      "PRED AVG SCORE: -0.6789, PRED PPL: 1.9716\n",
      "PRED AVG SCORE: -0.6957, PRED PPL: 2.0050\n",
      "PRED AVG SCORE: -0.6857, PRED PPL: 1.9852\n",
      "PRED AVG SCORE: -0.6819, PRED PPL: 1.9775\n",
      "PRED AVG SCORE: -0.6945, PRED PPL: 2.0027\n",
      "PRED AVG SCORE: -0.6961, PRED PPL: 2.0058\n",
      "PRED AVG SCORE: -0.6975, PRED PPL: 2.0087\n",
      "PRED AVG SCORE: -0.6599, PRED PPL: 1.9346\n",
      "PRED AVG SCORE: -0.5971, PRED PPL: 1.8168\n",
      "PRED AVG SCORE: -0.6562, PRED PPL: 1.9274\n",
      "PRED AVG SCORE: -0.6879, PRED PPL: 1.9896\n",
      "PRED AVG SCORE: -0.6763, PRED PPL: 1.9665\n",
      "PRED AVG SCORE: -0.6428, PRED PPL: 1.9019\n",
      "PRED AVG SCORE: -0.6690, PRED PPL: 1.9524\n",
      "PRED AVG SCORE: -0.6676, PRED PPL: 1.9496\n",
      "Progress: processed 140 files in 10.64 minutes\n",
      "PRED AVG SCORE: -0.6872, PRED PPL: 1.9880\n",
      "PRED AVG SCORE: -0.7222, PRED PPL: 2.0590\n",
      "PRED AVG SCORE: -0.6271, PRED PPL: 1.8722\n",
      "PRED AVG SCORE: -0.6312, PRED PPL: 1.8798\n",
      "PRED AVG SCORE: -0.6705, PRED PPL: 1.9552\n",
      "PRED AVG SCORE: -0.6746, PRED PPL: 1.9633\n",
      "PRED AVG SCORE: -0.6739, PRED PPL: 1.9619\n",
      "PRED AVG SCORE: -0.6635, PRED PPL: 1.9417\n",
      "PRED AVG SCORE: -0.6431, PRED PPL: 1.9024\n",
      "PRED AVG SCORE: -0.6940, PRED PPL: 2.0017\n",
      "PRED AVG SCORE: -0.6744, PRED PPL: 1.9628\n",
      "PRED AVG SCORE: -0.6717, PRED PPL: 1.9575\n",
      "PRED AVG SCORE: -0.7004, PRED PPL: 2.0145\n",
      "PRED AVG SCORE: -0.6733, PRED PPL: 1.9608\n",
      "PRED AVG SCORE: -0.6910, PRED PPL: 1.9957\n",
      "PRED AVG SCORE: -0.6912, PRED PPL: 1.9961\n",
      "PRED AVG SCORE: -0.6728, PRED PPL: 1.9597\n",
      "PRED AVG SCORE: -0.6747, PRED PPL: 1.9634\n",
      "PRED AVG SCORE: -0.6743, PRED PPL: 1.9626\n",
      "PRED AVG SCORE: -0.6782, PRED PPL: 1.9703\n",
      "Progress: processed 160 files in 9.60 minutes\n",
      "PRED AVG SCORE: -0.6515, PRED PPL: 1.9184\n",
      "PRED AVG SCORE: -0.6883, PRED PPL: 1.9903\n",
      "PRED AVG SCORE: -0.6708, PRED PPL: 1.9558\n",
      "PRED AVG SCORE: -0.6975, PRED PPL: 2.0087\n",
      "PRED AVG SCORE: -0.6501, PRED PPL: 1.9157\n",
      "PRED AVG SCORE: -0.6796, PRED PPL: 1.9731\n",
      "PRED AVG SCORE: -0.6949, PRED PPL: 2.0036\n",
      "PRED AVG SCORE: -0.6878, PRED PPL: 1.9894\n",
      "PRED AVG SCORE: -0.7046, PRED PPL: 2.0231\n",
      "PRED AVG SCORE: -0.6882, PRED PPL: 1.9901\n",
      "PRED AVG SCORE: -0.6862, PRED PPL: 1.9863\n",
      "PRED AVG SCORE: -0.6592, PRED PPL: 1.9333\n",
      "PRED AVG SCORE: -0.6976, PRED PPL: 2.0088\n",
      "PRED AVG SCORE: -0.7073, PRED PPL: 2.0286\n",
      "PRED AVG SCORE: -0.6117, PRED PPL: 1.8436\n",
      "PRED AVG SCORE: -0.6752, PRED PPL: 1.9644\n",
      "PRED AVG SCORE: -0.6774, PRED PPL: 1.9687\n",
      "PRED AVG SCORE: -0.6876, PRED PPL: 1.9890\n",
      "PRED AVG SCORE: -0.6438, PRED PPL: 1.9037\n",
      "PRED AVG SCORE: -0.6587, PRED PPL: 1.9323\n",
      "Progress: processed 180 files in 7.75 minutes\n",
      "PRED AVG SCORE: -0.6745, PRED PPL: 1.9630\n",
      "PRED AVG SCORE: -0.6381, PRED PPL: 1.8930\n",
      "PRED AVG SCORE: -0.6985, PRED PPL: 2.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.6670, PRED PPL: 1.9484\n",
      "PRED AVG SCORE: -0.6883, PRED PPL: 1.9902\n",
      "PRED AVG SCORE: -0.6952, PRED PPL: 2.0041\n",
      "PRED AVG SCORE: -0.6857, PRED PPL: 1.9851\n",
      "PRED AVG SCORE: -0.6960, PRED PPL: 2.0057\n",
      "PRED AVG SCORE: -0.7060, PRED PPL: 2.0259\n",
      "PRED AVG SCORE: -0.6267, PRED PPL: 1.8715\n",
      "PRED AVG SCORE: -0.6483, PRED PPL: 1.9122\n",
      "PRED AVG SCORE: -0.6839, PRED PPL: 1.9815\n",
      "PRED AVG SCORE: -0.6872, PRED PPL: 1.9881\n",
      "PRED AVG SCORE: -0.6604, PRED PPL: 1.9356\n",
      "PRED AVG SCORE: -0.6088, PRED PPL: 1.8382\n",
      "PRED AVG SCORE: -0.6800, PRED PPL: 1.9738\n",
      "PRED AVG SCORE: -0.6822, PRED PPL: 1.9783\n",
      "PRED AVG SCORE: -0.6470, PRED PPL: 1.9099\n",
      "PRED AVG SCORE: -0.6884, PRED PPL: 1.9905\n",
      "PRED AVG SCORE: -0.6750, PRED PPL: 1.9641\n",
      "Progress: processed 200 files in 10.88 minutes\n",
      "\n",
      "Processed 200 total files in 103.27 total minutes\n"
     ]
    }
   ],
   "source": [
    "generationq_dir = '~/GenerationQ/model'\n",
    "model = generationq_dir + '/trained/600rnn_step_16000.pt' # update depending on which model has lowest perplexity\n",
    "\n",
    "start = time.time()\n",
    "files = 0\n",
    "\n",
    "for foldername in os.listdir(unlabeled_sents):\n",
    "    \n",
    "    input_subfolder = unlabeled_sents + foldername\n",
    "    output_subfolder = questions + foldername\n",
    "    \n",
    "    if not os.path.exists(output_subfolder):\n",
    "        os.mkdir(output_subfolder)\n",
    "\n",
    "    # these are not files, just folders\n",
    "    print(\"Beginning question generation for files in {} folder...\".format(foldername))\n",
    "        \n",
    "    # each file represents several (variable #) wikipedia articles\n",
    "    for filename in os.listdir(input_subfolder):\n",
    "\n",
    "        \n",
    "        # input file is unlabeled sentence\n",
    "        input_file = input_subfolder + '/' + filename\n",
    "              \n",
    "        # save list of questions\n",
    "        output_file = output_subfolder + '/' + filename\n",
    "        \n",
    "        \n",
    "        !python -W ignore $generationq_dir/test.py -model $model -src $input_file -output $output_file \\\n",
    "        -replace_unk -beam_size 3 -gpu 0 -batch_size 30 2> /dev/null\n",
    "        \n",
    "        files += 1\n",
    "        if files % 20 == 0:\n",
    "            if files == 20:\n",
    "                print(\"Progress: processed 20 files in {:.2f} minutes, total: {} files\".format((time.time()-start)/60,files))\n",
    "                chunk_time = time.time()\n",
    "            else:\n",
    "                print(\"Progress: processed 20 files in {:.2f} minutes, total: {} files\".format((time.time()-chunk_time)/60),files)\n",
    "            chunk_time = time.time()\n",
    "\n",
    "\n",
    "print(\"\\nProcessed {} total files in {:.2f} total minutes\".format(files,(time.time()-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Remove \"problem\" files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the question generation model runs into an error and is unable to complete question generation for a file, it may output a blank file. Because we have many files/articles to choose from, it's OK to just throw these away. And, ideally, there are no files to remove anyway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying 'problem' files (with no generated questions) in AB folder...\n",
      "In folder AB, 100 files remain and 0 files were removed.\n",
      "\n",
      "Identifying 'problem' files (with no generated questions) in AA folder...\n",
      "In folder AA, 100 files remain and 0 files were removed.\n",
      "\n",
      "In total, 200 files remain and 0 files were removed.\n"
     ]
    }
   ],
   "source": [
    "total_empty = 0\n",
    "total_okay = 0\n",
    "for foldername in os.listdir(questions):\n",
    "    \n",
    "    input_subfolder = questions + foldername\n",
    "    #print(input_subfolder)\n",
    "    # these are not files, just folders\n",
    "    print(\"Identifying 'problem' files (with no generated questions) in {} folder...\".format(foldername))\n",
    "        \n",
    "    num_empty = 0\n",
    "    num_okay = 0\n",
    "    # each file represents several (variable #) wikipedia articles\n",
    "    for filename in os.listdir(input_subfolder):\n",
    "        \n",
    "        input_file = input_subfolder + '/' + filename\n",
    "        \n",
    "        if os.stat(input_file).st_size == 0:\n",
    "            num_empty += 1\n",
    "            print(\"Removing file {} from all corresponding directories\".format(foldername + '/' + filename))\n",
    "            \n",
    "            path_to_file = '/' + foldername + '/' + filename\n",
    "            for i,d in enumerate(directories):\n",
    "                del_file = d + path_to_file\n",
    "                !rm -rf $del_file\n",
    "        else:\n",
    "            num_okay += 1\n",
    "            \n",
    "    total_empty += num_empty\n",
    "    total_okay += num_okay\n",
    "            \n",
    "    print('In folder {}, {} files remain and {} files were removed.\\n'.format(foldername, num_okay, num_empty))\n",
    "print('In total, {} files remain and {} files were removed.'.format(total_okay, total_empty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3 Label questions, add back to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding questions to squad-formatted Wikipedia files in AB folder...\n",
      "Completed adding 47209 questions to squad-formatted Wikipedia files in AB folder.\n",
      "\n",
      "Adding questions to squad-formatted Wikipedia files in AA folder...\n",
      "Completed adding 48454 questions to squad-formatted Wikipedia files in AA folder.\n",
      "\n",
      "Complete. Added 95663 total questions to squad-formatted files.\n"
     ]
    }
   ],
   "source": [
    "total_questions = 0\n",
    "\n",
    "for foldername in os.listdir(labeled_sents):\n",
    "    \n",
    "    input_subfolder_q = questions + foldername \n",
    "    input_subfolder_s = labeled_sents + foldername \n",
    "    output_subfolder = wiki_squad + foldername\n",
    "    \n",
    "    print(\"Adding questions to squad-formatted Wikipedia files in {} folder...\".format(foldername))\n",
    "    \n",
    "    num_questions = 0 \n",
    "    # each file represents several (variable #) wikipedia articles\n",
    "    for filename in os.listdir(input_subfolder_q):\n",
    "        \n",
    "        # input file is questions with scores\n",
    "        input_file_q = input_subfolder_q + '/' + filename\n",
    "        input_file_s = input_subfolder_s + '/' + filename\n",
    "        output_file = output_subfolder + '/' + filename\n",
    "        \n",
    "        with open(output_file) as json_file:  \n",
    "            data = json.load(json_file)\n",
    "        wiki_dict = data['data']\n",
    "        \n",
    "        with open(input_file_q) as f:\n",
    "            pred_questions = f.read().splitlines()    \n",
    "            pred_questions = pred_questions[::2] # start with the 1st line, take every other line\n",
    "            \n",
    "        with open(input_file_s) as f2:\n",
    "            for line_q,line_s in zip(pred_questions,f2):\n",
    "                #print(line_q)\n",
    "                #print(line_s + '\\n')\n",
    "                line_tuple = eval(line_s)\n",
    "                item_id = line_tuple[0]\n",
    "\n",
    "                doc = wiki_dict[item_id[0]] # pull the whole document\n",
    "                context = doc[\"paragraphs\"][item_id[1]]\n",
    "\n",
    "                num_questions += 1\n",
    "                total_questions += 1\n",
    "                context['qas'].append({\"question\": line_q.rstrip(), \"answers\": [], \"id\": str(item_id)})\n",
    "            \n",
    "        with open(output_file, 'w') as outfile:  \n",
    "            json.dump(data, outfile)\n",
    "    \n",
    "    print(\"Completed adding {} questions to squad-formatted Wikipedia files in {} folder.\\n\".format(num_questions, foldername))\n",
    "\n",
    "print(\"Complete. Added {} total questions to squad-formatted files.\".format(total_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Answer Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 DrQA question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess\n",
    "! python ../DrQA/scripts/reader/preprocess.py $sample_file_abbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dir = '~/DrQA'\n",
    "model = qa_dir + '/data/reader/single.mdl'\n",
    "qa_predict = qa_dir + '/scripts/reader/predict.py' \n",
    "\n",
    "start = time.time()\n",
    "files = 0\n",
    "\n",
    "for foldername in os.listdir(wiki_squad):\n",
    "    \n",
    "    input_subfolder = wiki_squad + foldername\n",
    "    output_subfolder = answers + foldername\n",
    "    \n",
    "    if not os.path.exists(output_subfolder):\n",
    "        os.mkdir(output_subfolder)\n",
    "\n",
    "    # these are not files, just folders\n",
    "    print(\"Beginning question answering for files in {} folder...\".format(foldername))\n",
    "        \n",
    "    # each file represents several (variable #) wikipedia articles\n",
    "    for filename in os.listdir(input_subfolder):\n",
    "        \n",
    "        # input file is squad-formatted wikipedia articles, with questions sentence\n",
    "        input_file = input_subfolder + '/' + filename\n",
    "              \n",
    "        # save list of questions\n",
    "        output_file = output_subfolder + '/' + filenamw \n",
    "        \n",
    "        !python -W ignore $qa_predict --model $model --out-dir $output_subfolder $input_file\n",
    "        \n",
    "        files += 1\n",
    "        if files % 20 == 0:\n",
    "            if files == 20:\n",
    "                print(\"Progress: processed {} files in {:.2f} minutes\".format(files,(time.time()-start)/60))\n",
    "                chunk_time = time.time()\n",
    "            else:\n",
    "                print(\"Progress: processed {} files in {:.2f} minutes\".format(files,(time.time()-chunk_time)/60))\n",
    "            chunk_time = time.time()\n",
    "\n",
    "\n",
    "print(\"\\nProcessed {} total files in {:.2f} total minutes\".format(files,(time.time()-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/07/2019 02:55:24 PM: [ CUDA enabled (GPU -1) ]\n",
      "04/07/2019 02:55:24 PM: [ Initializing pipeline... ]\n",
      "04/07/2019 02:55:24 PM: [ Initializing document ranker... ]\n",
      "04/07/2019 02:55:24 PM: [ Loading /home/julia_buffinton/DrQA/data/wikipedia/docs-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz ]\n",
      "04/07/2019 02:56:01 PM: [ Initializing document reader... ]\n",
      "04/07/2019 02:56:01 PM: [ Loading model /home/julia_buffinton/DrQA/data/reader/multitask.mdl ]\n",
      "04/07/2019 02:56:06 PM: [ Initializing tokenizers and document retrievers... ]\n",
      "04/07/2019 02:56:09 PM: [ Loading queries from ../GenerationQ/model/data/squad-v1.1-dev.json ]\n",
      "Traceback (most recent call last):\n",
      "  File \"../DrQA/scripts/pipeline/predict.py\", line 109, in <module>\n",
      "    queries.append(data['question'])\n",
      "KeyError: 'question'\n"
     ]
    }
   ],
   "source": [
    "!python ../DrQA/scripts/pipeline/predict.py $sample_squad_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Add answers back to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding answers to squad-formatted Wikipedia files in AB folder...\n",
      "Adding answers to squad-formatted Wikipedia files in AA folder...\n"
     ]
    }
   ],
   "source": [
    "for foldername in os.listdir(answers):\n",
    "    \n",
    "    input_subfolder_a = answers + foldername \n",
    "    output_subfolder = './wikipedia_data/for_autoq/' + foldername \n",
    "    squad_subfolder = wiki_squad + foldername\n",
    "    \n",
    "    if not os.path.exists(output_subfolder):\n",
    "        os.mkdir(output_subfolder)\n",
    "    \n",
    "    print(\"Adding answers to squad-formatted Wikipedia files in {} folder...\".format(foldername))\n",
    "    \n",
    "    # each file represents several (variable #) wikipedia articles\n",
    "    for filename in os.listdir(input_subfolder_a):\n",
    "        \n",
    "        # input file is questions with scores\n",
    "        input_file_a = input_subfolder_a + '/' + filename\n",
    "        squad_file = squad_subfolder + '/' + filename[:7]\n",
    "        output_file = output_subfolder + '/' + filename[:7]\n",
    "\n",
    "        with open(squad_file) as json_file:  \n",
    "            data = json.load(json_file)\n",
    "        wiki_dict = data['data']\n",
    "\n",
    "\n",
    "        with open(input_file_a) as f:\n",
    "            qa_data = json.load(f)\n",
    "\n",
    "            for key, value in qa_data.items():\n",
    "                item_id = eval(key) # HOW DOES THIS WORK\n",
    "\n",
    "                doc = wiki_dict[item_id[0]] # pull the whole document\n",
    "                context = doc[\"paragraphs\"][item_id[1]] # find the appropriate paragraph\n",
    "\n",
    "                for i,question in enumerate(context['qas']):\n",
    "                    #print(question)\n",
    "                    #each paragraph may have several associated questions, so need to find the right one\n",
    "                    if question['id'] == key:\n",
    "                        #print(key)\n",
    "                        question['answers'].append({'answer_start' : 0, 'text': value[0][0]})\n",
    "\n",
    "        final_output = output_file + '.json'\n",
    "        with open(final_output, 'w') as outfile:  \n",
    "            json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch env",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
