{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Evaluation for Question Generation and Question Answering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used techniques for evaluating text in machine learning include [BLEU](https://en.wikipedia.org/wiki/BLEU) and [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric), both of which heavily focus on n-gram overlap between the reference text and what was generated (may or may not be sensitive to word order).\n",
    "\n",
    "However, given the generative nature of our problem, these scores may not fully indicate how \"good\" the results are. For example, the following question is a valid question that could be generated from the context, but it has almost no overlap with the 'gold standard' reference text, and thus achieves a poor BLEU and ROUGE score: \n",
    "\n",
    "- **Context**: As this was the 50th super bowl, the league emphasized the \"Golden Anniversary\" with various gold-themed initiatives , as well as temporarily suspending the tradition of naming each super bowl game with roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. \n",
    "- **Reference Question**: If Roman numerals were used, what would Super Bowl 50 have been called?\n",
    "- **Generated Question**: What is the “Golden Anniversary?”\n",
    "\n",
    "Many scholars use these scores, in conjunction with human ratings for \"naturalness,\" clarity, etc. to gain a better understanding of their performance. The evaluation package provided by [Chen et al. (2015)](https://arxiv.org/pdf/1504.00325.pdf) on [Github](https://github.com/tylin/coco-caption), originally used for image captions, includes a suite of evaluation metrics that can be used. \n",
    "\n",
    "The code below and enclosed subfolders are adapted from the Github repo associated with the above mentioned paper: https://github.com/tylin/coco-caption. The license is located in this folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up file names and pathes\n",
    "dataDir='./'\n",
    "\n",
    "annFile_ques = dataDir + 'annotated_data_ques.json'\n",
    "resFile_ques = dataDir + 'res_data_ques.json'\n",
    "\n",
    "annFile_ans = dataDir + 'annotated_data_ans.json'\n",
    "#subtypes = ['results', 'evalImgs', 'eval']\n",
    "resFile_ans = dataDir + 'res_data_ans.json'\n",
    "\n",
    "# download Stanford models\n",
    "#!./get_stanford_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.001604\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# create coco object and cocoRes object\n",
    "coco_ques = COCO(annFile_ques)\n",
    "cocoRes_ques = coco_ques.loadRes(resFile_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'reflen': 1063, 'guess': [977, 877, 777, 677], 'testlen': 977, 'correct': [321, 109, 51, 23]}\n",
      "ratio: 0.919096895578\n",
      "Bleu_1: 0.301\n",
      "Bleu_2: 0.185\n",
      "Bleu_3: 0.127\n",
      "Bleu_4: 0.089\n",
      "computing METEOR score...\n",
      "METEOR: 0.140\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.280\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.795\n",
      "computing SPICE score...\n",
      "SPICE: 0.178\n"
     ]
    }
   ],
   "source": [
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval_ques = COCOEvalCap(coco_ques, cocoRes_ques)\n",
    "\n",
    "# evaluate - using artificial tag for 'image id' because this was adapted from image caption scoring\n",
    "cocoEval_ques.params['image_id'] = cocoRes_ques.getImgIds()\n",
    "\n",
    "# evaluate results\n",
    "cocoEval_ques.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIDEr: 0.795\n",
      "Bleu_4: 0.089\n",
      "Bleu_3: 0.127\n",
      "Bleu_2: 0.185\n",
      "Bleu_1: 0.301\n",
      "ROUGE_L: 0.280\n",
      "METEOR: 0.140\n",
      "SPICE: 0.178\n"
     ]
    }
   ],
   "source": [
    "# Print summary of evaluation scores\n",
    "for metric, score in cocoEval_ques.eval.items():\n",
    "    print '%s: %.3f'%(metric, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.000828\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# create coco object and cocoRes object\n",
    "coco_ans = COCO(annFile_ans)\n",
    "cocoRes_ans = coco_ans.loadRes(resFile_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'reflen': 281, 'guess': [285, 185, 125, 85], 'testlen': 285, 'correct': [179, 95, 49, 23]}\n",
      "ratio: 1.01423487544\n",
      "Bleu_1: 0.628\n",
      "Bleu_2: 0.568\n",
      "Bleu_3: 0.502\n",
      "Bleu_4: 0.430\n",
      "computing METEOR score...\n",
      "METEOR: 0.372\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.679\n",
      "computing CIDEr score...\n",
      "CIDEr: 3.297\n",
      "computing SPICE score...\n",
      "SPICE: 0.529\n"
     ]
    }
   ],
   "source": [
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval_ans = COCOEvalCap(coco_ans, cocoRes_ans)\n",
    "\n",
    "# evaluate - using artificial tag for 'image id' because this was adapted from image caption scoring\n",
    "cocoEval_ans.params['image_id'] = cocoRes_ans.getImgIds()\n",
    "\n",
    "# evaluate results\n",
    "cocoEval_ans.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIDEr: 3.297\n",
      "Bleu_4: 0.430\n",
      "Bleu_3: 0.502\n",
      "Bleu_2: 0.568\n",
      "Bleu_1: 0.628\n",
      "ROUGE_L: 0.679\n",
      "METEOR: 0.372\n",
      "SPICE: 0.529\n"
     ]
    }
   ],
   "source": [
    "# Print summary of evaluation scores\n",
    "for metric, score in cocoEval_ans.eval.items():\n",
    "    print '%s: %.3f'%(metric, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
