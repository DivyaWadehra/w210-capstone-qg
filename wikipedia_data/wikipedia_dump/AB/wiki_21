{"id": "14889", "url": "https://en.wikipedia.org/wiki?curid=14889", "title": "Iran–Iraq War", "text": "Iran–Iraq War\n\nThe Iran–Iraq War was an armed conflict between Iran and Iraq, beginning on 22 September 1980, when Iraq invaded Iran, and ending on 20 August 1988, when Iran accepted the UN-brokered ceasefire. Iraq wanted to replace Iran as the dominant Persian Gulf state, and was worried that the 1979 Iranian Revolution would lead Iraq's Shi'ite majority to rebel against the Ba'athist government. The war also followed a long history of border disputes, and Iraq planned to annex the oil-rich Khuzestan Province and the east bank of the Shatt al-Arab (Arvand Rud).\n\nAlthough Iraq hoped to take advantage of Iran's post-revolutionary chaos, it made limited progress and was quickly repelled; Iran regained virtually all lost territory by June 1982. For the next six years, Iran was on the offensive until near the end of the war. There were a number of proxy forces—most notably the People's Mujahedin of Iran siding with Iraq and the Iraqi Kurdish militias of the KDP and PUK siding with Iran. The United States, Soviet Union, France, and most Arab countries provided support for Iraq, while Iran was largely isolated. After eight years, war-weariness, economic problems, decreased morale, repeated Iranian military failures, recent Iraqi successes, Iraqi use of weapons of mass destruction and lack of international sympathy, and increased U.S.–Iran military tension all led to a ceasefire brokered by the United Nations.\n\nThe conflict has been compared to World War I in terms of the tactics used, including large-scale trench warfare with barbed wire stretched across fortified defensive lines, manned machine guns posts, bayonet charges, Iranian human wave attacks, extensive use of chemical weapons by Iraq, and, later, deliberate attacks on civilian targets.\n\nAn estimated 500,000 Iraqi and Iranian soldiers died, in addition to a smaller number of civilians. The end of the war resulted in neither reparations nor border changes.\n\nThe Iran–Iraq War was originally referred to as the \"Gulf War\" until the Persian Gulf War of 1990 and 1991, after which it was known as the \"First Persian Gulf War\". The Iraq–Kuwait conflict, which was known as the \"Second Persian Gulf War,\" eventually became known simply as the \"Gulf War\". The Iraq War from 2003 to 2011 has been called the \"Second Persian Gulf War\".\n\nIn Iran, the war is known as the \"Imposed War\" ( ') and the \"Holy Defense\" ( '). State media in Iraq dubbed the war \"Saddam's Qadisiyyah\" (, \"\"), in reference to the seventh-century Battle of al-Qādisiyyah, in which Arab warriors overcame the Sasanian Empire during the Muslim conquest of Persia.\n\nThe relationship between the governments of Iran and Iraq briefly improved in 1978, when Iranian agents in Iraq discovered plans for a pro-Soviet \"coup d'état\" against Iraq's government. When informed of this plot, Saddam ordered the execution of dozens of his army's officers, and in a sign of reconciliation, expelled from Iraq Ruhollah Khomeini, an exiled leader of clerical opposition to the Shah. Nonetheless, Saddam considered the 1975 Algiers Agreement to be merely a truce, rather than a definite settlement, and waited for an opportunity to contest it.\n\nTensions between Iraq and Iran were fueled by Iran's Islamic revolution and its appearance of being a Pan-Islamic force, in contrast to Iraq's Arab nationalism. Despite Iraq's goal of regaining the Shatt al-Arab, the Iraqi government seemed to initially welcome the Iranian Revolution, which overthrew Shah Mohammad Reza Pahlavi, who was seen as a common enemy. It is difficult to pinpoint when tensions began to build, but there were frequent cross-border skirmishes, largely at Iran's instigation.\n\nAyatollah Ruhollah Khomeini called on Iraqis to overthrow the Ba'ath government, which was received with considerable anger in Baghdad. On 17 July 1979, despite Khomeini's call, Saddam gave a speech praising the Iranian Revolution and called for an Iraqi-Iranian friendship based on non-interference in each other's internal affairs. When Khomeini rejected Saddam's overture by calling for Islamic revolution in Iraq, Saddam was alarmed. Iran's new Islamic administration was regarded in Baghdad as an irrational, existential threat to the Ba'ath government, especially because the Ba'ath party, having a secular nature, discriminated against and posed a threat to the fundamentalist Shia movement in Iraq, whose clerics were Iran's allies within Iraq and whom Khomeini saw as oppressed.\n\nSaddam's primary interest in war may have also stemmed from his desire to right the supposed \"wrong\" of the Algiers Agreement, in addition to finally achieving his desire of annexing Khuzestan and becoming the regional superpower. Saddam's goal was to replace Egypt as the \"leader of the Arab world\" and to achieve hegemony over the Persian Gulf. He saw Iran's increased weakness due to revolution, sanctions, and international isolation. Saddam had invested heavily in Iraq's military since his defeat against Iran in 1975, buying large amounts of weaponry from the Soviet Union and France. By 1980, Iraq possessed 200,000 soldiers, 2,000 tanks and 450 aircraft. Watching the powerful Iranian army that frustrated him in 1974–1975 disintegrate, he saw an opportunity to attack, using the threat of Islamic Revolution as a pretext.\n\nOn 8 March 1980, Iran announced it was withdrawing its ambassador from Iraq, downgraded its diplomatic ties to the charge d'affaires level, and demanded that Iraq do the same. The following day, Iraq declared Iran's ambassador persona non-grata, and demanded his withdrawal from Iraq by 15 March. Iraq soon after expropriated the properties of 70,000 civilians believed to be of Iranian origin and expelled them from its territory. Many, if not most, of those expelled were in fact Arabic-speaking Iraqi Shias who had little to no family ties with Iran. This caused tensions between the two nations to increase further.\n\nIraq began planning offensives, confident that they would succeed. Iran lacked both cohesive leadership and spare parts for their American-made and British-made equipment. The Iraqis could mobilise up to 12 mechanised divisions, and morale was running high. Through the 1970s, Saddam had armed his forces with the latest military hardware from the Soviet Union.\n\nIn addition, the area around the Shatt al-Arab posed no obstacle for the Iraqis, as they possessed river crossing equipment. Iraq correctly deduced that Iran's defences at the crossing points around the Karkheh and Karoun Rivers were undermanned and that the rivers could be easily crossed. Iraqi intelligence was also informed that the Iranian forces in Khuzestan (which consisted of two divisions prior to the revolution) now only consisted of several ill-equipped and under-strength battalions. Only a handful of company-sized tank units remained operational.\n\nThe only qualms the Iraqis had were over the Islamic Republic of Iran Air Force (formerly the Imperial Iranian Air Force). Despite the purge of several key pilots and commanders, as well as the lack of spare parts, the air force showed its power during local uprisings and rebellions. They were also active after the failed U.S. attempt to rescue its hostages, Operation Eagle Claw. Based on these observations, Iraq's leaders decided to carry out a surprise airstrike against the Iranian air force's infrastructure prior to the main invasion.\n\nIn Iran, severe officer purges (including numerous executions ordered by Sadegh Khalkhali, the new Revolutionary Court judge), and shortages of spare parts for Iran's U.S.-made and British-made equipment had crippled Iran's once-mighty military. Between February and September 1979, Iran's government executed 85 senior generals and forced all major-generals and most brigadier-generals into early retirement. By September 1980, the government had purged 12,000 army officers. These purges resulted in a drastic decline in the Iranian military's operational capacities. Their regular army (which, in 1978, was considered the world's fifth most powerful) had been badly weakened. The desertion rate had reached 60%, and the officer corps was devastated. The most highly skilled soldiers and aviators were exiled, imprisoned, or executed. Throughout the war, Iran never managed to fully recover from this flight of human capital. Continuous sanctions prevented Iran from acquiring many heavy weapons, such as tanks and aircraft. When the invasion occurred, many pilots and officers were released from prison, or had their executions commuted to combat the Iraqis. In addition, many junior officers were promoted to generals, resulting in the army being more integrated as a part of the regime by the war's end, as it is today. Iran still had at least 1,000 operational tanks and several hundred functional aircraft, and could cannibalize equipment to procure spare parts.\n\nMeanwhile, a new paramilitary organisation gained prominence in Iran, the Islamic Revolutionary Guard Corps (often shortened to \"Revolutionary Guards\", and known in Iran as the \"Sepah-e-Pasdaran\"), which was intended to protect the new regime and counterbalance the decaying army. Despite having been trained as a paramilitary organisation, after the Iraqi invasion, they were forced to act as a regular army. Initially, they refused to fight alongside the army, which resulted in many defeats, but, by 1982, the two groups began carrying out combined operations. Another paramilitary militia was founded in response to the invasion, the \"Army of 20 Million\", commonly known as the Basij. The Basij were poorly armed and had members as young as 12 and as old as 70. They often acted in conjunction with the Revolutionary Guard, launching so-called human wave attacks and other campaigns against the Iraqis. They were subordinate to the Revolutionary Guards, and they made up most of the manpower that was used in the Revolutionary Guard's attacks.\n\nStephen Pelletiere wrote in his 1992 book \"The Iran–Iraq War: Chaos in a Vacuum\":\n\nThe most important dispute was over the Shatt al-Arab waterway. Iran repudiated the demarcation line established in the Constantinople Protocol of November 1913. Iran asked the border to run along the thalweg, the deepest point of the navigable channel. Iraq, encouraged by Britain, took Iran to the League of Nations in 1934, but their disagreement was not resolved. Finally in 1937 Iran and Iraq signed their first boundary treaty. The treaty established the waterway border on the eastern bank of the river except for a four-mile anchorage zone near Abadan, which was allotted to Iran and where the border ran along the thalweg. Iran sent a delegation to Iraq soon after the Ba'ath coup in 1969 and, when Iraq refused to proceed with negotiations over a new treaty, the treaty of 1937 was withdrawn by Iran. But five years later, on 17 September 1980, Iraq suddenly abrogated the Algiers Protocol following the Iranian revolution. Saddam Hussein claimed that the Islamic Republic of Iran refused to abide by the stipulations of the Algiers Protocol and, therefore, Iraq considered the Protocol null and void. Five days later, the Iraqi army crossed the border.\n\nIraq launched a full-scale invasion of Iran on 22 September 1980. The Iraqi Air Force launched surprise air strikes on ten Iranian airfields with the objective of destroying the Iranian Air Force. The attack failed to damage the Iranian Air Force significantly; it damaged some of Iran's airbase infrastructure, but failed to destroy a significant number of aircraft. The Iraqi Air Force was only able to strike in depth with a few MiG-23BN, Tu-22, and Su-20 aircraft, and Iran had built hardened aircraft shelters where most of its combat aircraft were stored.\n\nThe next day, Iraq launched a ground invasion along a front measuring in three simultaneous attacks. The invasion's purpose, according to Saddam, was to blunt the edge of Khomeini's movement and to thwart his attempts to export his Islamic revolution to Iraq and the Persian Gulf states. Saddam hoped that by annexing Khuzestan, he would cause such a blow to Iran's prestige that it would lead to the new government's downfall, or at least end Iran's calls for his overthrow.\n\nOf Iraq's six divisions that invaded by ground, four were sent to Khuzestan, which was located near the border's southern end, to cut off the Shatt al-Arab from the rest of Iran and to establish a territorial security zone. The other two divisions invaded across the northern and central part of the border to prevent an Iranian counter-attack. Two of the four Iraqi divisions, one mechanised and one armoured, operated near the southern end and began a siege of the strategically important port cities of Abadan and Khorramshahr.\n\nThe two armoured divisions secured the territory bounded by the cities of Khorramshahr, Ahvaz, Susangerd, and Musian. On the central front, the Iraqis occupied Mehran, advanced towards the foothills of the Zagros Mountains, and were able to block the traditional Tehran–Baghdad invasion route by securing territory forward of Qasr-e Shirin, Iran. On the northern front, the Iraqis attempted to establish a strong defensive position opposite Suleimaniya to protect the Iraqi Kirkuk oil complex. Iraqi hopes of an uprising by the ethnic Arabs of Khuzestan failed to materialise, as most of the ethnic Arabs remained loyal to Iran. The Iraqi troops advancing into Iran in 1980 were described by Patrick Brogan as \"badly led and lacking in offensive spirit\". The first known chemical weapons attack by Iraq on Iran probably took place during the fighting around Susangerd.\n\nThough the Iraqi air invasion surprised the Iranians, the Iranian air force retaliated the day after with a large-scale attack against Iraqi air bases and infrastructure in Operation Kaman 99. Groups of F-4 Phantom and F-5 Tiger fighter jets attacked targets throughout Iraq, such as oil facilities, dams, petrochemical plants, and oil refineries, and included Mosul Airbase, Baghdad, and the Kirkuk oil refinery. Iraq was taken by surprise at the strength of the retaliation, which caused the Iraqis heavy losses and economic disruption, but the Iranians took heavy losses as well as they lost many aircraft and aircrews to Iraqi air defenses.\n\nIranian Army Aviation's AH-1 Cobra helicopter gunships began attacks on the advancing Iraqi divisions, along with F-4 Phantoms armed with Maverick missiles; they destroyed numerous armoured vehicles and impeded the Iraqi advance, though not completely halting it. Meanwhile, Iraqi air attacks on Iran were repulsed by Iran's F-14 Tomcat interceptor fighter jets, using Phoenix missiles, which downed a dozen of Iraq's Soviet-built fighters in the first two days of battle. \n\nThe Iranian regular military, police forces, volunteer Basij, and Revolutionary Guards all conducted their operations separately; thus, the Iraqi invading forces did not face coordinated resistance. However, on 24 September, the Iranian Navy attacked Basra, Iraq, destroying two oil terminals near the Iraqi port Faw, which reduced Iraq's ability to export oil. The Iranian ground forces (primarily consisting of the Revolutionary Guard) retreated to the cities, where they set up defences against the invaders.\n\nOn 30 September, Iran's air force launched Operation Scorch Sword, striking and badly damaging the nearly-complete Osirak nuclear reactor near Baghdad. By 1 October, Baghdad had been subjected to eight air attacks. In response, Iraq launched aerial strikes against Iranian targets.\n\nThe mountainous border between Iran and Iraq made a deep ground invasion almost impossible, and air strikes were used instead. The invasion's first waves were a series of air strikes targeted at Iranian airfields. Iraq also attempted to bomb Tehran, Iran's capital and command centre, into submission.\n\nOn 22 September, a prolonged battle began in the city of Khorramshahr, eventually leaving 7,000 dead on each side. Reflecting the bloody nature of the struggle, Iranians came to call Khorramshahr \"City of Blood\".\n\nThe battle began with Iraqi air raids against key points and mechanised divisions advancing on the city in a crescent-like formation. They were slowed by Iranian air attacks and Revolutionary Guard troops with recoilless rifles, rocket-propelled grenades, and Molotov cocktails. The Iranians flooded the marsh areas around the city, forcing the Iraqis to traverse through narrow strips of land. Iraqi tanks launched attacks with no infantry support, and many tanks were lost to Iranian anti-tank teams. However, by 30 September, the Iraqis had managed to clear the Iranians from the outskirts of the city. The next day, the Iraqis launched infantry and armoured attacks into the city. After heavy house-to-house fighting, the Iraqis were repelled. On 14 October, the Iraqis launched a second offensive. The Iranians launched a controlled withdrawal from the city, street by street. By 24 October, most of the city was captured, and the Iranians evacuated across the Karun River. Some partisans remained, and fighting continued until 10 November.\n\nThe people of Iran, rather than turning against their still-weak Islamic Republic, rallied around their country. An estimated 200,000 fresh troops had arrived at the front by November, many of them ideologically committed volunteers.\n\nThough Khorramshahr was finally captured, the battle had delayed the Iraqis enough to allow the large-scale deployment of the Iranian military. In November, Saddam ordered his forces to advance towards Dezful and Ahvaz, and lay sieges to both cities. However, the Iraqi offensive had been badly damaged by Iranian militias and air power. Iran's air force had destroyed Iraq's army supply depots and fuel supplies, and was strangling the country through an aerial siege. On the other hand, Iran's supplies had not been exhausted, despite sanctions, and the military often cannibalised spare parts from other equipment and began searching for parts on the black market. On 28 November, Iran launched Operation \"Morvarid\" (Pearl), a combined air and sea attack which destroyed 80% of Iraq's navy and all of its radar sites in the southern portion of the country. When Iraq laid siege to Abadan and dug its troops in around the city, it was unable to blockade the port, which allowed Iran to resupply Abadan by sea.\n\nIraq's strategic reserves had been depleted, and by now it lacked the power to go on any major offensives until nearly the end of the war. On 7 December, Hussein announced that Iraq was going on the defensive. By the end of 1980, Iraq had destroyed about 500 Western-built Iranian tanks and captured 100 others.\n\nFor the next eight months, both sides were on a defensive footing (with the exception of the Battle of Dezful), as the Iranians needed more time to reorganise their forces after the damage inflicted by the purge of 1979–80. During this period, fighting consisted mainly of artillery duels and raids. Iraq had mobilised 21 divisions for the invasion, while Iran countered with only 13 regular army divisions and one brigade. Of the regular divisions, only seven were deployed to the border. The war bogged down into World War I-style trench warfare with tanks and modern late-20th century weapons. Due to the power of anti-tank weapons such as the RPG-7, armored manoeuvre by the Iraqis was very costly, and they consequently entrenched their tanks into static positions.\n\nIraq also began firing Scud missiles into Dezful and Ahvaz, and used terror bombing to bring the war to the Iranian civilian population. Iran launched dozens of \"human wave assaults\".\n\nOn 5 January 1981, Iran had reorganised its forces enough to launch a large-scale offensive, Operation \"Nasr\" (Victory). The Iranians launched their major armoured offensive from Dezful in the direction of Susangerd, consisting of tank brigades from the 16th \"Qazvin\", 77th \"Khorasan\", and 92nd \"Khuzestan\" armoured divisions, and broke through Iraqi lines. However, the Iranian tanks had raced through Iraqi lines with their flanks unprotected and with no infantry support; as a result, they were cut off by Iraqi tanks. In the ensuing Battle of Dezful, the Iranian armoured divisions were nearly wiped out in one of the biggest tank battles of the war. When the Iranian tanks tried to manoeuvre, they became stuck in the mud of the marshes, and many tanks were abandoned. The Iraqis lost 45 T-55 and T-62 tanks, while the Iranians lost 100–200 Chieftain and M-60 tanks. Reporters counted roughly 150 destroyed or deserted Iranian tanks, and also 40 Iraqi tanks. 141 Iranians were killed during the battle.\n\nThe battle had been ordered by Iranian president Abulhassan Banisadr, who was hoping that a victory might shore up his deteriorating political position; instead, the failure hastened his fall. Many of Iran's problems took place because of political infighting between President Banisadr, who supported the regular army, and the hardliners who supported the IRGC. Once he was impeached and the competition ended, the performance of the Iranian military improved. Iran was further distracted by internal fighting between the regime and the Islamic Marxist \"Mujaheddin e-Khalq\" (MEK) on the streets of Iran's major cities in June 1981 and again in September. After the end of these battles, the MEK gradually leaned towards Saddam, completely taking his side by the mid-1980s..\n\nThe People's Mujahedin of Iran started to take the side of Saddam in 1984 or 1986 (mid 1980s). In 1986, Rajavi moved from Paris to Iraq and set up a base on the Iranian border. The Battle of Dezful became a critical battle in Iranian military thinking. Less emphasis was placed on the Army with its conventional tactics, and more emphasis was placed on the Revolutionary Guard with its unconventional tactics.\n\nThe Iraqi Air Force, badly damaged by the Iranians, was moved to the H-3 Airbase in Western Iraq, near the Jordanian border and away from Iran. However, on 3 April 1981, the Iranian air force used eight F-4 Phantom fighter bombers, four F-14 Tomcats, three Boeing 707 refuelling tankers, and one Boeing 747 command plane to launch a surprise attack on H3, destroying 27–50 Iraqi fighter jets and bombers.\n\nDespite the successful H-3 airbase attack (in addition to other air attacks), the Iranian Air Force was forced to cancel its successful 180-day air offensive. In addition, they gave up trying to hold total control of Iranian airspace. Due to the heavy toll of sanctions and pre-war purges, the Iranian Air Force could not suffer further attrition, and made the decision in order to limit their losses. They were also damaged by a fresh purge, after the impeachment crisis of President Banisadr. The Iranian air force would fight heavily on the defensive, trying to hold back the Iraqis rather than engaging them. While throughout 1981–1982 the Iraqi air force would remain weak, within the next few years they would rearm and expand again, and begin to regain the strategic initiative.\n\nThe Iranians suffered from a shortage of heavy weapons, but had a large number of devoted volunteer troops, so they began using human wave attacks against the Iraqis. Typically, an Iranian assault would commence with poorly trained Basij who would launch the primary human wave assaults to swamp the weakest portions of the Iraqi lines en masse (on some occasions even bodily clearing minefields). This would be followed up by the more experienced Revolutionary Guard infantry, who would breach the weakened Iraqi lines, and followed up by the regular army using mechanized forces, who would maneuver through the breach and attempt to encircle and defeat the enemy.\n\nAccording to historian Stephen C. Pelletiere, the idea of Iranian \"human wave attacks\" was a misconception. Instead, the Iranian tactics consisted of using groups of 22 man infantry squads, which moved forward to attack specific objectives. As the squads surged forward to execute their missions, that gave the impression of a \"human wave attack\". Nevertheless, the idea of \"human wave attacks\" remained virtually synonymous with any large-scale infantry frontal assault Iran carried out. Large numbers of troops would be used, aimed at overwhelming the Iraqi lines (usually the weakest portion manned by the Iraqi Popular Army) regardless of losses.\n\nAccording to the former Iraqi general Ra'ad al-Hamdani, the Iranian human wave charges consisted of armed \"civilians\" who carried most of their necessary equipment themselves into battle and often lacked command and control and logistics. Operations were often carried out during the night and deception operations, infiltrations, and maneuvers became more common. The Iranians would also reinforce the infiltrating forces with new units to keep up their momentum. Once a weak point was found, the Iranians would concentrate all of their forces into that area in an attempt to break through with human wave attacks.\n\nThe human wave attacks, while extremely bloody (tens of thousands of troops died in the process), when used in combination with infiltration and surprise, caused major Iraqi defeats. As the Iraqis would dig in their tanks and infantry into static, entrenched positions, the Iranians would manage to break through the lines and encircle entire divisions. Merely the fact that the Iranian forces used maneuver warfare by their light infantry against static Iraqi defenses was often the decisive factor in the battle. However, lack of coordination between the Iranian Army and IRGC and shortages of heavy weaponry played a detrimental role, often with most of the infantry not being supported by artillery and armor.\n\nAfter the Iraqi offensive stalled in March 1981, there was little change in the front other than Iran retaking the high ground above Susangerd in May. By late 1981, Iran returned to the offensive and launched a new operation (Operation \"Samen-ol-A'emeh\" (The Eighth Imam)), ending the Iraqi Siege of Abadan on 27–29 September 1981. The Iranians used a combined force of regular army artillery with small groups of armor, supported by Pasdaran and Basij infantry. On 15 October, after breaking the siege, a large Iranian convoy was ambushed by Iraqi tanks, and during the ensuing tank battle Iran lost 20 Chieftains and other armored vehicles and withdrew from the previously gained territory.\n\nOn 29 November 1981, Iran began Operation \"Tariq al-Qods\" with three army brigades and seven Revolutionary Guard brigades. The Iraqis failed to properly patrol their occupied areas, and the Iranians constructed a road through the unguarded sand dunes, launching their attack from the Iraqi rear. The town of Bostan was retaken from Iraqi divisions by 7 December. By this time the Iraqi Army was experiencing serious morale problems, compounded by the fact that Operation Tariq al-Qods marked the first use of Iranian \"human wave\" tactics, where the Revolutionary Guard light infantry repeatedly charged at Iraqi positions, oftentimes without the support of armour or air power. The fall of Bostan exacerbated the Iraqis' logistical problems, forcing them to use a roundabout route from Ahvaz to the south to resupply their troops. 6,000 Iranians and over 2,000 Iraqis were killed in the operation.\n\nThe Iraqis, realising that the Iranians were planning to attack, decided to preempt them with Operation \"al-Fawz al-'Azim\" (Supreme Success) on 19 March. Using a large number of tanks, helicopters, and fighter jets, they attacked the Iranian buildup around the Roghabiyeh pass. Though Saddam and his generals assumed they had succeeded, in reality the Iranian forces remained fully intact. The Iranians had concentrated much of their forces by bringing them directly from the cities and towns throughout Iran via trains, buses, and private cars. The concentration of forces did not resemble a traditional military buildup, and although the Iraqis detected a population buildup near the front, they failed to realise that this was an attacking force. As a result, Saddam's army was unprepared for the Iranian offensives to come.\n\nIran's next major offensive, led by then Colonel Ali Sayad Shirazi, was Operation \"Fath-ol-Mobeen\" (Undeniable Victory). On 22 March 1982, Iran launched an attack which took the Iraqi forces by surprise: using Chinook helicopters, they landed behind Iraqi lines, silenced their artillery, and captured an Iraqi headquarters. The Iranian Basij then launched \"human wave\" attacks, consisting of 1,000 fighters per wave. Though they took heavy losses, they eventually broke through Iraqi lines.\n\nThe Revolutionary Guard and regular army followed up by surrounding the Iraqi 9th and 10th Armoured and 1st Mechanised divisions that had camped close to the Iranian town of Shush. The Iraqis launched a counter-attack using their 12th Armoured division to break the encirclement and rescue the surrounded divisions. Iraqi tanks came under attack by 95 Iranian F-4 Phantom and F-5 Tiger fighter jets, destroying much of the division.\n\nOperation Undeniable Victory was an Iranian victory; Iraqi forces were driven away from Shush, Dezful and Ahvaz. The Iranian armed forces destroyed 320–400 Iraqi tanks and armored vehicles in a costly success. In just the first day of the battle, the Iranians lost 196 tanks. By this time, most of the Khuzestan province had been recaptured.\n\nIn preparation for Operation \"Beit ol-Moqaddas\", the Iranians had launched numerous air raids against Iraq air bases, destroying 47 jets (including Iraq's brand new Mirage F-1 fighter jets from France); this gave the Iranians air superiority over the battlefield while allowing them to monitor Iraqi troop movements.\n\nOn 29 April, Iran launched the offensive. 70,000 Revolutionary Guard and Basij members struck on several axes – Bostan, Susangerd, the west bank of the Karun River, and Ahvaz. The Basij launched human wave attacks, which were followed up by the regular army and Revolutionary Guard support along with tanks and helicopters. Under heavy Iranian pressure, the Iraqi forces retreated. By 12 May, Iran had driven out all Iraqi forces from the Susangerd area. The Iranians captured several thousand Iraqi troops and a large number of tanks. Nevertheless, the Iranians took many losses as well, especially among the Basij.\n\nThe Iraqis retreated to the Karun River, with only Khorramshahr and a few outlying areas remaining in their possession. Saddam ordered 70,000 troops to be placed around the city of Khorramshahr. The Iraqis created a hastily constructed defence line around the city and outlying areas. To discourage airborne commando landings, the Iraqis also placed metal spikes and destroyed cars in areas likely to be used as troop landing zones. Saddam Hussein even visited Khorramshahr in a dramatic gesture, swearing that the city would never be relinquished. However, Khorramshahr's only resupply point was across the Shatt al-Arab, and the Iranian air force began bombing the supply bridges to the city, while their artillery zeroed in on the besieged garrison.\n\nIn the early morning hours of 23 May 1982, the Iranians began the drive towards Khorramshahr across the Karun River. This part of Operation Beit ol-Moqaddas was spearheaded by the 77th Khorasan division with tanks along with the Revolutionary Guard and Basij. The Iranians hit the Iraqis with destructive air strikes and massive artillery barrages, crossed the Karun River, captured bridgeheads, and launched human wave attacks towards the city. Saddam's defensive barricade collapsed; in less than 48 hours of fighting, the city fell and 19,000 Iraqis surrendered to the Iranians. A total of 10,000 Iraqis were killed or wounded in Khorramshahr, while the Iranians suffered 30,000 casualties. During the whole of Operation Beit ol-Moqaddas, 33,000 Iraqi soldiers were captured by the Iranians.\n\nThe fighting had battered the Iraqi military: its strength fell from 210,000 to 150,000 troops; over 20,000 Iraqi soldiers were killed and over 30,000 captured; two out of four active armoured divisions and at least three mechanised divisions fell to less than a brigade's strength; and the Iranians had captured over 450 tanks and armoured personnel carriers.\n\nThe Iraqi Air Force was also left in poor shape: after losing up to 55 aircraft since early December 1981, they had only 100 intact fighter-bombers and interceptors. A defector who flew his MiG-21 to Syria in June 1982 revealed that the Iraqi Air Force had only three squadrons of fighter-bombers left that were capable of mounting offensive operations into Iran. The Iraqi Army Air Corps was in slightly better shape, and could still operate more than 70 helicopters. Despite this, the Iraqis still held 3,000 tanks, while Iran held 1,000.\n\nAt this point, Saddam believed that his army was too demoralised and damaged to hold onto Khuzestan and major swathes of territory in Iran, and withdrew his remaining armed forces from those areas. He redeployed them along the border with Iran as a means of defence. However, his troops continued to occupy some key border areas of Iran, and continued to hold onto the disputed territories that prompted his invasion, including the Shatt al-Arab waterway. In response to their failures against the Iranians in Khorramshahr, Saddam ordered the executions of General Juwad Shitnah, General Salah al-Qadhi, and Colonel Masa and al-Jalil. At least a dozen high-ranking officers were also executed during this time. This became an increasingly common punishment for those who failed him in battle.\n\nIn April 1982, the rival Ba'athist regime in Syria, one of the few nations that supported Iran, closed the Kirkuk–Baniyas pipeline that had allowed Iraqi oil to reach tankers on the Mediterranean, reducing the Iraqi budget by $5 billion per month. Journalist Patrick Brogan wrote, \"It appeared for a while that Iraq would be strangled economically before it was defeated militarily.\" Syria's closure of the Kirkuk–Baniyas pipeline left Iraq with the pipeline to Turkey as the only means of exporting oil. However, that pipeline had a capacity of only , which was insufficient to pay for the war. However, Saudi Arabia, Kuwait, and the other Gulf states saved Iraq from bankruptcy by providing it with an average of $60 billion in subsidies per year. Though Iraq had previously been hostile towards other Gulf states, \"the threat of Persian fundamentalism was far more feared.\" They were especially inclined to fear Iranian victory after Ayatollah Khomeini declared monarchies to be illegitimate and an un-Islamic form of government. Khomeini's statement was widely received as a call to overthrow the Gulf monarchies. Journalists John Bulloch and Harvey Morris wrote:\n\nThe virulent Iranian campaign, which at its peak seemed to be making the overthrow of the Saudi regime a war aim on a par with the defeat of Iraq, did have an effect on the Kingdom [of Saudi Arabia], but not the one the Iranians wanted: instead of becoming more conciliatory, the Saudis became tougher, more self-confident, and less prone to seek compromise.\n\nSaudi Arabia was said to provide Iraq with $1 billion per month starting in mid-1982.\n\nIraq began receiving support from the United States and west European countries as well. Saddam was given diplomatic, monetary, and military support by the United States, including massive loans, political influence, and intelligence on Iranian deployments gathered by American spy satellites. The Iraqis relied heavily on American satellite footage and radar planes to detect Iranian troop movements, and they enabled Iraq to move troops to the site before the battle.\n\nWith Iranian success on the battlefield, the United States increased its support of the Iraqi government, supplying intelligence, economic aid, and dual-use equipment and vehicles, as well as normalizing its intergovernmental relations (which had been broken during the 1967 Six-Day War). President Ronald Reagan decided that the United States \"could not afford to allow Iraq to lose the war to Iran\", and that the United States \"would do whatever was necessary to prevent Iraq from losing\". Reagan formalised this policy by issuing a National Security Decision Directive to this effect in June 1982.\n\nIn 1982, Reagan removed Iraq from the list of countries \"supporting terrorism\" and sold weapons such as howitzers to Iraq via Jordan. France sold Iraq millions of dollars worth of weapons, including Gazelle helicopters, Mirage F-1 fighters, and Exocet missiles. Both the United States and West Germany sold Iraq dual-use pesticides and poisons that would be used to create chemical and other weapons, such as Roland missiles.\n\nAt the same time, the Soviet Union, angered with Iran for purging and destroying the communist Tudeh Party, sent large shipments of weapons to Iraq. The Iraqi Air Force was replenished with Soviet, Chinese, and French fighter jets and attack/transport helicopters. Iraq also replenished their stocks of small arms and anti-tank weapons such as AK-47s and rocket-propelled grenades from its supporters. The depleted tank forces were replenished with more Soviet and Chinese tanks, and the Iraqis were reinvigorated in the face of the coming Iranian onslaught. Iran was portrayed as the aggressor, and would be seen as such until the 1990–1991 Persian Gulf War, when Iraq would be condemned.\n\nIran did not have the money to purchase arms to the same extent as Iraq did. They counted on China, North Korea, Libya, Syria, and Japan for supplying anything from weapons and munitions to logistical and engineering equipment.\nThere were also clandestine purchases from certain elements within Israel and the United States, who also bought small arms from China, via North Korea.\n\nOn June 20, 1982, Saddam announced that he wanted to sue for peace and proposed an immediate ceasefire and withdrawal from Iranian territory within two weeks. Khomeini responded by saying the war would not end until a new government was installed in Iraq and reparations paid. He proclaimed that Iran would invade Iraq and would not stop until the Ba'ath regime was replaced by an Islamic republic. Iran supported a government in exile for Iraq, the Supreme Council of the Islamic Revolution in Iraq, led by exiled Iraqi cleric Mohammad Baqer al-Hakim, which was dedicated to overthrowing the Ba'ath party. They recruited POW's, dissidents, exiles, and Shias to join the Badr Brigade, the military wing of the organisation.\n\nThe decision to invade Iraq was taken after much debate within the Iranian government. One faction, comprising Prime Minister Mir-Hossein Mousavi, Foreign Minister Ali Akbar Velayati, President Ali Khamenei, Army Chief of Staff General Ali Sayad Shirazi as well as Major General Qasem-Ali Zahirnejad, wanted to accept the ceasefire, as most of Iranian soil had been recaptured. In particular, General Shirazi and Zahirnejad were both opposed to the invasion of Iraq on logistical grounds, and stated they would consider resigning if \"unqualified people continued to meddle with the conduct of the war\". Of the opposing view was a hardline faction led by the clerics on the Supreme Defence Council, whose leader was the politically powerful speaker of the \"Majlis\", Akbar Hashemi Rafsanjani.\n\nIran also hoped that their attacks would ignite a revolt against Saddam's rule by the Shia and Kurdish population of Iraq, possibly resulting in his downfall. They were successful in doing so with the Kurdish population, but not the Shia. Iran had captured large quantities of Iraqi equipment (enough to create several tank battalions, Iran once again had 1,000 tanks) and also managed to clandestinely procure spare parts as well.\n\nAt a cabinet meeting in Baghdad, Minister of Health Riyadh Ibrahim Hussein suggested that Saddam could step down temporarily as a way of easing Iran towards a ceasefire, and then afterwards would come back to power. Saddam, annoyed, asked if anyone else in the Cabinet agreed with the Health Minister's idea. When no one raised their hand in support, he escorted Riyadh Hussein to the next room, closed the door, and shot him with his pistol. Saddam returned to the room and continued with his meeting.\n\nFor the most part, Iraq remained on the defensive for the next six years, unable and unwilling to launch any major offensives, while Iran launched more than 70 offensives. Iraq's strategy changed from holding territory in Iran to denying Iran any major gains in Iraq (as well as holding onto disputed territories and Iran's border areas). Saddam commenced a policy of total war, gearing most of his country towards defending against Iran. By 1988, Iraq was spending 40–75% of its GDP on military equipment. Saddam had also more than doubled the size of the Iraqi army, from 200,000 soldiers (12 divisions and 3 independent brigades) to 500,000 (23 divisions and nine brigades). They also began launching air raids against Iranian border cities, greatly increasing the practice by 1984. By the end of 1982, Iraq had been resupplied with new Soviet and Chinese materiel, and the ground war entered a new phase. Iraq used newly acquired T-55, T-62 and T-72 tanks (as well as Chinese copies), BM-21 truck-mounted rocket launchers, and Mi-24 helicopter gunships to prepare a Soviet-type three-line defence, replete with obstacles such as barbed wire, minefields, fortified positions and bunkers. The Combat Engineer Corps built bridges across water obstacles, laid minefields, erected earthen revetments, dug trenches, built machinegun nests, and prepared new defence lines and fortifications.\n\nIraq began to focus on using defense in depth to defeat the Iranians. Iraq created multiple static defense lines to bleed the Iranians through sheer size. When faced against large Iranian attack, where human waves would overrun Iraq's entrenched infantry defences, the Iraqis would often retreat, but their static defences would bleed the Iranians and channel them into certain directions, drawing them into traps or pockets. Iraqi air and artillery attacks would then pin the Iranians down, while tanks and mechanised infantry attacks using mobile warfare would push them back. Sometimes, the Iraqis would launch \"probing attacks\" into the Iranian lines to provoke them into launching their attacks sooner. While Iranian human wave attacks were successful against the dug in Iraqi forces in Khuzestan, they had trouble breaking through Iraq's defense in depth lines. Iraq had a logistical advantage in their defence: the front was located near the main Iraqi bases and arms depots, allowing their army to be efficiently supplied. By contrast, the front in Iran was a considerable distance away from the main Iranian bases and arms depots, and as such, Iranian troops and supplies had to travel through mountain ranges before arriving at the front.\n\nIn addition, Iran's military power was weakened once again by large purges in 1982, resulting from another supposedly attempted coup.\n\nThe Iranian generals wanted to launch an all-out attack on Baghdad and seize it before the weapon shortages continued to manifest further. Instead, that was rejected as being unfeasible, and the decision was made to capture one area of Iraq after the other in the hopes that a series of blows delivered foremost by the Revolutionary Guards Corps would force a political solution to the war (including Iraq withdrawing completely from the disputed territories along the border).\n\nThe Iranians planned their attack in southern Iraq, near Basra. Called Operation Ramadan, it involved over 180,000 troops from both sides, and was one of the largest land battles since World War II. Iranian strategy dictated that they launch their primary attack on the weakest point of the Iraqi lines; however, the Iraqis were informed of Iran's battle plans and moved all of their forces to the area the Iranians planned to attack. The Iraqis were equipped with tear gas to use against the enemy, which would be first major use of chemical warfare during the conflict, throwing an entire attacking division into chaos.\n\nOver 100,000 Revolutionary Guards and Basij volunteer forces charged towards the Iraqi lines. The Iraqi troops had entrenched themselves in formidable defences, and had set up a network of bunkers and artillery positions. The Basij used human waves, and were even used to bodily clear the Iraqi minefields and allow the Revolutionary Guards to advance. Combatants came so close to one another that Iranians were able to board Iraqi tanks and throw grenades inside the hulls. By the eighth day, the Iranians had gained inside Iraq and had taken several causeways. Iran's Revolutionary Guards also used the T-55 tanks they had captured in earlier battles.\n\nHowever, the attacks came to a halt and the Iranians turned to defensive measures. Seeing this, Iraq used their Mi-25 helicopters, along with Gazelle helicopters armed with Euromissile HOT, against columns of Iranian mechanised infantry and tanks. These \"hunter-killer\" teams of helicopters, which had been formed with the help of East German advisors, proved to be very costly for Iranians. Aerial dogfights occurred between Iraqi MiGs and Iranian F-4 Phantoms.\n\nOn 16 July, Iran tried again further north and managed to push the Iraqis back. However, only from Basra, the poorly equipped Iranian forces were surrounded on three sides by Iraqis with heavy weaponry. Some were captured, while many were killed. Only a last-minute attack by Iranian AH-1 Cobra helicopters stopped the Iraqis from routing the Iranians. Three more similar attacks occurred around the Khorramshar-Baghdad road area towards the end of the month, but none were significantly successful. Iraq had concentrated three armoured divisions, the 3rd, 9th, and 10th, as a counter-attack force to attack any penetrations. They were successful in defeating the Iranian breakthroughs, but suffered heavy losses. The 9th Armoured Division in particular had to be disbanded, and was never reformed. The total casualty toll had grown to include 80,000 soldiers and civilians. 400 Iranian tanks and armored vehicles were destroyed or abandoned, while Iraq lost no fewer than 370 tanks.\n\nAfter Iran's failure in Operation Ramadan, they carried out only a few smaller attacks. Iran launched two limited offensives aimed at reclaiming the Sumar Hills and isolating the Iraqi pocket at Naft Shahr at the international border, both of which were part of the disputed territories still under Iraqi occupation. They then aimed to capture the Iraqi border town of Mandali. They planned to take the Iraqis by surprise using Basij militiamen, army helicopters, and some armoured forces, then stretch their defences and possibly break through them to open a road to Baghdad for future exploitation. During Operation \"Muslim ibn Aqil\" (1–7 October), Iran recovered of disputed territory straddling the international border and reached the outskirts of Mandali before being stopped by Iraqi helicopter and armoured attacks. During Operation \"Muharram\" (1–21 November), the Iranians captured part of the Bayat oilfield with the help of their fighter jets and helicopters, destroying 105 Iraqi tanks, 70 APCs, and 7 planes with few losses. They nearly breached the Iraqi lines but failed to capture Mandali after the Iraqis sent reinforcements, including brand new T-72 tanks, which possessed armour that could not be pierced from the front by Iranian TOW missiles. The Iranian advance was also impeded by heavy rains. 3,500 Iraqis and an unknown number of Iranians died, with only minor gains for Iran.\n\nAfter the failure of the 1982 summer offensives, Iran believed that a major effort along the entire breadth of the front would yield victory. During the course of 1983, the Iranians launched five major assaults along the front, though none achieved substantial success, as the Iranians staged more massive \"human wave\" attacks. By this time, it was estimated that no more than 70 Iranian fighter aircraft were still operational at any given time; Iran had its own helicopter repair facilities, left over from before the revolution, and thus often used helicopters for close air support. While Iranian fighter pilots had superior training compared to their Iraqi counterparts (as most had received training from US officers before the 1979 revolution) and would continue to dominate in combat, aircraft shortages, the size of defended territory and American intelligence supplied to Iraq allowed the Iraqis to exploit gaps in Iranian airspace. Iraqi air campaigns met little opposition, striking over half of Iran, and the Iraqis were able to gain air superiority towards the end of the war.\n\nIn Operation \"Fajr al-Nasr\" (Before the Dawn/Dawn of Victory), launched 6 February 1983, the Iranians shifted focus from the southern to the central and northern sectors. Employing 200,000 \"last reserve\" Revolutionary Guard troops, Iran attacked along a stretch near al-Amarah, Iraq, about southeast of Baghdad, in an attempt to reach the highways connecting northern and southern Iraq. The attack was stalled by of hilly escarpments, forests, and river torrents blanketing the way to al-Amarah, but the Iraqis could not force the Iranians back. Iran directed artillery on Basra, Al Amarah, and Mandali.\n\nThe Iranians suffered a large number of casualties clearing minefields and breaching Iraqi anti-tank mines, which Iraqi engineers were unable to replace. After this battle, Iran reduced its use of human wave attacks, though they still remained a key tactic as the war went on.\n\nFurther Iranian attacks were mounted in the Mandali–Baghdad north-central sector in April 1983, but were repelled by Iraqi mechanised and infantry divisions. Casualties were high, and by the end of 1983, an estimated 120,000 Iranians and 60,000 Iraqis had been killed. Iran, however, held the advantage in the war of attrition.\n\nFrom early 1983–1984, Iran launched a series of four \"Valfajr\" (Dawn) Operations (that eventually numbered to 10). During Operation Dawn-1, in early February 1983, 50,000 Iranian forces attacked westward from Dezful and were confronted by 55,000 Iraqi forces. The Iranian objective was to cut off the road from Basra to Baghdad in the central sector. The Iraqis carried out 150 air sorties against the Iranians, and even bombed Dezful, Ahvaz, and Khorramshahr in retribution. The Iraqi counterattack was broken up by Iran's 92nd Armoured Division.\n\nDuring Operation Dawn-2, the Iranian's directed insurgency operations by proxy in April 1983 by supporting the Kurds in the north. With Kurdish support, the Iranians attacked on 23 July 1983, capturing the Iraqi town of Haj Omran and maintaining it against an Iraqi poison gas counteroffensive. This operation incited Iraq to later conduct indiscriminate chemical attacks against the Kurds. The Iranians attempted to further exploit activities in the north on 30 July 1983, during Operation Dawn-3. Iran saw an opportunity to sweep away Iraqi forces controlling the roads between the Iranian mountain border towns of Mehran, Dehloran and Elam. Iraq launched airstrikes, and equipped attack helicopters with chemical warheads; while ineffective, it demonstrated both the Iraqi general staff's and Saddam's increasing interest in using chemical weapons. In the end, 17,000 had been killed on both sides, with no gain for either country.\n\nThe focus of Operation Dawn-4 in September 1983 was the northern sector in Iranian Kurdistan. Three Iranian regular divisions, the Revolutionary Guard, and Kurdistan Democratic Party (KDP) elements amassed in Marivan and Sardasht in a move to threaten the major Iraqi city Suleimaniyah. Iran's strategy was to press Kurdish tribes to occupy the Banjuin Valley, which was within of Suleimaniyah and from the oilfields of Kirkuk. To stem the tide, Iraq deployed Mi-8 attack helicopters equipped with chemical weapons and executed 120 sorties against the Iranian force, which stopped them into Iraqi territory. 5,000 Iranians and 2,500 Iraqis died. Iran gained of its territory back in the north, gained of Iraqi land, and captured 1,800 Iraqi prisoners while Iraq abandoned large quantities of valuable weapons and war materiel in the field. Iraq responded to these losses by firing a series of SCUD-B missiles into the cities of Dezful, Masjid Soleiman, and Behbehan. Iran's use of artillery against Basra while the battles in the north raged created multiple fronts, which effectively confused and wore down Iraq.\n\nPreviously, the Iranians had outnumbered the Iraqis on the battlefield, but Iraq expanded their military draft (pursuing a policy of total war), and by 1984, the armies were equal in size. By 1986, Iraq had twice as many soldiers as Iran. By 1988, Iraq would have 1 million soldiers, giving it the fourth largest army in the world. Some of their equipment, such as tanks, outnumbered the Iranians' by at least five to one. Iranian commanders, however, remained more tactically skilled.\n\nAfter the Dawn Operations, Iran attempted to change tactics. In the face of increasing Iraqi defense in depth, as well as increased armaments and manpower, Iran could no longer rely on simple human wave attacks. Iranian offensives became more complex and involved extensive maneuver warfare using primarily light infantry. Iran launched frequent, and sometimes smaller offensives to slowly gain ground and deplete the Iraqis through attrition. They wanted to drive Iraq into economic failure by wasting money on weapons and war mobilization, and to deplete their smaller population by bleeding them dry, in addition to creating an anti-government insurgency (they were successful in Kurdistan, but not southern Iraq). Iran also supported their attacks with heavy weaponry when possible and with better planning (although the brunt of the battles still fell to the infantry). The Army and Revolutionary Guards worked together better as their tactics improved. Human wave attacks became less frequent (although still used). To negate the Iraqi advantage of defense in depth, static positions, and heavy firepower, Iran began to focus on fighting in areas where the Iraqis could not use their heavy weaponry, such as marshes, valleys, and mountains, and frequently using infiltration tactics.\n\nIran began training troops in infiltration, patrolling, night-fighting, marsh warfare, and mountain warfare. They also began training thousands of Revolutionary Guard commandos in amphibious warfare, as southern Iraq is marshy and filled with wetlands. Iran used speedboats to cross the marshes and rivers in southern Iraq and landed troops on the opposing banks, where they would dig and set up pontoon bridges across the rivers and wetlands to allow heavy troops and supplies to cross. Iran also learned to integrate foreign guerrilla units as part of their military operations. On the northern front, Iran began working heavily with the Peshmerga, Kurdish guerrillas. Iranian military advisors organised the Kurds into raiding parties of 12 guerrillas, which would attack Iraqi command posts, troop formations, infrastructure (including roads and supply lines), and government buildings. The oil refineries of Kirkuk became a favourite target, and were often hit by homemade Peshmerga rockets.\n\nBy 1984, the Iranian ground forces were reorganised well enough for the Revolutionary Guard to start Operation Kheibar, which lasted from 24 February to 19 March. On 15 February 1984, the Iranians began launching attacks against the central section of the front, where the Second Iraqi Army Corps was deployed: 250,000 Iraqis faced 250,000 Iranians. The goal of this new major offensive was the capture of Basra-Baghdad Highway, cutting off Basra from Baghdad and setting the stage for an eventual attack upon the city. The Iraqi high command had assumed that the marshlands above Basra were natural barriers to attack, and had not reinforced them. The marshes negated Iraqi advantage in armor, and absorbed artillery rounds and bombs.\nPrior to the attack, Iranian commandos on helicopters had landed behind Iraqi lines and destroyed Iraqi artillery. Iran launched two preliminary attacks prior to the main offensive, Operation Dawn 5 and Dawn 6. They saw the Iranians attempting to capture Kut al-Imara, Iraq and sever the highway connecting Baghdad to Basra, which would impede Iraqi coordination of supplies and defences. Iranian troops crossed the river on motorboats in a surprise attack, though only came within of the highway.\n\nOperation Kheibar began on 24 February with Iranian infantrymen crossing the Hawizeh Marshes using motorboats and transport helicopters in an amphibious assault. The Iranians attacked the vital oil-producing Majnoon Island by landing troops via helicopters onto the islands and severing the communication lines between Amareh and Basra. They then continued the attack towards Qurna. By 27 February, they had captured the island, but suffered catastrophic helicopter losses to IRAF. On that day, a massive array of Iranian helicopters transporting Pasdaran troops were intercepted by Iraqi combat aircraft (MiGs, Mirages and Sukhois). In what was essentially an aerial slaughter, Iraqi jets shot down 49 of 50 Iranian helicopters. At times, fighting took place in waters over deep. Iraq ran live electrical cables through the water, electrocuting numerous Iranian troops and then displaying their corpses on state television.\n\nBy 29 February, the Iranians had reached the outskirts of Qurna and were closing in on the Baghdad–Basra highway. They had broken out of the marshes and returned to open terrain, where they were confronted by conventional Iraqi weapons, including artillery, tanks, air power, and mustard gas. 1,200 Iranian soldiers were killed in the counter-attack. The Iranians retreated back to the marshes, though they still held onto them along with Majnoon Island.\n\nThe Battle of the Marshes saw an Iraqi defence that had been under continuous strain since 15 February; they were relieved by their use of chemical weapons and defence-in-depth, where they layered defensive lines: even if the Iranians broke through the first line, they were usually unable to break through the second due to exhaustion and heavy losses. They also largely relied on Mi-24 Hind to \"hunt\" the Iranian troops in the marshes, and at least 20,000 Iranians were killed in the marsh battles. Iran used the marshes as a springboard for future attacks/infiltrations.\n\nFour years into the war, the human cost to Iran had been 170,000 combat fatalities and 340,000 wounded. Iraqi combat fatalities were estimated at 80,000 with 150,000 wounded.\n\nUnable to launch successful ground attacks against Iran, Iraq used their now expanded air force to carry out strategic bombing against Iranian shipping, economic targets, and cities in order to damage Iran's economy and morale. Iraq also wanted to provoke Iran into doing something that would cause the superpowers to be directly involved in the conflict on the Iraqi side.\n\nThe so-called \"Tanker War\" started when Iraq attacked the oil terminal and oil tankers at Kharg Island in early 1984. Iraq's aim in attacking Iranian shipping was to provoke the Iranians to retaliate with extreme measures, such as closing the Strait of Hormuz to all maritime traffic, thereby bringing American intervention; the United States had threatened several times to intervene if the Strait of Hormuz were closed. As a result, the Iranians limited their retaliatory attacks to Iraqi shipping, leaving the strait open to general passage.\n\nIraq declared that all ships going to or from Iranian ports in the northern zone of the Persian Gulf were subject to attack. They used F-1 Mirage, Super Etendard, Mig-23, Su-20/22, and Super Frelon helicopters armed with Exocet anti-ship missiles as well as Soviet-made air-to-surface missiles to enforce their threats. Iraq repeatedly bombed Iran's main oil export facility on Kharg Island, causing increasingly heavy damage. As a first response to these attacks, Iran attacked a Kuwaiti tanker carrying Iraqi oil near Bahrain on 13 May 1984, as well as a Saudi tanker in Saudi waters on 16 May. Because Iraq had become landlocked during the invasion, they had to rely on their Arab allies, primarily Kuwait, to transport their oil. Iran attacked tankers carrying Iraqi oil from Kuwait, later attacking tankers from any Persian Gulf state supporting Iraq. Attacks on ships of noncombatant nations in the Persian Gulf sharply increased thereafter, with both nations attacking oil tankers and merchant ships of neutral nations in an effort to deprive their opponent of trade. The Iranian attacks against Saudi shipping led to Saudi F-15s shooting down a pair of F-4 Phantom II on 5 June 1984.\n\nThe air and small-boat attacks, however, did little damage to Persian Gulf state economies, and Iran moved its shipping port to Larak Island in the Strait of Hormuz.\n\nThe Iranian Navy imposed a naval blockade of Iraq, using its British-built frigates to stop and inspect any ships thought to be trading with Iraq. They operated with virtual impunity, as Iraqi pilots had little training in hitting naval targets. Some Iranian warships attacked tankers with ship-to-ship missiles, while others used their radars to guide land-based anti-ship missiles to their targets. Iran began to rely on its new Revolutionary Guard's navy, which used Boghammar speedboats fitted with rocket launchers and heavy machine guns. These speedboats would launch surprise attacks against tankers and cause substantial damage. Iran also used F-4 Phantoms II and helicopters to launch Maverick missiles and unguided rockets at tankers.\n\nA U.S. Navy ship, , was struck on 17 May 1987 by two Exocet anti-ship missiles fired from an Iraqi F-1 Mirage plane. The missiles had been fired at about the time the plane was given a routine radio warning by \"Stark\". The frigate did not detect the missiles with radar, and warning was given by the lookout only moments before they struck. Both missiles hit the ship, and one exploded in crew quarters, killing 37 sailors and wounding 21.\n\nLloyd's of London, a British insurance market, estimated that the Tanker War damaged 546 commercial vessels and killed about 430 civilian sailors. The largest portion of the attacks was directed by Iraq against vessels in Iranian waters, with the Iraqis launching three times as many attacks as the Iranians. But Iranian speedboat attacks on Kuwaiti shipping led Kuwait to formally petition foreign powers on 1 November 1986 to protect its shipping. The Soviet Union agreed to charter tankers starting in 1987, and the United States Navy offered to provide protection for foreign tankers reflagged and flying the U.S. flag starting 7 March 1987 in Operation Earnest Will. Neutral tankers shipping to Iran were unsurprisingly not protected by Earnest Will, resulting in reduced foreign tanker traffic to Iran, since they risked Iraqi air attack. Iran accused the United States of helping Iraq.\n\nDuring the course of the war, Iran attacked two Soviet Navy ships which were protecting Kuwaiti tankers.\n\n\"Seawise Giant\", the largest ship ever built, was struck and damaged by Iraqi Exocet missiles as it was carrying Iranian crude oil out of the Gulf.\n\nMeanwhile, Iraq's air force also began carrying out strategic bombing raids against Iranian cities. While Iraq had launched numerous attacks with aircraft and missiles against border cities from the beginning of the war and sporadic raids on Iran's main cities, this was the first systematic strategic bombing that Iraq carried out during the war. This would become known as the \"War of the Cities\". With the help of the USSR and the west, Iraq's air force had been rebuilt and expanded. Meanwhile, Iran, due to sanctions and lack of spare parts, had heavily curtailed its air force operations. Iraq used Tu-22 Blinder and Tu-16 Badger strategic bombers to carry out long-range high-speed raids on Iranian cities, including Tehran. Fighter-bombers such as the Mig-25 Foxbat and Su-22 Fitter were used against smaller or shorter range targets, as well as escorting the strategic bombers. Civilian and industrial targets were hit by the raids, and each successful raid inflicted economic damage from regular strategic bombing.\n\nIn response, the Iranians deployed their F-4 Phantoms to combat the Iraqis, and eventually they deployed F-14s as well. Most of the Iraqi air raids were intercepted by the Iranian fighter jets and air defense, but some also successfully hit their targets, becoming a major headache for Iran. By 1986, Iran also expanded their air defense network heavily to take the load of the fighting off the air force. By later in the war, Iraqi raids primarily consisted of indiscriminate missile attacks while air attacks were used only on fewer, more important targets. Starting in 1987, Saddam also ordered several chemical attacks on civilian targets in Iran, such as the town of Sardasht.\n\nIran also launched several retaliatory air raids on Iraq, while primarily shelling border cities such as Basra. Iran also bought some Scud missiles from Libya, and launched them against Baghdad. These too inflicted damage upon Iraq.\n\nOn 7 February 1984, during the first war of the cities, Saddam ordered his air force to attack eleven Iranian cities; bombardments ceased on 22 February 1984. Though Saddam had aimed for the attacks to demoralise Iran and force them to negotiate, they had little effect, and Iran quickly repaired the damage. Iraq's air force took heavy losses, however, and Iran struck back, hitting Baghdad and other Iraqi cities. Nevertheless, the attacks resulted in tens of thousands of civilian casualties on both sides, and became known as the first \"war of the cities\". It was estimated that 1,200 Iranian civilians were killed during the raids in February alone. There would be five such major exchanges throughout the course of the war, and multiple minor ones. While interior cities such as Tehran, Tabriz, Qom, Isfahan and Shiraz did receive numerous raids, it was the cities of western Iran that suffered the most death and destruction.\n\nBy 1984, Iran's losses were estimated to be 300,000 soldiers, while Iraq's losses were estimated to be 150,000. Foreign analysts agreed that both Iran and Iraq failed to use their modern equipment properly, and both sides failed to carry out modern military assaults that could win the war. Both sides also abandoned equipment in the battlefield because their technicians were unable to carry out repairs. Iran and Iraq showed little internal coordination on the battlefield, and in many cases units were left to fight on their own. As a result, by the end of 1984, the war was a stalemate. One limited offensive Iran launched (Dawn 7) took place from 18–25 October 1984, when they recaptured the Iranian city of Mehran, which had been occupied by the Iraqis from the beginning of the war.\n\nBy 1985, Iraqi armed forces were receiving financial support from Saudi Arabia, Kuwait, and other Persian Gulf states, and were making substantial arms purchases from the Soviet Union, China, and France. For the first time since early 1980, Saddam launched new offensives.\n\nOn 6 January 1986, the Iraqis launched an offensive attempting to retake Majnoon Island. However, they were quickly bogged down into a stalemate against 200,000 Iranian infantrymen, reinforced by amphibious divisions. However, they managed to gain a foothold in the southern part of the island.\n\nIraq also carried out another \"war of the cities\" between 12–14 March, hitting up to 158 targets in over 30 towns and cities, including Tehran. Iran responded by launching 14 Scud missiles for the first time, purchased from Libya. More Iraqi air attacks were carried out in August, resulting in hundreds of additional civilian casualties. Iraqi attacks against both Iranian and neutral oil tankers in Iranian waters continued, with Iraq carrying out 150 airstrikes using French bought Super Etendard and Mirage F-1 jets as well as Super Frelon helicopters, using Exocet missiles.\n\nThe Iraqis attacked again on 28 January 1985; they were defeated, and the Iranians retaliated on 11 March 1985 with a major offensive directed against the Baghdad-Basra highway (one of the few major offensives conducted in 1985), codenamed Operation \"Badr\" (after the Battle of Badr, Muhammad's first military victory in Mecca). Ayatollah Khomeini urged Iranians on, declaring:\n\nIt is our belief that Saddam wishes to return Islam to blasphemy and polytheism...if America becomes victorious...and grants victory to Saddam, Islam will receive such a blow that it will not be able to raise its head for a long time...The issue is one of Islam versus blasphemy, and not of Iran versus Iraq.\n\nThis operation was similar to Operation Kheibar, though it invoked more planning. Iran used 100,000 troops, with 60,000 more in reserve. They assessed the marshy terrain, plotted points where they could land tanks, and constructed pontoon bridges across the marshes. The Basij forces were also equipped with anti-tank weapons.\n\nThe ferocity of the Iranian offensive broke through the Iraqi lines. The Revolutionary Guard, with the support of tanks and artillery, broke through north of Qurna on 14 March. That same night 3,000 Iranian troops reached and crossed the Tigris River using pontoon bridges and captured part of the Baghdad–Basra Highway 8, which they had failed to achieve in Operations Dawn 5 and 6.\n\nSaddam responded by launching chemical attacks against the Iranian positions along the highway and by initiating the aforementioned second \"war of the cities\", with an air and missile campaign against twenty to thirty Iranian population centres, including Tehran. Under General Sultan Hashim Ahmad al-Tai and General Jamal Zanoun (both considered to be among Iraq's most skilled commanders), the Iraqis launched air attacks against the Iranian positions and pinned them down. They then launched a pincer attack using mechanized infantry and heavy artillery. Chemical weapons were used, and the Iraqis also flooded Iranian trenches with specially constructed pipes delivering water from the Tigris River.\n\nThe Iranians retreated back to the Hoveyzeh marshes while being attacked by helicopters, and the highway was recaptured by the Iraqis. Operation Badr resulted in 10,000–12,000 Iraqi casualties and 15,000 Iranian ones.\n\nThe failure of the human wave attacks in earlier years had prompted Iran to develop a better working relationship between the Army and the Revolutionary Guard and to mould the Revolutionary Guard units into a more conventional fighting force. To combat Iraq's use of chemical weapons, Iran began producing an antidote. They also created and fielded their own homemade drones, the Mohajer 1's, fitted with six RPG-7's to launch attacks. They were primarily used in observation, being used for up to 700 sorties.\n\nFor the rest of 1986, and until the spring of 1988, the Iranian Air Force's efficiency in air defence increased, with weapons being repaired or replaced and new tactical methods being used. For example, the Iranians would loosely integrate their SAM sites and interceptors to create \"killing fields\" in which dozens of Iraqi planes were lost (which was reported in the West as the Iranian Air Force using F-14s as \"mini-AWACs\"). The Iraqi Air Force reacted by increasing the sophistication of its equipment, incorporating modern electronic countermeasure pods, decoys such as chaff and flare, and anti-radiation missiles. Due to the heavy losses in the last war of the cities, Iraq reduced their use of aerial attacks on Iranian cities. Instead, they would launch Scud missiles, which the Iranians could not stop. Since the range of the Scud missile was too short to reach Tehran, they converted them to al-Hussein missiles with the help of East German engineers, cutting up their Scuds into three chunks and attaching them together. Iran responded to these attacks by using their own Scud missiles.\n\nAside from extensive foreign help to Iraq, Iranian attacks were severely hampered by their shortages of weaponry, including heavy weaponry. Large portions of them had been lost during the last several years. Iran still managed to maintain 1,000 tanks (often by capturing Iraqi ones) and additional artillery, but many needed repairs to be operational. But by this time Iran managed to procure spare parts from various sources, helping them to restore some weapons. They secretly imported some weapons, such as RBS-70 anti-aircraft MANPADS. In an exception to the United States' support for Iraq, in exchange for Iran using its influence to help free western hostages in Lebanon, the United States secretly sold Iran some limited supplies (in Ayatollah Rafsanjani's postwar interview, he stated that during the period when Iran was succeeding, for a short time the United States supported Iran, then shortly after began helping Iraq again). Iran managed to get some advanced weapons, such as anti-tank TOW missiles, which worked better than rocket-propelled grenades. Iran later reverse-engineered and produced those weapons on their own as well. All of these almost certainly helped increase the effectiveness of Iran, although it did not reduce the human cost of their attacks.\n\nOn the night of 10–11 February 1986, the Iranians launched Operation Dawn 8, in which 30,000 troops comprising five Army divisions and men from the Revolutionary Guard and Basij advanced in a two-pronged offensive to capture the al-Faw peninsula in southern Iraq, the only area touching the Persian Gulf. The capture of Al Faw and Umm Qasr was a major goal for Iran. Iran began with a feint attack against Basra, which was stopped by the Iraqis; Meanwhile, an amphibious strike force landed at the foot of the peninsula. The resistance, consisting of several thousand poorly trained soldiers of the Iraqi Popular Army, fled or were defeated, and the Iranian forces set up pontoon bridges crossing the Shatt al-Arab, allowing 30,000 soldiers to cross in a short period of time. They drove north along the peninsula almost unopposed, capturing it after only 24 hours of fighting. Afterwards they dug in and set up defenses.\n\nThe sudden capture of al-Faw took the Iraqis by shock, since they had thought it impossible for the Iranians to cross the Shatt al-Arab. On 12 February 1986, the Iraqis began a counter-offensive to retake al-Faw, which failed after a week of heavy fighting. On 24 February 1986, Saddam sent one of his best commanders, General Maher Abd al-Rashid, and the Republican Guard to begin a new offensive to recapture al-Faw. A new round of heavy fighting took place. However, their attempts again ended in failure, costing them many tanks and aircraft: their 15th mechanised division was almost completely wiped out. The capture of al-Faw and the failure of the Iraqi counter-offensives were blows to the Ba'ath regime's prestige, and led the Gulf countries to fear that Iran might win the war. Kuwait in particular felt menaced with Iranian troops only away, and increased its support of Iraq accordingly.\n\nIn March 1986, the Iranians tried to follow up their success by attempting to take Umm Qasr, which would have completely severed Iraq from the Gulf and placed Iranian troops on the border with Kuwait. However, the offensive failed due to Iranian shortages of armor. By this time, 17,000 Iraqis and 30,000 Iranians were made casualties. The First Battle of al-Faw ended in March, but heavy combat operations lasted on the peninsula into 1988, with neither side being able to displace the other. The battle bogged down into a World War I-style stalemate in the marshes of the peninsula.\n\nImmediately after the Iranian capture of al-Faw, Saddam declared a new offensive against Iran, designed to drive deep into the state. The Iranian border city of Mehran, on the foot of the Zagros Mountains, was selected as the first target. On 15–19 May, Iraqi Army's Second Corps, supported by helicopter gunships, attacked and captured the city. Saddam then offered the Iranians to exchange Mehran for al-Faw. The Iranians rejected the offer. Iraq then continued the attack, attempting to push deeper into Iran. However, Iraq's attack was quickly warded off by Iranian AH-1 Cobra helicopters with TOW missiles, which destroyed numerous Iraqi tanks and vehicles.\n\nThe Iranians built up their forces on the heights surrounding Mehran. On 30 June, using mountain warfare tactics they launched their attack, recapturing the city by 3 July. Saddam ordered the Republican Guard to retake the city on 4 July, but their attack was ineffective. Iraqi losses were heavy enough to allow the Iranians to also capture territory inside Iraq, and depleted the Iraqi military enough to prevent them from launching a major offensive for the next two years. Iraq's defeats at al-Faw and at Mehran were severe blows to the prestige of the Iraqi regime, and western powers, including the US, became more determined to prevent an Iraqi loss.\n\nThrough the eyes of international observers, Iran was prevailing in the war by the end of 1986. In the northern front, the Iranians began launching attacks toward the city of Suleimaniya with the help of Kurdish fighters, taking the Iraqis by surprise. They came within of the city before being stopped by chemical and army attacks. Iran's army had also reached the Meimak Hills, only from Baghdad. Iraq managed to contain Iran's offensives in the south, but was under serious pressure, as the Iranians were slowly overwhelming them.\n\nIraq responded by launching another \"war of the cities\". In one attack, Tehran's main oil refinery was hit, and in another instance, Iraq damaged Iran's Assadabad satellite dish, disrupting Iranian overseas telephone and telex service for almost two weeks. Civilian areas were also hit, resulting in many casualties. Iraq continued to attack oil tankers via air. Iran responded by launching Scud missiles and air attacks at Iraqi targets.\n\nIraq continued to attack Kharg Island and the oil tankers and facilities as well. Iran created a tanker shuttle service of 20 tankers to move oil from Kharg to Larak Island, escorted by Iranian fighter jets. Once moved to Larak, the oil would be moved to oceangoing tankers (usually neutral). They also rebuilt the oil terminals damaged by Iraqi air raids and moved shipping to Larak Island, while attacking foreign tankers that carried Iraqi oil (as Iran had blocked Iraq's access to the open sea with the capture of al-Faw). By now they almost always used the armed speedboats of the IRGC navy, and attacked many tankers. The tanker war escalated drastically, with attacks nearly doubling in 1986 (the majority carried out by Iraq). Iraq got permission from the Saudi government to use its airspace to attack Larak Island, although due to the distance attacks were less frequent there. The escalating tanker war in the Gulf became an ever-increasing concern to foreign powers, especially the United States.\n\nIn April 1986, Ayatollah Khomeini issued a fatwa declaring that the war must be won by March 1987. The Iranians increased recruitment efforts, obtaining 650,000 volunteers. The animosity between the Army and the Revolutionary Guard arose again, with the Army wanting to use more refined, limited military attacks while the Revolutionary Guard wanted to carry out major offensives. Iran, confident in its successes, began planning their largest offensives of the war, which they called their \"final offensives.\"\n\nFaced with their recent defeats in al-Faw and Mehran, Iraq appeared to be losing the war. Iraq's generals, angered by Saddam's interference, threatened a full-scale mutiny against the Ba'ath Party unless they were allowed to conduct operations freely. In one of the few times during his career, Saddam gave in to the demands of his generals. Up to this point, Iraqi strategy was to ride out Iranian attacks. However, the defeat at al-Faw led Saddam to declare the war to be \"Al-Defa al-Mutaharakha\" (The Dynamic Defense), and announcing that all civilians had to take part in the war effort. The universities were closed and all of the male students were drafted into the military. Civilians were instructed to clear marshlands to prevent Iranian amphibious infiltrations and to help build fixed defenses.\n\nThe government tried to integrate the Shias into the war effort by recruiting many as part of the Ba'ath Party. In an attempt to counterbalance the religious fervor of the Iranians and gain support from the devout masses, the regime also began to promote religion and, on the surface, Islamization, despite the fact that Iraq was run by a secular regime. Scenes of Saddam praying and making pilgrimages to shrines became common on state-run television. While Iraqi morale had been low throughout the war, the attack on al-Faw raised patriotic fervor, as the Iraqis feared invasion. Saddam also recruited volunteers from other Arab countries into the Republican Guard, and received much technical support from foreign nations as well. While Iraqi military power had been depleted in recent battles, through heavy foreign purchases and support, they were able to expand their military even to much larger proportions by 1988.\n\nAt the same time, Saddam ordered the genocidal al-Anfal Campaign in an attempt to crush the Kurdish resistance, who were now allied with Iran. The result was the deaths of several hundred thousand Iraqi Kurds, and the destruction of villages, towns, and cities.\n\nIraq began to try to perfect its maneuver tactics. The Iraqis began to prioritize the professionalization of their military. Prior to 1986, the conscription-based Iraqi regular army and the volunteer-based Iraqi Popular Army conducted the bulk of the operations in the war, to little effect. The Republican Guard, formerly an elite praetorian guard, was expanded as a volunteer army and filled with Iraq's best generals. Loyalty to the state was no longer a primary requisite for joining. However, due to Saddam's paranoia, the former duties of the Republican Guard were transferred to a new unit, the Special Republican Guard. Full-scale war games against hypothetical Iranian positions were carried out in the western Iraqi desert against mock targets, and they were repeated over the course of a full year until the forces involved fully memorized their attacks. Iraq built its military massively, eventually possessing the 4th largest in the world, in order to overwhelm the Iranians through sheer size.\n\nMeanwhile, as the Iraqis were planning their strike, the Iranians continued to attack. 1987 saw a renewed series of major Iranian human wave offensives in both northern and southern Iraq. The Iraqis had elaborately fortified Basra with 5 defensive rings, exploiting natural waterways such as the Shatt-al-Arab and artificial ones, such as \"Fish Lake\" and the Jasim River, along with earth barriers. Fish Lake was a massive lake filled with mines, underwater barbed wire, electrodes and sensors. Behind each waterway and defensive line was radar-guided artillery, ground attack aircraft and helicopters; all capable of firing poison gas or conventional munitions.\n\nThe Iranian strategy was to penetrate the Iraqi defences and encircle Basra, cutting off the city as well as the Al-Faw peninsula from the rest of Iraq. Iran's plan was for three assaults: a diversionary attack near Basra, the main offensive and another diversionary attack using Iranian tanks in the north to divert Iraqi heavy armor from Basra. For these battles, Iran had re-expanded their military by recruiting many new Basij and Pasdaran volunteers. Iran brought 150,000–200,000 total troops into the battles.\n\nOn 25 December 1986, Iran launched Operation Karbala-4 (\"Karbala\" referring to Hussein ibn Ali's Battle of Karbala). According to Iraqi General Ra'ad al-Hamdani, this was a diversionary attack. The Iranians launched an amphibious assault against the Iraqi island of Umm al-Rassas in the Shatt-Al-Arab river, parallel to Khoramshahr; they then set up a pontoon bridge and continued the attack, eventually capturing it in a costly success and failing to advance further; the Iranians had 60,000 casualties, while the Iraqis 9,500. The Iraqi commanders exaggerated Iranian losses to Saddam and it was assumed that the main Iranian attack on Basra had been fully defeated and that it would take the Iranians six months to recover. When the main Iranian attack, Operation Karbala 5 began, many Iraqi troops were on leave.\n\nThe Siege of Basra, code-named Operation Karbala-5 (), was an offensive operation carried out by Iran in an effort to capture the Iraqi port city of Basra in early 1987. This battle, known for its extensive casualties and ferocious conditions, was the biggest battle of the war and proved to be the beginning of the end of the Iran–Iraq War.\n\nAt the same time as Operation Karbala 5, Iran also launched Operation Karbala-6 against the Iraqis in Qasr-e Shirin in central Iran to prevent the Iraqis from rapidly transferring units down to defend against the Karbala-5 attack. The attack was carried out by Basij infantry and the Revolutionary Guard's 31st \"Ashura\" and the Army's 77th \"Khorasan\" armored divisions. The Basij attacked the Iraqi lines, forcing the Iraqi infantry to retreat. An Iraqi armored counter-attack surrounded the Basij in a pincer movement, but the Iranian tank divisions attacked, breaking the encirclement. The Iranian attack was finally stopped by mass Iraqi chemical weapons attacks.\n\nOperation Karbala-5 was a severe blow to Iran's military and morale. To foreign observers, it appeared that Iran was continuing to strengthen. By 1988, Iran had become self-sufficient in many areas, such as anti-tank TOW missiles, Scud ballistic missiles (Shahab-1), Silkworm anti-ship missiles, Oghab tactical rockets, and producing spare parts for their weaponry. Iran had also improved its air defenses with smuggled surface to air missiles. Iran even was producing UAV's and the Pilatus PC-7 propeller aircraft for observation. Iran also doubled their stocks of artillery, and was self-sufficient in manufacture of ammunition and small arms.\n\nWhile it was not obvious to foreign observers, the Iranian public had become increasingly war-weary and disillusioned with the fighting, and relatively few volunteers joined the fight in 1987–88. Because the Iranian war effort relied on popular mobilization, their military strength actually declined, and Iran was unable to launch any major offensives after Karbala-5. As a result, for the first time since 1982, the momentum of the fighting shifted towards the regular army. Since the regular army was conscription based, it made the war even less popular. Many Iranians began to try to escape the conflict. As early as May 1985, anti-war demonstrations took place in 74 cities throughout Iran, which were crushed by the regime, resulting in some protesters being shot and killed. By 1987, draft-dodging had become a serious problem, and the Revolutionary Guards and police set up roadblocks throughout cities to capture those who tried to evade conscription. Other people (including the more nationalistic and religious) as well as the clergy, and the Revolutionary Guards, wished to continue the war.\n\nThe leadership acknowledged that the war was a stalemate, and began to plan accordingly. There were no more \"final offensives\" planned. The head of the Supreme Defense Council Hashemi Rafsanjani announced during a news conference to finally end the use of human wave attacks. Mohsen Rezaee, head of the IRGC, announced that Iran would focus exclusively on limited attacks and infiltrations, while arming and supporting opposition groups inside of Iraq.\n\nOn the Iranian home front, the combination of sanctions, declining oil prices, and Iraqi attacks on Iranian oil facilities and shipping took a heavy toll on the economy. While the attacks themselves were not as destructive as some analysts believed, the U.S.-led Operation Earnest Will (which protected Iraqi and allied oil tankers, but not Iranian ones) led many neutral countries to stop trading with Iran because of rising insurance and fear of air attack. Iranian oil and non-oil exports fell by 55%, inflation reached 50% by 1987, and unemployment skyrocketed. At the same time, Iraq was experiencing crushing debt and shortages of workers, encouraging its leadership to try to end the war quickly.\n\nBy the end of 1987, Iraq possessed 5,550 tanks (outnumbering the Iranians six to one) and 900 fighter aircraft (outnumbering the Iranians ten to one). After Operation Karbala-5, Iraq only had 100 qualified fighter pilots remaining; therefore, Iraq began to invest in recruiting foreign pilots from countries such as Belgium, South Africa, Pakistan, East Germany and the Soviet Union. They replenished their manpower by integrating volunteers from other Arab countries into their army. Iraq also became self-sufficient in chemical weapons and some conventional ones and received much equipment from abroad. Foreign support helped Iraq bypass its economic troubles and massive debt to continue the war and increase the size of its military.\n\nWhile the southern and central fronts were at a stalemate, Iran began to focus on carrying out offensives in northern Iraq with the help of the Peshmerga (Kurdish insurgents). The Iranians used a combination of semi-guerrilla and infiltration tactics in the Kurdish mountains with the Peshmerga. During Operation Karbala-9 in early April, Iran captured territory near Suleimaniya, provoking a severe poison gas counter-attack. During Operation Karbala-10, Iran attacked near the same area, capturing more territory. During Operation Nasr-4, the Iranians surrounded the city of Suleimaniya and with the help of the Peshmerga infiltrated over 140 km into Iraq and raided and threatened to capture the oil-rich city of Kirkuk and other northern oilfields. Nasr-4 was considered to be Iran's most successful individual operation of the war but Iranian forces were unable to consolidate their gains and continue their advance; while these offensives coupled with the Kurdish uprising sapped Iraqi strength, losses in the north would not mean a catastrophic failure for Iraq.\n\nOn 20 July, the UN Security Council passed the U.S.-sponsored Resolution 598, which called for an end to the fighting and a return to pre-war boundaries. This resolution was noted by Iran for being the first resolution to call for a return to the pre-war borders, and setting up a commission to determine the aggressor and compensation.\n\nWith the stalemate on land, the air/tanker war began to play an increasingly major role in the conflict. The Iranian air force had become very small, having only 20 F-4 Phantoms, 20 F-5 Tigers, and 15 F-14 Tomcats in operation. Despite that, Iran managed to restore some damaged planes into service. The Iranian Air Force, despite its once sophisticated equipment, lacked enough equipment and personnel to sustain the war of attrition that had arisen, and was unable to lead an outright onslaught against Iraq. The Iraqi Air Force, however, had originally lacked modern equipment and experienced pilots, but after pleas from Iraqi military leaders, Saddam decreased political influence on everyday operations and left the fighting to his combatants. The Soviets began delivering more advanced aircraft and weapons to Iraq, while the French improved training for flying crews and technical personnel and continually introduced new methods for countering Iranian weapons and tactics. Iranian ground air defense still shot down many Iraqi aircraft.\n\nThe main Iraqi air effort had shifted to the destruction of Iranian war-fighting capability (primarily Persian Gulf oil fields, tankers, and Kharg Island) and starting late 1986, the Iraqi Air Force moved on a comprehensive campaign against the Iranian economic infrastructure. By late 1987, the Iraqi Air Force could count on direct American support for conducting long-range operations against Iranian infrastructural targets and oil installations deep in the Persian Gulf. U.S. Navy ships tracked and reported movements of Iranian shipping and defences. In the massive Iraqi air strike against Kharg Island, flown on 18 March 1988 the Iraqis destroyed two supertankers but lost five aircraft to Iranian F-14 Tomcats, including two Tupolev Tu-22Bs and one Mikoyan MiG-25RB. The U.S. Navy was now becoming more involved in the fight in the Persian Gulf, launching Operations Earnest Will and Prime Chance against the Iranians.\nThe attacks on oil tankers continued. Both Iran and Iraq carried frequent attacks during the first four months of the year. Iran was effectively waging a naval guerilla war with its IRGC navy speedboats, while Iraq attacked with its aircraft. In 1987, Kuwait asked to reflag its tankers to the U.S. flag. They did so in March, and the U.S. Navy began Operation Earnest Will to escort the tankers. The result of Earnest Will would be that while oil tankers shipping Iraqi/Kuwaiti oil were protected, Iranian tankers, and neutral tankers shipping to Iran would be unprotected, resulting in both losses for Iran and the undermining of its trade with foreign countries, damaging Iran's economy further. Iran also deployed Silkworm missiles to attack some ships, but only a few were actually fired. Both the United States and Iran jockeyed for influence in the Gulf. To discourage the United States from escorting tankers, Iran secretly mined some areas in the Gulf. The United States began to escort the reflagged tankers, but one of them was damaged by a mine while under escort. While being a public-relations victory for Iran, the United States increased its reflagging efforts. While Iran mined the Persian Gulf, their speedboat attacks were reduced, primarily attacking unflagged tankers shipping in the area.\n\nOn 24 September, US Navy SEALS captured the Iranian mine-laying ship \"Iran Ajr\", a diplomatic disaster for the already isolated Iranians. On 8 October, the U.S. Navy destroyed four Iranian speedboats, and in response to Iranian Silkworm missile attacks on Kuwaiti oil tankers, launched Operation Nimble Archer, destroying two Iranian oil rigs in the Persian Gulf. During November and December, the Iraqi air force launched a bid to destroy all Iranian airbases in Khuzestan and the remaining Iranian air force. Iran managed to shoot down 30 Iraqi fighters with fighter jets, anti-aircraft guns, and missiles, allowing the Iranian air force to survive to the end of the war.\n\nOn 28 June, Iraqi fighter bombers attacked the Iranian town of Sardasht near the border, using chemical mustard gas bombs. While many towns and cities had been bombed before, and troops attacked with gas, this was the first time that the Iraqis had attacked a civilian area with poison gas. One quarter of the town's then population of 20,000 was burned and stricken, and 113 were killed immediately, with many more dying and suffering health effects over the next decades. Saddam ordered the attack in order to test the effects of the newly developed \"dusty mustard\" gas, which was designed to be even more crippling than traditional mustard gas. While little known outside of Iran (unlike the later Halabja chemical attack), the Sardasht bombing (and future similar attacks) had a tremendous effect on the Iranian people's psyche.\n\nBy 1988, with massive equipment imports and reduced Iranian volunteers, Iraq was ready to launch major offensives against Iran. In February 1988, Saddam began the fifth and most deadly \"war of the cities\". Over the next two months, Iraq launched over 200 al-Hussein missiles at 37 Iranian cities. Saddam also threatened to use chemical weapons in his missiles, which caused 30% of Tehran's population to leave the city. Iran retaliated, launching at least 104 missiles against Iraq in 1988 and shelling Basra. This event was nicknamed the \"Scud Duel\" in the foreign media. In all, Iraq launched 520 Scuds and al-Husseins against Iran and Iran fired 177 at them. The Iranian attacks were too few in number to deter Iraq from launching their attacks. Iraq also increased their airstrikes against Kharg Island and Iranian oil tankers. With their allies tankers protected by U.S. warships, they could operate with virtual impunity. To make matters worse, the West supplied Iraq's air force with laser-guided smart bombs, allowing them to attack economic targets while evading anti-aircraft defenses. These attacks began to have a major toll on the Iranian economy, morale, and caused many casualties as well.\n\nIn March 1988, the Iranians carried out Operation Dawn 10, Operation \"Beit ol-Moqaddas\" 2, and Operation \"Zafar\" 7 in Iraqi Kurdistan with the aim of capturing the Darbandikhan Dam and the power plant at Lake Dukan, which supplied Iraq with much of its electricity and water, as well as the city of Suleimaniya. Iran hoped that the capture of these areas would bring more favorable terms to the ceasefire agreement. This infiltration offensive was carried out in conjunction with the Peshmerga. Iranian airborne commandos landed behind the Iraqi lines and Iranian helicopters hit Iraqi tanks with TOW missiles. The Iraqis were taken by surprise, and Iranian F-5E Tiger fighter jets even damaged the Kirkuk oil refinery. Iraq carried out executions of multiple officers for these failures in March–April 1988, including Colonel Jafar Sadeq. The Iranians used infiltration tactics in the Kurdish mountains, captured the town of Halabja and began to fan out across the province.\n\nThough the Iranians advanced to within sight of Dukan and captured around and 4,000 Iraqi troops, the offensive failed due to the Iraqi use of chemical warfare. The Iraqis launched the deadliest chemical weapons attacks of the war. The Republican Guard launched 700 chemical shells, while the other artillery divisions launched 200–300 chemical shells each, unleashing a chemical cloud over the Iranians, killing or wounding 60% of them, the blow was felt particularly by the Iranian 84th infantry division and 55th paratrooper division. The Iraqi special forces then stopped the remains of the Iranian force. In retaliation for Kurdish collaboration with the Iranians, Iraq launched a massive poison gas attack against Kurdish civilians in Halabja, recently taken by the Iranians, killing thousands of civilians. Iran airlifted foreign journalists to the ruined city, and the images of the dead were shown throughout the world but Western mistrust of Iran and collaboration with Iraq led them to also blame Iran for the attack. At one point, the United States claimed that Iran had launched the attack and then tried to blame Iraq for it.\n\nOn 17 April 1988, Iraq launched Operation Ramadan \"Mubarak\" (Blessed Ramadan), a surprise attack against the 15,000 Basij troops on the peninsula. The attack on al-Faw was preceded by Iraqi diversionary attacks in northern Iraq, with a massive artillery and air barrage of Iranian front lines. Key areas, such as supply lines, command posts, and ammunition depots, were hit by a storm of mustard gas and nerve gas, as well as by conventional explosives. Helicopters landed Iraqi commandos behind Iranian lines while the main Iraqi force attacked in a frontal assault. Within 48 hours, all of the Iranian forces had been killed or cleared from the al-Faw Peninsula. The day was celebrated in Iraq as Faw Liberation Day throughout Saddam's rule. The Iraqis had planned the offensive well. Prior to the attack the Iraqi soldiers gave themselves poison gas antidotes to shield themselves from the effect of the saturation of gas. The heavy and well executed use of chemical weapons was the decisive factor in the Iraqi victory. Iraqi losses were relatively light, especially compared to Iran's casualties. The Iranians eventually managed to halt the Iraqi drive as they pushed towards Khuzestan.\n\nTo the shock of the Iranians, rather than breaking off the offensive, the Iraqis kept up their drive, and a new force attacked the Iranian positions around Basra. Following this, the Iraqis launched a sustained drive to clear the Iranians out of all of southern Iraq. One of the most successful Iraqi tactics was the \"one-two punch\" attack using chemical weapons. Using artillery, they would saturate the Iranian front line with rapidly dispersing cyanide and nerve gas, while longer-lasting mustard gas was launched via fighter-bombers and rockets against the Iranian rear, creating a \"chemical wall\" that blocked reinforcement.\n\nThe same day as Iraq's attack on al-Faw peninsula, the United States Navy launched Operation Praying Mantis in retaliation against Iran for damaging a warship with a mine. Iran lost oil platforms, destroyers, and frigates in this battle, which ended only when President Reagan decided that the Iranian navy had been put down enough. In spite of this, the Revolutionary Guard Navy continued their speedboat attacks against oil tankers. The defeats at al-Faw and in the Persian Gulf nudged Iranian leadership towards quitting the war, especially when facing the prospect of fighting the Americans.\n\nFaced with such losses, Khomeini appointed the cleric Hashemi Rafsanjani as the Supreme Commander of the Armed Forces, though he had in actuality occupied that position for months. Rafsanjani ordered a last desperate counter-attack into Iraq, which was launched 13 June 1988. The Iranians infiltrated through the Iraqi trenches and moved into Iraq and managed to strike Saddam's presidential palace in Baghdad using fighter aircraft. After three days of fighting, the decimated Iranians were driven back to their original positions again as the Iraqis launched 650 helicopter and 300 aircraft sorties.\n\nOn 18 June, Iraq launched Operation Forty Stars ( \"chehel cheragh\") in conjunction to the Mujahideen-e-Khalq (MEK) around Mehran. With 530 aircraft sorties and heavy use of nerve gas, they crushed the Iranian forces in the area, killing 3,500 and nearly destroying a Revolutionary Guard division. Mehran was captured once again and occupied by the MEK. Iraq also launched air raids on Iranian population centers and economic targets, setting 10 oil installations on fire.\n\nOn 25 May 1988, Iraq launched the first of five \"Tawakalna ala Allah\" (Trust in God) Operations, consisting of one of the largest artillery barrages in history, coupled with chemical weapons. The marshes had been dried by drought, allowing the Iraqis to use tanks to bypass Iranian field fortifications, expelling the Iranians from the border town of Shalamcheh after less than 10 hours of combat.\n\nOn 25 June, Iraq launched the second Tawakal ala Allah operation against the Iranians on Majnoon Island. Iraqi commandos used amphibious craft to block the Iranian rear, then used hundreds of tanks with massed conventional and chemical artillery barrages to recapture the island after 8 hours of combat. Saddam appeared live on Iraqi television to \"lead\" the charge against the Iranians. The majority of the Iranian defenders were killed during the quick assault. The final two Tawakal ala Allah operations took place near al-Amarah and Khaneqan. By 12 July, the Iraqis had captured the city of Dehloran, inside Iran, along with 2,500 troops and much armour and material, which took four days to transport to Iraq. These losses included more than 570 of the 1,000 remaining Iranian tanks, over 430 armored vehicles, 45 self-propelled artillery, 300 towed artillery pieces, and 320 antiaircraft guns. These figures only included what Iraq could actually put to use; total amount of captured materiel was higher. Since March, the Iraqis claimed to captured 1,298 tanks, 155 infantry fighting vehicles, 512 heavy artillery pieces, 6,196 mortars, 5,550 recoilless rifles and light guns, 8,050 man-portable rocket launchers, 60,694 rifles, 322 pistols, 454 trucks, and 1,600 light vehicles. The Iraqis withdrew from Dehloran soon after, claiming that they had \"no desire to conquer Iranian territory.\" History professor Kaveh Farrokh considered this to be Iran's greatest military disaster during the war. Stephen Pelletier, a Journalist, Middle East expert, and Author, noted that \"Tawakal ala Allah … resulted in the absolute destruction of Iran's military machine.\"\nDuring the 1988 battles, the Iranians put up little resistance to the Iraqi offensives, having been worn out by nearly eight years of war. They lost large amounts of equipment but they managed to rescue most of their troops from being captured by the Iraqis, leaving Iraq with relatively few prisoners. On 2 July, Iran belatedly set up a joint central command which unified the Revolutionary Guard, Army, and Kurdish rebels, and dispelled the rivalry between the Army and the Revolutionary Guard. However, this came too late, and due to the capture of 570 of their operable tanks and the destruction of hundreds more, Iran was believed to have fewer than 200 remaining operable tanks on the southern front, faced against thousands of Iraqi ones. The only area where the Iranians were not suffering major defeats was in Kurdistan.\n\nSaddam sent a warning to Khomeini in mid-1988, threatening to launch a new and powerful full-scale invasion and attack Iranian cities with weapons of mass destruction. Shortly afterwards, Iraqi aircraft bombed the Iranian town of Oshnavieh with poison gas, immediately killing and wounding over 2,000 civilians. The fear of an all out chemical attack against Iran's largely unprotected civilian population weighed heavily on the Iranian leadership, and they realized that the international community had no intention of restraining Iraq. The lives of the civilian population of Iran were becoming very disrupted, with a third of the urban population evacuating major cities in fear of the seemingly imminent chemical war. Meanwhile, Iraqi conventional bombs and missiles continuously hit towns and cities, as well as destroyed vital civilian and military infrastructure, and the death toll increased. Iran did reply with missile and air attacks as well, but not enough to deter the Iraqis from attacking.\n\nUnder the threat of a new and even more powerful invasion, Commander-in-Chief Rafsanjani ordered the Iranians to retreat from Haj Omran, Kurdistan on 14 July. The Iranians did not publicly describe this as a retreat, instead calling it a \"temporary withdrawal\". By July, Iran's army inside Iraq (except Kurdistan) had largely disintegrated. Iraq put up a massive display of captured Iranian weapons in Baghdad, claiming they captured 1,298 tanks, 5,550 recoil-less rifles, and thousands of other weapons. However, Iraq had taken heavy losses as well, and the battles were very costly.\n\nIn July 1988, Iraqi aircraft dropped bombs on the Iranian Kurdish village of Zardan. Dozens of villages, such as Sardasht, and some larger towns, such as Marivan, Baneh and Saqqez, were once again attacked with poison gas, resulting in even heavier civilian casualties. About the same time, the USS \"Vincennes\" shot down Iran Air Flight 655, killing 290 passengers and crew. The lack of international sympathy disturbed the Iranian leadership, and they came to the conclusion that the United States was on the verge of waging a full-scale war against them, and that Iraq was on the verge of unleashing its entire chemical arsenal upon their cities.\n\nAt this point, elements of the Iranian leadership, led by Rafsanjani (who had initially pushed for the extension of the war), persuaded Khomeini to accept the ceasefire. They stated that in order to win the war, Iran's military budget would have to be increased by 700% and the war would last until 1993. On 20 July 1988, Iran accepted Resolution 598, showing its willingness to accept a ceasefire. A statement from Khomeini was read out in a radio address, and he expressed deep displeasure and reluctance about accepting the ceasefire,\n\nHappy are those who have departed through martyrdom. Happy are those who have lost their lives in this convoy of light. Unhappy am I that I still survive and have drunk the poisoned chalice...\n\nThe news of the end of the war was greeted with celebration in Baghdad, with people dancing in the streets; in Tehran, however, the end of the war was greeted with a somber mood.\n\nOperation \"Mersad\" ( \"ambush\") was the last big military operation of the war. Both Iran and Iraq had accepted Resolution 598 but despite the ceasefire, after seeing Iraqi victories in the previous months, Mujahadeen-e-Khalq (MEK) decided to launch an attack of its own and wished to advance all the way to Teheran. Saddam and the Iraqi high command decided on a two pronged offensive across the border into central Iran and Iranian Kurdistan. Shortly after Iran accepted the ceasefire the MEK army began its offensive, attacking into Ilam province under cover of Iraqi air power. In the north, Iraq also launched an attack into Iraqi Kurdistan, which was blunted by the Iranians.\n\nOn 26 July 1988, the MEK started their campaign in central Iran, Operation \"Forough Javidan\" (Eternal Light), with the support of the Iraqi army. The Iranians had withdrawn their remaining soldiers to Khuzestan in fear of a new Iraqi invasion attempt, allowing the Mujahedeen to advance rapidly towards Kermanshah, seizing Qasr-e Shirin, Sarpol-e Zahab, Kerend-e Gharb, and Islamabad-e-Gharb. The MEK expected the Iranian population to rise up and support their advance; the uprising never materialised but they reached deep into Iran. In response, the Iranian military launched its counter-attack, Operation Mersad, under Lieutenant General Ali Sayyad Shirazi. Iranian paratroopers landed behind the MEK lines while the Iranian Air Force and helicopters launched an air attack, destroying much of the enemy columns. The Iranians defeated the MEK in the city of Kerend-e Gharb on 29 July 1988. On 31 July, Iran drove the MEK out of Qasr-e-Shirin and Sarpol Zahab, though MEK claimed to have \"voluntarily withdrawn\" from the towns. Iran estimated that 4,500 MEK were killed, while 400 Iranian soldiers died.\nThe last notable combat actions of the war took place on 3 August 1988, in the Persian Gulf when the Iranian navy fired on a freighter and Iraq launched chemical attacks on Iranian civilians, killing an unknown number of them and wounding 2,300. Iraq came under international pressure to curtail further offensives. Resolution 598 became effective on 8 August 1988, ending all combat operations between the two countries. By 20 August 1988, peace with Iran was restored. UN peacekeepers belonging to the UNIIMOG mission took the field, remaining on the Iran–Iraq border until 1991. The majority of Western analysts believe that the war had no winners while some believed that Iraq emerged as the victor of the war, based on Iraq's overwhelming successes between April and July 1988. Others can argue that Iran was the victor after successfully defending their country from invasion and repelling the aggressor despite being isolated and under international sanctions and while their enemy was exceptionally well funded and supported. While the war was now over, Iraq spent the rest of August and early September clearing the Kurdish resistance. Using 60,000 troops along with helicopter gunships, chemical weapons (poison gas), and mass executions, Iraq hit 15 villages, killing rebels and civilians, and forced tens of thousands of Kurds to relocate to settlements. Many Kurdish civilians fled to Iran. By 3 September 1988, the anti-Kurd campaign ended, and all resistance had been crushed. 400 Iraqi soldiers and 50,000–100,000 Kurdish civilians and soldiers had been killed.\n\nAt the war's conclusion, it took several weeks for the Armed Forces of the Islamic Republic of Iran to evacuate Iraqi territory to honor pre-war international borders set by the 1975 Algiers Agreement. The last prisoners of war were exchanged in 2003.\n\nThe Security Council did not identify Iraq as the aggressor of the war until 11 December 1991, some 12 years after Iraq invaded Iran and 16 months following Iraq's invasion of Kuwait.\n\nThe Iran–Iraq War was the deadliest conventional war ever fought between regular armies of developing countries. Iraqi casualties are estimated at 105,000–200,000 killed, while about 400,000 had been wounded and some 70,000 taken prisoner. Thousands of civilians on both sides died in air raids and ballistic missile attacks. Prisoners taken by both countries began to be released in 1990, though some were not released until more than 10 years after the end of the conflict. Cities on both sides had also been considerably damaged. While revolutionary Iran had been bloodied, Iraq was left with a large military and was a regional power, albeit with severe debt, financial problems, and labor shortages.\n\nAccording to Iranian government sources, the war cost Iran an estimated 200,000–220,000 killed, or up to 262,000 according to the conservative Western estimates. This includes 123,220 combatants, 60,711 MIA and 11,000–16,000 civilians. Combatants include 79,664 members of the Revolutionary Guard Corps and additional 35,170 soldiers from regular military. In addition, prisoners of war comprise 42,875 Iranian casualties, they were captured and kept in Iraqi detention centers from 2.5 to more than 15 years after the war was over. According to the Janbazan Affairs Organization, 398,587 Iranians sustained injuries that required prolonged medical and health care following primary treatment, including 52,195 (13%) injured due to the exposure to chemical warfare agents. From 1980 to 2012, 218,867 Iranians died due to war injuries and the mean age of combatants was 23 years old. This includes 33,430 civilians, mostly women and children. More than 144,000 Iranian children were orphaned as a consequence of these deaths. Other estimates put Iranian casualties up to 600,000.\n\nBoth Iraq and Iran manipulated loss figures to suit their purposes. At the same time, Western analysts accepted improbable estimates. By April 1988, such casualties were estimated at between 150,000 and 340,000 Iraqis dead, and 450,000 to 730,000 Iranians. Shortly after the end of the war, it was thought that Iran suffered even more than a million dead. Considering the style of fighting on the ground and the fact that neither side penetrated deeply into the other's territory, USMC analysts believe events do not substantiate the high casualties claimed. The Iraqi government has claimed 800,000 Iranians were killed in conflict, four times more than Iranian official figures. Iraqi losses were also revised downwards over time.\n\nWith the ceasefire in place, and UN peacekeepers monitoring the border, Iran and Iraq sent their representatives to Geneva, Switzerland, to negotiate a peace agreement on the terms of the ceasefire. However, peace talks stalled. Iraq, in violation of the UN ceasefire, refused to withdraw its troops from of disputed territory at the border area unless the Iranians accepted Iraq's full sovereignty over the Shatt al-Arab waterway. Foreign powers continued to support Iraq, which wanted to gain at the negotiating table what they failed to achieve on the battlefield, and Iran was portrayed as the one not wanting peace. Iran, in response, refused to release 70,000 Iraqi prisoners of war (compared to 40,000 Iranian prisoners of war held by Iraq). They also continued to carry out a naval blockade of Iraq, although its effects were mitigated by Iraqi use of ports in friendly neighbouring Arab countries. Iran also began to improve relations with many of the states that opposed it during the war. Because of Iranian actions, by 1990, Saddam had become more conciliatory, and in a letter to the now President Rafsanjani, he became more open to the idea of a peace agreement, although he still insisted on full sovereignty over the Shatt al-Arab.\n\nBy 1990, Iran was undergoing military rearmament and reorganization, and purchased $10 billion worth of heavy weaponry from the USSR and China, including aircraft, tanks, and missiles. Rafsanjani reversed Iran's self-imposed ban on chemical weapons, and ordered the manufacture and stockpile of them (Iran destroyed them in 1993 after ratifying the Chemical Weapons Convention). As war with the western powers loomed, Iraq became concerned about the possibility of Iran mending its relations with the west in order to attack Iraq. Iraq had lost its support from the West, and its position in Iran was increasingly untenable. Saddam realized that if Iran attempted to expel the Iraqis from the disputed territories in the border area, it was likely they would succeed. Shortly after his invasion of Kuwait, Saddam wrote a letter to Rafsanjani stating that Iraq recognised Iranian rights over the eastern half of the Shatt al-Arab, a reversion to \"status quo ante bellum\" that he had repudiated a decade earlier, and that he would accept Iran's demands and withdraw Iraq's military from the disputed territories. A peace agreement was signed finalizing the terms of the UN resolution, diplomatic relations were restored, and by late 1990-early 1991, the Iraqi military withdrew. The UN peacekeepers withdrew from the border shortly afterward. Most of the prisoners of war were released in 1990, although some remained as late as 2003. Iranian politicians declared it to be the \"greatest victory in the history of the Islamic Republic of Iran\".\n\nMost historians and analysts consider the war to be a stalemate. Certain analysts believe that Iraq won, on the basis of the successes of their 1988 offensives which thwarted Iran's major territorial ambitions in Iraq and persuaded Iran to accept the ceasefire. Iranian analysts believe that they won the war because although they did not succeed in overthrowing the Iraqi government, they thwarted Iraq's major territorial ambitions in Iran, and that, two years after the war had ended, Iraq permanently gave up its claim of ownership over the entire Shatt al-Arab as well.\n\nOn 9 December 1991, Javier Pérez de Cuéllar, UN Secretary General at the time, reported that Iraq's initiation of the war was unjustified, as was its occupation of Iranian territory and use of chemical weapons against civilians:\n\nThat [Iraq's] explanations do not appear sufficient or acceptable to the international community is a fact...[the attack] cannot be justified under the charter of the United Nations, any recognized rules and principles of international law, or any principles of international morality, and entails the responsibility for conflict. Even if before the outbreak of the conflict there had been some encroachment by Iran on Iraqi territory, such encroachment did not justify Iraq's aggression against Iran—which was followed by Iraq's continuous occupation of Iranian territory during the conflict—in violation of the prohibition of the use of force, which is regarded as one of the rules of jus cogens...On one occasion I had to note with deep regret the experts' conclusion that \"chemical weapons ha[d] been used against Iranian civilians in an area adjacent to an urban center lacking any protection against that kind of attack.\"\n\nHe also stated that had the UN accepted this fact earlier, the war would have almost certainly not lasted as long as it did. Iran, encouraged by the announcement, sought reparations from Iraq, but never received any.\nThroughout the 1990s and early 2000s, Iran and Iraq relations remained balanced between a cold war and a cold peace. Despite renewed and somewhat thawed relations, both sides continued to have low level conflicts. Iraq continued to host and support the Mujahedeen-e-Khalq, which carried out multiple attacks throughout Iran up until the 2003 invasion of Iraq (including the assassination of Iranian general Ali Sayyad Shirazi in 1998, cross border raids, and mortar attacks). Iran carried out several airstrikes and missile attacks against Mujahedeen targets inside of Iraq (the largest taking place in 2001, when Iran fired 56 Scud missiles at Mujahedeen targets). In addition, according to General Hamdani, Iran continued to carry out low-level infiltrations of Iraqi territory, using Iraqi dissidents and anti-government activists rather than Iranian troops, in order to incite revolts. After the fall of Saddam in 2003, Hamdani claimed that Iranian agents infiltrated and created numerous militias in Iraq and built an intelligence system operating within the country.\n\nIn 2005, the new government of Iraq apologised to Iran for starting the war. The Iraqi government also commemorated the war with various monuments, including the Hands of Victory and the al-Shaheed Monument, both in Baghdad. The war also helped to create a forerunner for the Coalition of the Gulf War, when the Gulf Arab states banded together early in the war to form the Gulf Cooperation Council to help Iraq fight Iran.\n\nThe economic loss at the time was believed to exceed $500 billion for each country ($1.2 trillion total). In addition, economic development stalled and oil exports were disrupted. Iraq had accrued more than $130 billion of international debt, excluding interest, and was also weighed down by a slowed GDP growth. Iraq's debt to Paris Club amounted to $21 billion, 85% of which had originated from the combined inputs of Japan, the USSR, France, Germany, the United States, Italy and the United Kingdom. The largest portion of Iraq's debt, amounting to $130 billion, was to its former Arab backers, with $67 billion loaned by Kuwait, Saudi Arabia, Qatar, UAE, and Jordan. After the war, Iraq accused Kuwait of slant drilling and stealing oil, inciting its invasion of Kuwait, which in turn worsened Iraq's financial situation: the United Nations Compensation Commission mandated Iraq to pay reparations of more than $200 billion to victims of the invasion, including Kuwait and the United States. To enforce payment, Iraq was put under a complete international embargo, which further strained the Iraqi economy and pushed its external debt to private and public sectors to more than $500 billion by the end of Saddam's rule. Combined with Iraq's negative economic growth after prolonged international sanctions, this produced a debt-to-GDP ratio of more than 1,000%, making Iraq the most indebted developing country in the world. The unsustainable economic situation compelled the new Iraqi government to request that a considerable portion of debt incurred during the Iran–Iraq war be written off.\n\nMuch of the oil industry in both countries was damaged in air raids.\n\nThe war had its impact on medical science: a surgical intervention for comatose patients with penetrating brain injuries was created by Iranian physicians treating wounded soldiers, later establishing neurosurgery guidelines to treat civilians who had suffered blunt or penetrating skull injuries. Iranian physicians' experience in the war reportedly helped U.S. congresswoman Gabrielle Giffords recover after the 2011 Tucson shooting.\n\nIn addition to helping trigger the Persian Gulf War, the Iran–Iraq War also contributed to Iraq's defeat in the Persian Gulf War. Iraq's military was accustomed to fighting the slow moving Iranian infantry formations with artillery and static defenses, while using mostly unsophisticated tanks to gun down and shell the infantry and overwhelm the smaller Iranian tank force; in addition to being dependent on weapons of mass destruction to help secure victories. Therefore, they were rapidly overwhelmed by the high-tech, quick-maneuvering U.S. forces using modern doctrines such as AirLand Battle.\n\nAt first, Saddam attempted to ensure that the Iraqi population suffered from the war as little as possible. There was rationing, but civilian projects begun before the war continued. At the same time, the already extensive personality cult around Saddam reached new heights while the regime tightened its control over the military.\n\nAfter the Iranian victories of the spring of 1982 and the Syrian closure of Iraq's main pipeline, Saddam did a volte-face on his policy towards the home front: a policy of austerity and total war was introduced, with the entire population being mobilised for the war effort. All Iraqis were ordered to donate blood and around 100,000 Iraqi civilians were ordered to clear the reeds in the southern marshes. Mass demonstrations of loyalty towards Saddam became more common. Saddam also began implementing a policy of discrimination against Iraqis of Iranian origin.\n\nIn the summer of 1982, Saddam began a campaign of terror. More than 300 Iraqi Army officers were executed for their failures on the battlefield. In 1983, a major crackdown was launched on the leadership of the Shia community. Ninety members of the al-Hakim family, an influential family of Shia clerics whose leading members were the émigrés Mohammad Baqir al-Hakim and Abdul Aziz al-Hakim, were arrested, and 6 were hanged. The crackdown on Kurds saw 8,000 members of the Barzani clan, whose leader (Massoud Barzani) also led the Kurdistan Democratic Party, similarly executed. From 1983 onwards, a campaign of increasingly brutal repression was started against the Iraqi Kurds, characterised by Israeli historian Efraim Karsh as having \"assumed genocidal proportions\" by 1988. The al-Anfal Campaign was intended to \"pacify\" Iraqi Kurdistan permanently.\n\nTo secure the loyalty of the Shia population, Saddam allowed more Shias into the Ba'ath Party and the government, and improved Shia living standards, which had been lower than those of the Iraqi Sunnis. Saddam had the state pay for restoring Imam Ali's tomb with white marble imported from Italy. The Baathists also increased their policies of repression against the Shia. The most infamous event was the massacre of 148 civilians of the Shia town of Dujail.\n\nDespite the costs of the war, the Iraqi regime made generous contributions to Shia \"waqf\" (religious endowments) as part of the price of buying Iraqi Shia support. The importance of winning Shia support was such that welfare services in Shia areas were expanded during a time in which the Iraqi regime was pursuing austerity in all other non-military fields. During the first years of the war in the early 1980s, the Iraqi government tried to accommodate the Kurds in order to focus on the war against Iran. In 1983, the Patriotic Union of Kurdistan agreed to cooperate with Baghdad, but the Kurdistan Democratic Party (KDP) remained opposed. In 1983, Saddam signed an autonomy agreement with Jalal Talabani of the Patriotic Union of Kurdistan (PUK), though Saddam later reneged on the agreement. By 1985, the PUK and KDP had joined forces, and Iraqi Kurdistan saw widespread guerrilla warfare up to the end of the war.\n\nIsraeli-British historian, Ephraim Karsh, argues that the Iranian government saw the outbreak of war as chance to strengthen its position and consolidate the Islamic revolution, noting that government propaganda presented it domestically as a glorious \"jihad\" and a test of Iranian national character. The Iranian regime followed a policy of total war from the beginning, and attempted to mobilise the nation as a whole. They established a group known as the Reconstruction Campaign, whose members were exempted from conscription and were instead sent into the countryside to work on farms to replace the men serving at the front.\n\nIranian workers had a day's pay deducted from their pay cheques every month to help finance the war, and mass campaigns were launched to encourage the public to donate food, money, and blood. To further help finance the war, the Iranian government banned the import of all non-essential items, and launched a major effort to rebuild the damaged oil plants.\n\nAccording to former Iraqi general Ra'ad al-Hamdani, the Iraqis believed that in addition to the Arab revolts, the Revolutionary Guards would be drawn out of Tehran, leading to a counter-revolution in Iran that would cause Khomeini's government to collapse and thus ensure Iraqi victory. However, rather than turning against the revolutionary government as experts had predicted, Iran's people (including Iranian Arabs) rallied in support of the country and put up a stiff resistance.\n\nIn June 1981, street battles broke out between the Revolutionary Guard and the left-wing Mujaheddin e-Khalq (MEK), continuing for several days and killing hundreds on both sides. In September, more unrest broke out on the streets of Iran as the MEK attempted to seize power. Thousands of left-wing Iranians (many of whom were not associated with the MEK) were shot and hanged by the government. The MEK began an assassination campaign that killed hundreds of regime officials by the fall of 1981. On 28 June 1981, they assassinated the secretary-general of the Islamic Republican Party, Mohammad Beheshti and on 30 August, killed Iran's president, Mohammad-Ali Rajai. The government responded with mass executions of suspected MEK members, a practice that lasted until 1985.\n\nIn addition to the open civil conflict with the MEK, the Iranian government was faced with Iraqi-supported rebellions in Iranian Kurdistan, which were gradually put down through a campaign of systematic repression. 1985 also saw student anti-war demonstrations, which were crushed by government forces.\n\nThe war furthered the decline of the Iranian economy that had begun with the revolution in 1978–79. Between 1979 and 1981, foreign exchange reserves fell from $14.6 billion to $1 billion. As a result of the war, living standards dropped dramatically, and Iran was described by British journalists John Bulloch and Harvey Morris as \"a dour and joyless place\" ruled by a harsh regime that \"seemed to have nothing to offer but endless war.\" Though Iran was becoming bankrupt, Khomeini interpreted Islam's prohibition of usury to mean they could not borrow against future oil revenues to meet war expenses. As a result, Iran funded the war by the income from oil exports after cash had run out. The revenue from oil dropped from $20 billion in 1982 to $5 billion in 1988.French historian Pierre Razoux argued that this sudden drop in economic industrial potential, in conjunction with the increasing aggression of Iraq, placed Iran in a challenging position that had little leeway other than accepting Iraq's conditions of peace.\n\nIn January 1985, former prime minister and anti-war Islamic Liberation Movement co-founder Mehdi Bazargan criticised the war in a telegram to the United Nations, calling it un-Islamic and illegitimate and arguing that Khomeini should have accepted Saddam's truce offer in 1982 instead of attempting to overthrow the Ba'ath. In a public letter to Khomeini sent in May 1988, he added \"Since 1986, you have not stopped proclaiming victory, and now you are calling upon population to resist until victory. Is that not an admission of failure on your part?\" Khomeini was annoyed by Bazargan's telegram, and issued a lengthy public rebuttal in which he defended the war as both Islamic and just.\n\nBy 1987, Iranian morale had begun to crumble, reflected in the failure of government campaigns to recruit \"martyrs\" for the front. Israeli historian Efraim Karsh points to the decline in morale in 1987–88 as being a major factor in Iran's decision to accept the ceasefire of 1988.\n\nNot all saw the war in negative terms. The Islamic Revolution of Iran was strengthened and radicalised. The Iranian government-owned \"Etelaat\" newspaper wrote, \"There is not a single school or town that is excluded from the happiness of 'holy defence' of the nation, from drinking the exquisite elixir of martyrdom, or from the sweet death of the martyr, who dies in order to live forever in paradise.\"\n\nIran's regular Army had been purged after the 1979 Revolution, with most high-ranking officers either having deserted (fled the country) or been executed.\n\nAt the beginning of the war, Iraq held a clear advantage in armour, while both nations were roughly equal in terms of artillery. The gap only widened as the war went on. Iran started with a stronger air force, but over time, the balance of power reversed in Iraq's favour (as Iraq was constantly expanding its military, while Iran was under arms sanctions). Estimates for 1980 and 1987 were:\n\nThe conflict has been compared to World War I in terms of the tactics used, including large-scale trench warfare with barbed wire stretched across trenches, manned machine gun posts, bayonet charges, human wave attacks across a no man's land, and extensive use of chemical weapons such as sulfur mustard by the Iraqi government against Iranian troops, civilians, and Kurds. The world powers United States and the Soviet Union, together with many Western and Arab countries, provided military, intelligence, economic, and political support for Iraq.\n\nDuring the war, Iraq was regarded by the West and the Soviet Union as a counterbalance to post-revolutionary Iran. The Soviet Union, Iraq's main arms supplier during the war, did not wish for the end of its alliance with Iraq, and was alarmed by Saddam's threats to find new arms suppliers in the West and China if the Kremlin did not provide him with the weapons he wanted. The Soviet Union hoped to use the threat of reducing arms supplies to Iraq as leverage for forming a Soviet-Iranian alliance.\n\nDuring the early years of the war, the United States lacked meaningful relations with either Iran or Iraq, the former due to the Iranian Revolution and the Iran hostage crisis and the latter because of Iraq's alliance with the Soviet Union and hostility towards Israel. Following Iran's success of repelling the Iraqi invasion and Khomeini's refusal to end the war in 1982, the United States made an outreach to Iraq, beginning with the restoration of diplomatic relations in 1984. The United States wished to both keep Iran away from Soviet influence and protect other Gulf states from any threat of Iranian expansion. As a result, it began to provide limited support to Iraq. In 1982, Henry Kissinger, former Secretary of State, outlined U.S. policy towards Iran:\n\nThe focus of Iranian pressure at this moment is Iraq. There are few governments in the world less deserving of our support and less capable of using it. Had Iraq won the war, the fear in the Gulf and the threat to our interest would be scarcely less than it is today. Still, given the importance of the balance of power in the area, it is in our interests to promote a ceasefire in that conflict; through not a cost that will preclude an eventual rapprochement with Iran either if a more moderate regime replaces Khomenini's or if the present rulers wake up to geopolitical reality that the historic threat to Iran's independence has always come from the country with which it shares a border of : the Soviet Union. A rapprochement with Iran, of course, must await at a minimum Iran's abandonment of hegemonic aspirations in the Gulf.\n\nRichard Murphy, Assistant Secretary of State during the war, testified to Congress in 1984 that the Reagan administration believed a victory for either Iran or Iraq was \"neither militarily feasible nor strategically desirable.\"\n\nSupport to Iraq was given via technological aid, intelligence, the sale of chemical and biological warfare technology and military equipment, and satellite intelligence. While there was direct combat between Iran and the United States, it is not universally agreed that the fighting between the United States and Iran was specifically to benefit Iraq, or for separate issues between the U.S. and Iran. American official ambiguity towards which side to support was summed up by Henry Kissinger when he remarked, \"It's a pity they both can't lose.\" The Americans and the British also either blocked or watered down UN resolutions that condemned Iraq for using chemical weapons against the Iranians and their own Kurdish citizens.\n\nMore than 30 countries provided support to Iraq, Iran, or both; most of the aid went to Iraq. Iran had a complex clandestine procurement network to obtain munitions and critical materials. Iraq had an even larger clandestine purchasing network, involving 10–12 allied countries, to maintain ambiguity over their arms purchases and to circumvent \"official restrictions\". Arab mercenaries and volunteers from Egypt and Jordan formed the Yarmouk Brigade and participated in the war alongside Iraqis.\n\nAccording to the Stockholm International Peace Institute, the Soviet Union, France, and China together accounted for over 90% of the value of Iraq's arms imports between 1980 and 1988.\n\nThe United States pursued policies in favour of Iraq by reopening diplomatic channels, lifting restrictions on the export of dual-use technology, overseeing the transfer of third-party military hardware, and providing operational intelligence on the battlefield. France, which from the 1970s had been one of Iraq's closest allies, was a major supplier of military hardware. The French sold weapons equal to $5 billion, which comprised well over a quarter of Iraq's total arms stockpile. Citing French magazine \"Le Nouvel Observateur\" as the primary source, but also quoting French officials, the \"New York Times\" reported France had been sending chemical precursors of chemical weapons to Iraq, since 1986. China, which had no direct stake in the victory of either side and whose interests in the war were entirely commercial, freely sold arms to both sides.\n\nIraq also made extensive use of front companies, middlemen, secret ownership of all or part of companies all over the world, forged end-user certificates, and other methods to hide what it was acquiring. Some transactions may have involved people, shipping, and manufacturing in as many as 10 countries. Support from Great Britain exemplified the methods by which Iraq would circumvent export controls. Iraq bought at least one British company with operations in the United Kingdom and the United States, and had a complex relationship with France and the Soviet Union, its major suppliers of actual weapons.\n\nThe United Nations Security Council initially called for a cease-fire after a week of fighting while Iraq was occupying Iranian territory, and renewed the call on later occasions. However, the UN did not come to Iran's aid to repel the Iraqi invasion, and the Iranians thus interpreted the UN as subtly biased in favour of Iraq.\n\nIraq's main financial backers were the oil-rich Persian Gulf states, most notably Saudi Arabia ($30.9 billion), Kuwait ($8.2 billion), and the United Arab Emirates ($8 billion). In all, Iraq received $35 billion in loans from the West and between $30 and $40 billion from the Persian Gulf states during the 1980s.\n\nThe Iraqgate scandal revealed that a branch of Italy's largest bank, Banca Nazionale del Lavoro (BNL), in Atlanta, Georgia, relied partially on U.S. taxpayer-guaranteed loans to funnel $5 billion to Iraq from 1985 to 1989. In August 1989, when FBI agents raided the Atlanta branch of BNL, branch manager Christopher Drogoul was charged with making unauthorised, clandestine, and illegal loans to Iraq—some of which, according to his indictment, were used to purchase arms and weapons technology. According to the \"Financial Times\", Hewlett-Packard, Tektronix, and Matrix Churchill's branch in Ohio were also involved, shipping militarily useful technology to Iraq were .\n\nWhile the United States directly fought Iran, citing freedom of navigation as a major \"casus belli\", it also indirectly supplied some weapons to Iran as part of a complex and illegal programme that became known as the Iran–Contra affair. These secret sales were partly to help secure the release of hostages held in Lebanon, and partly to make money to help the Contras rebel group in Nicaragua. This arms-for-hostages agreement turned into a major scandal.\n\nNorth Korea was a major arms supplier to Iran, often acting as a third party in arms deals between Iran and the Communist bloc. Support included domestically manufactured arms and Eastern-Bloc weapons, for which the major powers wanted deniability. Among the other arms suppliers and supporters of Iran's Islamic Revolution, the major ones where Libya, Syria, and China. According to the Stockholm International Peace Institute, China was the largest foreign arms supplier to Iran between 1980 and 1988.\n\nSyria and Libya, breaking Arab solidarity, supported Iran with arms, rhetoric and diplomacy.\n\nBesides the United States and the Soviet Union, Yugoslavia also sold weapons to both countries for the entire duration of the conflict. Likewise, Portugal helped both countries; it was not unusual to see Iranian and Iraqi flagged ships anchored at Setúbal, waiting their turn to dock.\n\nFrom 1980 to 1987, Spain sold €458 million in weapons to Iran and €172 million to Iraq. Weapons sold to Iraq included 4x4 vehicles, BO-105 helicopters, explosives, and ammunition. A research party later discovered that an unexploded chemical Iraqi warhead in Iran was manufactured in Spain.\n\nAlthough neither side acquired any weapons from Turkey, both sides enjoyed Turkish civilian trade during the conflict, although the Turkish government remained neutral and refused to support the U.S.-imposed trade embargo on Iran. Turkey's export market jumped from $220 million in 1981 to $2 billion in 1985, making up 25% of Turkey's overall exports. Turkish construction projects in Iraq totaled $2.5 billion between 1974 and 1990. Trading with both countries helped Turkey to offset its ongoing economic crisis, though the benefits decreased as the war neared its end and accordingly disappeared entirely with Iraq's invasion of Kuwait and the resulting Iraq sanctions Turkey imposed in response.\n\nAccording to \"Foreign Policy\", the \"Iraqis used mustard gas and sarin prior to four major offensives in early 1988 that relied on U.S. satellite imagery, maps, and other intelligence. ... According to recently declassified CIA documents and interviews with former intelligence officials like Francona, the U.S. had firm evidence of Iraqi chemical attacks beginning in 1983.\"\n\nA key element of U.S. political–military and energy–economic planning occurred in early 1983. The Iran–Iraq war had been going on for three years and there were significant casualties on both sides, reaching hundreds of thousands. Within the Reagan National Security Council concern was growing that the war could spread beyond the boundaries of the two belligerents. A National Security Planning Group meeting was called chaired by Vice President George Bush to review U.S. options. It was determined that there was a high likelihood that the conflict would spread into Saudi Arabia and other Gulf states, but that the United States had little capability to defend the region. Furthermore, it was determined that a prolonged war in the region would induce much higher oil prices and threaten the fragile world recovery which was just beginning to gain momentum. On 22 May 1984, President Reagan was briefed on the project conclusions in the Oval Office by William Flynn Martin who had served as the head of the NSC staff that organized the study. The full declassified presentation can be seen here. The conclusions were threefold: first oil stocks needed to be increased among members of the International Energy Agency and, if necessary, released early in the event of oil market disruption; second the United States needed to beef up the security of friendly Arab states in the region and thirdly an embargo should be placed on sales of military equipment to Iran and Iraq. The Plan was approved by the President and later affirmed by the G-7 leaders headed by Margaret Thatcher in the London Summit of 1984.\n\nOn 17 May 1987, an Iraqi Dassault Falcon 50–modified business jet launched two Exocet missiles at the , a \"Perry\" class frigate. The first struck the port side of the ship and failed to explode, though it left burning propellant in its wake; the second struck moments later in approximately the same place and penetrated through to crew quarters, where it exploded, killing 37 crew members and leaving 21 injured. Whether or not Iraqi leadership authorised the attack is still unknown. Initial claims by the Iraqi government (that \"Stark\" was inside the Iran–Iraq War zone) were shown to be false, and the motives and orders of the pilot remain unanswered. Though American officials claimed that the pilot who attacked \"Stark\" had been executed, an ex-Iraqi Air Force commander since stated he had not been punished, and was still alive at the time. The attack remains the only successful anti-ship missile strike on an American warship. Due to the extensive political and military cooperation between the Iraqis and Americans by 1987, the attack had little effect on relations between the two countries.\n\nU.S. attention was focused on isolating Iran as well as maintaining freedom of navigation. It criticised Iran's mining of international waters, and sponsored , which passed unanimously on 20 July, under which the U.S. and Iranian forces skirmished during Operation Earnest Will. During Operation Nimble Archer in October 1987, the United States attacked Iranian oil platforms in retaliation for an Iranian attack on the U.S.-flagged Kuwaiti tanker \"Sea Isle City\".\n\nOn 14 April 1988, the frigate was badly damaged by an Iranian mine, and 10 sailors were wounded. U.S. forces responded with Operation Praying Mantis on 18 April, the U.S. Navy's largest engagement of surface warships since World War II. Two Iranian oil platforms were damaged, and five Iranian warships and gunboats were sunk. An American helicopter also crashed. This fighting manifested in the International Court of Justice as Oil Platforms case (Islamic Republic of Iran v. United States of America), which was eventually dismissed in 2003.\n\nIn the course of escorts by the U.S. Navy, the cruiser shot down Iran Air Flight 655 on 3 July 1988, killing all 290 passengers and crew on board. The American government claimed that \"Vincennes\" was in international waters at the time (which was later proven to be untrue), that the Airbus A300 had been mistaken for an Iranian F-14 Tomcat, and that \"Vincennes\" feared that she was under attack. The Iranians maintain that \"Vincennes\" was in their own waters, and that the passenger jet was turning away and increasing altitude after take-off. U.S. Admiral William J. Crowe later admitted on \"Nightline\" that \"Vincennes\" was in Iranian territorial waters when it launched the missiles. At the time of the attack, Admiral Crowe claimed that the Iranian plane did not identify itself and sent no response to warning signals he had sent. In 1996, the United States expressed their regret for the event and the civilian deaths it caused.\n\nIn a declassified 1991 report, the CIA estimated that Iran had suffered more than 50,000 casualties from Iraq's use of several chemical weapons, though current estimates are more than 100,000 as the long-term effects continue to cause casualties. The official CIA estimate did not include the civilian population contaminated in bordering towns or the children and relatives of veterans, many of whom have developed blood, lung and skin complications, according to the Organization for Veterans of Iran. According to a 2002 article in the \"Star-Ledger\", 20,000 Iranian soldiers were killed on the spot by nerve gas. As of 2002, 5,000 of the 80,000 survivors continue to seek regular medical treatment, while 1,000 are hospital inpatients.\n\nAccording to Iraqi documents, assistance in developing chemical weapons was obtained from firms in many countries, including the United States, West Germany, the Netherlands, the United Kingdom, and France. A report stated that Dutch, Australian, Italian, French and both West and East German companies were involved in the export of raw materials to Iraqi chemical weapons factories. Declassified CIA documents show that the United States was providing reconnaissance intelligence to Iraq around 1987–88 which was then used to launch chemical weapon attacks on Iranian troops and that CIA fully knew that chemical weapons would be deployed and sarin and cyclosarin attacks followed.\n\nOn 21 March 1986, the United Nations Security Council made a declaration stating that \"members are profoundly concerned by the unanimous conclusion of the specialists that chemical weapons on many occasions have been used by Iraqi forces against Iranian troops, and the members of the Council strongly condemn this continued use of chemical weapons in clear violation of the Geneva Protocol of 1925, which prohibits the use in war of chemical weapons.\" The United States was the only member who voted against the issuance of this statement. A mission to the region in 1988 found evidence of the use of chemical weapons, and was condemned in Security Council Resolution 612.\n\nAccording to Walter Lang, senior defense intelligence officer at the U.S. Defense Intelligence Agency, \"the use of gas on the battlefield by the Iraqis was not a matter of deep strategic concern\" to Reagan and his aides, because they \"were desperate to make sure that Iraq did not lose\". He claimed that the Defense Intelligence Agency \"would have never accepted the use of chemical weapons against civilians, but the use against military objectives was seen as inevitable in the Iraqi struggle for survival\". The Reagan administration did not stop aiding Iraq after receiving reports of the use of poison gas on Kurdish civilians.\n\nThe United States accused Iran of using chemical weapons as well, though the allegations have been disputed. Joost Hiltermann, the principal researcher for Human Rights Watch between 1992 and 1994, conducted a two-year study that included a field investigation in Iraq, and obtained Iraqi government documents in the process. According to Hiltermann, the literature on the Iran–Iraq War reflects allegations of chemical weapons used by Iran, but they are \"marred by a lack of specificity as to time and place, and the failure to provide any sort of evidence\".\n\nAnalysts Gary Sick and Lawrence Potter have called the allegations against Iran \"mere assertions\" and stated, \"No persuasive evidence of the claim that Iran was the primary culprit [of using chemical weapons] was ever presented.\" Policy consultant and author Joseph Tragert stated, \"Iran did not retaliate with chemical weapons, probably because it did not possess any at the time\".\n\nAt his trial in December 2006, Saddam said he would take responsibility \"with honour\" for any attacks on Iran using conventional or chemical weapons during the war, but that he took issue with the charges that he ordered attacks on Iraqis. A medical analysis of the effects of Iraqi mustard gas is described in a U.S. military textbook and contrasted effects of World War I gas.\n\nAt the time of the conflict, the UN Security Council issued statements that \"chemical weapons had been used in the war\". UN statements never clarified that only Iraq was using chemical weapons, and according to retrospective authors \"the international community remained silent as Iraq used weapons of mass destruction against Iranian[s] as well as Iraqi Kurds.\"\n\nIran's attack on the \"Osirak\" nuclear reactor in September 1980 was the first attack on a nuclear reactor and one of only six military attacks on nuclear facilities in history. It was also the first instance of a pre-emptive attack on a nuclear reactor to forestall the development of a nuclear weapon, though it did not achieve its objective, as France repaired the reactor after the attack. (It took a second pre-emptive strike by the Israeli Air Force in June 1981 to disable the reactor, killing a French engineer in the process and causing France to pull out of \"Osirak\". The decommissioning of \"Osirak\" has been cited as causing a substantial delay to Iraqi acquisition of nuclear weapons.)\n\nThe Iran–Iraq War was the first and only conflict in the history of warfare in which both forces used ballistic missiles against each other. This war also saw the only confirmed air-to-air helicopter battles in history with the Iraqi Mi-25s flying against Iranian AH-1J SeaCobras (supplied by the United States before the Iranian Revolution) on several separate occasions. In November 1980, not long after Iraq's initial invasion of Iran, two Iranian SeaCobras engaged two Mi-25s with TOW wire-guided antitank missiles. One Mi-25 went down immediately, the other was badly damaged and crashed before reaching base. The Iranians repeated this accomplishment on 24 April 1981, destroying two Mi-25s without incurring losses to themselves. One Mi-25 was also downed by an IRIAF F-14A. The Iraqis hit back, claiming the destruction of a SeaCobra on 14 September 1983 (with YaKB machine gun), then three SeaCobras on 5 February 1984 and three more on 25 February 1984 (two with Falanga missiles, one with S-5 rockets). After a lull in helicopter losses, each side lost a gunship on 13 February 1986. Later, a Mi-25 claimed a SeaCobra shot down with YaKB gun on 16 February, and a SeaCobra claimed a Mi-25 shot down with rockets on 18 February. The last engagement between the two types was on 22 May 1986, when Mi-25s shot down a SeaCobra. The final claim tally was 10 SeaCobras and 6 Mi-25s destroyed. The relatively small numbers and the inevitable disputes over actual kill numbers makes it unclear if one gunship had a real technical superiority over the other. Iraqi Mi-25s also claimed 43 kills against other Iranian helicopters, such as Agusta-Bell UH-1 Hueys. Both sides, especially Iraq, also carried out air and missile attacks against population centers.\n\nIn October 1986, Iraqi aircraft began to attack civilian passenger trains and aircraft on Iranian soil, including an Iran Air Boeing 737 unloading passengers at Shiraz International Airport. In retaliation for the Iranian Operation Karbala 5, Iraq attacked 65 cities in 226 sorties over 42 days, bombing civilian neighbourhoods. Eight Iranian cities came under attack from Iraqi missiles. The bombings killed 65 children in an elementary school in Borujerd. The Iranians responded with Scud missile attacks on Baghdad and struck a primary school there. These events became known as the \"War of the Cities\".\n\nDespite the war, Iran and Iraq maintained diplomatic relations and embassies in each other's countries until mid-1987.\n\nIran's government used human waves to attack enemy troops and even in some cases to clear minefields. Children volunteered as well. Some reports mistakenly have the Basijis marching into battle while marking their expected entry to heaven by wearing \"plastic keys to paradise\" around their necks, although other analysts regard this story as a hoax involving a misinterpretation of the carrying of a prayer book called \"The Keys to Paradise\"(Mafatih al-Janan) by Sheikh Abbas Qumi given to all volunteers.\n\nAccording to journalist Robin Wright:\n\nDuring the Fateh offensive in February 1987, I toured the southwest front on the Iranian side and saw scores of boys, aged anywhere from nine to sixteen, who said with staggering and seemingly genuine enthusiasm that they had volunteered to become martyrs. Regular army troops, the paramilitary Revolutionary Guards and mullahs all lauded these youths, known as baseeji [Basij], for having played the most dangerous role in breaking through Iraqi lines. They had led the way, running over fields of mines to clear the ground for the Iranian ground assault. Wearing white headbands to signify the embracing of death, and shouting \"Shaheed, shaheed\" (Martyr, martyr) they literally blew their way into heaven. Their numbers were never disclosed. But a walk through the residential suburbs of Iranian cities provided a clue. Window after window, block after block, displayed black-bordered photographs of teenage or preteen youths.\n\nThe relationship between these two nations has warmed immensely since the downfall of Saddam Hussein, but mostly out of pragmatic interest. Iran and Iraq share many common interests, as they share a common enemy in the Islamic State. Significant military assistance has been provided by Iran to Iraq and this has bought them a large amount of political influence in Iraq's newly elected Shiite government. Iraq is also heavily dependent on the more stable and developed Iran for its energy needs, so a peaceful customer is likely a high priority for Iran, foreign policy wise. The Iran- Iraq War is also regarded as being a major trigger for rising sectarianism in the region, as it was viewed by many as a clash between Sunni Muslims (Iraq and other Arab States) and the Shiite revolutionaries Iran that had recently taken power in Iran. There remains lingering animosity however; despite the pragmatic alliance that has been formed as multiple government declarations from Iran have stated that the war will “affect every issue of internal and foreign policy” for decades to come. The sustained importance of this conflict is attributed mostly to the massive human and economic cost resulting from it, along with it's ties to the Iranian Revolution. Another significant effect that the war has on Iran's policy is the issue of remaining war reparations. The UN estimates that Iraq owes about $149 billion, while Iran contends that, with both the direct and indirect effects taken into account, the cost of the war reaches a trillion. Iran has not vocalized the desire for these reparations in recent years, and has even suggested forms of financial aid. This is due most likely to Iran's interest in keeping Iraq politically stable, and imposing these reparation costs would further burden the already impoverished nation. The most important factor that governs Iraq's current foreign policy is the national government's consistent fragility following the overthrow of Saddam Hussein. Iraq's need for any and all allies that can help bring stability and bring development has allowed Iran to exert significant influence over the new Iraqi state; despite lingering memories of the war. Iraq is far too weak of a state to attempt to challenge Iran regionally, so accepting support while focusing on counter insurgency and stabilization is in their best interest. \n\nCurrently, it seems as though Iraq is being pulled in two opposing directions, between a practical relationship with Iran, who can provide a reliable source of power as well as military support to the influential Shiite militias and political factions. The United States is pulling in the opposite direction as they offer Iraq significant economic aid packages, along with military support in the form of air and artillery strikes, all in the hopes to establish a stable ally in the region. If Iraq lurches too far in either direction, then the benefits offered to them by the other side will likely be gradually reduced or cut off completely. Another significant factor influencing relations is the shared cultural interests of their respective citizens, as they both wish to freely visit the multitude of holy sites located in both countries.       \nGeneral:\nPersons:\nMemoirs\nStories\nRelevant conflicts\n\n\n"}
{"id": "14891", "url": "https://en.wikipedia.org/wiki?curid=14891", "title": "Incremental reading", "text": "Incremental reading\n\nIncremental reading is a software-assisted method for learning and retaining information from reading, helping with the creation of flashcards out of electronic articles read in portions inside a prioritized reading list.\n\nIt is particularly targeted to people who are trying to learn for life a large amount of information, particularly if that information comes from various sources.\n\n\"Incremental reading\" means \"reading in portions\". Instead of a linear reading of articles one at a time, the method works by keeping a large reading list of electronic articles or books (often dozens or hundreds of them) and reading parts of several articles in each session. Articles in the reading list are prioritized by the user.\n\nIn the course of reading, key points of articles are broken up into flashcards, which are then learned and reviewed over an extended period of time with the help of a spaced repetition algorithm.\n\nThis use of flashcards at later stages of the process is based on psychological principles of long-term memory storage and retrieval, in particular the spacing effect (the phenomenon whereby learning is greater when studying is spread out over time) and the testing effect (the finding that long-term memory is increased when some of the learning period is devoted to retrieving the to-be-remembered information through testing).\n\nCurrent implementations include spaced repetition software Anki – through an add-on – and SuperMemo. There is also incremental reading support for the text editors Emacs and Yi.\n\nThe method itself is often credited to the Polish software developer Piotr Wozniak. He implemented the first version of incremental reading in 1999 in SuperMemo 99, providing the essential tools of the method: a prioritized reading list, and the possibility to extract portions of articles and to create cloze deletions. The term \"incremental reading\" itself appeared the next year with SuperMemo 2000. Later SuperMemo programmes subsequently enhanced the tools and techniques involved, such as webpage imports, material overload handling, etc.\n\nLimited incremental reading support for the text editor Emacs appeared in 2007.\n\nAn Anki add-on for incremental reading was later published in 2011; for Anki 2.0 and 2.1, another add-on is available.\n\nIncremental reading was the first of a series of related concepts invented by Piotr Wozniak: incremental image learning, incremental video, incremental audio, incremental mail processing, incremental problem solving, and incremental writing. \"Incremental learning\" is the term Wozniak uses to refer to those concepts as a whole.\n\nWhen reading an electronic article, the user extracts the most important parts (similar to underlining or highlighting a paper article) and gradually distills them into flashcards. Flashcards are information presented in a question-answer format (making active recall possible). Cloze deletions are often used in incremental reading, as they are easy to create out of text. Both extracts and flashcards are scheduled independently from the original article.\n\nWith time and reviews, articles are supposed to be gradually converted into extracts, and extracts into flashcards. Hence, incremental reading is a method of breaking down information from electronic articles into sets of flashcards.\n\nContrary to extracts, flashcards are reviewed with active recall. This means that extracts such as \"George Washington was the first U.S. President\" must ultimately be converted into questions such as \"Who was the first U.S. President?\" (Answer: George Washington), or \"Who was George Washington?\" (Answer: the first U.S. President), etc., or cloze deletions such as \"[BLANK] was the first U.S. President\", \"George Washington was [BLANK]\", etc.\n\nThis flashcard creation process is semi-automated – the reader chooses which material to learn and edits the precise wording of the questions, while the software assists in prioritizing articles and making the flashcards, and does the scheduling: it calculates the ideal time for the reader to review each chunk, according to the rules of spaced repetition. This means that all processed pieces of information are presented at increasing intervals.\n\nIndividual articles are read in portions proportional to the attention span, which depends on the user, their mood, the article, etc. This allows for a substantial gain in attention, according to Piotr Wozniak.\n\nWithout the use of spaced repetition, the reader would quickly get lost in the glut of information when studying dozens of subjects in parallel. However, spaced repetition makes it possible to retain traces of the processed material in memory.\n\n"}
{"id": "14892", "url": "https://en.wikipedia.org/wiki?curid=14892", "title": "Intelligence quotient", "text": "Intelligence quotient\n\nAn intelligence quotient (IQ) is a total score derived from several standardized tests designed to assess human intelligence. The abbreviation \"IQ\" was coined by the psychologist William Stern for the German term \"Intelligenzquotient\", his term for a scoring method for intelligence tests at University of Breslau he advocated in a 1912 book. Historically, IQ is a score obtained by dividing a person's mental age score, obtained by administering an intelligence test, by the person's chronological age, both expressed in terms of years and months. The resulting fraction is multiplied by 100 to obtain the IQ score. When current IQ tests were developed, the median raw score of the norming sample is defined as IQ 100 and scores each standard deviation (SD) up or down are defined as 15 IQ points greater or less, although this was not always so historically. By this definition, approximately two-thirds of the population scores are between IQ 85 and IQ 115. About 2.5 percent of the population scores above 130, and 2.5 percent below 70.\n\nScores from intelligence tests are estimates of intelligence. Unlike, for example, distance and mass, a concrete measure of intelligence cannot be achieved given the abstract nature of the concept of \"intelligence\". IQ scores have been shown to be associated with such factors as morbidity and mortality, parental social status, and, to a substantial degree, biological parental IQ. While the heritability of IQ has been investigated for nearly a century, there is still debate about the significance of heritability estimates and the mechanisms of inheritance.\n\nIQ scores are used for educational placement, assessment of intellectual disability, and evaluating job applicants. Even when students improve their scores on standardized tests, they do not always improve their cognitive abilities, such as memory, attention and speed. In research contexts they have been studied as predictors of job performance, and income. They are also used to study distributions of psychometric intelligence in populations and the correlations between it and other variables. Raw scores on IQ tests for many populations have been rising at an average rate that scales to three IQ points per decade since the early 20th century, a phenomenon called the Flynn effect. Investigation of different patterns of increases in subtest scores can also inform current research on human intelligence.\n\nHistorically, even before IQ tests were devised, there were attempts to classify people into intelligence categories by observing their behavior in daily life. Those other forms of behavioral observation are still important for validating classifications based primarily on IQ test scores. Both intelligence classification by observation of behavior outside the testing room and classification by IQ testing depend on the definition of \"intelligence\" used in a particular case and on the reliability and error of estimation in the classification procedure.\n\nThe English statistician Francis Galton made the first attempt at creating a standardized test for rating a person's intelligence. A pioneer of psychometrics and the application of statistical methods to the study of human diversity and the study of inheritance of human traits, he believed that intelligence was largely a product of heredity (by which he did not mean genes, although he did develop several pre-Mendelian theories of particulate inheritance). He hypothesized that there should exist a correlation between intelligence and other observable traits such as reflexes, muscle grip, and head size. He set up the first mental testing centre in the world in 1882 and he published \"Inquiries into Human Faculty and Its Development\" in 1883, in which he set out his theories. After gathering data on a variety of physical variables, he was unable to show any such correlation, and he eventually abandoned this research.\nFrench psychologist Alfred Binet, together with Victor Henri and Théodore Simon had more success in 1905, when they published the Binet-Simon test, which focused on verbal abilities. It was intended to identify mental retardation in school children, but in specific contradistinction to claims made by psychiatrists that these children were \"sick\" (not \"slow\") and should therefore be removed from school and cared for in asylums. The score on the Binet-Simon scale would reveal the child's mental age. For example, a six-year-old child who passed all the tasks usually passed by six-year-olds—but nothing beyond—would have a mental age that matched his chronological age, 6.0. (Fancher, 1985). Binet thought that intelligence was multifaceted, but came under the control of practical judgment.\n\nIn Binet's view, there were limitations with the scale and he stressed what he saw as the remarkable diversity of intelligence and the subsequent need to study it using qualitative, as opposed to quantitative, measures (White, 2000). American psychologist Henry H. Goddard published a translation of it in 1910. American psychologist Lewis Terman at Stanford University revised the Binet-Simon scale, which resulted in the Stanford-Binet Intelligence Scales (1916). It became the most popular test in the United States for decades.\n\nThe many different kinds of IQ tests include a wide variety of item content. Some test items are visual, while many are verbal. Test items vary from being based on abstract-reasoning problems to concentrating on arithmetic, vocabulary, or general knowledge.\n\nThe British psychologist Charles Spearman in 1904 made the first formal factor analysis of correlations between the tests. He observed that children's school grades across seemingly unrelated school subjects were positively correlated, and reasoned that these correlations reflected the influence of an underlying general mental ability that entered into performance on all kinds of mental tests. He suggested that all mental performance could be conceptualized in terms of a single general ability factor and a large number of narrow task-specific ability factors. Spearman named it \"g\" for \"general factor\" and labeled the specific factors or abilities for specific tasks \"s\". In any collection of test items that make up an IQ test, the score that best measures \"g\" is the composite score that has the highest correlations with all the item scores. Typically, the \"\"g\"-loaded\" composite score of an IQ test battery appears to involve a common strength in abstract reasoning across the test's item content. Therefore, Spearman and others have regarded \"g\" as closely related to the essence of human intelligence.\n\nSpearman's argument proposing a general factor of human intelligence is still accepted, in principle, to be the most important construct to intelligence by many psychometricians, though none would say that it is all there is to intelligence. Today's factor models of intelligence typically represent cognitive abilities as a three-level hierarchy, where there are a large number of narrow factors at the bottom of the hierarchy, a handful of broad, more general factors at the intermediate level, and at the apex a single factor, referred to as the \"g\" factor, which represents the variance common to all cognitive tasks. However, this view is not universally accepted; other factor analyses of the data, with different results, are possible. Some psychometricians regard \"g\" as a statistical artifact.\n\nDuring World War I, the Army needed a way to evaluate and assign recruits to appropriate tasks. This led to the development of several mental tests by Robert Yerkes, who worked with major hereditarians of American psychometrics—including Terman, Goddard—to write the test. The testing generated controversy and much public debate in the United States. Nonverbal or \"performance\" tests were developed for those who could not speak English or were suspected of malingering. Based on Goddard's translation of the Binet-Simon test, the tests had an impact in screening men for officer training:...the tests did have a strong impact in some areas, particularly in screening men for officer training. At the start of the war, the army and national guard maintained nine thousand officers. By the end, two hundred thousand officers presided, and two- thirds of them had started their careers in training camps where the tests were applied. In some camps, no man scoring below C could be considered for officer training.1.75 million men were tested in total, making the results the first mass-produced written tests of intelligence, though considered dubious and non-usable, for reasons including high variability of test implementation throughout different camps and questions testing for familiarity with American culture rather than intelligence. After the war, positive publicity promoted by army psychologists helped to make psychology a respected field. Subsequently, there was an increase in jobs and funding in psychology in the United States. Group intelligence tests were developed and became widely used in schools and industry.\n\nThe results of these tests, which at the time reaffirmed contemporary racism and nationalism, are considered controversial and dubious, having rested on certain contested assumptions: that intelligence was heritable, innate, and could be relegated to a single number, the tests were enacted systematically, and test questions actually tested for innate intelligence rather than subsuming environmental factors. The tests also allowed for the bolstering of jingoist narratives in the context of increased immigration, which may have influenced the passing of the Immigration Restriction Act of 1924.\n\nL.L. Thurstone argued for a model of intelligence that included seven unrelated factors (verbal comprehension, word fluency, number facility, spatial visualization, associative memory, perceptual speed, reasoning, and induction). While not widely used, Thurstone's model influenced later theories.\n\nDavid Wechsler produced the first version of his test in 1939. It gradually became more popular and overtook the Stanford-Binet in the 1960s. It has been revised several times, as is common for IQ tests, to incorporate new research. One explanation is that psychologists and educators wanted more information than the single score from the Binet. Wechsler's ten or more subtests provided this. Another is that the Stanford-Binet test reflected mostly verbal abilities, while the Wechsler test also reflected nonverbal abilities. The Stanford-Binet has also been revised several times and is now similar to the Wechsler in several aspects, but the Wechsler continues to be the most popular test in the United States.\n\nEugenics refers to the principles of heredity used to improve the human race. Francis Galton first used the term in the late 1800s. The eugenics movement was popularized by Progressivism in the US in the 1920s and 1930s.\n\nGoddard was a eugenicist. In 1908, he published his own version, \"The Binet and Simon Test of Intellectual Capacity\", and cordially promoted the test. He quickly extended the use of the scale to the public schools (1913), to immigration (Ellis Island, 1914) and to a court of law (1914).\n\nDifferent from Galton, who promoted eugenics through selective breeding for positive traits, Goddard went with the US eugenics movement to eliminate \"undesirable\" traits. Goddard used the term \"feeble-minded\" to refer to people who did not perform well in the test and thus were intellectually inferior. He argued that \"feeble-mindedness\" is caused by heredity, thus feeble-minded people should be prevented from giving birth, either by institutional isolation or sterilization surgeries. At first sterilization targeted the disabled and was extended to poor people. Goddard's intelligence test was endorsed by the eugenicists to push for laws for forced sterilization. Different states adopted the sterilization laws at different pace. These laws forced over 64,000 people to go through sterilization in the United States.\n\nCalifornia's sterilization program was so effective that the Nazis turned to the government for advice on how to prevent the birth of the \"unfit\". The US eugenics movement lost its momentum in 1940s and was halted in view of the horrors of Nazi Germany.\n\nRaymond Cattell (1941) proposed two types of cognitive abilities in a revision of Spearman's concept of general intelligence. Fluid intelligence (Gf) was hypothesized as the ability to solve novel problems by using reasoning, and crystallized intelligence (Gc) was hypothesized as a knowledge-based ability that was very dependent on education and experience. In addition, fluid intelligence was hypothesized to decline with age, while crystallized intelligence was largely resistant to the effects of aging. The theory was almost forgotten, but was revived by his student John L. Horn (1966) who later argued Gf and Gc were only two among several factors, and who eventually identified nine or ten broad abilities. The theory continued to be called Gf-Gc theory.\n\nJohn B. Carroll (1993), after a comprehensive reanalysis of earlier data, proposed the three stratum theory, which is a hierarchical model with three levels. The bottom stratum consists of narrow abilities that are highly specialized (e.g., induction, spelling ability). The second stratum consists of broad abilities. Carroll identified eight second-stratum abilities. Carroll accepted Spearman's concept of general intelligence, for the most part, as a representation of the uppermost, third stratum.\n\nIn 1999, a merging of the Gf-Gc theory of Cattell and Horn with Carroll's Three-Stratum theory has led to the Cattell–Horn–Carroll theory (CHC Theory). It has greatly influenced many of the current broad IQ tests.\n\nIn CHC theory, a hierarchy of factors is used; \"g\" is at the top. Under it are ten broad abilities that in turn are subdivided into seventy narrow abilities. The broad abilities are:\nModern tests do not necessarily measure all of these broad abilities. For example, Gq and Grw may be seen as measures of school achievement and not IQ. Gt may be difficult to measure without special equipment. \"g\" was earlier often subdivided into only Gf and Gc, which were thought to correspond to the nonverbal or performance subtests and verbal subtests in earlier versions of the popular Wechsler IQ test. More recent research has shown the situation to be more complex. Modern comprehensive IQ tests do not stop at reporting a single IQ score. Although they still give an overall score, they now also give scores for many of these more restricted abilities, identifying particular strengths and weaknesses of an individual.\n\nJ.P. Guilford's Structure of Intellect (1967) model used three dimensions which when combined yielded a total of 120 types of intelligence. It was popular in the 1970s and early 1980s, but faded owing to both practical problems and theoretical criticisms.\n\nAlexander Luria's earlier work on neuropsychological processes led to the PASS theory (1997). It argued that only looking at one general factor was inadequate for researchers and clinicians who worked with learning disabilities, attention disorders, intellectual disability, and interventions for such disabilities. The PASS model covers four kinds of processes (planning process, attention/arousal process, simultaneous processing, and successive processing). The planning processes involve decision making, problem solving, and performing activities and requires goal setting and self-monitoring.\n\nThe attention/arousal process involves selectively attending to a particular stimulus, ignoring distractions, and maintaining vigilance. Simultaneous processing involves the integration of stimuli into a group and requires the observation of relationships. Successive processing involves the integration of stimuli into serial order. The planning and attention/arousal components comes from structures located in the frontal lobe, and the simultaneous and successive processes come from structures located in the posterior region of the cortex. It has influenced some recent IQ tests, and been seen as a complement to the Cattell-Horn-Carroll theory described above.\n\nThere are a variety of individually administered IQ tests in use in the English-speaking world. The most commonly used individual IQ test series is the Wechsler Adult Intelligence Scale for adults and the Wechsler Intelligence Scale for Children for school-age test-takers. Other commonly used individual IQ tests (some of which do not label their standard scores as \"IQ\" scores) include the current versions of the Stanford-Binet Intelligence Scales, Woodcock-Johnson Tests of Cognitive Abilities, the Kaufman Assessment Battery for Children, the Cognitive Assessment System, and the Differential Ability Scales.\n\nIQ tests that measure intelligence also include:\n\nIQ scales are ordinally scaled. While one standard deviation is 15 points, and two SDs are 30 points, and so on, this does not imply that mental ability is linearly related to IQ, such that IQ 50 means half the cognitive ability of IQ 100. In particular, IQ points are not percentage points.\n\nOn a related note, this fixed standard deviation means that the proportion of the population who have IQs in a particular range is theoretically fixed, and current Wechsler tests only give Full Scale IQs between 40 and 160. This should be borne in mind when considering reports of people with much higher IQs.\n\nDifferential item functioning (DIF), sometimes referred to as measurement bias, is a phenomenon when participants from different groups (e.g. gender, race, disability) with the same latent abilities give different answers to specific questions on the same IQ test. DIF analysis measures such specific items on a test alongside measuring participants latent abilities on other similar questions. A consistent different group response to a specific question among similar type of questions can indicate an effect of DIF. It does not count as differential item functioning if both groups have equally valid chance of giving different responses to the same questions. Such bias can be a result of culture, educational level and other factors that are independent of group traits. DIF is only considered if test-takers from different groups \"with the same underlying latent ability level\" have a different chance of giving specific responses. Such questions are usually removed in order to make the test equally fair for both groups. Common techniques for analyzing DIF are item response theory (IRT) based methods, Mantel-Haenszel, and logistic regression.\n\nPsychometricians generally regard IQ tests as having high statistical reliability. A high reliability implies that – although test-takers may have varying scores when taking the same test on differing occasions, and although they may have varying scores when taking different IQ tests at the same age – the scores generally agree with one another and across time. Like all statistical quantities, any particular estimate of IQ has an associated standard error that measures uncertainty about the estimate. For modern tests, the standard error of measurement is about three points . Clinical psychologists generally regard IQ scores as having sufficient statistical validity for many clinical purposes. In a survey of 661 randomly sampled psychologists and educational researchers, published in 1988, Mark Snyderman and Stanley Rothman reported a general consensus supporting the validity of IQ testing. \"On the whole, scholars with any expertise in the area of intelligence and intelligence testing (defined very broadly) share a common view of the most important components of intelligence, and are convinced that it can be measured with some degree of accuracy.\" Almost all respondents picked out abstract reasoning, ability to solve problems and ability to acquire knowledge as the most important elements.\nSince the early 20th century, raw scores on IQ tests have increased in most parts of the world. When a new version of an IQ test is normed, the standard scoring is set so performance at the population median results in a score of IQ 100. The phenomenon of rising raw score performance means if test-takers are scored by a constant standard scoring rule, IQ test scores have been rising at an average rate of around three IQ points per decade. This phenomenon was named the Flynn effect in the book \"The Bell Curve\" after James R. Flynn, the author who did the most to bring this phenomenon to the attention of psychologists.\n\nResearchers have been exploring the issue of whether the Flynn effect is equally strong on performance of all kinds of IQ test items, whether the effect may have ended in some developed nations, whether there are social subgroup differences in the effect, and what possible causes of the effect might be. A 2011 textbook, \"IQ and Human Intelligence\", by N. J. Mackintosh, noted the Flynn effect demolishes the fears that IQ would be decreased. He also asks whether it represents a real increase in intelligence beyond IQ scores. A 2011 psychology textbook, lead authored by Harvard Psychologist Professor Daniel Schacter, noted that humans' inherited intelligence could be going down while acquired intelligence goes up.\n\nResearch has revealed that the Flynn effect has slowed or reversed course in several Western countries beginning in the late 20th century. The phenomenon has been termed the \"negative Flynn effect\". A study of Norwegian military conscripts' test records found that IQ scores have been falling for generations born after the year 1975, and that the underlying nature of both initial increasing and subsequent falling trends appears to be environmental rather than genetic.\n\nIQ can change to some degree over the course of childhood. However, in one longitudinal study, the mean IQ scores of tests at ages 17 and 18 were correlated at r=0.86 with the mean scores of tests at ages five, six, and seven and at r=0.96 with the mean scores of tests at ages 11, 12, and 13.\n\nFor decades, practitioners' handbooks and textbooks on IQ testing have reported IQ declines with age after the beginning of adulthood. However, later researchers pointed out this phenomenon is related to the Flynn effect and is in part a cohort effect rather than a true aging effect. A variety of studies of IQ and aging have been conducted since the norming of the first Wechsler Intelligence Scale drew attention to IQ differences in different age groups of adults. Current consensus is that fluid intelligence generally declines with age after early adulthood, while crystallized intelligence remains intact. Both cohort effects (the birth year of the test-takers) and practice effects (test-takers taking the same form of IQ test more than once) must be controlled to gain accurate data. It is unclear whether any lifestyle intervention can preserve fluid intelligence into older ages.\n\nThe exact peak age of fluid intelligence or crystallized intelligence remains elusive. Cross-sectional studies usually show that especially fluid intelligence peaks at a relatively young age (often in the early adulthood) while longitudinal data mostly show that intelligence is stable until mid-adulthood or later. Subsequently, intelligence seems to decline slowly.\n\nEnvironmental and genetic factors play a role in determining IQ. Their relative importance has been the subject of much research and debate.\n\nHeritability is defined as the proportion of variance in a trait which is attributable to genotype within a defined population in a specific environment. A number of points must be considered when interpreting heritability. Heritability, as a term, applies to populations, and in populations there are variations in traits between individuals. Heritability measures how much of that variation is caused by genetics. The value of heritability can change if the impact of environment (or of genes) in the population is substantially altered. A high heritability of a trait does not mean environmental effects, such as learning, are not involved. Since heritability increases during childhood and adolescence, one should be cautious drawing conclusions regarding the role of genetics and environment from studies where the participants are not followed until they are adults.\n\nThe general figure for the heritability of IQ, according to an authoritative American Psychological Association report, is 0.45 for children, and rises to around 0.75 for late adolescents and adults. Heritability measures in infancy are as low as 0.2, around 0.4 in middle childhood, and as high as 0.9 in adulthood. One proposed explanation is that people with different genes tend to reinforce the effects of those genes, for example by seeking out different environments.\n\nFamily members have aspects of environments in common (for example, characteristics of the home). This shared family environment accounts for 0.25–0.35 of the variation in IQ in childhood. By late adolescence, it is quite low (zero in some studies). The effect for several other psychological traits is similar. These studies have not looked at the effects of such extreme environments, such as in abusive families.\n\nAlthough parents treat their children differently, such differential treatment explains only a small amount of nonshared environmental influence. One suggestion is that children react differently to the same environment because of different genes. More likely influences may be the impact of peers and other experiences outside the family.\n\nA very large proportion of the over 17,000 human genes are thought to have an effect on the development and functionality of the brain. While a number of individual genes have been reported to be associated with IQ, none have a strong effect. Deary and colleagues (2009) reported that no finding of a strong single gene effect on IQ has been replicated. Recent findings of gene associations with normally varying intelligence differences in adults continue to show weak effects for any one gene; likewise in children, but see.\n\nDavid Rowe reported an interaction of genetic effects with socioeconomic status, such that the heritability was high in high-SES families, but much lower in low-SES families. In the US, this has been replicated in infants, children, adolescents, and adults. Outside the US, studies show no link between heritability and SES. Some effects may even reverse sign outside the US.\n\nDickens and Flynn (2001) have argued that genes for high IQ initiate an environment-shaping feedback cycle, with genetic effects causing bright children to seek out more stimulating environments that then further increase their IQ. In Dickens' model, environment effects are modeled as decaying over time. In this model, the Flynn effect can be explained by an increase in environmental stimulation independent of it being sought out by individuals. The authors suggest that programs aiming to increase IQ would be most likely to produce long-term IQ gains if they enduringly raised children's drive to seek out cognitively demanding experiences.\n\nIn general, educational interventions, as those described below, have shown short-term effects on IQ, but long-term follow-up is often missing. For example, in the US very large intervention programs such as the Head Start Program have not produced lasting gains in IQ scores. More intensive, but much smaller projects such as the Abecedarian Project have reported lasting effects, often on socioeconomic status variables, rather than IQ.\n\nRecent studies have shown that training in using one's working memory may increase IQ. A study on young adults published in April 2008 by a team from the Universities of Michigan and Bern supports the possibility of the transfer of fluid intelligence from specifically designed working memory training. Further research will be needed to determine nature, extent and duration of the proposed transfer. Among other questions, it remains to be seen whether the results extend to other kinds of fluid intelligence tests than the matrix test used in the study, and if so, whether, after training, fluid intelligence measures retain their correlation with educational and occupational achievement or if the value of fluid intelligence for predicting performance on other tasks changes. It is also unclear whether the training is durable of extended periods of time.\n\nMusical training in childhood has been found to correlate with higher than average IQ. It is popularly thought that listening to classical music raises IQ. However, multiple attempted replications (e.g.) have shown that this is at best a short-term effect (lasting no longer than 10 to 15 minutes), and is not related to IQ-increase.\n\nSeveral neurophysiological factors have been correlated with intelligence in humans, including the ratio of brain weight to body weight and the size, shape, and activity level of different parts of the brain. Specific features that may affect IQ include the size and shape of the frontal lobes, the amount of blood and chemical activity in the frontal lobes, the total amount of gray matter in the brain, the overall thickness of the cortex, and the glucose metabolic rate.\n\nHealth is important in understanding differences in IQ test scores and other measures of cognitive ability. Several factors can lead to significant cognitive impairment, particularly if they occur during pregnancy and childhood when the brain is growing and the blood–brain barrier is less effective. Such impairment may sometimes be permanent, sometimes be partially or wholly compensated for by later growth. \n\nSince about 2010, researchers such as Eppig, Hassel, and MacKenzie have found a very close and consistent link between IQ scores and infectious diseases, especially in the infant and preschool populations and the mothers of these children. They have postulated that fighting infectious diseases strains the child's metabolism and prevents full brain development. Hassel postulated that it is by far the most important factor in determining population IQ. However, they also found that subsequent factors such as good nutrition and regular quality schooling can offset early negative effects to some extent.\n\nDeveloped nations have implemented several health policies regarding nutrients and toxins known to influence cognitive function. These include laws requiring fortification of certain food products and laws establishing safe levels of pollutants (e.g. lead, mercury, and organochlorides). Improvements in nutrition, and in public policy in general, have been implicated in worldwide IQ increases. \n\nCognitive epidemiology is a field of research that examines the associations between intelligence test scores and health. Researchers in the field argue that intelligence measured at an early age is an important predictor of later health and mortality differences.\n\nThe American Psychological Association's report \"Intelligence: Knowns and Unknowns\" states that wherever it has been studied, children with high scores on tests of intelligence tend to learn more of what is taught in school than their lower-scoring peers. The correlation between IQ scores and grades is about .50. This means that the explained variance is 25%. Achieving good grades depends on many factors other than IQ, such as \"persistence, interest in school, and willingness to study\" (p. 81).\n\nIt has been found that the correlation of IQ scores with school performance depends on the IQ measurement used. For undergraduate students, the Verbal IQ as measured by WAIS-R has been found to correlate significantly (0.53) with the grade point average (GPA) of the last 60 hours. In contrast, Performance IQ correlation with the same GPA was only 0.22 in the same study.\n\nSome measures of educational aptitude correlate highly with IQ tests for instance, Frey and Detterman (2004) reported a correlation of 0.82 between \"g\" (general intelligence factor) and SAT scores; another research found a correlation of 0.81 between \"g\" and GCSE scores, with the explained variance ranging \"from 58.6% in Mathematics and 48% in English to 18.1% in Art and Design\".\n\nAccording to Schmidt and Hunter, \"for hiring employees without previous experience in the job the most valid predictor of future performance is general mental ability.\" The validity of IQ as a predictor of job performance is above zero for all work studied to date, but varies with the type of job and across different studies, ranging from 0.2 to 0.6. The correlations were higher when the unreliability of measurement methods was controlled for. While IQ is more strongly correlated with reasoning and less so with motor function, IQ-test scores predict performance ratings in all occupations. That said, for highly qualified activities (research, management) low IQ scores are more likely to be a barrier to adequate performance, whereas for minimally-skilled activities, athletic strength (manual strength, speed, stamina, and coordination) are more likely to influence performance. The prevailing view among academics is that it is largely through the quicker acquisition of job-relevant knowledge that higher IQ mediates job performance. This view has been challenged by Byington & Felps (2010), who argued that \"the current applications of IQ-reflective tests allow individuals with high IQ scores to receive greater access to developmental resources, enabling them to acquire additional capabilities over time, and ultimately perform their jobs better.\"\n\nIn establishing a causal direction to the link between IQ and work performance, longitudinal studies by Watkins and others suggest that IQ exerts a causal influence on future academic achievement, whereas academic achievement does not substantially influence future IQ scores. Treena Eileen Rohde and Lee Anne Thompson write that general cognitive ability, but not specific ability scores, predict academic achievement, with the exception that processing speed and spatial ability predict performance on the SAT math beyond the effect of general cognitive ability.\n\nThe US military has minimum enlistment standards at about the IQ 85 level. There have been two experiments with lowering this to 80 but in both cases these men could not master soldiering well enough to justify their costs.\n\nWhile it has been suggested that \"in economic terms it appears that the IQ score measures something with decreasing marginal value. It is important to have enough of it, but having lots and lots does not buy you that much\", large-scale longitudinal studies indicate an increase in IQ translates into an increase in performance at all levels of IQ: i.e. ability and job performance are monotonically linked at all IQ levels. Charles Murray, coauthor of \"The Bell Curve,\" found that IQ has a substantial effect on income independent of family background.\n\nThe link from IQ to wealth is much less strong than that from IQ to job performance. Some studies indicate that IQ is unrelated to net worth.\n\nThe American Psychological Association's 1995 report \"Intelligence: Knowns and Unknowns\" stated that IQ scores accounted for (explained variance) about a quarter of the social status variance and one-sixth of the income variance. Statistical controls for parental SES eliminate about a quarter of this predictive power. Psychometric intelligence appears as only one of a great many factors that influence social outcomes.\n\nIn a meta-analysis, Strenze (2006) reviewed much of the literature and estimated the correlation between IQ and income to be about 0.23.\n\nSome studies claim that IQ only accounts for (explains) a sixth of the variation in income because many studies are based on young adults, many of whom have not yet reached their peak earning capacity, or even their education. On pg 568 of \"\", Arthur Jensen claims that although the correlation between IQ and income averages a moderate 0.4 (one sixth or 16% of the variance), the relationship increases with age, and peaks at middle age when people have reached their maximum career potential. In the book, \"A Question of Intelligence\", Daniel Seligman cites an IQ income correlation of 0.5 (25% of the variance).\n\nA 2002 study further examined the impact of non-IQ factors on income and concluded that an individual's location, inherited wealth, race, and schooling are more important as factors in determining income than IQ.\n\nThe American Psychological Association's 1996 report \"Intelligence: Knowns and Unknowns\" stated that the correlation between IQ and crime was −0.2. It was −0.19 between IQ scores and number of juvenile offenses in a large Danish sample; with social class controlled, the correlation dropped to −0.17. A correlation of 0.20 means that the explained variance is 4%. The causal links between psychometric ability and social outcomes may be indirect. Children with poor scholastic performance may feel alienated. Consequently, they may be more likely to engage in delinquent behavior, compared to other children who do well.\n\nIn his book \"\" (1998), Arthur Jensen cited data which showed that, regardless of race, people with IQs between 70 and 90 have higher crime rates than people with IQs below or above this range, with the peak range being between 80 and 90.\n\nThe 2009 \"Handbook of Crime Correlates\" stated that reviews have found that around eight IQ points, or 0.5 SD, separate criminals from the general population, especially for persistent serious offenders. It has been suggested that this simply reflects that \"only dumb ones get caught\" but there is similarly a negative relation between IQ and self-reported offending. That children with conduct disorder have lower IQ than their peers \"strongly argues\" for the theory.\n\nA study of the relationship between US county-level IQ and US county-level crime rates found that higher average IQs were associated with lower levels of property crime, burglary, larceny rate, motor vehicle theft, violent crime, robbery, and aggravated assault. These results were not \"confounded by a measure of concentrated disadvantage that captures the effects of race, poverty, and other social disadvantages of the county.\"\n\nMultiple studies conducted in Scotland have found that higher IQs in early life are associated with lower mortality and morbidity rates later in life.\n\nThere is considerable variation within and overlap among these categories. People with high IQs are found at all levels of education and occupational categories. The biggest difference occurs for low IQs with only an occasional college graduate or professional scoring below 90.\n\nWith operationalization and methodology derived from the general intelligence factor \"g\", a new scientific understanding of collective intelligence, defined as a group’s general ability to perform a wide range of tasks, aims to explain intelligent behavior of groups. Goal is to detect and explain a general intelligence factor \"c\" for groups, parallel to the \"g\" factor for individuals. As \"g\" is highly interrelated with the concept of IQ, this measurement of collective intelligence can be interpreted as intelligence quotient for groups (Group-IQ) even though the score is not a quotient per se. Current evidence suggests that this Group-IQ is only moderately correlated with group members' IQs but with other correlates such as group members' Theory of Mind.\n\nAmong the most controversial issues related to the study of intelligence is the observation that intelligence measures such as IQ scores vary between ethnic and racial groups and sexes. While there is little scholarly debate about the \"existence\" of some of these differences, their \"causes\" remain highly controversial both within academia and in the public sphere.\n\nMost IQ tests are constructed so that there are no overall score differences between females and males. Popular IQ batteries such as the WAIS and the WISC-R are also constructed in order to eliminate sex differences. In a paper presented at the International Society for Intelligence Research in 2002, it was pointed out that because test constructors and the United States' Educational Testing Service (which developed the US SAT test) often eliminate items showing marked sex differences in order to reduce the perception of bias, the \"true sex\" difference is masked. Items like the Mental Rotations Test and reaction time tests, which show a male advantage in IQ, are often removed. Meta-analysis focusing on gender differences in math performance found nearly identical performance for boys and girls, and the subject of mathematical intelligence and gender has been controversial.\n\nThe 1996 Task Force investigation on Intelligence sponsored by the American Psychological Association concluded that there are significant variations in IQ across races. The problem of determining the causes underlying this variation relates to the question of the contributions of \"nature and nurture\" to IQ. Psychologists such as Alan S. Kaufman and Nathan Brody and statisticians such as Bernie Devlin argue that there are insufficient data to conclude that this is because of genetic influences. A review article published in 2012 by leading scholars on human intelligence concluded, after reviewing the prior research literature, that group differences in IQ are best understood as environmental in origin.\n\nIn considering disparities between test results of different ethnic groups, one might investigate the effects of stereotype threat (a situational predicament in which a person feels at risk of confirming negative stereotypes about the group(s) he identifies with), as well as culture and acculturation. This phenomenon has been criticized as a fiction of publication bias.\n\nIn the United States, certain public policies and laws regarding military service, education, public benefits, capital punishment, and employment incorporate an individual's IQ into their decisions. However, in the case of Griggs v. Duke Power Co. in 1971, for the purpose of minimizing employment practices that disparately impacted racial minorities, the U.S. Supreme Court banned the use of IQ tests in employment, except when linked to job performance via a job analysis. Internationally, certain public policies, such as improving nutrition and prohibiting neurotoxins, have as one of their goals raising, or preventing a decline in, intelligence.\n\nA diagnosis of intellectual disability is in part based on the results of IQ testing. Borderline intellectual functioning is a categorization where a person has below average cognitive ability (an IQ of 71–85), but the deficit is not as severe as intellectual disability (70 or below).\n\nIn the United Kingdom, the eleven plus exam which incorporated an intelligence test has been used from 1945 to decide, at eleven years of age, which type of school a child should go to. They have been much less used since the widespread introduction of comprehensive schools.\n\nIQ is the most thoroughly researched means of measuring intelligence, and by far the most widely used in practical settings. However, while IQ strives to measure some concepts of intelligence, it may fail to serve as an accurate measure of broader definitions of intelligence. IQ tests examine some areas of intelligence while neglecting others such as creativity and social intelligence.\n\nCritics such as Keith Stanovich do not dispute the reliability of IQ test scores or their capacity to predict some kinds of achievement, but argue that basing a concept of intelligence on IQ test scores alone neglects other important aspects of mental ability.\n\nSome scientists dispute the worthiness of IQ entirely. In \"The Mismeasure of Man\" (1996), paleontologist Stephen Jay Gould criticized IQ tests and argued that they were used for scientific racism. He argued that \"g\" was a mathematical artifact and criticized:\n\nArthur Jensen responded:\n\nJensen also argued that even if \"g\" were replaced by a model with several intelligences this would change the situation less than expected. He argues that all tests of cognitive ability would continue to be highly correlated with one another and there would still be a black-white gap on cognitive tests. Hans Eysenck responded to Gould by stating that no psychologist had said that intelligence was an area located in the brain. Eysenck also argued IQ tests were not racist, pointing out that Northeast Asians and Jews both scored higher than non-Jewish Europeans on IQ tests, and this would not please European racists.\n\nPsychologist Peter Schönemann persistently criticized IQ, calling it \"the IQ myth\". He argued that \"g\" is a flawed theory and that the high heritability estimates of IQ are based on false assumptions. Robert Sternberg, another significant critic of \"g\" as the main measure of human cognitive abilities, argued that reducing the concept of intelligence to the measure of \"g\" does not fully account for the different skills and knowledge types that produce success in human society.\n\nCecil Reynolds and Paul Kline argue that the construction of IQ tests around the rule that they should show a bell curve distribution in the population leads to systematic exclusion of cognitive tests that display threshold effects and are not gradually variable, as well as biasing IQ tests towards questions that can be made to fit a bell curve model and against questions that show any non-bell distributions. They argue that just as swarms change their collective behavior at certain thresholds of animals in the swarm, it is possible that brains change their abilities at thresholds of number of connected neurons and/or level of connectivity. Cecil Reynolds and Paul Kline argue that such a bias may be the reason why IQ tests yield paradoxes such as the heredity paradox between high heredity showed by twin studies and the high environmental effect shown by the Flynn effect and suggest that other cognitive tests that do not conform to bell curve distributions should be tried with the possibility that some of them may produce falsifiable predictions of key abilities requiring a critical level of underlying quantitative brain access and simpler proxies of such in every case, unlike IQ tests which are argued to fail the falsifiability criterion by defining away problems by systematically demanding bell curves and failing to make any absolute system requirement predictions of what brain capacity is required to make a certain performance. It is argued that probabilistic predictions with loopholes for \"exceptions\" are not scientifically applicable to capacity theories, as capacities follow minimum system requirements for performing tasks.\n\nThe American Psychological Association's report \"Intelligence: Knowns and Unknowns\" stated that in the United States IQ tests as predictors of social achievement are not biased against African Americans since they predict future performance, such as school achievement, similarly to the way they predict future performance for Caucasians. While agreeing that IQ tests predict performance equally well for all racial groups, Nicholas Mackintosh also points out that there may still be a bias inherent in IQ testing if the education system is also systematically biased against African Americans, in which case educational performance may in fact also be an underestimation of African American children's cognitive abilities. Earl Hunt points out that while this may be the case that would not be a bias of the test, but of society.\n\nHowever, IQ tests may well be biased when used in other situations. A 2005 study stated that \"differential validity in prediction suggests that the WAIS-R test may contain cultural influences that reduce the validity of the WAIS-R as a measure of cognitive ability for Mexican American students,\" indicating a weaker positive correlation relative to sampled white students. Other recent studies have questioned the culture-fairness of IQ tests when used in South Africa. Standard intelligence tests, such as the Stanford-Binet, are often inappropriate for autistic children; the alternative of using developmental or adaptive skills measures are relatively poor measures of intelligence in autistic children, and may have resulted in incorrect claims that a majority of autistic children are mentally retarded.\n\nBarbara P. Uzzell and Harvey N. Switzky argue that defining IQ from an average at a period of time and putting different thresholds of what is considered retarded based on average IQ test performance in a culture faces issues of defining who belongs to what culture. They argue that since people are not actually in boxes of isolated cultures, essentialistic classification of culture that is said to make IQ tests \"culturally fair\" will show false \"evidence\" of underlying cognitive differences between individuals who are formally classified in the same culture but have faced different cultural non-shared environments due to the fluid intermingling of cultures in real life, making IQ tests a false measure of mental retardation. It is argued by Barbara P. Uzzell and Harvey N. Switzky that different social treatment depending on hereditary factors in appearance that may not necessarily be classified as \"racial\" but can often be considered individual leads to an appearance of genes for such appearances being linked to learned behaviors and false evidence of the genes affecting the brain's response to the environment, and that environmental differences in whether other people with whom a person speaks are willing to counter misunderstandings with factual arguments or dismisses the person as \"incapable of reasoning if he or she does not already know it\" creates differences in the opportunities to learn rules that can be used for solving problems in IQ tests and that widespread persistence of such cultural treatment gives a false appearance of IQ tests being reliable and valid measures of underlying cognitive abilities. It is argued that such effects of different opportunities to sharpen arguments through debate in all walks of life, including prejudice towards choice of words shaped by earlier differences in debate opportunities, make different predictions than lab setting only stereotype threat theory and must therefore be tested by different evidence than that theory, and that discrimination in society based on choice of words is a possible explanation of apparent links between IQ test performance and success in society.\n\nAccording to a 2006 article, contemporary psychological research often did not reflect substantial recent developments in psychometrics and \"bears an uncanny resemblance to the psychometric state of the art as it existed in the 1950s.\"\n\nIn response to the controversy surrounding \"The Bell Curve\", the American Psychological Association's Board of Scientific Affairs established a task force in 1995 to write a report on the state of intelligence research which could be used by all sides as a basis for discussion, \"\". The full text of the report is available through several websites.\n\nIn this paper, the representatives of the association regret that IQ-related works are frequently written with a view to their political consequences: \"research findings were often assessed not so much on their merits or their scientific standing as on their supposed political implications\".\n\nThe task force concluded that IQ scores do have high predictive validity for individual differences in school achievement. They confirm the predictive validity of IQ for adult occupational status, even when variables such as education and family background have been statistically controlled. They stated that individual differences in intelligence are substantially influenced by both genetics and environment.\n\nThe report stated that a number of biological factors, including malnutrition, exposure to toxic substances, and various prenatal and perinatal stressors, result in lowered psychometric intelligence under at least some conditions. The task force agrees that large differences do exist between the average IQ scores of blacks and whites, saying:\n\nThe APA journal that published the statement, \"American Psychologist,\" subsequently published eleven critical responses in January 1997, several of them arguing that the report failed to examine adequately the evidence for partly genetic explanations.\n\nAn alternative to standard IQ tests originated in the writings of psychologist Lev Vygotsky (1896–1934) from his last two years of work. The notion of the zone of proximal development that he introduced in 1933, roughly a year before his death, served as the banner for his proposal to diagnose development as the level of actual development that can be measured by the child's independent problem solving and, at the same time, the level of proximal, or potential development that is measured in the situation of moderately assisted problem solving by the child. The maximum level of complexity and difficulty of the problem that the child is capable to solve under some guidance indicates the level of potential development. Then, the difference between the higher level of potential and the lower level of actual development indicates the zone of proximal development. Combination of the two indexesthe level of actual and the zone of the proximal developmentaccording to Vygotsky, provides a significantly more informative indicator of psychological development than the assessment of the level of actual development alone.\n\nThe ideas on the zone of development were later developed in a number of psychological and educational theories and practices. Most notably, they were developed under the banner of dynamic assessment that focuses on the testing of learning and developmental potential (for instance, in the work of Reuven Feuerstein and his associates, who has criticized standard IQ testing for its putative assumption or acceptance of \"fixed and immutable\" characteristics of intelligence or cognitive functioning). Grounded in developmental theories of Vygotsky and Feuerstein, who maintained that human beings are not static entities but are always in states of transition and transactional relationships with the world, dynamic assessment received also considerable support in the recent revisions of cognitive developmental theory by Joseph Campione, Ann Brown, and John D. Bransford and in theories of multiple intelligences by Howard Gardner and Robert Sternberg. Still, dynamic assessment has not been implemented in education on a large scale as is up to now, by admission of one of its notable proponents, \"in search of its identity\".\n\nIQ classification is the practice used by IQ test publishers for designating IQ score ranges into various categories with labels such as \"superior\" or \"average.\" IQ classification was preceded historically by attempts to classify human beings by general ability based on other forms of behavioral observation. Those other forms of behavioral observation are still important for validating classifications based on IQ tests.\n\nThere are social organizations, some international, which limit membership to people who have scores as high as or higher than the 98th percentile (2 standard deviations above the mean) on some IQ test or equivalent. Mensa International is perhaps the best known of these. The largest 99.9th percentile (3 standard deviations above the mean) society is the Triple Nine Society.\n\n\n\n\n"}
{"id": "14894", "url": "https://en.wikipedia.org/wiki?curid=14894", "title": "Indian Institute of Technology Kanpur", "text": "Indian Institute of Technology Kanpur\n\nIndian Institute of Technology Kanpur (also known as IIT Kanpur or IITK) is a public engineering institution located in Kanpur, Uttar Pradesh. It was declared to be an Institute of National Importance by the Government of India under the Institutes of Technology Act.\n\nEstablished in 1960 as one of the first Indian Institutes of Technology, the institute was created with the assistance of a consortium of nine US research universities as part of the Kanpur Indo-American Programme (KIAP).\n\nIIT Kanpur was established by an Act of Parliament in 1959. The institute was started in December 1959 in a room in the canteen building of the Harcourt Butler Technological Institute at Agricultural Gardens in Kanpur. In 1963, the institute moved to its present location, on the Grand Trunk Road near the locality of Kalyanpur in Kanpur district.\n\nDuring the first ten years of its existence, a consortium of nine US universities (namely M.I.T, University of California, Berkeley, California Institute of Technology, Princeton University, Carnegie Institute of Technology, University of Michigan, Ohio State University,\nCase Institute of Technology and Purdue University) helped set up IIT Kanpur's research laboratories and academic programmes under the Kanpur Indo-American Programme (KIAP). The first Director of the Institute was P. K. Kelkar (after whom the Central Library was renamed in 2002).\n\nUnder the guidance of economist John Kenneth Galbraith, IIT Kanpur was the first institute in India to offer Computer science education. The earliest computer courses were started at IIT Kanpur in August 1963 on an IBM 1620 system. The initiative for computer education came from the Electrical engineering department, then under the chairmanship of Prof. H.K. Kesavan, who was concurrently the chairman of Electrical Engineering and head of the Computer Centre. Prof. Harry Huskey of the University of California, Berkeley, who preceded Kesavan, helped with the computer activity at IIT-Kanpur. In 1971, the institute began an independent academic program in Computer Science and Engineering, leading to M.Tech. and Ph.D. degrees.\n\nIn 1972 the KIAP program ended, in part because of tensions due to the U.S. support of Pakistan. Government funding was also reduced as a reaction to the sentiment that the IIT's were contributing to the brain drain.\n\nIIT Kanpur is located on the Grand Trunk Road, west of Kanpur City and measures close to . This land was donated by the Government of Uttar Pradesh in 1960 and by March 1963 the Institute had moved to its current location.\n\nThe institute has around 6478 students with 3938 undergraduate students and 2540 postgraduate students and about 500 research associates.\n\nIIT Kanpur is to open an extension centre in Noida with the plan of making a small convention centre there for supporting outreach activities. Its foundation was laid on 4 December 2012 on 5 acres of land allocated by Uttar Pradesh state government in the sector-62 of Noida city, which is less than an hour`s journey from New Delhi and the Indira Gandhi International Airport. The cost of construction is estimated to be about 25 crores. The new campus will have an auditorium, seminar halls for organising national and international conferences and an International Relations Office along with a 7-storey guest house. Several short-term management courses and refresher courses meant for distance learning will be available at the extension center.News from. IITK. Retrieved on 9 October 2013.\n\nBeing a major industrial town, Kanpur has a good connectivity by rail and by road but it lags in terms of air connectivity. IIT Kanpur was suffering significantly in comparison to IIT Delhi and IIT Bombay due to this reason as far as visiting companies and other dignitaries are concerned\nOn 1 June 2013, a helicopter ferry service was started at IIT Kanpur run by Pawan Hans Helicopters Limited. In its initial run the service connects IIT Kanpur to Lucknow, but it is planned to later extend it to New Delhi. Currently there are two flights daily to and from Lucknow Airport with a duration of 25 minutes. Lucknow Airport operates both international and domestic flights to major cities. IIT Kanpur is the first academic institution in the country to provide such a service.\n\nThe Institute has set up an office in New York with alumnus, Sanjiv Khosla designated as the overseas brand ambassador of the institute. It is located on 62, William Street, Manhattan. The office aims to hunt for qualified and capable faculty abroad, facilitate internship opportunities in North American universities and be conduit for research tie ups with various US universities. The New York Office also tries to amass funds through the alumni based there. A system that invites students and faculty of foreign institutes to IIT Kanpur is also being formulated.\n\nUndergraduate admissions until 2012 were being done through the national-level Indian Institute of Technology Joint Entrance Examination (IIT-JEE). Following the Ministry of Human Resource Development's decision to replace IIT-JEE with a common engineering entrance examination, IIT Kanpur's admissions are now based on JEE (Joint Entrance Examination) -Advanced level along with other IITs.\n\nPostgraduate admissions are made through the Graduate Aptitude Test in Engineering and Common Aptitude Test.\n\n\nThe Students' Gymkhana is the students' government organization of IIT Kanpur, established in 1962.\n\nThe Students' Gymkhana functions mainly through the Students' Senate, an elected student representative body composed of Senators elected from each batch and the five elected executives:\n\nThe number of Senators in the Students' Senate is around 50-55. A senator is elected for every 100 students of IIT Kanpur.\n\nThe meetings of the Students' Senate are chaired by the Convener, Students' Senate, who is elected by the Senate.\nThe Senate lays down the guidelines for the functions of the executives, their associated councils, the Gymkhana Festivals and other matters pertaining to the Student body at large.\n\nThe Students' Senate has a say in the policy and decision making bodies of the institute. The President, Students' Gymkhana and the Convener, Students' Senate are special Invitees to the Institute Academic Senate. The President is usually invited to the meetings of the Board of Governors when matters affecting students are being discussed. Nominees of the Students' Senate are also members of the various standing Committees of the Institute Senate including the disciplinary committee, the Undergraduate and Postgraduate committee, the scholarship committee etc. All academic departments have Departmental Undergraduate and Post Graduate Committees consisting of members of the faculty and nominees of the Students' Gymkhana.\n\nInternationally, IIT Kanpur was ranked 283 in \"QS World University Rankings\" for 2019. It was ranked 59 in QS Asia Rankings 2018 and 21 among BRICS nations. The \"Times Higher Education World University Rankings\" ranked it 501-600 globally in the 2018 ranking 81 in Asia and 32 among BRICS & Emerging Economies University Rankings 2017.\n\nIn India, among engineering colleges, it ranked third by \"India Today\" in 2017, fourth in India by \"Outlook India\" and fifth by \"The Week\". It was ranked fifth among engineering colleges in India by the \"National Institutional Ranking Framework\" (NIRF) in 2018, and seventh overall.\n\nThe Department of Industrial and Management Engineering was ranked 17 among management schools in India by NIRF in 2018.\n\nIIT Kanpur offers various courses on management and social sciences.\n\nIIT Kanpur offers four-year B.Tech programs in Aerospace Engineering, Biological Sciences and Bio-engineering, Chemical Engineering, Civil Engineering, Computer Science and Engineering, Electrical Engineering, Materials Science and Engineering and Mechanical Engineering. The admission to these programs is procured through Joint Entrance Examination. IITK offers admission only to bachelor's degree now (discontinuing the integrated course programs), but it can be extended by 1 year to make it integrated, depending on the choice of student and based on his/her performance there at undergraduate level. IIT Kanpur also offers four-year B.S. Programs in Pure and Applied Sciences (Mathematics, Physics and Chemistry in particular), Earth Science and Economics.\n\nFrom 2011, IIT Kanpur has started offering a four-year BS program in sciences and has kept its B.Tech Program intact. Entry to the five-year M.Tech/MBA programs and Dual degree programme will be done based on the CPI of students instead of JEE rank. In order to reduce the number of student exams, IIT Kanpur has also abolished the earlier system of conducting two mid-term examinations. Instead, only two examinations (plus two quizzes in most courses depending on the instructor-in-charge, one before mid-semesters and the other after the mid-semesters and before the end-semesters examination), one between the semester and other towards the end of it would be held from the academic session starting July 2011 onward as per Academic Review Committee's recommendations.\n\nPostgraduate courses in Engineering offer Master of Technology (M.Tech), MS (R) and Ph.D. degrees. The institute also offers two tier M.Sc. courses in areas of basic sciences in which students are admitted through Joint Admission Test for M.Sc.(JAM) exam. The institute also offers M.Des. (2 years), M.B.A. (2 years) and M.Sc. (2 years) degrees. Admissions to M. Tech is made once a year through Graduate Aptitude Test in Engineering. Admissions to M. Des are made once a year through both Graduate Aptitude Test in Engineering(GATE) and Common Entrance Exam for Design(CEED). Until 2011, admissions to the M.B.A. program were accomplished through the Joint Management Entrance Test (JMET), held yearly, and followed by a Group Discussion/Personal Interview process. In 2011, JMET was replaced by Common Admission Test (CAT).\n\nThe academic departments at IIT Kanpur are:\nThe campus is spread over an area of . Facilities include the National Wind Tunnel Facility. Other large research centres include the Advanced Centre for Material Science, a Bio-technology centre, the Advanced Centre for Electronic Systems, and the Samtel Centre for Display Technology, Centre for Mechatronics, Centre for Laser Technology, Prabhu Goel Research Centre for Computer and Internet Security, Facility for Ecological and Analytical Testing. The departments have their own libraries.\n\nThe institute has its own airfield, for flight testing and gliding.\n\nPK Kelkar Library (formerly Central Library) is an academic library of the institute with a collection of more than 300,000 volumes, and subscriptions to more than 1,000 periodicals. The library was renamed to its present name in 2003 after Dr. P K Kelkar, the first director of the institute. It is housed in a three-story building, with a total floor area of 6973 square metres. The Abstracting and Indexing periodicals, Microform and CD-ROM databases, technical reports, Standards and thesis are in the library. Each year, about 4,500 books and journal volumes are added to the library.\n\nThe New Core Labs (NCL) is 3 storey building with state of the art physics and chemistry laboratories for courses in the first year. The New Core Labs also has Linux and Windows computer labs for the use of first year courses and a Mathematics department laboratory housing machines with high computing power.\n\nIIT Kanpur has set up the SIDBI Innovation and Incubation Centre(SIIC) in collaboration with the Small Industries development Bank of India (SIDBI) aiming to aid innovation, research, and entrepreneurial activities in technology-based areas. SIIC helps business Start-ups to develop their ideas into commercially viable products.\n\nA team of students, working under the guidance of faculty members of the institute and scientists of Indian Space Research Organisation (ISRO) have designed and built India's first nano satellite Jugnu, which was successfully launched in orbit on 12 Oct 2011 by ISRO's PSLV-C18.\n\nThe Computer Centre is one of the advanced computing service centre among academic institution in India. IT hosts IIT Kanpur website and provides personal web space for students and faculties. It also provides a spam filtered email server and high speed fibre optic Internet to all the hostels and the academics. Users have multiple options to choose among various interfaces to access mail service. It has Linux and windows laboratories equipped with dozens of high-end software like MATLAB, Autocad, Ansys, Abaqus etc. for use of students. Apart from departmental computer labs, computer centre hosts more than 300 Linux terminals and more than 100 Windows terminals and is continuously available to the students for academic work and recreation. Computer centre has recently adopted an open source software policy for its infrastructure and computing. Various high-end compute and GPU servers are remotely available from data centre for user computation.\n\nComputer centre has multiple super computing clusters for research and teaching activity. In June 2014 IIT Kanpur launched their 2nd supercomputer which is India’s 5th most powerful supercomputer as of now. The new supercomputer 'Cluster Platform SL230s Gen8' manufactured by Hewlett-Packard has 15,360 cores and a theoretical peak (Rpeak) 307.2 TFlop/s and is the world's 192th most powerful supercomputer as of June 2015.\n\nResearch is controlled by the Office of the Dean of Research and Development. Under the aegis of the Office the students publish the quarterly NERD Magazine (Notes on Engineering Research and Development) which publishes scientific and technical content created by students. Articles may be original work done by students in the form of hobby projects, term projects, internships, or theses. Articles of general interest which are informative but do not reflect original work are also accepted. The institute is part of the European Research and Education Collaboration with Asia (EURECA) programme since 2008.\n\nAlong with the magazine a student research organisation, PoWER (Promotion of Work Experience and Research) has been started. Under it several independent student groups are working on projects like the Lunar Rover for ISRO, alternate energy solutions under the Group for Environment and Energy Engineering, ICT solutions through a group Young Engineers, solution for diabetes, green community solutions through ideas like zero water and zero waste quality air approach. Through BRaIN (Biological Research and Innovation Network) students interested in solving biological problems get involved in research projects like genetically modifying fruit flies to study molecular systems and developing bio-sensors to detect alcohol levels. A budget of Rs 1.5 to 2 crore has been envisaged to support student projects that demonstrate technology.\n\n\nThe students of IIT Kanpur made a nano satellite called Jugnu, which was given by president Pratibha Patil to ISRO for launch. Jugnu is a remote sensing satellite which will be operated by the Indian Institute of Technology Kanpur. It is a nanosatellite which will be used to provide data for agriculture and disaster monitoring. It is a 3-kilogram (6.6 lb) spacecraft, which measures 34 centimetres (13 in) in length by 10 centimetres (3.9 in) in height and width. Its development programme cost around 25 million rupees. It has a design life of one year.\nJugnu's primary instrument is the Micro Imaging System, a near infrared camera which will be used to observe vegetation. It also carries a GPS receiver to aid tracking, and is intended to demonstrate a microelectromechanical inertial measurement unit.\n\nIITK motorsports is the biggest and most comprehensive student initiative of the college, founded in January 2011. It is a group of students from varied disciplines who aim at designing and fabricating a Formula-style race car for international Formula SAE\n(Society of Automotive Engineers) events. Most of the components of the car, except the engine, tyres and wheel rims, are designed and manufactured by the team members themselves. The car is designed to provide maximum performance under the constraints of the event, while ensuring the driveability, reliability, driver safety and aesthetics of the car are not compromised.\n\nResearchers at IIT Kanpur have developed a series of solar powered UAVs named MARAAL-1 & MARAAL-2. Development of Maraal is notable as it is the first solar powered UAV developed in India. Maraal-2 is fully indigenous.\n\n\n\n"}
{"id": "14895", "url": "https://en.wikipedia.org/wiki?curid=14895", "title": "Insulin", "text": "Insulin\n\nInsulin (from Latin \"insula\", island) is a peptide hormone produced by beta cells of the pancreatic islets; it is considered to be the main anabolic hormone of the body. It regulates the metabolism of carbohydrates, fats and protein by promoting the absorption of carbohydrates, especially glucose from the blood into liver, fat and skeletal muscle cells. In these tissues the absorbed glucose is converted into either glycogen via glycogenesis or fats (triglycerides) via lipogenesis, or, in the case of the liver, into both. Glucose production and secretion by the liver is strongly inhibited by high concentrations of insulin in the blood. Circulating insulin also affects the synthesis of proteins in a wide variety of tissues. It is therefore an anabolic hormone, promoting the conversion of small molecules in the blood into large molecules inside the cells. Low insulin levels in the blood have the opposite effect by promoting widespread catabolism, especially of reserve body fat.\nBeta cells are sensitive to glucose concentrations, also known as blood sugar levels. When the glucose level is high, the beta cells secrete insulin into the blood; when glucose levels are low, secretion of insulin is inhibited. Their neighboring alpha cells, by taking their cues from the beta cells, secrete glucagon into the blood in the opposite manner: increased secretion when blood glucose is low, and decreased secretion when glucose concentrations are high. Glucagon, through stimulating the liver to release glucose by glycogenolysis and gluconeogenesis, has the opposite effect of insulin. The secretion of insulin and glucagon into the blood in response to the blood glucose concentration is the primary mechanism of glucose homeostasis.\nIf beta cells are destroyed by an autoimmune reaction, insulin can no longer be synthesized or be secreted into the blood. This results in type 1 diabetes mellitus, which is characterized by abnormally high blood glucose concentrations, and generalized body wasting. In type 2 diabetes mellitus the destruction of beta cells is less pronounced than in type 1 diabetes, and is not due to an autoimmune process. Instead there is an accumulation of amyloid in the pancreatic islets, which likely disrupts their anatomy and physiology. The pathogenesis of type 2 diabetes is not well understood but patients exhibit a reduced population of islet beta-cells, reduced secretory function of islet beta-cells that survive, and peripheral tissue insulin resistance. Type 2 diabetes is characterized by high rates of glucagon secretion into the blood which are unaffected by, and unresponsive to the concentration of glucose in the blood. Insulin is still secreted into the blood in response to the blood glucose. As a result, the insulin levels, even when the blood sugar level is normal, are much higher than they are in healthy persons.\nThe human insulin protein is composed of 51 amino acids, and has a molecular mass of 5808 Da. It is a dimer of an A-chain and a B-chain, which are linked together by disulfide bonds. Insulin's structure varies slightly between species of animals. Insulin from animal sources differs somewhat in effectiveness (in carbohydrate metabolism effects) from human insulin because of these variations. Porcine insulin is especially close to the human version, and was widely used to treat type 1 diabetics before human insulin could be produced in large quantities by recombinant DNA technologies.\nThe crystal structure of insulin in the solid state was determined by Dorothy Hodgkin. It is on the WHO Model List of Essential Medicines, the most important medications needed in a basic health system.\n\nInsulin may have originated more than a billion years ago. The molecular origins of insulin go at least as far back as the simplest unicellular eukaryotes. Apart from animals, insulin-like proteins are also known to exist in the Fungi and Protista kingdoms.\n\nInsulin is produced by beta cells of the pancreatic islets in most vertebrates and by the Brockmann body in some teleost fish. Cone snails \"Conus geographus\" and \"Conus tulipa\", venomous sea snails that hunt small fish, use modified forms of insulin in their venom cocktails. The insulin toxin, closer in structure to fishes' than to snails' native insulin, slows down the prey fishes by lowering their blood glucose levels.\n\nThe preproinsulin precursor of insulin is encoded by the \"INS\" gene.\n\nA variety of mutant alleles with changes in the coding region have been identified. A read-through gene, INS-IGF2, overlaps with this gene at the 5' region and with the IGF2 gene at the 3' region.\n\nIn the pancreatic β cells, glucose is the primary physiological stimulus for the regulation of insulin synthesis. Insulin is mainly regulated through the transcription factors PDX1, NeuroD1, and MafA.\n\nPDX1 (Pancreatic and duodenal homeobox protein 1) is in the nuclear periphery upon low blood glucose levels interacting with corepressors HDAC1 and 2 which is downregulating the insulin secretion. An increase in blood glucose levels causes phosphorylation of PDX1 and it translocates centrally and binds the A3 element within the insulin promoter. Upon translocation it interacts with coactivators HAT p300 and acetyltransferase set 7/9. PDX1 affects the histone modifications through acetylation and deacetylation as well as methylation. It is also said to suppress glucagon.\n\nNeuroD1, also known as β2, regulates insulin exocytosis in pancreatic β cells by directly inducing the expression of genes involved in exocytosis. It is localized in the cytosol, but in response to high glucose it becomes glycosylated by OGT and/or phosphorylated by ERK, which causes translocation to the nucleus. In the nucleus β2 heterodimerizes with E47, binds to the E1 element of the insulin promoter and recruits co-activator p300 which acetylates β2. It is able to interact with other transcription factors as well in activation of the insulin gene.\n\nMafA is degraded by proteasomes upon low blood glucose levels. Increased levels of glucose make an unknown protein glycosylated. This protein works as a transcription factor for MafA in an unknown manner and MafA is transported out of the cell. MafA is then translocated back into the nucleus where it binds the C1 element of the insulin promoter.\n\nThese transcription factors work synergistically and in a complex arrangement. Increased blood glucose can after a while destroy the binding capacities of these proteins, and therefore reduce the amount of insulin secreted, causing diabetes. The decreased binding activities can be mediated by glucose induced oxidative stress and antioxidants are said to prevent the decreased insulin secretion in glucotoxic pancreatic β cells. Stress signalling molecules and reactive oxygen species inhibits the insulin gene by interfering with the cofactors binding the transcription factors and the transcription factors it self.\n\nSeveral regulatory sequences in the promoter region of the human insulin gene bind to transcription factors. In general, the A-boxes bind to Pdx1 factors, E-boxes bind to NeuroD, C-boxes bind to MafA, and cAMP response elements to CREB. There are also silencers that inhibit transcription.\n\nWithin vertebrates, the amino acid sequence of insulin is strongly conserved. Bovine insulin differs from human in only three amino acid residues, and porcine insulin in one. Even insulin from some species of fish is similar enough to human to be clinically effective in humans. Insulin in some invertebrates is quite similar in sequence to human insulin, and has similar physiological effects. The strong homology seen in the insulin sequence of diverse species suggests that it has been conserved across much of animal evolutionary history. The C-peptide of proinsulin (discussed later), however, differs much more among species; it is also a hormone, but a secondary one.\nThe primary structure of bovine insulin was first determined by Frederick Sanger in 1951. After that, this polypeptide was synthesized independently by several groups. The 3-dimensional structure of insulin was determined by X-ray crystallography in Dorothy Hodgkin's laboratory in 1969 (PDB file 1ins).\n\nInsulin is produced and stored in the body as a hexamer (a unit of six insulin molecules), while the active form is the monomer. The hexamer is an inactive form with long-term stability, which serves as a way to keep the highly reactive insulin protected, yet readily available. The hexamer-monomer conversion is one of the central aspects of insulin formulations for injection. The hexamer is far more stable than the monomer, which is desirable for practical reasons; however, the monomer is a much faster-reacting drug because diffusion rate is inversely related to particle size. A fast-reacting drug means insulin injections do not have to precede mealtimes by hours, which in turn gives people with diabetes more flexibility in their daily schedules. Insulin can aggregate and form fibrillar interdigitated beta-sheets. This can cause injection amyloidosis, and prevents the storage of insulin for long periods.\n\nInsulin is produced in the pancreas and the Brockmann body (in some fish), and released when any of several stimuli are detected. These stimuli include ingested protein and glucose in the blood produced from digested food. Carbohydrates can be polymers of simple sugars or the simple sugars themselves. If the carbohydrates include glucose, then that glucose will be absorbed into the bloodstream and blood glucose level will begin to rise. In target cells, insulin initiates a signal transduction, which has the effect of increasing glucose uptake and storage. Finally, insulin is degraded, terminating the response.\n\nIn mammals, insulin is synthesized in the pancreas within the beta cells. One million to three million pancreatic islets form the endocrine part of the pancreas, which is primarily an exocrine gland. The endocrine portion accounts for only 2% of the total mass of the pancreas. Within the pancreatic islets, beta cells constitute 65–80% of all the cells.\n\nInsulin consists of two polypeptide chains, the A- and B- chains, linked together by disulfide bonds. It is however first synthesized as a single polypeptide called preproinsulin in beta cells. Preproinsulin contains a 24-residue signal peptide which directs the nascent polypeptide chain to the rough endoplasmic reticulum (RER). The signal peptide is cleaved as the polypeptide is translocated into lumen of the RER, forming proinsulin. In the RER the proinsulin folds into the correct conformation and 3 disulfide bonds are formed. About 5–10 min after its assembly in the endoplasmic reticulum, proinsulin is transported to the trans-Golgi network (TGN) where immature granules are formed. Transport to the TGN may take about 30 min.\n\nProinsulin undergoes maturation into active insulin through the action of cellular endopeptidases known as prohormone convertases (PC1 and PC2), as well as the exoprotease carboxypeptidase E. The endopeptidases cleave at 2 positions, releasing a fragment called the C-peptide, and leaving 2 peptide chains, the B- and A- chains, linked by 2 disulfide bonds. The cleavage sites are each located after a pair of basic residues (lysine-64 and arginine-65, and arginine-31 and −32). After cleavage of the C-peptide, these 2 pairs of basic residues are removed by the carboxypeptidase. The C-peptide is the central portion of proinsulin, and the primary sequence of proinsulin goes in the order \"B-C-A\" (the B and A chains were identified on the basis of mass and the C-peptide was discovered later).\n\nThe resulting mature insulin is packaged inside mature granules waiting for metabolic signals (such as leucine, arginine, glucose and mannose) and vagal nerve stimulation to be exocytosed from the cell into the circulation.\n\nThe endogenous production of insulin is regulated in several steps along the synthesis pathway:\n\nInsulin and its related proteins have been shown to be produced inside the brain, and reduced levels of these proteins are linked to Alzheimer's disease.\n\nInsulin release is stimulated also by beta-2 receptor stimulation and inhibited by alpha-1 receptor stimulation.  In addition, cortisol, glucagon and growth hormone antagonize the actions of insulin during times of stress.  Insulin also inhibits fatty acid release by hormone sensitive lipase in adipose tissue.\n\nBeta cells in the islets of Langerhans release insulin in two phases. The first-phase release is rapidly triggered in response to increased blood glucose levels, and lasts about 10 minutes. The second phase is a sustained, slow release of newly formed vesicles triggered independently of sugar, peaking in 2 to 3 hours. Reduced first-phase insulin release may be the earliest detectable beta cell defect predicting onset of type 2 diabetes. First-phase release and insulin sensitivity are independent predictors of diabetes.\n\nThe description of first phase release is as follows:\n\nThis is the primary mechanism for release of insulin. Other substances known to stimulate insulin release include the amino acids arginine and leucine, parasympathetic release of acetylcholine (acting via the phospholipase C pathway), sulfonylurea, cholecystokinin (CCK, also via phospholipase C), and the gastrointestinally derived incretins, such as glucagon-like peptide-1 (GLP-1) and glucose-dependent insulinotropic peptide (GIP).\n\nRelease of insulin is strongly inhibited by norepinephrine (noradrenaline), which leads to increased blood glucose levels during stress. It appears that release of catecholamines by the sympathetic nervous system has conflicting influences on insulin release by beta cells, because insulin release is inhibited by α-adrenergic receptors and stimulated by β-adrenergic receptors. The net effect of norepinephrine from sympathetic nerves and epinephrine from adrenal glands on insulin release is inhibition due to dominance of the α-adrenergic receptors.\n\nWhen the glucose level comes down to the usual physiologic value, insulin release from the β-cells slows or stops. If the blood glucose level drops lower than this, especially to dangerously low levels, release of hyperglycemic hormones (most prominently glucagon from islet of Langerhans alpha cells) forces release of glucose into the blood from the liver glycogen stores, supplemented by gluconeogenesis if the glycogen stores become depleted. By increasing blood glucose, the hyperglycemic hormones prevent or correct life-threatening hypoglycemia.\n\nEvidence of impaired first-phase insulin release can be seen in the glucose tolerance test, demonstrated by a substantially elevated blood glucose level at 30 minutes after the ingestion of a glucose load (75 or 100 g of glucose), followed by a slow drop over the next 100 minutes, to remain above 120 mg/100 ml after two hours after the start of the test. In a normal person the blood glucose level is corrected (and may even be slightly over-corrected) by the end of the test.\n\nEven during digestion, in general, one or two hours following a meal, insulin release from the pancreas is not continuous, but oscillates with a period of 3–6 minutes, changing from generating a blood insulin concentration more than about 800 p mol/l to less than 100 pmol/l. This is thought to avoid downregulation of insulin receptors in target cells, and to assist the liver in extracting insulin from the blood. This oscillation is important to consider when administering insulin-stimulating medication, since it is the oscillating blood concentration of insulin release, which should, ideally, be achieved, not a constant high concentration. This may be achieved by delivering insulin rhythmically to the portal vein or by islet cell transplantation to the liver.\n\nThe blood insulin level can be measured in international units, such as µIU/mL or in molar concentration, such as pmol/L, where 1 µIU/mL equals 6.945 pmol/L. A typical blood level between meals is 8–11 μIU/mL (57–79 pmol/L).\n\nThe effects of insulin are initiated by its binding to a receptor present in the cell membrane. The receptor molecule contains an α- and β subunits. Two molecules are joined to form what is known as a homodimer. Insulin binds to the α-subunits of the homodimer, which faces the extracellular side of the cells. The β subunits have tyrosine kinase enzyme activity which is triggered by the insulin binding. This activity provokes the autophosphorylation of the β subunits and subsequently the phosphorylation of proteins inside the cell known as insulin receptor substrates (IRS). The phosphorylation of the IRS activates a signal transduction cascade that leads to the activation of other kinases as well as transcription factors that mediate the intracellular effects of insulin.\n\nThe cascade that leads to the insertion of GLUT4 glucose transporters into the cell membranes of muscle and fat cells, and to the synthesis of glycogen in liver and muscle tissue, as well as the conversion of glucose into triglycerides in liver, adipose, and lactating mammary gland tissue, operates via the activation, by IRS-1, of phosphoinositol 3 kinase (PI3K). This enzyme converts a phospholipid in the cell membrane by the name of phosphatidylinositol 4,5-bisphosphate (PIP2), into phosphatidylinositol 3,4,5-triphosphate (PIP3), which, in turn, activates protein kinase B (PKB). Activated PKB facilitates the fusion of GLUT4 containing endosomes with the cell membrane, resulting in an increase in GLUT4 transporters in the plasma membrane. PKB also phosphorylates glycogen synthase kinase (GSK), thereby inactivating this enzyme. This means that its substrate, glycogen synthase (GS), cannot be phosphorylated, and remains dephosphorylated, and therefore active. The active enzyme, glycogen synthase (GS), catalyzes the rate limiting step in the synthesis of glycogen from glucose. Similar dephosphorylations affect the enzymes controlling the rate of glycolysis leading to the synthesis of fats via malonyl-CoA in the tissues that can generate triglycerides, and also the enzymes that control the rate of gluconeogenesis in the liver. The overall effect of these final enzyme dephosphorylations is that, in the tissues that can carry out these reactions, glycogen and fat synthesis from glucose are stimulated, and glucose production by the liver through glycogenolysis and gluconeogenesis are inhibited. The breakdown of triglycerides by adipose tissue into free fatty acids and glycerol is also inhibited.\n\nAfter the intracellular signal that resulted from the binding of insulin to its receptor has been produced, termination of signaling is then needed. As mentioned below in the section on degradation, endocytosis and degradation of the receptor bound to insulin is a main mechanism to end signaling. In addition, the signaling pathway is also terminated by dephosphorylation of the tyrosine residues in the various signaling pathways by tyrosine phosphatases. Serine/Threonine kinases are also known to reduce the activity of insulin.\n\nThe structure of the insulin–insulin receptor complex has been determined using the techniques of X-ray crystallography.\n\nThe actions of insulin on the global human metabolism level include:\n\nThe actions of insulin (indirect and direct) on cells include:\n\nInsulin also influences other body functions, such as vascular compliance and cognition. Once insulin enters the human brain, it enhances learning and memory and benefits verbal memory in particular. Enhancing brain insulin signaling by means of intranasal insulin administration also enhances the acute thermoregulatory and glucoregulatory response to food intake, suggesting that central nervous insulin contributes to the co-ordination of a wide variety of homeostatic or regulatory processes in the human body. Insulin also has stimulatory effects on gonadotropin-releasing hormone from the hypothalamus, thus favoring fertility.\n\nOnce an insulin molecule has docked onto the receptor and effected its action, it may be released back into the extracellular environment, or it may be degraded by the cell. The two primary sites for insulin clearance are the liver and the kidney. The liver clears most insulin during first-pass transit, whereas the kidney clears most of the insulin in systemic circulation. Degradation normally involves endocytosis of the insulin-receptor complex, followed by the action of insulin-degrading enzyme. An insulin molecule produced endogenously by the beta cells is estimated to be degraded within about one hour after its initial release into circulation (insulin half-life ~ 4–6 minutes).\n\nInsulin is a major regulator of endocannabinoid (EC) metabolism and insulin treatment has been shown to reduce intracellular ECs, the 2-arachidonylglycerol (2-AG) and anandamide (AEA), which correspond with insulin-sensitive expression changes in enzymes of EC metabolism. In insulin-resistant adipocytes, patterns of insulin-induced enzyme expression is disturbed in a manner consistent with elevated EC synthesis and reduced EC degradation. Findings suggest that insulin-resistant adipocytes fail to regulate EC metabolism and decrease intracellular EC levels in response to insulin stimulation, whereby obese insulin-resistant individuals exhibit increased concentrations of ECs. This dysregulation contributes to excessive visceral fat accumulation and reduced adiponectin release from abdominal adipose tissue, and further to the onset of several cardiometabolic risk factors that are associated with obesity and type 2 diabetes.\n\nHypoglycemia, also known as \"low blood sugar\", is when blood sugar decreases to below normal levels. This may result in a variety of symptoms including clumsiness, trouble talking, confusion, loss of consciousness, seizures or death. A feeling of hunger, sweating, shakiness and weakness may also be present. Symptoms typically come on quickly.\n\nThe most common cause of hypoglycemia is medications used to treat diabetes mellitus such as insulin and sulfonylureas. Risk is greater in diabetics who have eaten less than usual, exercised more than usual or have drunk alcohol. Other causes of hypoglycemia include kidney failure, certain tumors, such as insulinoma, liver disease, hypothyroidism, starvation, inborn error of metabolism, severe infections, reactive hypoglycemia and a number of drugs including alcohol. Low blood sugar may occur in otherwise healthy babies who have not eaten for a few hours.\n\nThere are several conditions in which insulin disturbance is pathologic:\n\nBiosynthetic human insulin (insulin human rDNA, INN) for clinical use is manufactured by recombinant DNA technology. Biosynthetic human insulin has increased purity when compared with extractive animal insulin, enhanced purity reducing antibody formation. Researchers have succeeded in introducing the gene for human insulin into plants as another method of producing insulin (\"biopharming\") in safflower. This technique is anticipated to reduce production costs.\n\nSeveral analogs of human insulin are available. These insulin analogs are closely related to the human insulin structure, and were developed for specific aspects of glycemic control in terms of fast action (prandial insulins) and long action (basal insulins). The first biosynthetic insulin analog was developed for clinical use at mealtime (prandial insulin), Humalog (insulin lispro), it is more rapidly absorbed after subcutaneous injection than regular insulin, with an effect 15 minutes after injection. Other rapid-acting analogues are NovoRapid and Apidra, with similar profiles. All are rapidly absorbed due to amino acid sequences that will reduce formation of dimers and hexamers (monomeric insulins are more rapidly absorbed). Fast acting insulins do not require the injection-to-meal interval previously recommended for human insulin and animal insulins. The other type is long acting insulin; the first of these was Lantus (insulin glargine). These have a steady effect for an extended period from 18 to 24 hours. Likewise, another protracted insulin analogue (Levemir) is based on a fatty acid acylation approach. A myristic acid molecule is attached to this analogue, which associates the insulin molecule to the abundant serum albumin, which in turn extends the effect and reduces the risk of hypoglycemia. Both protracted analogues need to be taken only once daily, and are used for type 1 diabetics as the basal insulin. A combination of a rapid acting and a protracted insulin is also available, making it more likely for patients to achieve an insulin profile that mimics that of the body´s own insulin release.\n\nInsulin is usually taken as subcutaneous injections by single-use syringes with needles, via an insulin pump, or by repeated-use insulin pens with disposable needles. Inhaled insulin is also available in the U.S. market now.\n\nSynthetic insulin can trigger adverse effects, so some people with diabetes rely on animal-source insulin.\n\nUnlike many medicines, insulin currently cannot be taken orally because, like nearly all other proteins introduced into the gastrointestinal tract, it is reduced to fragments, whereupon all activity is lost. There has been some research into ways to protect insulin from the digestive tract, so that it can be administered orally or sublingually.\n\nIn 1869, while studying the structure of the pancreas under a microscope, Paul Langerhans, a medical student in Berlin, identified some previously unnoticed tissue clumps scattered throughout the bulk of the pancreas. The function of the \"little heaps of cells\", later known as the \"islets of Langerhans\", initially remained unknown, but Édouard Laguesse later suggested they might produce secretions that play a regulatory role in digestion. Paul Langerhans' son, Archibald, also helped to understand this regulatory role. The term \"insulin\" originates from \"insula\", the Latin word for islet/island.\n\nIn 1889, the physician Oskar Minkowski, in collaboration with Joseph von Mering, removed the pancreas from a healthy dog to test its assumed role in digestion. On testing the urine, they found sugar, establishing for the first time a relationship between the pancreas and diabetes. In 1901, another major step was taken by the American physician and scientist Eugene Lindsay Opie, when he isolated the role of the pancreas to the islets of Langerhans: \"Diabetes mellitus when the result of a lesion of the pancreas is caused by destruction of the islands of Langerhans and occurs only when these bodies are in part or wholly destroyed\".\n\nOver the next two decades researchers made several attempts to isolate the islets' secretions. In 1906 George Ludwig Zuelzer achieved partial success in treating dogs with pancreatic extract, but he was unable to continue his work. Between 1911 and 1912, E.L. Scott at the University of Chicago tried aqueous pancreatic extracts and noted \"a slight diminution of glycosuria\", but was unable to convince his director of his work's value; it was shut down. Israel Kleiner demonstrated similar effects at Rockefeller University in 1915, but World War I interrupted his work and he did not return to it.\n\nIn 1916, Nicolae Paulescu developed an aqueous pancreatic extract which, when injected into a diabetic dog, had a normalizing effect on blood-sugar levels. He had to interrupt his experiments because of World War I, and in 1921 he wrote four papers about his work carried out in Bucharest and his tests on a diabetic dog. Later that year, he published \"Research on the Role of the Pancreas in Food Assimilation\".\n\nIn October 1920, Canadian Frederick Banting concluded that the digestive secretions that Minkowski had originally studied were breaking down the islet secretion, thereby making it impossible to extract successfully. A surgeon by training, Banting knew certain arteries could be tied off that would lead most of the pancreas to atrophy, while leaving the islets of Langerhans intact. He reasoned that a relatively pure extract could be made from the islets once most of the rest of the pancreas was gone. He jotted a note to himself: \"Ligate pancreatic ducts of the dog. Keep dogs alive till acini degenerate leaving islets. Try to isolate internal secretion of these and relieve glycosuria.\"\n\nIn the spring of 1921, Banting traveled to Toronto to explain his idea to J.J.R. Macleod, Professor of Physiology at the University of Toronto. Macleod was initially skeptical, since Banting had no background in research and was not familiar with the latest literature, but he agreed to provide lab space for Banting to test out his ideas. Macleod also arranged for two undergraduates to be Banting's lab assistants that summer, but Banting required only one lab assistant. Charles Best and Clark Noble flipped a coin; Best won the coin toss and took the first shift. This proved unfortunate for Noble, as Banting kept Best for the entire summer and eventually shared half his Nobel Prize money and credit for the discovery with Best. On 30 July 1921, Banting and Best successfully isolated an extract (\"isleton\") from the islets of a duct-tied dog and injected it into a diabetic dog, finding that the extract reduced its blood sugar by 40% in 1 hour.\n\nBanting and Best presented their results to Macleod on his return to Toronto in the fall of 1921, but Macleod pointed out flaws with the experimental design, and suggested the experiments be repeated with more dogs and better equipment. He moved Banting and Best into a better laboratory and began paying Banting a salary from his research grants. Several weeks later, the second round of experiments was also a success, and Macleod helped publish their results privately in Toronto that November. Bottlenecked by the time-consuming task of duct-tying dogs and waiting several weeks to extract insulin, Banting hit upon the idea of extracting insulin from the fetal calf pancreas, which had not yet developed digestive glands. By December, they had also succeeded in extracting insulin from the adult cow pancreas. Macleod discontinued all other research in his laboratory to concentrate on the purification of insulin. He invited biochemist James Collip to help with this task, and the team felt ready for a clinical test within a month.\n\nOn January 11, 1922, Leonard Thompson, a 14-year-old diabetic who lay dying at the Toronto General Hospital, was given the first injection of insulin. However, the extract was so impure that Thompson suffered a severe allergic reaction, and further injections were cancelled. Over the next 12 days, Collip worked day and night to improve the ox-pancreas extract. A second dose was injected on January 23, completely eliminating the glycosuria that was typical of diabetes without causing any obvious side-effects. The first American patient was Elizabeth Hughes, the daughter of U.S. Secretary of State Charles Evans Hughes. The first patient treated in the U.S. was future woodcut artist James D. Havens; Dr. John Ralston Williams imported insulin from Toronto to Rochester, New York, to treat Havens.\n\nBanting and Best never worked well with Collip, regarding him as something of an interloper, and Collip left the project soon after. Over the spring of 1922, Best managed to improve his techniques to the point where large quantities of insulin could be extracted on demand, but the preparation remained impure. The drug firm Eli Lilly and Company had offered assistance not long after the first publications in 1921, and they took Lilly up on the offer in April. In November, Lilly's head chemist, George B. Walden discovered isoelectric precipitation and was able to produce large quantities of highly refined insulin. Shortly thereafter, insulin was offered for sale to the general public.\n\nPurified animal-sourced insulin was initially the only type of insulin available to diabetics. The amino acid structure of insulin was characterized in the early 1950s by Frederick Sanger, and the first synthetic insulin was produced simultaneously in the labs of Panayotis Katsoyannis at the University of Pittsburgh and Helmut Zahn at RWTH Aachen University in the early 1960s. Synthetic crystalline bovine insulin was achieved by Chinese researchers in 1965.\n\nThe first genetically engineered, synthetic \"human\" insulin was produced using \"E. coli\" in 1978 by Arthur Riggs and Keiichi Itakura at the Beckman Research Institute of the City of Hope in collaboration with Herbert Boyer at Genentech. Genentech, founded by Swanson, Boyer and Eli Lilly and Company, went on in 1982 to sell the first commercially available biosynthetic human insulin under the brand name Humulin. The vast majority of insulin currently used worldwide is now biosynthetic recombinant \"human\" insulin or its analogues.\n\nRecombinant insulin is produced either in yeast (usually \"Saccharomyces cerevisiae\") or \"E. coli\". In yeast, insulin may be engineered as a single-chain protein with a KexII endoprotease (a yeast homolog of PCI/PCII) site that separates the insulin A chain from a c-terminally truncated insulin B chain. A chemically synthesized c-terminal tail is then grafted onto insulin by reverse proteolysis using the inexpensive protease trypsin; typically the lysine on the c-terminal tail is protected with a chemical protecting group to prevent proteolysis. The ease of modular synthesis and the relative safety of modifications in that region accounts for common insulin analogs with c-terminal modifications (e.g. lispro, aspart, glulisine). The Genentech synthesis and completely chemical synthesis such as that by Bruce Merrifield are not preferred because the efficiency of recombining the two insulin chains is low, primarily due to competition with the precipitation of insulin B chain.\n\nThe Nobel Prize committee in 1923 credited the practical extraction of insulin to a team at the University of Toronto and awarded the Nobel Prize to two men: Frederick Banting and J.J.R. Macleod. They were awarded the Nobel Prize in Physiology or Medicine in 1923 for the discovery of insulin. Banting, insisted that Best was not mentioned, shared his prize with him, and Macleod immediately shared his with James Collip. The patent for insulin was sold to the University of Toronto for one dollar.\n\nTwo other Nobel Prizes have been awarded for work on insulin. British molecular biologist Frederick Sanger determined the primary structure of insulin in 1955, making it the first protein to be sequenced. Sanger was awarded the 1958 Nobel Prize in Chemistry for this work. Rosalyn Sussman Yalow received the 1977 Nobel Prize in Medicine for the development of the radioimmunoassay for insulin.\n\nSeveral Nobel Prizes also have an indirect connection with insulin. George Minot, co-recipient of the 1934 Nobel Prize for the development of the first effective treatment for pernicious anemia, had diabetes mellitus. Dr. William Castle observed that the 1921 discovery of insulin, arriving in time to keep Minot alive, was therefore also responsible for the discovery of a cure for pernicious anemia. Dorothy Hodgkin was awarded a Nobel Prize in Chemistry in 1964 for the development of crystallography. In 1969, after decades of work, Hodgkin determined the spatial conformation of insulin, the so-called tertiary structure, by means of X-ray diffraction studies.\n\nThe work published by Banting, Best, Collip and Macleod represented the preparation of purified insulin extract suitable for use on human patients. Although Paulescu discovered the principles of the treatment, his saline extract could not be used on humans; he was not mentioned in the 1923 Nobel Prize. Professor Ian Murray was particularly active in working to correct \"the historical wrong\" against Nicolae Paulescu. Murray was a professor of physiology at the Anderson College of Medicine in Glasgow, Scotland, the head of the department of Metabolic Diseases at a leading Glasgow hospital, vice-president of the British Association of Diabetes, and a founding member of the International Diabetes Federation. Murray wrote:\n\nInsufficient recognition has been given to Paulescu, the distinguished Romanian scientist, who at the time when the Toronto team were commencing their research had already succeeded in extracting the antidiabetic hormone of the pancreas and proving its efficacy in reducing the hyperglycaemia in diabetic dogs.\nIn a private communication, Professor Arne Tiselius, former head of the Nobel Institute, expressed his personal opinion that Paulescu was equally worthy of the award in 1923.\n\n\n"}
{"id": "14896", "url": "https://en.wikipedia.org/wiki?curid=14896", "title": "Inductor", "text": "Inductor\n\nAn inductor, also called a coil, choke, or reactor, is a passive two-terminal electrical component that stores energy in a magnetic field when electric current flows through it. An inductor typically consists of an insulated wire wound into a coil around a core. \n\nWhen the current flowing through an inductor changes, the time-varying magnetic field induces an electromotive force (\"e.m.f.\") (voltage) in the conductor, described by Faraday's law of induction. According to Lenz's law, the induced voltage has a polarity (direction) which opposes the change in current that created it. As a result, inductors oppose any changes in current through them.\n\nAn inductor is characterized by its inductance, which is the ratio of the voltage to the rate of change of current. In the International System of Units (SI), the unit of inductance is the henry (H) named for 19th century American scientist Joseph Henry. In the measurement of magnetic circuits, it is equivalent to weber/ampere. Inductors have values that typically range from 1µH (10H) to 20H. Many inductors have a magnetic core made of iron or ferrite inside the coil, which serves to increase the magnetic field and thus the inductance. Along with capacitors and resistors, inductors are one of the three passive linear circuit elements that make up electronic circuits. Inductors are widely used in alternating current (AC) electronic equipment, particularly in radio equipment. They are used to block AC while allowing DC to pass; inductors designed for this purpose are called chokes. They are also used in electronic filters to separate signals of different frequencies, and in combination with capacitors to make tuned circuits, used to tune radio and TV receivers.\n\nAn electric current flowing through a conductor generates a magnetic field surrounding it. The magnetic flux linkage formula_1 generated by a given current formula_2 depends on the geometric shape of the circuit. Their ratio defines the inductance formula_3. Thus\n\nThe inductance of a circuit depends on the geometry of the current path as well as the magnetic permeability of nearby materials. An inductor is a component consisting of a wire or other conductor shaped to increase the magnetic flux through the circuit, usually in the shape of a coil or helix. Winding the wire into a coil increases the number of times the magnetic flux lines link the circuit, increasing the field and thus the inductance. The more turns, the higher the inductance. The inductance also depends on the shape of the coil, separation of the turns, and many other factors. By adding a \"magnetic core\" made of a ferromagnetic material like iron inside the coil, the magnetizing field from the coil will induce magnetization in the material, increasing the magnetic flux. The high permeability of a ferromagnetic core can increase the inductance of a coil by a factor of several thousand over what it would be without it.\n\nAny change in the current through an inductor creates a changing flux, inducing a voltage across the inductor. By Faraday's law of induction, the voltage induced by any change in magnetic flux through the circuit is given by \n\nReformulating the definition of formula_3 above, we obtain\nIt follows, that\n\nfor formula_3 independent of time. \n\nSo inductance is also a measure of the amount of electromotive force (voltage) generated for a given rate of change of current. For example, an inductor with an inductance of 1 henry produces an EMF of 1 volt when the current through the inductor changes at the rate of 1 ampere per second. This is usually taken to be the constitutive relation (defining equation) of the inductor.\n\nThe dual of the inductor is the capacitor, which stores energy in an electric field rather than a magnetic field. Its current–voltage relation is obtained by exchanging current and voltage in the inductor equations and replacing \"L\" with the capacitance \"C\".\n\nThe polarity (direction) of the induced voltage is given by Lenz's law, which states that the induced voltage will be such as to oppose the change in current. For example, if the current through an inductor is increasing, the induced voltage will be positive at the terminal through which the current enters and negative at the terminal through which it leaves, tending to oppose the additional current. The energy from the external circuit necessary to overcome this potential \"hill\" is being stored in the magnetic field of the inductor. If the current is decreasing, the induced voltage will be negative at the terminal through which the current enters and positive at the terminal through which it leaves, tending to maintain the current. In this case energy from the magnetic field is being returned to the circuit.\n\nOne intuitive explanation as to why a potential difference is induced on a change of current in an inductor goes as follows:\n\nWe know that the work done per unit charge on a charged particle when passing the inductor is formula_10. The negative sign indicates that the work is done \"against\" the emf, and is not done \"by\" the emf.\n\nBy knowing that formula_2 is the charge per unit time, it follows that the rate of energy formula_12 done against the emf is given by\nWe may proceed to state that\n\nIf the magnetic field in the inductor approaches the level at which the core saturates, the inductance will begin to change with current and thus we will henceforth denote the inductance formula_3 with formula_16 to accommodate for this dependency. Neglecting losses, the energy formula_12 stored by an inductor with a current formula_18 passing through it is equal to the amount of work required to establish the current through the inductor. This is given by:\n\nIf the inductance is constant over the current range formula_20 , the stored energy is \n\nFor inductors with magnetic cores, the above equation is only valid for linear regions of the magnetic flux, at currents below the saturation level of the inductor, where the inductance is approximately constant. Where this is not the case, the integral form must be used with formula_16 variable.\n\nIn circuit theory, inductors are idealized as obeying the mathematical relation (2) above precisely. An \"ideal inductor\" has inductance, but no resistance or capacitance, and does not dissipate energy. However real inductors have nonideal properties which cause their behavior to depart from this simple model. They have resistance (due to the resistance of the wire and energy losses in the core), and parasitic capacitance due to electric potential between the turns of wire. This capacitive reactance rises with frequency; at some frequency, the inductor will behave as a resonant circuit, becoming self-resonant. Above the self-resonant frequency the capacitive reactance is the dominant part of the impedance. At higher frequencies, resistive losses in the windings increase due to skin effect and proximity effect.\n\nInductors with ferromagnetic cores have additional energy losses due to hysteresis and eddy currents in the core, which increase with frequency. At high currents, magnetic core inductors also show sudden departure from ideal behavior due to nonlinearity caused by magnetic saturation of the core. An inductor radiates electromagnetic energy into surrounding space and circuits, and may absorb electromagnetic emissions from other circuits, causing electromagnetic interference (EMI). For real-world inductor applications, these parasitic parameters may be as important as the inductance.\n\nAn early solid-state electrical switching and amplifying device called a saturable reactor exploits saturation of the core as a means of stopping the inductive transfer of current via the core.\n\nThe winding resistance appears as a resistance in series with the inductor; it is referred to as DCR (DC resistance). This resistance dissipates some of the reactive energy. The quality factor (or \"Q\") of an inductor is the ratio of its inductive reactance to its resistance at a given frequency, and is a measure of its efficiency. The higher the Q factor of the inductor, the closer it approaches the behavior of an ideal inductor. High Q inductors are used with capacitors to make resonant circuits in radio transmitters and receivers. The higher the Q is, the narrower the bandwidth of the resonant circuit.\n\nThe Q factor of an inductor is defined as, where \"L\" is the inductance, \"R\" is the DCR, \"ω\" is the radian operating frequency, and the product \"ωL\" is the inductive reactance:\n\n\"Q\" increases linearly with frequency if \"L\" and \"R\" are constant. Although they are constant at low frequencies, the parameters vary with frequency. For example, skin effect, proximity effect, and core losses increase \"R\" with frequency; winding capacitance and variations in permeability with frequency affect \"L\".\n\nAt low frequencies and within limits, increasing the number of turns \"N\" improves \"Q\" because \"L\" varies as \"N\" while \"R\" varies linearly with \"N\". Similarly, increasing the radius \"r\" of an inductor improves \"Q\" because \"L\" varies as \"r\" while \"R\" varies linearly with \"r\". So high \"Q\" air core inductors often have large diameters and many turns. Both of those examples assume the diameter of the wire stays the same, so both examples use proportionally more wire. If the total mass of wire is held constant, then there would be no advantage to increasing the number of turns or the radius of the turns because the wire would have to be proportionally thinner.\n\nUsing a high permeability ferromagnetic core can greatly increase the inductance for the same amount of copper, so the core can also increase the Q. Cores however also introduce losses that increase with frequency. The core material is chosen for best results for the frequency band. High Q inductors must avoid saturation; one way is by using a (physically larger) air core inductor. At VHF or higher frequencies an air core is likely to be used. A well designed air core inductor may have a Q of several hundred.\n\nInductors are used extensively in analog circuits and signal processing. Applications range from the use of large inductors in power supplies, which in conjunction with filter capacitors remove ripple which is a multiple of the mains frequency (or the switching frequency for switched-mode power supplies) from the direct current output, to the small inductance of the ferrite bead or torus installed around a cable to prevent radio frequency interference from being transmitted down the wire. Inductors are used as the energy storage device in many switched-mode power supplies to produce DC current. The inductor supplies energy to the circuit to keep current flowing during the \"off\" switching periods and enables topographies where the output voltage is higher than the input voltage.\n\nA tuned circuit, consisting of an inductor connected to a capacitor, acts as a resonator for oscillating current. Tuned circuits are widely used in radio frequency equipment such as radio transmitters and receivers, as narrow bandpass filters to select a single frequency from a composite signal, and in electronic oscillators to generate sinusoidal signals.\n\nTwo (or more) inductors in proximity that have coupled magnetic flux (mutual inductance) form a transformer, which is a fundamental component of every electric utility power grid. The efficiency of a transformer may decrease as the frequency increases due to eddy currents in the core material and skin effect on the windings. The size of the core can be decreased at higher frequencies. For this reason, aircraft use 400 hertz alternating current rather than the usual 50 or 60 hertz, allowing a great saving in weight from the use of smaller transformers. Transformers enable switched-mode power supplies that isolate the output from the input.\n\nInductors are also employed in electrical transmission systems, where they are used to limit switching currents and fault currents. In this field, they are more commonly referred to as reactors.\n\nInductors have parasitic effects which cause them to depart from ideal behavior. They create and suffer from electromagnetic interference (EMI). Their physical size prevents them from being integrated on semiconductor chips. So the use of inductors is declining in modern electronic devices, particularly compact portable devices. Real inductors are increasingly being replaced by active circuits such as the gyrator which can synthesize inductance using capacitors.\nAn inductor usually consists of a coil of conducting material, typically insulated copper wire, wrapped around a core either of plastic (to create an air-core inductor) or of a ferromagnetic (or ferrimagnetic) material; the latter is called an \"iron core\" inductor. Since power inductors require high induction levels, high permeability and low saturation points in the core materials are not ideal. The high permeability of the ferromagnetic core increases the magnetic field and confines it closely to the inductor, thereby increasing the inductance. Low frequency inductors are constructed like transformers, with cores of electrical steel laminated to prevent eddy currents. 'Soft' ferrites are widely used for cores above audio frequencies, since they do not cause the large energy losses at high frequencies that ordinary iron alloys do. Inductors come in many shapes. Some inductors have an adjustable core, which enables changing of the inductance. Inductors used to block very high frequencies are sometimes made by stringing a ferrite bead on a wire.\n\nSmall inductors can be etched directly onto a printed circuit board by laying out the trace in a spiral pattern. Some such planar inductors use a planar core. Small value inductors can also be built on integrated circuits using the same processes that are used to make interconnnects. Aluminium interconnect is typically used, laid out in a spiral coil pattern. However, the small dimensions limit the inductance, and it is far more common to use a circuit called a \"gyrator\" that uses a capacitor and active components to behave similarly to an inductor. Regardless of the design, because of the low inductances and low power dissipation on-die inductors allow, they're currently only commercially used for high frequency RF circuits.\nInductors used in power regulation systems, lighting, and other systems that require low-noise operating conditions, are often partially or fully shielded. In telecommunication circuits employing induction coils and repeating transformers shielding of inductors in close proximity reduces circuit cross-talk.\n\nThe term \"air core coil\" describes an inductor that does not use a magnetic core made of a ferromagnetic material. The term refers to coils wound on plastic, ceramic, or other nonmagnetic forms, as well as those that have only air inside the windings. Air core coils have lower inductance than ferromagnetic core coils, but are often used at high frequencies because they are free from energy losses called core losses that occur in ferromagnetic cores, which increase with frequency. A side effect that can occur in air core coils in which the winding is not rigidly supported on a form is 'microphony': mechanical vibration of the windings can cause variations in the inductance.\n\nAt high frequencies, particularly radio frequencies (RF), inductors have higher resistance and other losses. In addition to causing power loss, in resonant circuits this can reduce the Q factor of the circuit, broadening the bandwidth. In RF inductors, which are mostly air core types, specialized construction techniques are used to minimize these losses. The losses are due to these effects:\n\nTo reduce parasitic capacitance and proximity effect, high Q RF coils are constructed to avoid having many turns lying close together, parallel to one another. The windings of RF coils are often limited to a single layer, and the turns are spaced apart. To reduce resistance due to skin effect, in high-power inductors such as those used in transmitters the windings are sometimes made of a metal strip or tubing which has a larger surface area, and the surface is silver-plated.\n\nSmall inductors for low current and low power are made in molded cases resembling resistors. These may be either plain (phenolic) core or ferrite core. An ohmmeter readily distinguishes them from similar-sized resistors by showing the low resistance of the inductor.\n\nFerromagnetic-core or iron-core inductors use a magnetic core made of a ferromagnetic or ferrimagnetic material such as iron or ferrite to increase the inductance. A magnetic core can increase the inductance of a coil by a factor of several thousand, by increasing the magnetic field due to its higher magnetic permeability. However the magnetic properties of the core material cause several side effects which alter the behavior of the inductor and require special construction:\n\nLow-frequency inductors are often made with laminated cores to prevent eddy currents, using construction similar to transformers. The core is made of stacks of thin steel sheets or laminations oriented parallel to the field, with an insulating coating on the surface. The insulation prevents eddy currents between the sheets, so any remaining currents must be within the cross sectional area of the individual laminations, reducing the area of the loop and thus reducing the energy losses greatly. The laminations are made of low-conductivity silicon steel to further reduce eddy current losses.\n\nFor higher frequencies, inductors are made with cores of ferrite. Ferrite is a ceramic ferrimagnetic material that is nonconductive, so eddy currents cannot flow within it. The formulation of ferrite is xxFeO where xx represents various metals. For inductor cores soft ferrites are used, which have low coercivity and thus low hysteresis losses.\n\nAnother material is powdered iron cemented with a binder.\n\nIn an inductor wound on a straight rod-shaped core, the magnetic field lines emerging from one end of the core must pass through the air to re-enter the core at the other end. This reduces the field, because much of the magnetic field path is in air rather than the higher permeability core material and is a source of electromagnetic interference. A higher magnetic field and inductance can be achieved by forming the core in a closed magnetic circuit. The magnetic field lines form closed loops within the core without leaving the core material. The shape often used is a toroidal or doughnut-shaped ferrite core. Because of their symmetry, toroidal cores allow a minimum of the magnetic flux to escape outside the core (called \"leakage flux\"), so they radiate less electromagnetic interference than other shapes. Toroidal core coils are manufactured of various materials, primarily ferrite, powdered iron and laminated cores.\n\nProbably the most common type of variable inductor today is one with a moveable ferrite magnetic core, which can be slid or screwed in or out of the coil. Moving the core farther into the coil increases the permeability, increasing the magnetic field and the inductance. Many inductors used in radio applications (usually less than 100 MHz) use adjustable cores in order to tune such inductors to their desired value, since manufacturing processes have certain tolerances (inaccuracy). Sometimes such cores for frequencies above 100 MHz are made from highly conductive non-magnetic material such as aluminum. They decrease the inductance because the magnetic field must bypass them.\n\nAir core inductors can use sliding contacts or multiple taps to increase or decrease the number of turns included in the circuit, to change the inductance. A type much used in the past but mostly obsolete today has a spring contact that can slide along the bare surface of the windings. The disadvantage of this type is that the contact usually short-circuits one or more turns. These turns act like a single-turn short-circuited transformer secondary winding; the large currents induced in them cause power losses.\n\nA type of continuously variable air core inductor is the \"variometer\". This consists of two coils with the same number of turns connected in series, one inside the other. The inner coil is mounted on a shaft so its axis can be turned with respect to the outer coil. When the two coils' axes are collinear, with the magnetic fields pointing in the same direction, the fields add and the inductance is maximum. When the inner coil is turned so its axis is at an angle with the outer, the mutual inductance between them is smaller so the total inductance is less. When the inner coil is turned 180° so the coils are collinear with their magnetic fields opposing, the two fields cancel each other and the inductance is very small. This type has the advantage that it is continuously variable over a wide range. It is used in antenna tuners and matching circuits to match low frequency transmitters to their antennas.\n\nAnother method to control the inductance without any moving parts requires an additional DC current bias winding which controls the permeability of an easily saturable core material. See Magnetic amplifier.\n\nA choke is an inductor designed specifically for blocking high-frequency alternating current (AC) in an electrical circuit, while allowing DC or low-frequency signals to pass. It usually consists of a coil of insulated wire wound on a magnetic core, although some consist of a donut-shaped \"bead\" of ferrite material strung on a wire. Like other inductors, chokes resist changes in current passing through them increasingly with frequency. The difference between chokes and other inductors is that chokes do not require the high Q factor construction techniques that are used to reduce the resistance in inductors used in tuned circuits.\n\nThe effect of an inductor in a circuit is to oppose changes in current through it by developing a voltage across it proportional to the rate of change of the current. An ideal inductor would offer no resistance to a constant direct current; however, only superconducting inductors have truly zero electrical resistance.\n\nThe relationship between the time-varying voltage \"v\"(\"t\") across an inductor with inductance \"L\" and the time-varying current \"i\"(\"t\") passing through it is described by the differential equation:\n\nWhen there is a sinusoidal alternating current (AC) through an inductor, a sinusoidal voltage is induced. The amplitude of the voltage is proportional to the product of the amplitude (\"I\") of the current and the frequency (\"f\") of the current.\n\nIn this situation, the phase of the current lags that of the voltage by π/2 (90°). For sinusoids, as the voltage across the inductor goes to its maximum value, the current goes to zero, and as the voltage across the inductor goes to zero, the current through it goes to its maximum value.\n\nIf an inductor is connected to a direct current source with value \"I\" via a resistance \"R\" (at least the DCR of the inductor), and then the current source is short-circuited, the differential relationship above shows that the current through the inductor will discharge with an exponential decay:\n\nThe ratio of the peak voltage to the peak current in an inductor energised from an AC source is called the reactance and is denoted \"X\".\n\nThus,\n\nReactance is measured in ohms but referred to as \"impedance\" rather than resistance; energy is stored in the magnetic field as current rises and discharged as current falls. Inductive reactance is proportional to frequency. At low frequency the reactance falls; at DC, the inductor behaves as a short-circuit. As frequency increases the reactance increases and at a sufficiently high frequency the reactance approaches that of an open circuit.\n\nIn filtering applications,with respect to a particular load impedance, an inductor has a corner frequency defined as:\n\nWhen using the Laplace transform in circuit analysis, the impedance of an ideal inductor with no initial current is represented in the \"s\" domain by:\n\nwhere\n\nIf the inductor does have initial current, it can be represented by: \n\nInductors in a parallel configuration each have the same potential difference (voltage). To find their total equivalent inductance (\"L\"):\n\nThe current through inductors in series stays the same, but the voltage across each inductor can be different. The sum of the potential differences (voltage) is equal to the total voltage. To find their total inductance:\n\nThese simple relationships hold true only when there is no mutual coupling of magnetic fields between individual inductors.\n\nMutual inductance occurs when the magnetic field of an inductor induces a magnetic field in an adjacent inductor. Mutual induction is the basis of transformer construction.\nM=(L1×L2)^(1/2)\nwhere M is the maximum mutual inductance possible between 2 inductors and L1 and L2 are the two inductors.\nIn general M<=(L1×L2)^(1/2) as only a fraction of self flux is linked with the other. \nThis fraction is called \"Coefficient of flux linkage\" or \"Coefficient of coupling\".\nK=M÷((L1×L2)^0.5)\n\nThe table below lists some common simplified formulas for calculating the approximate inductance of several inductor constructions.\n\n\n\n"}
{"id": "14899", "url": "https://en.wikipedia.org/wiki?curid=14899", "title": "Insulin pump", "text": "Insulin pump\n\nAn insulin pump is a medical device used for the administration of insulin in the treatment of diabetes mellitus, also known as continuous subcutaneous insulin therapy.\nThe device configuration may vary depending on design. A traditional pump includes:\n\nOther configurations are possible. More recent models may include disposable or semi-disposable designs for the pumping mechanism and may eliminate tubing from the infusion set.\n\nAn insulin pump is an alternative to multiple daily injections of insulin by insulin syringes or an insulin pen and allows for flexible insulin therapy when used in conjunction with blood glucose monitoring and carbohydrate counting.\n\nInsulin pumps are used to deliver insulin on a continuous basis to a person with type I diabetes. \n\n\nInsulin pumps, cartridges, and infusion sets may be far more expensive than syringes used for insulin injection with several insulin pumps costing more than $6,000; necessary supplies can cost over $300. Another disadvantage of insulin pump use is a higher risk of developing diabetic ketoacidosis if the pump malfunctions. This can happen if the pump battery is discharged, if the insulin is inactivated by heat exposure, if the insulin reservoir runs empty, the tubing becomes loose and insulin leaks rather than being injected, or if the cannula becomes bent or kinked in the body, preventing delivery. Therefore, pump users typically monitor their blood sugars more frequently to evaluate the effectiveness of insulin delivery.\n\n\nUse of insulin pumps is increasing because of:\n\nIn 1963 Dr. Arnold Kadish designed the first insulin pump to be worn as a backpack. A more wearable version was later devised by Dean Kamen in 1976. Kamen formed a company called \"AutoSyringe\" to market the product, which he sold to Baxter Health Care in 1981.\n\nThe insulin pump was first endorsed in the United Kingdom in 2003, by the National Institute for Health and Care Excellence (NICE).\n\nNew insulin pumps are becoming \"smart\" as new features are added to their design. These simplify the tasks involved in delivering an insulin bolus.\n\nMiniMed 670G is a type of insulin pump and sensor system created by Medtronic. It was approved by the US FDA in September 2016 and was the first approved hybrid closed loop system which senses a diabetic person's basal insulin requirement and automatically adjusts its delivery to the body.\n\n\nAn insulin pump allows the replacement of slow-acting insulin for basal needs with a continuous infusion of rapid-acting insulin.\n\nThe insulin pump delivers a single type of rapid-acting insulin in two ways:\n\nAn insulin pump user can influence the profile of the rapid-acting insulin by shaping the bolus. Users can experiment with bolus shapes to determine what is best for any given food, which means that they can improve control of blood sugar by adapting the bolus shape to their needs.\n\nA standard bolus is an infusion of insulin pumped completely at the onset of the bolus. It's the most similar to an injection. By pumping with a \"spike\" shape, the expected action is the fastest possible bolus for that type of insulin. The standard bolus is most appropriate when eating high carb low protein low fat meals because it will return blood sugar to normal levels quickly.\n\nAn extended bolus is a slow infusion of insulin spread out over time. By pumping with a \"square wave\" shape, the bolus avoids a high initial dose of insulin that may enter the blood and cause low blood sugar before digestion can facilitate sugar entering the blood. The extended bolus also extends the action of insulin well beyond that of the insulin alone. The extended bolus is appropriate when covering high fat high protein meals such as steak, which will be raising blood sugar for many hours past the onset of the bolus. The extended bolus is also useful for those with slow digestion (such as with gastroparesis or coeliac disease).\n\nA combination bolus/multiwave bolus is the combination of a standard bolus spike with an extended bolus square wave. This shape provides a large dose of insulin up front, and then also extends the tail of the insulin action. The combination bolus is appropriate for high carb high fat meals such as pizza, pasta with heavy cream sauce, and chocolate cake.\n\nA super bolus is a method of increasing the spike of the standard bolus. Since the action of the bolus insulin in the blood stream will extend for several hours, the basal insulin could be stopped or reduced during this time. This facilitates the \"borrowing\" of the basal insulin and including it into the bolus spike to deliver the same total insulin with faster action than can be achieved with spike and basal rate together. The super bolus is useful for certain foods (like sugary breakfast cereals) which cause a large post-prandial peak of blood sugar. It attacks the blood sugar peak with the fastest delivery of insulin that can be practically achieved by pumping.\n\nSince the pump user is responsible to manually start a bolus, this provides an opportunity for the user to pre-bolus to improve upon the insulin pump's capability to prevent post-prandial hyperglycemia. A pre-bolus is simply a bolus of insulin given before it is actually needed to cover carbohydrates eaten.\n\nThere are two situations where a pre-bolus is helpful:\n\nSimilarly, a low blood sugar level or a low glycemic food might be best treated with a bolus \"after\" a meal is begun. The blood sugar level, the type of food eaten, and a person's individual response to food and insulin affect the ideal time to bolus with the pump.\n\nThe pattern for delivering basal insulin throughout the day can also be customized with a pattern to suit the pump user.\n\nBasal insulin requirements will vary between individuals and periods of the day. The basal rate for a particular time period is determined by fasting while periodically evaluating the blood sugar level. Neither food nor bolus insulin must be taken for 4 hours before or during the evaluation period. If the blood sugar level changes dramatically during evaluation, then the basal rate can be adjusted to increase or decrease insulin delivery to keep the blood sugar level approximately steady.\n\nFor instance, to determine an individual's morning basal requirement, they must skip breakfast. On waking, they would test their blood glucose level periodically until lunch. Changes in blood glucose level are compensated with adjustments in the morning basal rate. The process is repeated over several days, varying the fasting period, until a 24-hour basal profile has been built up which keeps fasting blood sugar levels relatively steady. Once the basal rate is matched to the fasting basal insulin need, the pump user will then gain the flexibility to skip or postpone meals such as sleeping late on the weekends or working overtime on a weekday.\n\nMany factors can change insulin requirements and require an adjustment to the basal rate:\n\nA pump user should be educated by their diabetes care professional about basal rate determination before beginning pump therapy.\n\nSince the basal insulin is provided as a rapid-acting insulin, the basal insulin can be immediately increased or decreased as needed with a temporary basal rate. Examples when this is helpful include:\n\nIn August 2011, an IBM researcher, Jay Radcliffe, demonstrated a security flaw in insulin pumps. Radcliffe was able to hack the wireless interface used to control the pump remotely. Pump manufacturer Medtronic later said security research by McAfee uncovered a flaw in its pumps that could be exploited.\n\n\n"}
{"id": "14900", "url": "https://en.wikipedia.org/wiki?curid=14900", "title": "ISO 3166", "text": "ISO 3166\n\nISO 3166 is a standard published by the International Organization for Standardization (ISO) that defines codes for the names of countries, dependent territories, special areas of geographical interest, and their principal subdivisions (e.g., provinces or states). The official name of the standard is Codes for the representation of names of countries and their subdivisions.\n\nIt consists of three parts:\n\nThe first edition of ISO 3166 was published in 1974, which included only alphabetic country codes. The second edition, published in 1981, also included numeric country codes, with the third and fourth editions published in 1988 and 1993 respectively. The fifth edition, published between 1997 and 1999, was expanded into three parts to include codes for subdivisions and former countries.\n\nThe ISO 3166 standard is maintained by the ISO 3166 Maintenance Agency (ISO 3166/MA), located at the ISO central office in Geneva. Originally it was located at the Deutsches Institut für Normung (DIN) in Berlin. Its principal tasks are:\n\nThere are fifteen experts with voting rights on the ISO 3166/MA. Nine are representatives of national standards organizations:\n\nThe other six are representatives of major United Nations agencies or other international organizations who are all users of ISO 3166-1:\n\nThe ISO 3166/MA has further associated members who do not participate in the votes but who, through their expertise, have significant influence on the decision-taking procedure in the maintenance agency.\n\nPlease see the List of ISO 3166 country codes.\n\n"}
{"id": "14904", "url": "https://en.wikipedia.org/wiki?curid=14904", "title": "Intensive insulin therapy", "text": "Intensive insulin therapy\n\nIntensive insulin therapy or flexible insulin therapy is a therapeutic regimen for diabetes mellitus treatment. This newer approach contrasts with conventional insulin therapy. Rather than minimize the number of insulin injections per day (a technique which demands a rigid schedule for food and activities), the intensive approach favors flexible meal times with variable carbohydrate as well as flexible physical activities. The trade-off is the increase from 2 or 3 injections per day to 4 or more injections per day, which was considered \"intensive\" relative to the older approach. In North America in 2004, many endocrinologists prefer the term \"flexible insulin therapy\" (FIT) to \"intensive therapy\" and use it to refer to any method of replacing insulin that attempts to mimic the pattern of small continuous basal insulin secretion of a working pancreas combined with larger insulin secretions at mealtimes. The semantic distinction reflects changing treatment.\n\nLong-term studies like the UK Prospective Diabetes Study (\"UKPDS\") and the Diabetes control and complications trial (\"DCCT\") showed that intensive insulin therapy achieved blood glucose levels closer to non-diabetic people and that this was associated with reduced frequency and severity of blood vessel damage. Damage to large and small blood vessels (macro- and microvascular disease) is central to the development of complications of diabetes mellitus.\n\nThis evidence convinced most physicians who specialize in diabetes care that an important goal of treatment is to make the biochemical profile of the diabetic patient (blood lipids, HbA1c, etc.) as close to the values of non-diabetic people as possible. This is especially true for young patients with many decades of life ahead.\n\nA working pancreas continually secretes small amounts of insulin into the blood to maintain normal glucose levels, which would otherwise rise from glucose release by the liver, especially during the early morning dawn phenomenon. This insulin is referred to as \"basal insulin secretion\", and constitutes almost half the insulin produced by the normal pancreas.\n\nBolus insulin is produced during the digestion of meals. Insulin levels rise immediately as we begin to eat, remaining higher than the basal rate for 1 to 4 hours. This meal-associated (\"prandial\") insulin production is roughly proportional to the amount of carbohydrate in the meal.\n\nIntensive or flexible therapy involves supplying a continual supply of insulin to serve as the \"basal insulin\", supplying meal insulin in doses proportional to nutritional load of the meals, and supplying extra insulin when needed to correct high glucose levels. These three components of the insulin regimen are commonly referred to as basal insulin, bolus insulin, and high glucose correction insulin.\n\nOne method of intensive insulinotherapy is based on multiple daily injections (sometimes referred to in medical literature as \"MDI\"). Meal insulin is supplied by injection of rapid-acting insulin before each meal in an amount proportional to the meal. Basal insulin is provided as a once or twice daily injection of dose of a long-acting insulin.\n\nIn an MDI regimen, long-acting insulins are preferred for basal use. An older insulin used for this purpose is ultralente, and beef ultralente in particular was considered for decades to be the gold standard of basal insulin. Long-acting insulin analogs such as insulin glargine (brand name \"Lantus\", made by Sanofi-Aventis) and insulin detemir (brand name \"Levemir\", made by Novo Nordisk) are also used, with insulin glargine used more than insulin detemir. Rapid-acting insulin analogs such as lispro (brand name \"Humalog\", made by Eli Lilly and Company) and aspart (brand name \"Novolog\"/\"Novorapid\", made by Novo Nordisk and \"Apidra\" made by Sanofi Aventis) are preferred by many clinicians over older regular insulin for meal coverage and high correction. Many people on MDI regimens carry insulin pens to inject their rapid-acting insulins instead of traditional syringes. Some people on an MDI regimen also use injection ports such as the I-port to minimize the number of daily skin punctures.\n\nThe other method of intensive/flexible insulin therapy is an insulin pump. It is a small mechanical device about the size of a deck of cards. It contains a syringe-like reservoir with about three days' insulin supply. This is connected by thin, disposable, plastic tubing to a needle-like cannula inserted into the patient's skin and held in place by an adhesive patch. The infusion tubing and cannula must be removed and replaced every few days.\n\nAn insulin pump can be programmed to infuse a steady amount of rapid-acting insulin under the skin. This steady infusion is termed the basal rate and is designed to supply the background insulin needs. Each time the patient eats, he or she must press a button on the pump to deliver a specified dose of insulin to cover that meal. Extra insulin is also given the same way to correct a high glucose reading. Although current pumps can include a glucose sensor, they cannot automatically respond to meals or to rising or falling glucose levels.\n\nBoth MDI and pumping can achieve similarly excellent glycemic control. Some people prefer injections because they are less expensive than pumps and do not require the wearing of a continually attached device. However, the clinical literature is very clear that patients whose basal insulin requirements tend not to vary throughout the day or do not require dosage precision smaller than 0.5 IU, are much less likely to realize much significant advantage of pump therapy. Another perceived advantage of pumps is the freedom from syringes and injections, however, infusion sets still require less frequent injections to guide infusion sets into the subcutaneous tissue.\n\nIntensive/flexible insulin therapy requires frequent blood glucose checking. To achieve the best balance of blood sugar with either intensive/flexible method, a patient must check his or her glucose level with a meter monitoring of blood glucose several times a day. This allows optimization of the basal insulin and meal coverage as well as correction of high glucose episodes.\n\nThe two primary advantages of intensive/flexible therapy over more traditional two or three injection regimens are: \n\nMajor disadvantages of intensive/flexible therapy are that it requires greater amounts of education and effort to achieve the goals, and it increases the daily cost for glucose monitoring four or more times a day. This cost can substantially increase when the therapy is implemented with an insulin pump and/or continuous glucose monitor.\n\nIt is a common notion that more frequent hypoglycemia is a disadvantage of intensive/flexible regimens. The frequency of hypoglycemia increases with increasing effort to achieve normal blood glucoses with most insulin regimens, but hypoglycemia can be minimized with appropriate glucose targets and control strategies. The difficulties lie in remembering to test, estimating meal size, taking the meal bolus and eating within the prescribed time, and being aware of snacks and meals that are not the expected size. When implemented correctly, flexible regimens offer greater ability to achieve good glycemic control with easier accommodation to variations of eating and physical activity.\n\nOver the last two decades, the evidence that better glycemic control (i.e., keeping blood glucose and HbA1c levels as close to normal as possible) reduces the rates of many complications of diabetes has become overwhelming. As a result, diabetes specialists have expended increasing effort to help most people with diabetes achieve blood glucose levels as close to normal as achievable. It takes about the same amount of effort to achieve good glycemic control with a traditional two or three injection regimen as it does with flexible therapy: frequent glucose monitoring, attention to timing and amounts of meals. Many diabetes specialists no longer think of flexible insulin therapy as \"intensive\" or \"special\" treatment for a select group of patients but simply as standard care for most patients with type 1 diabetes.\n\nThe insulin pump is one device used in intensive insulinotherapy. The insulin pump is about the size of a beeper. It can be programmed to send a steady stream of insulin as \"basal insulin\". It contains a reservoir or cartridge holding several days' worth of insulin, the tiny battery-operated pump, and the computer chip that regulates how much insulin is pumped. The infusion set is a thin plastic tube with a fine needle at the end. There are also newer \"pods\" which do not require tubing. It carries the insulin from the pump to the infusion site beneath the skin. It sends a larger amount before eating meals as \"bolus\" doses.\n\nThe insulin pump replaces insulin injections. This device is useful for people who regularly forget to inject themselves or for people who don't like injections. This machine does the injecting by replacing the slow-acting insulin for basal needs with an ongoing infusion of rapid-acting insulin.\n\nBasal insulin: the insulin that controls blood glucose levels between meals and overnight. It controls glucose in the fasting state.\n\nBoluses: the insulin that is released when food is eaten or to correct a high reading.\n\nAnother device used in intensive insulinotherapy is the injection port. An injection port is a small disposable device, similar to the infusion set used with an insulin pump, configured to accept a syringe. Standard insulin injections are administered through the injection port. When using an injection port, the syringe needle always stays above the surface of the skin, thus reducing the number of skin punctures associated with intensive insulinotheraphy.\n"}
{"id": "14906", "url": "https://en.wikipedia.org/wiki?curid=14906", "title": "Interwiki links", "text": "Interwiki links\n\nInterwiki linking (\"W-link\") is a facility for creating links to the many wikis on the World Wide Web. Users avoid pasting in entire URLs (as they would for regular web pages) and instead use a shorthand similar to links within the same wiki (intrawiki links).\n\nUnlike domain names on the Internet, there is no globally defined list of interwiki prefixes, so owners of a wiki must define an interwiki map (InterMap) appropriate to their needs. Users generally have to create separate accounts for each wiki they intend to use (unless they intend to edit anonymously). Variations in text formatting and layout can also hinder a seamless transition from one wiki to the next.\n\nBy making wiki links simpler to type for the members of a particular community, these features help bring the different wikis closer together. Furthering that goal, interwiki \"bus tours\" (similar to webrings) have been created to explain the purposes and highlights of different wikis. Such examples on Wikipedia include and .\n\nInterwiki link notation varies, depending largely on the syntax a wiki uses for markup. The two most common link patterns in wikis are CamelCase and free links (arbitrary phrases surrounded by some set delimiter, such as <nowiki>double square brackets</nowiki>). CURIE syntax—an emerging W3C standard—uses a single set of square brackets.\n\nInterwiki links on a CamelCase-based wiki frequently take the form of \"Code:PageName\", where \"Code\" is the defined InterMap prefix for another wiki. Thus, a link \"WikiPedia:InterWiki\" could be rendered in HTML as a link to an article on Wikipedia: for example, . Linking from a CamelCase-wiki to a page that contains spaces in its title typically requires replacing the spaces with underscores (e.g. WikiPedia:Main_Page).\n\nInterwiki links on wikis based on free links, such as Wikipedia, typically follow the same principle, but using the delimiters that would be used for internal links. These links can then be parsed and escaped as they would be if they were internal, allowing easier typing of spaces but potentially causing problems with other special characters. For example, on Wikipedia, codice_1 appears as , and codice_1 (former syntax: codice_1) appears as .\n\nThe MediaWiki software has an additional feature which uses similar notation to create automatic interlanguage links—for instance, the link codice_1 (with no leading colon) automatically creates a reference labeled \"Other languages: | ...\" at the top and bottom of, or in a sidebar next to, the article display. Various other wiki software systems have features for \"semi-internal\" links of this kind, such as support for namespaces or multiple sub-communities.\n\nMost InterMap implementations simply replace the interwiki prefix with a full URL prefix, so many non-wiki websites can also be referred to using the system. A reference to a definition on the Free On-line Dictionary of Computing, for instance, could take the form codice_1 which would tell the system to append and display the link as Foldoc:foo. This makes it very easy to link to commonly referenced resources from within a wiki page, without the need to even know the form of the URL in question.\n\nThe interwiki concept can equally be applied to links \"from\" non-wiki websites. Advogato, for instance, offers a syntax for creating shorthand links based on a MeatBall-derived InterMap.\n\nWordPress offers a similar \"shortcode\" shorthand notation for embedding images, videos, LaTeX formulas and equations, maps, etc. hosted on other websites.\n\nInternally, a wiki that uses interwiki links needs to have a mapping from wiki-code links to full URLs. For example, codice_1 might appear as , but link to codice_7.\n\nSince most wiki systems use URLs for individual pages where the page's title appears at the end of an otherwise unchanging address, the simplest way of defining such mappings is by substituting the interwiki prefix for the unchanging part of the URL. So in the example above, the codice_8 has simply been replaced by codice_9 in creating the target of the HTML rendered link.\n\nRather than creating a new list from scratch for every wiki, it is often useful to obtain a copy of that from another site. Sites such as MeatballWiki and the UseModWiki site contain comprehensive lists which are often used for this purpose - the former being publicly editable in the same way as any other wiki page, and the latter being verified as usable but potentially out of date. MediaWiki's default list of interwiki links is derived from an old version of MeatballWiki's list.\n\n\n"}
{"id": "14907", "url": "https://en.wikipedia.org/wiki?curid=14907", "title": "Inverse function", "text": "Inverse function\n\nIn mathematics, an inverse function (or anti-function) is a function that \"reverses\" another function: if the function applied to an input gives a result of , then applying its inverse function to gives the result , and vice versa, i.e., if and only if .\n\nAs an example, consider the real-valued function of a real variable given by . Thinking of this as a step-by-step procedure (namely, take a number , multiply it by 5, then subtract 7 from the result), to reverse this and get back from some output value, say , we should undo each step in reverse order. In this case that means that we should add 7 to and then divide the result by 5. In functional notation this inverse function would be given by,\nWith we have that and .\n\nNot all functions have inverse functions. In order for a function to have an inverse, it must have the property that for every in there must be one, and only one in so that . This property ensures that a function will exist having the necessary relationship with .\n\nLet be a function whose domain is the set , and whose image (range) is the set . Then is \"invertible\" if there exists a function with domain and image , with the property:\n\nIf is invertible, the function is unique, which means that there is exactly one function satisfying this property (no more, no less). That function is then called \"the\" inverse of , and is usually denoted as .\n\nStated otherwise, a function, considered as a binary relation, has an inverse if and only if the converse relation is a function on the range , in which case the converse relation is the inverse function.\n\nNot all functions have an inverse. For a function to have an inverse, each element must correspond to no more than one ; a function with this property is called one-to-one or an injection. If is to be a function on , then each element must correspond to some . Functions with this property are called surjections. This property is satisfied by definition if is the image (range) of , but may not hold in a more general context. To be invertible a function must be both an injection and a surjection. Such functions are called bijections. The inverse of an injection that is not a bijection, that is, a function that is not a surjection, is only a partial function on , which means that for some , is undefined. If a function is invertible, then both it and its inverse function are bijections.\n\nThere is another convention used in the definition of functions. This can be referred to as the \"set-theoretic\" or \"graph\" definition using ordered pairs in which a codomain is never referred to. Under this convention all functions are surjections, and so, being a bijection simply means being an injection. Authors using this convention may use the phrasing that a function is invertible if and only if it is an injection. The two conventions need not cause confusion as long as it is remembered that in this alternate convention the codomain of a function is always taken to be the range of the function.\n\nThe function given by is not injective since each possible result \"y\" (except 0) corresponds to two different starting points in – one positive and one negative, and so this function is not invertible. With this type of function it is impossible to deduce an input from its output. Such a function is called non-injective or, in some applications, information-losing.\n\nIf the domain of the function is restricted to the nonnegative reals, that is, the function is redefined to be with the same \"rule\" as before, then the function is bijective and so, invertible. The inverse function here is called the \"(positive) square root function\".\nIf is an invertible function with domain and range , then\n\nUsing the composition of functions we can rewrite this statement as follows:\n\nwhere is the identity function on the set ; that is, the function that leaves its argument unchanged. In category theory, this statement is used as the definition of an inverse morphism.\n\nConsidering function composition helps to understand the notation . Repeatedly composing a function with itself is called iteration. If is applied times, starting with the value , then this is written as ; so , etc. Since , composing and yields , \"undoing\" the effect of one application of .\n\nWhile the notation might be misunderstood, certainly denotes the multiplicative inverse of and has nothing to do with the inverse function of .\n\nIn keeping with the general notation, some English authors use expressions like to denote the inverse of the sine function applied to (actually a partial inverse; see below) Other authors feel that this may be confused with the notation for the multiplicative inverse of , which can be denoted as . To avoid any confusion, an inverse trigonometric function is often indicated by the prefix \"arc\" (for Latin \"arcus\"). For instance, the inverse of the sine function is typically called the arcsine function, written as . Similarly, the inverse of a hyperbolic function is indicated by the prefix \"ar\" (for Latin \"area\"). For instance, the inverse of the hyperbolic sine function is typically written as . Other inverse special functions are sometimes prefixed with the prefix \"inv\" if the ambiguity of the notation should be avoided.\n\nSince a function is a special type of binary relation, many of the properties of an inverse function correspond to properties of converse relations.\n\nIf an inverse function exists for a given function , then it is unique. This follows since the inverse function must be the converse relation which is completely determined by .\n\nThere is a symmetry between a function and its inverse. Specifically, if is an invertible function with domain and range , then its inverse has domain and range , and the inverse of is the original function . In symbols, for functions and ,\n\nThis statement is a consequence of the implication that for to be invertible it must be bijective. The involutory nature of the inverse can be concisely expressed by \nThe inverse of a composition of functions is given by \nNotice that the order of and have been reversed; to undo followed by , we must first undo and then undo .\n\nFor example, let and let . Then the composition is the function that first multiplies by three and then adds five,\nTo reverse this process, we must first subtract five, and then divide by three,\nThis is the composition\n\nIf is a set, then the identity function on is its own inverse:\n\nMore generally, a function is equal to its own inverse if and only if the composition is equal to . Such a function is called an involution.\n\nSingle-variable calculus is primarily concerned with functions that map real numbers to real numbers. Such functions are often defined through formulas, such as:\nA surjective function from the real numbers to the real numbers possesses an inverse as long as it is one-to-one, i.e. as long as the graph of has, for each possible value only one corresponding value, and thus passes the horizontal line test.\n\nThe following table shows several standard functions and their inverses:\n\nOne approach to finding a formula for , if it exists, is to solve the equation for . For example, if is the function\n\nthen we must solve the equation for :\n\nThus the inverse function is given by the formula\n\nSometimes the inverse of a function cannot be expressed by a formula with a finite number of terms. For example, if is the function\n\nthen is a bijection, and therefore possesses an inverse function . The formula for this inverse has an infinite number of terms:\n\nIf is invertible, then the graph of the function\n\nis the same as the graph of the equation\n\nThis is identical to the equation that defines the graph of , except that the roles of and have been reversed. Thus the graph of can be obtained from the graph of by switching the positions of the and axes. This is equivalent to reflecting the graph across the line\n\nA continuous function is invertible on its range (image) if and only if it is either strictly increasing or decreasing (with no local maxima or minima). For example, the function\n\nis invertible, since the derivative\n\nIf the function is differentiable on an interval and for each , then the inverse will be differentiable on . If , the derivative of the inverse is given by the inverse function theorem,\nUsing Leibniz's notation the formula above can be written as\nThis result follows from the chain rule (see the article on inverse functions and differentiation).\n\nThe inverse function theorem can be generalized to functions of several variables. Specifically, a differentiable multivariable function is invertible in a neighborhood of a point as long as the Jacobian matrix of at is invertible. In this case, the Jacobian of at is the matrix inverse of the Jacobian of at .\n\n\n\n\nEven if a function is not one-to-one, it may be possible to define a partial inverse of by restricting the domain. For example, the function\n\nis not one-to-one, since . However, the function becomes one-to-one if we restrict to the domain , in which case\n\n(If we instead restrict to the domain , then the inverse is the negative of the square root of .) Alternatively, there is no need to restrict the domain if we are content with the inverse being a multivalued function:\n\nSometimes this multivalued inverse is called the full inverse of , and the portions (such as and −) are called \"branches\". The most important branch of a multivalued function (e.g. the positive square root) is called the \"principal branch\", and its value at is called the \"principal value\" of .\n\nFor a continuous function on the real line, one branch is required between each pair of local extrema. For example, the inverse of a cubic function with a local maximum and a local minimum has three branches (see the adjacent picture).\nThese considerations are particularly important for defining the inverses of trigonometric functions. For example, the sine function is not one-to-one, since\n\nfor every real (and more generally for every integer ). However, the sine is one-to-one on the interval\n, and the corresponding partial inverse is called the arcsine. This is considered the principal branch of the inverse sine, so the principal value of the inverse sine is always between − and . The following table describes the principal branch of each inverse trigonometric function:\n\nIf , a left inverse for (or \"retraction\" of ) is a function such that\n\nThat is, the function satisfies the rule\n\nThus, must equal the inverse of on the image of , but may take any values for elements of not in the image. A function with a left inverse is necessarily injective. In classical mathematics, every injective function with a nonempty domain necessarily has a left inverse; however, this may fail in constructive mathematics. For instance, a left inverse of the inclusion of the two-element set in the reals violates indecomposability by giving a retraction of the real line to the set .\n\nA right inverse for (or \"section\" of ) is a function such that\n\nThat is, the function satisfies the rule\n\nThus, may be any of the elements of that map to under . A function has a right inverse if and only if it is surjective (though constructing such an inverse in general requires the axiom of choice).\n\nAn inverse which is both a left and right inverse must be unique. However, if is a left inverse for , then may or may not be a right inverse for ; and if is a right inverse for , then is not necessarily a left inverse for . For example, let denote the squaring map, such that for all in , and let denote the square root map, such that for all . Then for all in ; that is, is a right inverse to . However, is not a left inverse to , since, e.g., .\n\nIf is any function (not necessarily invertible), the preimage (or inverse image) of an element is the set of all elements of that map to :\n\nThe preimage of can be thought of as the image of under the (multivalued) full inverse of the function .\n\nSimilarly, if is any subset of , the preimage of is the set of all elements of that map to :\n\nFor example, take a function , where . This function is not invertible for reasons discussed . Yet preimages may be defined for subsets of the codomain:\n\nThe preimage of a single element – a singleton set – is sometimes called the \"fiber\" of . When is the set of real numbers, it is common to refer to as a \"level set\".\n\n\n\n\n"}
{"id": "14909", "url": "https://en.wikipedia.org/wiki?curid=14909", "title": "Inertia", "text": "Inertia\n\nInertia is the resistance, of any physical object, to any change in its velocity. This includes changes to the object's speed, or direction of motion. \n\nAn aspect of this property is the tendency of objects to keep moving in a straight line at a constant speed, when no forces are upon them—and this aspect in particular is also called inertia.\n\nThe principle of inertia is one of the fundamental principles in classical physics that are still used today to describe the motion of objects and how they are affected by the applied forces on them. \n\nInertia comes from the Latin word, \"iners\", meaning idle, sluggish. Inertia is one of the primary manifestations of mass, which is a quantitative property of physical systems. Isaac Newton defined inertia as his first law in his \"Philosophiæ Naturalis Principia Mathematica\", which states: \n\nIn common usage, the term \"inertia\" may refer to an object's \"amount of resistance to change in velocity\" (which is quantified by its mass), or sometimes to its momentum, depending on the context. The term \"inertia\" is more properly understood as shorthand for \"the principle of inertia\" as described by Newton in his First Law of Motion: an object not subject to any net external force moves at a constant velocity. Thus, an object will continue moving at its current velocity until some force causes its speed or direction to change.\n\nOn the surface of the Earth, inertia is often masked by the effects of friction and air resistance, both of which tend to decrease the speed of moving objects (commonly to the point of rest), and gravity. This misled the philosopher Aristotle to believe that objects would move only as long as force was applied to them:\n\nPrior to the Renaissance, the most generally accepted theory of motion in Western philosophy was based on Aristotle who around about 335 BC to 322 BC said that, in the absence of an external motive power, all objects (on Earth) would come to rest and that moving objects only continue to move so long as there is a power inducing them to do so. Aristotle explained the continued motion of projectiles, which are separated from their projector, by the action of the surrounding medium, which continues to move the projectile in some way. Aristotle concluded that such violent motion in a void was impossible.\n\nDespite its general acceptance, Aristotle's concept of motion was disputed on several occasions by notable philosophers over nearly two millennia. For example, Lucretius (following, presumably, Epicurus) stated that the \"default state\" of matter was motion, not stasis. In the 6th century, John Philoponus criticized the inconsistency between Aristotle's discussion of projectiles, where the medium keeps projectiles going, and his discussion of the void, where the medium would hinder a body's motion. Philoponus proposed that motion was not maintained by the action of a surrounding medium, but by some property imparted to the object when it was set in motion. Although this was not the modern concept of inertia, for there was still the need for a power to keep a body in motion, it proved a fundamental step in that direction. This view was strongly opposed by Averroes and by many scholastic philosophers who supported Aristotle. However, this view did not go unchallenged in the Islamic world, where Philoponus did have several supporters who further developed his ideas.\n\nIn the 14th century, Jean Buridan rejected the notion that a motion-generating property, which he named \"impetus\", dissipated spontaneously. Buridan's position was that a moving object would be arrested by the resistance of the air and the weight of the body which would oppose its impetus. Buridan also maintained that impetus increased with speed; thus, his initial idea of impetus was similar in many ways to the modern concept of momentum. Despite the obvious similarities to more modern ideas of inertia, Buridan saw his theory as only a modification to Aristotle's basic philosophy, maintaining many other peripatetic views, including the belief that there was still a fundamental difference between an object in motion and an object at rest. Buridan also believed that impetus could be not only linear, but also circular in nature, causing objects (such as celestial bodies) to move in a circle.\n\nBuridan's thought was followed up by his pupil Albert of Saxony (1316–1390) and the Oxford Calculators, who performed various experiments that further undermined the classical, Aristotelian view. Their work in turn was elaborated by Nicole Oresme who pioneered the practice of demonstrating laws of motion in the form of graphs.\n\nShortly before Galileo's theory of inertia, Giambattista Benedetti modified the growing theory of impetus to involve linear motion alone:\n\nBenedetti cites the motion of a rock in a sling as an example of the inherent linear motion of objects, forced into circular motion.\n\nThe principle of inertia which originated with Aristotle for \"motions in a void\" states that an object tends to resist a change in motion. According to Newton, an object will stay at rest or stay in motion (i.e. \"maintain its velocity\") unless acted on by a net external force, whether it results from gravity, friction, contact, or some other force. The Aristotelian division of motion into mundane and celestial became increasingly problematic in the face of the conclusions of Nicolaus Copernicus in the 16th century, who argued that the earth (and everything on it) was in fact never \"at rest\", but was actually in constant motion around the sun. Galileo, in his further development of the Copernican model, recognized these problems with the then-accepted nature of motion and, at least partially as a result, included a restatement of Aristotle's description of motion in a void as a basic physical principle:\n\nA body moving on a level surface will continue in the same direction at a constant speed unless disturbed. \n\nGalileo writes that \"all external impediments removed, a heavy body on a spherical surface concentric with the earth will maintain itself in that state in which it has been; if placed in movement towards the west (for example), it will maintain itself in that movement.\" This notion which is termed \"circular inertia\" or \"horizontal circular inertia\" by historians of science, is a precursor to, but distinct from, Newton's notion of rectilinear inertia. For Galileo, a motion is \"horizontal\" if it does not carry the moving body towards or away from the centre of the earth, and for him, \"a ship, for instance, having once received some impetus through the tranquil sea, would move continually around our globe without ever stopping.\"\n\nIt is also worth noting that Galileo later (in 1632) concluded that based on this initial premise of inertia, it is impossible to tell the difference between a moving object and a stationary one without some outside reference to compare it against. This observation ultimately came to be the basis for Einstein to develop the theory of Special Relativity.\n\nThe first physicist to completely break away from the Aristotelian model of motion was Isaac Beeckman in 1614.\n\nConcepts of inertia in Galileo's writings would later come to be refined, modified and codified by Isaac Newton as the first of his Laws of Motion (first published in Newton's work, \"Philosophiae Naturalis Principia Mathematica\", in 1687):\n\nUnless acted upon by a net unbalanced force, an object will maintain a constant velocity.\n\nNote that \"velocity\" in this context is defined as a vector, thus Newton's \"constant velocity\" implies both constant speed and constant direction (and also includes the case of zero speed, or no motion). Since initial publication, Newton's Laws of Motion (and by inclusion, this first law) have come to form the basis for the branch of physics known as classical mechanics.\n\nThe term \"inertia\" was first introduced by Johannes Kepler in his \"Epitome Astronomiae Copernicanae\" (published in three parts from 1617–1621); however, the meaning of Kepler's term (which he derived from the Latin word for \"idleness\" or \"laziness\") was not quite the same as its modern interpretation. Kepler defined inertia only in terms of a resistance to movement, once again based on the presumption that rest was a natural state which did not need explanation. It was not until the later work of Galileo and Newton unified rest and motion in one principle that the term \"inertia\" could be applied to these concepts as it is today.\n\nNevertheless, despite defining the concept so elegantly in his laws of motion, even Newton did not actually use the term \"inertia\" to refer to his First Law. In fact, Newton originally viewed the phenomenon he described in his First Law of Motion as being caused by \"innate forces\" inherent in matter, which resisted any acceleration. Given this perspective, and borrowing from Kepler, Newton attributed the term \"inertia\" to mean \"the innate force possessed by an object which resists changes in motion\"; thus, Newton defined \"inertia\" to mean the cause of the phenomenon, rather than the phenomenon itself. However, Newton's original ideas of \"innate resistive force\" were ultimately problematic for a variety of reasons, and thus most physicists no longer think in these terms. As no alternate mechanism has been readily accepted, and it is now generally accepted that there may not be one which we can know, the term \"inertia\" has come to mean simply the phenomenon itself, rather than any inherent mechanism. Thus, ultimately, \"inertia\" in modern classical physics has come to be a name for the same phenomenon described by Newton's First Law of Motion, and the two concepts are now considered to be equivalent.\n\nAlbert Einstein's theory of special relativity, as proposed in his 1905 paper entitled \"On the Electrodynamics of Moving Bodies\" was built on the understanding of inertia and inertial reference frames developed by Galileo and Newton. While this revolutionary theory did significantly change the meaning of many Newtonian concepts such as mass, energy, and distance, Einstein's concept of inertia remained unchanged from Newton's original meaning (in fact, the entire theory was based on Newton's definition of inertia). However, this resulted in a limitation inherent in special relativity: the principle of relativity could only apply to reference frames that were \"inertial\" in nature (meaning when no acceleration was present). In an attempt to address this limitation, Einstein proceeded to develop his general theory of relativity (\"The Foundation of the General Theory of Relativity,\" 1916), which ultimately provided a unified theory for both \"inertial\" and \"noninertial\" (accelerated) reference frames. However, in order to accomplish this, in general relativity, Einstein found it necessary to redefine several fundamental concepts (such as gravity) in terms of a new concept of \"curvature\" of space-time, instead of the more traditional system of forces understood by Newton.\n\nAs a result of this redefinition, Einstein also redefined the concept of \"inertia\" in terms of geodesic deviation instead, with some subtle but significant additional implications. The result of this is that, according to general relativity, inertia is the gravitational coupling between matter and spacetime.\n\nWhen dealing with very large scales, the traditional Newtonian idea of \"inertia\" does not actually apply and cannot necessarily be relied upon. Luckily, for sufficiently small regions of spacetime, the special theory can be used and \"inertia\" still means the same (and works the same) as in the classical model.\n\nAnother profound conclusion of the theory of special relativity—perhaps the most well known—was that energy and mass are not separate things but are, in fact, interchangeable. But this new relationship also carried with it new implications for the concept of inertia. The logical conclusion of special relativity was that if mass exhibits the principle of inertia, then inertia must also apply to energy. This theory, and subsequent experiments confirming some of its conclusions, have also served to radically expand the meaning of inertia to apply more widely and to include inertia of energy.\n\nPhysicists and mathematicians appear to be less inclined to use the popular concept of inertia as \"a tendency to maintain momentum\" and instead favor the mathematically useful definition of inertia as the measure of a body's resistance to changes in velocity or simply a body's inertial mass.\n\nThis was clear at the beginning of the 20th century, before the advent of the theory of relativity. Mass, \"m\", denoted something like an amount of substance or quantity of matter. At the same time, mass was the quantitative measure of inertia of a body.\n\nThe mass of a body determines the momentum, formula_1, of the body at given velocity, formula_2; it is a proportionality factor in the formula:\n\nThe factor \"m\" is referred to as inertial mass.\n\nBut mass, as related to the \"inertia\" of a body, can also be defined by the formula:\n\nwhere \"F\" is the force, \"m\" is the inertial mass, and \"a\" is the acceleration.\n\nBy this formula, the greater its mass, the less a body accelerates under a given force. The masses defined by the above formulas are equal because the latter formula is a consequence of the former, if mass does not depend on time and velocity. Thus, \"mass is the quantitative or numerical measure of a body’s inertia, that is of its resistance to being accelerated\".\n\nThis meaning of a \"body's inertia\" therefore is altered from the popular meaning as \"a tendency to maintain momentum\" to a description of the measure of how difficult it is to change the velocity of a body, but it is consistent with the fact that motion in one reference frame can disappear in another, so it is the change in velocity that is important.\n\nThere is no measurable difference between gravitational mass and inertial mass. The gravitational mass is defined by the quantity of gravitational field material a mass possesses, including its energy. The \"inertial mass\" (relativistic mass) is a function of the acceleration a mass has undergone and its resultant speed. A mass that has been accelerated to speeds close to the speed of light has its \"relativistic mass\" increased, and that is why the magnetic field strength in particle accelerators must be increased to force the mass's path to curve. In practice, \"inertial mass\" is normally taken to be \"invariant mass\" and so is identical to gravitational mass without the energy component.\n\nGravitational mass is measured by comparing the force of gravity of an unknown mass to the force of gravity of a known mass. This is typically done with some sort of balance. Equal masses will match on a balance because the gravitational field applies to them equally, producing identical weight. This assumption breaks down near supermassive objects such as black holes and neutron stars due to tidal effects. It also breaks down in weightless environments, because no matter what objects are compared, it will yield a balanced reading.\n\nInertial mass is found by applying a known net force to an unknown mass, measuring the resulting acceleration, and applying Newton's Second Law, formula_5. This gives an accurate value for mass, limited only by the accuracy of the measurements. When astronauts need to be measured in the weightlessness of free fall, they actually find their inertial mass in a special chair called a body mass measurement device (BMMD).\n\nAt high speeds, and especially near the speed of light (formula_6), inertial mass can be determined by measuring the magnetic field strength and the curvature of the path of an electrically-charged mass such as an electron.\n\nNo physical difference has been found between gravitational and inertial mass in a given inertial frame. In experimental measurements, the two always agree within the margin of error for the experiment. Einstein used the fact that gravitational and inertial mass were equal to begin his general theory of relativity, in which he postulated that gravitational mass was the same as inertial mass, and that the acceleration of gravity is a result of a \"valley\" or slope in the space-time continuum that masses \"fell down\". Dennis Sciama later showed that the reaction force produced by the combined gravity of all matter in the universe upon an accelerating object is mathematically equal to the object's inertia, but this would only be a workable physical explanation if, by some mechanism, the gravitational effects operated instantaneously.\n\nAt any non-zero speed, relativistic mass always exceeds gravitational mass. If the mass is made to travel close to the speed of light, its \"inertial mass\" (relativistic) as observed from a stationary frame would be very great while its gravitational mass would remain at its rest value, but the gravitational effect of the extra energy would exactly balance the measured increase in inertial mass.\n\nIn a location such as a steadily moving railway carriage, a dropped ball (as seen by an observer in the carriage) would behave as it would if it were dropped in a stationary carriage. The ball would simply descend vertically. It is possible to ignore the motion of the carriage by defining it as an inertial frame. In a moving but non-accelerating frame, the ball behaves normally because the train and its contents continue to move at a constant velocity. Before being dropped, the ball was traveling with the train at the same speed, and the ball's inertia ensured that it continued to move in the same speed and direction as the train, even while dropping. Note that, here, it is inertia which ensured that, not its mass.\n\nIn an inertial frame, all the observers in uniform (non-accelerating) motion will observe the same laws of physics. However, observers in another inertial frame can make a simple, and intuitively obvious, transformation (the Galilean transformation), to convert their observations. Thus, an observer from outside the moving train could deduce that the dropped ball within the carriage fell vertically downwards.\n\nHowever, in reference frames which are experiencing acceleration (non-inertial reference frames), objects appear to be affected by fictitious forces. For example, if the railway carriage were accelerating, the ball would not fall vertically within the carriage but would appear to an observer to be deflected because the carriage and the ball would not be traveling at the same speed while the ball was falling. Other examples of fictitious forces occur in rotating frames such as the earth. For example, a missile at the North Pole could be aimed directly at a location and fired southwards. An observer would see it apparently deflected away from its target by a force (the Coriolis force), but in reality, the southerly target has moved because earth has rotated while the missile is in flight. Because the earth is rotating, a useful inertial frame of reference is defined by the stars, which only move imperceptibly during most observations. Newton's first law of motion is known as the principle of inertia.\n\nIn summary, the principle of inertia is intimately linked with the principles of conservation of energy and conservation of momentum.\n\nAnother form of inertia is \"rotational inertia\" (→ moment of inertia), the property that a rotating rigid body maintains its state of uniform rotational motion. Its angular momentum is unchanged, unless an external torque is applied; this is also called conservation of angular momentum. Rotational inertia depends on the object remaining structurally intact as a rigid body, and also has practical consequences. For example, a gyroscope uses the property that it resists any change in the axis of rotation.\n\nA gas or liquid in a container will also resist changes in rotational rate.\n\n\n\n"}
{"id": "14910", "url": "https://en.wikipedia.org/wiki?curid=14910", "title": "Ibanez", "text": "Ibanez\n\nThe Hoshino Gakki company began in 1908 as the musical instrument sales division of the \"Hoshino Shoten\", a bookstore chain. Hoshino Gakki decided in 1935 to make Spanish-style acoustic guitars, at first using the \"Ibanez Salvador\" brand name, and later simply \"Ibanez.\"\n\nThe modern era of Ibanez guitars began in 1957. The late 1950s and 1960s Ibanez catalogues show guitars with some wild-looking designs, manufactured by Kiso Suzuki Violin, Guyatone, and their own Tama factory established in 1962. After the Tama factory stopped manufacturing guitars in 1966, Hoshino Gakki used the Teisco and FujiGen Gakki guitar factories to make Ibanez guitars, and after the Teisco String Instrument factory closed in 1969/1970, Hoshino Gakki used the FujiGen Gakki guitar factory to make Ibanez guitars.\nIn the 1960s, Japanese guitar makers mainly copied American guitar designs, and Ibanez-branded copies of Gibson, Fender and Rickenbacker models appear. This resulted in the so-called lawsuit period. During this period, Ibanez produced guitars under the Mann name to avoid authorities in the United States and Canada. \n\nHoshino Gakki introduced Ibanez models that were definitely not copies of the Gibson or Fender designs, such as the Iceman and the Roadstar series. The company has produced its own guitar designs ever since. The late 1980s and early 1990s were an important period for the Ibanez brand. Hoshino Gakki's relationship with guitarist Steve Vai resulted in the introduction of the Ibanez JEM and the Ibanez Universe models; after the earlier successes of the Roadstar and Iceman models in the late 1970s/early 1980s, Hoshino Gakki entered the superstrat market with the RG series, a lower-priced version of their JEM series.\nHoshino Gakki also had semi-acoustic, nylon- and steel-stringed acoustic guitars manufactured under the Ibanez name. Most Ibanez guitars were made by the FujiGen guitar factory in Japan up until the mid- to late 1980s, and from then on Ibanez guitars have also been made in other Asian countries such as Korea, China, and Indonesia. During the early 1980s, the FujiGen guitar factory also produced most of the Roland guitar synthesizers, including the Stratocaster-style Roland G-505, the twin-humbucker Roland G-202 (endorsed by Adrian Belew, Eric Clapton, Dean Brown, Jeff Baxter, Yannis Spathas, Christoforos Krokidis, Steve Howe, Mike Rutherford, Andy Summers, Neal Schon and Steve Hackett) and the Ibanez X-ING IMG-2010.\n\nCimar and Starfield were guitar and bass brands owned by Hoshino Gakki. In the 1970s, Hoshino Gakki and Kanda Shokai shared some guitar designs, and so some Ibanez and Greco guitars have the same features. The Greco versions were sold in Japan and the Ibanez versions were sold outside Japan. From 1982, Ibanez guitars have also been sold in Japan as well.\n\nGuitar brands such as Antoria and Mann shared some Ibanez guitar designs. The Antoria guitar brand was managed by JT Coppock Leeds Ltd England. CSL was a brand name managed by Charles Summerfield Ltd England. Maurice Summerfield of the Charles Summerfield Ltd company contributed some design ideas to Hoshino Gakki and also imported Ibanez and CSL guitars into the UK from 1964 to 1987. The Maxxas brand name came about because Hoshino Gakki thought that the guitar did not fit in with the Ibanez model range and was therefore named Maxxas by Rich Lasner from Hoshino USA.\n\nHarry Rosenbloom, founder of the (now-closed) Medley Music of Bryn Mawr, Pennsylvania, was manufacturing handmade guitars under the name \"Elger.\" By 1965, Rosenbloom had decided to stop manufacturing guitars and chose to become the exclusive North American distributor for Ibanez guitars. In September 1972, Hoshino began a partnership with Elger Guitars to import guitars from Japan. In September 1981, Elger was renamed \"Hoshino U.S.A.\", retaining the company headquarters in Bensalem, Pennsylvania as a distribution and quality-control center.\n\nOn June 28, 1977, in the Philadelphia Federal District Court, a lawsuit was filed by the Norlin Corporation, the parent company of Gibson Guitars, against Elger/Hoshino U.S.A.'s use of the Gibson headstock design and logo. Hoshino settled out of court in early 1978 and the case was officially closed on February 2, 1978.\n\nAfter the lawsuit, Hoshino Gakki abandoned the strategy of copying \"classic\" electric guitar designs, having already introduced a plethora of original designs. Hoshino was producing their original Artist models from 1974, introducing a set-neck model in 1975. In 1977, they upgraded and extended their Artist range and introduced a number of other top-quality original designs made to match or surpass famous American brands: the Performer and short-lived Concert ranges which competed with the Les Paul; through-neck Musicians; Studios in fixed- and through-neck construction; the radically shaped Iceman; and the Roadster which morphed into the Roadstar range, precursor to the popular superstrat era in the mid-1980s. The newer Ibanez models began incorporating more modern elements into their design such as radical body shapes, slimmer necks, 2-octave fingerboards, slim pointed headstocks, higher-output electronics, humbucker/single-coil/humbucker (H/S/H) pickup configurations, locking tremolo bridges and different finishes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the 1970s, the Nisshin Onpa company who owned the Maxon brand name, developed and began selling a series of effect pedals in Japan. Hoshino Gakki licensed these for sale using the name Ibanez outside Japan. These two companies eventually began doing less and less business together until Nisshin Onpa ceased manufacturing the TS-9 reissue for Hoshino Gakki in 2002.\n\n\n\n"}
{"id": "14912", "url": "https://en.wikipedia.org/wiki?curid=14912", "title": "Incest", "text": "Incest\n\nIncest is human sexual activity between family members or close relatives. This typically includes sexual activity between people in consanguinity (blood relations), and sometimes those related by affinity (marriage or stepfamily), adoption, clan, or lineage.\n\nThe incest taboo is one of the most widespread of all cultural taboos, both in present and in past societies. Most modern societies have laws regarding incest or social restrictions on closely consanguineous marriages. In societies where it is illegal, consensual adult incest is seen by some as a victimless crime. Some cultures extend the incest taboo to relatives with no consanguinity such as milk-siblings, step-siblings, and adoptive siblings, albeit sometimes with less intensity. Third-degree relatives (such as half-aunt, half-nephew, first cousin) on average share 12.5% genes, and sexual relations between them are viewed differently in various cultures, from being discouraged to being socially acceptable. Children of incestuous relationships have been regarded as illegitimate, and are still so regarded in some societies today. In most cases, the parents did not have the option to marry to remove that status, as incestuous marriages were, and are, normally also prohibited.\n\nA common justification for prohibiting incest is avoiding inbreeding: a collection of genetic disorders suffered by the children of parents with a close genetic relationship. Such children are at greater risk for congenital disorders, death, and developmental and physical disability, and that risk is proportional to their parents' coefficient of relationship—a measure of how closely the parents are related genetically. But cultural anthropologists have noted that inbreeding avoidance cannot form the sole basis for the incest taboo because the boundaries of the incest prohibition vary widely between cultures, and not necessarily in ways that maximize the avoidance of inbreeding.\n\nIn some societies, such as those of Ancient Egypt, brother–sister, father–daughter, mother–son, cousin–cousin, aunt–nephew, uncle–niece, and other combinations of relations within a royal family were married as a means of perpetuating the royal lineage. Some societies, such as the Balinese and some Inuit tribes, have different views about what constitutes illegal and immoral incest. However, sexual relations with a first-degree relative (such as a parent or sibling) are almost universally forbidden.\n\nThe English word \"incest\" is derived from the Latin \"incestus\", which has a general meaning of \"impure, unchaste\".\nIt was introduced into Middle English, both in the generic Latin sense (preserved throughout the Middle English period) and in the narrow modern sense.\nThe derived adjective \"incestuous\" appears in the 16th century.\nBefore the Latin term came in, incest was known in Old English as \"sib-leger\" (from \"sibb\" 'kinship' + \"leger\" 'to lie') or \"mǣġhǣmed\" (from \"mǣġ\" 'kin, parent' + \"hǣmed\" 'sexual intercourse') but in time, both words fell out of use. Terms like \"incester\" and \"incestual\" have been used to describe those interested or involved in sexual relations with relatives among humans, while \"inbreeder\" has been used in relation to similar behavior among non-human animals or organisms.\n\nOther words that describe sexual attraction to relatives include consanguinophilia, consanguinamory, synegenesophilia, incestuality and incestophilia.\n\nIn ancient China, first cousins with the same surnames (i.e., those born to the father's brothers) were not permitted to marry, while those with different surnames (i.e., maternal cousins and paternal cousins born to the father's sisters) were.\n\nSeveral of the Egyptian Pharaohs married their siblings and had several children with them. For example, Tutankhamun married his half-sister Ankhesenamun, and was himself the child of an incestuous union between Akhenaten and an unidentified sister-wife. It is now generally accepted that sibling marriages were widespread among all classes in Egypt during the Graeco-Roman period. Numerous papyri and the Roman census declarations attest to many husbands and wives being brother and sister, of the same father and mother. The most famous of these relationships were in the Ptolemaic royal family; Cleopatra VII was married to her younger brother, Ptolemy XIII, while her mother and father, Cleopatra V and Ptolemy XII, had also been brother and sister.\n\nThe fable of \"Oedipus\", with a theme of inadvertent incest between a mother and son, ends in disaster and shows ancient taboos against incest as Oedipus is punished for incestuous actions by blinding himself. In the \"sequel\" to Oedipus, \"Antigone\", his four children are also punished for their parents' incestuousness. Incest appears in the commonly accepted version of the birth of Adonis, when his mother, Myrrha has sex with her father Cinyras during a festival, disguised as a prostitute.\n\nIn Ancient Greece, Spartan King Leonidas I, hero of the legendary Battle of Thermopylae, was married to his niece Gorgo, daughter of his half-brother Cleomenes I. Greek law allowed marriage between a brother and sister if they had different mothers. For example, some accounts say that Elpinice was for a time married to her half-brother Cimon.\n\nIncest is mentioned and condemned in Virgil's \"Aeneid\" Book VI: \"hic thalamum invasit natae vetitosque hymenaeos;\" \"This one invaded a daughter's room and a forbidden sex act\".\nRoman civil law prohibited marriages within four degrees of consanguinity but had no degrees of affinity with regards to marriage. Roman civil laws prohibited any marriage between parents and children, either in the ascending or descending line \"ad infinitum\". Adoption was considered the same as affinity in that an adoptive father could not marry an unemancipated daughter or granddaughter even if the adoption had been dissolved. Incestuous unions were discouraged and considered \"nefas\" (against the laws of gods and man) in ancient Rome. In AD 295 incest was explicitly forbidden by an imperial edict, which divided the concept of \"incestus\" into two categories of unequal gravity: the \"incestus iuris gentium,\" which was applied to both Romans and non-Romans in the Empire, and the \"incestus iuris civilis,\" which concerned only Roman citizens. Therefore, for example, an Egyptian could marry an aunt, but a Roman could not. Despite the act of incest being unacceptable within the Roman Empire, Roman Emperor Caligula is rumored to have had sexual relationships with all three of his sisters (Julia Livilla, Drusilla, and Agrippina the Younger). Emperor Claudius, after executing his previous wife, married his brother's daughter Agrippina the Younger, and changed the law to allow an otherwise illegal union. The law prohibiting marrying a sister's daughter remained. The taboo against incest in Ancient Rome is demonstrated by the fact that politicians would use charges of incest (often false charges) as insults and means of political disenfranchisement.\n\nIn Norse mythology, there are themes of brother-sister marriage, a prominent example being between Njörðr and his unnamed sister (perhaps Nerthus), parents of Freyja and Freyr. Loki in turn also accuses Freyja and Freyr of having a sexual relationship.\n\nAccording to of the Hebrew Bible, the Patriarch Abraham married his half-sister Sarah. According to , Amram married his aunt Jochebed. According to 2 Samuel, Amnon, King David's son, raped his half-sister, Tamar.\n\nIn Genesis 19:30-38, living in an isolated area after the destruction of Sodom and Gomorrah, Lot's two daughters conspired to inebriate and seduce their father due to the lack of available partners to continue his line of descent. Because of intoxication, Lot \"perceived not\" when his firstborn, and the following night his younger daughter, lay with him. (Genesis 19:32-35)\n\nAccording to the Book of Jubilees, Cain married his sister Awan.\n\nMany European monarchs were related due to political marriages, sometimes resulting in distant cousins (and even first cousins) being married. This was especially true in the Habsburg, Hohenzollern, Savoy and Bourbon royal houses. However, relations between siblings, which may have been tolerated in other cultures, were considered abhorrent. For example, the accusation that Anne Boleyn and her brother George Boleyn had committed incest was one of the reasons that both siblings were executed in May 1536.\n\nIncestuous marriages were also seen in the royal houses of ancient Japan and Korea, Inca Peru, Ancient Hawaii, and, at times, Central Africa, Mexico, and Thailand. Like the pharaohs of ancient Egypt, the Inca rulers married their sisters. Huayna Capac, for instance, was the son of Topa Inca Yupanqui and the Inca's sister and wife.\n\nHalf-sibling marriages were found in ancient Japan such as the marriage of Emperor Bidatsu and his half-sister Empress Suiko. Japanese Prince Kinashi no Karu had sexual relationships with his full sister Princess Karu no Ōiratsume, although the action was regarded as foolish. In order to prevent the influence of the other families, a half-sister of Korean Goryeo Dynasty monarch Gwangjong became his wife in the 10th century. Her name was Daemok. Marriage with a family member not related by blood was also regarded as contravening morality and was therefore incest. One example of this is the 14th century Chunghye of Goryeo, who raped one of his deceased father's concubines, who was thus regarded to be his mother.\n\nIn India, the largest proportion of women aged 13–49 who marry their close relative are in Tamil Nadu, then Andhra Pradesh, Karnataka, and Maharashtra. While it is rare for uncle-niece marriages, it is more common in Andhra Pradesh and Tamil Nadu.\n\nIncest between an adult and a person under the age of consent is considered a form of child sexual abuse that has been shown to be one of the most extreme forms of childhood abuse; it often results in serious and long-term psychological trauma, especially in the case of parental incest. Its prevalence is difficult to generalize, but research has estimated 10–15% of the general population as having at least one such sexual contact, with less than 2% involving intercourse or attempted intercourse. Among women, research has yielded estimates as high as 20%.\n\nFather-daughter incest was for many years the most commonly reported and studied form of incest. More recently, studies have suggested that sibling incest, particularly older brothers having sexual relations with younger siblings, is the most common form of incest, with some studies finding sibling incest occurring more frequently than other forms of incest. Some studies suggest that adolescent perpetrators of sibling abuse choose younger victims, abuse victims over a lengthier period, use violence more frequently and severely than adult perpetrators, and that sibling abuse has a higher rate of penetrative acts than father or stepfather incest, with father and older brother incest resulting in greater reported distress than stepfather incest.\n\nSex between an adult family member and a child is usually considered a form of child sexual abuse known as child incestuous abuse, and for many years has been the most reported form of incest. Father–daughter and stepfather–stepdaughter sex is the most commonly reported form of adult–child incest, with most of the remaining involving a mother or stepmother. Many studies found that stepfathers tend to be far more likely than biological fathers to engage in this form of incest. One study of adult women in San Francisco estimated that 17% of women were abused by stepfathers and 2% were abused by biological fathers. Father–son incest is reported less often, but it is not known how close the frequency is to heterosexual incest because it is likely more under-reported. Prevalence of incest between parents and their children is difficult to assess due to secrecy and privacy.\n\nIn a 1999 news story, \"BBC\" reported, \"Close-knit family life in India masks an alarming amount of sexual abuse of children and teenage girls by family members, a new report suggests. Delhi organisation RAHI said 76% of respondents to its survey had been abused when they were children—40% of those by a family member.\"\n\nAccording to the National Center for Victims of Crime a large proportion of rape committed in the United States is perpetrated by a family member:\nA study of victims of father–daughter incest in the 1970s showed that there were \"common features\" within families before the occurrence of incest: estrangement between the mother and the daughter, extreme paternal dominance, and reassignment of some of the mother's traditional major family responsibility to the daughter. Oldest and only daughters were more likely to be the victims of incest. It was also stated that the incest experience was psychologically harmful to the woman in later life, frequently leading to feelings of low self-esteem, very unhealthy sexual activity, contempt for other women, and other emotional problems.\n\nAdults who as children were incestuously victimized by adults often suffer from low self-esteem, difficulties in interpersonal relationships, and sexual dysfunction, and are at an extremely high risk of many mental disorders, including depression, anxiety disorders, phobic avoidance reactions, somatoform disorder, substance abuse, borderline personality disorder, and complex post-traumatic stress disorder. Research by Leslie Margolin indicates that mother-son incest does not trigger some innate biological response, but that the effects are more directly related to the symbolic meanings attributed to this act by the participants.\n\nThe Goler clan in Nova Scotia is a specific instance in which child sexual abuse in the form of forced adult/child and sibling/sibling incest took place over at least three generations. A number of Goler children were victims of sexual abuse at the hands of fathers, mothers, uncles, aunts, sisters, brothers, cousins, and each other. During interrogation by police, several of the adults openly admitted to engaging in many forms of sexual activity, up to and including full intercourse, multiple times with the children. Sixteen adults (both men and women) were charged with hundreds of allegations of incest and sexual abuse of children as young as five. In July 2012, twelve children were removed from the 'Colt' family (a pseudonym) in New South Wales, Australia, after the discovery of four generations of incest. Child protection workers and psychologists said interviews with the children indicated \"a virtual sexual free-for-all\".\n\nIn Japan, there is a popular misconception that mother-son incestuous contact is common, due to the manner in which it is depicted in the press and popular media. According to Hideo Tokuoka, \"When Americans think of incest, they think of fathers and daughters; in Japan one thinks of mothers and sons\" due to the extensive media coverage of mother-son incest there. Some western researchers assumed that mother-son incest is common in Japan, but research into victimization statistics from police and health-care systems discredits this; it shows that the vast majority of sexual abuse, including incest, in Japan is perpetrated by men against young girls.\n\nWhile incest between adults and children generally involves the adult as the perpetrator of abuse, there are rare instances of sons sexually assaulting their mothers. These sons are typically mid adolescent to young adult, and, unlike parent-initiated incest, the incidents involve some kind of physical force. Although the mothers may be accused of being seductive with their sons and inviting the sexual contact, this is contrary to evidence. Such accusations can parallel other forms of rape, where, due to victim blaming, a woman is accused of somehow being at fault for the rape. In some cases, mother-son incest is best classified as acquaintance rape of the mother by the adolescent son.\n\nChildhood sibling–sibling incest is considered to be widespread but rarely reported. Sibling–sibling incest becomes child-on-child sexual abuse when it occurs without consent, without equality, or as a result of coercion. In this form, it is believed to be the most common form of intrafamilial abuse. The most commonly reported form of abusive sibling incest is abuse of a younger sibling by an older sibling. A 2006 study showed a large portion of adults who experienced sibling incest abuse have \"distorted\" or \"disturbed\" beliefs (such as that the act was \"normal\") both about their own experience and the subject of sexual abuse in general.\n\nSibling abusive incest is most prevalent in families where one or both parents are often absent or emotionally unavailable, with the abusive siblings using incest as a way to assert their power over a weaker sibling. Absence of the father in particular has been found to be a significant element of most cases of sexual abuse of female children by a brother. The damaging effects on both childhood development and adult symptoms resulting from brother–sister sexual abuse are similar to the effects of father–daughter, including substance abuse, depression, suicidality, and eating disorders.\n\nSexual activity between adult close relatives is sometimes ascribed to genetic sexual attraction. This form of incest has not been widely reported, but evidence has indicated that this behavior does take place, possibly more often than many people realize. Internet chatrooms and topical websites exist that provide support for incestuous couples.\n\nProponents of incest between consenting adults draw clear boundaries between the behavior of consenting adults and rape, child molestation, and abusive incest. However, even consensual relationships such as these are still legally classified as incest, and criminalized in almost all jurisdictions . James Roffee, a senior lecturer in criminology at Monash University and former worker on legal responses to familial sexual activity in England and Wales, and Scotland, discussed how the European Convention on Human Rights deems all familial sexual acts to be criminal, even if all parties give their full consent and are knowledgeable to all possible consequences. He also argues that the use of particular language tools in the legislation manipulates the reader to deem all familial sexual activities as immoral and criminal, even if all parties are consenting adults.\n\nAccording to one incest participant who was quoted for an article in \"The Guardian\":\n\nIn \"Slate\", William Saletan drew a legal connection between gay sex and incest between consenting adults. As he described in his article, in 2003, U.S. Senator Rick Santorum commented on a pending U.S. Supreme Court case involving sodomy laws (primarily as a matter of constitutional rights to privacy and equal protection under the law):\n\nSaletan argued that, legally and morally, there is essentially no difference between the two, and went on to support incest between consenting adults being covered by a legal right to privacy. UCLA law professor Eugene Volokh has made similar arguments. In a more recent article, Saletan said that incest is wrong because it introduces the possibility of irreparably damaging family units by introducing \"a notoriously incendiary dynamic—sexual tension—into the mix\".\n\nIn the Netherlands, marrying one's nephew or niece is legal, but only with the explicit permission of the Dutch Government, due to the possible risk of genetic defects among the offspring. Nephew-niece marriages predominantly occur among foreign immigrants. In November 2008, the Christian Democratic (CDA) party's Scientific Institute announced that it wanted a ban on marriages between nephews and nieces.\n\nConsensual sex between adults (persons of 18 years and older) is always lawful in the Netherlands and Belgium, even among closely related family members. Sexual acts between an adult family member and a minor are illegal, though they are not classified as incest, but as abuse of the authority such an adult has over a minor, comparable to that of a teacher, coach or priest.\n\nIn Florida, consensual adult sexual intercourse with someone known to be your aunt, uncle, niece or nephew constitutes a felony of the third degree. Other states also commonly prohibit marriages between such kin. The legality of sex with a half-aunt or half-uncle varies state by state.\n\nIn the United Kingdom, incest includes only sexual intercourse with a parent, grandparent, child or sibling, but the more recently introduced offence of \"sex with an adult relative\" extends also as far as half-siblings, uncles, aunts, nephews and nieces. The term 'incest' however remains widely used in popular culture to describe any form of sexual activity with a relative.\nIn Canada marriage between uncles and nieces and between aunts and nephews is legal\n\nThe most public case of consensual adult sibling incest in recent years is the case of a brother-sister couple from Germany, Patrick Stübing and Susan Karolewski. Because of violent behavior on the part of his father, Patrick was taken in at the age of 3 by foster parents, who adopted him later. At the age of 23 he learned about his biological parents, contacted his mother, and met her and his then 16-year-old sister Susan for the first time. The now-adult Patrick moved in with his birth family shortly thereafter. After their mother died suddenly six months later, the siblings became intimately close, and had their first child together in 2001. By 2004, they had four children together: Eric, Sarah, Nancy, and Sofia. The public nature of their relationship, and the repeated prosecutions and even jail time they have served as a result, has caused some in Germany to question whether incest between consenting adults should be punished at all. An article about them in \"Der Spiegel\" states that the couple are happy together. According to court records, the first three children have mental and physical disabilities, and have been placed in foster care. In April 2012, at the European Court of Human Rights, Patrick Stübing lost his case that the conviction violated his right to a private and family life. On September 24, 2014, the German Ethics Council has recommended that the government abolish laws criminalizing incest between siblings, arguing that such bans impinge upon citizens.\n\nSome societies differentiate between full sibling and half sibling relations. In ancient societies, full sibling and half sibling marriages occurred.\n\nMarriages and sexual relationships between first cousins are stigmatized as incest in some cultures, but tolerated in much of the world. Currently, 24 US states prohibit marriages between first cousins, and another seven permit them only under special circumstances.\nThe United Kingdom permits both marriage and sexual relations between first cousins.\n\nIn some non-Western societies, marriages between close biological relatives account for 20% to 60% of all marriages.\n\nFirst- and second-cousin marriages are rare, accounting for less than 1% of marriages in Western Europe, North America and Oceania, while reaching 9% in South America, East Asia and South Europe and about 50% in regions of the Middle East, North Africa and South Asia. Communities such as the Dhond and the Bhittani of Pakistan clearly prefer marriages between cousins as belief they ensure purity of the descent line, provide intimate knowledge of the spouses, and ensure that patrimony will not pass into the hands of \"outsiders\". Cross-cousin marriages are preferred among the Yanomami of Brazilian Amazonia, among many other tribal societies identified by anthropologists.\n\nThere are some cultures in Asia which stigmatize cousin marriage, in some instances even marriages between second cousins or more remotely related people. This is notably true in the culture of Korea. In South Korea, before 1997, anyone with the same last name and clan were prohibited from marriage. In light of this law being held unconstitutional, South Korea now only prohibits up to third cousins (see Article 809 of the Korean Civil Code). Hmong culture prohibits the marriage of anyone with the same last name – to do so would result in being shunned by the entire community, and they are usually stripped of their last name. Some Hindu communities in India prohibit cousin marriages.\n\nIn a review of 48 studies on the children parented by cousins, the rate of birth defects was twice that of non-related couples: 4% for cousin couples as opposed to 2% for the general population.\n\nSome cultures include relatives by marriage in incest prohibitions; these relationships are called affinity rather than consanguinity. For example, the question of the legality and morality of a widower who wished to marry his deceased wife's sister was the subject of long and fierce debate in the United Kingdom in the 19th century, involving, among others, Matthew Boulton and Charles La Trobe. The marriages were entered into in Scotland and Switzerland respectively, where they were legal. In medieval Europe, standing as a godparent to a child also created a bond of affinity. But in other societies, a deceased spouse's sibling was considered the ideal person to marry. The Hebrew Bible forbids a man from marrying his brother's widow with the exception that, if his brother died childless, the man is instead required to marry his brother's widow so as to \"raise up seed to him\" (per ). Some societies have long practiced sororal polygyny, a form of polygamy in which a man marries multiple wives who are sisters to each other (though not closely related to him).\n\nIn Islamic law, marriage among close blood relations like parents, stepparent, parents in-law, siblings, stepsiblings, the children of siblings, aunts and uncles is prohibited, while first or second cousins may marry. Marrying the widow of a brother, or the sister of deceased or divorced wife is also allowed.\n\nOffspring of biologically related parents are subject to the possible impact of inbreeding. Such offspring have a higher possibility of congenital birth defects (see Coefficient of relationship) because it increases the proportion of zygotes that are homozygous for deleterious recessive alleles that produce such disorders (see Inbreeding depression). Because most such alleles are rare in populations, it is unlikely that two unrelated marriage partners will both be heterozygous carriers. However, because close relatives share a large fraction of their alleles, the probability that any such rare deleterious allele present in the common ancestor will be inherited from both related parents is increased dramatically with respect to non-inbred couples. Contrary to common belief, inbreeding does not in itself alter allele frequencies, but rather increases the relative proportion of homozygotes to heterozygotes. This has two contrary effects.\nThe closer two persons are related, the higher the zygosity, and thus the more severe the biological costs of inbreeding. This fact likely explains why inbreeding between close relatives, such as siblings, is less common than inbreeding between cousins.\n\nThere may also be other deleterious effects besides those caused by recessive diseases. Thus, similar immune systems may be more vulnerable to infectious diseases (see Major histocompatibility complex and sexual selection).\n\nA 1994 study found a mean excess mortality with inbreeding among first cousins of 4.4%. Children of parent-child or sibling-sibling unions are at increased risk compared to cousin-cousin unions. Studies suggest that 20-36% of these children will die or have major disability due to the inbreeding. A study of 29 offspring resulting from brother-sister or father-daughter incest found that 20 had congenital abnormalities, including four directly attributable to autosomal recessive alleles.\n\nLaws regarding sexual activity between close relatives vary considerably between jurisdictions, and depend on the type of sexual activity and the nature of the family relationship of the parties involved, as well as the age and sex of the parties. Prohibition of incest laws may extend to restrictions on marriage rights, which also vary between jurisdictions. Most jurisdictions prohibit parent-child and sibling marriages, while others also prohibit first-cousin and uncle-niece and aunt-nephew marriages. In most places, incest is illegal, regardless of the ages of the two partners. In other countries, incestuous relationships between consenting adults (with the age varying by location) are permitted, including in the Netherlands, France, Slovenia and Spain. Sweden is the only country that allows marriage between half-siblings and they must seek government counseling before marriage.\n\nWhile the legality of consensual incest varies by country, sexual assault committed against a relative is usually seen as a very serious crime. In some legal systems, the fact of a perpetrator being a close relative to the victim constitutes an aggravating circumstance in the case of sexual crimes such as rape and sexual conduct with a minor – this is the case in Romania.\n\nAccording to the Torah, per Leviticus 18, \"the children of Israel\"—Israelite men and women alike—are forbidden from sexual relations between people who are \"near of kin\" (cf. verse 6), who are defined as:\n\n\nAnd Moses commanded the children of Israel according to the word of the LORD, saying: 'The tribe of the sons of Joseph speaketh right. This is the thing which the LORD hath commanded concerning the daughters of Zelophehad, saying: Let them be married to whom they think best; only into the family of the tribe of their father shall they be married. So shall no inheritance of the children of Israel remove from tribe to tribe; for the children of Israel shall cleave every one to the inheritance of the tribe of his fathers. And every daughter, that possesseth an inheritance in any tribe of the children of Israel, shall be wife unto one of the family of the tribe of her father, that the children of Israel may possess every man the inheritance of his fathers. So shall no inheritance remove from one tribe to another tribe; for the tribes of the children of Israel shall cleave each one to its own inheritance.' Even as the LORD commanded Moses, so did the daughters of Zelophehad. For Mahlah, Tirzah, and Hoglah, and Milcah, and Noah, the daughters of Zelophehad, were married unto their father's brothers' sons.\n\nIncestuous relationships are considered so severe among \"chillulim HaShem\", acts which bring shame to the name of God, as to be, along with the other forbidden relationships that are mentioned in Leviticus 18, punishable by death as specified in Leviticus 20.\n\nIn the 4th century BCE, the Soferim (\"scribes\") declared that there were relationships within which marriage constituted incest, in addition to those mentioned by the Torah. These additional relationships were termed \"seconds\" (Hebrew: \"sheniyyot\"), and included the wives of a man's grandfather and grandson. The classical rabbis prohibited marriage between a man and any of these \"seconds\" of his, on the basis that doing so would act as a \"safeguard\" against infringing the biblical incest rules, although there was inconclusive debate about exactly what the limits should be for the definition of \"seconds\".\n\nMarriages that are forbidden in the Torah (with the exception of uncle-niece marriages) were regarded by the rabbis of the Middle Ages as invalid – as if they had never occurred; any children born to such a couple were regarded as bastards under Jewish law, and the relatives of the spouse were not regarded as forbidden relations for a further marriage. On the other hand, those relationships which were prohibited due to qualifying as \"seconds\", and so forth, were regarded as wicked, but still valid; while they might have pressured such a couple to divorce, any children of the union were still seen as legitimate.\n\nThe Catholic Church does not generally permit the marriage if a doubt exists on whether the potential spouses are related by blood relations in any degree of the direct line or in the second degree of the collateral line.\n\nDefinitions of incest varied throughout history. The Fourth Lateran Council held in 1215 attempted to codify that marriage was forbidden up to and including third cousins, though permissible beyond this for fourth cousins, third cousins once removed, etc.\n\nIn the Eastern Orthodox Church, marriages are banned between second cousins or closer and between second uncles / aunts and second nieces / nephews (between first cousins once removed) or closer. Marrying one's godparent or deceased spouse's sibling is also prohibited, although marrying one's stepchild is not – e.g. Vyacheslav Ivanov exercised his right to marry his stepdaughter after her mother's (his first wife's) death.\n\nThe Book of Common Prayer of the Anglican Communion allows marriages up to and including first cousins.\n\nIn Augustine of Hippo's book \"The City of God\", a 5th Century AD book of Christian philosophy, he claimed that the children of Adam and Eve must have married each other in order to produce offspring.\n\nThe Quran gives specific rules regarding incest, which prohibit a man from marrying or having sexual relationships with:\n\nCousin marriage finds support in Islamic scriptures and is widespread in the Middle East.\n\nAlthough Islam allows cousin marriage, there are Hadiths attributed to Muhammad calling for distance from the marriage of relatives.\n\nIn Ancient Persia, incest between cousins is a blessed virtue although in some sources incest is believed to be related to that of parent-child or brothers-sisters. Under Zoroastrianism both royalty, priestly and commoners practiced incest. This tradition was called Xwedodah (). The tradition was considered so sacred, that the bodily fluids produced by an incestuous couple were thought to have curative powers. For instance, the Vendidad advised corpse-bearers to purify themselves with a mixture of urine of a married incestuous couple. Friedrich Nietzsche, in his book \"The Birth of Tragedy\", cited that among Zoroastrians a wise priest is born only by Xvaetvadatha.\n\nTo what extent Xvaetvadatha was practiced in Sasanian Iran and before, especially outside the royal and noble families (“dynastic incest”) and, perhaps, the clergy, and whether practices ascribed to them can be assumed to be characteristic of the general population is not clear. Evidence from Dura-Europos, however, combined with that of the Jewish and Christian sources citing actual cases under the Sasanians, strengthen the evidence of the Zoroastrian texts. In the post-Sasanian Zoroastrian literature, Xvaetvadatha is said to refer to marriages between cousins which have always been relatively common. When Anquetil Duperron visited the Parsis in the mid-18th century, he was told the term referred to marriage with cousins, and according to James Darmesteter, it was rare for a Parsi to marry out of the family; marriage between cousins (a marriage made in heaven) was both practical and normal, while other incestuous marriages were taboo and illegal.\n\nRigveda regard incest to be \"evil\". Hinduism speaks of incest in abhorrent terms. Hindus believe there are both karmic and practical bad effects of incest and thus practice strict rules of both endogamy and exogamy, in relation to the family tree (\"gotra\") or bloodline (\"Pravara\").\nMarriage within the \"gotra\" (\"swagotra\" marriages) are banned under the rule of exogamy in the traditional matrimonial system. People within the \"gotra\" are regarded as kin and marrying such a person would be thought of as incest. Marriage with paternal cousins (a form of parallel-cousin relationship) is strictly prohibited.\n\nAlthough generally marriages between persons having the same \"gotra\" are prohibited, how this is defined may vary regionally.\nDepending on culture and caste of the population in the region, marriage may be restricted up to seven generations of \"gotra\" of father, mother, and grandmother. In a few rural areas, marriage is banned within same local community is only allowed with those from outside of the community, as they consider a small village to be like brothers and sisters of one large family. These rules are strictly enforced and a couple breaking them is violently punished sometimes. The seven-generation prohibition in some areas is only applied to paternal descent, with \"gotra\" is transferred down the male lineage; in these areas, the \"gotra\" of a female changes upon marriage (i.e., upon marriage a woman would belong to her husband's lineage).\n\nIn the \"Mahabharata\", one of the two great Hindu Epics, Arjuna took as his fourth wife his first and cross cousin Subhadra, the sister of Krishna. Arjuna had gone into exile alone after having disturbed Yudhishthira and Draupadi in their private quarters. It was during the last part of his exile, while staying at the Dvaraka residence of his cousins, that he fell in love with Subhadra. While eating at the home of Balarama, Arjuna was struck with Subhadra's beauty and decided he would obtain her as his wife. Subhadra and Arjuna's son was the tragic hero Abhimanyu. According to Andhra Pradesh oral tradition, Abhimanyu himself married his first cross cousin Shashirekha, the daughter of Subhadra's brother Balarama. Cross-cousin marriages are evident from Arjuna's marriage with Subhadra, Pradyumna's (Eldest son of Krishna) marriage with Rukmi's (brother of Rukmini) daughter. Also, Krishna married his cross cousin Mitravinda (daughter of Vasudeva's sister Rajadhi who was queen of Avanti) and Bhadra (daughter of Vasudeva's sister Shrutakirti who was the queen of Kekaya Kingdom.)\n\nMany mammal species, including humanity's closest primate relatives, tend to avoid mating with close relatives, especially if there are alternative partners available. However, some chimpanzees have been recorded attempting to mate with their mothers. Male rats have been recorded engaging in mating with their sisters, but they tend to prefer non-related females over their sisters.\n\nLivestock breeders often practice controlled breeding to eliminate undesirable characteristics within a population, which is also coupled with culling of what is considered unfit offspring, especially when trying to establish a new and desirable trait in the stock.\n\nBed bugs: North Carolina State University found that bed bugs, in contrast to most other insects, tolerate incest and are able to genetically withstand the effects of inbreeding quite well.\n\n\n\n"}
{"id": "14914", "url": "https://en.wikipedia.org/wiki?curid=14914", "title": "Industrial Revolution", "text": "Industrial Revolution\n\nThe Industrial Revolution was the transition to new manufacturing processes in Europe and the US, in the period from about 1760 to sometime between 1820 and 1840. This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, the increasing use of steam power, the development of machine tools and the rise of the factory system. The Industrial Revolution also led to an unprecedented rise in the rate of population growth.\n\nTextiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested. The textile industry was also the first to use modern production methods.\n\nThe Industrial Revolution began in Great Britain, and many of the technological innovations were of British origin. By the mid-18th century Britain was the world's leading commercial nation, controlling a global trading empire with colonies in North America and the Caribbean, and with some political influence on the Indian subcontinent, through the activities of the East India Company. The development of trade and the rise of business were major causes of the Industrial Revolution.\n\nThe Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists say that the major impact of the Industrial Revolution was that the standard of living for the general population began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries.\n\nGDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy, while the Industrial Revolution began an era of per-capita economic growth in capitalist economies. Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals and plants.\n\nAlthough the structural change from agriculture to industry is widely associated with Industrial Revolution, in United Kingdom it was already almost complete by 1760.\n\nThe precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes. Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s, while T.S. Ashton held that it occurred roughly between 1760 and 1830. Rapid industrialization first began in Britain, starting with mechanized spinning in the 1780s, with high rates of growth in steam power and iron production occurring after 1800. Mechanized textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium and the United States and later textiles in France.\n\nAn economic recession occurred from the late 1830s to the early 1840s when the adoption of the original innovations of the Industrial Revolution, such as mechanized spinning and weaving, slowed and their markets matured. Innovations developed late in the period, such as the increasing adoption of locomotives, steamboats and steamships, hot blast iron smelting and new technologies, such as the electrical telegraph, widely introduced in the 1840s and 1850s, were not powerful enough to drive high rates of growth. Rapid economic growth began to occur after 1870, springing from a new group of innovations in what has been called the Second Industrial Revolution. These new innovations included new steel making processes, the large-scale manufacture of machine tools and the use of increasingly advanced machinery in steam-powered factories.\n\nThe earliest recorded use of the term \"Industrial Revolution\" seems to have been in a letter from 6 July 1799 written by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise. In his 1976 book \"\", Raymond Williams states in the entry for \"Industry\": \"The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811 and 1818, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century.\" The term \"Industrial Revolution\" applied to technological change was becoming more common by the late 1830s, as in Jérôme-Adolphe Blanqui's description in 1837 of \"la révolution industrielle\". Friedrich Engels in \"The Condition of the Working Class in England\" in 1844 spoke of \"an industrial revolution, a revolution which at the same time changed the whole of civil society\". However, although Engels wrote in the 1840s, his book was not translated into English until the late 1800s, and his expression did not enter everyday language until then. Credit for popularising the term may be given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term.\n\nSome historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and the term \"revolution\" is a misnomer. This is still a subject of debate among some historians.\n\nThe commencement of the Industrial Revolution is closely linked to a small number of innovations, beginning in the second half of the 18th century. By the 1830s the following gains had been made in important technologies:\n\nIn 1750 Britain imported 2.5 million pounds of raw cotton, most of which was spun and woven by cottage industry in Lancashire. The work was done by hand in workers' homes or occasionally in shops of master weavers. In 1787 raw cotton consumption was 22 million pounds, most of which was cleaned, carded and spun on machines. The British textile industry used 52 million pounds of cotton in 1800, which increased to 588 million pounds in 1850.\n\nThe share of value added by the cotton textile industry in Britain was 2.6% in 1760, 17% in 1801 and 22.4% in 1831. Value added by the British woollen industry was 14.1% in 1801. Cotton factories in Britain numbered approximately 900 in 1797. In 1760 approximately one-third of cotton cloth manufactured in Britain was exported, rising to two-thirds by 1800. In 1781 cotton spun amounted to 5.1 million pounds, which increased to 56 million pounds by 1800. In 1800 less than 0.1% of world cotton cloth was produced on machinery invented in Britain. In 1788 there were 50,000 spindles in Britain, rising to 7 million over the next 30 years.\n\nWages in Lancashire, a core region for cottage industry and later factory spinning and weaving, were about six times those in India in 1770, when overall productivity in Britain was about three times higher than in India.\n\nParts of India, China, Central America, South America and the Middle-East have a long history of hand manufacturing cotton textiles, which became a major industry sometime after 1000 AD. In tropical and subtropical regions where it was grown, most was grown by small farmers alongside their food crops and was spun and woven in households, largely for domestic consumption. In the 15th century China began to require households to pay part of their taxes in cotton cloth. By the 17th century almost all Chinese wore cotton clothing. Almost everywhere cotton cloth could be used as a medium of exchange. In India a significant amount of cotton textiles were manufactured for distant markets, often produced by professional weavers. Some merchants also owned small weaving workshops. India produced a variety of cotton cloth, some of exceptionally fine quality.\n\nCotton was a difficult raw material for Europe to obtain before it was grown on colonial plantations in the Americas. The early Spanish explorers found Native Americans growing unknown species of excellent quality cotton: sea island cotton (\"Gossypium barbadense\") and upland green seeded cotton \"Gossypium hirsutum\". Sea island cotton grew in tropical areas and on barrier islands of Georgia and South Carolina, but did poorly inland. Sea island cotton began being exported from Barbados in the 1650s. Upland green seeded cotton grew well on inland areas of the southern U.S., but was not economical because of the difficulty of removing seed, a problem solved by the cotton gin. A strain of cotton seed brought from Mexico to Natchez, Mississippi in 1806 became the parent genetic material for over 90% of world cotton production today; it produced bolls that were three to four times faster to pick.\n\nThe Age of Discovery was followed by a period of colonialism beginning around the 16th century. Following the discovery of a trade route to India around southern Africa by the Portuguese, the Dutch established the Verenigde Oostindische Compagnie (abbr. VOC) or Dutch East India Company and the British founded the East India Company, along with smaller companies of different nationalities which established trading posts and employed agents to engage in trade throughout the Indian Ocean region and between the Indian Ocean region and North Atlantic Europe. One of the largest segments of this trade was in cotton textiles, which were purchased in India and sold in Southeast Asia, including the Indonesian archipelago, where spices were purchased for sale to Southeast Asia and Europe. By the mid-1760s cloth was over three-quarters of the East India Company's exports. Indian textiles were in demand in North Atlantic region of Europe where previously only wool and linen were available; however, the amount of cotton goods consumed in Western Europe was minor until the early 19th century.\n\nBy 1600 Flemish refugees began weaving cotton cloth in English towns where cottage spinning and weaving of wool and linen was well established; however, they were left alone by the guilds who did not consider cotton a threat. Earlier European attempts at cotton spinning and weaving were in 12th-century Italy and 15th-century southern Germany, but these industries eventually ended when the supply of cotton was cut off. The Moors in Spain grew, spun and wove cotton beginning around the 10th century.\n\nBritish cloth could not compete with Indian cloth because India's labor cost was approximately one-fifth to one-sixth that of Britain's. In 1700 and 1721 the British government passed Calico Acts in order to protect the domestic woollen and linen industries from the increasing amounts of cotton fabric imported from India.\n\nThe demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft. Flax was used for the warp because wheel-spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew.\n\nOn the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption and as a cottage industry under the putting-out system. Occasionally the work was done in the workshop of a master weaver. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials. In the off season the women, typically farmers' wives, did the spinning and the men did the weaving. Using the spinning wheel, it took anywhere from four to eight spinners to supply one hand loom weaver.\n\nThe flying shuttle, patented in 1733 by John Kay, with a number of subsequent improvements including an important one in 1747, doubled the output of a weaver, worsening the imbalance between spinning and weaving. It became widely used around Lancashire after 1760 when John's son, Robert, invented the drop box, which facilitated changing thread colors.\n\nLewis Paul patented the roller spinning frame and the flyer-and-bobbin system for drawing wool to a more even thickness. The technology was developed with the help of John Wyatt of Birmingham. Paul and Wyatt opened a mill in Birmingham which used their new rolling machine powered by a donkey. In 1743 a factory opened in Northampton with 50 spindles on each of five of Paul and Wyatt's machines. This operated until about 1764. A similar mill was built by Daniel Bourn in Leominster, but this burnt down. Both Lewis Paul and Daniel Bourn patented carding machines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill. Lewis's invention was later developed and improved by Richard Arkwright in his water frame and Samuel Crompton in his spinning mule.\n\nIn 1764 in the village of Stanhill, Lancashire, James Hargreaves invented the spinning jenny, which he patented in 1770. It was the first practical spinning frame with multiple spindles. The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting. It was a simple, wooden framed machine that only cost about £6 for a 40-spindle model in 1792, and was used mainly by home spinners. The jenny produced a lightly twisted yarn only suitable for weft, not warp.\n\nThe spinning frame or water frame was developed by Richard Arkwright who, along with two partners, patented it in 1769. The design was partly based on a spinning machine built for Thomas High by clockmaker John Kay, who was hired by Arkwright. For each spindle the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre, which was then twisted by the spindle. The roller spacing was slightly longer than the fibre length. Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread. The top rollers were leather-covered and loading on the rollers was applied by a weight. The weights kept the twist from backing up before the rollers. The bottom rollers were wood and metal, with fluting along the length. The water frame was able to produce a hard, medium count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain. A horse powered the first factory to use the spinning frame. Arkwright and his partners used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name.\n\nSamuel Crompton's Spinning Mule was introduced in 1779. Mule implies a hybrid because it was a combination of the spinning jenny and the water frame, in which the spindles were placed on a carriage, which went through an operational sequence during which the rollers stopped while the carriage moved away from the drawing roller to finish drawing out the fibres as the spindles started rotating. Crompton's mule was able to produce finer thread than hand spinning and at a lower cost. Mule spun thread was of suitable strength to be used as warp, and finally allowed Britain to produce highly competitive yarn in large quantities.\n\nRealising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785. In 1776 he patented a two-man operated loom which was more conventional. Cartwright built two factories; the first burned down and the second was sabotaged by his workers. Cartwright's loom design had several flaws, the most serious being thread breakage. Samuel Horrocks patented a fairly successful loom in 1813. Horock's loom was improved by Richard Roberts in 1822 and these were produced in large numbers by Roberts, Hill & Co.\n\nThe demand for cotton presented an opportunity to planters in the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed. Eli Whitney responded to the challenge by inventing the inexpensive cotton gin. A man using a cotton gin could remove seed from as much upland cotton in one day as would previously, working at the rate of one pound of cotton per day, have taken a woman two months to process.\n\nThese advances were capitalised on by entrepreneurs, of whom the best known is Richard Arkwright. He is credited with a list of inventions, but these were actually developed by such people as Thomas Highs and John Kay; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and he developed the use of power – first horse power and then water powerwhich made cotton manufacture a mechanised industry. Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply of yarn increased greatly. Before long steam power was applied to drive textile machinery. Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories.\n\nAlthough mechanization dramatically decreased the cost of cotton cloth, by the mid-19th century machine-woven cloth still could not equal the quality of hand-woven Indian cloth, in part due to the fineness of thread made possible by the type of cotton used in India, which allowed high thread counts. However, the high productivity of British textile manufacturing allowed coarser grades of British cloth to undersell hand-spun and woven fabric in low-wage India, eventually destroying the industry.\n\nThe earliest European attempts at mechanized spinning were with wool; however, wool spinning proved more difficult to mechanize than cotton. Productivity improvement in wool spinning during the Industrial Revolution was significant but was far less than that of cotton.\n\nArguably the first highly mechanised factory was John Lombe's water-powered silk mill at Derby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, because the Italian silk industry guarded its secrets closely, the state of the industry at that time is unknown. Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition. In order to promote manufacturing the Crown paid for models of Lombe's machinery which were exhibited in the Tower of London.\n\nBar iron was the commodity form of iron used as the raw material for making hardware goods such as nails, wire, hinges, horse shoes, wagon tires, chains, etc. and for structural shapes. A small amount of bar iron was converted into steel. Cast iron was used for pots, stoves and other items where its brittleness was tolerable. Most cast iron was refined and converted to bar iron, with substantial losses. Bar iron was also made by the bloomery process, which was the predominant iron smelting process until the late 18th century.\n\nIn the UK in 1720 there were 20,500 tons of cast iron produced with charcoal and 400 tons with coke. In 1750 charcoal iron production was 24,500 and coke iron was 2,500 tons. In 1788 the production of charcoal cast iron was 14,000 tons while coke iron production was 54,000 tons. In 1806 charcoal cast iron production was 7,800 tons and coke cast iron was 250,000 tons.\n\nIn 1750 the UK imported 31,200 tons of bar iron and either refined from cast iron or directly produced 18,800 tons of bar iron using charcoal and 100 tons using coke. In 1796 the UK was making 125,000 tons of bar iron with coke and 6,400 tons with charcoal; imports were 38,000 tons and exports were 24,600 tons. In 1806 the UK did not import bar iron but exported 31,500 tons.\n\nA major change in the iron industries during the era of the Industrial Revolution was the replacement of wood and other bio-fuels with coal. For a given amount of heat, coal required much less labour to mine than cutting wood and converting it to charcoal, and coal was much more abundant than wood, supplies of which were becoming scarce before the enormous increase in iron production that took place in the late 18th century. By 1750 coke had generally replaced charcoal in smelting of copper and lead and was in widespread use in making glass. In the smelting and refining of iron, coal and coke produced inferior iron to that made with charcoal because of the coal's sulfur content. Low sulfur coals were known, but they still contained harmful amounts. Conversion of coal to coke only slightly reduces the sulfur content. A minority of coals are coking.\n\nAnother factor limiting the iron industry before the Industrial Revolution was the scarcity of water power to power blast bellows. This limitation was overcome by the steam engine.\n\nUse of coal in iron smelting started somewhat before the Industrial Revolution, based on innovations by Sir Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal. This has the advantage that impurities (such as sulphur ash) in the coal do not migrate into the metal. This technology was applied to lead from 1678 and to copper from 1687. It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace. (The foundry cupola is a different, and later, innovation.)\n\nBy 1709 Abraham Darby made progress using coke to fuel his blast furnaces at Coalbrookdale. However, the coke pig iron he made was not suitable for making wrought iron and was used mostly for the production of cast iron goods, such as pots and kettles. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs.\n\nCoke pig iron was hardly used to produce wrought iron until 1755–56, when Darby's son Abraham Darby II built furnaces at Horsehay and Ketley where low sulfur coal was available (and not far from Coalbrookdale). These new furnaces were equipped with water-powered bellows, the water being pumped by Newcomen steam engines. The Newcomen engines were not attached directly to the blowing cylinders because the engines alone could not produce a steady air blast. Abraham Darby III installed similar steam-pumped, water-powered blowing cylinders at the Dale Company when he took control in 1768. The Dale Company used several Newcomen engines to drain its mines and made parts for engines which it sold throughout the country.\n\nSteam engines made the use of higher-pressure and volume blast practical; however, the leather used in bellows was expensive to replace. In 1757, iron master John Wilkinson patented a hydraulic powered blowing engine for blast furnaces. The blowing cylinder for blast furnaces was introduced in 1760 and the first blowing cylinder made of cast iron is believed to be the one used at Carrington in 1768 that was designed by John Smeaton. Cast iron cylinders for use with a piston were difficult to manufacture; the cylinders had to be free of holes and had to be machined smooth and straight to remove any warping. James Watt had great difficulty trying to have a cylinder made for his first steam engine. In 1774 John Wilkinson, who built a cast iron blowing cylinder for his iron works, invented a precision boring machine for boring cylinders. After Wilkinson bored the first successful cylinder for a Boulton and Watt steam engine in 1776, he was given an exclusive contract for providing cylinders. After Watt developed a rotary steam engine in 1782, they were widely applied to blowing, hammering, rolling and slitting.\n\nThe solutions to the sulfur problem were the addition of sufficient limestone to the furnace to force sulfur into the slag and the use of low sulfur coal. Use of lime or limestone required higher furnace temperatures to form a free-flowing slag. The increased furnace temperature made possible by improved blowing also increased the capacity of blast furnaces and allowed for increased furnace height. In addition to lower cost and greater availability, coke had other important advantages over charcoal in that it was harder and made the column of materials (iron ore, fuel, slag) flowing down the blast furnace more porous and did not crush in the much taller furnaces of the late 19th century.\n\nAs cast iron became cheaper and widely available, it began being a structural material for bridges and buildings. A famous early example was the Iron Bridge built in 1778 with cast iron produced by Abraham Darby III. However, most cast iron was converted to wrought iron.\n\nEurope relied on the bloomery for most of its wrought iron until the large scale production of cast iron. Conversion of cast iron was done in a finery forge, as it long had been. An improved refining process known as potting and stamping was developed, but this was superseded by Henry Cort's puddling process. Cort developed two significant iron manufacturing processes: rolling in 1783 and puddling in 1784. Puddling produced a structural grade iron at a relatively low cost.\n\nPuddling was a means of decarburizing molten pig iron by slow oxidation in a reverberatory furnace by manually stirring it with a long rod. The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler. When the glob was large enough, the puddler would remove it. Puddling was backbreaking and extremely hot work. Few puddlers lived to be 40. Because puddling was done in a reverberatory furnace, coal or coke could be used as fuel. The puddling process continued to be used until the late 19th century when iron was being displaced by steel. Because puddling required human skill in sensing the iron globs, it was never successfully mechanised. Rolling was an important part of the puddling process because the grooved rollers expelled most of the molten slag and consolidated the mass of hot wrought iron. Rolling was 15 times faster at this than a trip hammer. A different use of rolling, which was done at lower temperatures than that for expelling slag, was in the production of iron sheets, and later structural shapes such as beams, angles and rails.\n\nThe puddling process was improved in 1818 by Baldwyn Rogers, who replaced some of the sand lining on the reverberatory furnace bottom with iron oxide. In 1838 John Hall patented the use of roasted tap cinder (iron silicate) for the furnace bottom, greatly reducing the loss of iron through increased slag caused by a sand lined bottom. The tap cinder also tied up some phosphorus, but this was not understood at the time. Hall's process also used iron scale or rust, which reacted with carbon in the molten iron. Hall's process, called \"wet puddling\", reduced losses of iron with the slag from almost 50% to around 8%.\n\nPuddling became widely used after 1800. Up to that time British iron manufacturers had used considerable amounts of iron imported from Sweden and Russia to supplement domestic supplies. Because of the increased British production, imports began to decline in 1785 and by the 1790s Britain eliminated imports and became a net exporter of bar iron.\n\nHot blast, patented by James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron. By using preheated combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coke or two-thirds using coal; however, the efficiency gains continued as the technology improved. Hot blast also raised the operating temperature of furnaces, increasing their capacity. Using less coal or coke meant introducing fewer impurities into the pig iron. This meant that lower quality coal or anthracite could be used in areas where coking coal was unavailable or too expensive; however, by the end of the 19th century transportation costs fell considerably.\n\nShortly before the Industrial Revolution an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs. Benjamin Huntsman developed his crucible steel technique in the 1740s. The raw material for this was blister steel, made by the cementation process.\n\nThe supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.\n\nThe development of the stationary steam engine was an important element of the Industrial Revolution; however, during the early period of the Industrial Revolution, most industrial power was supplied by water and wind. In Britain by 1800 an estimated 10,000 horsepower was being supplied by steam. By 1815 steam power had grown to 210,000 hp.\n\nThe first commercially successful industrial use of steam power was due to Thomas Savery in 1698. He constructed and patented in London a low-lift combined vacuum and pressure water pump, that generated about one horsepower (hp) and was used in numerous water works and in a few mines (hence its \"brand name\", \"The Miner's Friend\"). Savery's pump was economical in small horsepower ranges, but was prone to boiler explosions in larger sizes. Savery pumps continued to be produced until the late 18th century.\n\nThe first successful piston steam engine was introduced by Thomas Newcomen before 1712. A number of Newcomen engines were installed in Britain for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a significant amount of capital to build, and produced upwards of . They were also used to power municipal water supply pumps. They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, opened up a great expansion in coal mining by allowing mines to go deeper. Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century. By 1729, when Newcomen died, his engines had spread (first) to Hungary in 1722, Germany, Austria, and Sweden. A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad. In the 1770s the engineer John Smeaton built some very large examples and introduced a number of improvements. A total of 1,454 engines had been built by 1800.\n\nA fundamental change in working principles was brought about by Scotsman James Watt. With financial support from his business partner Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder, thereby making the low-pressure steam drive the top of the piston instead of the atmosphere, use of a steam jacket and the celebrated separate steam condenser chamber. The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam. Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency. These improvements increased engine efficiency so that Boulton & Watts engines used only 20–25% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened the Soho Foundry for the manufacture of such engines in 1795.\n\nBy 1783 the Watt steam engine had been fully developed into a double-acting rotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially very successful, and by 1800, the firm Boulton & Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from .\n\nUntil about 1800 the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained rotative engines (readily removable, but not on wheels) were developed, such as the table engine. Around the start of the 19th century, at which time the Boulton and Watt patent expired, the Cornish engineer Richard Trevithick and the American Oliver Evans began to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere. High pressure yielded an engine and boiler compact enough to be used on mobile road and rail locomotives and steam boats.\n\nThe development of machine tools, such as the engine lathe, planing, milling and shaping machines powered by these engines, enabled all the metal parts of the engines to be easily and accurately cut and in turn made it possible to build larger and more powerful engines.\n\nSmall industrial power requirements continued to be provided by animal and human muscle until widespread electrification in the early 20th century. These included crank-powered, treadle-powered and horse-powered workshop and light industrial machinery.\n\nPre-industrial machinery was built by various craftsmen – millwrights built water and windmills, carpenters made wooden framing, and smiths and turners made metal parts. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time. As the Industrial Revolution progressed, machines with metal parts and frames became more common. Other important uses of metal parts were in firearms and threaded fasteners, such as machine screws, bolts and nuts. There was also the need for precision in making parts. Precision would allow better working machinery, interchangeability of parts and standardization of threaded fasteners.\n\nThe demand for metal parts led to the development of several machine tools. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms.\n\nBefore the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws and chisels. Consequently, the use of metal machine parts was kept to a minimum. Hand methods of production were very laborious and costly and precision was difficult to achieve.\n\nThe first large precision machine tool was the cylinder boring machine invented by John Wilkinson in 1774. It used for boring the large-diameter cylinders on early steam engines. Wilkinson's boring machine differed from earlier cantilevered machines used for boring cannon in that the cutting tool was mounted on a beam that ran through the cylinder being bored and was supported outside on both ends.\n\nThe planing machine, the milling machine and the shaping machine were developed in the early decades of the 19th century. Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century.\n\nHenry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at the Royal Arsenal, Woolwich. He worked as an apprentice in the Royal Gun Foundry of Jan Verbruggen. In 1774 Jan Verbruggen had installed a horizontal boring machine in Woolwich which was the first industrial size Lathe in the UK. Maudslay was hired away by Joseph Bramah for the production of high-security metal locks that required precision craftsmanship. Bramah patented a lathe that had similarities to the slide rest lathe. Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw. Before its invention screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template. The slide rest lathe was called one of history's most important inventions. Although it was not entirely Maudslay's idea, he was the first person to build a functional lathe using a combination of known innovations of the lead screw, slide rest and change gears.\n\nMaudslay left Bramah's employment and set up his own shop. He was engaged to build the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills. These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops he trained a generation of men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth.\n\nJames Fox of Derby had a healthy export trade in machine tools for the first third of the century, as did Matthew Murray of Leeds. Roberts was a maker of high-quality machine tools and a pioneer of the use of jigs and gauges for precision workshop measurement.\n\nThe impact of machine tools during the Industrial Revolution was not that great because other than firearms, threaded fasteners and a few other industries there were few mass-produced metal parts. The techniques to make mass-produced metal parts made with sufficient precision to be interchangeable is largely attributed to a program of the U.S. Department of War which perfected interchangeable parts for firearms in the early 19th century.\n\nIn the half century following the invention of the fundamental machine tools the machine industry became the largest industrial sector of the U.S. economy, by value added.\n\nThe large-scale production of chemicals was an important development during the Industrial Revolution. The first of these was the production of sulphuric acid by the lead chamber process invented by the Englishman John Roebuck (James Watt's first partner) in 1746. He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made of riveted sheets of lead. Instead of making a small amount each time, he was able to make around in each of the chambers, at least a tenfold increase.\n\nThe production of an alkali on a large scale became an important goal as well, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate. The Leblanc process was a reaction of sulfuric acid with sodium chloride to give sodium sulfate and hydrochloric acid. The sodium sulfate was heated with limestone (calcium carbonate) and coal to give a mixture of sodium carbonate and calcium sulfide. Adding water separated the soluble sodium carbonate from the calcium sulfide. The process produced a large amount of pollution (the hydrochloric acid was initially vented to the air, and calcium sulfide was a useless waste product). Nonetheless, this synthetic soda ash proved economical compared to that from burning specific plants (barilla) or from kelp, which were the previously dominant sources of soda ash, and also to potash (potassium carbonate) produced from hardwood ashes.\n\nThese two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes. Sodium carbonate had many uses in the glass, textile, soap, and paper industries. Early uses for sulfuric acid included pickling (removing rust) iron and steel, and for bleaching cloth.\n\nThe development of bleaching powder (calcium hypochlorite) by Scottish chemist Charles Tennant in about 1800, based on the discoveries of French chemist Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk. Tennant's factory at St Rollox, North Glasgow, became the largest chemical plant in the world.\n\nAfter 1860 the focus on chemical innovation was in dyestuffs, and Germany took world leadership, building a strong chemical industry. Aspiring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques. British scientists by contrast, lacked research universities and did not train advanced students; instead, the practice was to hire German-trained chemists.\n\nIn 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement which was an important advance in the building trades. This process involves sintering a mixture of clay and limestone to about , then grinding it into a fine powder which is then mixed with water, sand and gravel to produce concrete. Portland cement was used by the famous English engineer Marc Isambard Brunel several years later when constructing the Thames Tunnel. Cement was used on a large scale in the construction of the London sewerage system a generation later.\n\nAnother major industry of the later Industrial Revolution was gas lighting. Though others made a similar innovation elsewhere, the large-scale introduction of this was the work of William Murdoch, an employee of Boulton & Watt, the Birmingham steam engine pioneers. The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They soon became one of the major consumers of coal in the UK. Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles or oil. Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.\n\nA new method of producing glass, known as the cylinder process, was developed in Europe during the early 19th century. In 1832 this process was used by the Chance Brothers to create sheet glass. They became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings. The Crystal Palace is the supreme example of the use of sheet glass in a new and innovative structure.\n\nA machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 by Nicholas Louis Robert who worked for Saint-Léger Didot family in France. The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London. Although greatly improved and with many variations, the Fourdriner machine is the predominant means of paper production today.\n\nThe method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.\n\nThe British Agricultural Revolution is considered one of the causes of the Industrial Revolution because improved agricultural productivity freed up workers to work in other sectors of the economy. However, per-capita food supply in Europe was stagnant or declining and did not improve in some parts of Europe until the late 18th century.\n\nIndustrial technologies that affected farming included the seed drill, the Dutch plough, which contained iron parts, and the threshing machine.\n\nJethro Tull invented an improved seed drill in 1701. It was a mechanical seeder which distributed seeds evenly across a plot of land and planted them at the correct depth. This was important because the yield of seeds harvested to seeds planted at that time was around four or five. Tull's seed drill was very expensive and not very reliable and therefore did not have much of an impact. Good quality seed drills were not produced until the mid 18th century.\n\nJoseph Foljambe's \"Rotherham plough\" of 1730 was the first commercially successful iron plough. The threshing machine, invented by Andrew Meikle in 1784, displaced hand threshing with a flail, a laborious job that took about one-quarter of agricultural labour. It took several decades to diffuse and was the final straw for many farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of the Swing Riots.\n\nMachine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders and combine harvesters.\n\nCoal mining in Britain, particularly in South Wales, started early. Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted. In other cases, if the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill. Shaft mining was done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets of water up the shaft or to a sough (a tunnel driven into a hill to drain a mine). In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity. The introduction of the steam pump by Thomas Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted. These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable. The Cornish engine, developed in the 1810s, was much more efficient than the Watt steam engine.\n\nCoal mining was very dangerous owing to the presence of firedamp in many coal seams. Some degree of safety was provided by the safety lamp which was invented in 1816 by Sir Humphry Davy and independently by George Stephenson. However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light. Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the entire 19th century. Conditions of work were very poor, with a high casualty rate from rock falls.\n\nAt the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea. Wagonways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed. Animals supplied all of the motive power on land, with sails providing the motive power on the sea. The first horse railways were introduced toward the end of the 18th century, with steam locomotives being introduced in the early decades of the 19th century. Improving sailing technologies boosted average sailing speed 50% between 1750 and 1830.\n\nThe Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network. Raw materials and finished products could be moved more quickly and cheaply than before. Improved transportation also allowed new ideas to spread quickly.\n\nBefore and during the Industrial Revolution navigation on several British rivers was improved by removing obstructions, straightening curves, widening and deepening and building navigation locks. Britain had over 1000 miles of navigable rivers and streams by 1750.\n\nCanals and waterways allowed bulk materials to be economically transported long distances inland. This was because a horse could pull a barge with a load dozens of times larger than the load that could be drawn in a cart.\n\nBuilding of canals dates to ancient times. The Grand Canal in China is \"the world's largest artificial waterway and oldest canal still in existence,\" parts of which were started between the 6th and 4th centuries BC, is long and links Hangzhou with Beijing.\n\nIn the UK, canals began to be built in the late 18th century to link the major manufacturing centres across the country. Known for its huge commercial success, the Bridgewater Canal in North West England, which opened in 1761 and was mostly funded by The 3rd Duke of Bridgewater. From Worsley to the rapidly growing town of Manchester its construction cost £168,000 (£ ), but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half. This success helped inspire a period of intense canal building, known as Canal Mania. New canals were hastily built in the aim of replicating the commercial success of the Bridgewater Canal, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively.\n\nBy the 1820s a national network was in existence. Canal construction served as a model for the organisation and methods later used to construct the railways. They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on. The last major canal to be built in the United Kingdom was the Manchester Ship Canal, which upon opening in 1894 was the largest ship canal in the world, and opened Manchester as a port. However it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper.\n\nBritain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain.\n\nFrance was known for having an excellent system of roads at the time of the Industrial Revolution; however, most of the roads on the European Continent and in the U.K. were in bad condition and dangerously rutted.\n\nMuch of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier) turnpike trusts were set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built by John Metcalf, Thomas Telford and most notably John McAdam, with the first 'macadamised' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816. The major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country. Heavy goods transport on these roads was by means of slow, broad wheeled, carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or by teams of pack horse. Stagecoaches carried the rich, and the less wealthy could pay to ride on carriers carts.\n\nReducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, England.\n“A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton. A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration. Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together. A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.”\n\nRailways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high-pressure steam engine also around 1800.\n\nWagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal. These were all horse drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used). Horse-drawn public railways did not begin until the early years of the 19th century when improvements to pig and wrought iron production were lowering costs. See: Metallurgy\n\nSteam locomotives began being built after the introduction of high-pressure steam engines after the expiration of the Boulton and Watt patent in 1800. High-pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water. They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines. A few of these early locomotives were used in mines. Steam-hauled public railways began with the Stockton and Darlington Railway in 1825.\n\nThe rapid introduction of railways followed the 1829 Rainhill Trials, which demonstrated Robert Stephenson's successful locomotive design and the 1828 development of Hot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity the blast furnace.\n\nOn 15 September 1830, the Liverpool and Manchester Railway was opened, the first inter-city railway in the world and was attended by Prime Minister, the Duke of Wellington. The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool. The opening was marred by problems, due to the primitive nature of the technology being employed, however problems were gradually ironed out and the railway became highly successful, transporting passengers and freight. The success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania.\n\nConstruction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories.\n\nOther developments included more efficient water wheels, based on experiments conducted by the British engineer John Smeaton the beginnings of a machine industry and the rediscovery of concrete (based on hydraulic lime mortar) by John Smeaton, which had been lost for 1300 years.\n\nPrior to the Industrial Revolution, most of the workforce was employed in agriculture, either as self-employed farmers as landowners or tenants, or as landless agricultural labourers. It was common for families in various parts of the world to spin yarn, weave cloth and make their own clothing. Households also spun and wove for market production. At the beginning of the Industrial Revolution India, China and regions of Iraq and elsewhere in Asia and the Middle East produced most of the world's cotton cloth while Europeans produced wool and linen goods.\n\nIn Britain by the 16th century the putting-out system, by which farmers and townspeople produced goods for market in their homes, often described as \"cottage industry\", was being practiced. Typical putting out system goods included spinning and weaving. Merchant capitalist typically provided the raw materials, paid workers by the piece, and were responsible for the sale of the goods. Embezzlement of supplies by workers and poor quality were common problems. The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations of the putting out system.\n\nSome early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792, was affordable for cottagers. Later machinery such as spinning frames, spinning mules and power looms were expensive (especially if water powered), giving rise to capitalist ownership of factories.\n\nThe majority of textile factory workers during the Industrial Revolution were unmarried women and children, including many orphans. They typically worked for 12 to 14 hours per day with only Sundays off. It was common for women take factory jobs seasonally during slack periods of farm work. Lack of adequate transportation, long hours and poor pay made it difficult to recruit and maintain workers. Many workers, such as displaced farmers and agricultural workers, who had nothing but their labour to sell, became factory workers out of necessity. (See: British Agricultural Revolution, Threshing machine)\n\nThe change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably by Karl Marx, however, he recognized the increase in productivity made possible by technology.\n\nSome economists, such as Robert E. Lucas, Jr., say that the real impact of the Industrial Revolution was that \"for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility.\" Others, however, argue that while growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries, and that in many ways workers' living standards declined under early capitalism: for instance, studies have shown that real wages in Britain only increased 15% between the 1780s and 1850s, and that life expectancy in Britain did not begin to dramatically increase until the 1870s. Similarly, the average height of the population declined during the Industrial Revolution, implying that their nutritional status was also decreasing. Real wages were not keeping up with the price of food.\n\nDuring the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.\n\nThe effects on living conditions the industrial revolution have been very controversial, and were hotly debated by economic and social historians from the 1950s to the 1980s. A series of 1950s essays by Henry Phelps Brown and Sheila V. Hopkins later set the academic consensus that the bulk of the population, that was at the bottom of the social ladder, suffered severe reductions in their living standards. During 1813–1913, there was a significant increase in worker wages.\n\nChronic hunger and malnutrition were the norm for the majority of the population of the world including Britain and France, until the late 19th century. Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years and about 40 years in Britain. The United States population of the time was adequately fed, much taller on average and had life expectancy of 45–50 years although U.S. life expectancy declined by a few years by the mid 19th century. Food consumption per capita also declined during an episode known as the Antebellum Puzzle.\n\nFood supply in Great Britain was adversely affected by the Corn Laws (1815–1846). The Corn Laws, which imposed tariffs on imported grain, were enacted to keep prices high in order to benefit domestic producers. The Corn Laws were repealed in the early years of the Great Irish Famine.\n\nThe initial technologies of the Industrial Revolution, such as mechanized textiles, iron and coal, did little, if anything, to lower food prices. In Britain and the Netherlands, food supply increased before the Industrial Revolution due to better agricultural practices; however, population grew too, as noted by Thomas Malthus. This condition is called the Malthusian trap, and it finally started to overcome by transportation improvements, such as canals, improved roads and steamships. Railroads and steamships were introduced near the end of the Industrial Revolution.\n\nThe very rapid growth in population in the 19th century in the cities included the new industrial and manufacturing cities, as well as service centers such as Edinburgh and London. The critical factor was financing, which was handled by building societies that dealt directly with large contracting firms. Private renting from housing landlords was the dominant tenure. P. Kemp says this was usually of advantage to tenants. People moved in so rapidly that there was not enough capital to build adequate housing for everyone, so low-income newcomers squeezed into increasingly overcrowded slums. Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults. Cholera from polluted water and typhoid were endemic. Unlike rural areas, there were no famines such as devastated Ireland in the 1840s.\n\nA large exposé literature grew up condemning the unhealthy conditions. By far the most famous publication was by one of the founders of the Socialist movement, \"The Condition of the Working Class in England\" in 1844 Friedrich Engels described backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors. These shanty towns had narrow walkways between irregularly shaped lots and dwellings. There were no sanitary facilities. Population density was extremely high. Not everyone lived in such poor conditions. The Industrial Revolution also created a middle class of businessmen, clerks, foremen and engineers who lived in much better conditions.\n\nConditions improved over the course of the 19th century due to new public health acts regulating things such as sewage, hygiene and home construction. In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved. For example, the Public Health Act 1875 led to the more sanitary byelaw terraced house.\n\nIn \"The Condition of the Working Class in England\" in 1844 Friedrich Engels described how untreated sewage created awful odors and turned the rivers green in industrial cities.\n\nIn 1854 John Snow traced a cholera outbreak in Soho to fecal contamination of a public water well by a home cesspit. Snow's findings that cholera could be spread by contaminated water took some years to be accepted, but his work led to fundamental changes in the design of public water and waste systems.\n\nPre-industrial water supply relied on gravity systems and pumping of water was done by water wheels. Pipes were typically made of wood. Steam powered pumps and iron pipes allowed the widespread piping of water to horse watering troughs and households.\n\nThe invention of the paper machine and the application of steam power to the industrial processes of printing supported a massive expansion of newspaper and popular book publishing, which contributed to rising literacy and demands for mass political participation.\n\nConsumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating. Coffee, tea, sugar, tobacco and chocolate became affordable to many in Europe. Watches and household clocks became popular consumer items.\n\nMeeting the demands of the consumer revolution and growth in wealth of the middle classes in Britain, potter and entrepreneur Josiah Wedgwood, founder of Wedgwood fine china and porcelain, created goods such as tableware, which was starting to become a common feature on dining tables.\n\nThe Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income.\n\nAccording to Robert Hughes in \"The Fatal Shore\", the population of England and Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million. Improved conditions led to the population of Britain increasing from 10 million to 40 million in the 1800s. Europe's population increased from about 100 million in 1700 to 400 million by 1900.\n\nThe growth of modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of the world's population lived in cities, compared to nearly 50% today (the beginning of the 21st century). Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.\n\nWomen's historians have debated the effect of the Industrial Revolution and capitalism generally on the status of women. Taking a pessimistic side, Alice Clark argued that when capitalism arrived in 17th-century England, it lowered the status of women as they lost much of their economic importance. Clark argues that in 16th-century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production and women played a vital role in running farms, and in some trades and landed estates. Their useful economic roles gave them a sort of equality with their husbands. However, Clark argues, as capitalism expanded in the 17th century, there was more and more division of labour with the husband taking paid labour jobs outside the home, and the wife reduced to unpaid household work. Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on powerful women.\n\nIn a more positive interpretation, Ivy Pinchbeck argues that capitalism created the conditions for women's emancipation. Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history. In the pre-industrial era, production was mostly for home use and women produce much of the needs of the households. The second stage was the \"family wage economy\" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife and older children. The third or modern stage is the \"family consumer economy,\" in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption.\n\nIdeas of thrift and hard work characterized middle-class families as the Industrial Revolution swept Europe. These values were displayed in Samuel Smiles' book \"Self-Help\", in which he states that the misery of the poorer classes was \"voluntary and self-imposed – the results of idleness, thriftlessness, intemperance, and misconduct.\"\n\nIn terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry. Ordinary working people found increased opportunities for employment in the new mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as the year 1900, most industrial workers in the United States still worked a 10-hour day (12 hours in the steel industry), yet earned from 20% to 40% less than the minimum deemed necessary for a decent life; however, most workers in textiles, which was by far the leading industry in terms of employment, were women and children. For workers of the laboring classes, industrial life \"was a stony desert, which they had to make habitable by their own efforts.\" Also, harsh working conditions were prevalent long before the Industrial Revolution took place. Pre-industrial society was very static and often cruel – child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.\n\nIndustrialisation led to the creation of the factory. The factory system contributed to the growth of urban areas, as large numbers of workers migrated into the cities in search of work in the factories. Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed \"Cottonopolis\", and the world's first industrial city. Manchester experienced a six-times increase in its population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851 and by 1851 only 50% of the population of Bradford was actually born there.\n\nIn addition, between 1815 and 1939, 20 percent of Europe's population left home, pushed by poverty, a rapidly growing population, and the displacement of peasant farming and artisan manufacturing. They were pulled abroad by the enormous demand for labor overseas, the ready availability of land, and cheap transportation. Still, many did not find a satisfactory life in their new homes, leading 7 million of them to return to Europe. This mass migration had large demographic impacts: in 1800, less than one percent of the world population consisted of overseas Europeans and their descendants; by 1930, they represented 11 percent. The Americas felt the brunt of this huge emigration, largely concentrated in the United States.\n\nFor much of the 19th century, production was done in small mills, which were typically water-powered and built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler.\n\nIn other industries, the transition to factory production was not so divisive. Some industrialists themselves tried to improve factory and living conditions for their workers. One of the earliest such reformers was Robert Owen, known for his pioneering efforts in improving conditions for workers at the New Lanark mills, and often regarded as one of the key thinkers of the early socialist movement.\n\nBy 1746 an integrated brass mill was working at Warmley near Bristol. Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods. Housing was provided for workers on site. Josiah Wedgwood and Matthew Boulton (whose Soho Manufactory was completed in 1766) were other prominent early industrialists, who employed the factory system.\n\nThe Industrial Revolution led to a population increase but the chances of surviving childhood did not improve throughout the Industrial Revolution, although \"infant\" mortality rates were reduced markedly. There was still limited opportunity for education and children were expected to work. Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was completely new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children.\n\nChild labour existed before the Industrial Revolution but with the increase in population and education it became more visible. Many children were forced to work in relatively bad conditions for much lower pay than their elders, 10–20% of an adult male's wage. Children as young as four were employed. Beatings and long hours were common, with some child coal miners and hurriers working from 4 am until 5 pm. Conditions were dangerous, with some children killed when they dozed off and fell into the path of the carts, while others died from gas explosions. Many children developed lung cancer and other diseases and died before the age of 25. Workhouses would sell orphans and abandoned children as \"pauper apprentices\", working without wages for board and lodging. Those who ran away would be whipped and returned to their masters, with some masters shackling them to prevent escape. Children employed as mule scavengers by cotton mills would crawl under machinery to pick up cotton, working 14 hours a day, six days a week. Some lost hands or limbs, others were crushed under the machines, and some were decapitated. Young girls worked at match factories, where phosphorus fumes would cause many to develop phossy jaw. Children employed at glassworks were regularly burned and blinded, and those working at potteries were vulnerable to poisonous clay dust.\n\nReports were written detailing some of the abuses, particularly in the coal mines and textile factories, and these helped to popularise the children's plight. The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare.\n\nPoliticians and the government tried to limit child labour by law but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: Children younger than nine were not allowed to work, children were not permitted to work at night, and the work day of youth under the age of 18 was limited to twelve hours. Factory inspectors supervised the execution of the law, however, their scarcity made enforcement difficult. About ten years later, the employment of children and women in mining was forbidden. Although laws such as these decreased the number of child labourers, child labour remained significantly present in Europe and the United States until the 20th century.\n\nThe Industrial Revolution concentrated labour into mills, factories and mines, thus facilitating the organisation of \"combinations\" or trade unions to help advance the interests of working people. The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production. Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production. Skilled workers were hard to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining.\n\nThe main method the unions used to effect change was strike action. Many strikes were painful events for both sides, the unions and the management. In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were still severely restricted. One British newspaper in 1834 described unions as \"the most dangerous institutions that were ever permitted to take root, under shelter of law, in any country...\"\n\nIn 1832, the Reform Act extended the vote in Britain but did not grant universal suffrage. That year six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s. They refused to work for less than ten shillings a week, although by this time wages had been reduced to seven shillings a week and were due to be further reduced to six. In 1834 James Frampton, a local landowner, wrote to the Prime Minister, Lord Melbourne, to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done. James Brine, James Hammett, George Loveless, George's brother James Loveless, George's brother in-law Thomas Standfield, and Thomas's son John Standfield were arrested, found guilty, and transported to Australia. They became known as the Tolpuddle Martyrs. In the 1830s and 1840s, the Chartist movement was the first large-scale organised working class political movement which campaigned for political equality and social justice. Its \"Charter\" of reforms received over three million signatures but was rejected by Parliament without consideration.\n\nWorking people also formed friendly societies and co-operative societies as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen also supported these organisations to improve the conditions of the working class.\n\nUnions slowly overcame the legal restrictions on the right to strike. In 1842, a general strike involving cotton workers and colliers was organised through the Chartist movement which stopped production across Great Britain.\n\nEventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to become the British Labour Party.\n\nThe rapid industrialisation of the English economy cost many craft workers their jobs. The movement started first with lace and hosiery workers near Nottingham and spread to other areas of the textile industry owing to early industrialisation. Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver. Many such unemployed workers, weavers, and others, turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure. The first attacks of the Luddite movement began in 1811. The Luddites rapidly gained popularity, and the British government took drastic measures, using the militia or army to protect industry. Those rioters who were caught were tried and hanged, or transported for life.\n\nUnrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances. Threshing machines were a particular target, and hayrick burning was a popular activity. However, the riots led to the first formation of trade unions, and further pressure for reform.\n\nThe traditional centers of hand textile production such as India, parts of the Middle East and later China could not withstand the competition from machine-made textiles, which over a period of decades destroyed the hand made textile industries and left millions of people without work, many of whom starved.\n\nThe Industrial Revolution also generated an enormous and unprecedented economic division in the world, as measured by the share of manufacturing output.\nCheap cotton textiles increased the demand for raw cotton; previously, it had primarily been consumed in subtropical regions where it was grown, with little raw cotton available for export. Consequently, prices of raw cotton rose. Some cotton had been grown in the West Indies, particularly in Hispaniola, but Haitian cotton production was halted by the Haitian Revolution in 1791. The invention of the cotton gin in 1792 allowed Georgia green seeded cotton to be profitable, leading to the widespread growth of cotton plantations in the United States and Brazil. In 1791 world cotton production was estimated to be 490,000,000 pounds with U.S. production accounting to 2,000,000 pounds. By 1800 U.S. production was 35,000,000 pounds, of which 17,790,000 were exported. In 1945 the U.S. produced seven-eights of the 1,169,600,000 pounds of world production.\n\nThe Americas, particularly the U.S., had labor shortages and high priced labor, which made slavery attractive. America's cotton plantations were highly efficient and profitable, and able to keep up with demand. The U.S. Civil war created a \"cotton famine\" that lead to increased production in other areas of the world, including new colonies in Africa.\n\nThe origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste. The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash. An Alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision.\n\nThe manufactured gas industry began in British cities in 1812–1820. The technique used produced highly toxic effluent that was dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish. Finally, Parliament wrote company charters to regulate toxicity. The industry reached the US around 1850 causing pollution and lawsuits.\n\nIn industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms. Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in Britain in 1898 making it one of the oldest environmental NGOs. It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash, and gritty particles and to empower local authorities to impose their own regulations.\n\nThe Industrial Revolution on Continental Europe came a little later than in Great Britain. In many industries, this involved the application of technology developed in Britain in new places. Often the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities. By 1809, part of the Ruhr Valley in Westphalia was called 'Miniature England' because of its similarities to the industrial areas of England. The German, Russian and Belgian governments all provided state funding to the new industries. In some cases (such as iron), the different availability of resources locally meant that only some aspects of the British technology were adopted.\n\nBelgium was the second country, after Britain, in which the Industrial Revolution took place and the first in continental Europe: Wallonia (French speaking southern Belgium) was the first region to follow the British model successfully. Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Liège and Charleroi. The leader was a transplanted Englishman John Cockerill. His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825.\n\nWallonia exemplified the radical evolution of industrial expansion. Thanks to coal (the French word \"houille\" was coined in Wallonia), the region geared up to become the 2nd industrial power in the world after Britain. But it is also pointed out by many researchers, with its \"Sillon industriel\", 'Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, [...] there was a huge industrial development based on coal-mining and iron-making...'. Philippe Raxhon wrote about the period after 1830: \"It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain.\" \"The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth making town of Ghent.\" Michel De Coster, Professor at the Université de Liège wrote also: \"The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory [...] But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated.\" \n\nWallonia was also the birthplace of a strong Socialist party and strong trade-unions in a particular sociological landscape. At the left, the \"Sillon industriel\", which runs from Mons in the west, to Verviers in the east (except part of North Flanders, in another period of the industrial revolution, after 1920). Even if Belgium is the second industrial country after Britain, the effect of the industrial revolution there was very different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:\n\nThe industrial revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the Early Modern Period, Flanders was characterised by the presence of large urban centres [...] at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 per cent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 per cent in Wallonia, barely 10 per cent in most West European countries, 16 per cent in France and 25 per cent in Britain. Nineteenth century industrialisation did not affect the traditional urban infrastructure, except in Ghent [...] Also, in Wallonia the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 per cent between 1831 and 1910. Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast. During these eighty years the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region. Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal-mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows.\n\nThe industrial revolution in France followed a particular course as it did not correspond to the main model followed by other countries. Notably, most French historians argue France did not go through a clear \"take-off\". Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice Lévy-Leboyer:\n\nBased on its leadership in chemical research in the universities and industrial laboratories, Germany, which was unified in 1871, became dominant in the world's chemical industry in the late 19th century. At first the production of dyes based on aniline was critical.\n\nGermany's political disunity – with three dozen states – and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike the situation in France, the goal was support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts, and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France\n\nDuring the period 1790–1815 Sweden experienced two parallel economic movements: an \"agricultural revolution\" with larger agricultural estates, new crops and farming tools and a commercialisation of farming, and a \"protoindustrialisation\", with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter. This led to economic growth benefiting large sections of the population and leading up to a \"consumption revolution\" starting in the 1820s.\n\nDuring 1815–1850 the protoindustries developed into more specialised and larger industries. This period witnessed increasing regional specialisation with mining in Bergslagen, textile mills in Sjuhäradsbygden and forestry in Norrland. Several important institutional changes took place in this period, such as free and mandatory schooling introduced 1842 (as first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848.\n\nDuring 1850–1890, Sweden experienced a veritable explosion in export, dominated by crops, wood and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873.\n\nDuring 1890–1930, Sweden experienced the second industrial revolution. New industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile.\n\nThe industrial revolution began about 1870 as Meiji period leaders decided to catch up with the West. The government built railroads, improved roads, and inaugurated a land reform programme to prepare the country for further development. It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Foreign government advisors in Meiji Japan).\n\nIn 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the United States to learn western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. The Bank of Japan, founded in 1882, used taxes to fund model steel and textile factories. Education was expanded and Japanese students were sent to study in the west.\n\nModern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.\n\nDuring the late 18th an early 19th centuries when the UK and parts of Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy. The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country of the period.\n\nImportant American technological contributions during the period of the Industrial Revolution were the cotton gin and the development of a system for making interchangeable parts, the latter aided by the development of the milling machine in the US. The development of machine tools and the system of interchangeable parts were the basis for the rise of the US as the world's leading industrial nation in the late 19th century.\n\nOliver Evans invented an automated flour mill in the mid-1780s that used control mechanisms and conveyors so that no labour was needed from the time grain was loaded into the elevator buckets until flour was discharged into a wagon. This is considered to be the first modern materials handling system an important advance in the progress toward mass production.\n\nThe United States originally used horse-powered machinery for small scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s. As a result, industrialisation was concentrated in New England and the Northeastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest.\n\nThomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era, and a significant milestone in the research and development of cotton mills in the future. This mill was designed to use horse power, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years. Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.\n\nIn 1793, Samuel Slater (1768–1835) founded the Slater Mill at Pawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills. Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US (The first was in Hartford, Connecticut, and the second at Watertown, Massachusetts.) The John H. Chafee Blackstone River Valley National Heritage Corridor retraces the history of \"America's Hardest-Working River', the Blackstone. The Blackstone River and its tributaries, which cover more than from Worcester, Massachusetts to Providence, Rhode Island, was the birthplace of America's Industrial Revolution. At its peak over 1100 mills operated in this valley, including Slater's mill, and with it the earliest beginnings of America's Industrial and Technological Development.\n\nMerchant Francis Cabot Lowell from Newburyport, Massachusetts memorised the design of textile machines on his tour of British factories in 1810. Realising that the War of 1812 had ruined his import business but that a demand for domestic finished cloth was emerging in America, on his return to the United States, he set up the Boston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in a public stock offering, one of the first uses of it in the United States. Lowell, Massachusetts, using of canals and 10,000 horsepower delivered by the Merrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution. The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following the Irish Potato Famine, the system had been replaced by poor immigrant labour.\n\nA major U.S. contribution to industrialization was the development of techniques to make interchangeable parts from metal. Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for small firearms. The development work took place at the Federal Arsenals at Springfield Armory and Harpers Ferry Armory. Techniques for precision machining using machine tools included using fixtures to hold the parts in proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy. The milling machine, a fundamental machine tool, is believed to have been invented by Eli Whitney, who was a government contractor who built firearms as part of this program. Another important invention was the Blanchard lathe, invented by Thomas Blanchard. The Blanchard lathe, or pattern tracing lathe, was actually a shaper that could produce copies of wooden gun stocks. The use of machinery and the techniques for producing standardized and interchangeable parts became known as the American system of manufacturing.\n\nPrecision manufacturing techniques made it possible to build machines that mechanized the shoe industry. and the watch industry. The industrialisation of the watch industry started 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.\n\n \nSteel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a \"Second Industrial Revolution\", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Sir Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality. Bessemer steel was being displaced by the open hearth furnace near the end of the 19th century.\n\nThis Second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum (refining and distribution), and, in the 20th century, the automotive industry, and was marked by a transition of technological leadership from Britain to the United States and Germany.\n\nThe increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation.\n\nA new revolution began with electricity and electrification in the electrical industries. The introduction of hydroelectric power generation in the Alps enabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s.\n\nBy the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets.\n\nThe causes of the Industrial Revolution were complicated and remain a topic for debate. Geographic factors include Britain's vast mineral resources. In addition to metal ores, Britain had the highest quality coal reserves known at the time. Britain also had abundant water power and highly productive agriculture. Britain also had numerous seaports and navigable waterways.\n\nSome historians believe the Industrial Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century, although feudalism began to break down after the Black Death of the mid 14th century, followed by other epidemics, until the population reached a low in the 14th century. This created labor shortages and led to falling food prices and a peak in real wages around 1500, after which population growth began reducing wages. Inflation caused by coinage debasement after 1540 followed by precious metals supply increasing from the Americas caused land rents (often long term leases that transferred to heirs on death) to fall in real terms.\n\nThe Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing the farmers who could no longer be self-sufficient in agriculture into cottage industry, for example weaving, and in the longer term into the cities and the newly developed factories. The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are also cited as factors, as is the scientific revolution of the 17th century. A change in marrying patterns to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development.\n\nUntil the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine. However, recent research into the Marketing Era has challenged the traditional, supply-oriented interpretation of the Industrial Revolution.\n\nLewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, much earlier than most estimates. He explains that the model for standardised mass production was the printing press and that \"the archetypal model for the industrial era was the clock\". He also cites the monastic emphasis on order and time-keeping, as well as the fact that medieval cities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine.\n\nThe presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them. Internal tariffs were abolished by Henry VIII of England, they survived in Russia until 1753, 1789 in France and 1839 in Spain.\n\nGovernments' grant of limited monopolies to inventors under a developing patent system (the Statute of Monopolies in 1623) is considered an influential factor. The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of the steam engine, the key enabling technology. In return for publicly revealing the workings of an invention the patent system rewarded inventors such as James Watt by allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development. However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors. Watt's monopoly prevented other inventors, such as Richard Trevithick, William Murdoch, or Jonathan Hornblower, whom Boulton and Watt sued, from introducing improved steam engines, thereby retarding the spread of steam power.\n\nOne question of active interest to historians is why the Industrial Revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China, India, and the Middle East (which pioneered in shipbuilding, textile production, water mills, and much more in the period between 750 and 1100), or at other times like in Classical Antiquity or the Middle Ages. A recent account argued that Europeans have been characterized for thousands of years by a freedom-loving culture originating from the aristocratic societies of early Indo-European invaders. Many historians, however, have challenged this explanation as being not only Eurocentric, but also ignoring historical context. In fact, before the Industrial Revolution, \"there existed something of a global economic parity between the most advanced regions in the world economy.\" These historians have suggested a number of other factors, including education, technological changes (see Scientific Revolution in Europe), \"modern\" government, \"modern\" work attitudes, ecology, and culture.\n\nChina was the world's most technologically advanced country for many centuries; however, China stagnated economically and technologically and was surpassed by Western Europe before the Age of Discovery, by which time China banned imports and denied entry to foreigners. China was also a totalitarian society. China also heavily taxed transported goods. \nModern estimates of per capita income in Western Europe in the late 18th century are of roughly 1,500 dollars in purchasing power parity (and Britain had a per capita income of nearly 2,000 dollars) whereas China, by comparison, had only 450 dollars. India was essentially feudal, politically fragmented and not as economically advanced as Western Europe.\n\nHistorians such as David Landes and sociologist Max Weber and Rodney Stark credit the different belief systems in Asia and Europe with dictating where the revolution occurred. The religion and beliefs of Europe were largely products of Judaeo-Christianity and Greek thought. Conversely, Chinese society was founded on men like Confucius, Mencius, Han Feizi (Legalism), Lao Tzu (Taoism), and Buddha (Buddhism), resulting in very different worldviews. Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigable Yellow River that connects these deposits to the sea.\n\nRegarding India, the Marxist historian Rajani Palme Dutt said: \"The capital to finance the Industrial Revolution in India instead went into financing the Industrial Revolution in Britain.\" In contrast to China, India was split up into many competing kingdoms after the decline of the Mughal Empire, with the major ones in its aftermath including the Marathas, Sikhs, Bengal Subah, and Kingdom of Mysore. In addition, the economy was highly dependent on two sectors – agriculture of subsistence and cotton, and there appears to have been little technical innovation. It is believed that the vast amounts of wealth were largely stored away in palace treasuries by totalitarian monarchs prior to the British take over.\n\nEconomic historian Joel Mokyr argued that political fragmentation (the presence of a large number of European states) made it possible for heterodox ideas to thrive, as entrepreneurs, innovators, ideologues and heretics could easily flee to a neighboring state in the event that the one state would try to suppress their ideas and activities. This is what set Europe apart from the technologically advanced, large unitary empires such as China and India by providing \"an insurance against economic and technological stagnation\". China had both a printing press and movable type, and India had similar levels scientific and technological achievement as Europe in 1700, yet the Industrial Revolution would occur in Europe, not China or India. In Europe, political fragmentation was coupled with an \"integrated market for ideas\" where Europe's intellectuals used the lingua franca of Latin, had a shared intellectual basis in Europe's classical heritage and the pan-European institution of the Republic of Letters.\n\nIn addition, Europe's monarchs desperately needed revenue, pushing them into alliances with their merchant classes. Small groups of merchants were granted monopolies and tax-collecting responsibilities in exchange for payments to the state. Located in a region \"at the hub of the largest and most varied network of exchange in history,\" Europe advanced as the leader of the Industrial Revolution. In the Americas, Europeans found a windfall of silver, timber, fish, and maize, leading historian Peter Stearns to conclude that \"Europe's Industrial Revolution stemmed in great part from Europe's ability to draw disproportionately on world resources.\"\n\nModern capitalism originated in the Italian city-states around the end of the first millennium. The city-states were prosperous cities that were independent from feudal lords. They were largely republics whose governments were typically composed of merchants, manufacturers, members of guilds, bankers and financiers. The Italian city-states built a network of branch banks in leading western European cities and introduced double entry bookkeeping. Italian commerce was supported by schools that taught numeracy in financial calculations through abacus schools.\n\nGreat Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the Industrial Revolution. Key factors fostering this environment were:\nThere were two main values that really drove the Industrial Revolution in Britain. These values were self-interest and an entrepreneurial spirit. Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth and a consumer revolution. These advancements also greatly benefitted the British society as a whole. Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own Industrial Revolutions.\n\nThe debate about the start of the Industrial Revolution also concerns the massive lead that Great Britain had over other countries. Some have stressed the importance of natural or financial resources that Britain received from its many overseas colonies or that profits from the British slave trade between Africa and the Caribbean helped fuel industrial investment. However, it has been pointed out that slave trade and West Indian plantations provided only 5% of the British national income during the years of the Industrial Revolution. Even though slavery accounted for so little, Caribbean-based demand accounted for 12% of Britain's industrial output.\nInstead, the greater liberalisation of trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia. Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by the Royal Navy). Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods. The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical position – an island separated from the rest of mainland Europe.\nAnother theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed. It had a dense population for its small geographical size. Enclosure of common land and the related agricultural revolution made a supply of this labour readily available. There was also a local coincidence of natural resources in the North of England, the English Midlands, South Wales and the Scottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power, resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.\n\nThe stable political situation in Britain from around 1688 following the Glorious Revolution, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution. Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism. (This point is also made in Hilaire Belloc's \"The Servile State\".)\n\nThe French philosopher Voltaire wrote about capitalism and religious tolerance in his book on English society, \"Letters on the English\" (1733), noting why England at that time was more prosperous in comparison to the country's less religiously tolerant European neighbours. \"Take a view of the Royal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind. There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts. There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker's word. If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another's throats; but as there are such a multitude, they all live happy and in peace.\"\n\nBritain's population grew 280% 1550–1820, while the rest of Western Europe grew 50–80%. Seventy percent of European urbanisation happened in Britain 1750–1800. By 1800, only the Netherlands was more urbanised than Britain. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs for fodder while even early steam engines produced four times more mechanical energy.\n\nIn 1700, 5/6 of coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, most urbanised, well paid, literate people and lowest taxes, it failed to industrialise. In the 18th century, it was the only European country whose cities and population shrank. Without coal, Britain would have run out of suitable river sites for mills by the 1830s. Based on science and experimentation from the continent, the steam engine was developed specifically for pumping water out of mines, many of which in Britain had been mined to below the water table. Although extremely inefficient they were economical because they used unsaleable coal. Iron rails were developed to transport coal, which was a major economic sector in Britain.\n\nEconomic historian Robert Allen has argued that high wages, cheap capital and very cheap energy in Britain made it the ideal place for the industrial revolution to occur. These factors made it vastly more profitable to invest in research and development, and to put technology to use in Britain than other societies. However, two 2018 studies in \"The Economic History Review\" showed that wages were not particularly high in the British spinning sector or the construction sector, casting doubt on Allen's explanation.\n\nKnowledge of innovation was spread by several means. Workers who were trained in the technique might move to another employer or might be poached. A common method was for someone to make a study tour, gathering information where he could. During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy. In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods. Study tours were common then, as now, as was the keeping of travel diaries. Records made by industrialists and technicians of the period are an incomparable source of information about their methods.\n\nAnother means for the spread of innovation was by the network of informal philosophical societies, like the Lunar Society of Birmingham, in which members met to discuss 'natural philosophy' (\"i.e.\" science) and often its application to manufacturing. The Lunar Society flourished from 1765 to 1809, and it has been said of them, \"They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth century revolutions, the Industrial Revolution\". Other such societies published volumes of proceedings and transactions. For example, the London-based Royal Society of Arts published an illustrated volume of new inventions, as well as papers about them in its annual \"Transactions\".\n\nThere were publications describing technology. Encyclopaedias such as Harris's \"Lexicon Technicum\" (1704) and Abraham Rees's \"Cyclopaedia\" (1802–1819) contain much of value. \"Cyclopaedia\" contains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings. Foreign printed sources such as the \"Descriptions des Arts et Métiers\" and Diderot's \"Encyclopédie\" explained foreign methods with fine engraved plates.\n\nPeriodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents. Foreign periodicals, such as the \"Annales des Mines\", published accounts of travels made by French engineers who observed British methods on study tours.\n\nAnother theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work. The existence of this class is often linked to the Protestant work ethic (see Max Weber) and the particular status of the Baptists and the dissenting Protestant sects, such as the Quakers and Presbyterians that had flourished with the English Civil War. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in the Glorious Revolution of 1688, and the emergence of a stable financial market there based on the management of the national debt by the Bank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures.\n\nDissenters found themselves barred or discouraged from almost all public offices, as well as education at England's only two universities at the time (although dissenters were still free to study at Scotland's four universities). When the restoration of the monarchy took place and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education. The Unitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences – areas of scholarship vital to the development of manufacturing technologies.\n\nHistorians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in the middle class, such as traditional financiers or other businessmen. Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.\n\nDuring the Industrial Revolution an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement. Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialization, urbanization and the wretchedness of the working classes. Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley. The movement stressed the importance of \"nature\" in art and language, in contrast to \"monstrous\" machines and factories; the \"Dark satanic mills\" of Blake's poem \"And did those feet in ancient time\". Mary Shelley's novel \"Frankenstein\" reflected concerns that scientific progress might be two-edged. French Romanticism likewise was highly critical of industry.\n\n\n\n"}
{"id": "14918", "url": "https://en.wikipedia.org/wiki?curid=14918", "title": "International Court of Justice", "text": "International Court of Justice\n\nThe International Court of Justice (abbreviated ICJ) is the principal judicial organ of the United Nations (UN). It settles legal disputes between member states and gives advisory opinions to authorized UN organs and specialized agencies. It comprises a panel of 15 judges elected by the General Assembly and Security Council for nine-year terms. It is seated in the Peace Palace in The Hague, Netherlands.\n\nEstablished in 1945 by the UN Charter, the court began work in 1946 as the successor to the Permanent Court of International Justice. The Statute of the International Court of Justice, similar to that of its predecessor, is the main constitutional document constituting and regulating the court.\n\nThe court's workload covers a wide range of judicial activity. After the court ruled that the United States's covert war against Nicaragua was in violation of international law (\"Nicaragua v. United States\"), the United States withdrew from compulsory jurisdiction in 1986 to accept the court's jurisdiction only on a case-by-case basis. Chapter XIV of the United Nations Charter authorizes the UN Security Council to enforce Court rulings. However, such enforcement is subject to the veto power of the five permanent members of the Council, which the United States used in the \"Nicaragua\" case.\n\nThe ICJ is composed of fifteen judges elected to nine-year terms by the UN General Assembly and the UN Security Council from a list of people nominated by the national groups in the Permanent Court of Arbitration. The election process is set out in Articles 4–19 of the ICJ statute. Elections are staggered, with five judges elected every three years to ensure continuity within the court. Should a judge die in office, the practice has generally been to elect a judge in a special election to complete the term.\n\nNo two judges may be nationals of the same country. According to Article 9, the membership of the court is supposed to represent the \"main forms of civilization and of the principal legal systems of the world\". Essentially, that has meant common law, civil law and socialist law (now post-communist law).\n\nThere is an informal understanding that the seats will be distributed by geographic regions so that there are five seats for Western countries, three for African states (including one judge of francophone civil law, one of Anglophone common law and one Arab), two for Eastern European states, three for Asian states and two for Latin American and Caribbean states. For most of the court's history, the five permanent members of the United Nations Security Council (France, Russia, China, the United Kingdom, and the United States) have always had a judge serving, thereby occupying three of the Western seats, one of the Asian seats and one of the Eastern European seats. Exceptions have been China not having a judge on the court from 1967 to 1985, during which time it did not put forward a candidate, and British judge Sir Christopher Greenwood being withdrawn as a candidate for election for a second nine-year term on the bench in 2017, leaving no judges from the United Kingdom on the court. Greenwood had been supported by the UN Security Council but failed to get a majority in the UN General Assembly. Indian judge Dalveer Bhandari instead took the seat.\n\nArticle 6 of the Statute provides that all judges should be \"elected regardless of their nationality among persons of high moral character\" who are either qualified for the highest judicial office in their home states or known as lawyers with sufficient competence in international law. Judicial independence is dealt with specifically in Articles 16–18. Judges of the ICJ are not able to hold any other post or act as counsel. In practice, members of the court have their own interpretation of these rules and allow them to be involved in outside arbitration and hold professional posts as long as there is no conflict of interest. A judge can be dismissed only by a unanimous vote of the other members of the court. Despite these provisions, the independence of ICJ judges has been questioned. For example, during the \"Nicaragua\" case, the United States issued a communiqué suggesting that it could not present sensitive material to the court because of the presence of judges from Eastern bloc states.\n\nJudges may deliver joint judgments or give their own separate opinions. Decisions and advisory opinions are by majority, and, in the event of an equal division, the President's vote becomes decisive, which occurred in the \"Legality of the Use by a State of Nuclear Weapons in Armed Conflict\" (Opinion requested by WHO), [1996] ICJ Reports 66. Judges may also deliver separate dissenting opinions.\n\nArticle 31 of the statute sets out a procedure whereby \"ad hoc\" judges sit on contentious cases before the court. The system allows any party to a contentious case (if it otherwise does not have one of that party's nationals sitting on the court) to select one additional person to sit as a judge on that case only. It is thus possible that as many as seventeen judges may sit on one case.\n\nThe system may seem strange when compared with domestic court processes, but its purpose is to encourage states to submit cases. For example, if a state knows that it will have a judicial officer who can participate in deliberation and offer other judges local knowledge and an understanding of the state's perspective, it may be more willing to submit to the jurisdiction of the court. Although this system does not sit well with the judicial nature of the body, it is usually of little practical consequence. \"Ad hoc\" judges usually (but not always) vote in favour of the state that appointed them and thus cancel each other out.\n\nGenerally, the court sits as full bench, but in the last fifteen years, it has on occasion sat as a chamber. Articles 26–29 of the statute allow the court to form smaller chambers, usually 3 or 5 judges, to hear cases. Two types of chambers are contemplated by Article 26: firstly, chambers for special categories of cases, and second, the formation of \"ad hoc\" chambers to hear particular disputes. In 1993, a special chamber was established, under Article 26(1) of the ICJ statute, to deal specifically with environmental matters (although it has never been used).\n\n\"Ad hoc\" chambers are more frequently convened. For example, chambers were used to hear the \"Gulf of Maine Case\" (Canada/US). In that case, the parties made clear they would withdraw the case unless the court appointed judges to the chamber acceptable to the parties. Judgments of chambers may either less authority than full Court judgments or diminish the proper interpretation of universal international law informed by a variety of cultural and legal perspectives. On the other hand, the use of chambers might encourage greater recourse to the court and thus enhance international dispute resolution.\n\n, the composition of the court is as follows:\n\nAs stated in Article 93 of the UN Charter, all UN members are automatically parties to the court's statute. Non-UN members may also become parties to the court's statute under the Article 93(2) procedure. For example, before becoming a UN member state, Switzerland used this procedure in 1948 to become a party, and Nauru became a party in 1988. Once a state is a party to the court's statute, it is entitled to participate in cases before the court. However, being a party to the statute does not automatically give the court jurisdiction over disputes involving those parties. The issue of jurisdiction is considered in the three types of ICJ cases: contentious issues, incidental jurisdiction, and advisory opinions.\n\nIn contentious cases (adversarial proceedings seeking to settle a dispute), the ICJ produces a binding ruling between states that agree to submit to the ruling of the court. Only states may be parties in contentious cases. Individuals, corporations, parts of a federal state, NGOs, UN organs and self-determination groups are excluded from direct participation in cases although the court may receive information from public international organizations. That does not preclude non-state interests from being the subject of proceedings if a state brings the case against another. For example, a state may, in cases of \"diplomatic protection\", bring a case on behalf of one of its nationals or corporations.\n\nJurisdiction is often a crucial question for the court in contentious cases. (See Procedure below.) The key principle is that the ICJ has jurisdiction only on the basis of consent. Article 36 outlines four bases on which the court's jurisdiction may be founded:\n\nUntil rendering a final judgment, the court has competence to order interim measures for the protection of the rights of a party to a dispute. One or both parties to a dispute may apply the ICJ for issuing interim measures. In the \"Frontier Dispute\" Case, both parties to the dispute, Burkina Faso and Mali submitted an application to the court to indicate interim measures. Incidental jurisdiction of the court derives from the Article 41 of the Statute of it. Such as the final judgment, the order for interim measures of the court are binding on state parties to the dispute. The ICJ has competence to indicate interim measures only if the \"prima facie\" jurisdiction is satisfied. \n\n An advisory opinion is a function of the court open only to specified United Nations bodies and agencies. The UN Charter grants the General Assembly or the Security Council a power to request the court to issue an advisory opinion on any legal question. Other organs of the UN rather than GA and SC may not request an advisory opinion of the ICJ unless the General Assembly authorizes them. Other organs of the UN only request an advisory opinion of the court regarding the matters falling into the scope of their activities. On receiving a request, the court decides which states and organizations might provide useful information and gives them an opportunity to present written or oral statements. Advisory opinions were intended as a means by which UN agencies could seek the court's help in deciding complex legal issues that might fall under their respective mandates.\n\nIn principle, the court's advisory opinions are only consultative in character but they are influential and widely respected. Certain instruments or regulations can provide in advance that the advisory opinion shall be specifically binding on particular agencies or states, but inherently, they are non-binding under the Statute of the Court. This non-binding character does not mean that advisory opinions are without legal effect, because the legal reasoning embodied in them reflects the court's authoritative views on important issues of international law. In arriving at them, the court follows essentially the same rules and procedures that govern its binding judgments delivered in contentious cases submitted to it by sovereign states.\n\nAn advisory opinion derives its status and authority from the fact that it is the official pronouncement of the principal judicial organ of the United Nations.\n\nAdvisory opinions have often been controversial because the questions asked are controversial or the case was pursued as an indirect way of bringing what is really a contentious case before the court. Examples of advisory opinions can be found in the section advisory opinions in the List of International Court of Justice cases article. One such well-known advisory opinion is the \"Nuclear Weapons Case\".\n\nArticle 94 establishes the duty of all UN members to comply with decisions of the court involving them. If parties do not comply, the issue may be taken before the Security Council for enforcement action. There are obvious problems with such a method of enforcement. If the judgment is against one of the permanent five members of the Security Council or its allies, any resolution on enforcement would then be vetoed. That occurred, for example, after the \"Nicaragua\" case, when Nicaragua brought the issue of the United States' noncompliance with the court's decision before the Security Council. Furthermore, if the Security Council refuses to enforce a judgment against any other state, there is no method of forcing the state to comply. Furthermore, the most effective form to take action for the Security Council, coercive action under Chapter VII of the United Nations Charter, can be justified only if international peace and security are at stake. The Security Council has never done that so far.\n\nThe relationship between the ICJ and the Security Council, and the separation of their powers, was considered by the court in 1992 in the \"Pan Am\" case. The court had to consider an application from Libya for the order of provisional measures to protect its rights, which, it alleged, were being infringed by the threat of economic sanctions by the United Kingdom and United States. The problem was that these sanctions had been authorized by the Security Council, which resulted in a potential conflict between the Chapter VII functions of the Security Council and the judicial function of the court. The court decided, by eleven votes to five, that it could not order the requested provisional measures because the rights claimed by Libya, even if legitimate under the Montreal Convention, could not be \"prima facie\" regarded as appropriate since the action was ordered by the Security Council. In accordance with Article 103 of the UN Charter, obligations under the Charter took precedence over other treaty obligations. Nevertheless, the court declared the application admissible in 1998. A decision on the merits has not been given since the parties (United Kingdom, United States, and Libya) settled the case out of court in 2003.\n\nThere was a marked reluctance on the part of a majority of the court to become involved in a dispute in such a way as to bring it potentially into conflict with the Council. The court stated in the \"Nicaragua\" case that there is no necessary inconsistency between action by the Security Council and adjudication by the ICJ. However, when there is room for conflict, the balance appears to be in favour of the Security Council.\n\nShould either party fail \"to perform the obligations incumbent upon it under a judgment rendered by the Court\", the Security Council may be called upon to \"make recommendations or decide upon measures\" if the Security Council deems such actions necessary. In practice, the court's powers have been limited by the unwillingness of the losing party to abide by the court's ruling and by the Security Council's unwillingness to impose consequences. However, in theory, \"so far as the parties to the case are concerned, a judgment of the Court is binding, final and without appeal\", and \"by signing the Charter, a State Member of the United Nations undertakes to comply with any decision of the International Court of Justice in a case to which it is a party.\"\n\nFor example, the United States had previously accepted the court's compulsory jurisdiction upon its creation in 1946 but in 1984, after \"Nicaragua v. United States\", withdrew its acceptance following the court's judgment that called on the US to \"cease and to refrain\" from the \"unlawful use of force\" against the government of Nicaragua. The court ruled (with only the American judge dissenting) that the United States was \"in breach of its obligation under the Treaty of Friendship with Nicaragua not to use force against Nicaragua\" and ordered the United States to pay war reparations.\n\n\nWhen deciding cases, the court applies international law as summarized in of the ICJ Statute, which provides that in arriving at its decisions the court shall apply international conventions, international custom and the \"general principles of law recognized by civilized nations.\" It may also refer to academic writing (\"the teachings of the most highly qualified publicists of the various nations\") and previous judicial decisions to help interpret the law although the court is not formally bound by its previous decisions under the doctrine of stare decisis. makes clear that the common law notion of precedent or \"stare decisis\" does not apply to the decisions of the ICJ. The court's decision binds only the parties to that particular controversy. Under 38(1)(d), however, the court may consider its own previous decisions.\n\nIf the parties agree, they may also grant the court the liberty to decide \"ex aequo et bono\" (\"in justice and fairness\"), granting the ICJ the freedom to make an equitable decision based on what is fair under the circumstances. That provision has not been used in the court's history. So far, the International Court of Justice has dealt with about 130 cases.\n\nThe ICJ is vested with the power to make its own rules. Court procedure is set out in the \"Rules of Court of the International Court of Justice 1978\" (as amended on 29 September 2005).\n\nCases before the ICJ will follow a standard pattern. The case is lodged by the applicant, which files a written memorial setting out the basis of the court's jurisdiction and the merits of its claim. The respondent may accept the court's jurisdiction and file its own memorial on the merits of the case.\n\nA respondent that does not wish to submit to the jurisdiction of the court may raise preliminary objections. Any such objections must be ruled upon before the court can address the merits of the applicant's claim. Often, a separate public hearing is held on the preliminary objections and the court will render a judgment. Respondents normally file preliminary objections to the jurisdiction of the court and/or the admissibility of the case. Inadmissibility refers to a range of arguments about factors the court should take into account in deciding jurisdiction, such as the fact that the issue is not justiciable or that it is not a \"legal dispute\".\n\nIn addition, objections may be made because all necessary parties are not before the court. If the case necessarily requires the court to rule on the rights and obligations of a state that has not consented to the court's jurisdiction, the court does not proceed to issue a judgment on the merits.\n\nIf the court decides it has jurisdiction and the case is admissible, the respondent then is required to file a Memorial addressing the merits of the applicant's claim. Once all written arguments are filed, the court holds a public hearing on the merits.\n\nOnce a case has been filed, any party (usually the applicant) may seek an order from the court to protect the \"status quo\" pending the hearing of the case. Such orders are known as Provisional (or Interim) Measures and are analogous to interlocutory injunctions in United States law. Article 41 of the statute allows the court to make such orders. The court must be satisfied to have \"prima facie\" jurisdiction to hear the merits of the case before it grants provisional measures.\n\nIn cases in which a third state's interests are affected, that state may be permitted to intervene in the case and participate as a full party. Under Article 62, a state \"with an interest of a legal nature\" may apply; however, it is within the court's discretion whether or not to allow the intervention. Intervention applications are rare, and the first successful application occurred only in 1991.\n\nOnce deliberation has taken place, the court issues a majority opinion. Individual judges may issue concurring opinions (if they agree with the outcome reached in the judgment of the court but differ in their reasoning) or dissenting opinions (if they disagree with the majority). No appeal is possible, but any party may ask for the court to clarify if there is a dispute as to the meaning or scope of the court's judgment.\n\nThe International Court has been criticized with respect to its rulings, its procedures, and its authority. As with criticisms of the United Nations, many of these criticisms refer more to the general authority assigned to the body by member states through its charter than to specific problems with the composition of judges or their rulings. Major criticisms include the following:\n\n\n\n\n"}
{"id": "14919", "url": "https://en.wikipedia.org/wiki?curid=14919", "title": "International Standard Book Number", "text": "International Standard Book Number\n\nThe International Standard Book Number (ISBN) is a numeric commercial book identifier which is intended to be unique. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.\n\nAn ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an e-book, a paperback and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. The method of assigning an ISBN is nation-based and varies from country to country, often depending on how large the publishing industry is within a country.\n\nThe initial ISBN configuration of recognition was generated in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the SBN code can be converted to a ten-digit ISBN by prefixing it with a zero digit \"0\").\n\nPrivately published books sometimes appear without an ISBN. The International ISBN agency sometimes assigns such books ISBNs on its own initiative.\n\nAnother identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines; and the International Standard Music Number (ISMN) covers for musical scores.\n\nThe Standard Book Numbering (SBN) code is a 9-digit commercial book identifier system created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin, for the booksellers and stationers WHSmith and others in 1965. The ISBN configuration of recognition was generated in 1967 in the United Kingdom by David Whitaker (regarded as the \"Father of the ISBN\") and in 1968 in the United States by Emery Koltay (who later became director of the U.S. ISBN agency R.R. Bowker).\n\nThe 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108. The United Kingdom continued to use the 9-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.\n\nAn SBN may be converted to an ISBN by prefixing the digit \"0\". For example, the second edition of \"Mr. J. G. Reeder Returns\", published by Hodder in 1965, has – 340 indicating the publisher, 01381 their serial number, and 8 being the check digit. This can be converted to ; the check digit does not need to be re-calculated.\n\nSince 1 January 2007, ISBNs have contained 13 digits, a format that is compatible with \"Bookland\" European Article Number EAN-13s.\n\nAn ISBN is assigned to each edition and variation (except reprintings) of a book. For example, an ebook, a paperback, and a hardcover edition of the same book would each have a different ISBN. The ISBN is 13 digits long if assigned on or after 1 January 2007, and 10 digits long if assigned before 2007. An International Standard Book Number consists of 4 parts (if it is a 10-digit ISBN) or 5 parts (for a 13-digit ISBN):\n\n\nA 13-digit ISBN can be separated into its parts (\"prefix element\", \"registration group\", \"registrant\", \"publication\" and \"check digit\"), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (\"registration group\", \"registrant\", \"publication\" and \"check digit\") of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.\n\nISBN is most often used among others special identifiers to describe references in Wikipedia and can help to find the same sources with different description in various language versions (for example different spelling of the title or authors depending on language).\n\nISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.\n\nA full directory of ISBN agencies is available on the International ISBN Agency website. Partial listing:\n\nThe ISBN registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979), and can be separated between hyphens, such as . Registration group identifiers have primarily been allocated within the 978 prefix element. The single-digit group identifiers within the 978-prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–622, 65, 7, 80–94, 950–989, 9920–9989, and 99901–99981. Books published in rare languages typically have longer group identifiers.\n\nWithin the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN. The registration group identifiers within prefix element 979 that have been assigned are 10 for France, 11 for the Republic of Korea, and 12 for Italy.\n\nThe original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.\n\nThe national ISBN agency assigns the registrant element (cf. ) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not required by law to assign an ISBN; however, most bookstores only handle ISBN bearing publications.\n\nA listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes. Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.\n\nPublishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.\n\nBy using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. Here are some sample <nowiki>ISBN-10</nowiki> codes, illustrating block length variations.\nEnglish-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:\n\nA check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the ten-digit code is an extension of that for SBNs, the two systems are compatible, and SBN prefixed with \"0\" will give the same check-digit as without – the digit is base eleven, and can be 0-9 or X. The system for thirteen-digit codes is not compatible and will, in general, give a different check digit from the corresponding 10-digit ISBN, and does not provide the same protection against transposition. This is because the thirteen-digit code was required to be compatible with the EAN format, and hence could not contain an \"X\".\n\nAccording to the 2001 edition of the official manual of the International ISBN Agency, the <nowiki>ISBN-10</nowiki> check digit – which is the last digit of the ten-digit ISBN – must range from 0 to 10 (the symbol X is used for 10), and must be such that the sum of all the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11.\n\nFor example, for an <nowiki>ISBN-10</nowiki> of 0-306-40615-2:\n\nFormally, using modular arithmetic, this is rendered:\n\nIt is also true for <nowiki>ISBN-10</nowiki>'s that the sum of all the ten digits, each multiplied by its weight in \"ascending\" order from 1 to 10, is a multiple of 11. For this example:\n\nFormally, this is rendered:\n\nThe two most common errors in handling an ISBN (e.g., typing or writing it) are a single altered digit or the transposition of adjacent digits. It can be proven that all possible valid <nowiki>ISBN-10</nowiki>s have at least two digits different from each other. It can also be proven that there are no pairs of valid <nowiki>ISBN-10</nowiki>s with eight identical digits and two transposed digits. (These are true only because the ISBN is less than 11 digits long, and because 11 is a prime number.) The ISBN check-digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e., if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error occurs in the publishing house and goes undetected, the book will be issued with an invalid ISBN.\n\nIn contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).\n\nEach of the first nine digits of the ten-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.\n\nFor example, the check digit for an <nowiki>ISBN-10</nowiki> of 0-306-40615-\"?\" is calculated as follows:\nAdding 2 to 130 gives a multiple of 11 (132 = 12 x 11) − this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is <nowiki>ISBN 0-306-40615-2</nowiki>. The value formula_6 required to satisfy this condition might be 10; if so, an 'X' should be used.\n\nAlternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation the calculation could end up with 11 – 0 = 11 which is invalid. (Strictly speaking the \"first\" \"modulo 11\" is unneeded, but it may be considered to simplify the calculation.)\n\nFor example, the check digit for the <nowiki>ISBN-10</nowiki> of 0-306-40615-\"?\" is calculated as follows:\n\nThus the check digit is 2.\n\nIt is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding codice_1 into codice_2 computes the necessary multiples:\n\nThe modular reduction can be done once at the end, as shown above (in which case codice_2 could hold a value as large as 496, for the invalid <nowiki>ISBN 99999-999-9-X</nowiki>), or codice_2 and codice_1 could be reduced by a conditional subtract after each addition.\n\nThe 2005 edition of the International ISBN Agency's official manual describes how the 13-digit ISBN check digit is calculated. The <nowiki>ISBN-13</nowiki> check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.\n\nFormally, using modular arithmetic, this is rendered:\n\nThe calculation of an <nowiki>ISBN-13</nowiki> check digit begins with the first 12 digits of the thirteen-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.\n\nFor example, the <nowiki>ISBN-13</nowiki> check digit of 978-0-306-40615-\"?\" is calculated as follows:\n\nThus, the check digit is 7, and the complete sequence is <nowiki>ISBN 978-0-306-40615-7</nowiki>.\n\nIn general, the <nowiki>ISBN-13</nowiki> check digit is calculated as follows.\n\nLet\n\nThen\n\nThis check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The <nowiki>ISBN-10</nowiki> formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0-9 to express the check digit.\n\nAdditionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).\n\nAn ISBN-10 is converted to ISBN-13 by prepending \"978\" to the ISBN-10 and recalculating the final checksum digit using the ISBN-13 algorithm. The reverse process can also be performed, but not for numbers commencing with a prefix other than 978, which have no 10 digit equivalent.\n\nPublishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers. For example, is shared by two books – \"Ninja gaiden®: a novel based on the best-selling game by Tecmo\" (1990) and \"Wacky laws\" (1997), both published by Scholastic.\n\nMost libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase \"Cancelled ISBN\". However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine. OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.\n\nOnly the term \"ISBN\" should be used; the terms \"eISBN\" and \"e-ISBN\" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic \"eISBN\" which encompasses all the e-book formats for a title.\n\nCurrently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits called an EAN-5 for the currency and the recommended retail price. For 10-digit ISBNs, the number \"978\", the Bookland \"country code\", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN13 formula (modulo 10, 1x and 3x weighting on alternating digits).\n\nPartly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a thirteen-digit ISBN (<nowiki>ISBN-13</nowiki>). The process began 1 January 2005 and was planned to conclude 1 January 2007. , all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. The 10-digit ISMN codes differed visually as they began with an \"M\" letter; the bar code represents the \"M\" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now 13 digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.\n\nPublisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the ten-digit ISBN check digit generally is not the same as the thirteen-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.\n\nBarcode format compatibility is maintained, because (aside from the group breaks) the <nowiki>ISBN-13</nowiki> barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the <nowiki>ISBN-13</nowiki> in North America.\n\n\n"}
{"id": "14921", "url": "https://en.wikipedia.org/wiki?curid=14921", "title": "IP address", "text": "IP address\n\nAn Internet Protocol address (IP address) is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication.\nAn IP address serves two principal functions: host or network interface identification and location addressing.\n\nInternet Protocol version 4 (IPv4) defines an IP address as a 32-bit number. However, because of the growth of the Internet and the depletion of available IPv4 addresses, a new version of IP (IPv6), using 128 bits for the IP address, was developed in 1995, and standardized in . In , a final definition of the protocol was published. IPv6 deployment has been ongoing since the mid-2000s.\n\nIP addresses are usually written and displayed in human-readable notations, such as in IPv4, and in IPv6. The size of the routing prefix of the address is designated in CIDR notation by suffixing the address with the number of significant bits, e.g., , which is equivalent to the historically used subnet mask .\n\nThe IP address space is managed globally by the Internet Assigned Numbers Authority (IANA), and by five regional Internet registries (RIRs) responsible in their designated territories for assignment to end users and local Internet registries, such as Internet service providers. IPv4 addresses have been distributed by IANA to the RIRs in blocks of approximately 16.8 million addresses each. Each ISP or private network administrator assigns an IP address to each device connected to its network. Such assignments may be on a \"static\" (fixed or permanent) or \"dynamic\" basis, depending on its software and practices.\n\nAn IP address serves two principal functions. It identifies the host, or more specifically its network interface, and it provides the location of the host in the network, and thus the capability of establishing a path to that host. Its role has been characterized as follows: \"A name indicates what we seek. An address indicates where it is. A route indicates how to get there.\"\nThe header of each IP packet contains the IP address of the sending host, and that of the destination host.\n\nTwo versions of the Internet Protocol are in common use in the Internet today. The original version of the Internet Protocol that was first deployed in 1983 in the ARPANET, the predecessor of the Internet, is Internet Protocol version 4 (IPv4).\n\nThe rapid exhaustion of IPv4 address space available for assignment to Internet service providers and end user organizations by the early 1990s, prompted the Internet Engineering Task Force (IETF) to explore new technologies to expand the addressing capability in the Internet. The result was a redesign of the Internet Protocol which became eventually known as Internet Protocol Version 6 (IPv6) in 1995.\nIPv6 technology was in various testing stages until the mid-2000s, when commercial production deployment commenced.\n\nToday, these two versions of the Internet Protocol are in simultaneous use. Among other technical changes, each version defines the format of addresses differently. Because of the historical prevalence of IPv4, the generic term \"IP address\" typically still refers to the addresses defined by IPv4. The gap in version sequence between IPv4 and IPv6 resulted from the assignment of version 5 to the experimental Internet Stream Protocol in 1979, which however was never referred to as IPv5.\n\nIP networks may be divided into subnetworks in both IPv4 and IPv6. For this purpose, an IP address is recognized as consisting of two parts: the \"network prefix\" in the high-order bits and the remaining bits called the \"rest field\", \"host identifier\", or \"interface identifier\" (IPv6), used for host numbering within a network. The subnet mask or CIDR notation determines how the IP address is divided into network and host parts.\n\nThe term \"subnet mask\" is only used within IPv4. Both IP versions however use the CIDR concept and notation. In this, the IP address is followed by a slash and the number (in decimal) of bits used for the network part, also called the \"routing prefix\". For example, an IPv4 address and its subnet mask may be and , respectively. The CIDR notation for the same IP address and subnet is , because the first 24 bits of the IP address indicate the network and subnet.\n\nAn IPv4 address has a size of 32 bits, which limits the address space to (2) addresses. Of this number, some addresses are reserved for special purposes such as private networks (~18 million addresses) and multicast addressing (~270 million addresses).\n\nIPv4 addresses are usually represented in dot-decimal notation, consisting of four decimal numbers, each ranging from 0 to 255, separated by dots, e.g., . Each part represents a group of 8 bits (an octet) of the address. In some cases of technical writing, IPv4 addresses may be presented in various hexadecimal, octal, or binary representations.\n\nIn the early stages of development of the Internet Protocol, the network number was always the highest order octet (most significant eight bits). Because this method allowed for only 256 networks, it soon proved inadequate as additional networks developed that were independent of the existing networks already designated by a network number. In 1981, the addressing specification was revised with the introduction of classful network architecture.\n\nClassful network design allowed for a larger number of individual network assignments and fine-grained subnetwork design. The first three bits of the most significant octet of an IP address were defined as the \"class\" of the address. Three classes (\"A\", \"B\", and \"C\") were defined for universal unicast addressing. Depending on the class derived, the network identification was based on octet boundary segments of the entire address. Each class used successively additional octets in the network identifier, thus reducing the possible number of hosts in the higher order classes (\"B\" and \"C\"). The following table gives an overview of this now obsolete system.\n\nClassful network design served its purpose in the startup stage of the Internet, but it lacked scalability in the face of the rapid expansion of networking in the 1990s. The class system of the address space was replaced with Classless Inter-Domain Routing (CIDR) in 1993. CIDR is based on variable-length subnet masking (VLSM) to allow allocation and routing based on arbitrary-length prefixes. Today, remnants of classful network concepts function only in a limited scope as the default configuration parameters of some network software and hardware components (e.g. netmask), and in the technical jargon used in network administrators' discussions.\n\nEarly network design, when global end-to-end connectivity was envisioned for communications with all Internet hosts, intended that IP addresses be globally unique. However, it was found that this was not always necessary as private networks developed and public address space needed to be conserved.\n\nComputers not connected to the Internet, such as factory machines that communicate only with each other via TCP/IP, need not have globally unique IP addresses. Today, such private networks are widely used and typically connect to the Internet with network address translation (NAT), when needed.\n\nThree non-overlapping ranges of IPv4 addresses for private networks are reserved. These addresses are not routed on the Internet and thus their use need not be coordinated with an IP address registry. Any user may use any of the reserved blocks. Typically, a network administrator will divide a block into subnets; for example, many home routers automatically use a default address range of through ().\n\nIn IPv6, the address size was increased from 32 bits in IPv4 to 128 bits, thus providing up to 2 (approximately ) addresses. This is deemed sufficient for the foreseeable future.\n\nThe intent of the new design was not to provide just a sufficient quantity of addresses, but also redesign routing in the Internet by allowing more efficient aggregation of subnetwork routing prefixes. This resulted in slower growth of routing tables in routers. The smallest possible individual allocation is a subnet for 2 hosts, which is the square of the size of the entire IPv4 Internet. At these levels, actual address utilization ratios will be small on any IPv6 network segment. The new design also provides the opportunity to separate the addressing infrastructure of a network segment, i.e. the local administration of the segment's available space, from the addressing prefix used to route traffic to and from external networks. IPv6 has facilities that automatically change the routing prefix of entire networks, should the global connectivity or the routing policy change, without requiring internal redesign or manual renumbering.\n\nThe large number of IPv6 addresses allows large blocks to be assigned for specific purposes and, where appropriate, to be aggregated for efficient routing. With a large address space, there is no need to have complex address conservation methods as used in CIDR.\n\nAll modern desktop and enterprise server operating systems include native support for the IPv6 protocol, but it is not yet widely deployed in other devices, such as residential networking routers, voice over IP (VoIP) and multimedia equipment, and some networking hardware.\n\nJust as IPv4 reserves addresses for private networks, blocks of addresses are set aside in IPv6. In IPv6, these are referred to as unique local addresses (ULAs). The routing prefix is reserved for this block, which is divided into two blocks with different implied policies. The addresses include a 40-bit pseudorandom number that minimizes the risk of address collisions if sites merge or packets are misrouted.\n\nEarly practices used a different block for this purpose (), dubbed site-local addresses. However, the definition of what constituted a \"site\" remained unclear and the poorly-defined addressing policy created ambiguities for routing. This address type was abandoned and must not be used in new systems.\n\nAddresses starting with , called Link-local addresses, are assigned to interfaces for communication on the attached link. The addresses are automatically generated by the operating system for each network interface. This provides instant and automatic communication between all IPv6 host on a link. This feature is used in the lower layers of IPv6 network administration, such as for the Neighbor Discovery Protocol.\n\nPrivate and link-local address prefixes may not be routed on the public Internet.\n\nIP addresses are assigned to a host either dynamically at the time of booting, or permanently by fixed configuration of the host hardware or software. Persistent configuration is also known as using a \"static IP address\". In contrast, when a computer's IP address is assigned newly each time it restarts, this is known as using a \"dynamic IP address\".\n\nThe configuration of a static IP address depends in detail on the software or hardware installed in the computer. Computers used for the network infrastructure, such as routers and mail servers, are typically configured with static addressing, Static addresses are also sometimes convenient for locating servers inside an enterprise.\n\nDynamic IP addresses are assigned using methods such as Zeroconf for self-configuration, or by the Dynamic Host Configuration Protocol (DHCP) from a network server. The address assigned with DHCP usually has an expiration period, after which the address may be assigned to another device, or to the originally associated host if it is still powered up. A network administrator may implement a DHCP method so that the same host always receives a specific address.\n\nDHCP is the most frequently used technology for assigning addresses. It avoids the administrative burden of assigning specific static addresses to each device on a network. It also allows devices to share the limited address space on a network if only some of them are online at a particular time. Typically, dynamic IP configuration is enabled by default in modern desktop operating systems. DHCP is not the only technology used to assign IP addresses dynamically. Dialup and some broadband networks use dynamic address features of the Point-to-Point Protocol.\n\nIn the absence or failure of static or stateful (DHCP) address configurations, an operating system may assign an IP address to a network interface using stateless auto-configuration methods, such as Zeroconf.\n\nA \"sticky dynamic IP address\" is an informal term used by cable and DSL Internet access subscribers to describe a dynamically assigned IP address which seldom changes. The addresses are usually assigned with DHCP. Since the modems are usually powered on for extended periods of time, the address leases are usually set to long periods and simply renewed. If a modem is turned off and powered up again before the next expiration of the address lease, it often receives the same IP address.\n\nAddress block is defined for the special use in link-local addressing for IPv4 networks.\nIn IPv6, every interface, whether using static or dynamic address assignments, also receives a link-local address automatically in the block .\n\nThese addresses are only valid on the link, such as a local network segment or point-to-point connection, that a host is connected to. These addresses are not routable and like private addresses cannot be the source or destination of packets traversing the Internet.\n\nWhen the link-local IPv4 address block was reserved, no standards existed for mechanisms of address autoconfiguration. Filling the void, Microsoft created an implementation that is called Automatic Private IP Addressing (APIPA). APIPA has been deployed on millions of machines and has, thus, become a de facto standard in the industry.\nMany years later, in May 2005, the IETF defined a formal standard for it.\n\nAn IP address conflict occurs when two devices on the same local physical or wireless network claim to have the same IP address. A second assignment of an address generally stops the IP functionality of one or both of the devices. Many modern operating systems notify the administrator of IP address conflicts. If one of the devices is the gateway, the network will be crippled. When IP addresses are assigned by multiple people and systems with differing methods, any of them may be at fault.\n\nIP addresses are classified into several classes of operational characteristics: unicast, multicast, anycast and broadcast addressing.\n\nThe most common concept of an IP address is in unicast addressing, available in both IPv4 and IPv6. It normally refers to a single sender or a single receiver, and can be used for both sending and receiving. Usually, a unicast address is associated with a single device or host, but a device or host may have more than one unicast address. Some individual PCs have several distinct unicast addresses, each for its own distinct purpose. Sending the same data to multiple unicast addresses requires the sender to send all the data many times over, once for each recipient.\n\nBroadcasting is an addressing technique available in IPv4 to send data to all possible destinations on a network in one transmission operation, while all receivers capture the network packet (\"all-hosts broadcast\"). The address is used for network broadcast. In addition, a directed (limited) broadcast uses the all-ones host address with the network prefix. For example, the destination address used for directed broadcast to devices on the network is .\n\nIPv6 does not implement broadcast addressing, and replaces it with multicast to the specially-defined all-nodes multicast address.\n\nA multicast address is associated with a group of interested receivers. In IPv4, addresses through (the former Class D addresses) are designated as multicast addresses. IPv6 uses the address block with the prefix for multicast applications. In either case, the sender sends a single datagram from its unicast address to the multicast group address and the intermediary routers take care of making copies and sending them to all receivers that have joined the corresponding multicast group.\n\nLike broadcast and multicast, anycast is a one-to-many routing topology. However, the data stream is not transmitted to all receivers, just the one which the router decides is logically closest in the network. Anycast address is an inherent feature of only IPv6. In IPv4, anycast addressing implementations typically operate using the shortest-path metric of BGP routing and do not take into account congestion or other attributes of the path. Anycast methods are useful for global load balancing and are commonly used in distributed DNS systems.\n\nA host may use geolocation software to deduce the geolocation of its communicating peer.\nA public IP address, in common parlance, is a globally routable unicast IP address, meaning that the address is not an address reserved for use in private networks, such as those reserved by , or the various IPv6 address formats of local scope or site-local scope, for example for link-local addressing. Public IP addresses may be used for communication between hosts on the global Internet.\n\nFor security and privacy considerations, network administrators often desire to restrict public Internet traffic within their private networks. The source and destination IP addresses contained in the headers of each IP packet are a convenient means to discriminate traffic by IP address blocking or by selectively tailoring responses to external requests to internal servers. This is achieved with firewall software running on the networks gateway router. A database of IP addresses of permissible traffic may be maintained in blacklists or whitelists.\n\nMultiple client devices can appear to share an IP address, either because they are part of a shared hosting web server environment or because an IPv4 network address translator (NAT) or proxy server acts as an intermediary agent on behalf of the client, in which case the real originating IP address might be masked from the server receiving a request. A common practice is to have a NAT mask a large number of devices in a private network. Only the \"outside\" interface(s) of the NAT needs to have an Internet-routable address.\n\nCommonly, the NAT device maps TCP or UDP port numbers on the side of the larger, public network to individual private addresses on the masqueraded network.\n\nIn residential networks, NAT functions are usually implemented in a residential gateway. In this scenario, the computers connected to the router have private IP addresses and the router has a public address on its external interface to communicate on the Internet. The internal computers appear to share one public IP address.\n\nComputer operating systems provide various diagnostic tools to examine their network interface and address configuration. Windows provides the command-line interface tools ipconfig and netsh and users of Unix-like systems can use ifconfig, netstat, route, lanstat, fstat, or iproute2 utilities to accomplish the task.\n"}
{"id": "14922", "url": "https://en.wikipedia.org/wiki?curid=14922", "title": "If and only if", "text": "If and only if\n\nIn logic and related fields such as mathematics and philosophy, if and only if (shortened iff) is a biconditional logical connective between statements.\n\nIn that it is biconditional (a statement of material equivalence), the connective can be likened to the standard material conditional (\"only if\", equal to \"if ... then\") combined with its reverse (\"if\"); hence the name. The result is that the truth of either one of the connected statements requires the truth of the other (i.e. either both statements are true, or both are false). It is controversial whether the connective thus defined is properly rendered by the English \"if and only if\", with its pre-existing meaning.\n\nIn writing, phrases commonly used, with debatable propriety, as alternatives to P \"if and only if\" Q include: \"Q is necessary and sufficient for P\", \"P is equivalent (or materially equivalent) to Q\" (compare material implication), \"P precisely if Q\", \"P precisely (or exactly) when Q\", \"P exactly in case Q\", and \"P just in case Q\". Some authors regard \"iff\" as unsuitable in formal writing; others use it freely.\n\nIn logical formulae, logical symbols are used instead of these phrases; see the discussion of notation.\n\nThe truth table of \"P\" formula_1 \"Q\" is as follows:\nIt is equivalent to that produced by the XNOR gate, and opposite to that produced by the XOR gate.\n\nThe corresponding logical symbols are \"↔\", \"formula_1\", and \"≡\", and sometimes \"iff\". These are usually treated as equivalent. However, some texts of mathematical logic (particularly those on first-order logic, rather than propositional logic) make a distinction between these, in which the first, ↔, is used as a symbol in logic formulas, while ⇔ is used in reasoning about those logic formulas (e.g., in metalogic). In Łukasiewicz's notation, it is the prefix symbol 'E'.\n\nAnother term for this logical connective is exclusive nor.\n\nIn TeX \"if and only if\" is shown as a long double arrow: formula_3 via command \\iff.\n\nIn most logical systems, one proves a statement of the form \"P iff Q\" by proving \"if P, then Q\" and \"if Q, then P\". Proving this pair of statements sometimes leads to a more natural proof since there are not obvious conditions in which one would infer a biconditional directly. An alternative is to prove the disjunction \"(P and Q) or (not-P and not-Q)\", which itself can be inferred directly from either of its disjuncts—that is, because \"iff\" is truth-functional, \"P iff Q\" follows if P and Q have both been shown true, or both false.\n\nUsage of the abbreviation \"iff\" first appeared in print in John L. Kelley's 1955 book \"General Topology\".\nIts invention is often credited to Paul Halmos, who wrote \"I invented 'iff,' for 'if and only if'—but I could never believe I was really its first inventor.\" \n\nIt is somewhat unclear how \"iff\" was meant to be pronounced. In current practice, the single 'word' \"iff\" is almost always read as the four words \"if and only if\". However, in the preface of \"General Topology\", Kelley suggests that it should be read differently: \"In some cases where mathematical content requires 'if and only if' and euphony demands something less I use Halmos' 'iff'\". The authors of one discrete mathematics textbook suggest: \"Should you need to pronounce iff, really hang on to the 'ff' so that people hear the difference from 'if'\", implying that \"iff\" could be pronounced as .\n\nTechnically, definitions are always \"if and only if\" statements; many texts such as Kelley's \"General Topology\" follow the strict demands of logic, and use \"if and only if\" or \"iff\" in definitions of new terms (for instance, from \"General Topology\", p. 25: \"A set is countable iff it is finite or countably infinite\" [boldface in original]). However, this usage of \"if and only if\" is not universal; often, mathematical definitions follow the special convention that \"if\" is interpreted to mean \"if and only if\" (for example, one might say, \"A topological space is compact if every open cover has a finite subcover\").\n\n\nSufficiency is the converse of necessity. That is to say, given \"P\"→\"Q\" (i.e. if \"P\" then \"Q\"), \"P\" would be a sufficient condition for \"Q\", and \"Q\" would be a necessary condition for \"P\". Also, given \"P\"→\"Q\", it is true that \"¬Q\"→\"¬P\" (where ¬ is the negation operator, i.e. \"not\"). This means that the relationship between \"P\" and \"Q\", established by \"P\"→\"Q\", can be expressed in the following, all equivalent, ways:\nAs an example, take (1), above, which states \"P\"→\"Q\", where \"P\" is \"the fruit in question is an apple\" and \"Q\" is \"Madison will eat the fruit in question\". The following are four equivalent ways of expressing this very relationship:\nSo we see that (2), above, can be restated in the form of \"if...then\" as \"If Madison will eat the fruit in question, then it is an apple\"; taking this in conjunction with (1), we find that (3) can be stated as \"If the fruit in question is an apple, then Madison will eat it; \"and\" if Madison will eat the fruit, then it is an apple\".\n\nEuler diagrams show logical relationships among events, properties, and so forth. \"P only if Q\", \"if P then Q\", and \"P→Q\" all mean that P is a subset, either proper or improper, of Q. \"P if Q\", \"if Q then P\", and Q→P all mean that Q is a proper or improper subset of P. \"P if and only if Q\" and \"Q if and only if P\" both mean that the sets P and Q are identical to each other.\n\nIff is used outside the field of logic. Wherever logic is applied, especially in mathematical discussions, it has the same meaning as above: it is an abbreviation for \"if and only if\", indicating that one statement is both necessary and sufficient for the other. This is an example of mathematical jargon. (However, as noted above, \"if\", rather than \"iff\", is more often used in statements of definition.)\n\nThe elements of \"X\" are \"all and only\" the elements of \"Y\" is used to mean: \"for any \"z\" in the domain of discourse, \"z\" is in \"X\" if and only if \"z\" is in \"Y\".\"\n\n\n"}
{"id": "14923", "url": "https://en.wikipedia.org/wiki?curid=14923", "title": "IP", "text": "IP\n\nIP may refer to:\n\n\n\n\n\n\n"}
{"id": "14926", "url": "https://en.wikipedia.org/wiki?curid=14926", "title": "List of Italian dishes", "text": "List of Italian dishes\n\nThis is a list of Italian dishes and foods. Italian cuisine has developed through centuries of social and political changes, with roots as far back as the 4th century BC. Italian cuisine has its origins in Etruscan, ancient Greek, and ancient Roman cuisines.\nSignificant changes occurred with the discovery of the New World and the introduction of potatoes, tomatoes, bell peppers and maize, now central to the cuisine but not introduced in quantity until the 18th century. The cuisine of Italy is noted for its regional diversity, abundance of difference in taste, and is known to be one of the most popular in the world, with influences abroad.\n\nPizza and spaghetti, both associated with the Neapolitan traditions of cookery, are especially popular abroad, but the varying geographical conditions of the twenty regions of Italy, together with the strength of local traditions, afford a wide range of dishes.\n\nThe cuisine of Italy has many unique dishes and foods.\n\n\n\n\nRice (\"riso\") dishes are very common in Northern Italy, especially in the Lombardia and Veneto regions, though rice dishes are found throughout the country.\n\n\n\n\n\n\nFeast of the Seven Fishes\n\n\n\n\n\n\n\n\nTuscan bread specialties\n\n\n\nSpecialties of the Norcineria (Umbrian Butcher)\n\nUnique ham and sausage specialties\n\n\n\n\n\nApulian bread specialties\n\n\n\n\nMost important ingredients (see also Italian Herbs and Spices):\n\nOther common ingredients:\n\n"}
{"id": "14928", "url": "https://en.wikipedia.org/wiki?curid=14928", "title": "Isaac Ambrose", "text": "Isaac Ambrose\n\nIsaac Ambrose (1604 – 20 January 1664) was an English Puritan divine. He graduated with a BA. from Brasenose College, Oxford, on 1624. He obtained the curacy of St Edmund’s Church, Castleton, Derbyshire, in 1627. He was one of king's four preachers in Lancashire in 1631. He was twice imprisoned by commissioners of array. He worked for establishment of Presbyterianism; successively at Leeds, Preston, and Garstang, whence he was ejected for nonconformity in 1662. He also published religious works.\n\nAmbrose was born in 1604. He was the son of Richard Ambrose, vicar of Ormskirk, and was probably descended from the Ambroses of Lowick in Furness, a well-known Roman Catholic family. He entered Brasenose College, Oxford, in 1621, in his seventeenth year.\n\nHaving graduated B.A. in 1624 and been ordained, Ambroses received in 1627 the little cure of Castleton in Derbyshire. By the influence of William Russell, earl of Bedford, he was appointed one of the king's itinerant preachers in Lancashire, and after living for a time in Garstang, he was selected by the Lady Margaret Hoghton as vicar of Preston. He associated himself with Presbyterianism, and was on the celebrated committee for the ejection of \"scandalous and ignorant ministers and schoolmasters\" during the Commonwealth.\n\nSo long as Ambrose continued at Preston he was favoured with the warm friendship of the Hoghton family, their ancestral woods and the tower near Blackburn affording him sequestered places for those devout meditations and \"experiences\" that give such a charm to his diary, portions of which are quoted in his \"Prima Media\" and \"Ultima\" (1650, 1659). The immense auditory of his sermon (\"Redeeming the Time\") at the funeral of Lady Hoghton was long a living tradition all over the county. On account of the feeling engendered by the civil war Ambrose left his great church of Preston in 1654, and became minister of Garstang, whence, however, in 1662 he was ejected along two thousand ministers who refused to conform (see Great Ejection). His after years were passed among old friends and in quiet meditation at Preston. He died of apoplexy about 20 January 1664.\n\nAs a religious writer Ambrose has a vividness and freshness of imagination possessed by scarcely any of the Puritan Nonconformists. Many who have no love for Puritan doctrine, nor sympathy with Puritan experience, have appreciated the pathos and beauty of his writings, and his \"Looking unto Jesus\" long held its own in popular appreciation with the writings of John Bunyan.\n\nDr Edmund Calamy the Elder (1600–1666) wrote about him:\nIn the opinion of John Eglington Bailey (his biographer in the DNB), his character has been misrepresented by Wood. He was of a peaceful disposition; and though he put his name to the fierce \"Harmonious Consent\", he was not naturally a partisan. He evaded the political controversies of the time. His gentleness of character and earnest presentation of the gospel attached him to his people. He was much given to secluding himself, retiring every May into the woods of Hoghton Tower and remaining there a month.\n\nBailey continues that Dr. Halley justly characterises him as the most meditative puritan of Lancashire. This quality pervades his writings, which abound, besides, in deep feeling and earnest piety. Mr. Hunter has called attention to his recommendation of diaries as a means of advancing personal piety, and has remarked, in reference to the fragments from Ambrose's diary quoted in the \"Media\", that \"with such passages before us we cannot but lament that the carelessness of later times should have suffered such a curious and valuable document to perish; for perished it is to be feared it has\".\n\n"}
{"id": "14933", "url": "https://en.wikipedia.org/wiki?curid=14933", "title": "International Convention for the Regulation of Whaling", "text": "International Convention for the Regulation of Whaling\n\nThe International Convention for the Regulation of Whaling is an international environmental agreement signed in 1946 in order to \"provide for the proper conservation of whale stocks and thus make possible the orderly development of the whaling industry\". It governs the commercial, scientific, and aboriginal subsistence whaling practices of eighty-nine member nations.\n\nIt was signed by 15 nations in Washington, D.C. on 2 December 1946 and took effect on 10 November 1948. Its protocol (which represented the first substantial revision of the convention and extended the definition of a \"whale-catcher\" to include helicopters as well as ships) was signed in Washington on 19 November 1956. The convention is a successor to the International Agreement for the Regulation of Whaling, signed in London on 8 June 1937, and the protocols for that agreement signed in London on 24 June 1938, and 26 November 1945.\n\nThe objectives of the agreement are the protection of all whale species from overhunting, the establishment of a system of international regulation for the whale fisheries to ensure proper conservation and development of whale stocks, and safeguarding for future generations the great natural resources represented by whale stocks. The primary instrument for the realization of these aims is the International Whaling Commission which was established pursuant to this convention. The commission has made many revisions to the schedule that makes up the bulk of the convention. The Commission process has also reserved for governments the right to carry out scientific research which involves killing of whales.\n\nThere have been consistent disagreement over the scope of the convention. According to the IWC:\nThe 1946 Convention does not define a 'whale', although a list of names in a number of languages was annexed to the Final Act of the Convention. Some Governments take the view that the IWC has the legal competence to regulate catches of only these named great whales (the baleen whales and the sperm whale). Others believe that all cetaceans, including the smaller dolphins and porpoises, also fall within IWC jurisdiction.\nAs of January 2014, membership consists of 89 states of the world. The initial signatory states were Argentina, Australia, Brazil, Canada, Chile, Denmark, France, the Netherlands, New Zealand, Norway, Peru, South Africa, the Soviet Union, the United Kingdom and the United States.\n\nAlthough Norway is a party to the convention, it maintains an objection to paragraph 10(e) of the convention (the section referring to the 1986 moratorium). Therefore, that provision is not binding upon Norway and the 1986 IWC global moratorium does not apply to it.\n\nAs of January 2014, eight states that were formerly parties to the convention have withdrawn by denouncing it. These states are Canada (which withdrew on 30 June 1982), Egypt, Greece, Jamaica, Mauritius, Philippines, the Seychelles and Venezuela. Belize, Brazil, Dominica, Ecuador, Iceland, Japan, New Zealand, and Panama have all withdrawn from the convention for a period of time after ratification but subsequently have ratified it a second time. The Netherlands, Norway, and Sweden have all withdrawn from the convention twice, only to have accepted it a third time.\n\n"}
{"id": "14934", "url": "https://en.wikipedia.org/wiki?curid=14934", "title": "International Organization for Standardization", "text": "International Organization for Standardization\n\nThe International Organization for Standardization (ISO ) is an international standard-setting body composed of representatives from various national standards organizations.\n\nFounded on 23 February 1947, the organization promotes worldwide proprietary, industrial and commercial standards. It is headquartered in Geneva, Switzerland, and works in 164 countries.\n\nIt was one of the first organizations granted general consultative status with the United Nations Economic and Social Council.\n\nThe International Organization for Standardization is an independent, non-governmental organization, the members of which are the standards organizations of the 164 member countries. It is the world's largest developer of voluntary international standards and facilitates world trade by providing common standards between nations. Over twenty thousand standards have been set covering everything from manufactured products and technology to food safety, agriculture and healthcare.\n\nUse of the standards aids in the creation of products and services that are safe, reliable and of good quality. The standards help businesses increase productivity while minimizing errors and waste. By enabling products from different markets to be directly compared, they facilitate companies in entering new markets and assist in the development of global trade on a fair basis. The standards also serve to safeguard consumers and the end-users of products and services, ensuring that certified products conform to the minimum standards set internationally.\n\nThe three official languages of the ISO are English, French, and Russian.\n\nThe name of the organization in French is ', and in Russian, ('). \"ISO\" is not an acronym. The organization adopted \"ISO\" as its abbreviated name in reference to the Greek word \"\" (, meaning \"equal\"), as its name in the three official languages would have different acronyms. During the founding meetings of the new organization, the Greek word explanation was not invoked, so this meaning may have been made public later.\n\nISO gives this explanation of the name: \"Because 'International Organization for Standardization' would have different acronyms in different languages (IOS in English, OIN in French), our founders decided to give it the short form \"ISO\". \"ISO\" is derived from the Greek \"\", meaning equal. Whatever the country, whatever the language, the short form of our name is always \"ISO\".\"\n\nBoth the name \"ISO\" and the ISO logo are registered trademarks, and their use is restricted.\n\nThe organization today known as ISO began in 1926 as the International Federation of the National Standardizing Associations (ISA). It was suspended in 1942 during World War II, but after the war ISA was approached by the recently formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body. In October 1946, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the new International Organization for Standardization; the new organization officially began operations in February 1947.\n\nISO is a voluntary organization whose members are recognized authorities on standards, each one representing one country. Members meet annually at a General Assembly to discuss ISO's strategic objectives. The organization is coordinated by a Central Secretariat based in Geneva.\n\nA Council with a rotating membership of 20 member bodies provides guidance and governance, including setting the Central Secretariat's annual budget.\n\nThe Technical Management Board is responsible for over 250 technical committees, who develop the ISO standards.\n\nISO has formed two joint committees with the International Electrotechnical Commission (IEC) to develop standards and terminology in the areas of electrical and electronic related technologies.\n\nISO/IEC Joint Technical Committee 1 (JTC 1) was created in 1987 to \"[d]evelop, maintain, promote and facilitate IT standards\", where IT refers to information technology.\n\nISO/IEC Joint Technical Committee 2 (JTC 2) was created in 2009 for the purpose of \"[s]tandardization in the field of energy efficiency and renewable energy sources\".\n\nISO has 163 national members.\n\nISO has three membership categories:\n\nParticipating members are called \"P\" members, as opposed to observing members, who are called \"O\" members.\n\nISO is funded by a combination of: \n\nISO's main products are international standards. ISO also publishes technical reports, technical specifications, publicly available specifications, technical corrigenda, and guides.\n\nInternational standards\n\nTechnical reports\n\nTechnical and publicly available specifications\n\nTechnical corrigenda\n\nISO guides\nThese are meta-standards covering \"matters related to international standardization\". They are named using the format \"\"ISO[/IEC] Guide N:yyyy: Title\"\".For example:\n\nISO documents are copyrighted and ISO charges for most copies. It does not, however, charge for most draft copies of documents in electronic format. Although they are useful, care must be taken using these drafts as there is the possibility of substantial change before they become finalized as standards. Some standards by ISO and its official U.S. representative (and, via the U.S. National Committee, the International Electrotechnical Commission) are made freely available.\n\nA standard published by ISO/IEC is the last stage of a long process that commonly starts with the proposal of new work within a committee. Here are some abbreviations used for marking a standard with its status:\n\nAbbreviations used for amendments:\n\nOther abbreviations:\nInternational Standards are developed by ISO technical committees (TC) and subcommittees (SC) by a process with six steps:\n\nThe TC/SC may set up working groups (WG) of experts for the preparation of a working drafts. Subcommittees may have several working groups, which can have several Sub Groups (SG).\n\nIt is possible to omit certain stages, if there is a document with a certain degree of maturity at the start of a standardization project, for example a standard developed by another organization. ISO/IEC directives allow also the so-called \"Fast-track procedure\". In this procedure a document is submitted directly for approval as a draft International Standard (DIS) to the ISO member bodies or as a final draft International Standard (FDIS) if the document was developed by an international standardizing body recognized by the ISO Council.\n\nThe first step—a proposal of work (New Proposal) is approved at the relevant subcommittee or technical committee (e.g., SC29 and JTC1 respectively in the case of Moving Picture Experts Group – ISO/IEC JTC1/SC29/WG11). A working group (WG) of experts is set up by the TC/SC for the preparation of a working draft. When the scope of a new work is sufficiently clarified, some of the working groups (e.g., MPEG) usually make open request for proposals—known as a \"call for proposals\". The first document that is produced for example for audio and video coding standards is called a verification model (VM) (previously also called a \"simulation and test model\"). When a sufficient confidence in the stability of the standard under development is reached, a working draft (WD) is produced. This is in the form of a standard but is kept internal to working group for revision. When a working draft is sufficiently solid and the working group is satisfied that it has developed the best technical solution to the problem being addressed, it becomes committee draft (CD). If it is required, it is then sent to the P-members of the TC/SC (national bodies) for ballot.\n\nThe CD becomes final committee draft (FCD) if the number of positive votes is above the quorum. Successive committee drafts may be considered until consensus is reached on the technical content. When it is reached, the text is finalized for submission as a draft International Standard (DIS). The text is then submitted to national bodies for voting and comment within a period of five months. It is approved for submission as a final draft International Standard (FDIS) if a two-thirds majority of the P-members of the TC/SC are in favour and not more than one-quarter of the total number of votes cast are negative. ISO will then hold a ballot with National Bodies where no technical changes are allowed (yes/no ballot), within a period of two months. It is approved as an International Standard (IS) if a two-thirds majority of the P-members of the TC/SC is in favour and not more than one-quarter of the total number of votes cast are negative. After approval, only minor editorial changes are introduced into the final text. The final text is sent to the ISO Central Secretariat, which publishes it as the International Standard.\n\nThe fact that many of the ISO-created standards are ubiquitous has led, on occasion, to common use of \"ISO\" to describe the actual product that conforms to a standard. Some examples of this are:\n\nWith the exception of a small number of isolated standards, ISO standards are normally not available free of charge, but for a purchase fee, which has been seen by some as too expensive for small open source projects.\n\nThe ISO/IEC JTC1 fast-track procedures (\"Fast-track\" as used by OOXML and \"PAS\" as used by OpenDocument) have garnered criticism in relation to the standardization of Office Open XML (ISO/IEC 29500). Martin Bryan, outgoing Convenor of ISO/IEC JTC1/SC34 WG1, is quoted as saying:\nI would recommend my successor that it is perhaps time to pass WG1’s outstanding standards over to OASIS, where they can get approval in less than a year and then do a PAS submission to ISO, which will get a lot more attention and be approved much faster than standards currently can be within WG1.\nThe disparity of rules for PAS, Fast-Track and ISO committee generated standards is fast making ISO a laughing stock in IT circles. The days of open standards development are fast disappearing. Instead we are getting 'standardization by corporation'.\nComputer security entrepreneur and Ubuntu investor, Mark Shuttleworth, commented on the Standardization of Office Open XML process by saying \"I think it de-values the confidence people have in the standards setting process,\" and Shuttleworth alleged that ISO did not carry out its responsibility. He also noted that Microsoft had intensely lobbied many countries that traditionally had not participated in ISO and stacked technical committees with Microsoft employees, solution providers and resellers sympathetic to Office Open XML.\nWhen you have a process built on trust and when that trust is abused, ISO should halt the process... ISO is an engineering old boys club and these things are boring so you have to have a lot of passion … then suddenly you have an investment of a lot of money and lobbying and you get artificial results. The process is not set up to deal with intensive corporate lobbying and so you end up with something being a standard that is not clear.\n\n\n\n"}
{"id": "14936", "url": "https://en.wikipedia.org/wiki?curid=14936", "title": "Individualist anarchism", "text": "Individualist anarchism\n\nIndividualist anarchism refers to several traditions of thought within the anarchist movement that emphasize the individual and his will over external determinants such as groups, society, traditions and ideological systems. Individualist anarchism is not a single philosophy, but it refers to a group of individualistic philosophies that sometimes are in conflict. Benjamin Tucker, a famous 19th century individualist anarchist, held that \"if the individual has the right to govern himself, all external government is tyranny\".\n\nAmong the early influences on individualist anarchism were William Godwin, Josiah Warren (\"sovereignty of the individual\"), Max Stirner (egoism), Lysander Spooner (\"natural law\"), Pierre-Joseph Proudhon (mutualism), Henry David Thoreau (transcendentalism), Herbert Spencer (\"law of equal liberty\") and Anselme Bellegarrigue.\n\nIndividualist anarchism of different kinds have a few things in common. These are the following:\n\nThe egoist form of individualist anarchism, derived from the philosophy of Max Stirner, supports the individual doing exactly what he pleases—taking no notice of God, state, or moral rules. To Stirner, rights were \"spooks\" in the mind, and he held that society does not exist but \"the individuals are its reality\"—he supported property by force of might rather than moral right. Stirner advocated self-assertion and foresaw \"associations of egoists\" drawn together by respect for each other's ruthlessness.\nFor American anarchist historian Eunice Minette Schuster, American individualist anarchism \"stresses the isolation of the individual – his right to his own tools, his mind, his body, and to the products of his labor. To the artist who embraces this philosophy it is \"aesthetic\" anarchism, to the reformer, ethical anarchism, to the independent mechanic, economic anarchism. The former is concerned with philosophy, the latter with practical demonstration. The economic anarchist is concerned with constructing a society on the basis of anarchism. Economically he sees no harm whatever in the private possession of what the individual produces by his own labor, but only so much and no more. The aesthetic and ethical type found expression in the transcendentalism, humanitarianism, and romanticism of the first part of the nineteenth century, the economic type in the pioneer life of the West during the same period, but more favorably after the Civil War\". It is for this reason that it has been suggested that in order to understand American individualist anarchism one must take into account \"the social context of their ideas, namely the transformation of America from a pre-capitalist to a capitalist society [...] the non-capitalist nature of the early U.S. can be seen from the early dominance of self-employment (artisan and peasant production). At the beginning of the 19th century, around 80% of the working (non-slave) male population were self-employed. The great majority of Americans during this time were farmers working their own land, primarily for their own needs\" and so \"[i]ndividualist anarchism is clearly a form of artisanal socialism [...] while communist anarchism and anarcho-syndicalism are forms of industrial (or proletarian) socialism\".\n\nContemporary individualist anarchist Kevin Carson characterizes American individualist anarchism saying that \"[u]nlike the rest of the socialist movement, the individualist anarchists believed that the natural wage of labor in a free market was its product, and that economic exploitation could only take place when capitalists and landlords harnessed the power of the state in their interests. Thus, individualist anarchism was an alternative both to the increasing statism of the mainstream socialist movement, and to a classical liberal movement that was moving toward a mere apologetic for the power of big business\".\nIn European individualist anarchism, a different social context helped the rise of European individualist illegalism and as such \"[t]he illegalists were proletarians who had nothing to sell but their labour power, and nothing to discard but their dignity; if they disdained waged-work, it was because of its compulsive nature. If they turned to illegality it was due to the fact that honest toil only benefited the employers and often entailed a complete loss of dignity, while any complaints resulted in the sack; to avoid starvation through lack of work it was necessary to beg or steal, and to avoid conscription into the army many of them had to go on the run\". A European tendency of individualist anarchism advocated violent individual acts of individual reclamation, propaganda by the deed and criticism of organization. Such individualist anarchist tendencies include French illegalism and Italian anti-organizational insurrectionarism. Bookchin reports that at the end of the 19th century and the beginning of the 20th \"it was in times of severe social repression and deadening social quiescence that individualist anarchists came to the foreground of libertarian activity – and then primarily as terrorists. In France, Spain, and the United States, individualistic anarchists committed acts of terrorism that gave anarchism its reputation as a violently sinister conspiracy\".\n\nAnother important tendency within individualist anarchist currents emphasizes individual subjective exploration and defiance of social conventions. Individualist anarchist philosophy attracted \"amongst artists, intellectuals and the well-read, urban middle classes in general\". As such, Murray Bookchin describes a lot of individualist anarchism as people who \"expressed their opposition in uniquely personal forms, especially in fiery tracts, outrageous behavior and aberrant lifestyles in the cultural ghettos of fin de siecle New York, Paris and London. As a credo, individualist anarchism remained largely a bohemian lifestyle, most conspicuous in its demands for sexual freedom ('free love') and enamored of innovations in art, behavior, and clothing\". In this way, free love currents and other radical lifestyles such as naturism had popularity among individualist anarchists.\n\nFor Catalan historian Xavier Diez, individualist anarchism \"under its iconoclastic, antiintelectual, antitheist run, which goes against all sacralized ideas or values it entailed, a philosophy of life which could be considered a reaction against the sacred gods of capitalist society. Against the idea of nation, it opposed its internationalism. Against the exaltation of authority embodied in the military institution, it opposed its antimilitarism. Against the concept of industrial civilization, it opposed its naturist vision\".\n\nIn regards to economic questions, there are diverse positions. There are adherents to mutualism (Proudhon, Émile Armand and early Benjamin Tucker), egoistic disrespect for \"ghosts\" such as private property and markets (Stirner, John Henry Mackay, Lev Chernyi and later Tucker) and adherents to anarcho-communism (Albert Libertad, illegalism and Renzo Novatore). Anarchist historian George Woodcock finds a tendency in individualist anarchism of a \"distrust (of) all co-operation beyond the barest minimum for an ascetic life\".\n\nOn the issue of violence opinions have gone from a violentist point of view mainly exemplified by illegalism and insurrectionary anarchism to one that can be called anarcho-pacifist. In the particular case of Spanish individualist anarchist Miguel Gimenez Igualada, he went from illegalist practice in his youth towards a pacifist position later in his life.\n\nWilliam Godwin can be considered an individualist anarchist and philosophical anarchist who was influenced by the ideas of the Age of Enlightenment, and developed what many consider the first expression of modern anarchist thought. According to Peter Kropotkin, Godwin \"the first to formulate the political and economical conceptions of anarchism, even though he did not give that name to the ideas developed in his work\". Godwin advocated extreme individualism, proposing that all cooperation in labor be eliminated. Godwin was a utilitarian who believed that all individuals are not of equal value, with some of us \"of more worth and importance\" than others depending on our utility in bringing about social good. Therefore, he does not believe in equal rights, but the person's life that should be favored that is most conducive to the general good. Godwin opposed government because it infringes on the individual's right to \"private judgement\" to determine which actions most maximize utility, but also makes a critique of all authority over the individual's judgement. This aspect of Godwin's philosophy, minus the utilitarianism, was developed into a more extreme form later by Stirner.\n\nGodwin took individualism to the radical extent of opposing individuals performing together in orchestras, writing in \"Political Justice\" that \"everything understood by the term co-operation is in some sense an evil\". The only apparent exception to this opposition to cooperation is the spontaneous association that may arise when a society is threatened by violent force. One reason he opposed cooperation is he believed it to interfere with an individual's ability to be benevolent for the greater good. Godwin opposes the idea of government, but wrote that a minimal state as a present \"necessary evil\" that would become increasingly irrelevant and powerless by the gradual spread of knowledge. He expressly opposed democracy, fearing oppression of the individual by the majority, though he believed it to be preferable to dictatorship.\nGodwin supported individual ownership of property, defining it as \"the empire to which every man is entitled over the produce of his own industry\". However, he also advocated that individuals give to each other their surplus property on the occasion that others have a need for it, without involving trade (e.g. gift economy). Thus while people have the right to private property, they should give it away as enlightened altruists. This was to be based on utilitarian principles and he said: \"Every man has a right to that, the exclusive possession of which being awarded to him, a greater sum of benefit or pleasure will result than could have arisen from its being otherwise appropriated\". However, benevolence was not to be enforced, being a matter of free individual \"private judgement\". He did not advocate a community of goods or assert collective ownership as is embraced in communism, but his belief that individuals ought to share with those in need was influential on the later development of anarcho-communism.\n\nGodwin's political views were diverse and do not perfectly agree with any of the ideologies that claim his influence as writers of the \"Socialist Standard\", organ of the Socialist Party of Great Britain, consider Godwin both an individualist and a communist; Murray Rothbard did not regard Godwin as being in the individualist camp at all, referring to him as the \"founder of communist anarchism\";<ref name=\"rothbard/burke\">Rothbard, Murray. \"Edmund Burke, Anarchist.\"</ref> and historian Albert Weisbord considers him an individualist anarchist without reservation. Some writers see a conflict between Godwin's advocacy of \"private judgement\" and utilitarianism as he says that ethics requires that individuals give their surplus property to each other resulting in an egalitarian society, but at the same time he insists that all things be left to individual choice. As noted by Kropotkin, many of Godwin's views changed over time.\n\nWilliam Godwin's influenced \"the socialism of Robert Owen and Charles Fourier. After success of his British venture, Owen himself established a cooperative community within the United States at New Harmony, Indiana during 1825. One member of this commune was Josiah Warren (1798–1874), considered to be the first individualist anarchist. After New Harmony failed Warren shifted his ideological loyalties from socialism to anarchism (which was no great leap, given that Owen's socialism had been predicated on Godwin's anarchism)\".\n\nPierre-Joseph Proudhon (1809–1865) was the first philosopher to label himself an \"anarchist\". Some consider Proudhon to be an individualist anarchist while others regard him to be a social anarchist. Some commentators do not identify Proudhon as an individualist anarchist due to his preference for association in large industries, rather than individual control. Nevertheless, he was influential among some of the American individualists—in the 1840s and 1850s, Charles A. Dana and William B. Greene introduced Proudhon's works to the United States. Greene adapted Proudhon's mutualism to American conditions and introduced it to Benjamin Tucker.\n\nProudhon opposed government privilege that protects capitalist, banking and land interests and the accumulation or acquisition of property (and any form of coercion that led to it) which he believed hampers competition and keeps wealth in the hands of the few. Proudhon favoured a right of individuals to retain the product of their labour as their own property, but believed that any property beyond that which an individual produced and could possess was illegitimate. Thus he saw private property as both essential to liberty and a road to tyranny, the former when it resulted from labour and was required for labour and the latter when it resulted in exploitation (profit, interest, rent and tax). He generally called the former \"possession\" and the latter \"property\". For large-scale industry, he supported workers associations to replace wage labour and opposed the ownership of land.\n\nProudhon maintained that those who labour should retain the entirety of what they produce and that monopolies on credit and land are the forces that prohibit such. He advocated an economic system that included private property as possession and exchange market, but without profit, which he called mutualism. It is Proudhon's philosophy that was explicitly rejected by Joseph Dejacque in the inception of anarcho-communism, with the latter asserting directly to Proudhon in a letter that \"it is not the product of his or her labour that the worker has a right to, but to the satisfaction of his or her needs, whatever may be their nature\". An individualist rather than anarcho-communist, Proudhon said that \"communism [...] is the very denial of society in its foundation\" and famously declared that \"property is theft!\" in reference to his rejection of ownership rights to land being granted to a person who is not using that land.\n\nAfter Dejacque and others split from Proudhon due to the latter's support of individual property and an exchange economy, the relationship between the individualists (who continued in relative alignment with the philosophy of Proudhon) and the anarcho-communists was characterised by various degrees of antagonism and harmony. For example, individualists like Tucker on the one hand translated and reprinted the works of collectivists like Mikhail Bakunin while on the other hand rejected the economic aspects of collectivism and communism as incompatible with anarchist ideals.\n\nMutualism is an anarchist school of thought which can be traced to the writings of Pierre-Joseph Proudhon, who envisioned a society where each person might possess a means of production, either individually or collectively, with trade representing equivalent amounts of labor in the free market. Integral to the scheme was the establishment of a mutual-credit bank which would lend to producers at a minimal interest rate only high enough to cover the costs of administration. Mutualism is based on a labor theory of value which holds that when labour or its product is sold, in exchange it ought to receive goods or services embodying \"the amount of labor necessary to produce an article of exactly similar and equal utility\". Some mutualists believe that if the state did not intervene, as a result of increased competition in the marketplace, individuals would receive no more income than that in proportion to the amount of labor they exert. Mutualists oppose the idea of individuals receiving an income through loans, investments and rent as they believe these individuals are not labouring. Some of them argue that if state intervention ceased, these types of incomes would disappear due to increased competition in capital. Though Proudhon opposed this type of income, he expressed: \"I never meant to [...] forbid or suppress, by sovereign decree, ground rent and interest on capital. I believe that all these forms of human activity should remain free and optional for all\".\nInsofar as they ensure the workers right to the full product of their labor, mutualists support markets and private property in the product of labor. However, they argue for conditional titles to land, whose private ownership is legitimate only so long as it remains in use or occupation (which Proudhon called \"possession\"). Proudhon's Mutualism supports labor-owned cooperative firms and associations for \"we need not hesitate, for we have no choice [...] it is necessary to form an ASSOCIATION among workers [...] because without that, they would remain related as subordinates and superiors, and there would ensue two [...] castes of masters and wage-workers, which is repugnant to a free and democratic society\" and so \"it becomes necessary for the workers to form themselves into democratic societies, with equal conditions for all members, on pain of a relapse into feudalism\". As for capital goods (man-made, non-land, \"means of production\"), mutualist opinion differs on whether these should be commonly managed public assets or private property.\n\nFollowing Proudhon, mutualists originally considered themselves to be libertarian socialists. However, \"some mutualists have abandoned the labor theory of value, and prefer to avoid the term \"socialist.\" But they still retain some cultural attitudes, for the most part, that set them off from the libertarian right\". Mutualists have distinguished themselves from state socialism and do not advocate social control over the means of production. Benjamin Tucker said of Proudhon that \"though opposed to socializing the ownership of capital, Proudhon aimed nevertheless to socialize its effects by making its use beneficial to all instead of a means of impoverishing the many to enrich the few [...] by subjecting capital to the natural law of competition, thus bringing the price of its own use down to cost\".\n\nJohann Kaspar Schmidt (October 25, 1806 – June 26, 1856), better known as Max Stirner (the \"nom de plume\" he adopted from a schoolyard nickname he had acquired as a child because of his high brow, in German 'Stirn'), was a German philosopher, who ranks as one of the literary fathers of nihilism, existentialism, post-modernism and anarchism, especially of individualist anarchism. Stirner's main work is \"The Ego and Its Own\", also known as \"The Ego and His Own\" (\"Der Einzige und sein Eigentum\" in German, which translates literally as \"The Only One [individual] and his Property\"). This work was first published in 1844 in Leipzig and has since appeared in numerous editions and translations.\n\nMax Stirner's philosophy, sometimes called egoism, is a form of individualist anarchism. Stirner was a Hegelian philosopher whose \"name appears with familiar regularity in historically oriented surveys of anarchist thought as one of the earliest and best-known exponents of individualist anarchism\". In 1844, his \"The Ego and Its Own\" (\"Der Einzige and sein Eigentum\" which may literally be translated as \"The Unique Individual and His Property\") was published, which is considered to be \"a founding text in the tradition of individualist anarchism\". Stirner does not recommend that the individual try to eliminate the state, but simply that they disregard the state when it conflicts with one's autonomous choices and go along with it when doing so is conducive to one's interests. He says that the egoist rejects pursuit of devotion to \"a great idea, a good cause, a doctirine, a system, a lofty calling,\" saying that the egoist has no political calling, but rather \"lives themselves out\" without regard to \"how well or ill humanity may fare thereby\". Stirner held that the only limitation on the rights of the individual is that individual's power to obtain what he desires. He proposes that most commonly accepted social institutions—including the notion of State, property as a right, natural rights in general, and the very notion of society—were mere spooks in the mind. Stirner wants to \"abolish not only the state but also society as an institution responsible for its members\". Stirner advocated self-assertion and foresaw Union of egoists, non-systematic associations, which Stirner proposed in as a form of organization in place of the state. A Union is understood as a relation between egoists which is continually renewed by all parties' support through an act of will. Even murder is permissible \"if it is right for me\", though it is claimed by egoist anarchists that egoism will foster genuine and spontaneous union between individuals.\nFor Stirner, property simply comes about through might: \"Whoever knows how to take, to defend, the thing, to him belongs property\". He further says: \"What I have in my power, that is my own. So long as I assert myself as holder, I am the proprietor of the thing\" and that \"I do not step shyly back from your property, but look upon it always as my property, in which I respect nothing. Pray do the like with what you call my property!\". His concept of \"egoistic property\" not only a lack of moral restraint on how one obtains and uses \"things\", but includes other people as well. His embrace of egoism is in stark contrast to Godwin's altruism. Stirner was opposed to communism, seeing it as a form of authority over the individual.\n\nThis position on property is much different from the Native American, natural law, form of individualist anarchism, which defends the inviolability of the private property that has been earned through labor and trade. However, Benjamin Tucker rejected the natural rights philosophy and adopted Stirner's egoism in 1886, with several others joining with him. This split the American individualists into fierce debate, \"with the natural rights proponents accusing the egoists of destroying libertarianism itself.\" Other egoists include James L. Walker, Sidney Parker, Dora Marsden and John Beverly Robinson.\n\nIn Russia, individualist anarchism inspired by Stirner combined with an appreciation for Friedrich Nietzsche attracted a small following of bohemian artists and intellectuals such as Lev Chernyi, as well as a few lone wolves who found self-expression in crime and violence. They rejected organizing, believing that only unorganized individuals were safe from coercion and domination, believing this kept them true to the ideals of anarchism. This type of individualist anarchism inspired anarcha-feminist Emma Goldman.\n\nThough Stirner's philosophy is individualist, it has influenced some libertarian communists and anarcho-communists. \"For Ourselves Council for Generalized Self-Management\" discusses Stirner and speaks of a \"communist egoism\", which is said to be a \"synthesis of individualism and collectivism\" and says that \"greed in its fullest sense is the only possible basis of communist society\". Forms of libertarian communism such as Situationism are influenced by Stirner. Anarcho-communist Emma Goldman was influenced by both Stirner and Peter Kropotkin and blended their philosophies together in her own as shown in books of hers such as \"Anarchism And Other Essays\".\n\nJosiah Warren is widely regarded as the first American anarchist and the four-page weekly paper he edited during 1833, \"The Peaceful Revolutionist\", was the first anarchist periodical published, an enterprise for which he built his own printing press, cast his own type and made his own printing plates. Warren was a follower of Robert Owen and joined Owen's community at New Harmony, Indiana. Warren termed the phrase \"Cost the limit of price\", with \"cost\" here referring not to monetary price paid but the labor one exerted to produce an item. Therefore, \"[h]e proposed a system to pay people with certificates indicating how many hours of work they did. They could exchange the notes at local time stores for goods that took the same amount of time to produce\". He put his theories to the test by establishing an experimental \"labor for labor store\" called the Cincinnati Time Store where trade was facilitated by notes backed by a promise to perform labor. The store proved successful and operated for three years after which it was closed so that Warren could pursue establishing colonies based on mutualism. These included Utopia and Modern Times. Warren said that Stephen Pearl Andrews' \"The Science of Society\" (published in 1852) was the most lucid and complete exposition of Warren's own theories. Catalan historian Xavier Diez report that the intentional communal experiments pioneered by Warren were influential in European individualist anarchists of the late 19th and early 20th centuries such as Émile Armand and the intentional communities started by them.\n\nHenry David Thoreau (1817–1862) was an important early influence in individualist anarchist thought in the United States and Europe. Thoreau was an American author, poet, naturalist, tax resister, development critic, surveyor, historian, philosopher and leading transcendentalist. He is best known for his book \"Walden\", a reflection upon simple living in natural surroundings; and his essay, \"Civil Disobedience\", an argument for individual resistance to civil government in moral opposition to an unjust state. His thought is an early influence on green anarchism, but with an emphasis on the individual experience of the natural world influencing later naturist currents, simple living as a rejection of a materialist lifestyle and self-sufficiency were Thoreau's goals and the whole project was inspired by transcendentalist philosophy. Many have seen in Thoreau one of the precursors of ecologism and anarcho-primitivism represented today in John Zerzan. For George Woodcock, this attitude can be also motivated by certain idea of resistance to progress and of rejection of the growing materialism which is the nature of American society in the mid 19th century.\n\nThe essay Civil Disobedience (\"Resistance to Civil Government\") was first published in 1849. It argues that people should not permit governments to overrule or atrophy their consciences and that people have a duty to avoid allowing such acquiescence to enable the government to make them the agents of injustice. Thoreau was motivated in part by his disgust with slavery and the Mexican–American War. The essay later influenced Mohandas Gandhi, Martin Luther King, Jr., Martin Buber and Leo Tolstoy through its advocacy of nonviolent resistance. It is also the main precedent for anarcho-pacifism. The American version of individualist anarchism has a strong emphasis on the non-aggression principle and individual sovereignty. Some individualist anarchists such as Thoreau do not speak of economics, but simply the right of \"disunion\" from the state and foresee the gradual elimination of the state through social evolution.\n\nAn important current within individualist anarchism is free love. Free love advocates sometimes traced their roots back to Josiah Warren and to experimental communities, and viewed sexual freedom as a clear, direct expression of an individual's self-ownership. Free love particularly stressed women's rights since most sexual laws, such as those governing marriage and use of birth control, discriminated against women. The most important American free love journal was \"Lucifer the Lightbearer\" (1883–1907) edited by Moses Harman and Lois Waisbrooker but also there existed Ezra Heywood and Angela Heywood's \"The Word\" (1872–1890, 1892–1893). M. E. Lazarus was also an important American individualist anarchist who promoted free love. John William Lloyd, a collaborator of Benjamin Tucker's periodical \"Liberty\", published in 1931 a sex manual that he called \"The Karezza Method: Or Magnetation, the Art of Connubial Love\".\n\nIn Europe, the main propagandist of free love within individualist anarchism was Émile Armand. He proposed the concept of \"la camaraderie amoureuse\" to speak of free love as the possibility of voluntary sexual encounter between consenting adults. He was also a consistent proponent of polyamory. In France, there was also feminist activity inside individualist anarchism as promoted by individualist feminists Marie Küge, Anna Mahé, Rirette Maitrejean and Sophia Zaïkovska.\n\nThe Brazilian individualist anarchist Maria Lacerda de Moura lectured on topics such as education, women's rights, free love and antimilitarism. Her writings and essays garnered her attention not only in Brazil, but also in Argentina and Uruguay. She also wrote for the Spanish individualist anarchist magazine \"Al Margen\" alongside Miguel Gimenez Igualada.\n\nIn Germany, the Stirnerists Adolf Brand and John Henry Mackay were pioneering campaigners for the acceptance of male bisexuality and homosexuality.\n\nFreethought as a philosophical position and as activism was important in both North American and European individualist anarchism, but in the United States freethought was basically an anti-Christian, anti-clerical movement whose purpose was to make the individual politically and spiritually free to decide for himself on religious matters. A number of contributors to \"Liberty\" were prominent figures in both freethought and anarchism. The individualist anarchist George MacDonald was a co-editor of \"Freethought\" and for a time \"The Truth Seeker\". E.C. Walker was co-editor of \"Lucifer, the Light-Bearer\". Many of the anarchists were ardent freethinkers; reprints from freethought papers such as \"Lucifer, the Light-Bearer\", \"Freethought\" and \"The Truth Seeker\" appeared in \"Liberty\". The church was viewed as a common ally of the state and as a repressive force in and of itself.\n\nIn Europe, a similar development occurred in French and Spanish individualist anarchist circles: \"Anticlericalism, just as in the rest of the libertarian movement, is another of the frequent elements which will gain relevance related to the measure in which the (French) Republic begins to have conflicts with the church [...] Anti-clerical discourse, frequently called for by the french individualist André Lorulot, will have its impacts in \"Estudios\" (a Spanish individualist anarchist publication). There will be an attack on institutionalized religion for the responsibility that it had in the past on negative developments, for its irrationality which makes it a counterpoint of philosophical and scientific progress. There will be a criticism of proselitism and ideological manipulation which happens on both believers and agnostics\". This tendencies will continue in French individualist anarchism in the work and activism of Charles-Auguste Bontemps and others. In the Spanish individualist anarchist magazine \"Ética\" and \"Iniciales\", \"there is a strong interest in publishing scientific news, usually linked to a certain atheist and anti-theist obsession, philosophy which will also work for pointing out the incompatibility between science and religion, faith and reason. In this way there will be a lot of talk on Darwin's theories or on the negation of the existence of the soul\".\n\nAnother important current, especially within French and Spanish individualist anarchist groups was naturism. Naturism promoted an ecological worldview, small ecovillages and most prominently nudism as a way to avoid the artificiality of the industrial mass society of modernity. Naturist individualist anarchists saw the individual in his biological, physical and psychological aspects and avoided and tried to eliminate social determinations. An early influence in this vein was Henry David Thoreau and his famous book \"Walden\". Important promoters of this were Henri Zisly and Émile Gravelle who collaborated in \"La Nouvelle Humanité\" followed by \"Le Naturien\", \"Le Sauvage\", \"L'Ordre Naturel\" and \"La Vie Naturelle\".\n\nThis relationship between anarchism and naturism was quite important at the end of the 1920s in Spain, when \"[t]he linking role played by the 'Sol y Vida' group was very important. The goal of this group was to take trips and enjoy the open air. The Naturist athenaeum, 'Ecléctico', in Barcelona, was the base from which the activities of the group were launched. First \"Etica\" and then \"Iniciales\", which began in 1929, were the publications of the group, which lasted until the Spanish Civil War. We must be aware that the naturist ideas expressed in them matched the desires that the libertarian youth had of breaking up with the conventions of the bourgeoisie of the time. That is what a young worker explained in a letter to 'Iniciales' He writes it under the odd pseudonym of 'silvestre del campo', (wild man in the country). \"I find great pleasure in being naked in the woods, bathed in light and air, two natural elements we cannot do without. By shunning the humble garment of an exploited person, (garments which, in my opinion, are the result of all the laws devised to make our lives bitter), we feel there no others left but just the natural laws. Clothes mean slavery for some and tyranny for others. Only the naked man who rebels against all norms, stands for anarchism, devoid of the prejudices of outfit imposed by our money-oriented society\". The relation between anarchism and naturism \"gives way to the Naturist Federation, in July 1928, and to the lV Spanish Naturist Congress, in September 1929, both supported by the Libertarian Movement. However, in the short term, the Naturist and Libertarian movements grew apart in their conceptions of everyday life. The Naturist movement felt closer to the Libertarian individualism of some French theoreticians such as Henri Ner (real name of Han Ryner) than to the revolutionary goals proposed by some Anarchist organisations such as the FAI, (Federación Anarquista Ibérica)\".\n\nThe thought of German philosopher Friedrich Nietzsche has been influential in individualist anarchism, specifically in thinkers such as France's Émile Armand, the Italian Renzo Novatore and the Colombian Biofilo Panclasta. Robert C. Holub, author of \"Nietzsche: Socialist, Anarchist, Feminist\" posits that \"translations of Nietzsche's writings in the United States very likely appeared first in \"Liberty\", the anarchist journal edited by Benjamin Tucker\".\n\nFor American anarchist historian Eunice Minette Schuster, \"[i]t is apparent [...] that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and Stephen Pearl Andrews [...] William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form\". William Batchelder Greene (1819–1878) is best known for the works \"Mutual Banking\"(1850), which proposed an interest-free banking system; and \"Transcendentalism\", a critique of the New England philosophical school. He saw mutualism as the synthesis of \"liberty and order\". His \"associationism [...] is checked by individualism [...] \"Mind your own business,\" \"Judge not that ye be not judged.\" Over matters which are purely personal, as for example, moral conduct, the individual is sovereign, as well as over that which he himself produces. For this reason he demands \"mutuality\" in marriage – the equal right of a woman to her own personal freedom and property and feminist and spiritualist tendencies\".\n\nContemporary American anarchist Hakim Bey reports that \"Steven Pearl Andrews [...] was not a fourierist (see Charles Fourier), but he lived through the brief craze for phalansteries in America & adopted a lot of fourierist principles & practices [...] a maker of worlds out of words. He syncretized Abolitionism, Free Love, spiritual universalism, (Josiah) Warren, & (Charles) Fourier into a grand utopian scheme he called the Universal Pantarchy [...] He was instrumental in founding several \"intentional communities,\" including the \"Brownstone Utopia\" on 14th St. in New York, & \"Modern Times\" in Brentwood, Long Island. The latter became as famous as the best-known fourierist communes (Brook Farm in Massachusetts & the North American Phalanx in New Jersey) – in fact, Modern Times became downright notorious (for \"Free Love\") & finally foundered under a wave of scandalous publicity. Andrews (& Victoria Woodhull) were members of the infamous Section 12 of the 1st International, expelled by Marx for its anarchist, feminist, & spiritualist tendencies\".\n\nAnother form of individualist anarchism was found in the United States as advocated by the so-called Boston anarchists. By default, American individualists had no difficulty accepting the concepts that \"one man employ another\" or that \"he direct him\", in his labor but rather demanded that \"all natural opportunities requisite to the production of wealth be accessible to all on equal terms and that monopolies arising from special privileges created by law be abolished\".\n\nThey believed state monopoly capitalism (defined as a state-sponsored monopoly) prevented labor from being fully rewarded. Voltairine de Cleyre summed up the philosophy by saying that the anarchist individualists \"are firm in the idea that the system of employer and employed, buying and selling, banking, and all the other essential institutions of Commercialism, centred upon private property, are in themselves good, and are rendered vicious merely by the interference of the State\".\n\nEven among the 19th-century American individualists, there was not a monolithic doctrine as they disagreed amongst each other on various issues including intellectual property rights and possession versus property in land. A major schism occurred later in the 19th century when Tucker and some others abandoned their traditional support of natural rights as espoused by Lysander Spooner and converted to an \"egoism\" modeled upon Max Stirner's philosophy. Lysander Spooner besides his individualist anarchist activism was also an important anti-slavery activist and became a member of the First International.\n\nSome Boston anarchists, including Benjamin Tucker, identified themselves as socialists, which in the 19th century was often used in the sense of a commitment to improving conditions of the working class (i.e. \"the labor problem\"). The Boston anarchists such as Tucker and his followers continue to be considered socialists due to their opposition to usury. They do so because as the modern economist Jim Stanford points out there are many different kinds of competitive markets such as market socialism and capitalism is only one type of a market economy. By around the start of the 20th century, the heyday of individualist anarchism had passed.\n\nGeorge Woodcock reports that the American individualist anarchists Lysander Spooner and William B. Greene had been members of the socialist First International\n\nTwo individualist anarchists who wrote in Benjamin Tucker's \"Liberty\" were also important labor organizers of the time. Joseph Labadie (April 18, 1850 – October 7, 1933) was an American labor organizer, individualist anarchist, social activist, printer, publisher, essayist and poet. In 1883, Labadie embraced a non-violent version of individualist anarchism. Without the oppression of the state, Labadie believed, humans would choose to harmonize with \"the great natural laws [...] without robbing [their] fellows through interest, profit, rent and taxes\". However, he supported community cooperation as he supported community control of water utilities, streets and railroads. Although he did not support the militant anarchism of the Haymarket anarchists, he fought for the clemency of the accused because he did not believe they were the perpetrators. In 1888, Labadie organized the Michigan Federation of Labor, became its first president and forged an alliance with Samuel Gompers. A colleague of Labadie's at \"Liberty\", Dyer Lum was another important individualist anarchist labor activist and poet of the era. A leading anarcho-syndicalist and a prominent left-wing intellectual of the 1880s, he is remembered as the lover and mentor of early anarcha-feminist Voltairine de Cleyre. Lum was a prolific writer who wrote a number of key anarchist texts and contributed to publications including \"Mother Earth\", \"Twentieth Century\", \"The Alarm\" (the journal of the International Working People's Association) and \"The Open Court\" among others. Lum's political philosophy was a fusion of individualist anarchist economics—\"a radicalized form of \"laissez-faire\" economics\" inspired by the Boston anarchists—with radical labor organization similar to that of the Chicago anarchists of the time. Herbert Spencer and Pierre-Joseph Proudhon influenced Lum strongly in his individualist tendency. He developed a \"mutualist\" theory of unions and as such was active within the Knights of Labor and later promoted anti-political strategies in the American Federation of Labor. Frustration with abolitionism, spiritualism and labor reform caused Lum to embrace anarchism and radicalize workers. Convinced of the necessity of violence to enact social change he volunteered to fight in the American Civil War, hoping thereby to bring about the end of slavery. Kevin Carson has praised Lum's fusion of individualist \"laissez-faire\" economics with radical labor activism as \"creative\" and described him as \"more significant than any in the Boston group\".\n\nSome of the American individualist anarchists later in this era, such as Benjamin Tucker, abandoned natural rights positions and converted to Max Stirner's egoist anarchism. Rejecting the idea of moral rights, Tucker said that there were only two rights, \"the right of might\" and \"the right of contract\". He also said after converting to Egoist individualism that \"[i]n times past [...] it was my habit to talk glibly of the right of man to land. It was a bad habit, and I long ago sloughed it off [...] Man's only right to land is his might over it\". In adopting Stirnerite egoism in 1886, Tucker rejected natural rights which had long been considered the foundation of libertarianism. This rejection galvanized the movement into fierce debates, with the natural rights proponents accusing the egoists of destroying libertarianism itself. So bitter was the conflict that a number of natural rights proponents withdrew from the pages of \"Liberty\" in protest even though they had hitherto been among its frequent contributors. Thereafter, \"Liberty\" championed egoism although its general content did not change significantly.\n\nSeveral periodicals were undoubtedly influenced by \"Liberty\"'s presentation of egoism. They included \"I\" published by Clarence Lee Swartz, edited by William Walstein Gordak and J. William Lloyd (all associates of \"Liberty\"); and \"The Ego\" and \"The Egoist\", both of which were edited by Edward H. Fulton. Among the egoist papers that Tucker followed were the German \"Der Eigene\", edited by Adolf Brand; and \"The Eagle\" and \"The Serpent\", issued from London. The latter, the most prominent English-language egoist journal, was published from 1898 to 1900 with the subtitle \"A Journal of Egoistic Philosophy and Sociology\".\n\nAmerican anarchists who adhered to egoism include Benjamin Tucker, John Beverley Robinson, Steven T. Byington, Hutchins Hapgood, James L. Walker, Victor Yarros and Edward H. Fulton. Robinson wrote an essay called \"Egoism\" in which he states that \"[m]odern egoism, as propounded by Stirner and Nietzsche, and expounded by Ibsen, Shaw and others, is all these; but it is more. It is the realization by the individual that they are an individual; that, as far as they are concerned, they are the only individual\". Walker published the work \"The Philosophy of Egoism\" in which he argued that egosim \"implies a rethinking of the self-other relationship, nothing less than \"a complete revolution in the relations of mankind\" that avoids both the \"archist\" principle that legitimates domination and the \"moralist\" notion that elevates self-renunciation to a virtue. Walker describes himself as an \"egoistic anarchist\" who believed in both contract and cooperation as practical principles to guide everyday interactions\". For Walker, \"what really defines egoism is not mere self-interest, pleasure, or greed; it is the sovereignty of the individual, the full expression of the subjectivity of the individual ego\".\n\nItalian anti-organizationalist individualist anarchism was brought to the United States by Italian born individualists such as Giuseppe Ciancabilla and others who advocated for violent propaganda by the deed there. Anarchist historian George Woodcock reports the incident in which the important Italian social anarchist Errico Malatesta became involved \"in a dispute with the individualist anarchists of Paterson, who insisted that anarchism implied no organization at all, and that every man must act solely on his impulses. At last, in one noisy debate, the individual impulse of a certain Ciancabilla directed him to shoot Malatesta, who was badly wounded but obstinately refused to name his assailant\".\n\nEnrico Arrigoni (pseudonym Frank Brand) was an Italian American individualist anarchist Lathe operator, house painter, bricklayer, dramatist and political activist influenced by the work of Max Stirner. He took the pseudonym Brand from a fictional character in one of Henrik Ibsen's plays. In the 1910s, he started becoming involved in anarchist and anti-war activism around Milan. From the 1910s until the 1920s, he participated in anarchist activities and popular uprisings in various countries including Switzerland, Germany, Hungary, Argentina and Cuba. He lived from the 1920s onwards in New York City, where he edited the individualist anarchist eclectic journal \"Eresia\" in 1928. He also wrote for other American anarchist publications such as \"L' Adunata dei refrattari\", \"Cultura Obrera\", \"Controcorrente\" and \"Intesa Libertaria\". During the Spanish Civil War, he went to fight with the anarchists, but he was imprisoned and was helped on his release by Emma Goldman. Afterwards, Arrigoni became a longtime member of the Libertarian Book Club in New York City. His written works include \"The Totalitarian Nightmare\" (1975), \"The Lunacy of the Superman\" (1977), \"Adventures in the Country of the Monoliths\" (1981) and \"Freedom: My Dream\" (1986).\n\nWithout the labor theory of value, 19th century individualist anarchists approximate the modern movement of anarcho-capitalism. As economic theory changed, the popularity of the labor theory of classical economics was superseded by the subjective theory of value of neo-classical economics. Murray Rothbard, a student of Ludwig von Mises, combined Mises' Austrian School of economics with the absolutist views of human rights and rejection of the state he had absorbed from studying the individualist American anarchists of the 19th century such as Lysander Spooner and Benjamin Tucker. In the mid-1950s, Rothbard wrote an article under a pseudonym, saying that \"we are not anarchists [...] but not archists either [...] Perhaps, then, we could call ourselves by a new name: nonarchist\", concerned with differentiating himself from communist and socialistic economic views of other anarchists (including the individualist anarchists of the 19th century). There is a strong current within anarchism which does not consider that anarcho-capitalism can be considered a part of the anarchist movement due to the fact that anarchism has historically been an anti-capitalist movement and for definitional reasons which see anarchism incompatible with capitalist forms.\n\nAgorism was developed from anarcho-capitalism in the late 20th century by Samuel Edward Konkin III. The goal of agorists is a society in which all \"relations between people are voluntary exchanges – a free market\". Agorists are market anarchists. Most agorists consider that property rights are natural rights deriving from the primary right of self-ownership. Because of this, they are not opposed in principle to collectively held property if individual owners of the property consent to collective ownership by contract or other voluntary mutual agreement. However, agorists are divided on the question of intellectual property rights.\n\nThough anarcho-capitalism has been regarded by some as a form of individualist anarchism, anarcho-capitalist author Murray Rothbard stated that individualist anarchism is different from capitalism due to the individualist anarchists retaining the labor theory of value and many writers deny that anarcho-capitalism is a form of anarchism at all, or that capitalism itself is compatible with anarchism.\n\nLeft-wing market anarchism, a form of left-libertarianism, individualist anarchism and libertarian socialism, is associated with scholars such as Kevin Carson, Roderick T. Long, Charles Johnson, Brad Spangler, Samuel Edward Konkin III, Sheldon Richman, Chris Matthew Sciabarra and Gary Chartier, who stress the value of radically free markets, termed \"freed markets\" to distinguish them from the common conception which these libertarians believe to be riddled with statist and capitalist privileges. Referred to as left-wing market anarchists or market-oriented left-libertarians, proponents of this approach strongly affirm the classical liberal ideas of self-ownership and free markets while maintaining that taken to their logical conclusions, these ideas support anti-capitalist, anti-corporatist, anti-hierarchical, pro-labor positions in economics; anti-imperialism in foreign policy; and thoroughly liberal or radical views regarding such cultural issues as gender, sexuality and race.\n\nThe genealogy of contemporary market-oriented left-libertarianism, sometimes labeled \"left-wing market anarchism\", overlaps to a significant degree with that of Steiner–Vallentyne left-libertarianism as the roots of that tradition are sketched in the book \"The Origins of Left-Libertarianism\". Carson–Long-style left-libertarianism is rooted in 19th-century mutualism and in the work of figures such as Thomas Hodgskin and the individualist anarchists Benjamin Tucker and Lysander Spooner. While with notable exceptions market-oriented libertarians after Tucker tended to ally with the political right, relationships between such libertarians and the New Left thrived in the 1960s, laying the groundwork for modern left-wing market anarchism. Left-wing market anarchism identifies with Left-libertarianism (or left-wing libertarianism) which names several related but distinct approaches to politics, society, culture, and political and social theory, which stress both individual freedom and social justice. Unlike right-libertarians, they believe that neither claiming nor mixing one's labor with natural resources is enough to generate full private property rights and maintain that natural resources (land, oil, gold, trees) ought to be held in some egalitarian manner, either unowned or owned collectively. Those left-libertarians who support private property do so under the condition that recompense is offered to the local community.\n\nMurray Bookchin has identified post-left anarchy as a form of individualist anarchism in \"\" where he identifies \"a shift among Euro-American anarchists away from social anarchism and toward individualist or lifestyle anarchism. Indeed, lifestyle anarchism today is finding its principal expression in spray-can graffiti, post-modernist nihilism, antirationalism, neo-primitivism, anti-technologism, neo-Situationist 'cultural terrorism', mysticism, and a 'practice' of staging Foucauldian 'personal insurrections'\". Post-left anarchist Bob Black in his long critique of Bookchin's philosophy called \"Anarchy After Leftism\" said about post-left anarchy that \"[i]t is, unlike Bookchinism, \"individualistic\" in the sense that if the freedom and happiness of the individual – i.e., each and every really existing person, every Tom, Dick and Murray – is not the measure of the good society, what is?\".\n\nA strong relationship does exist between post-left anarchism and the work of individualist anarchist Max Stirner. Jason McQuinn says that \"when I (and other anti-ideological anarchists) criticize ideology, it is always from a specifically critical, anarchist perspective rooted in both the skeptical, individualist-anarchist philosophy of Max Stirner. Bob Black and Feral Faun/Wolfi Landstreicher also strongly adhere to stirnerist egoist anarchism. Bob Black has humorously suggested the idea of \"marxist stirnerism\".\n\nHakim Bey has said: \"From Stirner's \"Union of Self-Owning Ones\" we proceed to Nietzsche's circle of \"Free Spirits\" and thence to Charles Fourier's \"Passional Series\", doubling and redoubling ourselves even as the Other multiplies itself in the eros of the group\". Bey also wrote: \"The Mackay Society, of which Mark & I are active members, is devoted to the anarchism of Max Stirner, Benj. Tucker & John Henry Mackay [...] The Mackay Society, incidentally, represents a little-known current of individualist thought which never cut its ties with revolutionary labor. Dyer Lum, Ezra & Angela Haywood represent this school of thought; Jo Labadie, who wrote for Tucker's \"Liberty\", made himself a link between the American \"plumb-line\" anarchists, the \"philosophical\" individualists, & the syndicalist or communist branch of the movement; his influence reached the Mackay Society through his son, Laurance. Like the Italian Stirnerites (who influenced us through our late friend Enrico Arrigoni) we support all anti-authoritarian currents, despite their apparent contradictions\".\n\nAs far as posterior individualist anarchists, Jason McQuinn for some time used the pseudonym Lev Chernyi in honor of the Russian individualist anarchist of the same name while Feral Faun has quoted Italian individualist anarchist Renzo Novatore and has translated both Novatore and the young Italian individualist anarchist Bruno Filippi\n\nEgoism has had a strong influence on insurrectionary anarchism, as can be seen in the work of Wolfi Landstreicher. Feral Faun wrote in 1995: In the game of insurgence – a lived guerilla war game – it is strategically necessary to use identities and roles. Unfortunately, the context of social relationships gives these roles and identities the power to define the individual who attempts to use them. So I, Feral Faun, became [...] an anarchist [...] a writer [...] a Stirner-influenced, post-situationist, anti-civilization theorist [...] if not in my own eyes, at least in the eyes of most people who've read my writings.\n\nEuropean individualist anarchism proceeded from the roots laid by William Godwin, Pierre-Joseph Proudhon and Max Stirner. Proudhon was an early pioneer of anarchism as well as of the important individualist anarchist current of mutualism. Stirner became a central figure of individualist anarchism through the publication of his seminal work \"The Ego and Its Own\" which is considered to be \"a founding text in the tradition of individualist anarchism\". Another early figure was Anselme Bellegarrigue. Individualist anarchism expanded and diversified through Europe, incorporating influences from North American individualist anarchism.\n\nEuropean individualist anarchists include Albert Libertad, Bellegarrigue, Oscar Wilde, Émile Armand, Lev Chernyi, John Henry Mackay, Han Ryner, Adolf Brand, Miguel Gimenez Igualada, Renzo Novatore and currently Michel Onfray. Important currents within it include free love, anarcho-naturism and illegalism.\n\nFrom the legacy of Proudhon and Stirner there emerged a strong tradition of French individualist anarchism. An early important individualist anarchist was Anselme Bellegarrigue. He participated in the French Revolution of 1848, was author and editor of \"Anarchie, Journal de l'Ordre and Au fait ! Au fait ! Interprétation de l'idée démocratique\" and wrote the important early Anarchist Manifesto in 1850. Catalan historian of individualist anarchism Xavier Diez reports that during his travels in the United States \"he at least contacted (Henry David) Thoreau and, probably (Josiah) Warren\". \"Autonomie Individuelle\" was an individualist anarchist publication that ran from 1887 to 1888. It was edited by Jean-Baptiste Louiche, Charles Schæffer and Georges Deherme.\n\nLater, this tradition continued with such intellectuals as Albert Libertad, André Lorulot, Émile Armand, Victor Serge, Zo d'Axa and Rirette Maitrejean, who in 1905 developed theory in the main individualist anarchist journal in France, \"L'Anarchie\". Outside this journal, Han Ryner wrote \"Petit Manuel individualiste\" (1903). In 1891, Zo d'Axa created the journal L'En-Dehors.\n\nAnarcho-naturism was promoted by Henri Zisly, Emile Gravelle and Georges Butaud. Butaud was an individualist \"partisan of the \"milieux libres\", publisher of \"Flambeau\" (\"an enemy of authority\") in 1901 in Vienna\" and most of his energies were devoted to creating anarchist colonies (communautés expérimentales) in which he participated in several.\n\nIn this sense, \"the theoretical positions and the vital experiences of [F]rench individualism are deeply iconoclastic and scandalous, even within libertarian circles. The call of nudist naturism, the strong defence of bith control methods, the idea of \"unions of egoists\" with the sole justification of sexual practices, that will try to put in practice, not without difficulties, will establish a way of thought and action, and will result in sympathy within some, and a strong rejection within others\".\nFrench individualist anarchists grouped behind Émile Armand, published \"L'Unique\" after World War II. \"L'Unique\" went from 1945 to 1956 with a total of 110 numbers. Gérard de Lacaze-Duthiers was a French writer, art critic, pacifist and anarchist. Lacaze-Duthiers, an art critic for the Symbolist review journal \"La Plume\", was influenced by Oscar Wilde, Friedrich Nietzsche and Max Stirner. His (1906) \"L'Ideal Humain de l'Art\" helped found the \"artistocracy movement\"—a movement advocating life in the service of art. His ideal was an anti-elitist aestheticism: \"All men should be artists\". Together with André Colomer and Manuel Devaldes, in 1913 he founded \"L'Action d'Art\", an anarchist literary journal. After World War II, he contributed to the journal \"L'Unique\". Within the synthesist anarchist organization, the Fédération Anarchiste, there existed an individualist anarchist tendency alongside anarcho-communist and anarchosyndicalist currents. Individualist anarchists participating inside the Fédération Anarchiste included Charles-Auguste Bontemps, Georges Vincey and André Arru. The new base principles of the francophone Anarchist Federation were written by the individualist anarchist Charles-Auguste Bontemps and the anarcho-communist Maurice Joyeux which established an organization with a plurality of tendencies and autonomy of federated groups organized around synthesist principles. Charles-Auguste Bontemps was a prolific author mainly in the anarchist, freethinking, pacifist and naturist press of the time. His view on anarchism was based around his concept of \"Social Individualism\" on which he wrote extensively. He defended an anarchist perspective which consisted on \"a collectivism of things and an individualism of persons\".\n\nIn 2002, Libertad organized a new version of the \"L'EnDehors\", collaborating with \"Green Anarchy\" and including several contributors, such as Lawrence Jarach, Patrick Mignard, Thierry Lodé, Ron Sakolsky and Thomas Slut. Numerous articles about capitalism, human rights, free love and social fights were published. \"The EnDehors\" continues now as a website, EnDehors.org.\n\nThe prolific contemporary French philosopher Michel Onfray has been writing from an individualist anarchist perspective influenced by Nietzsche, French post-structuralists thinkers such as Michel Foucault and Gilles Deleuze; and Greek classical schools of philosophy such as the Cynics and Cyrenaics. Among the books which best expose Onfray's individualist anarchist perspective include \"La sculpture de soi : la morale esthétique\" (\"The Sculpture of Oneself: Aesthetic Morality\"), \"La philosophie féroce : exercices anarchistes\", \"La puissance d'exister\" and \"Physiologie de Georges Palante, portrait d'un nietzchéen de gauche\" which focuses on French individualist philosopher Georges Palante.\n\nIllegalism is an anarchist philosophy that developed primarily in France, Italy, Belgium and Switzerland during the early 1900s as an outgrowth of Stirner's individualist anarchism. Illegalists usually did not seek moral basis for their actions, recognizing only the reality of \"might\" rather than \"right\"; and for the most part, illegal acts were done simply to satisfy personal desires, not for some greater ideal, although some committed crimes as a form of propaganda of the deed. The illegalists embraced direct action and propaganda of the deed.\n\nInfluenced by theorist Max Stirner's egoism as well as Pierre-Joseph Proudhon (his view that \"Property is theft!\"), Clément Duval and Marius Jacob proposed the theory of la \"reprise individuelle\" (individual reclamation) which justified robbery on the rich and personal direct action against exploiters and the system.\n\nIllegalism first rose to prominence among a generation of Europeans inspired by the unrest of the 1890s, during which Ravachol, Émile Henry, Auguste Vaillant and Sante Geronimo Caserio committed daring crimes in the name of anarchism in what is known as propaganda of the deed. France's Bonnot Gang was the most famous group to embrace illegalism.\n\nIn Italy, individualist anarchism had a strong tendency towards illegalism and violent propaganda by the deed similar to French individualist anarchism, but perhaps more extreme and which emphazised criticism of organization be it anarchist or of other type. In this respect, we can consider notorious magnicides carried out or attempted by individualists Giovanni Passannante, Sante Caserio, Michele Angiolillo, Luigi Luccheni and Gaetano Bresci who murdered King Umberto I. Caserio lived in France and coexisted within French illegalism and later assassinated French President Sadi Carnot. The theoretical seeds of current insurrectionary anarchism were already laid out at the end of 19th century Italy in a combination of individualist anarchism criticism of permanent groups and organization with a socialist class struggle worldview. During the rise of fascism, this thought also motivated Gino Lucetti, Michele Schirru and Angelo Sbardellotto in attempting the assassination of Benito Mussolini.\n\nDuring the early 20th century, the intellectual work of individualist anarchist Renzo Novatore came to importance and he was influenced by Max Stirner, Friedrich Nietzsche, Georges Palante, Oscar Wilde, Henrik Ibsen, Arthur Schopenhauer and Charles Baudelaire. He collaborated in numerous anarchist journals and participated in futurism avant-garde currents. In his thought, he adhered to Stirnerist disrespect for private property, only recognizing property of one's own spirit. Novatore collaborated in the individualist anarchist journal \"Iconoclasta!\" alongside the young Stirnerist illegalist Bruno Filippi.\n\nThe individualist philosopher and poet Renzo Novatore belonged to the leftist section of the avant-garde movement of futurism alongside other individualist anarcho-futurists such as Dante Carnesecchi, Leda Rafanelli, Auro d'Arcola and Giovanni Governato. There was also Pietro Bruzzi who published the journal \"L'Individualista\" in the 1920s alongside Ugo Fedeli and Francesco Ghezzi, but who fell to fascist forces later. Bruzzi also collaborated with the Italian American individualist anarchist publication \"Eresia\" of New York City edited by Enrico Arrigoni.\n\nIn 1945 in Italy during the Founding Congress of the Italian Anarchist Federation, there was a group of individualist anarchists led by Cesare Zaccaria who was an important anarchist of the time. Later during the IX Congress of the Italian Anarchist Federation in Carrara in 1965, a group decided to split off from this organization and created the \"Gruppi di Iniziativa Anarchica\". In the 1970s, it was mostly composed of \"veteran individualist anarchists with an of pacifism orientation, naturism\".\n\nIn the famous Italian insurrectionary anarchist essay written by an anonymous writer, \"At Daggers Drawn with the Existent, its Defenders and its False Critics\", there reads: \"The workers who, during a wildcat strike, carried a banner saying, 'We are not asking for anything' understood that the defeat is in the claim itself ('the claim against the enemy is eternal'). There is no alternative but to take everything. As Stirner said: 'No matter how much you give them, they will always ask for more, because what they want is no less than the end of every concession'\". The contemporary imprisoned Italian insurrectionary anarchist philosopher writes from an explicit individualist anarchist perspective in such essays as \"Critica individualista anarchica alla modernità\" (\"Individualist Anarchist Critique of Modernity\") Horst Fantazzini (March 4, 1939 – December 24, 2001) was an Italian-German individualist anarchist who pursued an illegalist lifestyle and practice until his death in 2001. He gained media notoriety mainly due to his many bank robberies through Italy and other countries. In 1999, the film \"Ormai è fatta!\" appeared based on his life.\n\nWhile Spain was influenced by American individualist anarchism, it was more closely related to the French currents. Around the start of the 20th century, individualism in Spain gathered force through the efforts of people such as Dorado Montero, Ricardo Mella, Federico Urales, Miguel Gimenez Igualada, Mariano Gallardo and J. Elizalde who translated French and American individualists. Important in this respect were also magazines such as \"La Idea Libre\", \"La revista blanca\", \"Etica\", \"Iniciales\", \"Al margen\", \"Estudios\" and \"Nosotros\". The most influential thinkers there were Max Stirner, Émile Armand and Han Ryner. Just as in France, the spread of Esperanto and anationalism had importance just as naturism and free love currents. Later, Armand and Ryner themselves started writing in the Spanish individualist press. Armand's concept of amorous camaraderie had an important role in motivating polyamory as realization of the individual.\n\nCatalan historian Xavier Diez reports that the Spanish individualist anarchist press was widely read by members of anarcho-communist groups and by members of the anarcho-syndicalist trade union CNT. There were also the cases of prominent individualist anarchists such as Federico Urales and Miguel Gimenez Igualada who were members of the CNT and J. Elizalde who was a founding member and first secretary of the Iberian Anarchist Federation (IAF).\n\nSpanish individualist anarchist Miguel Giménez Igualada wrote the lengthy theory book called \"Anarchism\" espousing his individualist anarchism. Between October 1937 and February 1938, he was editor of the individualist anarchist magazine \"Nosotros\" in which many works of Armand and Ryner appeared. He also participated in the publishing of another individualist anarchist maganize \"Al Margen: Publicación quincenal individualista\". In his youth, he engaged in illegalist activities. His thought was deeply influenced by Max Stirner, of which he was the main popularizer in Spain through his own writings. He published and wrote the preface to the fourth edition in Spanish of \"The Ego and Its Own\" from 1900. He proposed the creation of a \"Union of egoists\" to be a federation of individualist anarchists in Spain, but it did not succeed. In 1956, he published an extensive treatise on Stirner, dedicated to fellow individualist anarchist Émile Armand. Afterwards, he traveled and lived in Argentina, Uruguay and Mexico.\n\nFederico Urales was an important individualist anarchist who edited \"La Revista Blanca\". The individualist anarchism of Urales was influenced by Auguste Comte and Charles Darwin. He saw science and reason as a defense against blind servitude to authority. He was critical of influential individualist thinkers such as Nietzsche and Stirner for promoting an asocial egoist individualism and instead promoted an individualism with solidarity seen as a way to guarantee social equality and harmony. He was highly critical of anarcho-syndicalism, which he viewed as plagued by excessive bureaucracy; and he thought that it tended towards reformism. Instead, he favored small groups based on ideological alignment. He supported and participated in the establishment of the IAF in 1927.\n\nIn 1956, Miguel Giménez Igualada—on exile escaping from Franco's dictatorship—published an extensive treatise on Stirner which he dedicated to fellow individualist anarchist Émile Armand. On the subject of individualist anarchist theory, he publisheds \"Anarchism\" in 1968 during his exile in Mexico from Franco's dictatorship in Spain. He was present in the First Congress of the Mexican Anarchist Federation in 1945.\n\nIn 2000, Ateneo Libertario Ricardo Mella, Ateneo Libertario Al Margen, Ateneu Enciclopèdic Popular, Ateneo Libertario de Sant Boi and Ateneu Llibertari Poble Sec y Fundació D'Estudis Llibertaris i Anarcosindicalistes republished Émile Armand's writings on free love and individualist anarchism in a compilation titled \"Individualist anarchism and Amorous camaraderie\". Recently, Spanish historian Xavier Diez has dedicated extensive research on Spanish individualist anarchism as can be seen in his books \"El anarquismo individualista en España: 1923–1938\" and \"Utopia sexual a la premsa anarquista de Catalunya. La revista Ética-Iniciales(1927–1937)\" which deals with free love thought as present in the Spanish individualist anarchist magazine \"Iniciales\".\n\nIn Germany, the Scottish-German John Henry McKay became the most important propagandist for individualist anarchist ideas. He fused Stirnerist egoism with the positions of Benjamin Tucker and actually translated Tucker into German. Two semi-fictional writings of his own, \"Die Anarchisten\" and \"Der Freiheitsucher\", contributed to individualist theory through an updating of egoist themes within a consideration of the anarchist movement. English translations of these works arrived in the United Kingdom and in individualist American circles led by Tucker. McKay is also known as an important European early activist for gay rights.\n\nUsing the pseudonym Sagitta, Mackay wrote a series of works for pederastic emancipation, titled \"Die Buecher der namenlosen Liebe\" (\"Books of the Nameless Love\"). This series was conceived in 1905 and completed in 1913 and included the \"Fenny Skaller\", a story of a pederast. Under the same pseudonym, he also published fiction, such as \"Holland\" (1924) and a pederastic novel of the Berlin boy-bars, \"Der Puppenjunge\" (\"The Hustler\") (1926).\nAdolf Brand (1874–1945) was a German writer, Stirnerist anarchist and pioneering campaigner for the acceptance of male bisexuality and homosexuality. In 1896, Brand published a German homosexual periodical, \"Der Eigene\". This was the first ongoing homosexual publication in the world. The name was taken from writings of egoist philosopher Max Stirner (who had greatly influenced the young Brand) and refers to Stirner's concept of \"self-ownership\" of the individual. \"Der Eigene\" concentrated on cultural and scholarly material and may have had an average of around 1,500 subscribers per issue during its lifetime, although the exact numbers are uncertain. Contributors included Erich Mühsam, Kurt Hiller, John Henry Mackay (under the pseudonym Sagitta) and artists Wilhelm von Gloeden, Fidus and Sascha Schneider. Brand contributed many poems and articles himself. Benjamin Tucker followed this journal from the United States.\n\n\"Der Einzige\" was a German individualist anarchist magazine. It appeared in 1919 as a weekly, then sporadically until 1925 and was edited by cousins Anselm Ruest (pseudonym for Ernst Samuel) and Mynona (pseudonym for Salomo Friedlaender). Its title was adopted from the book \"Der Einzige und sein Eigentum\" (\"The Ego and Its Own\") by Max Stirner. Another influence was the thought of German philosopher Friedrich Nietzsche. The publication was connected to the local expressionist artistic current and the transition from it towards Dada.\n\nThe English Enlightenment political theorist William Godwin was an important influence as mentioned before. The Irish anarchist writer of the Decadent Movement Oscar Wilde influenced individualist anarchists such as Renzo Novatore and gained the admiration of Benjamin Tucker. In his important essay \"The Soul of Man under Socialism\" from 1891, Wilde defended socialism as the way to guarantee individualism and so he saw that \"[w]ith the abolition of private property, then, we shall have true, beautiful, healthy Individualism. Nobody will waste his life in accumulating things, and the symbols for things. One will live. To live is the rarest thing in the world. Most people exist, that is all\". For anarchist historian George Woodcock, \"Wilde's aim in \"The Soul of Man under Socialism\" is to seek the society most favorable to the artist [...] for Wilde art is the supreme end, containing within itself enlightenment and regeneration, to which all else in society must be subordinated [...] Wilde represents the anarchist as aesthete\". Woodcock finds that \"[t]he most ambitious contribution to literary anarchism during the 1890s was undoubtedly Oscar Wilde \"The Soul of Man under Socialism\"\" and finds that it is influenced mainly by the thought of William Godwin.\n\nIn the late 19th century in the United Kingdom, there existed individualist anarchists such as Wordsworth Donisthorpe, Joseph Hiam Levy, Joseph Greevz Fisher, John Badcock Jr., Albert Tarn and Henry Albert Seymour who were close to the United States group around Benjamin Tucker's magazine \"Liberty\". In the mid-1880s, Seymour published a journal called \"The Anarchist\" and also later took a special interest in free love as he participated in the journal \"The Adult: A Journal for the Advancement of Freedom in Sexual Relationships\". \"The Serpent\", issued from London, was the most prominent English-language egoist journal and published from 1898 to 1900 with the subtitle \"A Journal of Egoistic Philosophy and Sociology\". Henry Meulen was another British anarchist who was notable for his support of free banking.\n\nIn the United Kingdom, Herbert Read was influenced highly by egoism as he later approached existentialism (see existentialist anarchism). Albert Camus devoted a section of \"The Rebel\" to Stirner. Although throughout his book Camus is concerned to present \"the rebel\" as a preferred alternative to \"the revolutionary\", he nowhere acknowledges that this distinction is taken from the one that Stirner makes between \"the revolutionary\" and \"the insurrectionist\". Sidney Parker is a British egoist individualist anarchist who wrote articles and edited anarchist journals from 1963 to 1993 such as \"Minus One\", \"Egoist\", and \"Ego\". Donald Rooum is an English anarchist cartoonist and writer with a long association with Freedom Press. Rooum stated that for his thought, \"[t]he most influential source is Max Stirner. I am happy to be called a Stirnerite anarchist, provided 'Stirnerite' means one who agrees with Stirner's general drift, not one who agrees with Stirner's every word\". \"An Anarchist FAQ\" reports: \"From meeting anarchists in Glasgow during the Second World War, long-time anarchist activist and artist Donald Rooum likewise combined Stirner and anarcho-communism\".\n\nIn the hybrid of post-structuralism and anarchism called post-anarchism, Saul Newman has written a lot on Stirner and his similarities to post-structuralism. He writes: Max Stirner's impact on contemporary political theory is often neglected. However in Stirner's political thinking there can be found a surprising convergence with poststructuralist theory, particularly with regard to the function of power. Andrew Koch, for instance, sees Stirner as a thinker who transcends the Hegelian tradition he is usually placed in, arguing that his work is a precursor poststructuralist ideas about the foundations of knowledge and truth.\n\nNewman has published several essays on Stirner. \"War on the State: Stirner and Deleuze's Anarchism\" and \"Empiricism, Pluralism, and Politics in Deleuze and Stirner\" discusses what he sees are similarities between Stirner's thought and that of Gilles Deleuze. In \"Spectres of Stirner: A Contemporary Critique of Ideology\", he discusses the conception of ideology in Stirner. In \"Stirner and Foucault: Toward a Post-Kantian Freedom\", similarities between Stirner and Michel Foucault. He also wrote \"Politics of the Ego: Stirner's Critique of Liberalism\".\n\nIndividualist anarchism was one of the three categories of anarchism in Russia, along with the more prominent anarcho-communism and anarcho-syndicalism. The ranks of the Russian individualist anarchists were predominantly drawn from the intelligentsia and the working class. For anarchist historian Paul Avrich, \"[t]he two leading exponents of individualist anarchism, both based in Moscow, were Aleksei Alekseevich Borovoi and Lev Chernyi (Pavel Dmitrievich Turchaninov). From Nietzsche, they inherited the desire for a complete overturn of all values accepted by bourgeois society political, moral, and cultural. Furthermore, strongly influenced by Max Stirner and Benjamin Tucker, the German and American theorists of individualist anarchism, they demanded the total liberation of the human personality from the fetters of organized society\".\n\nSome Russian individualists anarchists \"found the ultimate expression of their social alienation in violence and crime, others attached themselves to avant-garde literary and artistic circles, but the majority remained \"philosophical\" anarchists who conducted animated parlor discussions and elaborated their individualist theories in ponderous journals and books\".\n\nLev Chernyi was an important individualist anarchist involved in resistance against the rise to power of the Bolshevik Party as he adhered mainly to Stirner and the ideas of Tucker. In 1907, he published a book entitled \"Associational Anarchism\", in which he advocated the \"free association of independent individuals\". On his return from Siberia in 1917, he enjoyed great popularity among Moscow workers as a lecturer. Chernyi was also Secretary of the Moscow Federation of Anarchist Groups, which was formed in March 1917. He was an advocate \"for the seizure of private homes\", which was an activity seen by the anarchists after the October Revolution as direct expropriation on the bourgoise. He died after being accused of participation in an episode in which this group bombed the headquarters of the Moscow Committee of the Communist Party. Although most likely not being really involved in the bombing, he might have died of torture.\n\nChernyi advocated a Nietzschean overthrow of the values of bourgeois Russian society, and rejected the voluntary communes of anarcho-communist Peter Kropotkin as a threat to the freedom of the individual. Scholars including Avrich and Allan Antliff have interpreted this vision of society to have been greatly influenced by the individualist anarchists Max Stirner and Benjamin Tucker. Subsequent to the book's publication, Chernyi was imprisoned in Siberia under the Russian Czarist regime for his revolutionary activities.\n\nOn the other hand, Aleksei Borovoi (1876?–1936) was a professor of philosophy at Moscow University, \"a gifted orator and the author of numerous books, pamphlets, and articles which attempted to reconcile individualist anarchism with the doctrines of syndicallism\". He wrote among other theoretical works \"Anarkhizm\" in 1918, just after the October Revolution; and \"Anarchism and Law\". For him, \"the chief importance is given not to Anarchism as the aim but to Anarchy as the continuous quest for the aim\". He manifests there that \"[n]o social ideal, from the point of view of anarchism, could be referred to as absolute in a sense that supposes it's the crown of human wisdom, the end of social and ethical quest of man\".\n\nArgentine anarchist historian Angel Cappelletti reports that in Argentina \"[a]mong the workers that came from Europe in the 2 first decades of the century, there was curiously some stirnerian individualists influenced by the philosophy of Nietzsche, that saw syndicalism as a potential enemy of anarchist ideology. They established [...] affinity groups that in 1912 came to, according to Max Nettlau, to the number of 20. In 1911 there appeared, in Colón, the periodical \"El Único\", that defined itself as 'Publicación individualista'\".\n\nVicente Rojas Lizcano, whose pseudonym was Biófilo Panclasta, was a Colombian individualist anarchist writer and activist. In 1904, he began using the name Biofilo Panclasta. \"Biofilo\" in Spanish stands for \"lover of life\" and \"Panclasta\" for \"enemy of all\". He visited more than fifty countries propagandizing for anarchism which in his case was highly influenced by the thought of Stirner and Nietszche. Among his written works there are \"Siete años enterrado vivo en una de las mazmorras de Gomezuela: Horripilante relato de un resucitado\"(1932) and \"Mis prisiones, mis destierros y mi vida\" (1929) which talk about his many adventures while living his life as an adventurer, activist and vagabond as well as his thought and the many times he was imprisoned in different countries.\nMaria Lacerda de Moura was a Brazilian teacher, journalist, anarcha-feminist and individualist anarchist. Her ideas regarding education were largely influenced by Francisco Ferrer. She later moved to São Paulo and became involved in journalism for the anarchist and labor press. There she also lectured on topics including education, women's rights, free love and antimilitarism. Her writings and essays garnered her attention not only in Brazil, but also in Argentina and Uruguay. In February 1923, she launched \"Renascença\", a periodical linked with the anarchist, progressive and freethinking circles of the period. Her thought was mainly influenced by individualist anarchists such as Han Ryner and Émile Armand. She maintained contact with Spanish individualist anarchist circles.\n\nHorst Matthai Quelle was a Spanish language German anarchist philosopher influenced by Max Stirner. In 1938 at the beginning of the German economic crisis and the rise of Nazism and fascism in Europe, Quelle moved to Mexico. Quelle earned his undergraduate degree, master's and doctorate in philosophy at the National Autonomous University of Mexico, where he returned as a professor of philosophy in the 1980s. He argued that since the individual gives form to the world, he is those objects, the others and the whole universe. One of his main views was a \"theory of infinite worlds\" which for him was developed by pre-socratic philosophers.\n\nDuring the 1990s in Argentina, there appeared a Stirnerist publication called \"El Único: publicacion periódica de pensamiento individualista\".\n\nPhilosopher Murray Bookchin criticized individualist anarchism for its opposition to democracy and its embrace of \"lifestylism\" at the expense of class struggle. Bookchin claimed that individualist anarchism supports only negative liberty and rejects the idea of positive liberty. Philosopher Albert Meltzer proposed that individualist anarchism differs radically from revolutionary anarchism and that it \"is sometimes too readily conceded 'that this is, after all, anarchism'\". He claimed that Benjamin Tucker's acceptance of the use of a private police force (including to break up violent strikes to protect the \"employer's 'freedom'\") is contradictory to the definition of anarchism as \"no government\".\n\nPhilosopher George Bernard Shaw initially had flirtations with individualist anarchism before coming to the conclusion that it was \"the negation of socialism, and is, in fact, unsocialism carried as near to its logical conclusion as any sane man dare carry it\". Shaw's argument was that even if wealth was initially distributed equally, the degree of \"laissez-faire\" advocated by Tucker would result in the distribution of wealth becoming unequal because it would permit private appropriation and accumulation. According to academic Carlotta Anderson, American individualist anarchists accept that free competition results in unequal wealth distribution, but they \"do not see that as an injustice\". Tucker explained: \"If I go through life free and rich, I shall not cry because my neighbor, equally free, is richer. Liberty will ultimately make all men rich; it will not make all men equally rich. Authority may (and may not) make all men equally rich in purse; it certainly will make them equally poor in all that makes life best worth living\".\n\n The term \"individualist anarchism\" is often used as a classificatory term, but in very different ways. Some sources, such as \"An Anarchist FAQ\", use the classification \"social anarchism/individualist anarchism\". Some see individualist anarchism as distinctly non-socialist and use the classification \"socialist anarchism/individualist anarchism\" accordingly. Other classifications include \"mutualist/communal\" anarchism.<br>\nSee for example the Winter 2006 issue of the \"Journal of Libertarian Studies\" dedicated to reviews of Kevin Carson's \"Studies in Mutualist Political Economy.\" Mutualists compose one bloc, along with agorists and geo-libertarians, in the recently formed Alliance of the Libertarian Left.<br>\nKonkin wrote the article \"Copywrongs\" in opposition to the concept and Schulman countered SEK3's arguments in \"Informational Property: Logorights\".<br>\n\n"}
{"id": "14937", "url": "https://en.wikipedia.org/wiki?curid=14937", "title": "Italo Calvino", "text": "Italo Calvino\n\nItalo Calvino (; 15 October 1923 – 19 September 1985) was an Italian journalist and writer of short stories and novels. His best known works include the \"Our Ancestors\" trilogy (1952–1959), the \"Cosmicomics\" collection of short stories (1965), and the novels \"Invisible Cities\" (1972) and \"If on a winter's night a traveler\" (1979).\n\nHe was the most-translated contemporary Italian writer at the time of his death.\n\nItalo Calvino was born in Santiago de las Vegas, a suburb of Havana, Cuba, in 1923. His father, Mario, was a tropical agronomist and botanist who also taught agriculture and floriculture. Born 47 years earlier in Sanremo, Italy, Mario Calvino had emigrated to Mexico in 1909 where he took up an important position with the Ministry of Agriculture. In an autobiographical essay, Italo Calvino explained that his father \"had been in his youth an anarchist, a follower of Kropotkin and then a Socialist Reformist\". In 1917, Mario left for Cuba to conduct scientific experiments, after living through the Mexican Revolution.\n\nCalvino's mother, Giuliana Luigia Evelina \"Eva\" Mameli, was a botanist and university professor. A native of Sassari in Sardinia and 11 years younger than her husband, she married while still a junior lecturer at Pavia University. Born into a secular family, Eva was a pacifist educated in the \"religion of civic duty and science\". Eva gave Calvino his unusual first name to remind him of his Italian heritage, although since he wound up growing up in Italy after all, Calvino thought his name sounded \"belligerently nationalist\". Calvino described his parents as being \"very different in personality from one another\", suggesting perhaps deeper tensions behind a comfortable, albeit strict, middle-class upbringing devoid of conflict. As an adolescent, he found it hard relating to poverty and the working-class, and was \"ill at ease\" with his parents' openness to the laborers who filed into his father's study on Saturdays to receive their weekly paycheck.\n\nIn 1925, less than two years after Calvino's birth, the family returned to Italy and settled permanently in Sanremo on the Ligurian coast. Calvino's brother Floriano, who became a distinguished geologist, was born in 1927.\n\nThe family divided their time between the Villa Meridiana, an experimental floriculture station which also served as their home, and Mario's ancestral land at San Giovanni Battista. On this small working farm set in the hills behind Sanremo, Mario pioneered in the cultivation of then exotic fruits such as avocado and grapefruit, eventually obtaining an entry in the \"Dizionario biografico degli italiani\" for his achievements. The vast forests and luxuriant fauna omnipresent in Calvino's early fiction such as \"The Baron in the Trees\" derives from this \"legacy\". In an interview, Calvino stated that \"San Remo continues to pop out in my books, in the most diverse pieces of writing.\" He and Floriano would climb the tree-rich estate and perch for hours on the branches reading their favorite adventure stories. Less salubrious aspects of this \"paternal legacy\" are described in \"The Road to San Giovanni\", Calvino's memoir of his father in which he exposes their inability to communicate: \"Talking to each other was difficult. Both verbose by nature, possessed of an ocean of words, in each other's presence we became mute, would walk in silence side by side along the road to San Giovanni.\" A fan of Rudyard Kipling's \"The Jungle Book\" as a child, Calvino felt that his early interest in stories made him the \"black sheep\" of a family that held literature in less esteem than the sciences. Fascinated by American movies and cartoons, he was equally attracted to drawing, poetry, and theatre. On a darker note, Calvino recalled that his earliest memory was of a Marxist professor who had been brutally assaulted by Benito Mussolini's Blackshirts: \"I remember clearly that we were at dinner when the old professor came in with his face beaten up and bleeding, his bowtie all torn, asking for help.\"\n\nOther legacies include the parents' beliefs in Freemasonry, Republicanism with elements of Anarchism and Marxism. Austere freethinkers with an intense hatred of the ruling National Fascist Party, Eva and Mario also refused to give their sons any education in the Catholic Faith or any other religion. Italo attended the English nursery school St George's College, followed by a Protestant elementary private school run by Waldensians. His secondary schooling, with a classical lyceum curriculum, was completed at the state-run Liceo Gian Domenico Cassini where, at his parents' request, he was exempted from religion classes but frequently asked to justify his anti-conformism to teachers, janitors, and fellow pupils. In his mature years, Calvino described the experience as having made him \"tolerant of others' opinions, particularly in the field of religion, remembering how irksome it was to hear myself mocked because I did not follow the majority's beliefs\". In 1938, Eugenio Scalfari, who went on to found the weekly magazine \"L'Espresso\" and \"La Repubblica\", a major Italian newspaper, came from Civitavecchia to join the same class though a year younger, and they shared the same desk. The two teenagers formed a lasting friendship, Calvino attributing his political awakening to their university discussions. Seated together \"on a huge flat stone in the middle of a stream near our land\", he and Scalfari founded the MUL (University Liberal Movement).\n\nEva managed to delay her son's enrolment in the Party's armed scouts, the \"Balilla Moschettieri\", and then arranged that he be excused, as a non-Catholic, from performing devotional acts in Church. But later on, as a compulsory member, he could not avoid the assemblies and parades of the \"Avanguardisti\", and was forced to participate in the Italian invasion of the French Riviera in June 1940.\n\nIn 1941, Calvino enrolled at the University of Turin, choosing the Agriculture Faculty where his father had previously taught courses in agronomy. Concealing his literary ambitions to please his family, he passed four exams in his first year while reading anti-Fascist works by Elio Vittorini, Eugenio Montale, Cesare Pavese, Johan Huizinga, and Pisacane, and works by Max Planck, Werner Heisenberg, and Albert Einstein on physics. Disdainful of Turin students, Calvino saw himself as enclosed in a \"provincial shell\" that offered the illusion of immunity from the Fascist nightmare: \"We were ‘hard guys’ from the provinces, hunters, snooker-players, show-offs, proud of our lack of intellectual sophistication, contemptuous of any patriotic or military rhetoric, coarse in our speech, regulars in the brothels, dismissive of any romantic sentiment and desperately devoid of women.\"\n\nCalvino transferred to the University of Florence in 1943 and reluctantly passed three more exams in agriculture. By the end of the year, the Germans had succeeded in occupying Liguria and setting up Benito Mussolini's puppet Republic of Salò in northern Italy. Now twenty years old, Calvino refused military service and went into hiding. Reading intensely in a wide array of subjects, he also reasoned politically that, of all the partisan groupings, the communists were the best organized with \"the most convincing political line\".\n\nIn spring 1944, Eva encouraged her sons to enter the Italian Resistance in the name of \"natural justice and family virtues\". Using the battlename of \"Santiago\", Calvino joined the \"Garibaldi Brigades\", a clandestine Communist group and, for twenty months, endured the fighting in the Maritime Alps until 1945 and the Liberation. As a result of his refusal to be a conscript, his parents were held hostage by the Nazis for an extended period at the Villa Meridiana. Calvino wrote of his mother's ordeal that \"she was an example of tenacity and courage… behaving with dignity and firmness before the SS and the Fascist militia, and in her long detention as a hostage, not least when the blackshirts three times pretended to shoot my father in front of her eyes. The historical events which mothers take part in acquire the greatness and invincibility of natural phenomena\".\n\nCalvino settled in Turin in 1945, after a long hesitation over living there or in Milan. He often humorously belittled this choice, describing Turin as a \"city that is serious but sad\". Returning to university, he abandoned Agriculture for the Arts Faculty. A year later, he was initiated into the literary world by Elio Vittorini, who published his short story \"Andato al comando\" (1945; \"Gone to Headquarters\") in \"Il Politecnico\", a Turin-based weekly magazine associated with the university. The horror of the war had not only provided the raw material for his literary ambitions but deepened his commitment to the Communist cause. Viewing civilian life as a continuation of the partisan struggle, he confirmed his membership of the Italian Communist Party. On reading Vladimir Lenin's \"State and Revolution\", he plunged into post-war political life, associating himself chiefly with the worker's movement in Turin.\n\nIn 1947, he graduated with a Master's thesis on Joseph Conrad, wrote short stories in his spare time, and landed a job in the publicity department at the Einaudi publishing house run by Giulio Einaudi. Although brief, his stint put him in regular contact with Cesare Pavese, Natalia Ginzburg, Norberto Bobbio, and many other left-wing intellectuals and writers. He then left Einaudi to work as a journalist for the official Communist daily, \"L'Unità\", and the newborn Communist political magazine, \"Rinascita\". During this period, Pavese and poet Alfonso Gatto were Calvino's closest friends and mentors.\n\nHis first novel, \"Il sentiero dei nidi di ragno\" (\"The Path to the Nest of Spiders\") written with valuable editorial advice from Pavese, won the Premio Riccione on publication in 1947. With sales topping 5000 copies, a surprise success in postwar Italy, the novel inaugurated Calvino's neorealist period. In a clairvoyant essay, Pavese praised the young writer as a \"squirrel of the pen\" who \"climbed into the trees, more for fun than fear, to observe partisan life as a fable of the forest\". In 1948, he interviewed one of his literary idols, Ernest Hemingway, travelling with Natalia Ginzburg to his home in Stresa.\n\n\"Ultimo viene il corvo\" (\"The Crow Comes Last\"), a collection of stories based on his wartime experiences, was published to acclaim in 1949. Despite the triumph, Calvino grew increasingly worried by his inability to compose a worthy second novel. He returned to Einaudi in 1950, responsible this time for the literary volumes. He eventually became a consulting editor, a position that allowed him to hone his writing talent, discover new writers, and develop into \"a reader of texts\". In late 1951, presumably to advance in the Communist Party, he spent two months in the Soviet Union as correspondent for \"l'Unità\". While in Moscow, he learned of his father's death on 25 October. The articles and correspondence he produced from this visit were published in 1952, winning the Saint-Vincent Prize for journalism.\n\nOver a seven-year period, Calvino wrote three realist novels, \"The White Schooner\" (1947–1949), \"Youth in Turin\" (1950–1951), and \"The Queen's Necklace\" (1952–54), but all were deemed defective. During the eighteen months it took to complete \"I giovani del Po\" (\"Youth in Turin\"), he made an important self-discovery: \"I began doing what came most naturally to me – that is, following the memory of the things I had loved best since boyhood. Instead of making myself write the book I \"ought\" to write, the novel that was expected of me, I conjured up the book I myself would have liked to read, the sort by an unknown writer, from another age and another country, discovered in an attic.\" The result was \"Il visconte dimezzato\" (1952; \"The Cloven Viscount\") composed in 30 days between July and September 1951. The protagonist, a seventeenth century viscount sundered in two by a cannonball, incarnated Calvino's growing political doubts and the divisive turbulence of the Cold War. Skillfully interweaving elements of the fable and the fantasy genres, the allegorical novel launched him as a modern \"fabulist\". In 1954, Giulio Einaudi commissioned his \"Fiabe Italiane\" (1956; \"Italian Folktales\") on the basis of the question, \"Is there an Italian equivalent of the Brothers Grimm?\" For two years, Calvino collated tales found in 19th century collections across Italy then translated 200 of the finest from various dialects into Italian. Key works he read at this time were Vladimir Propp's \"Morphology of the Folktale\" and \"Historical Roots of Russian Fairy Tales\", stimulating his own ideas on the origin, shape and function of the story.\n\nIn 1952 Calvino wrote with Giorgio Bassani for \"Botteghe Oscure\", a magazine named after the popular name of the party's head-offices in Rome. He also worked for \"Il Contemporaneo\", a Marxist weekly.\n\nFrom 1955 to 1958 Calvino had an affair with Italian actress Elsa De Giorgi, a married, older woman. Excerpts of the hundreds of love letters Calvino wrote to her were published in the \"Corriere della Sera\" in 2004, causing some controversy.\n\nIn 1957, disillusioned by the 1956 Soviet invasion of Hungary, Calvino left the Italian Communist Party. In his letter of resignation published in \"L'Unità\" on 7 August, he explained the reason of his dissent (the violent suppression of the Hungarian uprising and the revelation of Joseph Stalin's crimes) while confirming his \"confidence in the democratic perspectives\" of world Communism. He withdrew from taking an active role in politics and never joined another party. Ostracized by the PCI party leader Palmiro Togliatti and his supporters on publication of \"Becalmed in the Antilles\" (\"La gran bonaccia delle Antille\"), a satirical allegory of the party's immobilism, Calvino began writing \"The Baron in the Trees\". Completed in three months and published in 1957, the fantasy is based on the \"problem of the intellectual's political commitment at a time of shattered illusions\". He found new outlets for his periodic writings in the journals \"Città aperta\" and \"Tempo presente\", the magazine \"Passato e presente\", and the weekly \"Italia Domani\". With Vittorini in 1959, he became co-editor of \"'Il Menabò\", a cultural journal devoted to literature in the modern industrial age, a position he held until 1966.\n\nDespite severe restrictions in the US against foreigners holding communist views, Calvino was allowed to visit the United States, where he stayed six months from 1959 to 1960 (four of which he spent in New York), after an invitation by the Ford Foundation. Calvino was particularly impressed by the \"New World\": \"Naturally I visited the South and also California, but I always felt a New Yorker. My city is New York.\" The letters he wrote to Einaudi describing this visit to the United States were first published as \"American Diary 1959–1960\" in \"Hermit in Paris\" in 2003.\n\nIn 1962 Calvino met Argentinian translator Esther Judith Singer (\"Chichita\") and married her in 1964 in Havana, during a trip in which he visited his birthplace and was introduced to Ernesto \"Che\" Guevara. On 15 October 1967, a few days after Guevara's death, Calvino wrote a tribute to him that was published in Cuba in 1968, and in Italy thirty years later. He and his wife settled in Rome in the via Monte Brianzo where their daughter, Giovanna, was born in 1965. Once again working for Einaudi, Calvino began publishing some of his \"Cosmicomics\" in \"Il Caffè\", a literary magazine.\n\nVittorini's death in 1966 greatly affected Calvino. He went through what he called an \"intellectual depression\", which the writer himself described as an important passage in his life: \"...I ceased to be young. Perhaps it's a metabolic process, something that comes with age, I'd been young for a long time, perhaps too long, suddenly I felt that I had to begin my old age, yes, old age, perhaps with the hope of prolonging it by beginning it early.\"\n\nIn the fermenting atmosphere that evolved into 1968's cultural revolution (the French May), he moved with his family to Paris in 1967, setting up home in a villa in the Square de Châtillon. Nicknamed \"L'ironique amusé\", he was invited by Raymond Queneau in 1968 to join the Oulipo (\"Ouvroir de littérature potentielle\") group of experimental writers where he met Roland Barthes, Georges Perec, and Claude Lévi-Strauss, all of whom influenced his later production. That same year, he turned down the Viareggio Prize for \"Ti con zero\" (\"Time and the Hunter\") on the grounds that it was an award given by \"institutions emptied of meaning\". He accepted, however, both the Asti Prize and the Feltrinelli Prize for his writing in 1970 and 1972, respectively. In two autobiographical essays published in 1962 and 1970, Calvino described himself as \"atheist\" and his outlook as \"non-religious\".\n\nCalvino had more intense contacts with the academic world, with notable experiences at the Sorbonne (with Barthes) and the University of Urbino. His interests included classical studies: Honoré de Balzac, Ludovico Ariosto, Dante, Ignacio de Loyola, Cervantes, Shakespeare, Cyrano de Bergerac, and Giacomo Leopardi. Between 1972–1973 Calvino published two short stories, \"The Name, the Nose\" and the Oulipo-inspired \"The Burning of the Abominable House\" in the Italian edition of \"Playboy\". He became a regular contributor to the Italian newspaper \"Corriere della Sera\", spending his summer vacations in a house constructed in Roccamare near Castiglione della Pescaia, Tuscany.\n\nIn 1975 Calvino was made Honorary Member of the American Academy. Awarded the Austrian State Prize for European Literature in 1976, he visited Mexico, Japan, and the United States where he gave a series of lectures in several American towns. After his mother died in 1978 at the age of 92, Calvino sold Villa Meridiana, the family home in San Remo. Two years later, he moved to Rome in Piazza Campo Marzio near the Pantheon and began editing the work of Tommaso Landolfi for Rizzoli. Awarded the French Légion d'honneur in 1981, he also accepted to be jury president of the 29th Venice Film Festival.\n\nDuring the summer of 1985, Calvino prepared a series of texts on literature for the Charles Eliot Norton Lectures to be delivered at Harvard University in the fall. On 6 September, he was admitted to the ancient hospital of Santa Maria della Scala in Siena where he died during the night between 18 and 19 September of a cerebral hemorrhage. His lecture notes were published posthumously in Italian in 1988 and in English as \"Six Memos for the Next Millennium\" in 1993.\n\n\nA selected bibliography of Calvino's writings follows, listing the works that have been translated into and published in English, along with a few major untranslated works. More exhaustive bibliographies can be found in Martin McLaughlin's \"Italo Calvino\", and Beno Weiss's \"Understanding Italo Calvino\".\n\n\n\n\nThe \"Scuola Italiana Italo Calvino\", an Italian curriculum school in Moscow, Russia, is named after him. A crater on the planet Mercury, Calvino, and a main belt asteroid, \"22370 Italocalvino\", are also named after him.\n\n\n\n\nGeneral\n\n\n"}
{"id": "14939", "url": "https://en.wikipedia.org/wiki?curid=14939", "title": "Intercontinental ballistic missile", "text": "Intercontinental ballistic missile\n\nAn intercontinental ballistic missile (ICBM) is a guided ballistic missile with a minimum range of primarily designed for nuclear weapons delivery (delivering one or more thermonuclear warheads). Similarly, conventional, chemical, and biological weapons can also be delivered with varying effectiveness, but have never been deployed on ICBMs. Most modern designs support multiple independently targetable reentry vehicles (MIRVs), allowing a single missile to carry several warheads, each of which can strike a different target.\n\nEarly ICBMs had limited precision, which made them suitable for use only against the largest targets, such as cities. They were seen as a \"safe\" basing option, one that would keep the deterrent force close to home where it would be difficult to attack. Attacks against military targets (especially hardened ones) still demanded the use of a more precise, manned bomber. Second- and third-generation designs (such as the LGM-118 Peacekeeper) dramatically improved accuracy to the point where even the smallest point targets can be successfully attacked.\n\nICBMs are differentiated by having greater range and speed than other ballistic missiles: intermediate-range ballistic missiles (IRBMs), medium-range ballistic missiles (MRBMs), short-range ballistic missiles (SRBMs) and tactical ballistic missiles (TBMs). Short and medium-range ballistic missiles are known collectively as theatre ballistic missiles.\n\nThe development of the world's first practical design for an ICBM, A9/10, intended for use in bombing New York and other American cities, was undertaken in Nazi Germany by the team of Wernher von Braun under \"Projekt Amerika\". The ICBM A9/A10 rocket initially was intended to be guided by radio, but was changed to be a piloted craft after the failure of Operation Elster. The second stage of the A9/A10 rocket was tested a few times in January and February 1945. The progenitor of the A9/A10 was the German V-2 rocket, also designed by von Braun and widely used at the end of World War II to bomb British and Belgian cities. All of these rockets used liquid propellants. Following the war, von Braun and other leading German scientists were relocated to the United States to work directly for the US Army through Operation Paperclip, developing the IRBMs, ICBMs, and launchers.\n\nThis technology was also predicted by US Army General Hap Arnold, who wrote in 1943:\n\nIn the immediate post-war era, the US and USSR both started rocket research programs based on the German wartime designs, especially the V-2. In the US, each branch of the military started its own programs, leading to considerable duplication of effort. In the USSR, rocket research was centrally organized, although several teams worked on different designs. Early designs from both countries were short-range missiles, like the V-2, but improvements quickly followed.\n\nIn the USSR, early development was focused on missiles able to attack European targets. This changed in 1953 when Sergei Korolyov was directed to start development of a true ICBM able to deliver newly developed hydrogen bombs. Given steady funding throughout, the R-7 developed with some speed. The first launch took place on 15 May 1957 and led to an unintended crash from the site. The first successful test followed on 21 August 1957; the R-7 flew over and became the world's first ICBM. The first strategic-missile unit became operational on 9 February 1959 at Plesetsk in north-west Russia.\n\nIt was the same R-7 launch vehicle that placed the first artificial satellite in space, Sputnik, on 4 October 1957. The first human spaceflight in history was accomplished on a derivative of R-7, Vostok, on 12 April 1961, by Soviet cosmonaut Yuri Gagarin. A heavily modernized version of the R-7 is still used as the launch vehicle for the Soviet/Russian Soyuz spacecraft, marking more than 60 years of operational history of Sergei Korolyov's original rocket design.\nThe U.S. initiated ICBM research in 1946 with the RTV-A-2 Hiroc project. This was a three-stage effort with the ICBM development not starting until the third stage. However, funding was cut after only three partially successful launches in 1948 of the second stage design, used to test variations on the V-2 design. With overwhelming air superiority and truly intercontinental bombers, the newly forming US Air Force did not take the problem of ICBM development seriously. Things changed in 1953 with the Soviet testing of their first thermonuclear weapon, but it was not until 1954 that the Atlas missile program was given the highest national priority. The Atlas A first flew on 11 June 1957; the flight lasted only about 24 seconds before the rocket blew up. The first successful flight of an Atlas missile to full range occurred 28 November 1958. The first armed version of the Atlas, the Atlas D, was declared operational in January 1959 at Vandenberg, although it had not yet flown. The first test flight was carried out on 9 July 1959, and the missile was accepted for service on 1 September.\n\nThe R-7 and Atlas each required a large launch facility, making them vulnerable to attack, and could not be kept in a ready state. Failure rates were very high throughout the early years of ICBM technology. Human spaceflight programs (Vostok, Mercury, Voskhod, Gemini, etc.) served as a highly visible means of demonstrating confidence in reliability, with successes translating directly to national defense implications. The US was well behind the Soviet Union in the Space Race, so U.S. President John F. Kennedy increased the stakes with the Apollo program, which used Saturn rocket technology that had been funded by President Dwight D. Eisenhower.\nThese early ICBMs also formed the basis of many space launch systems. Examples include R-7, Atlas, Redstone, Titan, and Proton, which was derived from the earlier ICBMs but never deployed as an ICBM. The Eisenhower administration supported the development of solid-fueled missiles such as the LGM-30 Minuteman, Polaris and Skybolt. Modern ICBMs tend to be smaller than their ancestors, due to increased accuracy and smaller and lighter warheads, and use solid fuels, making them less useful as orbital launch vehicles.\n\nThe Western view of the deployment of these systems was governed by the strategic theory of Mutual Assured Destruction. In the 1950s and 1960s, development began on Anti-Ballistic Missile systems by both the U.S. and USSR; these systems were restricted by the 1972 ABM treaty. The first successful ABM test were conducted by the USSR in 1961, that later deployed a fully operating system defending Moscow in the 1970s (see Moscow ABM system).\n\nThe 1972 SALT treaty froze the number of ICBM launchers of both the U.S. and the USSR at existing levels, and allowed new submarine-based SLBM launchers only if an equal number of land-based ICBM launchers were dismantled. Subsequent talks, called SALT II, were held from 1972 to 1979 and actually reduced the number of nuclear warheads held by the U.S. and USSR. SALT II was never ratified by the United States Senate, but its terms were nevertheless honored by both sides until 1986, when the Reagan administration \"withdrew\" after accusing the USSR of violating the pact.\n\nIn the 1980s, President Ronald Reagan launched the Strategic Defense Initiative as well as the MX and Midgetman ICBM programs.\n\nChina developed a minimal independent nuclear deterrent entering its own cold war after an ideological split with the Soviet Union beginning in the early 1960s. After first testing a domestic built nuclear weapon in 1964, it went on to develop various warheads and missiles. Beginning in the early 1970s, the liquid fuelled DF-5 ICBM was developed and used as a satellite launch vehicle in 1975. The DF-5, with range of long enough to strike the western US and the USSR, was silo deployed with the first pair in service by 1981 with possibly twenty missiles in service by the late 1990s. China also deployed the JL-1 Medium-range ballistic missile with a reach of aboard the ultimately unsuccessful type 92 submarine.\n\nIn 1991, the United States and the Soviet Union agreed in the START I treaty to reduce their deployed ICBMs and attributed warheads.\n\n, all five of the nations with permanent seats on the United Nations Security Council have operational long-range ballistic missile systems; Russia, the United States, and China also have land-based ICBMs (the US missiles are silo-based, while China and Russia have both silo and road-mobile (DF-31, RT-2PM2 Topol-M missiles).\n\nIsrael is believed to have deployed a road mobile nuclear ICBM, the Jericho III, which entered service in 2008; an upgraded version is in development.\n\nIndia successfully test fired Agni V, with a strike range of more than on 19 April 2012, claiming entry into the ICBM club. The missile's actual range is speculated by foreign researchers to be up to with India having downplayed its capabilities to avoid causing concern to other countries.\n\nBy 2012 there was speculation by some intelligence agencies that North Korea is developing an ICBM. North Korea successfully put a satellite into space on 12 December 2012 using the Unha-3 rocket. The United States claimed that the launch was in fact a way to test an ICBM. (See Timeline of first orbital launches by country.) In early July 2017, North Korea claimed for the first time to have tested successfully an ICBM capable of carrying a large thermonuclear warhead.\n\nIn July 2014, China announced the development of its newest generation of ICBM, the Dongfeng-41 (DF-41), which has a range of 12,000 kilometres (7,500 miles), capable of reaching the United States, and which analysts believe is capable of being outfitted with MIRV technology.\n\nMost countries in the early stages of developing ICBMs have used liquid propellants, with the known exceptions being the Indian Agni-V, the planned but cancelled South African RSA-4 ICBM, and the now in service Israeli Jericho III.\n\nThe RS-28 Sarmat (Russian: РС-28 Сармат; NATO reporting name: SATAN 2), is a Russian liquid-fueled, MIRV-equipped, super-heavy thermonuclear armed intercontinental ballistic missile in development by the Makeyev Rocket Design Bureau from 2009, intended to replace the previous R-36 missile. Its large payload would allow for up to 10 heavy warheads or 15 lighter ones or up to 24 hypersonic glide vehicles Yu-74, or a combination of warheads and massive amounts of countermeasures designed to defeat anti-missile systems; it was heralded by the Russian military as a response to the U.S. Prompt Global Strike.\n\nThe following flight phases can be distinguished:\n\nICBMs usually use the trajectory which optimizes range for a given amount of payload (the \"minimum-energy trajectory\"); an alternative is a depressed trajectory, which allows less payload, shorter flight time, and has a much lower apogee.\n\nModern ICBMs typically carry multiple independently targetable reentry vehicles (\"MIRVs\"), each of which carries a separate nuclear warhead, allowing a single missile to hit multiple targets. MIRV was an outgrowth of the rapidly shrinking size and weight of modern warheads and the Strategic Arms Limitation Treaties which imposed limitations on the number of launch vehicles (SALT I and SALT II). It has also proved to be an \"easy answer\" to proposed deployments of Anti-ballistic missile (ABM) systems—it is far less expensive to add more warheads to an existing missile system than to build an ABM system capable of shooting down the additional warheads; hence, most ABM system proposals have been judged to be impractical. The first operational ABM systems were deployed in the U.S. during the 1970s. Safeguard ABM facility was located in North Dakota and was operational from 1975 to 1976. The USSR deployed its ABM-1 Galosh system around Moscow in the 1970s, which remains in service. Israel deployed a national ABM system based on the Arrow missile in 1998, but it is mainly designed to intercept shorter-ranged theater ballistic missiles, not ICBMs. The Alaska-based United States national missile defense system attained initial operational capability in 2004.\nICBMs can be deployed from multiple platforms:\nThe last three kinds are mobile and therefore hard to find.\nDuring storage, one of the most important features of the missile is its serviceability. One of the key features of the first computer-controlled ICBM, the Minuteman missile, was that it could quickly and easily use its computer to test itself.\nAfter launch, a booster pushes the missile and then falls away. Most modern boosters are solid-fueled rocket motors, which can be stored easily for long periods of time. Early missiles used liquid-fueled rocket motors. Many liquid-fueled ICBMs could not be kept fueled all the time as the cryogenic fuel liquid oxygen boiled off and caused ice formation, and therefore fueling the rocket was necessary before launch. This procedure was a source of significant operational delay, and might allow the missiles to be destroyed by enemy counterparts before they could be used. To resolve this problem the United Kingdom invented the missile silo that protected the missile from a first strike and also hid fuelling operations underground.\n\nOnce the booster falls away, the remaining \"bus\" releases several warheads, each of which continues on its own unpowered ballistic trajectory, much like an artillery shell or cannonball. The warhead is encased in a cone-shaped reentry vehicle and is difficult to detect in this phase of flight as there is no rocket exhaust or other emissions to mark its position to defenders. The high speeds of the warheads make them difficult to intercept and allow for little warning, striking targets many thousands of kilometers away from the launch site (and due to the possible locations of the submarines: anywhere in the world) within approximately 30 minutes.\n\nMany authorities say that missiles also release aluminized balloons, electronic noise-makers, and other items intended to confuse interception devices and radars.\n\nAs the nuclear warhead reenters the Earth's atmosphere its high speed causes compression of the air, leading to a dramatic rise in temperature which would destroy it if it were not shielded in some way. As a result, warhead components are contained within an aluminium honeycomb substructure, sheathed in a pyrolytic carbon-epoxy synthetic resin composite material heat shield. Warheads are also often radiation-hardened (to protect against nuclear-tipped ABMs or the nearby detonation of friendly warheads), one neutron-resistant material developed for this purpose in the UK is three-dimensional quartz phenolic.\n\nCircular error probable is crucial, because halving the circular error probable decreases the needed warhead energy by a factor of four. Accuracy is limited by the accuracy of the navigation system and the available geodetic information.\n\nStrategic missile systems are thought to use custom integrated circuits designed to calculate navigational differential equations thousands to millions of FLOPS in order to reduce navigational errors caused by calculation alone. These circuits are usually a network of binary addition circuits that continually recalculate the missile's position. The inputs to the navigation circuit are set by a general purpose computer according to a navigational input schedule loaded into the missile before launch.\n\nOne particular weapon developed by the Soviet Union—the Fractional Orbital Bombardment System—had a partial orbital trajectory, and unlike most ICBMs its target could not be deduced from its orbital flight path. It was decommissioned in compliance with arms control agreements, which address the maximum range of ICBMs and prohibit orbital or fractional-orbital weapons. However, according to reports, Russia is working on the new Sarmat ICBM which leverages Fractional Orbital Bombardment concepts to use a Southern polar approach instead of flying over the Northern polar regions. Using this approach, it is theorized, avoids the US missile defense batteries in California and Alaska.\n\nNew development of ICBM technology are ICBMs able to carry hypersonic glide vehicles as a payload such as RS-28 Sarmat.\n\nSpecific types of ICBMs (current, past and under development) include:\n\nRussia, the United States, China, North Korea and India are the only countries currently known to possess land-based ICBMs, Israel has also tested ICBMs but is not open about actual deployment.\nThe United States currently operates 405 ICBMs in three USAF bases. The only model deployed is LGM-30G Minuteman-III. All previous USAF Minuteman II missiles were destroyed in accordance with START II, and their launch silos have been sealed or sold to the public. The powerful MIRV-capable Peacekeeper missiles were phased out in 2005.\nThe Russian Strategic Rocket Forces have 286 ICBMs able to deliver 958 nuclear warheads: 46 silo-based R-36M2 (SS-18), 30 silo-based UR-100N (SS-19), 36 mobile RT-2PM \"Topol\" (SS-25), 60 silo-based RT-2UTTH \"Topol M\" (SS-27), 18 mobile RT-2UTTH \"Topol M\" (SS-27), 84 mobile RS-24 \"Yars\" (SS-29), and 12 silo-based RS-24 \"Yars\" (SS-29).\n\nChina has developed several long range ICBMs, like the DF-31. The Dongfeng 5 or DF-5 is a 3-stage liquid fuel ICBM and has an estimated range of 13,000 kilometers. The DF-5 had its first flight in 1971 and was in operational service 10 years later. One of the downsides of the missile was that it took between 30 and 60 minutes to fuel. The Dong Feng 31 (a.k.a. CSS-10) is a medium-range, three-stage, solid-propellant intercontinental ballistic missile, and is a land-based variant of the submarine-launched JL-2.\n\nThe DF-41 or CSS-X-10 can carry up to 10 nuclear warheads, which are MIRVs and has a range of approximately . The DF-41 deployed in underground Xinjiang, Qinghai, Gansu and Inner Mongolia area. The mysterious underground subway ICBM carrier systems they called \"Underground Great Wall Project\".\n\nIsrael is believed to have deployed a road mobile nuclear ICBM, the Jericho III, which entered service in 2008. It is possible for the missile to be equipped with a single nuclear warhead or up to three MIRV warheads. It is believed to be based on the Shavit space launch vehicle and is estimated to have a range of . In November 2011 Israel tested an ICBM believed to be an upgraded version of the Jericho III.\n\nIndia has a series of ballistic missiles called Agni. On 19 April 2012, India successfully test fired its first Agni-V, a three-stage solid fueled missile, with a strike range of more than . The missile was test-fired for the second time on 15 September 2013. On 31 January 2015, India conducted a third successful test flight of the Agni-V from the Wheeler Island facility. The test used a canisterised version of the missile, mounted over a Tatra truck.\n\nAn anti-ballistic missile is a missile which can be deployed to counter an incoming nuclear or non-nuclear ICBM. ICBMs can be intercepted in three regions of their trajectory: boost phase, mid-course phase or terminal phase. Currently China, the US, Russia, France, India and Israel have developed anti-ballistic missile systems, of which the Russian A-135 anti-ballistic missile system, US Ground-Based Midcourse Defense, Systems have the capability to intercept ICBMs carrying nuclear, chemical, biological, or conventional warheads.\n\n\n"}
{"id": "14943", "url": "https://en.wikipedia.org/wiki?curid=14943", "title": "Irish traditional music session", "text": "Irish traditional music session\n\nIrish traditional music sessions are mostly informal gatherings at which people play Irish traditional music. The Irish language word for \"session\" is \"seisiún\". This article discusses tune-playing, although \"session\" can also refer to a singing session or a mixed session (tunes and songs).\n\nBarry Foy's \"Field Guide to the Irish Music Session\" defines a session as:\n\n\"...a gathering of Irish traditional musicians for the purpose of celebrating their common interest in the music by playing it together in a relaxed, informal setting, while in the process generally beefing up the mystical cultural mantra that hums along uninterruptedly beneath all manifestations of Irishness worldwide.\"\n\nThe general session scheme is that someone starts a tune, and those who know it join in. Good session etiquette requires not playing if one does not know the tune (or at least quietly playing an accompaniment part) and waiting until a tune one knows comes along. In an \"open\" session, anyone who is able to play Irish music is welcome. Most often there are more-or-less recognized session leaders; sometimes there are no leaders. At times a song will be sung or a slow air played by a single musician between sets.\n\nThe objective in a session is not to provide music for an audience of passive listeners; although the \"punters\" (non-playing attendees) often come for the express purpose of listening, the music is most of all for the musicians themselves. The session is an experience that is shared, not a performance that is bought and sold.\n\nThe sessions are a key aspect of traditional music; it is the main sphere in which the music is formulated and innovated. Further, the sessions enable less advanced musicians to practice in a group. Socially, sessions have often been compared to an evening of playing card games, where the conversation and camaraderie are an essential component. In many rural communities in Ireland, sessions are an integral part of community life.\n\nTypically, the first tune is followed by another two or three tunes in a \"set\". The art of putting together a set is hard to put into words, but the tunes must flow from one to another in terms of key and melodic structure, without being so similar as to all sound the same. The tunes of a set will usually all be of the same sort, i.e. all jigs or all reels, although on rare occasions and amongst a more skilled group of players a complementary tune of a different sort will be included, such as a slip jig amongst the jigs. Although stage performers sometimes arrange sets of reels and jigs together, this is uncommon in an Irish session context. \n\nSome sets are specific to a locale, or even to a single session, whilst others, like the \"Coleman set\" of reels (\"The Tarbolton\"/\"The Longford Collector\"/The Sailor's Bonnet\"), represent longstanding combinations that have been played together for decades. Sets are sometimes thrown together \"ad hoc\", which sometimes works brilliantly and sometimes fails on the spot.\n\nThe term 'set' is used for groups of three or more tunes, and for the 'set' dances which those tunes were originally arranged and played for. As the music was originally played for dancers, typically at 'house dances', a large number of standard sets (of tunes) were developed to suit the various dance sets. Over a hundred standard sets have been recorded by Comhaltas Ceolteoiri na hEirenann (generally known as 'Comhaltas') at sessions in Dublin and Clare and released on three CDs in the 'Foinn Seisiun' series. Comhaltas has also published the musical notation for the tunes in three volumes.\n\nAfter one set ends, someone will usually start another. A 'regular' at the session may launch into a set, knowing that many or all of the musicians know the tunes; a visitor starting a set will check that at least some of the other musicians know the tunes. While there may be occasional singing at a session, solo playing is not part of the session, nor is the session an opportunity to 'show off', or demonstrate 'party pieces'. The origin and spirit of the session, playing standard sets for dancers, ensures that individual musical virtuosity is less valued than the collective effort. It also ensures that visitors from other sessions will know many of the tunes and be able to participate, in true 'bothántiocht' style (see Peig below).\n\nSessions are usually held in public houses or taverns. A pub owner might have one or two musicians paid to come regularly in order for the session to have a base. These musicians can perform during any gaps during the day or evening when no other performers are there and wish to play. Sunday afternoons and weekday nights (especially Tuesday and Wednesday) are common times for sessions to be scheduled, on the theory that these are the least likely times for dances and concerts to be held, and therefore the times that professional musicians will be most able to show up.\n\nSessions can be held in homes or at various public places in addition to pubs; often at a festival sessions will be got together in the beer tent or in the vendor's booth of a music-loving craftsperson or dealer. When a particularly large musical event \"takes over\" an entire village, spontaneous sessions may erupt on the street corners. Sessions may also take place occasionally at wakes. House sessions are not as common now as they were in the past. This can be seen in the book \"Peig\" by Peig Sayers. In the early stages of the book when Peig was young they often went to sessions at peoples houses in a practice called 'bothántiocht'.\n\n"}
{"id": "14946", "url": "https://en.wikipedia.org/wiki?curid=14946", "title": "Ice", "text": "Ice\n\nIce is water frozen into a solid state. Depending on the presence of impurities such as particles of soil or bubbles of air, it can appear transparent or a more or less opaque bluish-white color.\n\nIn the Solar System, ice is abundant and occurs naturally from as close to the Sun as Mercury to as far away as the Oort cloud objects. Beyond the Solar System, it occurs as interstellar ice. It is abundant on Earth's surfaceparticularly in the polar regions and above the snow lineand, as a common form of precipitation and deposition, plays a key role in Earth's water cycle and climate. It falls as snowflakes and hail or occurs as frost, icicles or ice spikes.\n\nIce molecules can exhibit eighteen or more different phases (packing geometries) that depend on temperature and pressure. When water is cooled rapidly (quenching), up to three different types of amorphous ice can form depending on the history of its pressure and temperature. When cooled slowly correlated proton tunneling occurs below (, ) giving rise to macroscopic quantum phenomena. Virtually all the ice on Earth's surface and in its atmosphere is of a hexagonal crystalline structure denoted as ice I (spoken as \"ice one h\") with minute traces of cubic ice denoted as ice I. The most common phase transition to ice I occurs when liquid water is cooled below (, ) at standard atmospheric pressure. It may also be deposited directly by water vapor, as happens in the formation of frost. The transition from ice to water is melting and from ice directly to water vapor is sublimation.\n\nIce is used in a variety of ways, including cooling, winter sports and ice sculpture.\n\nAs a naturally occurring crystalline inorganic solid with an ordered structure, ice is considered to be a mineral. It possesses a regular crystalline structure based on the molecule of water, which consists of a single oxygen atom covalently bonded to two hydrogen atoms, or H–O–H. However, many of the physical properties of water and ice are controlled by the formation of hydrogen bonds between adjacent oxygen and hydrogen atoms; while it is a weak bond, it is nonetheless critical in controlling the structure of both water and ice.\n\nAn unusual property of ice frozen at atmospheric pressure is that the solid is approximately 8.3% less dense than liquid water (which is equivalent to volumetric expansion of 9%). The density of ice is 0.9167–0.9168 g/cm at 0 °C and standard atmospheric pressure (101,325 Pa), whereas water has a density of 0.9998–0.999863 g/cm at the same temperature and pressure. Liquid water is densest, essentially 1.00 g/cm, at 4 °C and becomes less dense as the water molecules begin to form the hexagonal crystals of ice as the freezing point is reached. This is due to hydrogen bonding dominating the intermolecular forces, which results in a packing of molecules less compact in the solid. Density of ice increases slightly with decreasing temperature and has a value of 0.9340 g/cm at −180 °C (93 K).\n\nWhen water freezes, it increases in volume (about 9% for fresh water). The effect of expansion during freezing can be dramatic, and ice expansion is a basic cause of freeze-thaw weathering of rock in nature and damage to building foundations and roadways from frost heaving. It is also a common cause of the flooding of houses when water pipes burst due to the pressure of expanding water when it freezes.\n\nThe result of this process is that ice (in its most common form) floats on liquid water, which is an important feature in Earth's biosphere. It has been argued that without this property, natural bodies of water would freeze, in some cases permanently, from the bottom up, resulting in a loss of bottom-dependent animal and plant life in fresh and sea water. Sufficiently thin ice sheets allow light to pass through while protecting the underside from short-term weather extremes such as wind chill. This creates a sheltered environment for bacterial and algal colonies. When sea water freezes, the ice is riddled with brine-filled channels which sustain sympagic organisms such as bacteria, algae, copepods and annelids, which in turn provide food for animals such as krill and specialised fish like the bald notothen, fed upon in turn by larger animals such as emperor penguins and minke whales.\n\nWhen ice melts, it absorbs as much energy as it would take to heat an equivalent mass of water by 80 °C. During the melting process, the temperature remains constant at 0 °C. While melting, any energy added breaks the hydrogen bonds between ice (water) molecules. Energy becomes available to increase the thermal energy (temperature) only after enough hydrogen bonds are broken that the ice can be considered liquid water. The amount of energy consumed in breaking hydrogen bonds in the transition from ice to water is known as the \"heat of fusion\".\n\nAs with water, ice absorbs light at the red end of the spectrum preferentially as the result of an overtone of an oxygen–hydrogen (O–H) bond stretch. Compared with water, this absorption is shifted toward slightly lower energies. Thus, ice appears blue, with a slightly greener tint than liquid water. Since absorption is cumulative, the color effect intensifies with increasing thickness or if internal reflections cause the light to take a longer path through the ice.\n\nOther colors can appear in the presence of light absorbing impurities, where the impurity is dictating the color rather than the ice itself. For instance, icebergs containing impurities (e.g., sediments, algae, air bubbles) can appear brown, grey or green.\n\nIce may be any one of the 18 known solid crystalline phases of water, or in an amorphous solid state at various densities.\n\nMost liquids under increased pressure freeze at \"higher\" temperatures because the pressure helps to hold the molecules together. However, the strong hydrogen bonds in water make it different: For some pressures higher than , water freezes at a temperature \"below\" 0 °C, as shown in the phase diagram below. The melting of ice under high pressures is thought to contribute to the movement of glaciers.\n\nIce, water, and water vapour can coexist at the triple point, which is exactly 273.16 K (0.01 °C) at a pressure of 611.657 Pa. The kelvin is in fact defined as of the difference between this triple point and absolute zero, though this definition is due to change in May 2019. Unlike most other solids, ice is difficult to superheat. In an experiment, ice at −3 °C was superheated to about 17 °C for about 250 picoseconds.\n\nSubjected to higher pressures and varying temperatures, ice can form in 18 separate known crystalline phases. With care, at least 15 of these phases (one of the known exceptions being ice X) can be recovered at ambient pressure and low temperature in metastable form. The types are differentiated by their crystalline structure, proton ordering, and density. There are also two metastable phases of ice under pressure, both fully hydrogen-disordered; these are IV and XII. Ice XII was discovered in 1996. In 2006, XIII and XIV were discovered. Ices XI, XIII, and XIV are hydrogen-ordered forms of ices I, V, and XII respectively. In 2009, ice XV was found at extremely high pressures and −143 °C. At even higher pressures, ice is predicted to become a metal; this has been variously estimated to occur at 1.55 TPa or 5.62 TPa.\n\nAs well as crystalline forms, solid water can exist in amorphous states as amorphous ice (ASW) of varying densities. Water in the interstellar medium is dominated by amorphous ice, making it likely the most common form of water in the universe. Low-density ASW (LDA), also known as hyperquenched glassy water, may be responsible for noctilucent clouds on Earth and is usually formed by deposition of water vapor in cold or vacuum conditions. High-density ASW (HDA) is formed by compression of ordinary ice I or LDA at GPa pressures. Very-high-density ASW (VHDA) is HDA slightly warmed to 160K under 1–2 GPa pressures.\n\nIn outer space, hexagonal crystalline ice (the predominant form found on Earth) is extremely rare. Amorphous ice is more common; however, hexagonal crystalline ice can be formed by volcanic action.\n\nThe low coefficient of friction (\"slipperiness\") of ice has been attributed to the pressure of an object coming into contact with the ice, melting a thin layer of the ice and allowing the object to glide across the surface. For example, the blade of an ice skate, upon exerting pressure on the ice, would melt a thin layer, providing lubrication between the ice and the blade. This explanation, called \"pressure melting\", originated in the 19th century. It, however, did not account for skating on ice temperatures lower than , which is often skated upon.\n\nA second theory describing the coefficient of friction of ice suggested that ice molecules at the interface cannot properly bond with the molecules of the mass of ice beneath (and thus are free to move like molecules of liquid water). These molecules remain in a semi-liquid state, providing lubrication regardless of pressure against the ice exerted by any object. However, the significance of this hypothesis is disputed by experiments showing a high coefficient of friction for ice using atomic force microscopy.\n\nA third theory is \"friction heating\", which suggests that friction of the material is the cause of the ice layer melting. However, this theory does not sufficiently explain why ice is slippery when standing still even at below-zero temperatures.\n\nA comprehensive theory of ice friction takes into account all the above-mentioned friction mechanisms. This model allows quantitative estimation of the friction coefficient of ice against various materials as a function of temperature and sliding speed. In typical conditions related to winter sports and tires of a vehicle on ice, melting of a thin ice layer due to the frictional heating is the primary reason for the slipperiness.\n\nThe term that collectively describes all of the parts of the Earth's surface where water is in frozen form is the \"cryosphere.\" Ice is an important component of the global climate, particularly in regard to the water cycle. Glaciers and snowpacks are an important storage mechanism for fresh water; over time, they may sublimate or melt. Snowmelt is an important source of seasonal fresh water. The World Meteorological Organization defines several kinds of ice depending on origin, size, shape, influence and so on. Clathrate hydrates are forms of ice that contain gas molecules trapped within its crystal lattice.\n\nIce that is found at sea may be in the form of drift ice floating in the water, fast ice fixed to a shoreline or anchor ice if attached to the sea bottom. Ice which calves (breaks off) from an ice shelf or glacier may become an iceberg. Sea ice can be forced together by currents and winds to form pressure ridges up to tall. Navigation through areas of sea ice occurs in openings called \"polynyas\" or \"leads\" or requires the use of a special ship called an \"icebreaker\".\n\nIce on land ranges from the largest type called an \"ice sheet\" to smaller ice caps and ice fields to glaciers and ice streams to the snow line and snow fields.\n\nAufeis is layered ice that forms in Arctic and subarctic stream valleys. Ice, frozen in the stream bed, blocks normal groundwater discharge, and causes the local water table to rise, resulting in water discharge on top of the frozen layer. This water then freezes, causing the water table to rise further and repeat the cycle. The result is a stratified ice deposit, often several meters thick.\n\nFreezing rain is a type of winter storm called an ice storm where rain falls and then freezes producing a glaze of ice. Ice can also form icicles, similar to stalactites in appearance, or stalagmite-like forms as water drips and re-freezes.\n\nThe term \"ice dam\" has three meanings (others discussed below). On structures, an ice dam is the buildup of ice on a sloped roof which stops melt water from draining properly and can cause damage from water leaks in buildings.\n\nIce which forms on moving water tends to be less uniform and stable than ice which forms on calm water. Ice jams (sometimes called \"ice dams\"), when broken chunks of ice pile up, are the greatest ice hazard on rivers. Ice jams can cause flooding, damage structures in or near the river, and damage vessels on the river. Ice jams can cause some hydropower industrial facilities to completely shut down. An ice dam is a blockage from the movement of a glacier which may produce a proglacial lake. Heavy ice flows in rivers can also damage vessels and require the use of an icebreaker to keep navigation possible.\n\nIce discs are circular formations of ice surrounded by water in a river.\n\nPancake ice is a formation of ice generally created in areas with less calm conditions.\n\nIce forms on calm water from the shores, a thin layer spreading across the surface, and then downward. Ice on lakes is generally four types: Primary, secondary, superimposed and agglomerate. Primary ice forms first. Secondary ice forms below the primary ice in a direction parallel to the direction of the heat flow. Superimposed ice forms on top of the ice surface from rain or water which seeps up through cracks in the ice which often settles when loaded with snow.\n\nShelf ice occurs when floating pieces of ice are driven by the wind piling up on the windward shore.\n\nCandle ice is a form of rotten ice that develops in columns perpendicular to the surface of a lake.\n\nRime is a type of ice formed on cold objects when drops of water crystallize on them. This can be observed in foggy weather, when the temperature drops during the night. Soft rime contains a high proportion of trapped air, making it appear white rather than transparent, and giving it a density about one quarter of that of pure ice. Hard rime is comparatively dense.\n\nIce pellets are a form of precipitation consisting of small, translucent balls of ice. This form of precipitation is also referred to as \"sleet\" by the United States National Weather Service. (In British English \"sleet\" refers to a mixture of rain and snow). Ice pellets are usually smaller than hailstones. They often bounce when they hit the ground, and generally do not freeze into a solid mass unless mixed with freezing rain. The METAR code for ice pellets is \"PL\".\n\nIce pellets form when a layer of above-freezing air is located between above the ground, with sub-freezing air both above and below it. This causes the partial or complete melting of any snowflakes falling through the warm layer. As they fall back into the sub-freezing layer closer to the surface, they re-freeze into ice pellets. However, if the sub-freezing layer beneath the warm layer is too small, the precipitation will not have time to re-freeze, and freezing rain will be the result at the surface. A temperature profile showing a warm layer above the ground is most likely to be found in advance of a warm front during the cold season, but can occasionally be found behind a passing cold front.\n\nLike other precipitation, hail forms in storm clouds when supercooled water droplets freeze on contact with condensation nuclei, such as dust or dirt. The storm's updraft blows the hailstones to the upper part of the cloud. The updraft dissipates and the hailstones fall down, back into the updraft, and are lifted up again. Hail has a diameter of or more. Within METAR code, GR is used to indicate larger hail, of a diameter of at least and GS for smaller. Stones just larger than golf ball-sized are one of the most frequently reported hail sizes. Hailstones can grow to and weigh more than . In large hailstones, latent heat released by further freezing may melt the outer shell of the hailstone. The hailstone then may undergo 'wet growth', where the liquid outer shell collects other smaller hailstones. The hailstone gains an ice layer and grows increasingly larger with each ascent. Once a hailstone becomes too heavy to be supported by the storm's updraft, it falls from the cloud.\n\nHail forms in strong thunderstorm clouds, particularly those with intense updrafts, high liquid water content, great vertical extent, large water droplets, and where a good portion of the cloud layer is below freezing . Hail-producing clouds are often identifiable by their green coloration. The growth rate is maximized at about , and becomes vanishingly small much below as supercooled water droplets become rare. For this reason, hail is most common within continental interiors of the mid-latitudes, as hail formation is considerably more likely when the freezing level is below the altitude of . Entrainment of dry air into strong thunderstorms over continents can increase the frequency of hail by promoting evaporational cooling which lowers the freezing level of thunderstorm clouds giving hail a larger volume to grow in. Accordingly, hail is actually less common in the tropics despite a much higher frequency of thunderstorms than in the mid-latitudes because the atmosphere over the tropics tends to be warmer over a much greater depth. Hail in the tropics occurs mainly at higher elevations.\n\nSnow crystals form when tiny supercooled cloud droplets (about 10 μm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than , because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice; then the droplet freezes around this \"nucleus.\" Experiments show that this \"homogeneous\" nucleation of cloud droplets only occurs at temperatures lower than . In warmer clouds an aerosol particle or \"ice nucleus\" must be present in (or in contact with) the droplet to act as a nucleus. Our understanding of what particles make efficient ice nuclei is poor – what we do know is they are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles may be effective, although to what extent is unclear. Artificial nuclei are used in cloud seeding. The droplet then grows by condensation of water vapor onto the ice surfaces.\n\nSo-called \"diamond dust\", also known as ice needles or ice crystals, forms at temperatures approaching due to air with slightly higher moisture from aloft mixing with colder, surface-based air. The METAR identifier for diamond dust within international hourly weather reports is \"IC\".\n\nAblation of ice refers to both its melting and its dissolution.\n\nIn fresh ambient melting describes a phase transition from solid to liquid.\n\nTo melt ice means breaking the hydrogen bonds between the water molecules. The ordering of the molecules in the solid breaks down to a less ordered state and the solid melts to become a liquid. This is achieved by increasing the internal energy of the ice beyond the melting point. When ice melts it absorbs as much energy as would be required to heat an equivalent amount of water by 80 °C. While melting, the temperature of the ice surface remains constant at 0 °C. The velocity of the melting process depends on the efficiency of the energy exchange process. An ice surface in fresh water melts solely by free convection with a velocity that depends linearly on the water temperature, \"T\", when \"T\" is less than 3.98 °C, and superlinearly when \"T\" is equal to or greater than 3.98 °C, with the rate being proportional to (T − 3.98 °C), with \"α\" =  for \"T\" much greater than 8 °C, and α =  for in between temperatures \"T\".\n\nIn salty ambient conditions, dissolution rather than melting often causes the ablation of ice. For example, the temperature of the Arctic Ocean is generally below the melting point of ablating sea ice. The phase transition from solid to liquid is achieved by mixing salt and water molecules, similar to the dissolution of sugar in water, even though the water temperature is far below the melting point of the sugar. Hence dissolution is rate limited by salt transport whereas melting can occur at much higher rates that are characteristic for heat transport.\n\nHumans have used ice for cooling and food preservation for centuries, relying on harvesting natural ice in various forms and then transitioning to the mechanical production of the material. Ice also presents a challenge to transportation in various forms and a setting for winter sports.\n\nIce has long been valued as a means of cooling. In 400 BC Iran, Persian engineers had already mastered the technique of storing ice in the middle of summer in the desert. The ice was brought in during the winters from nearby mountains in bulk amounts, and stored in specially designed, naturally cooled \"refrigerators\", called yakhchal (meaning \"ice storage\"). This was a large underground space (up to 5000 m) that had thick walls (at least two meters at the base) made of a special mortar called \"sarooj\", composed of sand, clay, egg whites, lime, goat hair, and ash in specific proportions, and which was known to be resistant to heat transfer. This mixture was thought to be completely water impenetrable. The space often had access to a qanat, and often contained a system of windcatchers which could easily bring temperatures inside the space down to frigid levels on summer days. The ice was used to chill treats for royalty.\n\nThere were thriving industries in 16th/17th century England whereby low-lying areas along the Thames Estuary were flooded during the winter, and ice harvested in carts and stored inter-seasonally in insulated wooden houses as a provision to an icehouse often located in large country houses, and widely used to keep fish fresh when caught in distant waters. This was allegedly copied by an Englishman who had seen the same activity in China. Ice was imported into England from Norway on a considerable scale as early as 1823.\n\nIn the United States, the first cargo of ice was sent from New York City to Charleston, South Carolina in 1799, and by the first half of the 19th century, ice harvesting had become big business. Frederic Tudor, who became known as the \"Ice King\", worked on developing better insulation products for the long distance shipment of ice, especially to the tropics; this became known as the ice trade.\n\nTrieste sent ice to Egypt, Corfu, and Zante; Switzerland sent it to France; and Germany sometimes was supplied from Bavarian lakes. The Hungarian Parliament building used ice harvested in the winter from Lake Balaton for air conditioning.\n\nIce houses were used to store ice formed in the winter, to make ice available all year long, and early refrigerators were known as iceboxes, because they had a block of ice in them. In many cities, it was not unusual to have a regular ice delivery service during the summer. The advent of artificial refrigeration technology has since made delivery of ice obsolete.\n\nIce is still harvested for ice and snow sculpture events. For example, a swing saw is used to get ice for the Harbin International Ice and Snow Sculpture Festival each year from the frozen surface of the Songhua River.\n\nIce is now produced on an industrial scale, for uses including food storage and processing, chemical manufacturing, concrete mixing and curing, and consumer or packaged ice. Most commercial icemakers produce three basic types of fragmentary ice: flake, tubular and plate, using a variety of techniques. Large batch ice makers can produce up to 75 tons of ice per day. In 2002, there were 426 commercial ice-making companies in the United States, with a combined value of shipments of $595,487,000. Home refrigerators can also make ice with a built in icemaker, which will typically make ice cubes or crushed ice. Stand-alone icemaker units that make ice cubes are often called ice machines.\n\nIce can present challenges to safe transportation on land, sea and in the air.\n\nIce forming on roads is a dangerous winter hazard. Black ice is very difficult to see, because it lacks the expected frosty surface. Whenever there is freezing rain or snow which occurs at a temperature near the melting point, it is common for ice to build up on the windows of vehicles. Driving safely requires the removal of the ice build-up. Ice scrapers are tools designed to break the ice free and clear the windows, though removing the ice can be a long and laborious process.\n\nFar enough below the freezing point, a thin layer of ice crystals can form on the inside surface of windows. This usually happens when a vehicle has been left alone after being driven for a while, but can happen while driving, if the outside temperature is low enough. Moisture from the driver's breath is the source of water for the crystals. It is troublesome to remove this form of ice, so people often open their windows slightly when the vehicle is parked in order to let the moisture dissipate, and it is now common for cars to have rear-window defrosters to solve the problem. A similar problem can happen in homes, which is one reason why many colder regions require double-pane windows for insulation.\n\nWhen the outdoor temperature stays below freezing for extended periods, very thick layers of ice can form on lakes and other bodies of water, although places with flowing water require much colder temperatures. The ice can become thick enough to drive onto with automobiles and trucks. Doing this safely requires a thickness of at least 30 cm (one foot).\n\nFor ships, ice presents two distinct hazards. Spray and freezing rain can produce an ice build-up on the superstructure of a vessel sufficient to make it unstable, and to require it to be hacked off or melted with steam hoses. And icebergs – large masses of ice floating in water (typically created when glaciers reach the sea) – can be dangerous if struck by a ship when underway. Icebergs have been responsible for the sinking of many ships, the most famous being the \"Titanic\". For harbors near the poles, being ice-free is an important advantage. Ideally, all year long. Examples are Murmansk (Russia), Petsamo (Russia, formerly Finland) and Vardø (Norway). Harbors which are not ice-free are opened up using icebreakers.\n\nFor aircraft, ice can cause a number of dangers. As an aircraft climbs, it passes through air layers of different temperature and humidity, some of which may be conducive to ice formation. If ice forms on the wings or control surfaces, this may adversely affect the flying qualities of the aircraft. During the first non-stop flight across the Atlantic, the British aviators Captain John Alcock and Lieutenant Arthur Whitten Brown encountered such icing conditions – Brown left the cockpit and climbed onto the wing several times to remove ice which was covering the engine air intakes of the Vickers Vimy aircraft they were flying.\n\nOne vulnerability effected by icing that is associated with reciprocating internal combustion engines is the carburetor. As air is sucked through the carburetor into the engine, the local air pressure is lowered, which causes adiabatic cooling. Thus, in humid near-freezing conditions, the carburetor will be colder, and tend to ice up. This will block the supply of air to the engine, and cause it to fail. For this reason, aircraft reciprocating engines with carburetors are provided with carburetor air intake heaters. The increasing use of fuel injection—which does not require carburetors—has made \"carb icing\" less of an issue for reciprocating engines.\n\nJet engines do not experience carb icing, but recent evidence indicates that they can be slowed, stopped, or damaged by internal icing in certain types of atmospheric conditions much more easily than previously believed. In most cases, the engines can be quickly restarted and flights are not endangered, but research continues to determine the exact conditions which produce this type of icing, and find the best methods to prevent, or reverse it, in flight.\n\nIce also plays a central role in winter recreation and in many sports such as ice skating, tour skating, ice hockey, bandy, ice fishing, ice climbing, curling, broomball and sled racing on bobsled, luge and skeleton. Many of the different sports played on ice get international attention every four years during the Winter Olympic Games.\n\nA sort of sailboat on blades gives rise to ice yachting. Another sport is ice racing, where drivers must speed on lake ice, while also controlling the skid of their vehicle (similar in some ways to dirt track racing). The sport has even been modified for ice rinks.\n\n\n\nThe solid phases of several other volatile substances are also referred to as \"ices\"; generally a volatile is classed as an ice if its melting point lies above or around 100 K. The best known example is dry ice, the solid form of carbon dioxide.\n\nA \"magnetic analogue\" of ice is also realized in some insulating magnetic materials in which the magnetic moments mimic the position of protons in water ice and obey energetic constraints similar to the Bernal-Fowler ice rules arising from the geometrical frustration of the proton configuration in water ice. These materials are called spin ice.\n\n"}
{"id": "14951", "url": "https://en.wikipedia.org/wiki?curid=14951", "title": "Ionic bonding", "text": "Ionic bonding\n\nIonic bonding is a type of chemical bonding that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. It is one of the main bonds along with Covalent bond and Metallic bonding. Ions are atoms that have gained one or more electrons (known as anions, which are negatively charged) and atoms that have lost one or more electrons (known as cations, which are positively charged). This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complex nature, e.g. molecular ions like or . In simpler words, an ionic bond is the transfer of electrons from a metal to a non-metal in order to obtain a full valence shell for both atoms.\n\nIt is important to recognize that \"clean\" ionic bonding – in which one atom or molecule completely transfers an electron to another cannot exist: all ionic compounds have some degree of covalent bonding, or electron sharing. Thus, the term \"ionic bonding\" is given when the ionic character is greater than the covalent character – that is, a bond in which a large electronegativity difference exists between the two atoms, causing the bonding to be more polar (ionic) than in covalent bonding where electrons are shared more equally. Bonds with partially ionic and partially covalent character are called polar covalent bonds. \n\nIonic compounds conduct electricity when molten or in solution, typically as a solid. Ionic compounds generally have a high melting point, depending on the charge of the ions they consist of. The higher the charges the stronger the cohesive forces and the higher the melting point. They also tend to be soluble in water; the stronger the cohesive forces, the lower the solubility. \n\nAtoms that have an almost full or almost empty valence shell tend to be very reactive. Atoms that are strongly electronegative (as is the case with halogens) often have only one or two empty orbitals in their valence shell, and frequently bond with other molecules or gain electrons to form anions. Atoms that are weakly electronegative (such as alkali metals) have relatively few valence electrons, which can easily be shared with atoms that are strongly electronegative. As a result, weakly electronegative atoms tend to distort their electron cloud and form cations.\n\nIonic bonding can result from a redox reaction when atoms of an element (usually metal), whose ionization energy is low, give some of their electrons to achieve a stable electron configuration. In doing so, cations are formed. An atom of another element (usually nonmetal) with greater electron affinity accepts the electron(s) to attain a stable electron configuration, and after accepting electron(s) an atom becomes an anion. Typically, the stable electron configuration is one of the noble gases for elements in the s-block and the p-block, and particular stable electron configurations for d-block and f-block elements. The electrostatic attraction between the anions and cations leads to the formation of a solid with a crystallographic lattice in which the ions are stacked in an alternating fashion. In such a lattice, it is usually not possible to distinguish discrete molecular units, so that the compounds formed are not molecular in nature. However, the ions themselves can be complex and form molecular ions like the acetate anion or the ammonium cation.\n\nFor example, common table salt is sodium chloride. When sodium (Na) and chlorine (Cl) are combined, the sodium atoms each lose an electron, forming cations (Na), and the chlorine atoms each gain an electron to form anions (Cl). These ions are then attracted to each other in a 1:1 ratio to form sodium chloride (NaCl).\n\nHowever, to maintain charge neutrality, strict ratios between anions and cations are observed so that ionic compounds, in general, obey the rules of stoichiometry despite not being molecular compounds. For compounds that are transitional to the alloys and possess mixed ionic and metallic bonding, this may not be the case anymore. Many sulfides, e.g., do form non-stoichiometric compounds.\n\nMany ionic compounds are referred to as salts as they can also be formed by the neutralization reaction of an Arrhenius base like NaOH with an Arrhenius acid like HCl\n\nThe salt NaCl is then said to consist of the acid rest Cl and the base rest Na.\n\nThe removal of electrons from the cation is endothermic, raising the system's overall energy. There may also be energy changes associated with breaking of existing bonds or the addition of more than one electron to form anions. However, the action of the anion's accepting the cation's valence electrons and the subsequent attraction of the ions to each other releases (lattice) energy and, thus, lowers the overall energy of the system.\n\nIonic bonding will occur only if the overall energy change for the reaction is favorable. In general, the reaction is exothermic, but, e.g., the formation of mercuric oxide (HgO) is endothermic. The charge of the resulting ions is a major factor in the strength of ionic bonding, e.g. a salt CA is held together by electrostatic forces roughly four times weaker than CA according to Coulombs law, where C and A represent a generic cation and anion respectively. The sizes of the ions and the particular packing of the lattice are ignored in this rather simplistic argument.\n\nIonic compounds in the solid state form lattice structures. The two principal factors in determining the form of the lattice are the relative charges of the ions and their relative sizes. Some structures are adopted by a number of compounds; for example, the structure of the rock salt sodium chloride is also adopted by many alkali halides, and binary oxides such as magnesium oxide. Pauling's rules provide guidelines for predicting and rationalizing the crystal structures of ionic crystals\n\nFor a solid crystalline ionic compound the enthalpy change in forming the solid from gaseous ions is termed the lattice energy.\nThe experimental value for the lattice energy can be determined using the Born–Haber cycle. It can also be calculated (predicted) using the Born–Landé equation as the sum of the electrostatic potential energy, calculated by summing interactions between cations and anions, and a short-range repulsive potential energy term. The electrostatic potential can be expressed in terms of the interionic separation and a constant (Madelung constant) that takes account of the geometry of the crystal. The further away from the nucleus the weaker the shield. The Born-Landé equation gives a reasonable fit to the lattice energy of, e.g., sodium chloride, where the calculated (predicted) value is −756 kJ/mol, which compares to −787 kJ/mol using the Born–Haber cycle.\n\nIons in crystal lattices of purely ionic compounds are spherical; however, if the positive ion is small and/or highly charged, it will distort the electron cloud of the negative ion, an effect summarised in Fajans' rules. This polarization of the negative ion leads to a build-up of extra charge density between the two nuclei, that is, to partial covalency. Larger negative ions are more easily polarized, but the effect is usually important only when positive ions with charges of 3+ (e.g., Al) are involved. However, 2+ ions (Be) or even 1+ (Li) show some polarizing power because their sizes are so small (e.g., LiI is ionic but has some covalent bonding present). Note that this is not the ionic polarization effect that refers to displacement of ions in the lattice due to the application of an electric field.\n\nIn ionic bonding, the atoms are bound by attraction of oppositely charged ions, whereas, in covalent bonding, atoms are bound by sharing electrons to attain stable electron configurations. In covalent bonding, the molecular geometry around each atom is determined by valence shell electron pair repulsion VSEPR rules, whereas, in ionic materials, the geometry follows maximum packing rules. One could say that covalent bonding is more \"directional\" in the sense that the energy penalty for not adhering to the optimum bond angles is large, whereas ionic bonding has no such penalty. There are no shared electron pairs to repel each other, the ions should simply be packed as efficiently as possible. This often leads to much higher coordination numbers. In NaCl, each ion has 6 bonds and all bond angles are 90°. In CsCl the coordination number is 8. By comparison carbon typically has a maximum of four bonds.\n\nPurely ionic bonding cannot exist, as the proximity of the entities involved in the bonding allows some degree of sharing electron density between them. Therefore, all ionic bonding has some covalent character. Thus, bonding is considered ionic where the ionic character is greater than the covalent character. The larger the difference in electronegativity between the two types of atoms involved in the bonding, the more ionic (polar) it is. Bonds with partially ionic and partially covalent character are called polar covalent bonds. For example, Na–Cl and Mg–O interactions have a few percent covalency, while Si–O bonds are usually ~50% ionic and ~50% covalent. Pauling estimated that an electronegativity difference of 1.7 (on the Pauling scale) corresponds to 50% ionic character, so that a difference greater than 1.7 corresponds to a bond which is predominantly ionic.\nIonic character in covalent bonds can be directly measured for atoms having quadrupolar nuclei (H, N, Br, Cl or I). These nuclei are generally objects of NQR nuclear quadrupole resonance and NMR nuclear magnetic resonance studies. Interactions between the nuclear quadrupole moments \"Q\" and the electric field gradients (EFG) are characterized via the nuclear quadrupole coupling constants\nwhere the \"eq\" term corresponds to the principal component of the EFG tensor and \"e\" is the elementary charge. In turn, the electric field gradient opens the way to description of bonding modes in molecules when the QCC values are accurately determined by NMR or NQR methods. \n\nIn general, when ionic bonding occurs in the solid (or liquid) state, it is not possible to talk about a single \"ionic bond\" between two individual atoms, because the cohesive forces that keep the lattice together are of a more collective nature. This is quite different in the case of covalent bonding, where we can often speak of a distinct bond localized between two particular atoms. However, even if ionic bonding is combined with some covalency, the result is \"not\" necessarily discrete bonds of a localized character. In such cases, the resulting bonding often requires description in terms of a band structure consisting of gigantic molecular orbitals spanning the entire crystal. Thus, the bonding in the solid often retains its collective rather than localized nature. When the difference in electronegativity is decreased, the bonding may then lead to a semiconductor, a semimetal or eventually a metallic conductor with metallic bonding.\n\n\n"}
{"id": "14952", "url": "https://en.wikipedia.org/wiki?curid=14952", "title": "IBF (disambiguation)", "text": "IBF (disambiguation)\n\nIBF may refer to:\n\n\n"}
{"id": "14958", "url": "https://en.wikipedia.org/wiki?curid=14958", "title": "Immune system", "text": "Immune system\n\nThe immune system is a host defense system comprising many biological structures and processes within an organism that protects against disease. To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, and distinguish them from the organism's own healthy tissue. In many species, the immune system can be classified into subsystems, such as the innate immune system versus the adaptive immune system, or humoral immunity versus cell-mediated immunity. In humans, the blood–brain barrier, blood–cerebrospinal fluid barrier, and similar fluid–brain barriers separate the peripheral immune system from the neuroimmune system, which protects the brain.\n\nPathogens can rapidly evolve and adapt, and thereby avoid detection and neutralization by the immune system; however, multiple defense mechanisms have also evolved to recognize and neutralize pathogens. Even simple unicellular organisms such as bacteria possess a rudimentary immune system in the form of enzymes that protect against bacteriophage infections. Other basic immune mechanisms evolved in ancient eukaryotes and remain in their modern descendants, such as plants and invertebrates. These mechanisms include phagocytosis, antimicrobial peptides called defensins, and the complement system. Jawed vertebrates, including humans, have even more sophisticated defense mechanisms, including the ability to adapt over time to recognize specific pathogens more efficiently. Adaptive (or acquired) immunity creates immunological memory after an initial response to a specific pathogen, leading to an enhanced response to subsequent encounters with that same pathogen. This process of acquired immunity is the basis of vaccination.\n\nDisorders of the immune system can result in autoimmune diseases, inflammatory diseases and cancer. Immunodeficiency occurs when the immune system is less active than normal, resulting in recurring and life-threatening infections. In humans, immunodeficiency can either be the result of a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication. In contrast, autoimmunity results from a hyperactive immune system attacking normal tissues as if they were foreign organisms. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus. Immunology covers the study of all aspects of the immune system.\n\nImmunology is a science that examines the structure and function of the immune system. It originates from medicine and early studies on the causes of immunity to disease. The earliest known reference to immunity was during the plague of Athens in 430 BC. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. In the 18th century, Pierre-Louis Moreau de Maupertuis made experiments with scorpion venom and observed that certain dogs and mice were immune to this venom. In the 10th century, Persian physician al-Razi (also known as Rhazes) wrote the first recorded theory of acquired immunity, noting that a smallpox bout protected its survivors from future infections. Although he explained the immunity in terms of \"excess moisture\" getting expelled from the blood—therefore preventing the disease to occur for a second time—this theory explained many observations about smallpox known during this time.\n\nThese and other observations of acquired immunity were later exploited by Louis Pasteur in his development of vaccination and his proposed germ theory of disease. Pasteur's theory was in direct opposition to contemporary theories of disease, such as the miasma theory. It was not until Robert Koch's 1891 proofs, for which he was awarded a Nobel Prize in 1905, that microorganisms were confirmed as the cause of infectious disease. Viruses were confirmed as human pathogens in 1901, with the discovery of the yellow fever virus by Walter Reed.\n\nImmunology made a great advance towards the end of the 19th century, through rapid developments, in the study of humoral immunity and cellular immunity. Particularly important was the work of Paul Ehrlich, who proposed the side-chain theory to explain the specificity of the antigen-antibody reaction; his contributions to the understanding of humoral immunity were recognized by the award of a Nobel Prize in 1908, which was jointly awarded to the founder of cellular immunology, Elie Metchnikoff.\n\nThe immune system protects organisms from infection with layered defenses of increasing specificity. In simple terms, physical barriers prevent pathogens such as bacteria and viruses from entering the organism. If a pathogen breaches these barriers, the innate immune system provides an immediate, but non-specific response. Innate immune systems are found in all plants and animals. If pathogens successfully evade the innate response, vertebrates possess a second layer of protection, the adaptive immune system, which is activated by the innate response. Here, the immune system adapts its response during an infection to improve its recognition of the pathogen. This improved response is then retained after the pathogen has been eliminated, in the form of an immunological memory, and allows the adaptive immune system to mount faster and stronger attacks each time this pathogen is encountered.\n\nBoth innate and adaptive immunity depend on the ability of the immune system to distinguish between self and non-self molecules. In immunology, \"self\" molecules are those components of an organism's body that can be distinguished from foreign substances by the immune system. Conversely, \"non-self\" molecules are those recognized as foreign molecules. One class of non-self molecules are called antigens (short for \"anti\"body \"gen\"erators) and are defined as substances that bind to specific immune receptors and elicit an immune response.\n\nMicroorganisms or toxins that successfully enter an organism encounter the cells and mechanisms of the innate immune system. The innate response is usually triggered when microbes are identified by pattern recognition receptors, which recognize components that are conserved among broad groups of microorganisms, or when damaged, injured or stressed cells send out alarm signals, many of which (but not all) are recognized by the same receptors as those that recognize pathogens. Innate immune defenses are non-specific, meaning these systems respond to pathogens in a generic way. This system does not confer long-lasting immunity against a pathogen. The innate immune system is the dominant system of host defense in most organisms.\n\nSeveral barriers protect organisms from infection, including mechanical, chemical, and biological barriers. The waxy cuticle of most leaves, the exoskeleton of insects, the shells and membranes of externally deposited eggs, and skin are examples of mechanical barriers that are the first line of defense against infection. However, as organisms cannot be completely sealed from their environments, other systems act to protect body openings such as the lungs, intestines, and the genitourinary tract. In the lungs, coughing and sneezing mechanically eject pathogens and other irritants from the respiratory tract. The flushing action of tears and urine also mechanically expels pathogens, while mucus secreted by the respiratory and gastrointestinal tract serves to trap and entangle microorganisms.\n\nChemical barriers also protect against infection. The skin and respiratory tract secrete antimicrobial peptides such as the β-defensins. Enzymes such as lysozyme and phospholipase A2 in saliva, tears, and breast milk are also antibacterials. Vaginal secretions serve as a chemical barrier following menarche, when they become slightly acidic, while semen contains defensins and zinc to kill pathogens. In the stomach, gastric acid serves as a powerful chemical defense against ingested pathogens.\n\nWithin the genitourinary and gastrointestinal tracts, commensal flora serve as biological barriers by competing with pathogenic bacteria for food and space and, in some cases, by changing the conditions in their environment, such as pH or available iron. As a result of the symbiotic relationship between commensals and the immune system, the probability that pathogens will reach sufficient numbers to cause illness is reduced. However, since most antibiotics non-specifically target bacteria and do not affect fungi, oral antibiotics can lead to an \"overgrowth\" of fungi and cause conditions such as a vaginal candidiasis (a yeast infection). There is good evidence that re-introduction of probiotic flora, such as pure cultures of the lactobacilli normally found in unpasteurized yogurt, helps restore a healthy balance of microbial populations in intestinal infections in children and encouraging preliminary data in studies on bacterial gastroenteritis, inflammatory bowel diseases, urinary tract infection and post-surgical infections.\n\nInflammation is one of the first responses of the immune system to infection. The symptoms of inflammation are redness, swelling, heat, and pain, which are caused by increased blood flow into tissue. Inflammation is produced by eicosanoids and cytokines, which are released by injured or infected cells. Eicosanoids include prostaglandins that produce fever and the dilation of blood vessels associated with inflammation, and leukotrienes that attract certain white blood cells (leukocytes). Common cytokines include interleukins that are responsible for communication between white blood cells; chemokines that promote chemotaxis; and interferons that have anti-viral effects, such as shutting down protein synthesis in the host cell. Growth factors and cytotoxic factors may also be released. These cytokines and other chemicals recruit immune cells to the site of infection and promote healing of any damaged tissue following the removal of pathogens.\n\nThe complement system is a biochemical cascade that attacks the surfaces of foreign cells. It contains over 20 different proteins and is named for its ability to \"complement\" the killing of pathogens by antibodies. Complement is the major humoral component of the innate immune response. Many species have complement systems, including non-mammals like plants, fish, and some invertebrates.\n\nIn humans, this response is activated by complement binding to antibodies that have attached to these microbes or the binding of complement proteins to carbohydrates on the surfaces of microbes. This recognition signal triggers a rapid killing response. The speed of the response is a result of signal amplification that occurs after sequential proteolytic activation of complement molecules, which are also proteases. After complement proteins initially bind to the microbe, they activate their protease activity, which in turn activates other complement proteases, and so on. This produces a catalytic cascade that amplifies the initial signal by controlled positive feedback. The cascade results in the production of peptides that attract immune cells, increase vascular permeability, and opsonize (coat) the surface of a pathogen, marking it for destruction. This deposition of complement can also kill cells directly by disrupting their plasma membrane.\n\nLeukocytes (white blood cells) act like independent, single-celled organisms and are the second arm of the innate immune system. The innate leukocytes include the phagocytes (macrophages, neutrophils, and dendritic cells), innate lymphoid cells, mast cells, eosinophils, basophils, and natural killer cells. These cells identify and eliminate pathogens, either by attacking larger pathogens through contact or by engulfing and then killing microorganisms. Innate cells are also important mediators in lymphoid organ development and the activation of the adaptive immune system.\n\nPhagocytosis is an important feature of cellular innate immunity performed by cells called phagocytes that engulf, or eat, pathogens or particles. Phagocytes generally patrol the body searching for pathogens, but can be called to specific locations by cytokines. Once a pathogen has been engulfed by a phagocyte, it becomes trapped in an intracellular vesicle called a phagosome, which subsequently fuses with another vesicle called a lysosome to form a phagolysosome. The pathogen is killed by the activity of digestive enzymes or following a respiratory burst that releases free radicals into the phagolysosome. Phagocytosis evolved as a means of acquiring nutrients, but this role was extended in phagocytes to include engulfment of pathogens as a defense mechanism. Phagocytosis probably represents the oldest form of host defense, as phagocytes have been identified in both vertebrate and invertebrate animals.\n\nNeutrophils and macrophages are phagocytes that travel throughout the body in pursuit of invading pathogens. Neutrophils are normally found in the bloodstream and are the most abundant type of phagocyte, normally representing 50% to 60% of the total circulating leukocytes, and consisting of neutrophil-killer and neutrophil-cager subpopulations. During the acute phase of inflammation, particularly as a result of bacterial infection, neutrophils migrate toward the site of inflammation in a process called chemotaxis, and are usually the first cells to arrive at the scene of infection. Macrophages are versatile cells that reside within tissues and produce a wide array of chemicals including enzymes, complement proteins, and cytokines, while they can also act as scavengers that rid the body of worn-out cells and other debris, and as antigen-presenting cells that activate the adaptive immune system.\n\nDendritic cells (DC) are phagocytes in tissues that are in contact with the external environment; therefore, they are located mainly in the skin, nose, lungs, stomach, and intestines. They are named for their resemblance to neuronal dendrites, as both have many spine-like projections, but dendritic cells are in no way connected to the nervous system. Dendritic cells serve as a link between the bodily tissues and the innate and adaptive immune systems, as they present antigens to T cells, one of the key cell types of the adaptive immune system.\n\nMast cells reside in connective tissues and mucous membranes, and regulate the inflammatory response. They are most often associated with allergy and anaphylaxis. Basophils and eosinophils are related to neutrophils. They secrete chemical mediators that are involved in defending against parasites and play a role in allergic reactions, such as asthma. \n\nNatural killer cells, or NK cells, are lymphocytes and a component of the innate immune system which does not directly attack invading microbes. Rather, NK cells destroy compromised host cells, such as tumor cells or virus-infected cells, recognizing such cells by a condition known as \"missing self.\" This term describes cells with low levels of a cell-surface marker called MHC I (major histocompatibility complex)—a situation that can arise in viral infections of host cells. They were named \"natural killer\" because of the initial notion that they do not require activation in order to kill cells that are \"missing self.\" For many years it was unclear how NK cells recognize tumor cells and infected cells. It is now known that the MHC makeup on the surface of those cells is altered and the NK cells become activated through recognition of \"missing self\". Normal body cells are not recognized and attacked by NK cells because they express intact self MHC antigens. Those MHC antigens are recognized by killer cell immunoglobulin receptors (KIR) which essentially put the brakes on NK cells.\n\nThe adaptive immune system evolved in early vertebrates and allows for a stronger immune response as well as immunological memory, where each pathogen is \"remembered\" by a signature antigen. The adaptive immune response is antigen-specific and requires the recognition of specific \"non-self\" antigens during a process called antigen presentation. Antigen specificity allows for the generation of responses that are tailored to specific pathogens or pathogen-infected cells. The ability to mount these tailored responses is maintained in the body by \"memory cells\". Should a pathogen infect the body more than once, these specific memory cells are used to quickly eliminate it.\n\nThe cells of the adaptive immune system are special types of leukocytes, called lymphocytes. B cells and T cells are the major types of lymphocytes and are derived from hematopoietic stem cells in the bone marrow. B cells are involved in the humoral immune response, whereas T cells are involved in cell-mediated immune response.\n\nBoth B cells and T cells carry receptor molecules that recognize specific targets. T cells recognize a \"non-self\" target, such as a pathogen, only after antigens (small fragments of the pathogen) have been processed and presented in combination with a \"self\" receptor called a major histocompatibility complex (MHC) molecule. There are two major subtypes of T cells: the killer T cell and the helper T cell. In addition there are regulatory T cells which have a role in modulating immune response. Killer T cells only recognize antigens coupled to Class I MHC molecules, while helper T cells and regulatory T cells only recognize antigens coupled to Class II MHC molecules. These two mechanisms of antigen presentation reflect the different roles of the two types of T cell. A third, minor subtype are the γδ T cells that recognize intact antigens that are not bound to MHC receptors. The double-positive T cells are exposed to a wide variety of self-antigens in the thymus, in which iodine is necessary for its thymus development and activity.\n\nIn contrast, the B cell antigen-specific receptor is an antibody molecule on the B cell surface, and recognizes whole pathogens without any need for antigen processing. Each lineage of B cell expresses a different antibody, so the complete set of B cell antigen receptors represent all the antibodies that the body can manufacture.\n\nKiller T cells are a sub-group of T cells that kill cells that are infected with viruses (and other pathogens), or are otherwise damaged or dysfunctional. As with B cells, each type of T cell recognizes a different antigen. Killer T cells are activated when their T-cell receptor (TCR) binds to this specific antigen in a complex with the MHC Class I receptor of another cell. Recognition of this MHC:antigen complex is aided by a co-receptor on the T cell, called CD8. The T cell then travels throughout the body in search of cells where the MHC I receptors bear this antigen. When an activated T cell contacts such cells, it releases cytotoxins, such as perforin, which form pores in the target cell's plasma membrane, allowing ions, water and toxins to enter. The entry of another toxin called granulysin (a protease) induces the target cell to undergo apoptosis. T cell killing of host cells is particularly important in preventing the replication of viruses. T cell activation is tightly controlled and generally requires a very strong MHC/antigen activation signal, or additional activation signals provided by \"helper\" T cells (see below).\n\nHelper T cells regulate both the innate and adaptive immune responses and help determine which immune responses the body makes to a particular pathogen. These cells have no cytotoxic activity and do not kill infected cells or clear pathogens directly. They instead control the immune response by directing other cells to perform these tasks.\n\nHelper T cells express T cell receptors (TCR) that recognize antigen bound to Class II MHC molecules. The MHC:antigen complex is also recognized by the helper cell's CD4 co-receptor, which recruits molecules inside the T cell (e.g., Lck) that are responsible for the T cell's activation. Helper T cells have a weaker association with the MHC:antigen complex than observed for killer T cells, meaning many receptors (around 200–300) on the helper T cell must be bound by an MHC:antigen in order to activate the helper cell, while killer T cells can be activated by engagement of a single MHC:antigen molecule. Helper T cell activation also requires longer duration of engagement with an antigen-presenting cell. The activation of a resting helper T cell causes it to release cytokines that influence the activity of many cell types. Cytokine signals produced by helper T cells enhance the microbicidal function of macrophages and the activity of killer T cells. In addition, helper T cell activation causes an upregulation of molecules expressed on the T cell's surface, such as CD40 ligand (also called CD154), which provide extra stimulatory signals typically required to activate antibody-producing B cells.\n\nGamma delta T cells (γδ T cells) possess an alternative T-cell receptor (TCR) as opposed to CD4+ and CD8+ (αβ) T cells and share the characteristics of helper T cells, cytotoxic T cells and NK cells. The conditions that produce responses from γδ T cells are not fully understood. Like other 'unconventional' T cell subsets bearing invariant TCRs, such as CD1d-restricted Natural Killer T cells, γδ T cells straddle the border between innate and adaptive immunity. On one hand, γδ T cells are a component of adaptive immunity as they rearrange TCR genes to produce receptor diversity and can also develop a memory phenotype. On the other hand, the various subsets are also part of the innate immune system, as restricted TCR or NK receptors may be used as pattern recognition receptors. For example, large numbers of human Vγ9/Vδ2 T cells respond within hours to common molecules produced by microbes, and highly restricted Vδ1+ T cells in epithelia respond to stressed epithelial cells.\nA B cell identifies pathogens when antibodies on its surface bind to a specific foreign antigen. This antigen/antibody complex is taken up by the B cell and processed by proteolysis into peptides. The B cell then displays these antigenic peptides on its surface MHC class II molecules. This combination of MHC and antigen attracts a matching helper T cell, which releases lymphokines and activates the B cell. As the activated B cell then begins to divide, its offspring (plasma cells) secrete millions of copies of the antibody that recognizes this antigen. These antibodies circulate in blood plasma and lymph, bind to pathogens expressing the antigen and mark them for destruction by complement activation or for uptake and destruction by phagocytes. Antibodies can also neutralize challenges directly, by binding to bacterial toxins or by interfering with the receptors that viruses and bacteria use to infect cells.\n\nEvolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T-cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.\n\nWhen B cells and T cells are activated and begin to replicate, some of their offspring become long-lived memory cells. Throughout the lifetime of an animal, these memory cells remember each specific pathogen encountered and can mount a strong response if the pathogen is detected again. This is \"adaptive\" because it occurs during the lifetime of an individual as an adaptation to infection with that pathogen and prepares the immune system for future challenges. Immunological memory can be in the form of either passive short-term memory or active long-term memory.\n\nNewborn infants have no prior exposure to microbes and are particularly vulnerable to infection. Several layers of passive protection are provided by the mother. During pregnancy, a particular type of antibody, called IgG, is transported from mother to baby directly through the placenta, so human babies have high levels of antibodies even at birth, with the same range of antigen specificities as their mother. Breast milk or colostrum also contains antibodies that are transferred to the gut of the infant and protect against bacterial infections until the newborn can synthesize its own antibodies. This is passive immunity because the fetus does not actually make any memory cells or antibodies—it only borrows them. This passive immunity is usually short-term, lasting from a few days up to several months. In medicine, protective passive immunity can also be transferred artificially from one individual to another via antibody-rich serum.\nLong-term \"active\" memory is acquired following infection by activation of B and T cells. Active immunity can also be generated artificially, through vaccination. The principle behind vaccination (also called immunization) is to introduce an antigen from a pathogen in order to stimulate the immune system and develop specific immunity against that particular pathogen without causing disease associated with that organism. This deliberate induction of an immune response is successful because it exploits the natural specificity of the immune system, as well as its inducibility. With infectious disease remaining one of the leading causes of death in the human population, vaccination represents the most effective manipulation of the immune system mankind has developed.\n\nMost viral vaccines are based on live attenuated viruses, while many bacterial vaccines are based on acellular components of micro-organisms, including harmless toxin components. Since many antigens derived from acellular vaccines do not strongly induce the adaptive response, most bacterial vaccines are provided with additional adjuvants that activate the antigen-presenting cells of the innate immune system and maximize immunogenicity.\n\nThe immune system is a remarkably effective structure that incorporates specificity, inducibility and adaptation. Failures of host defense do occur, however, and fall into three broad categories: immunodeficiencies, autoimmunity, and hypersensitivities.\n\nImmunodeficiencies occur when one or more of the components of the immune system are inactive. The ability of the immune system to respond to pathogens is diminished in both the young and the elderly, with immune responses beginning to decline at around 50 years of age due to immunosenescence. In developed countries, obesity, alcoholism, and drug use are common causes of poor immune function. However, malnutrition is the most common cause of immunodeficiency in developing countries. Diets lacking sufficient protein are associated with impaired cell-mediated immunity, complement activity, phagocyte function, IgA antibody concentrations, and cytokine production. Additionally, the loss of the thymus at an early age through genetic mutation or surgical removal results in severe immunodeficiency and a high susceptibility to infection.\n\nImmunodeficiencies can also be inherited or 'acquired'. Chronic granulomatous disease, where phagocytes have a reduced ability to destroy pathogens, is an example of an inherited, or congenital, immunodeficiency. AIDS and some types of cancer cause acquired immunodeficiency.\n\nOveractive immune responses comprise the other end of immune dysfunction, particularly the autoimmune disorders. Here, the immune system fails to properly distinguish between self and non-self, and attacks part of the body. Under normal circumstances, many T cells and antibodies react with \"self\" peptides. One of the functions of specialized cells (located in the thymus and bone marrow) is to present young lymphocytes with self antigens produced throughout the body and to eliminate those cells that recognize self-antigens, preventing autoimmunity.\n\nHypersensitivity is an immune response that damages the body's own tissues. They are divided into four classes (Type I – IV) based on the mechanisms involved and the time course of the hypersensitive reaction. Type I hypersensitivity is an immediate or anaphylactic reaction, often associated with allergy. Symptoms can range from mild discomfort to death. Type I hypersensitivity is mediated by IgE, which triggers degranulation of mast cells and basophils when cross-linked by antigen.\nType II hypersensitivity occurs when antibodies bind to antigens on the patient's own cells, marking them for destruction. This is also called antibody-dependent (or cytotoxic) hypersensitivity, and is mediated by IgG and IgM antibodies.\nImmune complexes (aggregations of antigens, complement proteins, and IgG and IgM antibodies) deposited in various tissues trigger Type III hypersensitivity reactions. Type IV hypersensitivity (also known as cell-mediated or \"delayed type hypersensitivity\") usually takes between two and three days to develop. Type IV reactions are involved in many autoimmune and infectious diseases, but may also involve \"contact dermatitis\" (poison ivy). These reactions are mediated by T cells, monocytes, and macrophages.\n\nInflammation is one of the first responses of the immune system to infection, but it can appear without known cause.\n\nInflammation is produced by eicosanoids and cytokines, which are released by injured or infected cells. Eicosanoids include prostaglandins that produce fever and the dilation of blood vessels associated with inflammation, and leukotrienes that attract certain white blood cells (leukocytes). Common cytokines include interleukins that are responsible for communication between white blood cells; chemokines that promote chemotaxis; and interferons that have anti-viral effects, such as shutting down protein synthesis in the host cell. Growth factors and cytotoxic factors may also be released. These cytokines and other chemicals recruit immune cells to the site of infection and promote healing of any damaged tissue following the removal of pathogens.\n\nIt is likely that a multicomponent, adaptive immune system arose with the first vertebrates, as invertebrates do not generate lymphocytes or an antibody-based humoral response. Many species, however, utilize mechanisms that appear to be precursors of these aspects of vertebrate immunity. Immune systems appear even in the structurally most simple forms of life, with bacteria using a unique defense mechanism, called the restriction modification system to protect themselves from viral pathogens, called bacteriophages. Prokaryotes also possess acquired immunity, through a system that uses CRISPR sequences to retain fragments of the genomes of phage that they have come into contact with in the past, which allows them to block virus replication through a form of RNA interference. Prokaryotes also possess other defense mechanisms. Offensive elements of the immune systems are also present in unicellular eukaryotes, but studies of their roles in defense are few.\n\nPattern recognition receptors are proteins used by nearly all organisms to identify molecules associated with pathogens. Antimicrobial peptides called defensins are an evolutionarily conserved component of the innate immune response found in all animals and plants, and represent the main form of invertebrate systemic immunity. The complement system and phagocytic cells are also used by most forms of invertebrate life. Ribonucleases and the RNA interference pathway are conserved across all eukaryotes, and are thought to play a role in the immune response to viruses.\n\nUnlike animals, plants lack phagocytic cells, but many plant immune responses involve systemic chemical signals that are sent through a plant. Individual plant cells respond to molecules associated with pathogens known as Pathogen-associated molecular patterns or PAMPs. When a part of a plant becomes infected, the plant produces a localized hypersensitive response, whereby cells at the site of infection undergo rapid apoptosis to prevent the spread of the disease to other parts of the plant. Systemic acquired resistance (SAR) is a type of defensive response used by plants that renders the entire plant resistant to a particular infectious agent. RNA silencing mechanisms are particularly important in this systemic response as they can block virus replication.\n\nAnother important role of the immune system is to identify and eliminate tumors. This is called immune surveillance. The \"transformed cells\" of tumors express antigens that are not found on normal cells. To the immune system, these antigens appear foreign, and their presence causes immune cells to attack the transformed tumor cells. The antigens expressed by tumors have several sources; some are derived from oncogenic viruses like human papillomavirus, which causes cervical cancer, while others are the organism's own proteins that occur at low levels in normal cells but reach high levels in tumor cells. One example is an enzyme called tyrosinase that, when expressed at high levels, transforms certain skin cells (e.g. melanocytes) into tumors called melanomas. A third possible source of tumor antigens are proteins normally important for regulating cell growth and survival, that commonly mutate into cancer inducing molecules called oncogenes.\n\nThe main response of the immune system to tumors is to destroy the abnormal cells using killer T cells, sometimes with the assistance of helper T cells. Tumor antigens are presented on MHC class I molecules in a similar way to viral antigens. This allows killer T cells to recognize the tumor cell as abnormal. NK cells also kill tumorous cells in a similar way, especially if the tumor cells have fewer MHC class I molecules on their surface than normal; this is a common phenomenon with tumors. Sometimes antibodies are generated against tumor cells allowing for their destruction by the complement system.\n\nClearly, some tumors evade the immune system and go on to become cancers. Tumor cells often have a reduced number of MHC class I molecules on their surface, thus avoiding detection by killer T cells. Some tumor cells also release products that inhibit the immune response; for example by secreting the cytokine TGF-β, which suppresses the activity of macrophages and lymphocytes. In addition, immunological tolerance may develop against tumor antigens, so the immune system no longer attacks the tumor cells.\n\nParadoxically, macrophages can promote tumor growth when tumor cells send out cytokines that attract macrophages, which then generate cytokines and growth factors such as tumor-necrosis factor alpha that nurture tumor development or promote stem-cell-like plasticity. In addition, a combination of hypoxia in the tumor and a cytokine produced by macrophages induces tumor cells to decrease production of a protein that blocks metastasis and thereby assists spread of cancer cells.\n\nThe immune system is involved in many aspects of physiological regulation in the body. The immune system interacts intimately with other systems, such as the endocrine and the nervous systems. The immune system also plays a crucial role in embryogenesis (development of the embryo), as well as in tissue repair and regeneration.\n\nHormones can act as immunomodulators, altering the sensitivity of the immune system. For example, female sex hormones are known immunostimulators of both adaptive and innate immune responses. Some autoimmune diseases such as lupus erythematosus strike women preferentially, and their onset often coincides with puberty. By contrast, male sex hormones such as testosterone seem to be immunosuppressive. Other hormones appear to regulate the immune system as well, most notably prolactin, growth hormone and vitamin D.\n\nWhen a T-cell encounters a foreign pathogen, it extends a vitamin D receptor. This is essentially a signaling device that allows the T-cell to bind to the active form of vitamin D, the steroid hormone calcitriol. T-cells have a symbiotic relationship with vitamin D. Not only does the T-cell extend a vitamin D receptor, in essence asking to bind to the steroid hormone version of vitamin D, calcitriol, but the T-cell expresses the gene CYP27B1, which is the gene responsible for converting the pre-hormone version of vitamin D, calcidiol into the steroid hormone version, calcitriol. Only after binding to calcitriol can T-cells perform their intended function. Other immune system cells that are known to express CYP27B1 and thus activate vitamin D calcidiol, are dendritic cells, keratinocytes and macrophages.\n\nIt is conjectured that a progressive decline in hormone levels with age is partially responsible for weakened immune responses in aging individuals. Conversely, some hormones are regulated by the immune system, notably thyroid hormone activity. The age-related decline in immune function is also related to decreasing vitamin D levels in the elderly. As people age, two things happen that negatively affect their vitamin D levels. First, they stay indoors more due to decreased activity levels. This means that they get less sun and therefore produce less cholecalciferol via UVB radiation. Second, as a person ages the skin becomes less adept at producing vitamin D.\n\nThe immune system is affected by sleep and rest, and sleep deprivation is detrimental to immune function. Complex feedback loops involving cytokines, such as interleukin-1 and tumor necrosis factor-α produced in response to infection, appear to also play a role in the regulation of non-rapid eye movement (REM) sleep. Thus the immune response to infection may result in changes to the sleep cycle, including an increase in slow-wave sleep relative to REM sleep.\n\nWhen suffering from sleep deprivation, active immunizations may have a diminished effect and may result in lower antibody production, and a lower immune response, than would be noted in a well-rested individual. Additionally, proteins such as NFIL3, which have been shown to be closely intertwined with both T-cell differentiation and our circadian rhythms, can be affected through the disturbance of natural light and dark cycles through instances of sleep deprivation, shift work, etc. As a result, these disruptions can lead to an increase in chronic conditions such as heart disease, chronic pain, and asthma.\n\nIn addition to the negative consequences of sleep deprivation, sleep and the intertwined circadian system have been shown to have strong regulatory effects on immunological functions affecting both the innate and the adaptive immunity. First, during the early slow-wave-sleep stage, a sudden drop in blood levels of cortisol, epinephrine, and norepinephrine induce increased blood levels of the hormones leptin, pituitary growth hormone, and prolactin. These signals induce a pro-inflammatory state through the production of the pro-inflammatory cytokines interleukin-1, interleukin-12, TNF-alpha and IFN-gamma. These cytokines then stimulate immune functions such as immune cells activation, proliferation, and differentiation. It is during this time that undifferentiated, or less differentiated, like naïve and central memory T cells, peak (i.e. during a time of a slowly evolving adaptive immune response). In addition to these effects, the milieu of hormones produced at this time (leptin, pituitary growth hormone, and prolactin) support the interactions between APCs and T-cells, a shift of the T1/T2 cytokine balance towards one that supports T1, an increase in overall T cell proliferation, and naïve T cell migration to lymph nodes. This milieu is also thought to support the formation of long-lasting immune memory through the initiation of Th1 immune responses.\n\nIn contrast, during wake periods differentiated effector cells, such as cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes), peak in order to elicit an effective response against any intruding pathogens. As well during awake active times, anti-inflammatory molecules, such as cortisol and catecholamines, peak. There are two theories as to why the pro-inflammatory state is reserved for sleep time. First, inflammation would cause serious cognitive and physical impairments if it were to occur during wake times. Second, inflammation may occur during sleep times due to the presence of melatonin. Inflammation causes a great deal of oxidative stress and the presence of melatonin during sleep times could actively counteract free radical production during this time.\n\nOvernutrition is associated with diseases such as diabetes and obesity, which are known to affect immune function. More moderate malnutrition, as well as certain specific trace mineral and nutrient deficiencies, can also compromise the immune response.\n\nFoods rich in certain fatty acids may foster a healthy immune system. Likewise, fetal undernourishment can cause a lifelong impairment of the immune system.\n\nThe immune system, particularly the innate component, plays a decisive role in tissue repair after an insult. Key actors include macrophages and neutrophils, but other cellular actors, including γδ T cells, innate lymphoid cells (ILCs), and regulatory T cells (Tregs), are also important. The plasticity of immune cells and the balance between pro-inflammatory and anti-inflammatory signals are crucial aspects of efficient tissue repair. Immune components and pathways are involved in regeneration as well, for example in amphibians. According to one hypothesis, organisms that can regenerate could be less immunocompetent than organisms that cannot regenerate.\n\nThe immune response can be manipulated to suppress unwanted responses resulting from autoimmunity, allergy, and transplant rejection, and to stimulate protective responses against pathogens that largely elude the immune system (see immunization) or cancer.\n\nImmunosuppressive drugs are used to control autoimmune disorders or inflammation when excessive tissue damage occurs, and to prevent transplant rejection after an organ transplant.\n\nAnti-inflammatory drugs are often used to control the effects of inflammation. Glucocorticoids are the most powerful of these drugs; however, these drugs can have many undesirable side effects, such as central obesity, hyperglycemia, osteoporosis, and their use must be tightly controlled. Lower doses of anti-inflammatory drugs are often used in conjunction with cytotoxic or immunosuppressive drugs such as methotrexate or azathioprine.\nCytotoxic drugs inhibit the immune response by killing dividing cells such as activated T cells. However, the killing is indiscriminate and other constantly dividing cells and their organs are affected, which causes toxic side effects. Immunosuppressive drugs such as cyclosporin prevent T cells from responding to signals correctly by inhibiting signal transduction pathways.\n\nCancer immunotherapy covers the medical ways to stimulate the immune system to attack cancer tumours.\n\nImmunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between \"cellular\" and \"humoral\" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells—more precisely, phagocytes—that were responsible for immune responses. In contrast, the humoral theory of immunity, held, among others, by Robert Koch and Emil von Behring, stated that the active immune agents were soluble components (molecules) found in the organism’s “humors” rather than its cells.\n\nIn the mid-1950s, Frank Burnet, inspired by a suggestion made by Niels Jerne, formulated the clonal selection theory (CST) of immunity. On the basis of CST, Burnet developed a theory of how an immune response is triggered according to the self/nonself distinction: \"self\" constituents (constituents of the body) do not trigger destructive immune responses, while \"nonself\" entities (pathogens, an allograft) trigger a destructive immune response. The theory was later modified to reflect new discoveries regarding histocompatibility or the complex \"two-signal\" activation of T cells. The self/nonself theory of immunity and the self/nonself vocabulary have been criticized, but remain very influential.\n\nMore recently, several theoretical frameworks have been suggested in immunology, including \"autopoietic\" views, \"cognitive immune\" views, the \"danger model\" (or \"danger theory\"), and the \"discontinuity\" theory. The danger model, suggested by Polly Matzinger and colleagues, has been very influential, arousing many comments and discussions.\n\nLarger drugs (>500 Da) can provoke a neutralizing immune response, particularly if the drugs are administered repeatedly, or in larger doses. This limits the effectiveness of drugs based on larger peptides and proteins (which are typically larger than 6000 Da). In some cases, the drug itself is not immunogenic, but may be co-administered with an immunogenic compound, as is sometimes the case for Taxol. Computational methods have been developed to predict the immunogenicity of peptides and proteins, which are particularly useful in designing therapeutic antibodies, assessing likely virulence of mutations in viral coat particles, and validation of proposed peptide-based drug treatments. Early techniques relied mainly on the observation that hydrophilic amino acids are overrepresented in epitope regions than hydrophobic amino acids; however, more recent developments rely on machine learning techniques using databases of existing known epitopes, usually on well-studied virus proteins, as a training set. A publicly accessible database has been established for the cataloguing of epitopes from pathogens known to be recognizable by B cells. The emerging field of bioinformatics-based studies of immunogenicity is referred to as \"immunoinformatics\". Immunoproteomics is the study of large sets of proteins (proteomics) involved in the immune response.\n\nThe success of any pathogen depends on its ability to elude host immune responses. Therefore, pathogens evolved several methods that allow them to successfully infect a host, while evading detection or destruction by the immune system. Bacteria often overcome physical barriers by secreting enzymes that digest the barrier, for example, by using a type II secretion system. Alternatively, using a type III secretion system, they may insert a hollow tube into the host cell, providing a direct route for proteins to move from the pathogen to the host. These proteins are often used to shut down host defenses.\n\nAn evasion strategy used by several pathogens to avoid the innate immune system is to hide within the cells of their host (also called intracellular pathogenesis). Here, a pathogen spends most of its life-cycle inside host cells, where it is shielded from direct contact with immune cells, antibodies and complement. Some examples of intracellular pathogens include viruses, the food poisoning bacterium \"Salmonella\" and the eukaryotic parasites that cause malaria (\"Plasmodium falciparum\") and leishmaniasis (\"Leishmania spp.\"). Other bacteria, such as \"Mycobacterium tuberculosis\", live inside a protective capsule that prevents lysis by complement. Many pathogens secrete compounds that diminish or misdirect the host's immune response. Some bacteria form biofilms to protect themselves from the cells and proteins of the immune system. Such biofilms are present in many successful infections, e.g., the chronic \"Pseudomonas aeruginosa\" and \"Burkholderia cenocepacia\" infections characteristic of cystic fibrosis. Other bacteria generate surface proteins that bind to antibodies, rendering them ineffective; examples include \"Streptococcus\" (protein G), \"Staphylococcus aureus\" (protein A), and \"Peptostreptococcus magnus\" (protein L).\n\nThe mechanisms used to evade the adaptive immune system are more complicated. The simplest approach is to rapidly change non-essential epitopes (amino acids and/or sugars) on the surface of the pathogen, while keeping essential epitopes concealed. This is called antigenic variation. An example is HIV, which mutates rapidly, so the proteins on its viral envelope that are essential for entry into its host target cell are constantly changing. These frequent changes in antigens may explain the failures of vaccines directed at this virus. The parasite \"Trypanosoma brucei\" uses a similar strategy, constantly switching one type of surface protein for another, allowing it to stay one step ahead of the antibody response. Masking antigens with host molecules is another common strategy for avoiding detection by the immune system. In HIV, the envelope that covers the virion is formed from the outermost membrane of the host cell; such \"self-cloaked\" viruses make it difficult for the immune system to identify them as \"non-self\" structures.\n\n"}
{"id": "14959", "url": "https://en.wikipedia.org/wiki?curid=14959", "title": "Immunology", "text": "Immunology\n\nImmunology is a branch of biology that covers the study of immune systems in all organisms. Immunology charts, measures, and contextualizes the physiological functioning of the immune system in states of both health and diseases; malfunctions of the immune system in immunological disorders (such as autoimmune diseases, hypersensitivities, immune deficiency, and transplant rejection); and the physical, chemical, and physiological characteristics of the components of the immune system \"in vitro\", \"in situ\", and \"in vivo\". Immunology has applications in numerous disciplines of medicine, particularly in the fields of organ transplantation, oncology, rheumatology, virology, bacteriology, parasitology, psychiatry, and dermatology.\n\nThe term was coined by Russian biologist Ilya Ilyich Mechnikov, who advanced studies on immunology and received the Nobel Prize for his work in 1908. He pinned small thorns into starfish larvae and noticed unusual cells surrounding the thorns. This was the active response of the body trying to maintain its integrity. It was Mechnikov who first observed the phenomenon of phagocytosis, in which the body defends itself against a foreign body.\n\nPrior to the designation of immunity, from the etymological root \"immunis\", which is Latin for \"exempt\", early physicians characterized organs that would later be proven as essential components of the immune system. The important lymphoid organs of the immune system are the thymus, bone marrow, and chief lymphatic tissues such as spleen, tonsils, lymph vessels, lymph nodes, adenoids, and liver. When health conditions worsen to emergency status, portions of immune system organs, including the thymus, spleen, bone marrow, lymph nodes, and other lymphatic tissues, can be surgically excised for examination while patients are still alive.\n\nMany components of the immune system are typically cellular in nature and not associated with any specific organ, but rather are embedded or circulating in various tissues located throughout the body.\n\nClassical immunology ties in with the fields of epidemiology and medicine. It studies the relationship between the body systems, pathogens, and immunity. The earliest written mention of immunity can be traced back to the plague of Athens in 430 BCE. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. Many other ancient societies have references to this phenomenon, but it was not until the 19th and 20th centuries before the concept developed into scientific theory.\n\nThe study of the molecular and cellular components that comprise the immune system, including their function and interaction, is the central science of immunology. The immune system has been divided into a more primitive innate immune system and, in vertebrates, an acquired or adaptive immune system. The latter is further divided into humoral (or antibody) and cell-mediated components.\n\nThe immune system has the capability of self and non-self-recognition. An antigen is a substance that ignites the immune response. The cells involved in recognizing the antigen are Lymphocytes. Once they recognize, they secrete antibodies. Antibodies are proteins that neutralize the disease-causing microorganisms. The antibodies don’t kill the pathogens rather phagocytes are involved in it.\n\nThe humoral (antibody) response is defined as the interaction between antibodies and antigens. Antibodies are specific proteins released from a certain class of immune cells known as B lymphocytes, while antigens are defined as anything that elicits the generation of antibodies (\"anti\"body \"gen\"erators). Immunology rests on an understanding of the properties of these two biological entities and the cellular response to both.\n\nIt’s now getting clear that the immune responses contribute to the development of many common disorders not traditionally viewed as immunologic, including metabolic, cardiovascular, cancer, and neurodegenerative conditions like Alzheimer’s disease. Besides, there are direct implications of the immune system in the infectious diseases (tuberculosis, malaria, hepatitis, pneumonia, dysentery, and helminth infestations) as well. Hence, research in the field of immunology is of prime importance for the advancements in the fields of modern medicine, biomedical research, and biotechnology.\n\nImmunological research continues to become more specialized, pursuing non-classical models of immunity and functions of cells, organs and systems not previously associated with the immune system (Yemeserach 2010).\n\nClinical immunology is the study of diseases caused by disorders of the immune system (failure, aberrant action, and malignant growth of the cellular elements of the system). It also involves diseases of other systems, where immune reactions play a part in the pathology and clinical features.\n\nThe diseases caused by disorders of the immune system fall into two broad categories:\nOther immune system disorders include various hypersensitivities (such as in asthma and other allergies) that respond inappropriately to otherwise harmless compounds.\n\nThe most well-known disease that affects the immune system itself is AIDS, an immunodeficiency characterized by the suppression of CD4+ (\"helper\") T cells, dendritic cells and macrophages by the Human Immunodeficiency Virus (HIV).\n\nClinical immunologists also study ways to prevent the immune system's attempts to destroy allografts (transplant rejection).\n\nThe body’s capability to react to antigens depends on a person's age, antigen type, maternal factors and the area where the antigen is presented. Neonates are said to be in a state of physiological immunodeficiency, because both their innate and adaptive immunological responses are greatly suppressed. Once born, a child’s immune system responds favorably to protein antigens while not as well to glycoproteins and polysaccharides. In fact, many of the infections acquired by neonates are caused by low virulence organisms like \"Staphylococcus\" and \"Pseudomonas\". In neonates, opsonic activity and the ability to activate the complement cascade is very limited. For example, the mean level of C3 in a newborn is approximately 65% of that found in the adult. Phagocytic activity is also greatly impaired in newborns. This is due to lower opsonic activity, as well as diminished up-regulation of integrin and selectin receptors, which limit the ability of neutrophils to interact with adhesion molecules in the endothelium. Their monocytes are slow and have a reduced ATP production, which also limits the newborn's phagocytic activity. Although, the number of total lymphocytes is significantly higher than in adults, the cellular and humoral immunity is also impaired. Antigen-presenting cells in newborns have a reduced capability to activate T cells. Also, T cells of a newborn proliferate poorly and produce very small amounts of cytokines like IL-2, IL-4, IL-5, IL-12, and IFN-g which limits their capacity to activate the humoral response as well as the phagocitic activity of macrophage. B cells develop early during gestation but are not fully active.\n\nMaternal factors also play a role in the body’s immune response. At birth, most of the immunoglobulin present is maternal IgG. Because IgM, IgD, IgE and IgA don’t cross the placenta, they are almost undetectable at birth. Some IgA is provided by breast milk. These passively-acquired antibodies can protect the newborn for up to 18 months, but their response is usually short-lived and of low affinity. These antibodies can also produce a negative response. If a child is exposed to the antibody for a particular antigen before being exposed to the antigen itself then the child will produce a dampened response. Passively acquired maternal antibodies can suppress the antibody response to active immunization. Similarly the response of T-cells to vaccination differs in children compared to adults, and vaccines that induce Th1 responses in adults do not readily elicit these same responses in neonates. Between six and nine months after birth, a child’s immune system begins to respond more strongly to glycoproteins, but there is usually no marked improvement in their response to polysaccharides until they are at least one year old. This can be the reason for distinct time frames found in vaccination schedules.\n\nDuring adolescence, the human body undergoes various physical, physiological and immunological changes triggered and mediated by hormones, of which the most significant in females is 17-β-estradiol (an estrogen) and, in males, is testosterone. Estradiol usually begins to act around the age of 10 and testosterone some months later. There is evidence that these steroids not only act directly on the primary and secondary sexual characteristics but also have an effect on the development and regulation of the immune system, including an increased risk in developing pubescent and post-pubescent autoimmunity. There is also some evidence that cell surface receptors on B cells and macrophages may detect sex hormones in the system.\n\nThe female sex hormone 17-β-estradiol has been shown to regulate the level of immunological response, while some male androgens such as testosterone seem to suppress the stress response to infection. Other androgens, however, such as DHEA, increase immune response. As in females, the male sex hormones seem to have more control of the immune system during puberty and post-puberty than during the rest of a male's adult life.\n\nPhysical changes during puberty such as thymic involution also affect immunological response.\n\nEcoimmunology, or ecological immunology, explores the relationship between the immune system of an organism and its social, biotic and abiotic environment.\n\nMore recent ecoimmunological research has focused on host pathogen defences traditionally considered \"non-immunological\", such as pathogen avoidance, self-medication, symbiont-mediated defenses, and fecundity trade-offs. Behavioural immunity, a phrase coined by Mark Schaller, specifically refers to psychological pathogen avoidance drivers, such as disgust aroused by stimuli encountered around pathogen-infected individuals, such as the smell of vomit. More broadly, \"behavioural\" ecological immunity has been demonstrated in multiple species. For example, the Monarch butterfly often lays its eggs on certain toxic milkweed species when infected with parasites. These toxins reduce parasite growth in the offspring of the infected Monarch. However, when uninfected Monarch butterflies are forced to feed only on these toxic plants, they suffer a fitness cost as reduced lifespan relative to other uninfected Monarch butterflies. This indicates that laying eggs on toxic plants is a costly behaviour in Monarchs which has probably evolved to reduce the severity of parasite infection.\n\nSymbiont-mediated defenses are also heritable across host generations, despite a non-genetic direct basis for the transmission. Aphids, for example, rely on several different symbionts for defense from key parasites, and can vertically transmit their symbionts from parent to offspring. Therefore, a symbiont which successfully confers protection from a parasite is more likely to be passed to the host offspring, allowing coevolution with parasites attacking the host in a way similar to traditional immunity.\n\nThe use of immune system components to treat a disease or disorder is known as immunotherapy. Immunotherapy is most commonly used in the context of the treatment of cancers together with chemotherapy (drugs) and radiotherapy (radiation). However, immunotherapy is also often used in the immunosuppressed (such as HIV patients) and people suffering from other immune deficiencies or autoimmune diseases.\nThis includes regulating factors such as IL-2, IL-10, GM-CSF B, IFN-α.\n\nThe specificity of the bond between antibody and antigen has made the antibody an excellent tool for the detection of substances by a variety of diagnostic techniques. Antibodies specific for a desired antigen can be conjugated with an isotopic (radio) or fluorescent label or with a color-forming enzyme in order to detect it. However, the similarity between some antigens can lead to false positives and other errors in such tests by antibodies cross-reacting with antigens that aren't exact matches.\n\nThe study of the interaction of the immune system with cancer cells can lead to diagnostic tests and therapies with which to find and fight cancer.\n\nThis area of the immunology is devoted to the study of immunological aspects of the reproductive process including fetus acceptance. The term has also been used by fertility clinics to address fertility problems, recurrent miscarriages, premature deliveries and dangerous complications such as pre-eclampsia.\n\nImmunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between \"cellular\" and \"humoral\" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells – more precisely, phagocytes – that were responsible for immune responses. In contrast, the humoral theory of immunity, held by Robert Koch and Emil von Behring, among others, stated that the active immune agents were soluble components (molecules) found in the organism's \"humors\" rather than its cells.\n\nIn the mid-1950s, Macfarlane Burnet, inspired by a suggestion made by Niels Jerne, formulated the clonal selection theory (CST) of immunity. On the basis of CST, Burnet developed a theory of how an immune response is triggered according to the self/nonself distinction: \"self\" constituents (constituents of the body) do not trigger destructive immune responses, while \"nonself\" entities (e.g., pathogens, an allograft) trigger a destructive immune response. The theory was later modified to reflect new discoveries regarding histocompatibility or the complex \"two-signal\" activation of T cells. The self/nonself theory of immunity and the self/nonself vocabulary have been criticized, but remain very influential.\n\nMore recently, several theoretical frameworks have been suggested in immunology, including \"autopoietic\" views, \"cognitive immune\" views, the \"danger model\" (or \"danger theory\"), and the \"discontinuity\" theory. The danger model, suggested by Polly Matzinger and colleagues, has been very influential, arousing many comments and discussions.\n\n"}
{"id": "14960", "url": "https://en.wikipedia.org/wiki?curid=14960", "title": "IPA", "text": "IPA\n\nIPA commonly refers to:\n\nIPA may also refer to:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "14961", "url": "https://en.wikipedia.org/wiki?curid=14961", "title": "Ice beer", "text": "Ice beer\n\nIce beer is a Canadian term for pale lager beer which have undergone some degree of fractional freezing during production. These brands generally have higher alcohol content than typical beer and generally have a low price relative to their alcohol content.\n\nThe process of \"icing\" beer involves lowering the temperature of a batch of beer until ice crystals form. Since ethanol has a much lower freezing point (-114 °C; -173.2 °F) than water (0 °C; 32 °F), when the ice is removed, the alcohol concentration of the beer increases. The process is known as fractional freezing or freeze distillation.\n\nDeveloped by brewing a strong, dark lager, then freezing the beer and removing some of the ice. This would concentrate the aroma and taste of the beer, and also raise the alcoholic strength of the finished beer. More specifically, the method is as follows. \"By cooling beer to just below freezing, you separate out a large portion of water from the alcohol, which has a lower freezing point. You then skim off the ice crystals from the brew leaving behind a beer that is twice as potent as the original.\" That produces a beer with 12 to 15 per cent alcohol. In North America, water would be added to lower the alcohol level.\n\nEisbock was introduced to Canada in 1989 by the microbrewery Niagara Falls Brewing Company. The brewers started with a strong dark lager (15.3 degrees Plato/1.061 original gravity, 6% alcohol by volume), then used the traditional method of freezing and removing ice to concentrate aroma and flavours while increasing the alcoholic strength to 8% ABV. Niagara Falls Eisbock was released annually as a seasonal winter beer – each year the label would feature a different historic view of the nearby Niagara Falls in the winter. This continued each year until the company was sold in 1994.\n\nDespite this precedent, the large Canadian brewer Molson (now part of Molson Coors) claimed to have made the first ice beer in North America when it introduced \"Canadian Ice\" in April 1993. However, Molson's main competitor in Canada, Labatt (now part of Anheuser-Busch InBev) claimed to have patented the ice beer process earlier. When Labatt introduced an ice beer in August 1993, capturing a 10% market share in Canada, this instigated the so-called \"Ice Beer Wars\" of the 1990s.\n\nLabatt had patented a specific method for making ice beer in 1997, 1998 and 2000: \"A process for chill-treating, which is exemplified by a process for preparing a fermented malt beverage wherein brewing materials are mashed with water and the resulting mash is heated and wort separated therefrom. The wort is boiled cooled and fermented and the beer is subjected to a finishing stage, which includes aging, to produce the final beverage. The improvement comprises subjecting the beer to a cold stage comprising rapidly cooling the beer to a temperature of about its freezing point in such a manner that ice crystals are formed therein in only minimal amounts. The resulting cooled beer is then mixed for a short period of time with a beer slurry containing ice crystals, without any appreciable collateral increase in the amount of ice crystals in the resulting mixture. Finally, the so-treated beer is extracted from the mixture.\" The company provides the following explanation for the layman: \"During this unique process, the temperature is reduced until fine ice crystals form in the beer. Then using an exclusive process, the crystals are removed. The result is a full flavoured balanced beer.\"\n\nMiller acquired the U.S. marketing and distribution rights to Molson's products, and first introduced the Molson product in the United States in August 1993 as \"Molson Ice\". Miller also introduced the \"Icehouse\" brand under the \"Plank Road Brewery\" brand name shortly thereafter, and it is still sold nationwide.\n\nAnheuser-Busch introduced \"Bud Ice\" (5.5% ABV) in 1994, and it remains one of the country's top selling ice beers. \"Bud Ice\" has a somewhat lower alcohol content than most other ice beer brands. In 1995, Anheuser-Busch also introduced two other major brands: \"Busch Ice\" (5.9% ABV, introduced 1995) and \"Natural Ice\" (also 5.9% ABV, also introduced in 1995). \"Natural Ice\" is the No. 1 selling ice beer brand in the United States; its low price makes it very popular on college campuses all over the country. Keystone Ice, a value-based subdivision of Coors, also produces a 5.9% ABV brew labeled \"Keystone Ice\".\n\nCommon ice beer brands in Canada in 2017, with approximately 5.5 to 6 per cent alcohol content, include Carling Ice, Molson Keystone Ice, Molson's Black Ice, Busch Ice, Old Milwaukee Ice, Brick's Laker Ice and Labatt Ice. There is a Labatt Maximum Ice too, with 7.1 per cent alcohol.\n\nThe ice beers are typically known for their high alcohol-to-dollar ratio. In some areas, a substantial number of ice beer products are considered to often be bought by \"street drunks\", and are prohibited for sale. For example, most of the products that are explicitly listed as prohibited in the beer and malt liquor category in the Seattle area are ice beers.\n\n"}
{"id": "14962", "url": "https://en.wikipedia.org/wiki?curid=14962", "title": "Identity element", "text": "Identity element\n\nIn mathematics, an identity element or neutral element is a special type of element of a set with respect to a binary operation on that set, which leaves other elements unchanged when combined with them. This concept is used in algebraic structures such as groups and rings. The term \"identity element\" is often shortened to \"identity\" (as will be done in this article) when there is no possibility of confusion, but the identity implicitly depends on the binary operation it is associated with.\n\nLet be a set  with a binary operation ∗ on it. Then an element  of  is called a left identity if for all  in , and a right identity if for all  in . If is both a left identity and a right identity, then it is called a two-sided identity, or simply an identity.\n\nAn identity with respect to addition is called an additive identity (often denoted as 0) and an identity with respect to multiplication is called a multiplicative identity (often denoted as 1). These need not be ordinary addition and multiplication, but rather arbitrary operations. The distinction is used most often for sets that support both binary operations, such as rings and fields. The multiplicative identity is often called unity in the latter context (a ring with unity). This should not be confused with a unit in ring theory, which is any element having a multiplicative inverse. Unity itself is necessarily a unit.\n\nAs the last example (a semigroup) shows, it is possible for to have several left identities. In fact, every element can be a left identity. Similarly, there can be several right identities. But if there is both a right identity and a left identity, then they are equal and there is just a single two-sided identity. To see this, note that if is a left identity and is a right identity then . In particular, there can never be more than one two-sided identity. If there were two, and , then would have to be equal to both and .\n\nIt is also quite possible for to have \"no\" identity element. A common example of this is the cross product of vectors; in this case, the absence of an identity element is related to the fact that the direction of any nonzero cross product is always orthogonal to any element multiplied – so that it is not possible to obtain a non-zero vector in the same direction as the original. Another example would be the additive semigroup of positive natural numbers.\n\n\n"}
{"id": "14963", "url": "https://en.wikipedia.org/wiki?curid=14963", "title": "International Tropical Timber Agreement, 1983", "text": "International Tropical Timber Agreement, 1983\n\nThe International Tropical Timber Agreement (ITTA, 1983) is an agreement to provide an effective framework for cooperation between tropical timber producers and consumers and to encourage the development of national policies aimed at sustainable utilization and conservation of tropical forests and their genetic resources.\nThe International Tropical Timber Organization was established under this agreement.\n\nOpened for signature – November 18, 1983\n\nEntered into force – April 1, 1985; this agreement expired when the International Tropical Timber Agreement, 1994, went into force.\n\nFifty eight parties signed up to the agreement:\n\nAustralia, Austria, Belgium, Bolivia, Brazil, Burma, Cameroon, Canada, People's Republic of China, Colombia, Democratic Republic of the Congo, Republic of the Congo, Ivory Coast, Denmark, Ecuador, Egypt, European Union, Fiji, Finland, France, Gabon, Germany, Ghana, Greece, Guyana, Honduras, India, Indonesia, Ireland, Italy, Japan, South Korea, Liberia, Luxembourg, Malaysia, Nepal, Netherlands, New Zealand, Norway, Panama, Papua New Guinea, Peru, Philippines, Portugal, Russia, Spain, Sweden, Switzerland, Thailand, Togo, Trinidad and Tobago, United Kingdom, United States, Venezuela\n\n"}
{"id": "14964", "url": "https://en.wikipedia.org/wiki?curid=14964", "title": "International Tropical Timber Agreement, 1994", "text": "International Tropical Timber Agreement, 1994\n\nInternational Tropical Timber Agreement, 1994 (ITTA, 1994 or ITTA2) was drafted to ensure that by the year 2000 exports of tropical timber originated from sustainably managed sources and to establish a fund to assist tropical timber producers in obtaining the resources necessary to reach this objective.\nIt defined the mandate of the International Tropical Timber Organization.\n\nThe agreement was opened for signature on January 26, 1994, and entered into force on January 1, 1997.\n\nIt replaced the International Tropical Timber Agreement, 1983, and was superseded by the International Tropical Timber Agreement, 2006.\n\nSixty-two parties ultimately ratified the agreement:\n\nAustralia, Austria, Belgium, Bolivia, Brazil, Burma, Cambodia, Cameroon, Canada, Central African Republic, People's Republic of China, Colombia, Democratic Republic of the Congo, Republic of the Congo, Ivory Coast, Denmark, Ecuador, Egypt, European Union, Fiji, Finland, France, Gabon, Germany, Ghana, Greece, Guatemala, Guyana, Honduras, India, Indonesia, Ireland, Italy, Japan, South Korea, Liberia, Luxembourg, Malaysia, Mexico, Nepal, Netherlands, New Zealand, Nigeria, Norway, Panama, Papua New Guinea, Peru, Philippines, Poland, Portugal, Spain, Suriname, Sweden, Switzerland, Thailand, Togo, Trinidad and Tobago, United Kingdom, United States, Uruguay, Vanuatu, Venezuela\n\n"}
{"id": "14967", "url": "https://en.wikipedia.org/wiki?curid=14967", "title": "Instrumental", "text": "Instrumental\n\nAn instrumental is a musical composition or recording without lyrics, or singing, although it might include some inarticulate vocals, such as shouted backup vocals in a Big Band setting. Through semantic widening, a broader sense of the word song may refer to instrumentals. The music is primarily or exclusively produced using musical instruments. An instrumental can exist in music notation, after it is written by a composer; in the mind of the composer (especially in cases where the composer himself will perform the piece, as in the case of a blues solo guitarist or a folk music fiddle player); as a piece that is performed live by a single instrumentalist or a musical ensemble, which could range in components from a duo or trio to a large Big Band, concert band or orchestra.\n\nIn a song that is otherwise sung, a section that is not sung but which is played by instruments can be called an instrumental interlude, or, if it occurs at the beginning of the song, before the singer starts to sing, an instrumental introduction. If the instrumental section highlights the skill, musicality, and often the virtuosity of a particular performer (or group of performers), the section may be called a \"solo\" (e.g., the guitar solo that is a key section of heavy metal music and hard rock songs). If the instruments are percussion instruments, the interlude can be called a percussion interlude or \"percussion break\". These interludes are a form of break in the song.\n\nIn commercial popular music, instrumental tracks are sometimes renderings, remixes of a corresponding release that features vocals, but they may also be compositions originally conceived without vocals. One example of a genre in which both vocal/instrumental and solely instrumental songs are produced is blues. A blues band often uses mostly songs that have lyrics that are sung, but during the band's show, they may also perform instrumental songs which only include electric guitar, harmonica, upright bass/electric bass and drum kit.\n\nThe opposite of instrumental music, that is, music for voices alone, without any accompaniment instruments, is a cappella, an Italian phrase that means \"in the chapel\". In early music, instruments such as trumpet and drums were considered outdoor instruments, and music for inside a chapel typically used quieter instruments, voices, or just voices alone. A capella music exists in both Classical music choir pieces (for choir without any accompanist piano or pipe organ) and in popular music styles such as doo wop groups and Barbershop quartets. For genres in which a non-vocal song or interlude is conceived using computers and software, rather than with acoustic musical instruments or electronic musical instruments, the term instrumental is still used for it.\n\nSome recordings which include brief or non-musical use of the human voice are typically considered instrumentals. Examples include songs with the following:\n\nSongs including actual musical—rhythmic, melodic, and lyrical—vocals might still be categorized as instrumentals if the vocals appear only as a short part of an extended piece (e.g., \"Unchained Melody\" (Les Baxter), \"TSOP (The Sound of Philadelphia)\", \"Pick Up the Pieces\", \"The Hustle\", \"Fly, Robin, Fly\", \"Get Up and Boogie\", \"Do It Any Way You Wanna\", and \"Gonna Fly Now\"), though this definition is loose and subjective.\n\nFalling just outside of that definition is \"Theme From Shaft\" by Isaac Hayes.\n\n\n"}
{"id": "14968", "url": "https://en.wikipedia.org/wiki?curid=14968", "title": "Regular icosahedron", "text": "Regular icosahedron\n\nIn geometry, a regular icosahedron ( or ) is a convex polyhedron with 20 faces, 30 edges and 12 vertices. It is one of the five Platonic solids, and the one with the most sides.\n\nIt has five equilateral triangular faces meeting at each vertex. It is represented by its Schläfli symbol {3,5}, or sometimes by its vertex figure as 3.3.3.3.3 or 3. It is the dual of the dodecahedron, which is represented by {5,3}, having three pentagonal faces around each vertex.\n\nA regular icosahedron is a gyroelongated pentagonal bipyramid and a biaugmented pentagonal antiprism in any of six orientations.\n\nThe name comes . The plural can be either \"icosahedrons\" or \"icosahedra\" ().\n\nIf the edge length of a regular icosahedron is \"a\", the radius of a circumscribed sphere (one that touches the icosahedron at all vertices) is\n\nand the radius of an inscribed sphere (tangent to each of the icosahedron's faces) is\n\nwhile the midradius, which touches the middle of each edge, is\n\nwhere \"ϕ\" is the golden ratio.\n\nThe surface area \"A\" and the volume \"V\" of a regular icosahedron of edge length \"a\" are:\n\nThe latter is \"F\" = \"20\" times the volume of a general tetrahedron with apex at the center of the\ninscribed sphere, where the volume of the tetrahedron is one third times the base area times its height \"r\".\n\nThe volume filling factor of the circumscribed sphere is:\n\nThe vertices of an icosahedron centered at the origin with an edge-length of 2 and a circumradius of formula_7 are described by circular permutations of:\nwhere \"ϕ\" =  is the golden ratio.\n\nTaking all permutations (not just cyclic ones) results in the Compound of two icosahedra.\n\nNote that these vertices form five sets of three concentric, mutually orthogonal golden rectangles, whose edges form Borromean rings.\n\nIf the original icosahedron has edge length 1, its dual dodecahedron has edge length = = \"ϕ\" − 1.\nThe 12 edges of a regular octahedron can be subdivided in the golden ratio so that the resulting vertices define a regular icosahedron. This is done by first placing vectors along the octahedron's edges such that each face is bounded by a cycle, then similarly subdividing each edge into the golden mean along the direction of its vector. The five octahedra defining any given icosahedron form a regular polyhedral compound, while the two icosahedra that can be defined in this way from any given octahedron form a uniform polyhedron compound.\nThe locations of the vertices of a regular icosahedron can be described using spherical coordinates, for instance as latitude and longitude. If two vertices are taken to be at the north and south poles (latitude ±90°), then the other ten vertices are at latitude ±arctan() ≈ ±26.57°. These ten vertices are at evenly spaced longitudes (36° apart), alternating between north and south latitudes.\n\nThis scheme takes advantage of the fact that the regular icosahedron is a pentagonal gyroelongated bipyramid, with D dihedral symmetry—that is, it is formed of two congruent pentagonal pyramids joined by a pentagonal antiprism.\n\nThe icosahedron has three special orthogonal projections, centered on a face, an edge and a vertex:\nThe icosahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.\n\nThe following construction of the icosahedron avoids tedious computations in the number field [] necessary in more elementary approaches.\n\nThe existence of the icosahedron amounts to the existence of six equiangular lines in . Indeed, intersecting such a system of equiangular lines with a Euclidean sphere centered at their common intersection yields the twelve vertices of a regular icosahedron as can easily be checked. Conversely, supposing the existence of a regular icosahedron, lines defined by its six pairs of opposite vertices form an equiangular system.\n\nIn order to construct such an equiangular system, we start with this 6 × 6 square matrix:\n\nA straightforward computation yields (where \"\" is the 6 × 6 identity matrix). This implies that \"A\" has eigenvalues – and , both with multiplicity 3 since \"A\" is symmetric and of trace zero.\n\nThe matrix induces thus a Euclidean structure on the quotient space , which is isomorphic to since the kernel of has dimension 3. The image under the projection of the six coordinate axes \"v\", …, \"v\" in forms thus a system of six equiangular lines in intersecting pairwise at a common acute angle of arccos . Orthogonal projection of ±\"v\", …, ±\"v\" onto the -eigenspace of \"A\" yields thus the twelve vertices of the icosahedron.\n\nA second straightforward construction of the icosahedron uses representation theory of the alternating group \"A\" acting by direct isometries on the icosahedron.\n\nThe rotational symmetry group of the regular icosahedron is isomorphic to the alternating group on five letters. This non-abelian simple group is the only non-trivial normal subgroup of the symmetric group on five letters. Since the Galois group of the general quintic equation is isomorphic to the symmetric group on five letters, and this normal subgroup is simple and non-abelian, the general quintic equation does not have a solution in radicals. The proof of the Abel–Ruffini theorem uses this simple fact, and Felix Klein wrote a book that made use of the theory of icosahedral symmetries to derive an analytical solution to the general quintic equation, . See icosahedral symmetry: related geometries for further history, and related symmetries on seven and eleven letters.\n\nThe full symmetry group of the icosahedron (including reflections) is known as the full icosahedral group, and is isomorphic to the product of the rotational symmetry group and the group \"C\" of size two, which is generated by the reflection through the center of the icosahedron.\n\nThe icosahedron has a large number of stellations. According to specific rules defined in the book \"The Fifty-Nine Icosahedra\", 59 stellations were identified for the regular icosahedron. The first form is the icosahedron itself. One is a regular Kepler–Poinsot polyhedron. Three are regular compound polyhedra.\n\nThe small stellated dodecahedron, great dodecahedron, and great icosahedron are three facetings of the regular icosahedron. They share the same vertex arrangement. They all have 30 edges. The regular icosahedron and great dodecahedron share the same edge arrangement but differ in faces (triangles vs pentagons), as do the small stellated dodecahedron and great icosahedron (pentagrams vs triangles).\nThere are distortions of the icosahedron that, while no longer regular, are nevertheless vertex-uniform. These are invariant under the same rotations as the tetrahedron, and are somewhat analogous to the snub cube and snub dodecahedron, including some forms which are chiral and some with T-symmetry, i.e. have different planes of symmetry from the tetrahedron.\n\nThe icosahedron is unique among the Platonic solids in possessing a dihedral angle not less than 120°. Its dihedral angle is approximately 138.19°. Thus, just as hexagons have angles not less than 120° and cannot be used as the faces of a convex regular polyhedron because such a construction would not meet the requirement that at least three faces meet at a vertex and leave a positive defect for folding in three dimensions, icosahedra cannot be used as the cells of a convex regular polychoron because, similarly, at least three cells must meet at an edge and leave a positive defect for folding in four dimensions (in general for a convex polytope in \"n\" dimensions, at least three facets must meet at a peak and leave a positive defect for folding in \"n\"-space). However, when combined with suitable cells having smaller dihedral angles, icosahedra can be used as cells in semi-regular polychora (for example the snub 24-cell), just as hexagons can be used as faces in semi-regular polyhedra (for example the truncated icosahedron). Finally, non-convex polytopes do not carry the same strict requirements as convex polytopes, and icosahedra are indeed the cells of the icosahedral 120-cell, one of the ten non-convex regular polychora.\n\nAn icosahedron can also be called a gyroelongated pentagonal bipyramid. It can be decomposed into a gyroelongated pentagonal pyramid and a pentagonal pyramid or into a pentagonal antiprism and two equal pentagonal pyramids.\n\nIt can be projected to 3D from the 6D 6-demicube using the same basis vectors that form the hull of the Rhombic triacontahedron from the 6-cube. Shown here including the inner 20 vertices which are not connected by the 30 outer hull edges of 6D norm length . The inner vertices form an dodecahedron.\n\n<br>The 3D projection basis vectors [u,v,w] used are:\n\nThere are 3 uniform colorings of the icosahedron. These colorings can be represented as 11213, 11212, 11111, naming the 5 triangular faces around each vertex by their color.\n\nThe icosahedron can be considered a snub tetrahedron, as snubification of a regular tetrahedron gives a regular icosahedron having chiral tetrahedral symmetry. It can also be constructed as an alternated truncated octahedron, having pyritohedral symmetry. The pyritohedral symmetry version is sometimes called a pseudoicosahedron, and is dual to the pyritohedron.\n\nMany viruses, e.g. herpes virus, have icosahedral shells. Viral structures are built of repeated identical protein subunits known as capsomeres, and the icosahedron is the easiest shape to assemble using these subunits. A \"regular\" polyhedron is used because it can be built from a single basic unit protein used over and over again; this saves space in the viral genome.\n\nVarious bacterial organelles with an icosahedral shape were also found. The icosahedral shell encapsulating enzymes and labile intermediates are built of different types of proteins with BMC domains.\n\nIn 1904, Ernst Haeckel described a number of species of Radiolaria, including \"Circogonia icosahedra\", whose skeleton is shaped like a regular icosahedron. A copy of Haeckel's illustration for this radiolarian appears in the article on regular polyhedra.\n\nThe closo-carboranes are chemical compounds with shape very close to icosahedron. Icosahedral twinning also occurs in crystals, especially nanoparticles.\n\nMany borides and allotropes of boron contain boron B icosahedron as a basic structure unit.\n\nIcosahedral dice with twenty sides have been used since ancient times.\n\nIn several roleplaying games, such as \"Dungeons & Dragons\", the twenty-sided die (d20 for short) is commonly used in determining success or failure of an action. This die is in the form of a regular icosahedron. It may be numbered from \"0\" to \"9\" twice (in which form it usually serves as a ten-sided die, or d10), but most modern versions are labeled from \"1\" to \"20\". See d20 System.\n\nAn icosahedron is the three-dimensional game board for Icosagame, formerly known as the Ico Crystal Game.\n\nAn icosahedron is used in the board game \"Scattergories\" to choose a letter of the alphabet. Six letters are omitted (Q, U, V, X, Y, and Z).\n\nIn the \"Nintendo 64\" game \"\", the boss Miracle Matter is a regular icosahedron.\n\nInside a Magic 8-Ball, various answers to yes-no questions are inscribed on a regular icosahedron.\n\nR. Buckminster Fuller and Japanese cartographer Shoji Sadao designed a world map in the form of an unfolded icosahedron, called the Fuller projection, whose maximum distortion is only 2%. The American electronic music duo ODESZA use a regular icosahedron as their logo.\n\nThe skeleton of the icosahedron (the vertices and edges) forms a graph. It is one of 5 Platonic graphs, each a skeleton of its Platonic solid.\n\nThe high degree of symmetry of the polygon is replicated in the properties of this graph, which is distance-transitive and symmetric. The automorphism group has order 120. The vertices can be colored with 4 colors, the edges with 5 colors, and the diameter is 3.\n\nThe icosahedral graph is Hamiltonian: there is a cycle containing all the vertices. It is also a planar graph.\n\nThere are 4 related Johnson solids, including pentagonal faces with a subset of the 12 vertices. The similar dissected regular icosahedron has 2 adjacent vertices diminished, leaving two trapezoidal faces, and a bifastigium has 2 opposite sets of vertices removed and 4 trapezoidal faces. The pentagonal antiprism is formed by removing two opposite vertices.\n\nThe icosahedron can be transformed by a truncation sequence into its dual, the dodecahedron:\nAs a snub tetrahedron, and alternation of a truncated octahedron it also exists in the tetrahedral and octahedral symmetry families:\n\nThis polyhedron is topologically related as a part of sequence of regular polyhedra with Schläfli symbols {3,\"n\"}, continuing into the hyperbolic plane.\nThe regular icosahedron, seen as a \"snub tetrahedron\", is a member of a sequence of snubbed polyhedra and tilings with vertex figure (3.3.3.3.\"n\") and Coxeter–Dynkin diagram . These figures and their duals have (\"n\"32) rotational symmetry, being in the Euclidean plane for \"n\" = 6, and hyperbolic plane for any higher \"n\". The series can be considered to begin with \"n\" = 2, with one set of faces degenerated into digons.\n\nThe icosahedron can tessellate hyperbolic space in the order-3 icosahedral honeycomb, with 3 icosahedra around each edge, 12 icosahedra around each vertex, with Schläfli symbol {3,5,3}. It is one of four regular tessellations in the hyperbolic 3-space.\n\n"}
{"id": "14971", "url": "https://en.wikipedia.org/wiki?curid=14971", "title": "Industrial archaeology of Dartmoor", "text": "Industrial archaeology of Dartmoor\n\nThe industrial archaeology of Dartmoor covers a number of the industries which have, over the ages, taken place on Dartmoor, and the remaining evidence surrounding them. Currently only three industries are economically significant, yet all three will inevitably leave their own traces on the moor: china clay mining, farming and tourism.\n\nA good general guide to the commercial activities on Dartmoor at the end of the 19th century is William Crossing's \"The Dartmoor Worker\".\n\nIn former times, lead, silver, tin and copper were mined extensively on Dartmoor. The most obvious evidence of mining to the casual visitor to Dartmoor are the remains of the old engine-house at Wheal Betsy which is alongside the A386 road between Tavistock and Okehampton. The word \"Wheal\" has a particular meaning in Devon and Cornwall being either a tin or a copper mine, however in the case of Wheal Betsy it was principally lead and silver which were mined.\n\nOnce widely practised by many miners across the moor, by the early 1900s only a few tinners remained, and mining had almost completely ceased twenty years later. Some of the more significant mines were Eylesbarrow, Knock Mine, Vitifer Mine and Hexworthy Mine. The last active mine in the Dartmoor area was Great Rock Mine, which shut down in 1969.\n\nDartmoor granite has been used in many Devon and Cornish buildings. The prison at Princetown was built from granite taken from Walkhampton Common. When the horse tramroad from Plymouth to Princetown was completed in 1823, large quantities of granite were more easily transported.\n\nThere were three major granite quarries on the moor: Haytor, Foggintor and Merrivale. The granite quarries around Haytor were the source of the stone used in several famous structures, including the New London Bridge, completed in 1831. This granite was transported from the moor via the Haytor Granite Tramway, stretches of which are still visible.\n\nThe extensive quarries at Foggintor provided granite for the construction of London's Nelson's Column in the early 1840s, and New Scotland Yard was faced with granite from the quarry at Merrivale. Merrivale Quarry continued excavating and working its own granite until the 1970s, producing gravestones and agricultural rollers. Work at Merrivale continued until the 1990s, for the last 20 years imported stone such as gabbro from Norway and Italian marble was dressed and polished. The unusual pink granite at Great Trowlesworthy Tor was also quarried, and there were many other small granite quarries dotted around the moor. Various metamorphic rocks were also quarried in the metamorphic aureole around the edge of the moor, most notably at Meldon.\n\nIn 1844 a factory for making gunpowder was built on the open moor, not far from Postbridge. Gunpowder was needed for the tin mines and granite quarries then in operation on the moor. The buildings were widely spaced from one another for safety and the mechanical power for grinding (\"incorporating\") the powder was derived from waterwheels driven by a leat.\n\nNow known as \"Powdermills\" or \"Powder Mills\", there are extensive remains of this factory still visible. Two chimneys still stand and the walls of the two sturdily-built incorporating mills with central waterwheels survive well: they were built with substantial walls but flimsy roofs so that in the event of an explosion, the force of the blast would be directed safely upwards. The ruins of a number of ancillary buildings also survive. A proving mortar—a type of small cannon used to gauge the strength of the gunpowder—used by the factory still lies by the side of the road to the nearby pottery.\n\nPeat-cutting for fuel occurred at some locations on Dartmoor until certainly the 1970s, usually for personal use. The right of Dartmoor commoners to cut peat for fuel is known as \"turbary\". These rights were conferred a long time ago, pre-dating most written records. The area once known as the \"Turbary of Alberysheved\" between the River Teign and the headwaters of the River Bovey is mentioned in the Perambulation of the Forest of Dartmoor of 1240 (by 1609 the name of the area had changed to Turf Hill).\n\nAn attempt was made to commercialise the cutting of peat in 1901 at Rattle Brook Head, however this quickly failed.\n\nFrom at least the 13th century until early in the 20th, rabbits were kept on a commercial scale, both for their flesh and their fur. Documentary evidence for this exists in place names such as Trowlesworthy Warren (mentioned in a document dated 1272) and Warren House Inn. The physical evidence, in the form of pillow mounds is also plentiful, for example there are 50 pillow mounds at Legis Tor Warren. The sophistication of the warreners is shown by the existence of vermin traps that were placed near the warrens to capture weasels and stoats attempting to get at the rabbits.\n\nThe significance of the term \"warren\" nowadays is not what it once was. In the Middle Ages it was a privileged place, and the creatures of the warren were protected by the king 'for his princely delight and pleasure'.\n\nThe subject of warrening on Dartmoor was addressed in Eden Phillpotts' story \"The River\".\n\nFarming has been practised on Dartmoor since time immemorial. The dry-stone walls which separate fields and mark boundaries give an idea of the extent to which the landscape has been shaped by farming. There is little or no arable farming within the moor, mostly being given over to livestock farming on account of the thin and rocky soil. Some Dartmoor farms are remote in the extreme.\n\n\n"}
{"id": "14972", "url": "https://en.wikipedia.org/wiki?curid=14972", "title": "Idempotence", "text": "Idempotence\n\nIdempotence (, ) is the property of certain operations in mathematics and computer science whereby they can be applied multiple times without changing the result beyond the initial application. The concept of idempotence arises in a number of places in abstract algebra (in particular, in the theory of projectors and closure operators) and functional programming (in which it is connected to the property of referential transparency).\n\nThe term was introduced by Benjamin Peirce in the context of elements of algebras that remain invariant when raised to a positive integer power, and literally means \"(the quality of having) the same power\", from + \"potence\" (same + power).\n\nAn element \"x\" of a magma (\"M\", •) is said to be \"idempotent\" if:\nIf all elements are idempotent with respect to •, then • is called idempotent.\nThe formula ∀\"x\", is called the idempotency law for •.\n\n\n\nIn the monoid (\"F\", ∘) of the functions from a set \"E\" to a subset \"F\" of \"E\" with the function composition ∘, idempotent elements are the functions such that , in other words such that for all \"x\" in \"E\", (the image of each element in \"E\" is a fixed point of \"f\"). For example:\nOther examples include:\n\nIf the set \"E\" has \"n\" elements, we can partition it into \"k\" chosen fixed points and non-fixed points under \"f\", and then \"k\" is the number of different idempotent functions. Hence, taking into account all possible partitions,\nis the total number of possible idempotent functions on the set. The integer sequence of the number of idempotent functions as given by the sum above for \"n\" = 0, 1, 2, 3, 4, 5, 6, 7, 8, … starts with 1, 1, 3, 10, 41, 196, 1057, 6322, 41393, … .\n\nNeither the property of being idempotent nor that of being not is preserved under function composition. As an example for the former, mod 3 and \"g\"(\"x\") = max(\"x\", 5) are both idempotent, but is not, although happens to be. As an example for the latter, the negation function ¬ on the Boolean domain is not idempotent, but is. Similarly, unary negation of real numbers is not idempotent, but is.\n\nIn computer science, the term \"idempotence\" may have a different meaning depending on the context in which it is applied:\n\nThis is a very useful property in many situations, as it means that an operation can be repeated or retried as often as necessary without causing unintended effects. With non-idempotent operations, the algorithm may have to keep track of whether the operation was already performed or not.\n\nA function looking up a customer's name and address in a database is typically idempotent, since this will not cause the database to change. Similarly, changing a customer's address is typically idempotent, because the final address will be the same no matter how many times it is submitted. However, placing an order for a car for the customer is typically not idempotent, since running the call several times will lead to several orders being placed. Canceling an order is idempotent, because the order remains canceled no matter how many requests are made.\n\nA composition of idempotent methods or subroutines, however, is not necessarily idempotent if a later method in the sequence changes a value that an earlier method depends on – \"idempotence is not closed under composition\". For example, suppose the initial value of a variable is 3 and there is a sequence that reads the variable, then changes it to 5, and then reads it again. Each step in the sequence is idempotent: both steps reading the variable have no side effects and changing a variable to 5 will always have the same effect no matter how many times it is executed. Nonetheless, executing the entire sequence once produces the output (3, 5), but executing it a second time produces the output (5, 5), so the sequence is not idempotent.\n\nIn the Hypertext Transfer Protocol (HTTP), idempotence and safety are the major attributes that separate HTTP verbs. Of the major HTTP verbs, GET, PUT, and DELETE should be implemented in an idempotent manner according to the standard, but POST need not be. GET retrieves a resource; PUT stores content at a resource; and DELETE eliminates a resource. As in the example above, reading data usually has no side effects, so it is idempotent (in fact \"nullipotent\"). Storing and deleting a given set of content are each usually idempotent as long as the request specifies a location or identifier that uniquely identifies that resource and only that resource again in the future. The PUT and DELETE operations with unique identifiers reduce to the simple case of assignment to an immutable variable of either a value or the null-value, respectively, and are idempotent for the same reason; the end result is always the same as the result of the initial execution, even if the response differs.\n\nViolation of the unique identification requirement in storage or deletion typically causes violation of idempotence. For example, storing or deleting a given set of content without specifying a unique identifier: POST requests, which do not need to be idempotent, often do not contain unique identifiers, so the creation of the identifier is delegated to the receiving system which then creates a corresponding new record. Similarly, PUT and DELETE requests with nonspecific criteria may result in different outcomes depending on the state of the system - for example, a request to delete the most recent record. In each case, subsequent executions will further modify the state of the system, so they are not idempotent. \n\nIn Event stream processing, idempotence refers to the ability of a system to produce the same outcome, even if the same file, event or message is received more than once.\n\nIn a load-store architecture, instructions that might possibly cause a page fault are idempotent. So if a page fault occurs, the OS can load the page from disk and then simply re-execute the faulted instruction. In a processor where such instructions are not idempotent, dealing with page faults is much more complex.\n\nWhen reformatting output, pretty-printing is expected to be idempotent. In other words, if the output is already \"pretty\", there should be nothing to do for the pretty-printer.\n\nApplied examples that many people could encounter in their day-to-day lives include elevator call buttons and crosswalk buttons. The initial activation of the button moves the system into a requesting state, until the request is satisfied. Subsequent activations of the button between the initial activation and the request being satisfied have no effect, unless the system is designed to adjust the time for satisfying the request based on the number of activations.\n\n\n"}
{"id": "14973", "url": "https://en.wikipedia.org/wiki?curid=14973", "title": "Ithaca, New York", "text": "Ithaca, New York\n\nIthaca is a city in the Finger Lakes region of New York. It is the seat of Tompkins County, as well as the largest community in the Ithaca–Tompkins County metropolitan area. This area contains the municipalities of the Town of Ithaca, the village of Cayuga Heights, and other towns and villages in Tompkins County. The city of Ithaca is located on the southern shore of Cayuga Lake, in Central New York, about south-west-west of Syracuse. It is named for the Greek island of Ithaca.\n\nIthaca is home to Cornell University, an Ivy League school of over 20,000 students, most of whom study at its local campus. Ithaca College a private, nonsectarian, liberal arts college of over 7,000 students is located just south of the city in the Town of Ithaca, adding to the area's \"college town\" atmosphere. Nearby is Tompkins Cortland Community College (TC3). These three colleges bring tens of thousands of students, who increase Ithaca's seasonal population during the school year. Some students settle in the area after graduation. The city's voters are notably more liberal than those in the remainder of Tompkins County or in upstate New York, generally voting for Democratic Party candidates.\n\nAs of 2010, the city's population was 30,014. A 2017 census estimate stated the population was 31,006.\n\nNamgyal Monastery in Ithaca is the North American seat of Tenzin Gyatso, the 14th Dalai Lama.\n\nIndigenous people occupied this area for thousands of years. At the time of European contact, this area was controlled by the Cayuga Indians, one of the powerful Five Nations of the \"Haudenosaunee\" or Iroquois League. Jesuit missionaries from New France (Quebec) are said to have had a mission to the Cayuga as early as 1657.\n\nSaponi and Tutelo Indians, Siouan-speaking tribes, later occupied lands at the south end of Cayuga Lake. Dependent tributaries of the Cayuga, they had been permitted to settle on the tribe's hunting lands at the south end of Cayuga Lake, as well as in Pony (originally Sapony) Hollow of what is known as present-day Newfield, New York. Remnants of these tribes had been forced from Virginia and North Carolina by tribal conflicts and European colonial encroachment. Similarly, the Tuscarora people, an Iroquoian-speaking tribe from the Carolinas, migrated after defeat in the Yamasee War; they settled with the Oneida people and became the sixth nation of the Haudenosaunee, with chiefs stating the migration was complete in 1722.\nDuring the Revolutionary War, four of the then six Iroquois nations were allied with the British, although bands made decisions on fighting in a highly decentralized way. Conflict with the rebel colonists was fierce throughout the Mohawk Valley and western New York. In retaliation for conflicts to the east, the 1779 Sullivan Expedition was conducted against the Iroquois peoples in the west of the state, destroying more than 40 villages and stored winter crops. It destroyed the Tutelo village of Coregonal, located near what is now the junction of state routes 13 and 13A just south of the Ithaca city limits. Most Iroquois were forced from the state after the Revolutionary War, but some remnants remained. The state sold off the former Iroquois lands to stimulate development and settlement by European Americans; lands were also granted as payment to veterans of the war.\n\nWithin the current boundaries of the City of Ithaca, Native Americans maintained only a temporary hunting camp at the base of Cascadilla Gorge. In 1788, eleven men from Kingston, New York came to the area with two Delaware people (Lenape) guides, to explore what they considered wilderness. The following year Jacob Yaple, Isaac Dumond, and Peter Hinepaw returned with their families and constructed log cabins. That same year Abraham Bloodgood of Albany obtained a patent from the state for 1,400 acres, which included all of the present downtown west of Tioga Street. In 1790, the federal government and state began an official program to grant land in the area, known as the Central New York Military Tract, as payment for service to the American soldiers of the Revolutionary War, as the government was cash poor. Most local land titles trace back to these Revolutionary war grants.\n\nAs part of this process, the Central New York Military Tract, which included northern Tompkins County, was surveyed by Simeon De Witt, Bloodgood's son-in-law. De Witt was also the nephew of Governor George Clinton. The Commissioners of Lands of New York State (chairman Gov. George Clinton) met in 1790. The Military Tract township in which proto-Ithaca was located was named the Town of Ulysses. A few years later De Witt moved to Ithaca, then called variously \"The Flats,\" \"The City,\" or \"Sodom\"; he renamed it for the Greek island home of Ulysses in the spirit of the multitude of settlement names in the region derived from classical literature, such as Aurelius, Ovid, and especially of Ulysses, New York, the town that contained Ithaca at the time.\n\nAround 1791 De Witt surveyed what is now the downtown area into lots and sold them at modest prices. That same year John Yaple built a grist mill on Cascadilla Creek. The first frame house was erected in 1800 by Abram Markle. In 1804 the village had a postmaster, and in 1805 a tavern.\n\nIthaca became a transshipping point for salt from curing beds near Salina, New York to buyers south and east. This prompted construction in 1810 of the Owego Turnpike. When the War of 1812 cut off access to Nova Scotia gypsum, used for fertilizer, Ithaca became the center of trade in Cayuga gypsum. The Cayuga Steamboat Company was organized in 1819 and in 1820 launched the first steamboat on Cayuga Lake, the \"Enterprise.\" In 1821, the village was incorporated at the same time the Town of Ithaca was organized and separated from the parent Town of Ulysses. In 1834, the Ithaca and Owego Railroad's first horse-drawn train began service, connecting traffic on the east-west Erie Canal (completed in 1825) with the Susquehanna River to the south to expand the trade network.\n\nWith the depression of 1837, the railroad was re-organized as the Cayuga & Susquehanna. It was re-engineered with switchbacks in the late 1840s; in the late 20th century a short section of this route in the city and town of Ithaca was used for the South Hill Recreation Way.\n\nHowever, easier railroad routes were constructed, such as that of the Syracuse, Binghamton & New York (1854). In the decade following the Civil War, railroads were built from Ithaca to surrounding points (Geneva; Cayuga; Cortland; and Elmira, New York; and Athens, Pennsylvania), mainly with financing from Ezra Cornell. The geography of the city, on a steep hill by the lake, has prevented it from being directly connected to a major transportation artery. When the Lehigh Valley Railroad built its main line from Pennsylvania to Buffalo, New York in 1890, it bypassed Ithaca (running via eastern Schuyler County on easier grades), as the Delaware, Lackawanna and Western Railroad had done in the 1850s.\nIn the late 19th century, more industry developed in Ithaca. In 1883 William Henry Baker and his partners started the Ithaca Gun Company, making shotguns. The original factory was located in the Fall Creek neighborhood of the city, on a slope later known as Gun Hill, where the nearby waterfall supplied the main source of energy for the plant. The company became an icon in the hunting and shooting world, its shotguns famous for their fine decorative work. Wooden gunstocks with knots or other imperfections were donated to the high school woodworking shop to be made into lamps. John Philip Sousa and trick-shooter Annie Oakley favored Ithaca guns. In 1937 the company began producing the Ithaca 37, based on a 1915 patent by noted firearms designer John Browning. Its 12-gauge shotguns were the standard used for decades by the New York Police Department and Los Angeles Police Department.\n\nIn 1885, Ithaca Children's Home was established on West Seneca Street. The orphanage had two programs at the time: a residential home for both orphaned and destitute children, and a day nursery. The village established its first trolley in 1887. Ithaca developed as a small manufacturing and retail center and was incorporated as a city in 1888. The largest industrial company in the area was Morse Chain, elements of which were absorbed into Emerson Power Transmission on South Hill and Borg Warner Automotive in Lansing, New York.\n\nIthaca claims to be the birthplace of the ice cream sundae, created in 1892 when fountain shop owner Chester Platt \"served his local priest vanilla ice cream covered in cherry syrup with a dark candied cherry on top. The priest suggested the dessert be named after the day, Sunday—although the spelling was later changed out of fear some would find it offensive.\" The local Unitarian church, where the priest, Rev. John Scott, preached, has an annual \"Sundae Sunday\" every September in commemoration. Ithaca's claim has long been disputed by Two Rivers, Wisconsin. Also in 1892, the Ithaca Kitty became one of the first mass-produced stuffed animal toys in the United States.\nIn 1903 a typhoid epidemic, resulting from poor sanitation infrastructure, devastated the city. One out of 10 citizens fell ill or died.\n\nIn 1900 Cornell anatomy professor G.S. Moler made an early movie using frame-by-frame technology. For \"The Skeleton Dance,\" he took single-frame photos of a human skeleton in varying positions, giving the illusion of a dancing skeleton. During the early 20th century, Ithaca was an important center in the silent film industry. These films often featured the local natural scenery. Many of these films were the work of Leopold Wharton and his brother Theodore; their studio was on the site of what is now Stewart Park.\n\nThe Star Theatre on East Seneca Street was built in 1911 and became the most popular vaudeville venue in the region. Wharton movies were also filmed and shown there. After the film industry centralized in Hollywood, production in Ithaca effectively ceased. Few of the silent films made in Ithaca have been preserved.\n\nAfter World War II, the Langmuir Research Labs of General Electric developed as a major employer; the defense industry continued to expand. GE's headquarters were in Schenectady, New York, to the east in the Mohawk Valley.\n\nFor decades, the Ithaca Gun Company tested their shotguns behind the plant on Lake Street; the shot fell into the Fall Creek gorge at the base of Ithaca Falls. Lead accumulated in the soil in and around the factory and gorge. A major lead clean-up effort sponsored by the United States Superfund took place from 2002 to 2004, managed through the Environmental Protection Agency. The old Ithaca Gun building has been dismantled. It was scheduled to be replaced by development of an apartment complex on the cleaned land.\n\nThe former Morse Chain company factory on South Hill, now owned by Emerson Power Transmission, was the site of extensive groundwater and soil contamination from its industrial operations. Emerson Power Transmission has been working with the state and South Hill residents to determine the extent and danger of the contamination and aid in cleanup.\n\nIn 2004, Gayraud Townsend, a 20-year-old senior in Cornell's School of Industrial and Labor Relations, was sworn in as alderman of the city council, the first black male to be elected to the council and the youngest African American to be elected to office in the United States. He served his full term and has mentored other student politicians. In 2011 Cornell Class of 2009 graduate Svante Myrick was elected as the youngest mayor of the city of Ithaca.\n\nThe valley in which Cayuga Lake is located is long and narrow with a north-south orientation. Ithaca is at the southern end (the \"head\") of the lake, but the valley continues to the southwest behind the city. Originally a river valley, it was deepened and widened by the action of Pleistocene ice sheets over the last several hundred thousand years. The lake, which drains to the north, formed behind a dam of glacial moraine. The rock is predominantly Devonian and, north of Ithaca, is relatively fossil rich. Glacial erratics can be found in the area. The world-renowned fossils found in this area can be examined at the Museum of the Earth.\n\nIthaca was founded on flat land just south of the lake—land that formed in fairly recent geological times when silt filled the southern end of the lake. The city ultimately spread to the adjacent hillsides, which rise several hundred feet above the central flats: East Hill, West Hill, and South Hill. Its sides are fairly steep, and a number of the streams that flow into the valley from east or west have cut deep canyons, usually with several waterfalls.\n\nThe natural vegetation of the Ithaca area, seen in areas unbuilt and unfarmed, is northern temperate broadleaf forest, dominated by deciduous trees.\n\nIthaca experiences a moderate continental climate. Winters are long, cold, and snowy, with temperatures reaching or lower on an average 9.9 nights annually and an average of of snow per season. The largest snowfall in one day was on February 14, 1914. Summers are warm and humid, with usually comfortable temperatures. Readings of or higher occur on an average of just 5.2 days per year, and + temperatures have only occurred ten times since record-keeping began in 1893. The average date of the first freeze is October 5, and the average date of the last freeze is May 15, giving Ithaca a growing season of 141 days. The average date of the first and last snowfalls are November 12 and April 7, respectively. Extreme temperatures range from as recently as February 2, 1961 up to on July 9, 1936.\n\nThe valley flatland has slightly milder weather in winter, and occasionally Ithacans experience simultaneous snow on the hills and rain in the valley. The phenomenon of mixed precipitation (rain, wind, and snow), common in the late fall and early spring, is known tongue-in-cheek as \"ithacation\" to many of the local residents.\n\nDue to the microclimates created by the impact of the lakes, the region surrounding Ithaca (Finger Lakes American Viticultural Area) experiences a short but adequate growing season for winemaking similar to the Rhine Valley wine district of Germany. As such, the region is home to many wineries.\nIthaca is the larger principal city of the Ithaca-Cortland CSA, a Combined Statistical Area that includes the Ithaca metropolitan area (Tompkins County) and the Cortland micropolitan area (Cortland County), which had a combined population of 145,100 at the 2000 census.\n\nAs of the census of 2000, there were 29,287 people, 10,287 households, and 2,962 families residing in the city. The population density was 5,360.9 people per square mile (2,071.0/km²). There were 10,736 housing units at an average density of 1,965.2 per square mile (759.2/km²). The racial makeup of the city was 73.97% White, 13.65% Asian, 6.71% Black or African American, 0.39% Native American, 0.05% Pacific Islander, 1.86% from other races, and 3.36% from two or more races. Hispanic or Latino of any race were 5.31% of the population.\n\nThere were 10,287 households out of which 14.2% had children under the age of 18 living with them, 19.0% were married couples living together, 7.8% had a female householder with no husband present, and 71.2% were non-families. 43.3% of all households were made up of individuals and 7.4% had someone living alone who was 65 years of age or older. The average household size was 2.13 and the average family size was 2.81.\n\nIn the city, the population was spread out with 9.2% under the age of 18, 53.8% from 18 to 24, 20.1% from 25 to 44, 10.6% from 45 to 64, and 6.3% who were 65 years of age or older. The median age was 22 years. For every 100 females, there were 102.6 males. For every 100 females age 18 and over, there were 102.2 males.\n\nThe median income for a household in the city was $21,441, and the median income for a family was $42,304. Males had a median income of $29,562 versus $27,828 for females. The per capita income for the city was $13,408. About 13.2% of individuals and 4.2% of families were below the poverty line.\n\nThe term \"Greater Ithaca\" encompasses both the City and Town of Ithaca, as well as several smaller settled places within or adjacent to the Town:\n\nMunicipalities\n\n\nCensus-designated places\n\nThere are two governmental entities in the area: the Town of Ithaca and the City of Ithaca. The Town of Ithaca is one of the nine towns comprising Tompkins County. The City of Ithaca is surrounded by, but legally independent of, the Town.\n\nThe City of Ithaca has a mayor-council government. The charter of the City of Ithaca provides for a full-time mayor and city judge, each independent and elected at-large. Since 1995, the mayor has been elected to a four-year term, and since 1989, the city judge has been elected to a six-year term.\n\nSince 1983, the city has been divided into five wards. Each elects two representatives to the city council, known as the Common Council, for staggered four-year terms. In March 2015, the Common Council unanimously adopted a resolution recognizing freedom from domestic violence as a fundamental human right.\n\nSince students won the right to vote where they attend colleges, some have become more active in local politics. In 2004, Gayraud Townsend, a 20-year-old senior in Cornell's School of Industrial and Labor Relations, was sworn in as alderman of the city council, representing the 4th Ward. He is the first black male to be elected to the council and was then the youngest African American to be elected to office in the United States. He served his full term and has mentored other young student politicians. In 2011, Cornell graduate Svante Myrick was elected Mayor of the City of Ithaca, becoming the youngest mayor in the city's history.\n\nIn December 2005, the City and Town governments began discussing opportunities for increased government consolidation, including the possibility of joining the two into a single entity. This topic had been previously discussed in 1963 and 1969. Cayuga Heights, a village adjacent to the city on its northeast, voted against annexation into the city of Ithaca in 1954.\n\nPolitically, the majority of city's voters (many of them students) have supported liberalism and the Democratic Party. A November 2004 study by ePodunk lists it as New York's most liberal city. This contrasts with the more conservative leanings of the generally rural Upstate New York region; the city's voters are also more liberal than those in the rest of Tompkins County. In 2008, Barack Obama, running against New York State's US Senator Hillary Clinton, won Tompkins County in the Democratic Presidential Primary, the only county that he won in New York State. Obama won Tompkins County (including Ithaca) by a wide margin of 41% over his opponent John McCain in the November 2008 election.\n\nThe two major postsecondary educational institutions located in Ithaca were each founded in the late nineteenth century. In 1865, Ezra Cornell founded Cornell University, which overlooks the town from East Hill. It was opened as a coeducational institution. Women first enrolled in 1870. Ezra Cornell also established a public library for the city. Ithaca College was founded as the Ithaca Conservatory of Music in 1892. Ithaca College was originally located in the downtown area, but relocated to South Hill in the 1960s.\n\nIthaca is a major educational center in Central New York. In 2011 there were about 21,000 students enrolled at Cornell and about 6,400 at Ithaca College. Tompkins Cortland Community College is located in the neighboring Town of Dryden, and has an extension center in downtown Ithaca. Empire State College offers non-traditional college courses to adults in downtown Ithaca.\n\nThe public school system is based in Ithaca. The Ithaca City School District, which encompasses Ithaca and the surrounding area, enrolls about 5,500 K-12 students in eight elementary schools, two middle schools, Ithaca High School, and the Lehman Alternative Community School. Several private elementary and secondary schools are located in the Ithaca area, including the Roman Catholic Immaculate Conception School, the Cascadilla School, the New Roots Charter School, the Elizabeth Ann Clune Montessori School, and the Ithaca Waldorf School. Ithaca has two networks for supporting its home-schooling families: Loving Education At Home (LEAH) and the Northern Light Learning Center (NLLC).\n\nThe economy of Ithaca is based on education, with agriculture, technology and tourism in supporting roles. As of 2006, Ithaca has continued to have one of the few expanding economies in New York State outside New York City. It draws commuters for work from the neighboring rural counties of Cortland, Tioga, and Schuyler, as well as from the more urbanized Chemung County.\n\nIthaca has tried to maintain its traditional downtown shopping area with its pedestrian orientation; this includes the Ithaca Commons pedestrian mall and Center Ithaca, a small mixed-use complex built at the end of the urban renewal era. Another commercial center, Collegetown, is located next to the Cornell campus. It features a number of restaurants, shops, and bars, and an increasing number of high-rise apartments. It is primarily frequented by Cornell University students.\n\nIthaca has many of the businesses characteristic of small American university towns: bookstores, art house cinemas, craft stores, and vegetarian-friendly restaurants. The collective Moosewood Restaurant, founded in 1973, published a number of vegetarian cookbooks. \"Bon Appetit\" magazine ranked it among the thirteen most influential restaurants of the 20th century. Ithaca has many local restaurants and chains both in the city and town with a range of ethnic foods. It has become a destination and residence for retirees.\n\nThe Ithaca Farmers Market, a cooperative with 150 vendors who live within 30 miles of Ithaca, first opened for business on Saturdays in 1973. It is located at Steamboat Landing, where steamboats from Cayuga Lake used to dock.\n\nThe South Hills Business Campus originally opened in 1957 as the regional headquarters of the National Cash Register Company. Running three full factory shifts, NCR was a major employer. Although it was sold in 1991 to American Telephone and Telegraph and later acquired by Cognitive TPG, TPG remains a major tenant of the South Hill Business Campus, which is now owned by a group of private investors.\n\nIthaca, home to Cornell University College of Agriculture and Life Sciences, has a deep connection to Central New York's farming and dairy industries. About 60 small farms are located in the greater Ithaca/Trumansburg area, including a number of research farms managed by the Cornell University Agricultural Experiment Station. Cornell's Dairy Research Facility is a center of research and support for New York's large and growing dairy and yogurt industries. \n\nThe \"Ithaca Journal was\" founded in 1815 and is a morning daily newspaper which has been owned by Gannett since 1912.\n\nIthaca is home to several radio stations:\n\nPublic radio:\nOther FM stations include: Saga's \"98.7 The Vine\", a low-powered translator station; WFIZ \"Z95.5\", airing a top-40 (CHR) format; contemporary Christian music station WCII 88.9; and classic rock \"The Wall\" WLLW 99.3 and 96.3, based in Seneca Falls with a transmitter in Ithaca.\n\nFounded in 1983, the Sciencenter, is a non-profit hands-on science museum, accredited by the American Alliance of Museums (AAM) and is a member of the Association of Science-Technology Centers (ASTC) and Association of Children's Museums (ACM).\n\nThe Museum of the Earth is a natural history museum created in 2003 by the Paleontological Research Institution (PRI). The PRI was founded in Ithaca in 1932 and is the publisher of the oldest journal of paleontology in the western hemisphere. Exhibits cover the 4.5 billion year history of the earth in an accessible manner, including interactive displays. As of 2004, the PRI is now formally affiliated with Cornell.\n\nThe Cayuga Nature Center occupies the site of the 1914 Cayuga Preventorium, a facility for children with tuberculosis; treatment of what was then considered an incurable disease was based on rest and good nutrition. In 1981, the Cayuga Nature Center was incorporated as an independent, private, non-profit educational organization, offering environmental education to local school districts. In 2011, the PRI merged with the Cayuga Nature Center, making it a sister organization to the Museum of the Earth.\n\nThe Cornell Lab of Ornithology is located in the Imogene Powers Johnson Center for Birds and Biodiversity. The Lab's Visitors' Center and observation areas are open to the public. Displays include a surround sound theater, object-theater presentation, sound studio, and informational kiosks featuring bird sounds and information.\n\nThe Herbert F. Johnson Museum of Art houses one of the finest collections of art in upstate New York. Special exhibitions are mounted each year, plus selections from a global permanent collection, which is displayed on six public floors. The collection includes art from throughout Asia, Africa, Europe, the Americas, graphic arts, medallic art, and Tiffany glass, ranging from the ancient to the contemporary.\n\nThe Center for the Arts at Ithaca, Inc., operates the \"Hangar Theatre\". Opened in 1975 in a renovated municipal airport hangar, the Hangar hosts a summer season and brings a range of theatre to regional audiences including students, producing a school tour and Artists-in-the-Schools programs.\nIthaca is also the home to Kitchen Theatre Company, a non-profit professional company with a theatre on West State Street; and Civic Ensemble, a creative collaborative ensemble staging emerging playwrights' work and community-based original productions.\n\nIthaca is noted for its annual community celebration, The Ithaca Festival. The Constance Saltonstall Foundation for the Arts provides grants and summer fellowships at the Saltonstall Arts Colony for New York State artists and writers. Ithaca also hosts one of the largest used-book sales in the United States.\n\nThe city and town also sponsor The Apple Festival in the fall, the Chili Fest in February, the Finger Lakes International Dragon Boat Festival in July; Porchfest in late September, and the Ithaca Brew Fest in Stewart Park in September.\n\nIthaca has also pioneered the Ithaca Health Fund, a popular cooperative health insurance. Ithaca is home to one of the United States' first local currency systems, Ithaca Hours, developed by Paul Glover.\n\nIthaca is the home of the Cayuga Chamber Orchestra.\n\nThe Cornell Concert Series has been hosting musicians and ensembles of international stature since 1903. For its initial 84 years, the series featured Western classical artists exclusively. In 1987, however, the series broke with tradition to present Ravi Shankar and has since grown to encompass a broader spectrum of the world's great musics. Now, it balances of a mix of Western classical music, traditions from around the world, jazz, and new musics in these genres. In a single season, Cornell Concert Series presents performers ranging from the Leipzig Tomanerchor and Danish Quartet to Simon Shaheen, Vida Guitar Quartet, and Eighth Blackbird.\n\nThe School of Music at Ithaca College was founded in 1892 by William Egbert as a music conservatory on Buffalo Street. Among the degree programs offered are those in Performance, Theory, Music Education, and Composition. Since 1941, the School of Music has been accredited by the National Association of Schools of Music.\n\nIthaca's Suzuki school, Ithaca Talent Education, provides musical training for children of all ages and also teacher training for undergraduate and graduate-level students. The Community School of Music and Art uses an extensive scholarship system to offer classes and lessons to any student, regardless of age, background, economic status, or artistic ability.\n\nA number of musicians call Ithaca home, most notably Samite of Uganda, The Burns Sisters, The Horse Flies, Johnny Dowd, Mary Lorson, cellist Hank Roberts, reggae band John Brown's Body, Kurt Riley, and X Ambassadors. Old-time music is a staple and folk music is featured weekly on WVBR-FM's \"Bound for Glory\", North America's longest-running live folk concert broadcast. The Finger Lakes GrassRoots Festival of Music and Dance, hosted by local band Donna the Buffalo, is held annually during the third week in July in the nearby village of Trumansburg, with more than 60 local, national and international acts.\n\nIthaca is the center of a thriving live music scene, featuring over 200 groups playing most genres of American popular music, the predominant genres being Folk, Rock, Blues, Jazz, and Country. There are over 80 live music venues within a 40-mile radius of the city, including cafes, pubs, clubs, and concert halls.\n\nIn 2009, the Ithaca metropolitan statistical area (MSA) ranked as the highest in the United States for percentage of commuters who walked to work (15.1 percent). In 2013, the Ithaca MSA ranked as the second lowest in the United States for percentage of commuters who traveled by private vehicle (68.7 percent). During the same year, 17.5 percent of commuters in the Ithaca MSA walked to work.\n\nIthaca is in the rural Finger Lakes region about northwest of New York City; the nearest larger cities, Binghamton and Syracuse, are an hour's drive away by car, Rochester and Scranton are two hours, Buffalo and Albany are three. New York City, Philadelphia, Toronto, and Ottawa are about four hours away.\n\nIthaca lies at over a half hour's drive from any interstate highway, and all car trips to Ithaca involve some driving on two-lane state rural highways. The city is at the convergence of many regional two-lane state highways: Routes 13, 13A, 34, 79, 89, 96, 96B, and 366. These are usually not congested except in Ithaca proper. However, Route 79 between the I-81 access at Whitney Point and Ithaca receives a significant amount of Ithaca-bound congestion right before Ithaca's colleges reopen after breaks.\n\nIn July 2008, a non-profit called Ithaca Carshare began a carsharing service in Ithaca. Ithaca Carshare has a fleet of vehicles shared by over 1500 members as of July 2015 and has become a popular service among both city residents and the college communities. Vehicles are located throughout Ithaca downtown and the two major institutions. With Ithaca Carshare as the first locally run carsharing organization in New York State, others have since launched in Buffalo, Albany, NY, and Syracuse.\n\nRideshare services to promote carpooling and vanpooling are operated by ZIMRIDE and VRIDE. A community mobility education program, Way2Go is operated by Cornell Cooperative Extension of Tompkins County. Way2Go's website provides consumer information and videos. Way2Go works collaboratively to help people save money, stress less, go green and improve mobility options. The 2-1-1 Tompkins/Cortland Help line connects people with services, including transportation, in the community, by telephone and web on a 24/7 basis. The information and referral service is operated by the Human Services Coalition of Tompkins County, Inc. Together, 2-1-1 Information and Referral and Way2Go are a one-call, one-click resource designed to mobility services information for Ithaca and throughout Tompkins County.\n\nAs a growing urban area, Ithaca is facing steady increases in levels of vehicular traffic on the city grid and on the state highways. Outlying areas have limited bus service, and many people consider a car essential. However, many consider Ithaca a walkable and bikeable community. One positive trend for the health of downtown Ithaca is the new wave of increasing urban density in and around the Ithaca Commons. Because the downtown area is the region's central business district, dense mixed-use development that includes housing may increase the proportion of people who can walk to work and recreation, and mitigate the likely increased pressure on already busy roads as Ithaca grows. The downtown area is also the area best served by frequent public transportation. Still, traffic congestion around the Commons is likely to progressively increase.\n\nThere is frequent intercity bus service by Greyhound Lines, New York Trailways, OurBus, and Shortline (Coach USA), particularly to Binghamton and New York City, with limited service to Rochester, Buffalo and Syracuse, and (via connections in Binghamton) to Utica and Albany. OurBus also connects Ithaca to Allentown, Pennsylvania, Philadelphia, and Washington, DC. The Greyhound bus station is the former Delaware, Lackawanna & Western railway station on Meadow Street between West State and West Seneca streets, a little over half a mile west of downtown Ithaca. Cornell University runs a premium campus to campus bus between its Ithaca campus and its medical school in Manhattan, New York City which is open to the public.\n\nIthaca is the center of an extensive bus public transportation network. TCAT, Inc (Tompkins Consolidated Area Transit, Inc.) is a not-for-profit corporation that provides public transportation for Tompkins County New York. TCAT was reorganized as a non-profit corporation in 2004 and is primarily supported locally by Cornell University, the City of Ithaca and Tompkins County. TCAT's ridership increased from 2.7 million in 2004 to 4.4 million in 2013. https://web.archive.org/web/20141027015210/http://www.tcatbus.com/files/all/tcat_2013_yearbook_-_final.pdf TCAT operates 33 routes, many running seven days a week. It has frequent service to downtown, Cornell, Ithaca College, and the Shops at Ithaca Mall in the neighboring Town of Lansing, but less frequent service to many residential and rural areas, including Trumansburg and Newfield. Chemung County Transit (C-TRAN) runs weekday commuter service from Chemung County to Ithaca. Cortland Transit runs commuter service to Cornell University. Tioga County Public Transit operates three routes to Ithaca and Cornell, but will cease operating on November 30, 2014.\n\nGADABOUT Transportation Services, Inc. provides demand-response paratransit service for seniors over 60 and people with disabilities. Ithaca Dispatch provides local and regional taxi service. In addition, Ithaca Airline Limousine and IthaCar Service connect to the local airports.\n\nIthaca is served by Ithaca Tompkins Regional Airport, located about three miles to the northeast of the city center. American Eagle offers flights to its hub at Philadelphia, operated by Piedmont Airlines using Embraer ERJ-145 airliners. Delta Connection provides service to its hub at Detroit Metro airport, operated by SkyWest Airlines using Bombardier CRJ-200 airliners. United Express offers three daily flights to Newark Liberty International Airport, operated by CommutAir using Embraer ERJ-145. Some residents choose to travel to Syracuse Hancock International Airport, Greater Binghamton Airport, Elmira-Corning Regional Airport or Greater Rochester International Airport for more airline service options.\n\nNorfolk Southern freight trains reach Ithaca from Sayre, Pennsylvania, mainly to deliver coal to AES Cayuga, a coal power plant (known as Milliken Station during NYSEG ownership) and haul out salt from the Cargill salt mine, both on the east shore of Cayuga Lake. There is no passenger rail service, although from the 1870s through the 1950s there were trains to Buffalo via Geneva, New York; to New York City via Wilkes-Barre, Pennsylvania (Lehigh Valley Railroad) and Scranton, Pennsylvania (DL&W); to Auburn, New York; and to the US northeast via Cortland, New York; service to Buffalo and New York City lasted until 1961. The Lehigh Valley's top New York City-Ithaca-Buffalo passenger train, \"The Black Diamond\", was optimistically publicized as 'The Handsomest Train in the World', perhaps to compensate for its roundabout route to Buffalo. It was named after the railroad's largest commodity, anthracite coal.\n\nIthaca was the fourth community in New York state with a street railway; streetcars ran from 1887 to summer 1935.\n\n\nIn addition to its liberal politics, Ithaca is commonly listed among the most culturally liberal of American small cities. The \"Utne Reader\" named Ithaca \"America's most enlightened town\" in 1997. According to ePodunk's Gay Index, Ithaca has a score of 231, versus a national average score of 100.\n\nLike many small college towns, Ithaca has also received accolades for having a high overall quality of life. In 2004, \"Cities Ranked and Rated\" named Ithaca the best \"emerging city\" to live in the United States. In 2006, the Internet realty website \"Relocate America\" named Ithaca the fourth best city in the country to relocate to. In July 2006, Ithaca was listed as one of the \"12 Hippest Hometowns for Vegetarians\" by \"VegNews Magazine\" and chosen by \"Mother Earth News\" as one of the \"12 Great Places You've Never Heard Of.\"\n\nIn 2012, the city was listed among the 10 best places to retire in the U.S. by U.S. News.\n\nIthaca was also ranked 13th among America's Best College Towns by \"Travel + Leisure\" in 2013 and ranked as the #1 Best College Town in America in the American Institute for Economic Research's 2013–2014 College Destination Index.\n\nIn its earliest years, during the frontier days, what is now Ithaca was briefly known by the names \"The Flats\" and \"Sodom,\" the name of the Biblical city of sin, due to its reputation as a town of \"notorious immorality\", a place of horse racing, gambling, profanity, Sabbath breaking, and readily available liquor. These names did not last long; Simeon De Witt renamed the town Ithaca in the early 19th century, though nearby Robert H. Treman State Park still contains Lucifer Falls. Today, Ithaca is primarily known for its growing wineries and microbreweries, live music, colleges, and small dairy farms.\n\n"}
{"id": "14975", "url": "https://en.wikipedia.org/wiki?curid=14975", "title": "Ivy League", "text": "Ivy League\n\nThe Ivy League is an American collegiate athletic conference comprising sports teams from eight private universities in the Northeastern United States. The term \"Ivy League\" is typically used to refer to those eight schools as a group of elite colleges beyond the sports context. The eight members are Brown University, Columbia University, Cornell University, Dartmouth College, Harvard University, the University of Pennsylvania, Princeton University, and Yale University. \"Ivy League\" has connotations of academic excellence, selectivity in admissions, and social elitism. \n\nWhile the term was in use as early as 1933, it became official only after the formation of the NCAA Division I athletic conference in 1954. Seven of the eight schools were founded during the colonial period (Cornell was founded in 1865), and thus account for seven of the nine Colonial Colleges chartered before the American Revolution.\n\nIvy League schools are generally viewed as some of the most prestigious, and are ranked among the best universities worldwide by \"U.S. News & World Report\". All eight universities place in the top fourteen of the 2019 \"U.S. News & World Report\" national university rankings, including four Ivies in the top three (Columbia and Yale are tied for third). In the 2019 \"U.S. News & World Report\" global university rankings, three Ivies rank in the top ten (Harvard – 1st, Columbia – 8th, and Princeton – 9th) and six in the top twenty. Undergraduate-focused Ivies such as Brown University and Dartmouth College rank 99th and 197th, respectively. \"U.S. News\" has named a member of the Ivy League as the best national university in each of the past 18 years ending with the 2018 rankings: Princeton eleven times, Harvard twice, and the two schools tied for first five times.\n\nUndergraduate enrollments range from about 4,000 to 14,000, making them larger than those of a typical private liberal arts college and smaller than a typical public state university. Total enrollments, including graduate students, range from approximately 6,400 at Dartmouth to over 20,000 at Columbia, Cornell, Harvard, and Penn. Ivy League financial endowments range from Brown's $3.5 billion to Harvard's $34.5 billion, the largest financial endowment of any academic institution in the world.\n\nThe Ivy League has drawn many comparisons to other elite grouping of universities in other nations such as Oxbridge and the Golden Triangle in the United Kingdom, C9 League in China, Group of Eight in Australia, and Imperial Universities in Japan. These counterparts are often referred to in the American media as the \"Ivy League\" of their respective nations. Additionally, groupings of schools use the \"Ivy\" nomenclature to denote a perceived comparability, such as American liberal arts colleges (Little Ivies), lesser known schools (Hidden Ivies), public universities (Public Ivies), and schools in the Southern United States (Southern Ivies). \n\nIvy League universities have some of the largest university financial endowments in the world, which allows the universities to provide many resources for their academic programs and research endeavors. , Harvard University has an endowment of $37.1 billion, the highest of any US university Additionally, each university receives millions of dollars in research grants and other subsidies from federal and state governments.\n\nStudents have long revered the ivied walls of older colleges. \"Planting the ivy\" was a customary class day ceremony at many colleges in the 1800s. In 1893, an alumnus told \"The Harvard Crimson\", \"In 1850, class day was placed upon the University Calendar. ... the custom of planting the ivy, while the ivy oration was delivered, arose about this time.\" At Penn, graduating seniors started the custom of planting ivy at a university building each spring in 1873 and that practice was formally designated as \"Ivy Day\" in 1874. Ivy planting ceremonies are reported for Yale, Simmons, Bryn Mawr and many others. Princeton's \"Ivy Club\" was founded in 1879.\n\nThe first usage of \"Ivy\" in reference to a group of colleges is from sportswriter Stanley Woodward (1895–1965).\n\nThe first known instance of the term \"Ivy League\" being used appeared in \"The Christian Science Monitor\" on February 7, 1935. Several sportswriters and other journalists used the term shortly later to refer to the older colleges, those along the northeastern seaboard of the United States, chiefly the nine institutions with origins dating from the colonial era, together with the United States Military Academy (West Point), the United States Naval Academy, and a few others. These schools were known for their long-standing traditions in intercollegiate athletics, often being the first schools to participate in such activities. However, at this time, none of these institutions made efforts to form an athletic league.\n\nA common folk etymology attributes the name to the Roman numeral for four (IV), asserting that there was such a sports league originally with four members. The \"Morris Dictionary of Word and Phrase Origins\" helped to perpetuate this belief. The supposed \"IV League\" was formed over a century ago and consisted of Harvard, Yale, Princeton, and a fourth school that varies depending on who is telling the story. However, it is clear that Harvard, Princeton, Yale and Columbia met on November 23, 1876 at the so-called Massasoit Convention to decide on uniform rules for the emerging game of American football, which rapidly spread.\n\nSeven out of the eight Ivy League schools were founded before the American Revolution; Cornell was founded just after the American Civil War. These seven were the primary colleges in the Northern and Middle Colonies, and their early faculties and founding boards were largely drawn from other Ivy League institutions. There were also some British graduates from the University of Cambridge, the University of Oxford, the University of St. Andrews, the University of Edinburgh, and elsewhere on their boards. Similarly, the founder of The College of William & Mary, in 1693, was a British graduate of the University of Edinburgh. Cornell provided Stanford University with its first president.\n\nThe influence of these institutions on the founding of other colleges and universities is notable. This included the Southern public college movement which blossomed in the decades surrounding the turn of the 19th century when Georgia, South Carolina, North Carolina and Virginia established what became the flagship universities for each of these states. In 1801, a majority of the first board of trustees for what became the University of South Carolina were Princeton alumni. They appointed Jonathan Maxcy, a Brown graduate, as the university's first president. Thomas Cooper, an Oxford alumnus and University of Pennsylvania faculty member, became the second president of the South Carolina college. The founders of the University of California came from Yale, hence the school colors of University of California are Yale Blue and California Gold.\n\nSome of the Ivy League schools have identifiable Protestant roots, while others were founded as non-sectarian schools. Church of England \"King's College\" broke up during the Revolution and was reformed as public nonsectarian Columbia College. In the early nineteenth century, the specific purpose of training Calvinist ministers was handed off to theological seminaries, but a denominational tone and such relics as compulsory chapel often lasted well into the twentieth century. Penn and Brown were officially founded as nonsectarian schools. Brown's charter promised no religious tests and \"full liberty of conscience\", but placed control in the hands of a board of twenty-two Baptists, five Quakers, four Congregationalists, and five Episcopalians. Cornell has been strongly nonsectarian from its founding.\n\n\"Ivy League\" is sometimes used as a way of referring to an elite class, even though institutions such as Cornell University were among the first in the United States to reject racial and gender discrimination in their admissions policies. This dates back to at least 1935. Novels and memoirs attest this sense, as a social elite; to some degree independent of the actual schools.\n\nAfter the Second World War, the present Ivy League institutions slowly widened their selection of their students. They had always had distinguished faculties; some of the first Americans with doctorates had taught for them; but they now decided that they could not both be world-class research institutions and be competitive in the highest ranks of American college sport; in addition, the schools experienced the scandals of any other big-time football programs, although more quietly.\n\nThe first formal athletic league involving eventual Ivy League schools (or any US colleges, for that matter) was created in 1870 with the formation of the Rowing Association of American Colleges. The RAAC hosted a de facto national championship in rowing during the period 1870–1894. In 1895, Cornell, Columbia, and Penn founded the Intercollegiate Rowing Association, which remains the oldest collegiate athletic organizing body in the US. To this day, the IRA Championship Regatta determines the national champion in rowing and all of the Ivies are regularly invited to compete.\n\nA basketball league was later created in 1902, when Columbia, Cornell, Harvard, Yale and Princeton formed the Eastern Intercollegiate Basketball League; they were later joined by Penn and Dartmouth.\n\nIn 1906, the organization that eventually became the National Collegiate Athletic Association was formed, primarily to formalize rules for the emerging sport of football. But of the 39 original member colleges in the NCAA, only two of them (Dartmouth and Penn) later became Ivies.\n\nIn February 1903, intercollegiate wrestling began when Yale accepted a challenge from Columbia, published in the Yale News. The dual meet took place prior to a basketball game hosted by Columbia and resulted in a tie. Two years later, Penn and Princeton also added wrestling teams, leading to the formation of the student-run Intercollegiate Wrestling Association, now the Eastern Intercollegiate Wrestling Association (EIWA), the first and oldest collegiate wrestling league in the US.\n\nIn 1930, Columbia, Cornell, Dartmouth, Penn, Princeton and Yale formed the Eastern Intercollegiate Baseball League; they were later joined by Harvard, Brown, Army and Navy.\n\nBefore the formal establishment of the Ivy League, there was an \"unwritten and unspoken agreement among certain Eastern colleges on athletic relations\". The earliest reference to the \"Ivy colleges\" came in 1933, when Stanley Woodward of the New York Herald Tribune used it to refer to the eight current members plus Army. In 1935, the Associated Press reported on an example of collaboration between the schools:\n\nDespite such collaboration, the universities did not seem to consider the formation of the league as imminent. Romeyn Berry, Cornell's manager of athletics, reported the situation in January 1936 as follows:\n\nWithin a year of this statement and having held month-long discussions about the proposal, on December 3, 1936, the idea of \"the formation of an Ivy League\" gained enough traction among the undergraduate bodies of the universities that the \"Columbia Daily Spectator\", \"The Cornell Daily Sun\", \"The Dartmouth\", \"The Harvard Crimson\", \"The Daily Pennsylvanian\", \"The Daily Princetonian\" and the \"Yale Daily News\" would simultaneously run an editorial entitled \"Now Is the Time\", encouraging the seven universities to form the league in an effort to preserve the ideals of athletics. Part of the editorial read as follows:\n\nThe Ivies have been competing in sports as long as intercollegiate sports have existed in the United States. Rowing teams from Harvard and Yale met in the first sporting event held between students of two U.S. colleges on Lake Winnipesaukee, New Hampshire, on August 3, 1852. Harvard's team, \"The Oneida\", won the race and was presented with trophy black walnut oars from then presidential nominee General Franklin Pierce.\nThe proposal did not succeed—on January 11, 1937, the athletic authorities at the schools rejected the \"possibility of a heptagonal league in football such as these institutions maintain in basketball, baseball and track.\" However, they noted that the league \"has such promising possibilities that it may not be dismissed and must be the subject of further consideration.\"\n\nIn 1945 the presidents of the eight schools signed the first \"Ivy Group Agreement\", which set academic, financial, and athletic standards for the football teams. The principles established reiterated those put forward in the Harvard-Yale-Princeton Presidents' Agreement of 1916. The Ivy Group Agreement established the core tenet that an applicant's ability to play on a team would not influence admissions decisions:\n\nIn 1954, the presidents extended the Ivy Group Agreement to all intercollegiate sports, effective with the 1955–56 basketball season. This is generally reckoned as the formal formation of the Ivy League. As part of the transition, Brown, the only Ivy that hadn't joined the EIBL, did so for the 1954–55 season. A year later, the Ivy League absorbed the EIBL. The Ivy League claims the EIBL's history as its own. Through the EIBL, it is the oldest basketball conference in Division I.\n\nAs late as the 1960s many of the Ivy League universities' undergraduate programs remained open only to men, with Cornell the only one to have been coeducational from its founding (1865) and Columbia being the last (1983) to become coeducational. Before they became coeducational, many of the Ivy schools maintained extensive social ties with nearby Seven Sisters women's colleges, including weekend visits, dances and parties inviting Ivy and Seven Sisters students to mingle. This was the case not only at Barnard College and Radcliffe College, which are adjacent to Columbia and Harvard, but at more distant institutions as well. The movie \"Animal House\" includes a satiric version of the formerly common visits by Dartmouth men to Massachusetts to meet Smith and Mount Holyoke women, a drive of more than two hours. As noted by Irene Harwarth, Mindi Maline, and Elizabeth DeBra, \"The 'Seven Sisters' was the name given to Barnard, Smith, Mount Holyoke, Vassar, Bryn Mawr, Wellesley, and Radcliffe, because of their parallel to the Ivy League men's colleges.\"\n\nIn 1982 the Ivy League considered adding two members, with Army, Navy, and Northwestern as the most likely candidates; if it had done so, the league could probably have avoided being moved into the recently created Division I-AA (now Division I FCS) for football. In 1983, following the admission of women to Columbia College, Columbia University and Barnard College entered into an athletic consortium agreement by which students from both schools compete together on Columbia University women's athletic teams, which replaced the women's teams previously sponsored by Barnard.\n\nWhen Army and Navy departed the Eastern Intercollegiate Baseball League in 1992, all intercollegiate competition involving the eight schools became united under the Ivy League banner.\n\nThe Ivy League schools are highly selective, with acceptance rates since 2018 being 10% or less at each of the universities. Admitted students come from around the world, although students from New England and the Northeastern United States make up a significant proportion of students. In the most recent academic year, seven of the eight Ivy League schools reported record-high application numbers; seven also reported record-low acceptance rates.\n\nMembers of the League have been highly ranked by various university rankings. In addition to the broad rankings listed in the accompanying chart, several Ivy League schools are highly ranked in the current \"2018 US News & World Report\" assessment of Best Undergraduate Teaching: 1. Princeton; 2. Dartmouth; 3. Brown; 10. Yale. The \"Wall Street Journal\" rankings place all eight of the universities within the top 20 in the country.\n\nFurther, Ivy League members have produced many Nobel laureates, winners of the Nobel Prize and the Nobel Memorial Prize in Economic Sciences. According to the Nobel Foundation's website, the number of prize-winners affiliated with each Ivy League university at the time of their awards is: Brown, 2; Columbia, 17; Cornell, 8; Dartmouth, 0; Harvard, 36; Penn, 4; Princeton, 14; and Yale, 8. In addition, each university self-reports their number of affiliated Nobel laureates, but they use varying definitions for which Nobel winners they claim (for example, alumni, active faculty, former faculty, visiting faculty, adjunct faculty, etc.)\n\nCollaboration between the member schools is illustrated by the student-led Ivy Council that meets in the fall and spring of each year, with representatives from every Ivy League school. The governing body of the Ivy League is the Council of Ivy Group Presidents, composed of each university president. During meetings, the presidents often discuss common procedures and initiatives for the universities.\n\nThe universities also collaborate academically through the IvyPlus Exchange program, which allows students to cross-register at one of the Ivies or one of the eligible schools, which include the University of California at Berkeley, University of Chicago, the Massachusetts Institute of Technology, and Stanford University.\n\nDifferent fashion trends and styles have emerged from Ivy League campuses over time, and fashion trends such as Ivy League and Preppy are styles often associated with the Ivy League and its culture.\n\nIvy League style is a style of men's dress, popular during the late 1950s, believed to have originated on Ivy League campuses. The clothing stores J. Press and Brooks Brothers represent perhaps the quintessential Ivy League dress manner. The Ivy League style is said to be the predecessor to the preppy style of dress.\n\nPreppy fashion started around 1912 to the late 1940s and 1950s as the Ivy League style of dress. J. Press represents the quintessential preppy clothing brand, stemming from the collegiate traditions that shaped the preppy subculture. In the mid-twentieth century J. Press and Brooks Brothers, both being pioneers in preppy fashion, had stores on Ivy League school campuses, including Harvard, Princeton, and Yale.\n\nSome typical preppy styles also reflect traditional upper class New England leisure activities, such as equestrian, sailing or yachting, hunting, fencing, rowing, lacrosse, tennis, golf, and rugby. Longtime New England outdoor outfitters, such as L.L. Bean, became part of conventional preppy style. This can be seen in sport stripes and colours, equestrian clothing, plaid shirts, field jackets and nautical-themed accessories. Vacationing in Palm Beach, Florida, long popular with the East Coast upper class, led to the emergence of bright colour combinations in leisure wear seen in some brands such as Lilly Pulitzer. By the 1980s, other brands such as Lacoste, Izod and Dooney & Bourke became associated with preppy style.\n\nToday, these styles continue to be popular on Ivy League campuses, throughout the U.S., and abroad, and are oftentimes labeled as \"Classic American style\" or \"Traditional American style\".\n\nThe Ivy League is often associated with the upper class White Anglo-Saxon Protestant community of the Northeast, Old Money, or more generally, the American upper middle and upper classes. Although most Ivy League students come from upper middle- and upper-class families, the student body has become increasingly more economically and ethnically diverse. The universities provide significant financial aid to help increase the enrollment of lower income and middle class students. Several reports suggest, however, that the proportion of students from less-affluent families remains low.\n\nPhrases such as \"Ivy League snobbery\" are ubiquitous in nonfiction and fiction writing of the early and mid-twentieth century. A Louis Auchincloss character dreads \"the aridity of snobbery which he knew infected the Ivy League colleges\". A business writer, warning in 2001 against discriminatory hiring, presented a cautionary example of an attitude to avoid (the bracketed phrase is his):\n\nThe phrase \"Ivy League\" historically has been perceived as connected not only with academic excellence, but also with social elitism. In 1936, sportswriter John Kieran noted that student editors at Harvard, Yale, Princeton, Cornell, Columbia, Dartmouth, and Penn were advocating the formation of an athletic association. In urging them to consider \"Army and Navy and Georgetown and Fordham and Syracuse and Brown and Pitt\" as candidates for membership, he exhorted:\n\nAspects of Ivy stereotyping were illustrated during the 1988 presidential election, when George H. W. Bush (Yale '48) derided Michael Dukakis (graduate of Harvard Law School) for having \"foreign-policy views born in Harvard Yard's boutique.\" \"New York Times\" columnist Maureen Dowd asked \"Wasn't this a case of the pot calling the kettle elite?\" Bush explained, however, that, unlike Harvard, Yale's reputation was \"so diffuse, there isn't a symbol, I don't think, in the Yale situation, any symbolism in it. ... Harvard boutique to me has the connotation of liberalism and elitism\" and said \"Harvard\" in his remark was intended to represent \"a philosophical enclave\" and not a statement about class. Columnist Russell Baker opined that \"Voters inclined to loathe and fear elite Ivy League schools rarely make fine distinctions between Yale and Harvard. All they know is that both are full of rich, fancy, stuck-up and possibly dangerous intellectuals who never sit down to supper in their undershirt no matter how hot the weather gets.\" Still, the last five presidents have all attended Ivy League schools for at least part of their education— George H. W. Bush (Yale undergrad), Bill Clinton (Yale Law School), George W. Bush (Yale undergrad, Harvard Business School), Barack Obama (Columbia undergrad, Harvard Law School), and Donald Trump (Penn undergrad).\n\nOf the 44 men who have served as President of the United States, 16 have graduated from an Ivy League university. Of them, eight have degrees from Harvard, five from Yale, three from Columbia, two from Princeton and one from Penn. Twelve presidents have earned Ivy undergraduate degrees. Three of these were transfer students: Donald Trump transferred from Fordham University, Barack Obama transferred from Occidental College and John F. Kennedy transferred from another Ivy, Princeton, where he had been class of 1939. John Adams was the first president to graduate from college, graduating from Harvard in 1755.\n\nStudents of the Ivy League largely hail from the Northeast, largely from the New York City, Boston, and Philadelphia areas. As all eight Ivy League universities are within the Northeast, it is no surprise that most graduates end up working and residing in the Northeast after graduation. An unscientific survey of Harvard seniors from the Class of 2013 found that 42% hailed from the Northeast and 55% overall were planning on working and residing in the Northeast. Boston and New York City are traditionally where many Ivy League graduates end up living.\n\nStudents of the Ivy League, both graduate and undergraduate, come primarily from upper middle and upper class families. In recent years, however, the universities have looked towards increasing socioeconomic and class diversity, by providing greater financial aid packages to applicants from lower, working, and middle class American families.\n\nIn 2013, 46% of Harvard undergraduate students came from families in the top 3.8% of all American households (i.e., over $200,000 annual income). In 2012, the bottom 25% of the American income distribution accounted for only 3–4% of students at Brown, a figure that had remained unchanged since 1992. In 2014, 69% of incoming freshmen students at Yale College came from families with annual incomes of over $120,000, putting most Yale College students in the upper middle and/or upper class. (The median household income in the U.S. in 2013 was $52,700.)\n\nIn the 2011–2012 academic year, students qualifying for Pell Grants (federally funded scholarships on the basis of need) comprised 20% at Harvard, 18% at Cornell, 17% at Penn, 16% at Columbia, 15% at Dartmouth and Brown, 14% at Yale, and 12% at Princeton. Nationally, 35% of American university students qualify for a Pell Grant.\n\nIvy champions are recognized in sixteen men's and sixteen women's sports. In some sports, Ivy teams actually compete as members of another league, the Ivy championship being decided by isolating the members' records in play against each other; for example, the six league members who participate in ice hockey do so as members of ECAC Hockey, but an Ivy champion is extrapolated each year. In one sport, rowing, the Ivies recognize team champions for each sex in both heavyweight and lightweight divisions. While the Intercollegiate Rowing Association governs all four sex- and bodyweight-based divisions of rowing, the only one that is sanctioned by the NCAA is women's heavyweight. The Ivy League was the last Division I basketball conference to institute a conference postseason tournament; the first tournaments for men and women were held at the end of the 2016–17 season. The tournaments only award the Ivy League automatic bids for the NCAA Division I Men's and Women's Basketball Tournaments; the official conference championships continue to be awarded based solely on regular-season results. Before the 2016–17 season, the automatic bids were based solely on regular-season record, with a one-game playoff (or series of one-game playoffs if more than two teams were tied) held to determine the automatic bid. The Ivy League is one of only two Division I conferences which award their official basketball championships solely on regular-season results; the other is the Southeastern Conference. Since its inception, an Ivy League school has yet to win either the men's or women's Division I NCAA Basketball Tournament.\n\nOn average, each Ivy school has more than 35 varsity teams. All eight are in the top 20 for number of sports offered for both men and women among Division I schools. Unlike most Division I athletic conferences, the Ivy League prohibits the granting of athletic scholarships; all scholarships awarded are need-based (financial aid). In addition, the Ivies have a rigid policy against redshirting, even for medical reasons; an athlete loses a year of eligibility for every year enrolled at an Ivy institution. Ivy League teams' non-league games are often against the members of the Patriot League, which have similar academic standards and athletic scholarship policies (although unlike the Ivies, the Patriot League allows redshirting).\n\nIn the time before recruiting for college sports became dominated by those offering athletic scholarships and lowered academic standards for athletes, the Ivy League was successful in many sports relative to other universities in the country. In particular, Princeton won 26 recognized national championships in college football (last in 1935), and Yale won 18 (last in 1927). Both of these totals are considerably higher than those of other historically strong programs such as Alabama, which has won 15, Notre Dame, which claims 11 but is credited by many sources with 13, and USC, which has won 11. Yale, whose coach Walter Camp was the \"Father of American Football,\" held on to its place as the all-time wins leader in college football throughout the entire 20th century, but was finally passed by Michigan on November 10, 2001. Harvard, Yale, Princeton and Penn each have over a dozen former scholar-athletes enshrined in the College Football Hall of Fame. Currently Dartmouth holds the record for most Ivy League football titles, with 18, followed closely by Harvard and Penn, each with 17 titles. In addition, the Ivy League has produced Super Bowl winners Kevin Boothe (Cornell), two-time Pro Bowler Zak DeOssie (Brown), Sean Morey (Brown), All-Pro selection Matt Birk (Harvard), Calvin Hill (Yale), Derrick Harmon (Cornell) and 1999 \"Mr. Irrelevant\" Jim Finn (Penn).\n\nBeginning with the 1982 football season, the Ivy League has competed in Division I-AA (renamed FCS The Ivy League teams are eligible for the FCS tournament held to determine the national champion, and the league champion is eligible for an automatic bid (and any other team may qualify for an at-large selection) from the NCAA. However, since its inception in 1956, the Ivy League has not played any postseason games due to concerns about the extended December schedule's effects on academics. (The last postseason game for a member was , the 1934 Rose Bowl, won by For this reason, any Ivy League team invited to the FCS playoffs turns down the bid. The Ivy League plays a strict 10-game schedule, compared to other FCS members' schedules of 11 (or, in some seasons, 12) regular season games, plus post-season, which expanded in 2013 to five rounds with 24 teams, with a bye week for the top eight teams. Football is the only sport in which the Ivy League declines to compete for a national title.\n\nIn addition to varsity football, Penn, Princeton and Cornell also field teams in the eight-team Collegiate Sprint Football League, in which all players must weigh 172 pounds or less. Penn and Princeton are the last remaining founding members of the league from its 1934 debut, and Cornell is the next-oldest, joining in 1937. Yale and Columbia previously fielded teams in the league but no longer do so.\n\nThe Ivy League is home to some of the oldest college rugby teams in the United States. Although these teams are not \"varsity\" sports, they compete annually in the Ivy Rugby Conference.\n\nThe table above includes the number of team championships won from the beginning of official Ivy League competition (1956–57 academic year) through 2016-17. Princeton and Harvard have on occasion won ten or more Ivy League titles in a year, an achievement accomplished 10 times by Harvard and 24 times by Princeton, including a conference-record 15 championships in 2010–11. Only once has one of the other six schools earned more than eight titles in a single academic year (Cornell with nine in 2005–06). In the 38 academic years beginning 1979–80, Princeton has averaged 10 championships per year, one-third of the conference total of 33 sponsored sports.\nIn the 12 academic years beginning 2005–06 Princeton has won championships in 31 different sports, all except wrestling and men's tennis.\n\nRivalries run deep in the Ivy League. For instance, Princeton and Penn are longstanding men's basketball rivals; \"Puck Frinceton\" T-shirts are worn by Quaker fans at games. In only 11 instances in the history of Ivy League basketball, and in only seven seasons since Yale's 1962 title, has neither Penn nor Princeton won at least a share of the Ivy League title in basketball, with Princeton champion or co-champion 26 times and Penn 25 times. Penn has won 21 outright, Princeton 19 outright. Princeton has been a co-champion 7 times, sharing 4 of those titles with Penn (these 4 seasons represent the only times Penn has been co-champion). Harvard won its first title of either variety in 2011, losing a dramatic play-off game to Princeton for the NCAA tournament bid, then rebounded to win outright championships in 2012, 2013, and 2014. Harvard also won the 2013 Great Alaska Shootout, defeating TCU to become the only Ivy League school to win the now-defunct tournament.\n\nRivalries exist between other Ivy league teams in other sports, including Cornell and Harvard in hockey, Harvard and Princeton in swimming, and Harvard and Penn in football (Penn and Harvard have won 28 Ivy League Football Championships since 1982, Penn-16; Harvard-12). During that time Penn has had 8 undefeated Ivy League Football Championships and Harvard has had 6 undefeated Ivy League Football Championships. In men's lacrosse, Cornell and Princeton are perennial rivals, and they are two of three Ivy League teams to have won the NCAA tournament. In 2009, the Big Red and Tigers met for their 70th game in the NCAA tournament. No team other than Harvard or Princeton has won the men's swimming conference title outright since 1972, although Yale, Columbia, and Cornell have shared the title with Harvard and Princeton during this time. Similarly, no program other than Princeton and Harvard has won the women's swimming championship since Brown's 1999 title. Princeton or Cornell has won every indoor and outdoor track and field championship, both men's and women's, every year since 2002–03, with one exception (Columbia women won indoor championship in 2012). Harvard and Yale are football and crew rivals although the competition has become unbalanced; Harvard has won all but one of the last 15 football games and all but one of the last 13 crew races.\n\nThe Yale-Princeton series is the nation's second longest, exceeded only by \"The Rivalry\" between Lehigh and Lafayette, which began later in 1884 but included two or three games in each of 17 early seasons. For the first three decades of the Yale-Princeton rivalry, the two played their season-ending game at a neutral site, usually New York City, and with one exception (1890: Harvard), the winner of the game also won at least a share of the national championship that year, covering the period 1869 through 1903. This phenomenon of a finale contest at a neutral site for the national title created a social occasion for the society elite of the metropolitan area akin to a Super Bowl in the era prior to the establishment of the NFL in 1920. These football games were also financially profitable for the two universities, so much that they began to play baseball games in New York City as well, drawing record crowds for that sport also, largely from the same social demographic. In a period when the only professional sports were fledgling baseball leagues, these high-profile early contests between Princeton and Yale played a role in popularizing spectator sports, demonstrating their financial potential and raising public awareness of Ivy universities at a time when few people attended college.\n\nThis list, which is current through July 1, 2015, includes NCAA championships and women's AIAW championships (one each for Yale and Dartmouth). Excluded from this list are all other national championships earned outside the scope of NCAA competition, including football titles and retroactive Helms Foundation titles.\n\nThe term \"Ivy\" is often used to connote a positive comparison to or association with the Ivy League, often along academic lines. The term has been used to describe the Little Ivies, a grouping of small liberal arts colleges in the Northeastern United States. Other uses include the Southern Ivies, Hidden Ivies, and the Public Ivies. The 2007 edition of \"Newsweek's How to Get Into College Now\", the editors designated 25 schools as \"New Ivies\".\n\nThe term \"Ivy Plus\" is sometimes used to refer to the Ancient Eight plus several other schools for purposes of alumni associations, university affiliations, or endowment comparisons. In his book \"Untangling the Ivy League\", Zawel writes, \"The inclusion of non–Ivy League schools under this term is commonplace for some schools and extremely rare for others. Among these other schools, Massachusetts Institute of Technology and Stanford University are almost always included. The University of Chicago and Duke University are often included as well.\" In their 2015 book \"Acing Admissions\", Mehta and Dixit write, \"The [\"Ivy Plus\" schools] include, but are not limited to: Massachusetts Institute of Technology (MIT), Stanford University and Northwestern University. Besides selectivity, these Ivy Plus colleges are thought to share similar values around academic and professional excellence, intellectual curiosity, leadership and civil engagement.\"\n\n"}
{"id": "14976", "url": "https://en.wikipedia.org/wiki?curid=14976", "title": "Ithaca Hours", "text": "Ithaca Hours\n\nThe Ithaca HOUR is a local currency used in Ithaca, New York and is the oldest and largest local currency system in the United States that is still operating. It has inspired other similar systems in Madison, Wisconsin; Corvallis, Oregon; and a proposed system in the Lehigh Valley, Pennsylvania. One Ithaca HOUR is valued at US$10 and is generally recommended to be used as payment for one hour's work, although the rate is negotiable.\n\nIthaca HOURS are not backed by national currency and cannot be freely converted to national currency, although some businesses may agree to buy them.\nHOURS are printed on high-quality paper and use faint graphics that would be difficult to reproduce, and each bill is stamped with a serial number, in order to discourage counterfeiting.\n\nIn 2002, a one-tenth hour bill was introduced, partly due to the encouragement and funding from Alternatives Federal Credit Union and feedback from retailers who complained about the awkwardness of only having larger denominations to work with; the bills bear the signatures of both HOURS president Steve Burke and the president of AFCU.\n\nWhile the Ithaca HOUR continues to exist, in recent years it has fallen into disuse. Media accounts from the year 2011 indicate that the number of businesses accepting HOURS has declined. Several reasons are attributed to this. First has been the founder, Paul Glover, moving out of town. While in Ithaca, Glover had acted as an evangelist and networker for HOURS, helping spread their use and helping businesses find ways to spend HOURS they had received. Secondly, a general shift away from cash transactions towards electronic transfers with debit or credit cards. Glover has emphasized that every local currency needs at least one full-time networker to \"promote, facilitate and troubleshoot\" currency circulation.\n\nIthaca HOURS were started by Paul Glover in November 1991. The system has historical roots in scrip and alternative and local currencies that proliferated in America during the Great Depression.\n\nWhile doing research into local economics during 1989, Glover had seen an \"Hour\" note 19th century British industrialist Robert Owen issued to his workers for spending at his company store. After Ithaca HOURS began, he discovered that Owen's Hours were based on Josiah Warren's \"Time Store\" notes of 1827.\n\nIn May 1991, local student Patrice Jennings interviewed Glover about the Ithaca LETS enterprise. This conversation strongly reinforced his interest in trade systems. Jennings's research on the Ithaca LETS and its failure was integral to the development of the HOUR currency; conversations between Jennings and Glover helped ensure that HOURS used knowledge of what had not worked with the LETS system.\n\nWithin a few days, he had designs for the HOUR and Half HOUR notes. He established that each HOUR would be worth the equivalent of $10, which was about the average hourly amount that workers earned in surrounding Tompkins County, although the exact rate of exchange for any given transaction was to be decided by the parties themselves. At GreenStar Cooperative Market, a local food co-op, Glover approached Gary Fine, a local massage therapist, with photocopied samples. Fine became the first person to sign a list formally agreeing to accept HOURS in exchange for services. Soon after, Jim Rohrrsen, the proprietor of a local toy store, became the first retailer to sign-up to accept Ithaca HOURS in exchange for merchandise.\n\nWhen the system was first started, 90 people agreed to accept HOURS as pay for their services. They all agreed to accept HOURS despite the lack of a business plan or guarantee. Glover then began to ask for small donations to help pay for printing HOURS.\n\nFine Line Printing completed the first run of 1,500 HOURS and 1,500 Half HOURS in October 1991. These notes, the first modern local currency, were nearly twice as large as the current Ithaca HOURS. Because they didn't fit well in people's wallets, almost all of the original notes have been removed from circulation.\n\nThe first issue of Ithaca Money was printed at Our Press, a printing shop in Chenango Bridge, New York, on October 16, 1991. The next day Glover issued 10 HOURS to Ithaca Hours, the organization he founded to run the system, as the first of four reimbursements for the cost of printing HOURS. The day after that, October 18, 1991, 382 HOURS were disbursed and prepared for mailing to the first 93 pioneers.\n\nOn October 19, 1991, Glover bought a samosa from Catherine Martinez at the Farmers' Market with Half HOUR #751—the first use of an HOUR. Several other Market vendors enrolled that day. During the next years more than a thousand individuals enrolled to accept HOURS, plus 500 businesses.\n\nStacks of the Ithaca Money newspaper were distributed all over town with an invitation to \"join the fun.\"\n\nA Barter Potluck was held at GIAC on November 12, 1991, the first of many monthly gatherings where food and skills were exchanged, acquaintances made, and friendships renewed.\n\nIn 1996, Glover was running the Ithaca Hours system from his home, and the system had an advisory board and a governing board called the \"Barter Potluck\". The board and Glover put forth the idea that economic interactions should be based on harmony rather than on more Hobbsian forms of competition. In one interview, Glover stated that \"There's a growing movement called \"ecological economics\" and Ithaca HOURS is part of that cosmos. Last year I wrote an article which discusses moving us toward the provision of food, fuel, clothing, housing, transportation, [and other] necessities in ways which are healing of nature, or which are less depleting at least and which bring people together on the basis of their shared pride, not arrogance.\" Thus one underlying principle of the local currency movement is to create \"fair trade\" with a minimum of conflict or exploitation of either people or natural resources.\n\nThe Advisory Board incorporated the Ithaca HOUR system as Ithaca Hours, Inc. in October 1998, and hosted the first elections for Board of Directors in March 1999. The first Board of Directors included Monica Hargraves, Dan Cogan, Margaret McCasland, Erica Van Etten, Greg Spence Wolf, Bob LeRoy, LeGrace Benson, Wally Woods, Jennifer Elges, and Donald Stephenson. In May 1999 Glover turned the administration of Ithaca HOURS over to the newly elected Board of Directors. Glover has continued to support Ithaca Hours through community outreach to present, most notably through the Ithaca Health Fund (now incorporated as part\nof the Ithaca Health Alliance) and Ithaca Community News.\n\nThe current Board of Directors, 2014-2015, includes Erik Lehmann (Chair), Danielle Klock, and Bob LeRoy.\n\nSeveral million dollars value of HOURS have been traded since 1991 among thousands of residents and over 500 area businesses, including the Cayuga Medical Center, Alternatives Federal Credit Union, the public library, many local farmers, movie theatres, restaurants, healers, plumbers, carpenters, electricians, and landlords.\n\nOne of the primary functions of the Ithaca Hours system is to promote local economic development. Businesses who receive Hours must spend them on local goods and services, thus building a network of inter-supporting local businesses. While non-local businesses are welcome to accept Hours, those businesses need to spend them on local goods and services to be economically sustainable.\n\nIn their mission to promote local economic development, the Board of Directors also makes interest-free loans of Ithaca HOURS to local businesses and grants to local non-profit organizations.\n\n\n"}
{"id": "14979", "url": "https://en.wikipedia.org/wiki?curid=14979", "title": "Interstellar cloud", "text": "Interstellar cloud\n\nAn interstellar cloud is generally an accumulation of gas, plasma, and dust in our and other galaxies. Put differently, an interstellar cloud is a denser-than-average region of the interstellar medium, (ISM), the matter and radiation that exists in the space between the star systems in a galaxy. Depending on the density, size, and temperature of a given cloud, its hydrogen can be neutral, making an H I region; ionized, or plasma making it an H II region; or molecular, which are referred to simply as molecular clouds, or sometimetimes dense clouds. Neutral and ionized clouds are sometimes also called \"diffuse clouds\". An interstellar cloud is formed by the gas and dust particles from a red giant in its later life.\n\nThe chemical composition of interstellar clouds is determined by studying electromagnetic radiation or EM radiation that they emanate, and we receive – from radio waves through visible light, to gamma rays on the electromagnetic spectrum – that we receive from them. Large radio telescopes scan the intensity in the sky of particular frequencies of electromagnetic radiation which are characteristic of certain molecules' spectra. Some interstellar clouds are cold and tend to give out EM radiation of large wavelengths. A map of the abundance of these molecules can be made, enabling an understanding of the varying composition of the clouds. In hot clouds, there are often ions of many elements, whose spectra can be seen in visible and ultraviolet light.\n\nRadio telescopes can also scan over the frequencies from one point in the map, recording the intensities of each type of molecule. Peaks of frequencies mean that an abundance of that molecule or atom is present in the cloud. The height of the peak is proportional to the relative percentage that it makes up.\n\nUntil recently the rates of reactions in interstellar clouds were expected to be very slow, with minimal products being produced due to the low temperature and density of the clouds. However, organic molecules were observed in the spectra that scientists would not have expected to find under these conditions, such as formaldehyde, methanol, and vinyl alcohol. The reactions needed to create such substances are familiar to scientists only at the much higher temperatures and pressures of earth and earth-based laboratories. The fact that they were found indicates that these chemical reactions in interstellar clouds take place faster than suspected, likely in gas-phase reactions unfamiliar to organic chemistry as observed on earth. These reactions are studied in the CRESU experiment.\n\nInterstellar clouds also provide a medium to study the presence and proportions of metals in space. The presence and ratios of these elements may help develop theories on the means of their production, especially when their proportions are inconsistent with those expected to arise from stars as a result of fusion and thereby suggest alternate means, such as cosmic ray spallation.\n\nThese interstellar clouds possess a velocity higher than can be explained by the rotation of the Milky Way. By definition, these clouds must have a v greater than 90 km s, where v is the local standard rest velocity. They are detected primarily in the 21 cm line of neutral hydrogen, and typically have a lower portion of heavy elements than is normal for interstellar clouds in the Milky Way.\n\nTheories intended to explain these unusual clouds include materials left over from the formation of the galaxy, or tidally-displaced matter drawn away from other galaxies or members of the Local Group. An example of the latter is the Magellanic Stream. To narrow down the origin of these clouds, a better understanding of their distances and metallicity is needed.\n\nHigh-velocity clouds are identified with an HVC prefix, as with HVC 127-41-330.\n\n\n"}
{"id": "14980", "url": "https://en.wikipedia.org/wiki?curid=14980", "title": "Imhotep", "text": "Imhotep\n\nImhotep (; Egyptian: \"ỉỉ-m-ḥtp\" \"*jā-im-ḥātap\", in Unicode hieroglyphs: 𓇍𓅓𓊵:𓏏*𓊪, \"the one who comes in peace\"; fl. late 27th century BC) was an Egyptian chancellor to the pharaoh Djoser, probable architect of the step pyramid, and high priest of the sun god Ra at Heliopolis. Very little is known of Imhotep as a historical figure, but in the 3000 years following his death, he was gradually glorified and deified.\n\nToday, outside the Egyptological community, he is referred to as a polymath, poet, judge, engineer, magician, scribe, astronomer, astrologer, and especially a physician; indeed, some have considered Imhotep alongside Hippocrates and Charaka as the fathers of early medicine. These claims are founded on the legends that flourished in the millennia after his death, not on historical records. No text from his lifetime mentions these capacities and no text mentions his name in the first 1200 years following his death. Apart from the three short contemporary inscriptions that establish him as chancellor to the pharaoh, the first text to reference Imhotep dates to the time of Amenhotep III (c. 1391–1353 BC). It is addressed to the owner of a tomb, and reads:\n\nIt appears that this libation to Imhotep was done regularly, as they are attested on papyruses associated to statues of Imhotep until the Late Period (c. 664–332 BC). To Wildung, this cult holds its origin in the slow evolution of the memory of Imhotep among intellectuals from his death onwards. To Alan Gardiner, this cult is so distinct from the offerings usually made to commoners that the epithet of \"demi-god\" is likely justified to describe the way Imhotep was venerated in the New Kingdom (c. 1550–1077 BC).\n\nThe first references to the healing abilities of Imhotep occur from the Thirtieth Dynasty (c. 380–343 BC) onwards, some 2200 years after his death.\n\nHe was one of only two commoners ever to be deified after death (the other being Amenhotep, son of Hapu). The center of his cult was in Memphis.\nThe location of his tomb remains unknown, despite efforts to find it. The consensus is that it is hidden somewhere at Saqqara.\n\nImhotep's historicity is confirmed by two contemporary inscriptions made during his lifetime on the base or pedestal of one of Djoser's statues (Cairo JE 49889) and also by a graffito on the enclosure wall surrounding Sekhemkhet's unfinished step-pyramid. The latter inscription suggests that Imhotep outlived Djoser by a few years and went on to serve in the construction of King Sekhemkhet's pyramid, which was abandoned due to this ruler's brief reign.\n\nImhotep was one of the chief officials of the Pharaoh Djoser. Egyptologists ascribe to him the design of the Pyramid of Djoser, a step pyramid at Saqqara in Egypt in 2630–2611 B.C. He may also have been responsible for the first known use of stone columns to support a building. Despite these later attestations, the pharaonic Egyptians themselves never credited Imhotep as the designer of the stepped pyramid nor with the invention of stone architecture.\n\nTwo thousand years after his death, Imhotep's status had risen to that of a god of medicine and healing. He was eventually equated with Thoth, the god of architecture, mathematics and medicine, and patron of scribes: Imhotep's cult had merged with that of his former tutelary god.\n\nHe was revered in the region of Thebes as the \"brother\" of Amenhotep, son of Hapu, another deified architect, in the temples dedicated to Thoth. Imhotep was also linked to Asklepios by the Greeks.\n\nAccording to myth, Imhotep's mother was a mortal named Kheredu-ankh, she too being eventually revered as a demi-goddess as daughter of Banebdjedet. Alternatively, since Imhotep was known as the \"Son of Ptah\", his mother was sometimes claimed to be Sekhmet, the patron of Upper Egypt whose consort was Ptah.\n\nThe Upper Egyptian Famine Stela, which dates from the Ptolemaic period (305–30 B.C.), bears an inscription containing a legend about a famine lasting seven years during the reign of Djoser. Imhotep is credited with having been instrumental in ending it. One of his priests explained the connection between the god Khnum and the rise of the Nile to the king, who then had a dream in which the Nile god spoke to him, promising to end the drought.\n\nA demotic papyrus from the temple of Tebtunis, dating to the 2nd century A.D., preserves a long story about Imhotep. King Djoser plays a prominent role in the story, which also mentions Imhotep's family; his father the god Ptah, his mother Khereduankh, and his younger sister Renpetneferet. At one point Djoser desires Renpetneferet, and Imhotep disguises himself and tries to rescue her. The text also refers to the royal tomb of Djoser. Part of the legend includes an anachronistic battle between the Old Kingdom and the Assyrian armies where Imhotep fights an Assyrian sorceress in a duel of magic.\n\nAs an instigator of Egyptian culture, Imhotep's idealized image lasted well into the Roman period. In the Ptolemaic period, the Egyptian priest and historian Manetho credited him with inventing the method of a stone-dressed building during Djoser's reign, though he was not the first to actually build with stone. Stone walling, flooring, lintels, and jambs had appeared sporadically during the Archaic Period, though it is true that a building of the size of the step pyramid made entirely out of stone had never before been constructed. Prior to Djoser, pharaohs were buried in mastaba tombs.\n\nEgyptologist James Peter Allen states that \"The Greeks equated him with their own god of medicine, Asklepios, although ironically there is no evidence that Imhotep himself was a physician.\"\n\n\n\n\n"}
{"id": "14981", "url": "https://en.wikipedia.org/wiki?curid=14981", "title": "Ictinus", "text": "Ictinus\n\nIctinus (; , \"Iktinos\") was an architect active in the mid 5th century BC. Ancient sources identify Ictinus and Callicrates as co-architects of the Parthenon. He co-wrote a book on the project – which is now lost – in collaboration with Carpion.\n\nPausanias identifies Ictinus as architect of the Temple of Apollo at Bassae. That temple was Doric on the exterior, Ionic on the interior, and incorporated a Corinthian column, the earliest known, at the center rear of the cella. Sources also identify Ictinus as architect of the Telesterion at Eleusis, a gigantic hall used in the Eleusinian Mysteries.\n\nPericles also commissioned Ictinus to design the Telesterion (Hall of Mysteries) at Eleusis, but his involvement was terminated when Pericles fell from power. Three other architects took over instead. It seems likely that Ictinus's reputation was harmed by his links with the fallen ruler, as he is singled out for condemnation by Aristophanes in his play \"The Birds\", dated to around 414 BC. It depicts the royal kite or \"ictinus\" – a play on the architect's name – not as a noble bird of prey but as a scavenger stealing sacrifices from the gods and money from men. As no other classical author describes the bird in this fashion, Aristophanes likely intended it to be a dig at the architect.\n\nThe artist Jean Auguste Dominique Ingres painted a scene showing Ictinus together with the lyric poet Pindar. The painting is known as \"Pindar and Ictinus\" and is exhibited at the National Gallery, London.\n\n"}
{"id": "14982", "url": "https://en.wikipedia.org/wiki?curid=14982", "title": "Isidore of Miletus", "text": "Isidore of Miletus\n\nIsidore of Miletus (; ; ) was one of the two main Byzantine Greek architects (Anthemius of Tralles was the other) that Emperor Justinian I commissioned to design the cathedral Hagia Sophia in Constantinople from 532 to 537. The creation of an important compilation of Archimedes' works has been attributed to him. The spurious Book XV from Euclid's Elements has been partly attributed to Isidore of Miletus.\n\nIsidore of Miletus was a renowned scientist and mathematician before Emperor Justinian I hired him. Isidorus taught stereometry and physics at the universities, first of Alexandria then of Constantinople, and wrote a commentary on an older treatise on vaulting. Eutocius together with Isidore studied Archimedes work. Isidore is also renowned for producing the first comprehensive compilation of Archimedes' work, the Archimedes palimpsest survived to the present.\n\nEmperor Justinian I appointed his architects to rebuild the Hagia Sophia following his victory over protesters within the capital city of his Roman Empire, Constantinople. The first basilica was completed in 360 and remodelled from 404 to 415, but had been damaged in 532 in the course of the Nika Riot, “The temple of Sophia, the baths of Zeuxippus, and the imperial courtyard from the Propylaia all the way to the so-called House of Ares were burned up and destroyed, as were both of the great porticoes that lead to the forum that is named after Constantine, houses of prosperous people, and a great deal of other properties.”\n\nThe warring factions of Byzantine society, the Blues and the Greens, opposed each other in the chariot races at the Hippodrome and often resorted to violence. During the Nika Riot, more than thirty thousand people died. Emperor Justinian I ensured that his new structure would not be burned down, like its predecessors, by commissioning architects that would build the church mainly out of stone, rather than wood, “He compacted it of baked brick and mortar, and in many places bound it together with iron, but made no use of wood, so that the church should no longer prove combustible.”\n\nIsidore of Miletus and Anthemius of Tralles originally planned on a main hall of the Hagia Sophia that measured 70 by 75 metres (230 x 250 ft), making it the largest church in Constantinople, but the original dome was nearly 6 metres (20 ft) lower than it was constructed, “Justinian suppressed these riots and took the opportunity of marking his victory by erecting in 532-7 the new Hagia Sophia, one of the largest, most lavish, and most expensive buildings of all time.”\n\nAlthough Isidore of Miletus and Anthemius of Tralles were not formally educated in architecture, they were scientists that could organize the logistics of drawing thousands of labourers and unprecedented loads of rare raw materials from around the Roman Empire to create the Hagia Sophia for Emperor Justinian I. The finished product was built in admirable form for the Roman Emperor, “All of these elements marvellously fitted together in mid-air, suspended from one another and reposing only on the parts adjacent to them, produce a unified and most remarkable harmony in the work, and yet do not allow the spectators to rest their gaze upon any one of them for a length of time.”\n\nThe Hagia Sophia architects innovatively combined the longitudinal structure of a Roman basilica and the central plan of a drum-supported dome, in order to withstand the high magnitude earthquakes of the Marmara Region, “However, in May 558, little more than 20 years after the Church’s dedication, following the earthquakes of August 553 and December 557, parts of the central dome and its supporting structure system collapsed.” The Hagia Sophia was repeatedly cracked by earthquakes and was quickly repaired. Isidore of Miletus’ nephew, Isidore the Younger, introduced the new dome design that can be viewed in the Hagia Sophia in present-day Istanbul, Turkey.\n\nAfter a great earthquake in 989 ruined the dome of Hagia Sophia, the Byzantine officials summoned Trdat the Architect to Byzantium to organize repairs. The restored dome was completed by 994.\n\n"}
{"id": "14984", "url": "https://en.wikipedia.org/wiki?curid=14984", "title": "International Atomic Energy Agency", "text": "International Atomic Energy Agency\n\nThe International Atomic Energy Agency (IAEA) is an international organization that seeks to promote the peaceful use of nuclear energy, and to inhibit its use for any military purpose, including nuclear weapons. The IAEA was established as an autonomous organisation on 29 July 1957. Though established independently of the United Nations through its own international treaty, the IAEA Statute, the IAEA reports to both the United Nations General Assembly and Security Council.\n\nThe IAEA has its headquarters in Vienna, Austria. The IAEA has two \"Regional Safeguards Offices\" which are located in Toronto, Canada, and in Tokyo, Japan. The IAEA also has two liaison offices which are located in New York City, United States, and in Geneva, Switzerland. In addition, the IAEA has laboratories and research centers located in Seibersdorf, Austria, in Monaco and in Trieste, Italy. \n\nThe IAEA serves as an intergovernmental forum for scientific and technical co-operation in the peaceful use of nuclear technology and nuclear power worldwide. The programs of the IAEA encourage the development of the peaceful applications of nuclear energy, science and technology, provide international safeguards against misuse of nuclear technology and nuclear materials, and promote nuclear safety (including radiation protection) and nuclear security standards and their implementation.\n\nThe IAEA and its former Director General, Mohamed ElBaradei, were jointly awarded the Nobel Peace Prize on 7 October 2005. The IAEA's current Director General is Yukiya Amano.\n\nIn 1953, the President of the United States, Dwight D. Eisenhower, proposed the creation of an international body to both regulate and promote the peaceful use of atomic power (nuclear power), in his Atoms for Peace address to the UN General Assembly. In September 1954, the United States proposed to the General Assembly the creation of an international agency to take control of fissile material, which could be used either for nuclear power or for nuclear weapons. This agency would establish a kind of \"nuclear bank.\"\n\nThe United States also called for an international scientific conference on all of the peaceful aspects of nuclear power. By November 1954, it had become clear that the Soviet Union would reject any international custody of fissile material if the United States did not agree to a disarmament first, but that a \"clearing house\" for nuclear transactions might be possible. From 8 to 20 August 1955, the United Nations held the International Conference on the Peaceful Uses of Atomic Energy in Geneva, Switzerland. In October 1957, a Conference on the IAEA Statute was held at the Headquarters of the United Nations to approve the founding document for the IAEA, which was negotiated in 1955–1957 by a group of twelve countries. The Statute of the IAEA was approved on 23 October 1956 and came into force on 29 July 1957.\n\nFormer US Congressman W. Sterling Cole served as the IAEA's first Director General from 1957 to 1961. Cole served only one term, after which the IAEA was headed by two Swedes for nearly four decades: the scientist Sigvard Eklund held the job from 1961 to 1981, followed by former Swedish Foreign Minister Hans Blix, who served from 1981 to 1997. Blix was succeeded as Director General by Mohamed ElBaradei of Egypt, who served until November 2009.\n\nBeginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA increased its efforts in the field of nuclear safety. The same happened after the 2011 Fukushima disaster in Fukushima, Japan.\n\nBoth the IAEA and its then Director General, ElBaradei, were awarded the Nobel Peace Prize in 2005. In ElBaradei's acceptance speech in Oslo, he stated that only one percent of the money spent on developing new weapons would be enough to feed the entire world, and that, if we hope to escape self-destruction, then nuclear weapons should have no place in our collective conscience, and no role in our security.\n\nOn 2 July 2009, Yukiya Amano of Japan was elected as the Director General for the IAEA, defeating Abdul Samad Minty of South Africa and Luis E. Echávarri of Spain. On 3 July 2009, the Board of Governors voted to appoint Yukiya Amano \"by acclamation,\" and IAEA General Conference in September 2009 approved. He took office on 1 December 2009.\n\nThe IAEA's mission is guided by the interests and needs of Member States, strategic plans and the vision embodied in the IAEA Statute (see below). Three main pillars – or areas of work – underpin the IAEA's mission: Safety and Security; Science and Technology; and Safeguards and Verification.\n\nThe IAEA as an autonomous organisation is not under direct control of the UN, but the IAEA does report to both the UN General Assembly and Security Council. Unlike most other specialised international agencies, the IAEA does much of its work with the Security Council, and not with the United Nations Economic and Social Council. The structure and functions of the IAEA are defined by its founding document, the IAEA Statute (see below). The IAEA has three main bodies: the Board of Governors, the General Conference, and the Secretariat.\n\nThe IAEA exists to pursue the \"safe, secure and peaceful uses of nuclear sciences and technology\" (Pillars 2005). The IAEA executes this mission with three main functions: the inspection of existing nuclear facilities to ensure their peaceful use, providing information and developing standards to ensure the safety and security of nuclear facilities, and as a hub for the various fields of science involved in the peaceful applications of nuclear technology.\n\nThe IAEA recognises knowledge as the nuclear energy industry's most valuable asset and resource, without which the industry cannot operate safely and economically. Following the IAEA General Conference since 2002 resolutions the Nuclear Knowledge Management, a formal programme was established to address Member States' priorities in the 21st century.\n\nIn 2004, the IAEA developed a Programme of Action for Cancer Therapy (PACT). PACT responds to the needs of developing countries to establish, to improve, or to expand radiotherapy treatment programs. The IAEA is raising money to help efforts by its Member States to save lives and to reduce suffering of cancer victims.\n\nThe IAEA has established programs to help developing countries in planning to build systematically the capability to manage a nuclear power program, including the Integrated Nuclear Infrastructure Group, which has carried out Integrated Nuclear Infrastructure Review missions in Indonesia, Jordan, Thailand and Vietnam. The IAEA reports that roughly 60 countries are considering how to include nuclear power in their energy plans.\n\nTo enhance the sharing of information and experience among IAEA Member States concerning the seismic safety of nuclear facilities, in 2008 the IAEA established the International Seismic Safety Centre. This centre is establishing safety standards and providing for their application in relation to site selection, site evaluation and seismic design.\n\nThe Board of Governors is one of two policy making bodies of the IAEA. The Board consists of 22 member states elected by the General Conference, and at least 10 member states nominated by the outgoing Board. The outgoing Board designates the ten members who are the most advanced in atomic energy technology, plus the most advanced members from any of the following areas that are not represented by the first ten: North America, Latin America, Western Europe, Eastern Europe, Africa, Middle East and South Asia, South East Asia, the Pacific, and the Far East. These members are designated for one year terms. The General Conference elects 22 members from the remaining nations to two-year terms. Eleven are elected each year. The 22 elected members must also represent a stipulated geographic diversity. The 35 Board members for the 2018–2019 period are: Argentina, Armenia, Australia, Azerbaijan, Belgium, Brazil, Canada, Chile, China, Ecuador, Egypt, France, Germany, India, Indonesia, Italy, Japan, Jordan, Kenya, the Republic of Korea, Morocco, the Netherlands, Niger, Pakistan, Portugal, the Russian Federation, Serbia, South Africa, the Sudan, Sweden, Thailand, the United Kingdom of Great Britain and Northern Ireland, the United States of America, Uruguay and the Bolivarian Republic of Venezuela.\n\nThe Board, in its five yearly meetings, is responsible for making most of the policy of the IAEA. The Board makes recommendations to the General Conference on IAEA activities and budget, is responsible for publishing IAEA standards and appoints the Director General subject to General Conference approval. Board members each receive one vote. Budget matters require a two-thirds majority. All other matters require only a simple majority. The simple majority also has the power to stipulate issues that will thereafter require a two-thirds majority. Two-thirds of all Board members must be present to call a vote. The Board elects its own chairman.\n\nThe General Conference is made up of all 170 member states. It meets once a year, typically in September, to approve the actions and budgets passed on from the Board of Governors. The General Conference also approves the nominee for Director General and requests reports from the Board on issues in question (Statute). Each member receives one vote. Issues of budget, Statute amendment and suspension of a member's privileges require a two- thirds majority and all other issues require a simple majority. Similar to the Board, the General Conference can, by simple majority, designate issues to require a two- thirds majority. The General Conference elects a President at each annual meeting to facilitate an effective meeting. The President only serves for the duration of the session (Statute).\n\nThe main function of the General Conference is to serve as a forum for debate on current issues and policies. Any of the other IAEA organs, the Director General, the Board and member states can table issues to be discussed by the General Conference (IAEA Primer). This function of the General Conference is almost identical to the General Assembly of the United Nations.\n\nThe Secretariat is the professional and general service staff of the IAEA. The Secretariat is headed by the Director General. The Director General is responsible for enforcement of the actions passed by the Board of Governors and the General Conference. The Director General is selected by the Board and approved by the General Conference for renewable four-year terms. The Director General oversees six departments that do the actual work in carrying out the policies of the IAEA: Nuclear Energy, Nuclear Safety and Security, Nuclear Sciences and Applications, Safeguards, Technical Cooperation, and Management.\n\nThe IAEA budget is in two parts. The regular budget funds most activities of the IAEA and is assessed to each member nation (€344 million in 2014). The Technical Cooperation Fund is funded by voluntary contributions with a general target in the US$90 million range.\n\nThe IAEA is generally described as having three main missions:\n\nAccording to Article II of the IAEA Statute, the objective of the IAEA is \"to accelerate and enlarge the contribution of atomic energy to peace, health and prosperity throughout the world.\" Its primary functions in this area, according to Article III, are to encourage research and development, to secure or provide materials, services, equipment and facilities for Member States, to foster exchange of scientific and technical information and training.\n\nThree of the IAEA's six Departments are principally charged with promoting the peaceful uses of nuclear energy. The Department of Nuclear Energy focuses on providing advice and services to Member States on nuclear power and the nuclear fuel cycle. The Department of Nuclear Sciences and Applications focuses on the use of non-power nuclear and isotope techniques to help IAEA Member States in the areas of water, energy, health, biodiversity, and agriculture. The Department of Technical Cooperation provides direct assistance to IAEA Member States, through national, regional, and inter-regional projects through training, expert missions, scientific exchanges, and provision of equipment.\n\nArticle II of the IAEA Statute defines the Agency's twin objectives as promoting peaceful uses of atomic energy and \"<nowiki>ensur[ing]</nowiki>, so far as it is able, that assistance provided by it or at its request or under its supervision or control is not used in such a way as to further any military purpose.\" To do this, the IAEA is authorised in Article III.A.5 of the Statute \"to establish and administer safeguards designed to ensure that special fissionable and other materials, services, equipment, facilities, and information made available by the Agency or at its request or under its supervision or control are not used in such a way as to further any military purpose; and to apply safeguards, at the request of the parties, to any bilateral or multilateral arrangement, or at the request of a State, to any of that State's activities in the field of atomic energy.\"\n\nThe Department of Safeguards is responsible for carrying out this mission, through technical measures designed to verify the correctness and completeness of states' nuclear declarations.\n\nThe IAEA classifies safety as one of its top three priorities. It spends 8.9 percent of its 352 million-euro ($469 million) regular budget in 2011 on making plants secure from accidents. Its resources are used on the other two priorities: technical co-operation and preventing\nnuclear weapons proliferation.\n\nThe IAEA itself says that, beginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA redoubled its efforts in the field of nuclear safety. The IAEA says that the same happened after the Fukushima disaster in Fukushima, Japan.\n\nIn June 2011, the IAEA chief said he had \"broad support for his plan to strengthen international safety checks on nuclear power plants to help avoid any repeat of Japan's Fukushima crisis\". Peer-reviewed safety checks on reactors worldwide, organised by the IAEA, have been proposed.\n\nRussian nuclear accident specialist Iouli Andreev is critical of the response to Fukushima, and says that the IAEA did not learn from the 1986 Chernobyl disaster. He has accused the IAEA and corporations of \"wilfully ignoring lessons from the world's worst nuclear accident 25 years ago to protect the industry's expansion\". The IAEA's role \"as an advocate for nuclear power has made it a target for protests\".\n\nThe journal \"Nature\" has reported that the IAEA response to the Fukushima I nuclear accidents in Japan was \"sluggish and sometimes confusing\", drawing calls for the agency to \"take a more proactive role in nuclear safety\". But nuclear experts say that the agency's complicated mandate and the constraints imposed by its member states mean that reforms will not happen quickly or easily, although its INES \"emergency scale is very likely to be revisited\" given the confusing way in which it was used in Japan.\n\nSome scientists say that the 2011 Japanese nuclear accidents have revealed that the nuclear industry lacks sufficient oversight, leading to renewed calls to redefine the mandate of the IAEA so that it can better police nuclear power plants worldwide. There are several problems with the IAEA says Najmedin Meshkati of University of Southern California:\n\nIt recommends safety standards, but member states are not required to comply; it promotes nuclear energy, but it also monitors nuclear use; it is the sole global organisation overseeing the nuclear energy industry, yet it is also weighed down by checking compliance with the Nuclear Non-Proliferation Treaty (NPT).\nThe journal \"Nature\" has reported that \"the world must strengthen the ability of the International Atomic Energy Agency to make independent assessments of nuclear safety\" and that \"the public would be better served by an IAEA more able to deliver frank and independent assessments of nuclear crises as they unfold\".\n\nThe process of joining the IAEA is fairly simple. Normally, a State would notify the Director General of its desire to join, and the Director would submit the application to the Board for consideration. If the Board recommends approval, and the General Conference approves the application for membership, the State must then submit its instrument of acceptance of the IAEA Statute to the United States, which functions as the depositary Government for the IAEA Statute. The State is considered a member when its acceptance letter is deposited. The United States then informs the IAEA, which notifies other IAEA Member States. Signature and ratification of the Nuclear Non-Proliferation Treaty (NPT) are not preconditions for membership in the IAEA.\n\nThe IAEA has 170 member states. Most UN members and the Holy See are Member States of the IAEA. Non-member states Cape Verde (2007), Tonga (2011), Comoros (2014), Gambia (2016) and Saint Lucia (2016) have been approved for membership and will become a Member State if they deposit the necessary legal instruments.\n\nFour states have withdrawn from the IAEA. North Korea was a Member State from 1974 to 1994, but withdrew after the Board of Governors found it in non-compliance with its safeguards agreement and suspended most technical co-operation. Nicaragua became a member in 1957, withdrew its membership in 1970, and rejoined in 1977, Honduras joined in 1957, withdrew in 1967, and rejoined in 2003, while Cambodia joined in 1958, withdrew in 2003, and rejoined in 2009.\n\nThere are four regional cooperative areas within IAEA, that share information, and organize conferences within their regions:\n\nThe African Regional Cooperative Agreement for Research, Development and Training Related to Nuclear Science and Technology (AFRA):\nCooperative Agreement for Arab States in Asia for Research, Development and Training related to Nuclear Science and Technology (ARASIA):\nRegional Cooperative Agreement for Research, Development and Training Related to Nuclear Science and Technology for Asia and the Pacific (RCA):\nCooperation Agreement for the Promotion of Nuclear Science and Technology in Latin America and the Caribbean (ARCAL):\n\n\n \n"}
{"id": "14985", "url": "https://en.wikipedia.org/wiki?curid=14985", "title": "International Civil Aviation Organization", "text": "International Civil Aviation Organization\n\nThe International Civil Aviation Organization (ICAO; ), is a specialized agency of the United Nations. It codifies the principles and techniques of international air navigation and fosters the planning and development of international air transport to ensure safe and orderly growth. Its headquarters is located in the \"Quartier International\" of Montreal, Quebec, Canada.\n\nThe ICAO Council adopts standards and recommended practices concerning air navigation, its infrastructure, flight inspection, prevention of unlawful interference, and facilitation of border-crossing procedures for international civil aviation. ICAO defines the protocols for air accident investigation followed by in countries signatory to the Chicago Convention on International Civil Aviation.\n\nThe Air Navigation Commission (ANC) is the technical body within ICAO. The Commission is composed of 19 Commissioners, nominated by the ICAO's contracting states, and appointed by the ICAO Council. Commissioners serve as independent experts, who although nominated by their states, do not serve as state or political representatives. The development of international Standards And Recommended Practices is done under the direction of the ANC through the formal process of ICAO Panels. Once approved by the Commission, standards are sent to the Council, the political body of ICAO, for consultation and coordination with the Member States before final adoption.\n\nICAO is distinct from other international air transport organizations, like the International Air Transport Association (IATA), a trade association representing airlines; the Civil Air Navigation Services Organization (CANSO), an organization for Air navigation service providers (ANSPs); and the Airports Council International, a trade association of airport authorities.\n\nThe forerunner to ICAO was the International Commission for Air Navigation (ICAN). It held its first convention in 1903 in Berlin, Germany, but no agreements were reached among the eight countries that attended. At the second convention in 1906, also held in Berlin, 27 countries attended. The third convention, held in London in 1912 allocated the first radio callsigns for use by aircraft. ICAN continued to operate until 1945.\n\nFifty-two countries signed the Chicago Convention on International Civil Aviation, also known as the Chicago Convention, in Chicago, Illinois, on 7 December 1944. Under its terms, a Provisional International Civil Aviation Organization (PICAO) was to be established, to be replaced in turn by a permanent organization when 26 countries ratified the convention. Accordingly, PICAO began operating on 6 June 1945, replacing ICAN. The 26th country ratified the Convention on 5 March 1947 and, consequently PICAO was disestablished on 4 April 1947 and replaced by ICAO, which began operations the same day. In October 1947, ICAO became an agency of the United Nations linked to the United Nations Economic and Social Council (ECOSOC).\n\nIn April 2013 Qatar offered to serve as the new permanent seat of the Organization. Qatar promised to construct a massive new headquarters for ICAO and cover all moving expenses, stating that Montreal \"was too far from Europe and Asia\", \"had cold winters,\" was hard to attend due to the refusal of the Canadian government to provide visas in a timely manner, and that the taxes imposed on ICAO by Canada were too high. According to \"The Globe and Mail\", Qatar's move was at least partly motivated by the pro-Israel foreign policy of Canadian Prime Minister Stephen Harper. Approximately one month later, Qatar withdrew its bid after a separate proposal to the ICAO's governing council to move the ICAO triennial conference to Doha was defeated by a vote of 22–14.\n\nThe 9th edition of the Convention on International Civil Aviation includes modifications from 1948 up to year 2006. ICAO refers to its current edition of the Convention as the \"Statute\", and designates it as ICAO Document 7300/9. The Convention has 19 Annexes that are listed by title in the article Convention on International Civil Aviation.\n\n, there are 192 ICAO members, consisting of 191 of the 193 UN members (all but Dominica, Liechtenstein), plus the Cook Islands.\n\nLiechtenstein has delegated Switzerland to implement the treaty to make it applicable in the territory of Liechtenstein.\n\nThe Republic of China was a founding member of ICAO but was replaced by People's Republic of China as the legal representative of China in 1971 and as such, did not take part in the organization. In 2013, the Republic of China was for the first time invited to attend 38th session of ICAO Assembly as a guest under the name of Chinese Taipei.\n\nThe Council of ICAO is elected by the Assembly every 3 years and consists of 36 members elected in 3 groups. The present Council was elected on 4 October 2016 at the 39th Assembly of ICAO at Montreal. \nThe structure of the present Council is as follows:\n\nICAO also standardizes certain functions for use in the airline industry, such as the Aeronautical Message Handling System (AMHS). This makes it a standards organization.\n\nEach country should have an accessible Aeronautical Information Publication (AIP), based on standards defined by ICAO, containing information essential to air navigation. Countries are required to update their AIP manuals every 28 days and so provide definitive regulations, procedures and information for each country about airspace and airports. ICAO's standards also dictate that temporary hazards to aircraft are regularly published using NOTAMs.\n\nICAO defines an International Standard Atmosphere (also known as ICAO Standard Atmosphere), a model of the standard variation of pressure, temperature, density, and viscosity with altitude in the Earth's atmosphere. This is useful in calibrating instruments and designing aircraft.\n\nICAO standardizes machine-readable passports worldwide. Such passports have an area where some of the information otherwise written in textual form is written as strings of alphanumeric characters, printed in a manner suitable for optical character recognition. This enables border controllers and other law enforcement agents to process such passports quickly, without having to enter the information manually into a computer. ICAO publishes Document 9303 \"Machine Readable Travel Documents\", the technical standard for machine-readable passports. A more recent standard is for biometric passports. These contain biometrics to authenticate the identity of travellers. The passport's critical information is stored on a tiny RFID computer chip, much like information stored on smartcards. Like some smartcards, the passport book design calls for an embedded contactless chip that is able to hold digital signature data to ensure the integrity of the passport and the biometric data.\n\nICAO is active in infrastructure management, including Communication, navigation and surveillance / Air Traffic Management (CNS/ATM) systems, which employ digital technologies (like satellite systems with various levels of automation) in order to maintain a seamless global air traffic management system.\n\nBoth ICAO and IATA have their own airport and airline code systems.\nICAO uses 4-letter airport codes (vs. IATA's 3-letter codes). The ICAO code is based on the region and country of the airport—for example, Charles de Gaulle Airport has an ICAO code of LFPG, where L indicates Southern Europe, F, France, PG, Paris de Gaulle, while Orly Airport has the code LFPO (the 3rd letter sometimes refers to the particular flight information region (FIR) or the last two may be arbitrary). In most parts of the world, ICAO and IATA codes are unrelated; for example, Charles de Gaulle Airport has an IATA code of CDG. However, the location prefix for continental United States is K and ICAO codes are usually the IATA code with this prefix. For example, the ICAO code for Los Angeles International Airport is KLAX. Canada follows a similar pattern, where a prefix of C is usually added to an IATA code to create the ICAO code. For example, Calgary International Airport is YYC or CYYC. (In contrast, airports in Hawaii are in the Pacific region and so have ICAO codes that start with PH; Kona International Airport's code is PHKO. Similarly, airports in Alaska have ICAO codes that start with PA. Merrill Field, for instance is PAMR.) Note that not all airports are assigned codes in both systems; for example, airports that do not have airline service do not need an IATA code.\n\nICAO also assigns 3-letter airline codes (versus the more-familiar 2-letter IATA codes—for example, UAL vs. UA for United Airlines). ICAO also provides telephony designators to aircraft operators worldwide, a one- or two-word designator used on the radio, usually, but not always, similar to the aircraft operator name. For example, the identifier for Japan Airlines International is JAL and the designator is Japan Air, but Aer Lingus is EIN and Shamrock. Thus, a Japan Airlines flight numbered 111 would be written as \"JAL111\" and pronounced \"Japan Air One One One\" on the radio, while a similarly numbered Aer Lingus would be written as \"EIN111\" and pronounced \"Shamrock One One One\". In the US, FAA practices require the digits of the flight number to be spoken in group format (\"Japan Air One Eleven\" in the above example) while individual digits are used for the aircraft tail number used for unscheduled civil flights.\n\nICAO maintains the standards for aircraft registration (\"tail numbers\"), including the alphanumeric codes that identify the country of registration. For example, airplanes registered in the United States have tail numbers starting with N.\n\nICAO is also responsible for issuing 2-4 character alphanumeric aircraft type codes. These codes provide an abbreviated aircraft type identification, typically used in flight plans. For example, the Boeing 747-100, -200 and -300 are given the type codes B741, B742 and B743 respectively.\n\nICAO has a headquarters, seven regional offices, and one regional sub-office:\n\nEmissions from international aviation are specifically excluded from the targets agreed under the Kyoto Protocol. Instead, the Protocol invites developed countries to pursue the limitation or reduction of emissions through the International Civil Aviation Organization. ICAO's environmental committee continues to consider the potential for using market-based measures such as trading and charging, but this work is unlikely to lead to global action. It is currently developing guidance for states who wish to include aviation in an emissions trading scheme (ETS) to meet their Kyoto commitments, and for airlines who wish to participate voluntarily in a trading scheme.\n\nEmissions from domestic aviation are included within the Kyoto targets agreed by countries. This has led to some national policies such as fuel and emission taxes for domestic air travel in the Netherlands and Norway, respectively. Although some countries tax the fuel used by domestic aviation, there is no duty on kerosene used on international flights.\n\nICAO is currently opposed to the inclusion of aviation in the European Union Emission Trading Scheme (EU ETS). The EU, however, is pressing ahead with its plans to include aviation.\n\nOn October 6, 2016, the ICAO finalized an agreement among its 191 member nations to address the more than of carbon dioxide emitted annually by international passenger and cargo flights. The agreement will use an offsetting scheme called CORSIA (the Carbon Offsetting and Reduction Scheme for International Aviation) under which forestry and other carbon-reducing activities are directly funded, amounting to about 2% of annual revenues for the sector. Rules against 'double counting' should ensure that existing forest protection efforts are not recycled. The scheme does not take effect until 2021 and will be voluntary until 2027, but many countries, including the US and China, have promised to begin at its 2020 inception date. Under the agreement, the global aviation emissions target is a 50% reduction by 2050 relative to 2005. NGO reaction to the deal was mixed.\n\nThe agreement has critics. It is not aligned with the 2015 Paris climate agreement, which set the objective of restricting global warming to 1.5 to 2 °C. A late draft of the agreement would have required the air transport industry to assess its share of global carbon budgeting to meet that objective, but the text was removed in the agreed version. CORSIA will regulate only about 25 percent of aviation's international emissions, since it grandfathers all emissions below the 2020 level, allowing unregulated growth until then. Only 65 nations will participate in the initial voluntary period, not including significant emitters Russia, India and perhaps Brazil. The agreement does not cover domestic emissions, which are 40% of the global industry's overall emissions. One observer of the ICAO convention made this summary: although another critic called it \"a timid step in the right direction.\"\n\nMost air accident investigations are carried out by an agency of a country that is associated in some way with the accident. For example, the Air Accidents Investigation Branch conducts accident investigations on behalf of the British Government. ICAO has conducted four investigations involving air disasters, of which two were passenger airliners shot down while in international flight over hostile territory.\n\nICAO is looking at having a singular ledger for drone registration to help law enforcement globally. Currently, ICAO is responsible for creating drone regulations across the globe, and it is expected that it will only maintain the registry. This activity is seen as a forerunner to global regulations on the drone flying under the auspices of the ICAO.\n\nICAO currently maintains the 'UAS Regulation Portal' for various countries to list their country's UAS regulations and also review the best practices from across the globe.\n\n\n"}
{"id": "14986", "url": "https://en.wikipedia.org/wiki?curid=14986", "title": "International Maritime Organization", "text": "International Maritime Organization\n\nThe International Maritime Organization (IMO), known as the Inter-Governmental Maritime Consultative Organization (IMCO) until 1982, is a specialised agency of the United Nations responsible for regulating shipping. The IMO was established following agreement at a UN conference held in Geneva in 1948 and the IMO came into existence ten years later, meeting for the first time in 1959. Headquartered in London, United Kingdom, the IMO currently has 174 member states and three associate members.\n\nThe IMO's primary purpose is to develop and maintain a comprehensive regulatory framework for shipping and its remit today includes safety, environmental concerns, legal matters, technical co-operation, maritime security and the efficiency of shipping. IMO is governed by an assembly of members and is financially administered by a council of members elected from the assembly. The work of IMO is conducted through five committees and these are supported by technical subcommittees. Other UN organisations may observe the proceedings of the IMO. Observer status is granted to qualified non-governmental organisations.\n\nIMO is supported by a permanent secretariat of employees who are representative of the organisation's members. The secretariat is composed of a Secretary-General who is periodically elected by the assembly, and various divisions such as those for marine safety, environmental protection and a conference section.\n\nInter-Governmental Maritime Consultative Organization (IMCO) was formed in order to bring the regulation of the safety of shipping into an international framework, for which the creation of the United Nations provided an opportunity. Hitherto such international conventions had been initiated piecemeal, notably the Safety of Life at Sea Convention (SOLAS), first adopted in 1914 following the \"Titanic\" disaster. IMCO's first task was to update that convention; the resulting 1960 convention was subsequently recast and updated in 1974 and it is that convention that has been subsequently modified and updated to adapt to changes in safety requirements and technology.\n\nWhen IMCO began its operations in 1959 certain other pre-existing conventions were brought under its aegis, most notable the International Convention for the Prevention of Pollution of the Sea by Oil (OILPOL) 1954. The first meetings of the newly formed IMCO were held in London in 1959. Throughout its existence IMCO, later renamed the IMO in 1982, has continued to produce new and updated conventions across a wide range of maritime issues covering not only safety of life and marine pollution but also encompassing safe navigation, search and rescue, wreck removal, tonnage measurement, liability and compensation, ship recycling, the training and certification of seafarers, and piracy. More recently SOLAS has been amended to bring an increased focus on maritime security through the International Ship and Port Facility Security (ISPS) Code. The IMO has also increased its focus on smoke emissions from ships.\n\nIn January 1959, IMO began to maintain and promote the 1954 OILPOL Convention. Under the guidance of IMO, the convention was amended in 1962, 1969, and 1971.\n\nAs oil trade and industry developed, many people in the industry began to recognise a need for further improvements in regards to oil pollution prevention at sea. This became increasingly apparent in 1967, when the tanker \"Torrey Canyon\" spilled 120,000 tons of crude oil when it ran aground entering the English Channel\n\nThe \"Torrey Canyon\" grounding was the largest oil pollution incident recorded up to that time. This incident prompted a series of new conventions.\n\nIMO held an emergency session of its Council to deal with the need to readdress regulations pertaining to maritime pollution. In 1969, the IMO Assembly decided to host an international gathering in 1973 dedicated to this issue. The goal at hand was to develop an international agreement for controlling general environmental contamination by ships when out at sea.\n\nDuring the next few years IMO brought to the forefront a series of measures designed to prevent large ship accidents and to minimise their effects. It also detailed how to deal with the environmental threat caused by routine ship duties such as the cleaning of oil cargo tanks or the disposal of engine room wastes. By tonnage, the aforementioned was a bigger problem than accidental pollution.\n\nThe most significant thing to come out of this conference was the International Convention for the Prevention of Pollution from Ships, 1973. It covers not only accidental and operational oil pollution but also different types of pollution by chemicals, goods in packaged form, sewage, garbage and air pollution.\n\nThe original MARPOL was signed on 17 February 1973, but did not come into force due to lack of ratifications. The current convention is a combination of 1973 Convention and the 1978 Protocol. It entered into force on 2 October 1983. As of May 2013, 152 states, representing 99.2 per cent of the world's shipping tonnage, are involved in the convention.\n\nIn 1983 the IMO established the World Maritime University in Malmö, Sweden.\n\nThe IMO headquarters are located in a large purpose-built building facing the River Thames on the Albert Embankment, in Lambeth, London. The organisation moved into its new headquarters in late 1982, with the building being officially opened by Queen Elizabeth II on 17 May 1983. The architects of the building were Douglass Marriott, Worby & Robinson. The front of the building is dominated by a seven-metre high, ten-tonne bronze sculpture of the bow of a ship, with a lone seafarer maintaining a look-out. The previous headquarters of IMO were at 101 Piccadilly (now the home of the Embassy of Japan), prior to that at 22 Berners Street in Fitzrovia and originally in Chancery Lane.\n\nTo become a member of the IMO, a state ratifies a multilateral treaty known as the Convention on the International Maritime Organization. As of 2018, there are 173 member states of the IMO, which includes 172 of the UN member states plus the Cook Islands. The first state to ratify the convention was the United Kingdom in 1949. The most recent members to join were Armenia and Nauru, which became IMO members in January and May 2018, respectively.\n\nThe three associate members of the IMO are the Faroe Islands, Hong Kong and Macao.\n\nMost UN member states that are not members of IMO are landlocked countries. These include Afghanistan, Andorra, Bhutan, Botswana, Burkina Faso, Burundi, Central African Republic, Chad, Kyrgyzstan, Laos, Lesotho, Liechtenstein, Mali, Niger, Rwanda, South Sudan, Swaziland, Tajikistan and Uzbekistan. However, the Federated States of Micronesia, an island nation in the Pacific Ocean, is also a non-member.\n\nThe IMO consists of an Assembly, a Council and five main Committees: the Maritime Safety Committee; the Marine Environment Protection Committee; the Legal Committee; the Technical Co-operation Committee and the Facilitation Committee. A number of Sub-Committees support the work of the main technical committees.\n\nIMO is the source of approximately 60 legal instruments that guide the regulatory development of its member states to improve safety at sea, facilitate trade among seafaring states and protect the maritime environment. The most well known is the International Convention for the Safety of Life at Sea (SOLAS), as well as International Convention on Oil Pollution Preparedness, Response and Co-operation (OPRC). Others include the International Oil Pollution Compensation Funds (IOPC). It also functions as a depository of yet to be ratified treaties, such as the International Convention on Liability and Compensation for Damage in Connection with the Carriage of Hazardous and Noxious Substances by Sea, 1996 (HNS Convention) and Nairobi International Convention of Removal of Wrecks (2007).\n\nIMO regularly enacts regulations, which are broadly enforced by national and local maritime authorities in member countries, such as the International Regulations for Preventing Collisions at Sea (COLREG). The IMO has also enacted a Port State Control (PSC) authority, allowing domestic maritime authorities such as coast guards to inspect foreign-flag ships calling at ports of the many port states. Memoranda of Understanding (protocols) were signed by some countries unifying Port State Control procedures among the signatories.\n\nConventions, Codes and Regulations:\n\nRecent initiatives at the IMO have included amendments to SOLAS, which upgraded fire protection standards on passenger ships, the International Convention on Standards of Training, Certification and Watchkeeping for Seafarers (STCW) which establishes basic requirements on training, certification and watchkeeping for seafarers and to the Convention on the Prevention of Maritime Pollution (MARPOL 73/78), which required double hulls on all tankers.\n\nIn December 2002, new amendments to the 1974 SOLAS Convention were enacted. These amendments gave rise to the International Ship and Port Facility Security (ISPS) Code, which went into effect on 1 July 2004. The concept of the code is to provide layered and redundant defences against smuggling, terrorism, piracy, stowaways, etc. The ISPS Code required most ships and port facilities engaged in international trade to establish and maintain strict security procedures as specified in ship and port specific Ship Security Plans and Port Facility Security Plans.\n\nThe IMO has a role in tackling international climate change. The First Intersessional Meeting of IMO's Working Group on Greenhouse Gas Emissions from Ships took place in Oslo, Norway (23–27 June 2008), tasked with developing the technical basis for the reduction mechanisms that may form part of a future IMO regime to control greenhouse gas emissions from international shipping, and a draft of the actual reduction mechanisms themselves, for further consideration by IMO's Marine Environment Protection Committee (MEPC). The IMO participated in the 2015 United Nations Climate Change Conference in Paris seeking to establish itself as the \"appropriate international body to address greenhouse gas emissions from ships engaged in international trade\". Nonetheless, there has been widespread criticism of the IMO's relative inaction since the conclusion of the Paris conference, with the initial data-gathering step of a three-stage process to reduce maritime greenhouse emissions expected to last until 2020. The IMO has also taken action to mitigate the global effects of ballast water and sediment discharge, through the 2004 Ballast Water Management Convention, which entered into force in September 2017.\n\nThe IMO is also responsible for publishing the International Code of Signals for use between merchant and naval vessels. IMO has harmonised information available to seafarers and shore-side traffic services called e-Navigation. An e-Navigation strategy was ratified in 2005, and an implementation plan was developed through three IMO sub-committees. The plan was completed by 2014 and implemented in November of that year. IMO has also served as a key partner and enabler of US international and interagency efforts to establish Maritime Domain Awareness.\n\nOn 4 December 2018 it was reported that at least 34 member countries of the International Maritime Organization supported Ukraine with respect to Russia’s armed military attack on Ukrainian boats on the Ukrainian and international waters near the Kerch Strait that occurred on 25 November 2018.\n\nThe governing body of the International Maritime Organization is the Assembly which meets every two years. In between Assembly sessions a Council, consisting of 40 Member States elected by the Assembly, acts as the governing body. The technical work of the International Maritime Organization is carried out by a series of Committees. The Secretariat consists of some 300 international civil servants headed by a Secretary-General.\n\nThe current Secretary-General is Kitack Lim (South Korea), elected for a four-year term at the 106th session of the IMO Council in June 2015 and at the 27th session of the IMO's Assembly in November 2015. His mandate started on 1 January 2016.\n\nPrevious Secretaries-General:\n\nThe technical work of the International Maritime Organisation is carried out by a series of Committees. These include:\n\nIt is regulated in the Article 28(a) of the Convention on the IMO:\n\nThe Maritime Safety Committee is the most senior of these and is the main Technical Committee; it oversees the work of its nine sub-committees and initiates new topics. One broad topic it deals with is the effect of the human element on casualties; this work has been put to all of the sub-committees, but meanwhile, the Maritime Safety Committee has developed a code for the management of ships which will ensure that agreed operational procedures are in place and followed by the ship and shore-side staff.\n\nThe MSC and MEPC are assisted in their work by a number of sub-committees which are open to all Member States. The committees are:\n\n\nThe names of the IMO sub-committees were changed in 2013. Prior to 2013 there were nine Sub-Committees as follows:\n\n\nResolution MSC.255(84), of 16 May 2008, adopts the \"Code of the International Standards and Recommended Practices for a Safety Investigation into a Marine casualty or Marine Incident\". It is also known as the Casualty Investigation Code.\n\n\n\n"}
