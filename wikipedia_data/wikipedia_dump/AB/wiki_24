{"id": "15215", "url": "https://en.wikipedia.org/wiki?curid=15215", "title": "Internet Explorer", "text": "Internet Explorer\n\nInternet Explorer (formerly Microsoft Internet Explorer and Windows Internet Explorer, commonly referred to as Explorer and abbreviated IE or MSIE) is a series of graphical web browsers developed by Microsoft and included in the Microsoft Windows line of operating systems, starting in 1995. It was first released as part of the add-on package Plus! for Windows 95 that year. Later versions were available as free downloads, or in service packs, and included in the original equipment manufacturer (OEM) service releases of Windows 95 and later versions of Windows. The browser is discontinued, but still maintained.\n\nInternet Explorer was one of the most widely used web browsers, attaining a peak of about 95% usage share by 2003. This came after Microsoft used bundling to win the first browser war against Netscape, which was the dominant browser in the 1990s. Its usage share has since declined with the launch of Firefox (2004) and Google Chrome (2008), and with the growing popularity of operating systems such as Android and iOS that do not run Internet Explorer. Estimates for Internet Explorer's market share are about 2.55% across all platforms or by StatCounter's numbers ranked 6th, while on desktop, the only platform it's ever had significant share (i.e. excluding mobile and Xbox) it's ranked 3rd at 5.4%, just after Firefox (others place IE 2nd with 9.48% just ahead of), (browser market share is notoriously difficult to calculate). Microsoft spent over per year on Internet Explorer in the late 1990s, with over 1,000 people involved in the project by 1999.\n\nVersions of Internet Explorer for other operating systems have also been produced, including an Xbox 360 version called Internet Explorer for Xbox and for platforms Microsoft no longer supports: Internet Explorer for Mac and Internet Explorer for UNIX (Solaris and HP-UX), and an embedded OEM version called Pocket Internet Explorer, later rebranded Internet Explorer Mobile made for Windows Phone, Windows CE, and previously, based on Internet Explorer 7 for Windows Mobile.\n\nOn March 17, 2015, Microsoft announced that Microsoft Edge would replace Internet Explorer as the default browser on its Windows 10 devices. This effectively makes Internet Explorer 11 the last release (however IE 10 and 9 also receive security updates as of 2018). Internet Explorer, however, remains on Windows 10 primarily for enterprise purposes. Since January 12, 2016, only Internet Explorer 11 has been supported. Support varies based on the operating system's technical capabilities and its support lifecycle.\n\nThe browser has been scrutinized throughout its development for use of third-party technology (such as the source code of Spyglass Mosaic, used without royalty in early versions) and security and privacy vulnerabilities, and the United States and the European Union have alleged that integration of Internet Explorer with Windows has been to the detriment of fair browser competition.\n\nThe Internet Explorer project was started in the summer of 1994 by Thomas Reardon, who, according to the Massachusetts Institute of Technology Review of 2003, used source code from Spyglass, Inc. Mosaic, which was an early commercial web browser with formal ties to the pioneering National Center for Supercomputing Applications (\"NCSA\") Mosaic browser. In late 1994, Microsoft licensed Spyglass Mosaic for a quarterly fee plus a percentage of Microsoft's non-Windows revenues for the software. Although bearing a name similar to NCSA Mosaic, Spyglass Mosaic had used the NCSA Mosaic source code sparingly.\n\nThe first version, dubbed Microsoft Internet Explorer, made its debut on August 16, 1995. It was installed as part of the \"Internet Jumpstart Kit\" in Microsoft Plus! for Windows 95 and Plus!. The Internet Explorer team began with about six people in early development. Internet Explorer 1.5 was released several months later for Windows NT and added support for basic table rendering. By including it free of charge on their operating system, they did not have to pay royalties to Spyglass Inc, resulting in a lawsuit and a US$8 million settlement on January 22, 1997.\n\nMicrosoft was sued by Synet Inc. in 1996, over the trademark infringement.\n\nInternet Explorer 11 is featured in a Windows 8.1 update which was released on October 17, 2013. It includes an incomplete mechanism for syncing tabs. It is a major update to its developer tools, enhanced scaling for high DPI screens, HTML5 prerender and prefetch, hardware-accelerated JPEG decoding, closed captioning, HTML5 full screen, and is the first Internet Explorer to support WebGL and Google's protocol SPDY (starting at v3). This version of IE has features dedicated to Windows 8.1, including cryptography (WebCrypto), adaptive bitrate streaming (Media Source Extensions) and Encrypted Media Extensions.\n\nInternet Explorer 11 was made available for Windows 7 users to download on November 7, 2013, with Automatic Updates in the following weeks.\n\nInternet Explorer 11's user agent string now identifies the agent as \"Trident\" (the underlying layout engine) instead of \"MSIE\". It also announces compatibility with Gecko (the layout engine of Firefox).\n\nMicrosoft claimed that Internet Explorer 11, running the WebKit SunSpider JavaScript Benchmark, was the fastest browser as of October 15, 2013.\n\nMicrosoft Edge, officially unveiled on January 21, 2015, has replaced Internet Explorer as the default browser on Windows 10. Internet Explorer is still installed in Windows 10 in order to maintain compatibility with older websites and intranet sites that require ActiveX and other Microsoft legacy web technologies.\n\nAccording to Microsoft, development of new features for Internet Explorer has ceased. However, it will continue to be maintained as part of the support policy for the versions of Windows with which it is included.\n\nInternet Explorer has been designed to view a broad range of web pages and provide certain features within the operating system, including Microsoft Update. During the heyday of the browser wars, Internet Explorer superseded Netscape only when it caught up technologically to support the progressive features of the time.\n\nInternet Explorer, using the Trident layout engine:\n\n\nInternet Explorer uses DOCTYPE sniffing to choose between standards mode and a \"quirks mode\" in which it deliberately mimicks nonstandard behaviours of old versions of MSIE for HTML and CSS rendering on screen (Internet Explorer always uses standards mode for printing). It also provides its own dialect of ECMAScript called JScript.\n\nInternet Explorer was criticised by Tim Berners-Lee for its limited support for SVG which is promoted by W3C.\n\nInternet Explorer has introduced an array of proprietary extensions to many of the standards, including HTML, CSS, and the DOM. This has resulted in a number of web pages that appear broken in standards-compliant web browsers and has introduced the need for a \"quirks mode\" to allow for rendering improper elements meant for Internet Explorer in these other browsers.\n\nInternet Explorer has introduced a number of extensions to the DOM that have been adopted by other browsers. These include the innerHTML property, which provides access to the HTML string within an element; the XMLHttpRequest object, which allows the sending of HTTP request and receiving of HTTP response, and may be used to perform AJAX; and the designMode attribute of the contentDocument object, which enables rich text editing of HTML documents. Some of these functionalities were not possible until the introduction of the W3C DOM methods. Its Ruby character extension to HTML is also accepted as a module in W3C XHTML 1.1, though it is not found in all versions of W3C HTML.\n\nMicrosoft submitted several other features of IE for consideration by the W3C for standardization. These include the 'behaviour' CSS property, which connects the HTML elements with JScript behaviours (known as HTML Components, HTC); HTML+TIME profile, which adds timing and media synchronization support to HTML documents (similar to the W3C XHTML+SMIL), and the VML vector graphics file format. However, all were rejected, at least in their original forms; VML was subsequently combined with PGML (proposed by Adobe and Sun), resulting in the W3C-approved SVG format, one of the few vector image formats being used on the web, which IE did not support until version 9.\n\nOther non-standard behaviours include: support for vertical text, but in a syntax different from W3C CSS3 candidate recommendation, support for a variety of image effects and page transitions, which are not found in W3C CSS, support for obfuscated script code, in particular JScript.Encode. Support for embedding EOT fonts in web pages.\n\nSupport for favicons was first added in Internet Explorer 5. Internet Explorer supports favicons in PNG, static GIF and native Windows icon formats. In Windows Vista and later, Internet Explorer can display native Windows icons that have embedded PNG files.\n\nInternet Explorer makes use of the accessibility framework provided in Windows. Internet Explorer is also a user interface for FTP, with operations similar to that of Windows Explorer. Pop-up blocking and tabbed browsing were added respectively in Internet Explorer 6 and Internet Explorer 7. Tabbed browsing can also be added to older versions by installing MSN Search Toolbar or Yahoo Toolbar.\n\nInternet Explorer caches visited content in the Temporary Internet Files folder to allow quicker access (or offline access) to previously visited pages. The content is indexed in a database file, known as Index.dat. Multiple Index.dat files exist which index different content—visited content, web feeds, visited URLs, cookies, etc.\n\nPrior to IE7, clearing the cache used to clear the index but the files themselves were not reliably removed, posing a potential security and privacy risk. In IE7 and later, when the cache is cleared, the cache files are more reliably removed, and the index.dat file is overwritten with null bytes.\n\nCaching has been improved in IE9.\n\nInternet Explorer is fully configurable using Group Policy. Administrators of Windows Server domains (for domain-joined computers) or the local computer can apply and enforce a variety of settings on computers that affect the user interface (such as disabling menu items and individual configuration options), as well as underlying security features such as downloading of files, zone configuration, per-site settings, ActiveX control behaviour and others. Policy settings can be configured for each user and for each machine. Internet Explorer also supports Integrated Windows Authentication.\n\nInternet Explorer uses a componentized architecture built on the Component Object Model (COM) technology. It consists of several major components, each of which is contained in a separate Dynamic-link library (DLL) and exposes a set of COM programming interfaces hosted by the Internet Explorer main executable, iexplore.exe:\n\nInternet Explorer does not include any native scripting functionality. Rather, MSHTML.dll exposes an API that permits a programmer to develop a scripting environment to be plugged-in and to access the DOM tree. Internet Explorer 8 includes the bindings for the Active Scripting engine, which is a part of Microsoft Windows and allows any language implemented as an Active Scripting module to be used for client-side scripting. By default, only the JScript and VBScript modules are provided; third party implementations like ScreamingMonkey (for ECMAScript 4 support) can also be used. Microsoft also makes available the Microsoft Silverlight runtime (not supported in Windows RT) that allows CLI languages, including DLR-based dynamic languages like IronPython and IronRuby, to be used for client-side scripting.\n\nInternet Explorer 8 introduces some major architectural changes, called \"Loosely Coupled IE\" (LCIE). LCIE separates the main window process (frame process) from the processes hosting the different web applications in different tabs (tab processes). A frame process can create multiple tab processes, each of which can be of a different integrity level; each tab process can host multiple web sites. The processes use asynchronous Inter-Process Communication to synchronize themselves. Generally, there will be a single frame process for all web sites. In Windows Vista with Protected Mode turned on, however, opening privileged content (such as local HTML pages) will create a new tab process as it will not be constrained by Protected Mode.\n\nInternet Explorer exposes a set of Component Object Model (COM) interfaces that allows add-ons to extend the functionality of the browser. Extensibility is divided into two types: Browser extensibility and content extensibility. Browser extensibility involves adding context menu entries, toolbars, menu items or Browser Helper Objects (BHO). BHOs are used to extend the feature set of the browser, whereas the other extensibility options are used to expose that feature in the user interface. Content extensibility adds support for non-native content formats. It allows Internet Explorer to handle new file formats and new protocols, e.g. WebM or SPDY. In addition, web pages can integrate widgets known as ActiveX controls which run on Windows only but have vast potentials to extend the content capabilities; Adobe Flash Player and Microsoft Silverlight are examples. Add-ons can be installed either locally, or directly by a web site.\n\nSince malicious add-ons can compromise the security of a system, Internet Explorer implements several safeguards. Internet Explorer 6 with Service Pack 2 and later feature an Add-on Manager for enabling or disabling individual add-ons, complemented by a \"No Add-Ons\" mode. Starting with Windows Vista, Internet Explorer and its BHOs run with restricted privileges and are isolated from the rest of the system. Internet Explorer 9 introduced a new component – Add-on Performance Advisor. Add-on Performance Advisor shows a notification when one or more of installed add-ons exceed a pre-set performance threshold. The notification appears in the Notification Bar when the user launches the browser. Windows 8 and Windows RT introduce a Metro-style version of Internet Explorer that is entirely sandboxed and does not run add-ons at all. In addition, Windows RT cannot download or install ActiveX controls at all; although existing ones bundled with Windows RT still run in the traditional version of Internet Explorer.\n\nInternet Explorer itself can be hosted by other applications via a set of COM interfaces. This can be used to embed the browser functionality inside a computer program or create Internet Explorer shells.\n\nInternet Explorer uses a zone-based security framework that groups sites based on certain conditions, including whether it is an Internet- or intranet-based site as well as a user-editable whitelist. Security restrictions are applied per zone; all the sites in a zone are subject to the restrictions.\n\nInternet Explorer 6 SP2 onwards uses the \"Attachment Execution Service\" of Microsoft Windows to mark executable files downloaded from the Internet as being potentially unsafe. Accessing files marked as such will prompt the user to make an explicit trust decision to execute the file, as executables originating from the Internet can be potentially unsafe. This helps in preventing accidental installation of malware.\n\nInternet Explorer 7 introduced the phishing filter, that restricts access to phishing sites unless the user overrides the decision. With version 8, it also blocks access to sites known to host malware. Downloads are also checked to see if they are known to be malware-infected.\n\nIn Windows Vista, Internet Explorer by default runs in what is called \"Protected Mode\", where the privileges of the browser itself are severely restricted—it cannot make any system-wide changes. One can optionally turn this mode off but this is not recommended. This also effectively restricts the privileges of any add-ons. As a result, even if the browser or any add-on is compromised, the damage the security breach can cause is limited.\n\nPatches and updates to the browser are released periodically and made available through the Windows Update service, as well as through Automatic Updates. Although security patches continue to be released for a range of platforms, most feature additions and security infrastructure improvements are only made available on operating systems which are in Microsoft's mainstream support phase.\n\nOn December 16, 2008, Trend Micro recommended users switch to rival browsers until an emergency patch was released to fix a potential security risk which \"could allow outside users to take control of a person's computer and steal their passwords\". Microsoft representatives countered this recommendation, claiming that \"0.02% of internet sites\" were affected by the flaw. A fix for the issue was released the following day with the Security Update for Internet Explorer KB960714, on Microsoft Windows Update. \n\nIn 2011, a report by Accuvant, funded by Google, rated the security (based on sandboxing) of Internet Explorer worse than Google Chrome but better than Mozilla Firefox. \n\nA more recent browser security white paper comparing Google Chrome, Microsoft Edge, and Internet Explorer 11 by X41 D-Sec in 2017 came to similar conclusions, also based on sandboxing and support of legacy web technologies.\n\nInternet Explorer has been subjected to many security vulnerabilities and concerns: much of the spyware, adware, and computer viruses across the Internet are made possible by exploitable bugs and flaws in the security architecture of Internet Explorer, sometimes requiring nothing more than viewing of a malicious web page in order to install themselves. This is known as a \"drive-by install\". There are also attempts to trick the user into installing malicious software by misrepresenting the software's true purpose in the description section of an ActiveX security alert.\n\nA number of security flaws affecting IE originated not in the browser itself, but ActiveX-based add-ons used by it. Because the add-ons have the same privilege as IE, the flaws can be as critical as browser flaws. This has led to the ActiveX-based architecture being criticized for being fault-prone. By 2005, some experts maintained that the dangers of ActiveX have been overstated and there were safeguards in place. In 2006, new techniques using automated testing found more than a hundred vulnerabilities in standard Microsoft ActiveX components. Security features introduced in Internet Explorer 7 mitigated some of these vulnerabilities.\n\nInternet Explorer in 2008, had a number of published security vulnerabilities. According to research done by security research firm Secunia, Microsoft did not respond as quickly as its competitors in fixing security holes and making patches available. The firm also reported 366 vulnerabilities in ActiveX controls, an increase from the prior year.\n\nAccording to an October 2010 report in \"The Register\", researcher Chris Evans had detected a known security vulnerability which, then dating back to 2008, had not been fixed for at least 600 days. Microsoft says that it had known about this vulnerability but it was of very low severity as the victim web site must be configured in a special way for this attack to be feasible at all.\n\nIn December 2010, researchers were able to bypass the \"Protected Mode\" feature in Internet Explorer.\n\nIn an advisory on January 14, 2010, Microsoft said that attackers targeting Google and other U.S. companies used software that exploits a security hole, which had already been patched, in Internet Explorer. The vulnerability affected Internet Explorer 6 on Windows XP and Server 2003, IE6 SP1 on Windows 2000 SP4, IE7 on Windows Vista, XP, Server 2008 and Server 2003, and IE8 on Windows 7, Vista, XP, Server 2003, and Server 2008 (R2).\n\nThe German government warned users against using Internet Explorer and recommended switching to an alternative web browser, due to the major security hole described above that was exploited in Internet Explorer. The Australian and French Government issued a similar warning a few days later.\n\nOn April 26, 2014, Microsoft issued a security advisory relating to CVE-2014-1776 (use-after-free vulnerability in Microsoft Internet Explorer 6 through 11), a vulnerability that could allow \"remote code execution\" in Internet Explorer versions 6 to 11. On April 28, 2014, the United States Department of Homeland Security's United States Computer Emergency Readiness Team (US-CERT) released an advisory stating that the vulnerability could result in \"the complete compromise\" of an affected system. US-CERT recommended reviewing Microsoft's suggestions to mitigate an attack or using an alternate browser until the bug is fixed. The UK National Computer Emergency Response Team (CERT-UK) published an advisory announcing similar concerns and for users to take the additional step of ensuring their antivirus software is up-to-date. Symantec, a cyber security firm, confirmed that \"the vulnerability crashes Internet Explorer on Windows XP\". The vulnerability was resolved on May 1, 2014, with a security update.\n\nThe adoption rate of Internet Explorer seems to be closely related to that of Microsoft Windows, as it is the default web browser that comes with Windows. Since the integration of Internet Explorer 2.0 with Windows 95 OSR 1 in 1996, and especially after version 4.0's release in 1997, the adoption was greatly accelerated: from below 20% in 1996, to about 40% in 1998, and over 80% in 2000. This made Microsoft the winner in the infamous 'first browser war' against Netscape. Netscape Navigator was the dominant browser during 1995 and until 1997, but rapidly lost share to IE starting in 1998, and eventually slipped behind in 1999. The integration of IE with Windows led to a lawsuit by AOL, Netscape's owner, accusing Microsoft of unfair competition. The infamous case was eventually won by AOL but by then it was too late, as Internet Explorer had already become the dominant browser.\n\nInternet Explorer peaked during 2002 and 2003, with about 95% share. Its first notable competitor after beating Netscape was Firefox from Mozilla, which itself was an offshoot from Netscape.\n\nFirefox 1.0 had surpassed Internet Explorer 5 in early 2005, with Firefox 1.0 at roughly 8 percent market share.\n\nApproximate usage over time based on various usage share counters averaged for the year overall, or for the fourth quarter, or for the last month in the year depending on availability of reference.\n\nAccording to StatCounter Internet Explorer's marketshare fell below 50% in September 2010. In May 2012, Google Chrome overtook Internet Explorer as the most used browser worldwide, according to StatCounter. \n\nBrowser Helper Objects are also used by many search engine companies and third parties for creating add-ons that access their services, such as search engine toolbars. Because of the use of COM, it is possible to embed web-browsing functionality in third-party applications. Hence, there are a number of Internet Explorer shells, and a number of content-centric applications like RealPlayer also use Internet Explorer's web browsing module for viewing web pages within the applications.\n\nWhile a major upgrade of Internet Explorer can be uninstalled in a traditional way if the user has saved the original application files for installation, the matter of uninstalling the version of the browser that has shipped with an operating system remains a controversial one.\n\nThe idea of removing a stock install of Internet Explorer from a Windows system was proposed during the \"United States v. Microsoft Corp.\" case. One of Microsoft's arguments during the trial was that removing Internet Explorer from Windows may result in system instability. Indeed, programs that depend on libraries installed by IE, including Windows help and support system, fail to function without IE. Before Windows Vista, it was not possible to run Windows Update without IE because the service used ActiveX technology, which no other web browser supports.\n\nThe popularity of Internet Explorer has led to the appearance of malware abusing its name. On January 28, 2011, a fake Internet Explorer browser calling itself \"Internet Explorer – Emergency Mode\" appeared. It closely resembles the real Internet Explorer, but has fewer buttons and no search bar. If a user launches any other browser such as Google Chrome, Mozilla Firefox, Opera, Safari or the real Internet Explorer, this browser will pop-up instead. It also displays a fake error message, claiming that the computer is infected with malware and Internet Explorer has entered Emergency Mode. It blocks access to legitimate sites such as Google if the user tries to access them.\n\n\n\n"}
{"id": "15220", "url": "https://en.wikipedia.org/wiki?curid=15220", "title": "Imprecise language", "text": "Imprecise language\n\nOften, informal, spoken language, \"everyday language\" is less precise than any more formal or academic languages.\n\nLanguage might be said to be imprecise because it exhibits one or more of the following features:\n\nWhile imprecise language is not desirable in various scientific fields, it may be helpful, illustrative or discussion-stimulative in other contexts. Imprecision in a discourse may or may not be the intention of the author(s) or speaker(s). The role of imprecision may depend on audience, end goal, extended context and subject matter. Relevant players and real stakes will also bear on truth-grounds of statements.\n"}
{"id": "15221", "url": "https://en.wikipedia.org/wiki?curid=15221", "title": "Intel 80188", "text": "Intel 80188\n\nThe Intel 80188 microprocessor was a variant of the Intel 80186. The 80188 had an 8-bit external data bus instead of the 16-bit bus of the 80186; this made it less expensive to connect to peripherals. The 16-bit registers and the one megabyte address range were unchanged, however. It had a throughput of 1 million instructions per second.\n\nThe 80188 series was generally intended for embedded systems, as microcontrollers with external memory. Therefore, to reduce the number of chips required, it included features such as clock generator, interrupt controller, timers, wait state generator, DMA channels, and external chip select lines.\nWhile the N80188 was compatible with the 8087 numerics co-processor, the 80C188 was not. It didn't have the ESC control codes integrated.\n\nThe initial clock rate of the 80188 was 6 MHz, but due to more hardware available for the microcode to use, especially for address calculation, many individual instructions ran faster than on an 8086 at the same clock frequency. For instance, the common \"register+immediate\" addressing mode was significantly faster than on the 8086, especially when a memory location was both (one of the) operand(s) and the destination. Multiply and divide also showed great improvement, being several times as fast as on the original 8086 and multi-bit shifts were done almost four times as quickly as in the 8086.\n\nAlong with hundreds of other processor models, Intel discontinued the 80188 processor 30 March 2006, after a life of about 24 years.\n\n"}
{"id": "15222", "url": "https://en.wikipedia.org/wiki?curid=15222", "title": "IEEE 802.2", "text": "IEEE 802.2\n\nIEEE 802.2 is the original name of the ISO/IEC 8802-2 standard which defines logical link control (LLC) as the upper portion of the data link layer of the OSI Model. The original standard developed by the Institute of Electrical and Electronics Engineers (IEEE) in collaboration with the American National Standards Institute (ANSI) was adopted by the International Organization for Standardization (ISO) in 1998, but it still remains an integral part of the family of IEEE 802 standards for local and metropolitan networks.\n\nLLC is a software component that provides a uniform interface to the user of the data link service, usually the network layer. LLC may offer three types of services:\n\nConversely, the LLC uses the services of the media access control (MAC), which is dependent on the specific transmission medium (Ethernet, Token Ring, FDDI, 802.11, etc.). Using LLC is compulsory for all IEEE 802 networks with the exception of Ethernet. It is also used in Fiber Distributed Data Interface (FDDI) which is not part of the IEEE 802 family.\n\nThe IEEE 802.2 sublayer adds some control information to the message created by the upper layer and passed to the LLC for transmission to another node on the same data link. The resulting packet is generally referred to as \"LLC protocol data unit (PDU)\" and the additional information added by the LLC sublayer is the \"LLC HEADER\". The LLC Header consist of \"DSAP\" (\"Destination Service Access Point\"), \"SSAP\" (\"Source Service Access Point\") and the \"Control\" field.\n\nThe two 8-bit fields DSAP and SSAP allow to multiplex various upper layer protocols above LLC. However, many protocols use the Subnetwork Access Protocol (SNAP) extension which allows using EtherType values to specify the protocol being transported atop IEEE 802.2. It also allows vendors to define their own protocol value spaces.\n\nThe 8 or 16 bit HDLC-style Control field serves to distinguish communication mode, to specify a specific operation and to facilitate connection control and flow control (in connection mode) or acknowledgements (in acknowledged connectionless mode).\n\nIEEE 802.2 provides two connectionless and one connection-oriented operational modes:\nThe use of multicasts and broadcasts reduce network traffic when the same information needs to be propagated to all stations of the network. However the Type 1 service provides no guarantees regarding the order of the received frames compared to the order in which they have been sent; the sender does not even get an acknowledgment that the frames have been received.\n\nEach device conforming the IEEE 802.2 standard must support service type 1. Each network node is assigned an LLC Class according to which service types it supports:\n\nAny 802.2 LLC PDU has the following format:\nWhen Subnetwork Access Protocol (SNAP) extension is used, it is located at the start of the Information field:\nThe 802.2 header includes two eight-bit address fields, called service access points (SAP) or collectively LSAP in the OSI terminology:\n\nAlthough the LSAP fields are 8 bits long, the low-order bit is reserved for special purposes, leaving only 128 values available for most purposes.\n\nThe low-order bit of the DSAP indicates whether it contains an individual or a group address:\n\nThe low-order bit of the SSAP indicates whether the packet is a command or response packet:\nThe remaining 7 bits of the SSAP specify the LSAP (always an individual address) from which the packet was transmitted.\n\nLSAP numbers are globally assigned by the IEEE to uniquely identify well established international standards.\n\nThe protocols or families of protocols which have assigned one or more SAPs may operate directly on top of 802.2 LLC. Other protocols may use the Subnetwork Access Protocol (SNAP) with IEEE 802.2 which is indicated by the hexadecimal value 0xAA (or 0xAB, if the source of a response) in SSAP and DSAP. The SNAP extension allows using EtherType values or private protocol ID spaces in all IEEE 802 networks. It can be used both in datagram and in connection-oriented network services.\n\nEthernet (IEEE 802.3) networks are an exception; the IEEE 802.3x-1997 standard explicitly allowed using of the Ethernet II framing, where the 16-bit field after the MAC addresses does not carry the length of the frame followed by the IEEE 802.2 LLC header, but the EtherType value followed by the upper layer data. With this framing only datagram services are supported on the data link layer.\n\nAlthough IPv4 has been assigned an LSAP value of 6 (0x6) and ARP has been assigned an LSAP value of 152 (0x98), IPv4 is almost never directly encapsulated in 802.2 LLC frames without SNAP headers. Instead, the Internet standard RFC 1042 is usually used for encapsulating IPv4 traffic in 802.2 LLC frames with SNAP headers on FDDI and on IEEE 802 networks other than Ethernet. Ethernet networks typically use Ethernet II framing with EtherType 0x800 for IP and 0x806 for ARP.\n\nThe IPX protocol used by Novell NetWare networks supports an additional Ethernet frame type, 802.3 raw, ultimately supporting four frame types on Ethernet (802.3 raw, 802.2 LLC, 802.2 SNAP, and Ethernet II) and two frame types on FDDI and other (non-Ethernet) IEEE 802 networks (802.2 LLC and 802.2 SNAP).\n\nIt is possible to use diverse framings on a single network. It is possible to do it even for the same upper layer protocol, but in such a case the nodes using unlike framings cannot directly communicate with each other.\n\nFollowing the destination and source SAP fields is a control field. IEEE 802.2 was conceptually derived from HDLC, and has the same three types of PDUs:\n\nTo carry data in the most-often used unacknowledged connectionless mode the U-format is used. It is identified by the value '11' in lower two bits of the single-byte control field.\n"}
{"id": "15223", "url": "https://en.wikipedia.org/wiki?curid=15223", "title": "Invertebrate", "text": "Invertebrate\n\nInvertebrates are animals that neither possess nor develop a vertebral column (commonly known as a \"backbone\" or \"spine\"), derived from the notochord. This includes all animals apart from the subphylum Vertebrata. Familiar examples of invertebrates include arthropods (insects, arachnids, crustaceans, and myriapods), mollusks (chitons, snails, bivalves, squids, and octopuses), annelids (earthworms and leeches), and cnidarians (hydras, jellyfishes, sea anemones, and corals).\n\nThe majority of animal species are invertebrates; one estimate puts the figure at 97%. Many invertebrate taxa have a greater number and variety of species than the entire subphylum of Vertebrata.\n\nSome of the so-called invertebrates, such as the Tunicata and Cephalochordata are more closely related to the vertebrates than to other invertebrates. This makes the invertebrates paraphyletic, so the term has little meaning in taxonomy.\n\nThe word \"invertebrate\" comes from the Latin word \"vertebra\", which means a joint in general, and sometimes specifically a joint from the spinal column of a vertebrate. The jointed aspect of \"vertebra\" is derived from the concept of turning, expressed in the root \"verto\" or \"vorto\", to turn. The prefix \"in-\" means \"not\" or \"without\".\n\nThe term \"invertebrates\" is not always precise among non-biologists since it does not accurately describe a taxon in the same way that Arthropoda, Vertebrata or Manidae do. Each of these terms describes a valid taxon, phylum, subphylum or family. \"Invertebrata\" is a term of convenience, not a taxon; it has very little circumscriptional significance except within the Chordata. The Vertebrata as a subphylum comprises such a small proportion of the Metazoa that to speak of the kingdom Animalia in terms of \"Vertebrata\" and \"Invertebrata\" has limited practicality. In the more formal taxonomy of Animalia other attributes that logically should precede the presence or absence of the vertebral column in constructing a cladogram, for example, the presence of a notochord. That would at least circumscribe the Chordata. However, even the notochord would be a less fundamental criterion than aspects of embryological development and symmetry or perhaps bauplan.\n\nDespite this, the concept of \"invertebrates\" as a taxon of animals has persisted for over a century among the laity, and within the zoological community and in its literature it remains in use as a term of convenience for animals that are not members of the Vertebrata. The following text reflects earlier scientific understanding of the term and of those animals which have constituted it. According to this understanding, invertebrates do not possess a skeleton of bone, either internal or external. They include hugely varied body plans. Many have fluid-filled, hydrostatic skeletons, like jellyfish or worms. Others have hard exoskeletons, outer shells like those of insects and crustaceans. The most familiar invertebrates include the Protozoa, Porifera, Coelenterata, Platyhelminthes, Nematoda, Annelida, Echinodermata, Mollusca and Arthropoda. Arthropoda include insects, crustaceans and arachnids.\n\nBy far the largest number of described invertebrate species are insects. The following table lists the number of described extant species for major invertebrate groups as estimated in the IUCN Red List of Threatened Species\", 2014.3.\nThe IUCN estimates that 66,178 extant vertebrate species have been described, which means that over 95% of the described animal species in the world are invertebrates.\n\nThe trait that is common to all invertebrates is the absence of a vertebral column (backbone): this creates a distinction between invertebrates and vertebrates. The distinction is one of convenience only; it is not based on any clear biologically homologous trait, any more than the common trait of having wings functionally unites insects, bats, and birds, or than not having wings unites tortoises, snails and sponges. Being animals, invertebrates are heterotrophs, and require sustenance in the form of the consumption of other organisms. With a few exceptions, such as the Porifera, invertebrates generally have bodies composed of differentiated tissues. There is also typically a digestive chamber with one or two openings to the exterior.\n\nThe body plans of most multicellular organisms exhibit some form of symmetry, whether radial, bilateral, or spherical. A minority, however, exhibit no symmetry. One example of asymmetric invertebrates includes all gastropod species. This is easily seen in snails and sea snails, which have helical shells. Slugs appear externally symmetrical, but their pneumostome (breathing hole) is located on the right side. Other gastropods develop external asymmetry, such as Glaucus atlanticus that develops asymmetrical cerata as they mature. The origin of gastropod asymmetry is a subject of scientific debate.\n\nOther examples of asymmetry are found in fiddler crabs and hermit crabs. They often have one claw much larger than the other. If a male fiddler loses its large claw, it will grow another on the opposite side after moulting. Sessile animals such as sponges are asymmetrical alongside coral colonies (with the exception of the individual polyps that exhibit radial symmetry); alpheidae claws that lack pincers; and some copepods, polyopisthocotyleans, and monogeneans which parasitize by attachment or residency within the gill chamber of their fish hosts).\n\nNeurons differ in invertebrates from mammalian cells. Invertebrates cells fire in response to similar stimuli as mammals, such as tissue trauma, high temperature, or changes in pH. The first invertebrate in which a neuron cell was identified was the medicinal leech, \"Hirudo medicinalis.\"\n\nLearning and memory using nociceptors in the sea hare, \"Aplysia\" has been described. Mollusk neurons are able to detect increasing pressures and tissue trauma.\n\nNeurons have been identified in a wide range of invertebrate species, including annelids, molluscs, nematodes and arthropods.\n\nOne type of invertebrate respiriatory system is the open respiratory system composed of spiracles, tracheae, and tracheoles that terrestrial arthropods have to transport metabolic gases to and from tissues. The distribution of spiracles can vary greatly among the many orders of insects, but in general each segment of the body can have only one pair of spiracles, each of which connects to an atrium and has a relatively large tracheal tube behind it. The tracheae are invaginations of the cuticular exoskeleton that branch (anastomose) throughout the body with diameters from only a few micrometres up to 0.8 mm. The smallest tubes, tracheoles, penetrate cells and serve as sites of diffusion for water, oxygen, and carbon dioxide. Gas may be conducted through the respiratory system by means of active ventilation or passive diffusion. Unlike vertebrates, insects do not generally carry oxygen in their haemolymph.\n\nA tracheal tube may contain ridge-like circumferential rings of taenidia in various geometries such as loops or helices. In the head, thorax, or abdomen, tracheae may also be connected to air sacs. Many insects, such as grasshoppers and bees, which actively pump the air sacs in their abdomen, are able to control the flow of air through their body. In some aquatic insects, the tracheae exchange gas through the body wall directly, in the form of a gill, or function essentially as normal, via a plastron. Note that despite being internal, the tracheae of arthropods are shed during moulting (ecdysis).\n\nLike vertebrates, most invertebrates reproduce at least partly through sexual reproduction. They produce specialized reproductive cells that undergo meiosis to produce smaller, motile spermatozoa or larger, non-motile ova. These fuse to form zygotes, which develop into new individuals. Others are capable of asexual reproduction, or sometimes, both methods of reproduction.\n\nSocial behavior is widespread in invertebrates, including cockroaches, termites, aphids, thrips, ants, bees, Passalidae, Acari, spiders, and more. Social interaction is particularly salient in eusocial species but applies to other invertebrates as well.\n\nInsects recognize information transmitted by other insects.\n\nThe term invertebrates covers several phyla. One of these are the sponges (Porifera). They were long thought to have diverged from other animals early. They lack the complex organization found in most other phyla. Their cells are differentiated, but in most cases not organized into distinct tissues. Sponges typically feed by drawing in water through pores. Some speculate that sponges are not so primitive, but may instead be secondarily simplified. The Ctenophora and the Cnidaria, which includes sea anemones, corals, and jellyfish, are radially symmetric and have digestive chambers with a single opening, which serves as both the mouth and the anus. Both have distinct tissues, but they are not organized into organs. There are only two main germ layers, the ectoderm and endoderm, with only scattered cells between them. As such, they are sometimes called diploblastic.\n\nThe Echinodermata are radially symmetric and exclusively marine, including starfish (Asteroidea), sea urchins, (Echinoidea), brittle stars (Ophiuroidea), sea cucumbers (Holothuroidea) and feather stars (Crinoidea).\n\nThe largest animal phylum is also included within invertebrates: the Arthropoda, including insects, spiders, crabs, and their kin. All these organisms have a body divided into repeating segments, typically with paired appendages. In addition, they possess a hardened exoskeleton that is periodically shed during growth. Two smaller phyla, the Onychophora and Tardigrada, are close relatives of the arthropods and share these traits. The Nematoda or roundworms, are perhaps the second largest animal phylum, and are also invertebrates. Roundworms are typically microscopic, and occur in nearly every environment where there is water. A number are important parasites. Smaller phyla related to them are the Kinorhyncha, Priapulida, and Loricifera. These groups have a reduced coelom, called a pseudocoelom. Other invertebrates include the Nemertea or ribbon worms, and the Sipuncula.\n\nAnother phylum is Platyhelminthes, the flatworms. These were originally considered primitive, but it now appears they developed from more complex ancestors. Flatworms are acoelomates, lacking a body cavity, as are their closest relatives, the microscopic Gastrotricha. The Rotifera or rotifers, are common in aqueous environments. Invertebrates also include the Acanthocephala or spiny-headed worms, the Gnathostomulida, Micrognathozoa, and the Cycliophora.\n\nAlso included are two of the most successful animal phyla, the Mollusca and Annelida. The former, which is the second-largest animal phylum by number of described species, includes animals such as snails, clams, and squids, and the latter comprises the segmented worms, such as earthworms and leeches. These two groups have long been considered close relatives because of the common presence of trochophore larvae, but the annelids were considered closer to the arthropods because they are both segmented. Now, this is generally considered convergent evolution, owing to many morphological and genetic differences between the two phyla.\n\nAmong lesser phyla of invertebrates are the Hemichordata, or acorn worms, and the Chaetognatha, or arrow worms. Other phyla include Acoelomorpha, Brachiopoda, Bryozoa, Entoprocta, Phoronida, and Xenoturbellida.\n\nInvertebrates can be classified into several main categories, some of which are taxonomically obsolescent or debatable, but still used as terms of convenience. Each however appears in its own article at the following links.\n\n\nThe earliest animal fossils appear to be those of invertebrates. 665-million-year-old fossils in the Trezona Formation at Trezona Bore, West Central Flinders, South Australia have been interpreted as being early sponges. Some paleontologists suggest that animals appeared much earlier, possibly as early as 1 billion years ago. Trace fossils such as tracks and burrows found in the Tonian era indicate the presence of triploblastic worms, like metazoans, roughly as large (about 5 mm wide) and complex as earthworms.\n\nAround 453 MYA, animals began diversifying, and many of the important groups of invertebrates diverged from one another. Fossils of invertebrates are found in various types of sediment from the Phanerozoic. Fossils of invertebrates are commonly used in stratigraphy.\n\nCarl Linnaeus divided these animals into only two groups, the Insecta and the now-obsolete Vermes (worms). Jean-Baptiste Lamarck, who was appointed to the position of \"Curator of Insecta and Vermes\" at the Muséum National d'Histoire Naturelle in 1793, both coined the term \"invertebrate\" to describe such animals, and divided the original two groups into ten, by splitting Arachnida and Crustacea from the Linnean Insecta, and Mollusca, Annelida, Cirripedia, Radiata, Coelenterata and Infusoria from the Linnean Vermes. They are now classified into over 30 phyla, from simple organisms such as sea sponges and flatworms to complex animals such as arthropods and molluscs.\n\nInvertebrates are animals \"without\" a vertebral column. This has led to the conclusion that \"in\"vertebrates are a group that deviates from the normal, vertebrates. This has been said to be because researchers in the past, such as Lamarck, viewed vertebrates as a \"standard\": in Lamarck's theory of evolution, he believed that characteristics acquired through the evolutionary process involved not only survival, but also progression toward a \"higher form\", to which humans and vertebrates were closer than invertebrates were. Although goal-directed evolution has been abandoned, the distinction of invertebrates and vertebrates persists to this day, even though the grouping has been noted to be \"hardly natural or even very sharp.\" Another reason cited for this continued distinction is that Lamarck created a precedent through his classifications which is now difficult to escape from. It is also possible that some humans believe that, they themselves being vertebrates, the group deserves more attention than invertebrates. In any event, in the 1968 edition of \"Invertebrate Zoology\", it is noted that \"division of the Animal Kingdom into vertebrates and invertebrates is artificial and reflects human bias in favor of man's own relatives.\" The book also points out that the group lumps a vast number of species together, so that no one characteristic describes all invertebrates. In addition, some species included are only remotely related to one another, with some more related to vertebrates than other invertebrates (see Paraphyly).\n\nFor many centuries, invertebrates were neglected by biologists, in favor of big vertebrates and \"useful\" or charismatic species. Invertebrate biology was not a major field of study until the work of Linnaeus and Lamarck in the 18th century. During the 20th century, invertebrate zoology became one of the major fields of natural sciences, with prominent discoveries in the fields of medicine, genetics, palaeontology, and ecology. The study of invertebrates has also benefited law enforcement, as arthropods, and especially insects, were discovered to be a source of information for forensic investigators.\n\nTwo of the most commonly studied model organisms nowadays are invertebrates: the fruit fly \"Drosophila melanogaster\" and the nematode \"Caenorhabditis elegans\". They have long been the most intensively studied model organisms, and were among the first life-forms to be genetically sequenced. This was facilitated by the severely reduced state of their genomes, but many genes, introns, and linkages have been lost. Analysis of the starlet sea anemone genome has emphasised the importance of sponges, placozoans, and choanoflagellates, also being sequenced, in explaining the arrival of 1500 ancestral genes unique to animals. Invertebrates are also used by scientists in the field of aquatic biomonitoring to evaluate the effects of water pollution and climate change.\n\n\n\n"}
{"id": "15225", "url": "https://en.wikipedia.org/wiki?curid=15225", "title": "Ivar Aasen", "text": "Ivar Aasen\n\nIvar Andreas Aasen (; 5 August 1813 – 23 September 1896) was a Norwegian philologist, lexicographer, playwright, and poet. He is best known for having assembled from dialects one of the two official written versions of the Norwegian language, Nynorsk.\n\nAasen was born at Åsen in Ørsta (then Ørsten), in the district of Sunnmøre, on the west coast of Norway. His father, a peasant with a small farm, Ivar Jonsson, died in 1826. The younger Ivar was brought up to farmwork, but he assiduously cultivated all his leisure in reading. An early interest of his was botany. When he was eighteen, he opened an elementary school in his native parish. In 1833 he entered the household of Hans Conrad Thoresen, the husband of the eminent writer Magdalene Thoresen, in Herøy (then Herø), and there he picked up the elements of Latin. Gradually, and by dint of infinite patience and concentration, the young peasant mastered many languages, and began the scientific study of their structure. Ivar single-handedly created a new language for Norway to become the \"literary\" language.\n\nAbout 1846 he had freed himself from all the burden of manual labour, and could occupy his thoughts with the dialect of his native district, Sunnmøre; his first publication was a small collection of folk songs in the Sunnmøre dialect (1843). His remarkable abilities now attracted general attention, and he was helped to continue his studies undisturbed. His \"Grammar of the Norwegian Dialects\" (, 1848) was the result of much labour, and of journeys taken to every part of the country. Aasen's famous \"Dictionary of the Norwegian Dialects\" () appeared in its original form in 1850, and from this publication dates all the wide cultivation of the popular language in Norwegian, since Aasen really did no less than construct, out of the different materials at his disposal, a popular language or definite \"folke-maal\" (people's language) for Norway. By 1853, he had created the norm for utilizing his new language, which he called Landsmaal, meaning country language. With certain modifications, the most important of which were introduced later by Aasen himself, but also through a latter policy aiming to merge this Norwegian language with Dano-Norwegian, this language has become \"Nynorsk\" (\"New Norwegian\"), the second of Norway's two official languages (the other being \"Bokmål\", the Dano-Norwegian descendant of the Danish language used in Norway in Aasen's time). An unofficial variety of Norwegian more close to Aasen's language is still found in Høgnorsk (\"High Norwegian\"). Today, some consider Nynorsk on equal footing with bokmål, as bokmål tends to be used more in radio and television and most newspapers, whereas New Norse (Nynorsk) is used equally in government work as well as approximately 17% of schools. Although it is not as common as its brother language, it needs to be looked upon as a viable language, as a large minority of Norwegians use it as their primary language including many scholars and authors. New Norse is both a written and spoken language.\nAasen composed poems and plays in the composite dialect to show how it should be used; one of these dramas, \"The Heir\" (1855), was frequently acted, and may be considered as the pioneer of all the abundant dialect-literature of the last half-century of the 1800s, from Vinje to Garborg. In 1856, he published \"Norske Ordsprog\", a treatise on Norwegian proverbs. Aasen continuously enlarged and improved his grammars and his dictionary. He lived very quietly in lodgings in Oslo (then Christiania), surrounded by his books and shrinking from publicity, but his name grew into wide political favour as his ideas about the language of the peasants became more and more the watch-word of the popular party. In 1864, he published his definitive grammar of Nynorsk and in 1873 he published the definitive dictionary.\n\nQuite early in his career, in 1842, he had begun to receive a grant to enable him to give his entire attention to his philological investigations; and the Storting (Norwegian parliament), conscious of the national importance of his work, treated him in this respect with more and more generosity as he advanced in years. He continued his investigations to the last, but it may be said that, after the 1873 edition of his \"Dictionary\" (with a new title: ), he added but little to his stores. Aasen holds perhaps an isolated place in literary history as the one man who has invented, or at least selected and constructed, a language which has pleased so many thousands of his countrymen that they have accepted it for their schools, their sermons and their songs. He died in Christiania on 23 September 1896, and was buried with public honours.\n\nIvar Aasen-tunet, an institution devoted to the Nynorsk language, opened in June 2000. The building in Ørsta was designed by Norwegian architect Sverre Fehn. Their web page includes most of Aasens' texts, numerous other examples of Nynorsk literature (in Nettbiblioteket, the Internet Library), and some articles, including some in English, about language history in Norway.\n\n\"Språkåret 2013\" (The Language Year 2013) celebrated Ivar Aasen's 200 year anniversary, as well as the 100 year anniversary of Det Norske Teateret. The year's main focus was to celebrate linguistic diversity in Norway. In a poll released in connection with the celebration, 56% of Norwegians said they held positive views of Aasen, while 7% held negative views. On Aasen's 200 anniversary, 5 August 2013, \"Bergens Tidende\", which is normally published mainly in bokmål, published an edition fully in nynorsk in memory of Aasen.\n\nAasen published a wide range of material, some of it released posthumously.\n\n\n"}
{"id": "15226", "url": "https://en.wikipedia.org/wiki?curid=15226", "title": "Irredentism", "text": "Irredentism\n\nIrredentism is any political or popular movement that seeks to claim/reclaim and occupy a land that the movement's members consider to be a \"lost\" (or \"unredeemed\") territory from their nation's past.\n\nMany states formalize their irredentist claims by including them in their constitutional documents, or through other means of legal enshrinement. Such territorial claims are justified on the basis of real or imagined national notions of historic territorial, religious or ethnic affiliations. Irredentist policies may be advocated by nationalist and pan-nationalist movements and have been a feature of identity politics, and of cultural, and political geography. Irredentism may operate as a device for a government to redirect their citizens' discontent against outsiders.\n\nThe word (from Italian \"irredento\" for \"unredeemed\") was coined in Italy from the phrase \"Italia irredenta\" (\"unredeemed Italy\"). This originally referred to rule by Austria-Hungary over territories mostly or partly inhabited by ethnic Italians, such as Trentino, Trieste, Gorizia, Istria, Fiume and Dalmatia during the 19th and early 20th centuries.\n\nAn area that may be subjected to a potential claim is sometimes called an \"irredenta\"; but not all irredentas are necessarily involved in irredentism.\n\nA common way to express a claim to adjacent territories on the grounds of historical or ethnic association is by using the adjective \"Greater\" as a prefix to the country name. This conveys the image of national territory at its maximum conceivable extent with the country \"proper\" at its core. The use of \"Greater\" does not always convey an irredentistic meaning.\n\nThe Afghan border with Pakistan, known as the Durand Line, was agreed to by Afghanistan and British India in 1893. The Pashtun tribes inhabiting the border areas were divided between what have become two nations; Afghanistan never accepted the still-porous border and clashes broke out in the 1950s and 1960s between Afghanistan and Pakistan over the issue. All Afghan governments of the past century have declared, with varying intensity, a long-term goal of re-uniting all Pashtun-dominated areas under Afghan rule.\n\nThe Argentine government has intermittently maintained a claim over the Falkland Islands (Malvinas, in Spanish) since 1833, and renewed it as recently as January 2013. It considers the archipelago part of the Tierra del Fuego Province, along with South Georgia and the South Sandwich Islands.\n\nThe Argentine claim is included in the transitional provisions of the Constitution of Argentina as amended in 1994:\n\nUnited Bengal is a political ideology of a Unified Bengali-speaking Nation in South Asia. The ideology was developed by Bengali Nationalists after the First Partition of Bengal in 1905. The British-ruled Bengal Presidency was divided into Western Bengal and Eastern Bengal and Assam to weaken the Independence Movement; after much protest, Bengal was reunited in 1911.\n\nThe second attempt by British to partition the Bengal along communal lines was in 1947. The United Bengal proposal was the bid made by Prime Minister of Bengal Huseyn Shaheed Suhrawardy and Bengali Nationalist Leader Sarat Chandra Bose to found a united and independent nation-state of Bengal. The proposal was floated as an alternative to the partition of Bengal on communal lines. The initiative failed due to British diplomacy and communal conflict between Bengali Muslims and Bengali Hindus that eventually led to the Second Partition of Bengal.\n\nThe 2009 constitution of Bolivia states that the country has an \"unrenounceable right over the territory that gives it access to the Pacific Ocean and its maritime space\". This is understood as territory that Bolivia and Peru ceded to Chile after the War of the Pacific, which left Bolivia as a landlocked country.\n\nThe preamble to the Constitution of the People's Republic of China states, \"Taiwan is part of the sacred territory of the People's Republic of China (PRC). It is the lofty duty of the entire Chinese people, including our compatriots in Taiwan, to accomplish the great task of reunifying the motherland.\" The PRC claim to sovereignty over Taiwan is generally based on the theory of the succession of states, with the PRC claiming that it is the successor state to the Republic of China. It disregards the fact that the Qing Empire ceded Taiwan and the Pescadores to Japan in perpetuity in the Treaty of Shimonoseki in 1895. \n\nThe Government of the Republic of China formerly administered both mainland China and Taiwan; the government has been administering only Taiwan since its defeat in the Chinese Civil War by the armed forces of the Communist Party of China. While the official name of the state remains 'Republic of China', the country is commonly called 'Taiwan', since Taiwan makes up 99% of the controlled territory of the ROC.\n\nArticle 4 of the Constitution of the Republic of China originally stated that \"[t]he territory of the Republic of China within its existing national boundaries shall not be altered except by a resolution of the National Assembly\" Throughout the 1950s and 1960s, the Government of the Republic of China on Taiwan maintained itself to be the legitimate ruler of Mainland China as well. As part of its current policy of continuing the 'status quo', the ROC has not renounced claims over the territories currently controlled by the People's Republic of China, Mongolia, Russia, Myanmar and some Central Asian states. However, Taiwan does not actively pursue these claims in practice; the remaining claims that Taiwan is actively seeking are of uninhabited islands: the Senkaku Islands, whose sovereignty is also asserted by Japan and the PRC; and the Paracel Islands and Spratly Islands in the South China Sea, which are currently being developed by China (PRC).\n\nArticle 1 of the Constitution of the Union of the Comoros begins: \"The Union of the Comoros is a republic, composed of the autonomous islands of Mohéli, Mayotte, Anjouan, and Grande Comore.\" Mayotte, geographically a part of the Comoro Islands, was the only island of the four to vote against independence from France (independence losing 37%–63%) in the referendum held December 22, 1974. Mayotte is currently a department of the French Republic.\n\nAll of the European colonies on the Indian subcontinent which were not part of the British Raj have been annexed by India since it gained its independence from the British Empire. An example of such territories was the 1961 Indian annexation of Goa. An example of annexation of a territory from the British Raj was the Indian integration of Junagadh.\n\nAkhand Bharat, literally Undivided India or Whole India, is an irredentist call to reunite Pakistan and Bangladesh (and for some Sri Lanka, Maldives, Nepal and Bhutan) with India to form an \"Undivided India\" as it existed before partition in 1947 during the British Raj (and before that, during other periods of political unity in South Asia when most of the Indian Subcontinent was under the rule of one power, such as during the Maurya Empire, the Gupta Empire, the Mughal Empire or the Maratha Empire). The call for \"Akhanda Bharata\" has often been raised by mainstream Indian nationalistic cultural and political organizations such as the Rashtriya Swayamsevak Sangh (RSS) and the Bharatiya Janata Party (BJP). Other major Indian political parties such as the Indian National Congress, while maintaining positions against the partition of India on religious grounds, do not necessarily subscribe to a call to reunite South Asia in the form of Akhanda Bharata.\n\nThe region of Kashmir in north India has been the issue of a territorial dispute between India and Pakistan since 1947, the Kashmir conflict. Multiple wars have been fought over the issue, the first one immediately upon independence and partition in 1947 itself. To stave off a Pakistani and tribal invasion, Maharaja Hari Singh of the princely state of Jammu and Kashmir signed the Instrument of Accession with India. Kashmir has remained divided in three parts, administered by India, Pakistan and China, since then. However, on the basis of the instrument of accession, India continues to claim the entire Kashmir region as its integral part. All modern Indian political parties support the return of the entirety of Kashmir to India, and all official maps of India show the entire Jammu and Kashmir state (including parts under Pakistani or Chinese administration after 1947) as an integral part of India.\n\nIndonesia claimed all territories of the former Dutch East Indies, and previously viewed British plans to group the British Malaya and Borneo into a new independent federation of Malaysia as a threat to its objective to create a united state called Greater Indonesia. The Indonesian opposition of Malaysian formation has led to the Indonesia–Malaysia confrontation in the early 1960s. It also held Portuguese Timor (modern East Timor) from 1975 to 2002 based on irredentist claims.\n\nThe idea of uniting former British and Dutch colonial possessions in Southeast Asia actually has its roots in the early 20th century, as the concept of Greater Malay (\"Melayu Raya\") was coined in British Malaya espoused by students and graduates of Sultan Idris Training College for Malay Teachers in the late 1920s. Some political figures in Indonesia including Mohammad Yamin and Sukarno revived the idea in the 1950s and named the political union concept as Greater Indonesia.\n\nThe nation state of Israel was established in 1948. The United Nations General Assembly passed U.N. Resolution 181, otherwise known as the United Nations Partition Plan for Palestine, with 72% of the valid votes. Eventually, Israeli independence was achieved following the liquidation of the former British-administered Mandate of Palestine, the departure of the British and the \"Independence War\" between the Jews in ex-Mandatory Palestine and five Arab states' armies. The Jewish claim to Palestine as a Jewish homeland can be seen as an example of irredentist reclamation of what is considered lost Jewish land by Zionists. These claims are based on ancestral inhabitance (and in some periods sovereignty) in the land and the cultural/religious significance of it in the Hebrew Bible. The latter is particularly relevant to the Israeli claim to Jerusalem. Mandatory Palestine had sizable Jewish and Arab populations before the Second World War.\n\nJudea and Samaria, as they are called in the Bible, were part of the ancient Kingdom of Israel (designated the West Bank by Jordan in 1947) and the Gaza Strip, previously annexed by Jordan and occupied by Egypt respectively, were conquered and occupied by Israel in the Six-Day War in 1967. Israel withdrew from Gaza in August 2005; Judea and Samaria (West Bank) remain under Israeli control. Israel has never explicitly claimed sovereignty over any part of the West Bank apart from East Jerusalem, which it unilaterally annexed in 1980. However, the Israeli military supports and defends hundreds of thousands of Israeli citizens who have migrated to the West Bank, incurring criticism by some who otherwise support Israel. The United Nations Security Council, the United Nations General Assembly, and some countries and international organizations continue to regard Israel as occupying Gaza. \"(See Israeli-Occupied Territories)\"\nThe Israeli annexing instrument, the Jerusalem Law—one of the Basic Laws of Israel (Israel does not have a constitution)—declares Jerusalem, \"complete and united\", to be the capital of Israel. Article 3 of the Basic Law of the Palestinian Authority, which was ratified in 2002 by the Palestinian National Authority and serves as an interim constitution, claims that \"Jerusalem is the capital of Palestine\". \"De facto\", the Palestinian government administers the parts of the West Bank that Israel has granted it authority over from Ramallah, while the Gaza Strip is administered by the Hamas movement from Gaza.\n\nThe United States has until now not recognized Israeli sovereignty over East Jerusalem and maintained its embassy in Tel Aviv. In Jerusalem, the United States maintained two Consulates General as a diplomatic representation to the city of Jerusalem alone, separate from representation to the state of Israel. One of the Consulates General was established before the 1967 war, and the other in a recently constructed building on the Israeli side of Jerusalem. Moreover, Congress passed the Jerusalem Embassy Act in 1995 that says the US shall move its embassy from Tel Aviv to Jerusalem, but allows the president to delay the move every year if it is deemed contrary to national security interests. Since 1995, every president delayed the move. However, President Donald Trump in December 2017 declared his intention to move the embassy to Jerusalem and by May 2018 the embassy will have officially moved to Jerusalem.\n\nA number of Israelis and Jews regard the East Bank of the Jordan river (which is today the Kingdom of Jordan) as the eastern parts of the Land of Israel (following the revisionist idea) because, according to the Bible, the Israelite tribes of Menasseh, Gad, and Reuben settled on the east bank of the Jordan, and because that area was designated a Jewish national home by the League of Nations in the Mandate for Palestine based upon the recognized historical connection of the Jewish people to the land. Cited as an explicit basis not to create, but to reconstitute the historical homeland of the Jewish people as a nation-state roughly analogous to the former Kingdom of Israel subject to change by treaty, capitulation, grant, usage, sufferance or other lawful means, it forms a basis for claims of sovereign jurisdiction.\n\nSince their founding, both Korean states have disputed the legitimacy of the other. North Korea's constitution stresses the importance of reunification, but, while it makes no similar formal provision for administering the South, it effectively claims its territory as it does not diplomatically recognise the Republic of Korea, deeming it an \"entity occupying the Korean territory\".\n\nSouth Korea's constitution also claims jurisdiction over the entire Korean peninsula. It acknowledges the division of Korea only indirectly by requiring the president to work for reunification. The Committee for the Five Northern Korean Provinces, established in 1949, is the South Korean authority charged with the administration of Korean territory north of the Military Demarcation Line (i.e., North Korea), and consists of the governors of the five provinces, who are appointed by the President. However the body is purely symbolic and largely tasked with dealing with Northern defectors; if reunification were to occur the Committee would be dissolved and new administrators appointed by the Ministry of Unification.\n\nOther territories sometimes disputed to belong to Korea are Manchuria and Gando.\n\nPakistan has from its inception sought to have the territory of Kashmir incorporated into it. This singular demand has predominated Pakistan's policy strategy and decision-making as well as its diplomacy, throughout its existence. Pakistan's dispute with India over the territory of Kashmir stems from events leading up to the 1948 war between the 2 countries.\n\nThe Guayana Esequiba is a territory administered by Guyana but claimed by Venezuela. It was first included in the Viceroyalty of New Granada and the Captaincy General of Venezuela by Spain, but was later included in Essequibo by the Dutch and in British Guiana by the United Kingdom. Originally, parts of what is now eastern Venezuela were included in the disputed area. This territory of is the subject of a long-running boundary dispute inherited from the colonial powers and complicated by the independence of Guyana in 1966. The status of the territory is subject to the Treaty of Geneva, which was signed by the United Kingdom, Venezuela and British Guiana governments on February 17, 1966. This treaty stipulates that the parties will agree to find a practical, peaceful and satisfactory solution to the dispute.\n\nSome of the most violent irredentist conflicts of recent times in Europe flared up as a consequence of the break-up of the former Yugoslavian federal state in the early 1990s. The conflict erupted further south with the ethnic Albanian majority in Kosovo seeking to switch allegiance to the adjoining state of Albania.\n\nGreater Albania or \"Ethnic Albania\" as called by the Albanian nationalists themselves, is an irredentist concept of lands outside the borders of Albania which are considered part of a greater national homeland by most Albanians, based on claims on the present-day or historical presence of Albanian populations in those areas. The term incorporates claims to Kosovo, as well as territories in the neighbouring countries Montenegro, Greece, and the Republic of Macedonia. Albanians themselves mostly use the term \"ethnic Albania\" instead. According to the \"Gallup Balkan Monitor\" 2010 report, the idea of a Greater Albania was supported by the majority of Albanians in Albania (63%), Kosovo (81%) and Macedonia (53%).\n\nIn 2012, as part of the celebrations for the 100th Anniversary of the Independence of Albania, Prime Minister Sali Berisha spoke of \"Albanian lands\" stretching from Preveza in Greece to Preševo in Serbia, and from the Macedonian capital of Skopje to the Montenegrin capital of Podgorica, angering Albania's neighbours. The comments were also inscribed on a parchment that will be displayed at a museum in the city of Vlore, where the country's independence from the Ottoman Empire was declared in 1912.\n\nBased on the territorial definition of a historic Bulgarian state, a \"Greater Bulgaria\" nationalist movement has been active for more than a century that would annex most of Macedonia, Thrace, and Moesia.\n\nThe idea of the natural borders of France is a political theory conceptualized primarily in the late 18th and early 19th centuries that focused on widening the borders primarily based on either practical reasons or the territory that was thought to be the maximum extent that the ancient Gauls inhabited. Under this theory France's eastern border would extend to the Rhine river and would require the annexation of Belgium, Luxembourg, 27,264 km² of German territory on the left bank of the Rhine river, and 10,545 km² of Dutch territory south of Waal and Merwede rivers. If implemented today France would increase its territory by 70,923 km² and increase its population by 25,170,400.\n\nAlthough the Finnish State is not actively pursuing any policy to reclaim lost territory, there is a growing movement in both Finland and the Republic of Karelia to restore the territory annexed by the Soviet Union following the Winter and Continuation Wars, including Petsamo and parts of Salla and Kuusamo. The idea of a Greater Finland first gained popularity and influence in 1917, but lost support after World War II.\n\nIn addition, many in Eastern Karelia seek to form an independent Karelian state.\n\nDuring the debate of what was then called the German Question (\"die deutsche Frage\") in the 19th century prior to the unification of Germany (1871), the term \"Großdeutschland\", \"Greater Germany\", referred to a possible German nation consisting of the states that later comprised the German Empire and Austria. The term \"Kleindeutschland\" \"Lesser Germany\" referred to a possible German state without Austria. The term was later used by Germans referring to Greater Germany, a state consisting of pre–World War I Germany, Austria and the Sudetenland.\n\nA main point of Nazi ideology was to reunify all Germans either born or living outside of Germany to create an \"all-German Reich\". These beliefs ultimately resulted in the Munich Agreement, which ceded to Germany areas of Czechoslovakia that were mainly inhabited by those of German descent, and the \"Anschluss\", which ceded the entire country of Austria to Germany; both events occurred in 1938.\n\nFollowing the Greek War of Independence in 1821–1832, Greece began to contest areas inhabited by Greeks, primarily against the Ottoman Empire. The Megali Idea (Great Idea) envisioned Greek incorporation of Greek-inhabited lands, but also historical lands in Asia Minor corresponding with the predominantly Greek and Orthodox Byzantine Empire and the dominions of the ancient Greeks.\n\nThe Greek quest began with the acquisition of Thessaly through the Convention of Constantinople in 1881, a failed war against Turkey in 1897 and the Balkan Wars (Macedonia, Epirus, some Aegean Islands). After World War I, Greece acquired Western Thrace from Bulgaria as per the Treaty of Neuilly-sur-Seine, but also Ionia/Smyrna and Eastern Thrace (excluding Constantinople) from the Ottoman Empire as ordained in the Treaty of Sèvres. Subsequently, Greece launched an unsuccessful campaign to further their gains in Asia Minor, but were halted by the Turkish revolution. The events culminated into the Great Fire of Smyrna, Population exchange between Greece and Turkey and Treaty of Lausanne (1923) which returned Eastern Thrace and Ionia to the newfound Turkish Republic. The events are known as the \"Asia Minor Catastrophe\" to Greeks. The Ionian Islands were ceded by Britain in 1864, and the Dodecanese by Italy in 1947.\n\nAnother concern of the Greeks is the incorporation of Cyprus which was ceded by the Ottomans to the British. As a result of the Cyprus Emergency the island gained independence as the Republic of Cyprus in 1960. The failed incorporation by Greece through coup d'état and the Turkish invasion of Cyprus in 1974 led to the formation of the mostly unrecognized Northern Cyprus and has culminated into the present-day Cyprus issue.\n\nThe Aegean islands of Imbros and Tenedos which were not ceded to Greece over the course of the 20th century and where the dominant Greek community has faced persecution are also of concern.\n\nThe restoration of the borders of Hungary to their state prior to World War I, in order to unite all ethnic Hungarians within the same country once again.\n\nThe Irish Free State achieved independence from the United Kingdom in 1922. This state did not include Northern Ireland, which comprised six counties in the north-east of the island of Ireland which remained in the United Kingdom. The Constitution of Ireland adopted in 1937 provided that the name of the state is \"Ireland\" and Articles 2 and 3 provided that \"[t]he national territory consists of the whole island of Ireland\", while stipulating that \"[p]ending the re-integration of the national territory\", the powers of the state were restricted to legislate only for the area which had formed part of the Irish Free State. Arising from the Northern Ireland peace process, the matter was mutually resolved as part of the Good Friday Agreement in 1998. Ireland's constitution was altered by referendum and its territorial claim to Northern Ireland was removed.\n\nThe amended constitution asserts that while it is the entitlement of \"every person born in the island of Ireland … to be part of the Irish Nation\" and to hold Irish citizenship, \"a united Ireland shall be brought about only by peaceful means with the consent of a majority of the people, democratically expressed, in both jurisdictions in the island\". A North/South Ministerial Council was created between the two jurisdictions and given executive authority. The advisory and consultative role of the government of Ireland in the government of Northern Ireland granted by the United Kingdom, that had begun with the 1985 Anglo-Irish Agreement, was maintained, although that Agreement itself was ended. The two states also settled the long-running dispute concerning their respective names: \"Ireland\" and the \"United Kingdom of Great Britain and Northern Ireland\", with both governments agreeing to use those names.\n\nUnder the Irish republican theory of legitimism, the Irish Republic declared in 1916 was in existence from then on, denying the legitimacy of either the state of Ireland or the position of Northern Ireland within the United Kingdom. Through much of its history, this was the position of Sinn Féin; however, it effectively abandoned this stance after accepting the Good Friday Agreement. Small groups which split from Sinn Féin continue to adopt this stance, including Republican Sinn Féin, linked with the Continuity IRA, and the 32 County Sovereignty Movement, linked with the Real IRA.\n\nItaly's territorial claims were on the basis of re-establishing a Romanesque Empire, a fourth shore according to the concept of Mare Nostrum (Latin for 'Our Sea') and traditional ethnic borders. Evident in Italy's rapid takeover of surrounding territories under Fascist leader Benito Mussolini and claims following the collapsed 1915 Treaty of London and 1919 Treaty of Versailles which established feelings of betrayal. Mussolini and Hitler's similarities including a joint hatred towards the French and wanting to expand their territories brought the two leaders together, solidified in the Pact of Steel and later WW2. By 1942 Italy had conquered Abyssinia (modern day Ethiopia), Libya, much of Egypt, Tunisia, Kenya and Somalia. And – on the European continent – Istria, Dalmatia, Albania, Slovenia, Croatia, Macedonia, the Spanish island of Majorca and France's Corsica; Malta was also bombed. Underlying tensions remained with France, over its territories of Corsica, Nice and Savoy.\n\nSome Macedonian nationalists promoted the irredentist concept of a United Macedonia () among ethnic Macedonian nationalists, which involves territorial claims on the northern province of Macedonia in Greece, but also in Blagoevgrad Province (\"Pirin Macedonia\") in Bulgaria, Albania, and Serbia. The United Macedonia concept aims to unify the transnational region of Macedonia in the Balkans (which they claim as their homeland and which they assert was wrongfully divided under the Treaty of Bucharest in 1913), into a single state under Macedonian domination, with the Greek city of Thessaloniki (\"Solun\" in the Slavic languages) as its capital.\n\nThe Kingdom of Norway maintains some claim to territories lost at the dissolution of the Denmark–Norway union. The Old Kingdom of Norway, which was the Norwegian territories at its maximum extent, included Iceland, the settleable areas of Greenland, the Faroe Islands and Northern Isles (today part of Scotland). Under Danish sovereignty since they established a hegemonic position in the Kalmar Union, the territories were considered as Norwegian colonies. When in the Treaty of Kiel in 1814, Norway's territories were transferred from Denmark to Sweden, the territories of Iceland, Greenland, and the Faroe Islands were maintained by Denmark.\n\nIn 1919, Norway declared sovereignty over an area in Eastern Greenland in the Ihlen Declaration, which led to a dispute with Denmark that was not settled until 1933, by the Permanent Court of International Justice. Norway formerly included the provinces Jämtland, Härjedalen, Idre-Särna (lost since the Second Treaty of Brömsebro), and Bohuslän (lost since the Treaty of Roskilde), which were ceded to Sweden after Danish defeats in wars such as the Thirty Years' War and Second Northern War.\n\nKresy (\"Borderlands\") are the eastern lands that formerly belonged to Poland. In 1921, Polish troops crossed the Curzon Line, the border between ethnic Polish and ethnic Ukrainian and Belorussian territories, and seized large Ukrainian and Belorussian territories, and also seized 7 percent of Lithuania's territory in 1920. These territories were re-annexed by the Soviet Union in 1939 under the Molotov-Ribbentrop pact, and include major cities, like Lviv (Ukraine), Vilnius (the capital of Lithuania), and Hrodna (Belarus). Even though \"Kresy\", or the \"Eastern Borderlands\", are no longer Polish territories, the area is still inhabited by a significant Polish minority, and the memory of the Polish \"Kresy\" is still cultivated. The attachment to the \"myth of Kresy\", the vision of the region as a peaceful, idyllic, rural land, has been criticized in Polish discourse.\n\nIn January, February and March 2012, the Centre for Public Opinion Research conducted a survey, asking Poles about their ties to the Kresy. It turned out that almost 15% of the population of Poland (4.3–4.6 million people) declared that they had either been born in the Kresy, or had a parent or a grandparent who came from that region. Numerous treasures of Polish culture remain and there are numerous Kresy-oriented organizations. There are Polish sports clubs (Pogoń Lwów, FK Polonia Vilnius), newspapers (Gazeta Lwowska, Kurier Wileński), radio stations (in Lviv and Vilnius), numerous theatres, schools, choirs and folk ensembles. Poles living in \"Kresy\" are helped by Fundacja Pomoc Polakom na Wschodzie, a Polish government-sponsored organization, as well as other organizations, such as The \"Association of Help of Poles in the East Kresy\" (see also Karta Polaka). Money is frequently collected to help those Poles who live in the \"Kresy\", and there are several annual events, such as a \"Christmas Package for a Polish Veteran in Kresy\", and \"Summer with Poland\", sponsored by the Association \"Polish Community\", in which Polish children from \"Kresy\" are invited to visit Poland. Polish language handbooks and films, as well as medicines and clothes are collected and sent to \"Kresy\". Books are most often sent to Polish schools which exist there — for example, in December 2010, The University of Wrocław organized an event called \"Become a Polish Santa Claus and Give a Book to a Polish Child in Kresy\". Polish churches and cemeteries (such as Cemetery of the Defenders of Lwów) are renovated with money from Poland.\n\nPortugal does not recognize Spanish sovereignty over the territory of Olivenza, ceded to Spain during the Napoleonic Wars. Since the Rexurdimento of the mid-nineteenth century, there has been an intellectual movement pleading for the reintegration between Portugal and the region of Galicia. Although this movement has become increasingly popular on both sides of the border, there is no consensus in regard to the nature of such \"reintegration\": whether political, socio-cultural or merely linguistic.\n\nRomania lays claims to Greater Romania, which include Bessarabia and Bucovina as Moldova, since they were parts of Romania between 1918 and 1940, and are still inhabited for the most by Romanians. Moldovans are ethnically Romanians, and the Moldovan language is the Soviet name for the Romanian language. There is some (but not universal) support by Moldovans for a peaceful and voluntary reunion with Romania, not least because (having joined the European Union), the economy has burgeoned and Romanian citizens have gained freedom of movement in Europe. Also Russian irredentism in Transnistria has caused alarm and resentment.\n\nThe annexation of Crimea by the Russian Federation in 2014 was based on a claim of protecting ethnic Russians residing there. Crimea was part of the Russian Empire from 1783 to 1917, after which it enjoyed a few years of autonomy until it was made part of the Russian Soviet Federative Socialist Republic (which was a part of the Soviet Union) from 1921 to 1954 and then transferred to Soviet Ukraine (which also was a part of the Soviet Union) in 1954. After the dissolution of the Soviet Union, Crimea still remained part of Ukraine until February 2014. Russia declared Crimea to be part of the Russian Federation in March 2014, and effective administration commenced. The Russian regional status is not currently recognised by the UN General Assembly and by many countries.\n\nRussian irredentism also includes southeastern and coastal Ukraine, known as \"Novorossiya\", a term from the Russian Empire.\n\nSerbian irredentism is manifested in \"Greater Serbia\". Used in the context of the Yugoslav wars, however, the Serbian struggle for Serbs to remain united in one country does not quite fit the term \"irredentism\". In the 19th century, Pan-Serbism sought to unite all of the Serb people across the Balkans, under Ottoman and Habsburg rule. Some intellectuals sought to unite all South Slavs (regardless of religion) into a Serbian state. Serbia had gained independence from the Ottoman Empire in 1878. Bosnia and Herzegovina, annexed by the Austrians in 1908, was viewed of as a part of the Serbian homeland. Serbia directed its territorial aspirations to the south, as the north and west was held by Austria. Macedonia was divided between Serbia and Greece after the Balkan Wars.\n\nIn 1914, aspirations were directed towards Austria-Hungary. A government policy sought to incorporate all Serb-inhabited areas, and other South Slavic areas, thereby laying the foundation of Yugoslavia. With the establishment of the Kingdom of Serbs, Croats and Slovenes (later Yugoslavia), the Serbs now lived united in one country. During the breakup of Yugoslavia, the Serb political leadership in break-away Croatia and Bosnia and Herzegovina declared their territories to be part of the Federal Republic of Yugoslavia (Serbia and Montenegro).\n\nThe project of unification of Serb-inhabited areas in Croatia and Bosnia and Herzegovina during the Yugoslav wars (see United Serb Republic) ultimately failed. The Croatian Operation Storm ended large-scale combat and captured most of the Republic of Serbian Krajina forcing almost complete Serbian population to leave their centuries-old homeland, while the Dayton Agreement ended the Bosnian War. Bosnia and Herzegovina was established as a federal republic, made up by two separate entities, one being Serb-inhabited Republika Srpska. There has since been calls by Bosnian Serb politicians for the secession of Republika Srpska, and possible unification with Serbia.\n\nAfter the Kosovo War (1998–99), Kosovo became a UN protectorate, still \"de jure\" part of Serbia. The Albanian-majority Kosovo assembly unilaterally declared the independence of Kosovo in 2008, and its status is since disputed.\n\nSpain maintains a claim on Gibraltar, a British Overseas Territory near the southernmost tip of the Iberian Peninsula, which has been British since the 18th Century.\n\nGibraltar was captured in 1704, during the War of the Spanish Succession (1701–1714). The Kingdom of Spain formally ceded the territory in perpetuity to the British Crown in 1713, under of the Treaty of Utrecht. Spain's territorial claim was formally reasserted by the Spanish dictator Francisco Franco in the 1960s and has been continued by successive Spanish governments. In 2002 an agreement in principle on joint sovereignty over Gibraltar between the governments of the United Kingdom and Spain was decisively rejected in a referendum. The British Government now refuses to discuss sovereignty without the consent of the Gibraltarians.\n\nIrredentism is acute in the Caucasus region. The Nagorno-Karabakh movement's original slogan of \"miatsum\" ('union') was explicitly oriented towards re-unification with Armenia as to the pre-Soviet status, feeding an Azerbaijani understanding of the conflict as a bilateral one between itself and an irredentist Armenia. According to Prof. Thomas Ambrosio, \"Armenia's successful irredentist project in the Nagorno-Karabakh region of Azerbaijan\" and \"From 1992 to the cease-fire in 1994, Armenia encountered a highly permissive or tolerant international environment that allowed its annexation of some 15 percent of Azerbaijani territory\".\n\nIn the view of Nadia Milanova, Nagorno-Karabakh represents a combination of separatism and irredentism. However, the area has historically been Armenian, known as the Kingdom of Artsakh or Khachen. When the Caucuses came under the rule of the Soviet Union, the land was given to Azerbaijan abruptly and arbitrarily due to pressure by Joseph Stalin, along with the ancient Armenian lands of Nakhichevan, to appease Turkey during 1919-1921.\nAzerbaijan's irredentism, on the other hand, is quite explicit in official statements of the Azerbaijani officials by claiming the UN member-state Republic of Armenia as Azerbaijani territory despite the absence of historical evidence of Azerbaijan existing as a separate state up until 1918. On his official meeting in Gyanja on 21 January 2014, President Ilham Aliyev said, \"The present-day Armenia is actually located on historical lands of Azerbaijan. Therefore, we will return to all our historical lands in the future. This should be known to young people and children. We must live, we live and we will continue to live with this idea.\"\n\nThe Assyrian homeland is a geographic and cultural region situated in Northern Mesopotamia that has been traditionally inhabited by Assyrian people. The area with the greatest concentration of Assyrians on earth is located in the Assyrian homeland, or the \"Assyrian Triangle\", a region which comprises the Nineveh plains, southern Hakkari and the Barwari regions. This is where some Assyrian groups seek to create an independent nation state. The land roughly mirrors the boundaries of ancient Assyria proper, and the later Achaemenid, Seleucid, Parthian, Roman and Sassanid provinces of Assyria (Athura/Assuristan) that was extant between the 25th century BC and 7th century AD.\n\nWhole Azerbaijan is a concept of the political and historical union of territories currently and historically inhabited by Azerbaijanis or historically controlled by them. Western Azerbaijan is an irredentist political concept that is used in Azerbaijan mostly to refer to Armenia. Azerbaijani statements claim that the territory of the modern Armenian republic were lands that once belonged to Azerbaijanis.\n\nPan-Iranism is an ideology that advocates solidarity and reunification of Iranian peoples living in the Iranian plateau and other regions that have significant Iranian cultural influence, including the Persians, Azerbaijanis, Ossetians, Kurds, Zazas, Tajiks of Tajikistan and Afghanistan, the Pashtuns and the Baloch of Pakistan. The first theoretician was Dr Mahmoud Afshar Yazdi.\n\nThe ideology of pan-Iranism is most often used in conjunction with the idea of forming a Greater Iran, which refers to the regions of the Caucasus, West Asia, Central Asia, and parts of South Asia that have significant Iranian cultural influence due to having been either long historically ruled by the various Iranian (Persian) empires (such as those of the Medes, Achaemenids, Parthians, Sassanians, Samanids, Timurids, Safavids, and Afsharids and the Qajar Empire), having considerable aspects of Persian culture in their own culture due to extensive contact with the various Empires based in Persia (e.g., those regions and peoples in the North Caucasus that were not under direct Iranian rule), or are simply nowadays still inhabited by a significant amount of Iranic-speaking people who patronize their respective cultures (as it goes for the western parts of South Asia, Bahrain and China). It roughly corresponds to the territory on the Iranian plateau and its bordering plains. It is also referred to as \"Greater Persia\", while the Encyclopædia Iranica uses the term \"Iranian Cultural Continent\".\n\nSaddam Hussein's Iraq aimed to annex Khuzestan Province of Iran during the Iran–Iraq War due to the Arab population living there.\n\nThe Lebanese nationalism incorporates irredentist views seeking to unify all the lands of ancient Phoenicia around present day Lebanon. This comes from the fact that present day Lebanon, the Mediterranean coast of Syria, and northern Israel is the area that roughly corresponds to ancient Phoenicia and as a result the majority of the Lebanese people identify with the ancient Phoenician population of that region. The proposed Greater Lebanese country includes Lebanon, Mediterranean coast of Syria, and northern Israel.\n\nThe French Mandate of Syria handed over the Sanjak of Alexandretta to Turkey which turned it into Hatay Province. Syria disputes this and still regards the region as belonging to Syria.\n\nThe Syrian Social Nationalist Party, which operates in Lebanon and Syria, works for the unification of most modern states of the Levant and beyond in a single state referred to as Greater Syria. The proposed Syrian country includes Israel, Syria, Jordan, and parts of Turkey, and has at times been expanded to include Iraq, Cyprus, and the Sinai peninsula.\n\nMisak-ı Millî is the set of six important decisions made by the last term of the Ottoman Parliament. Parliament met on 28 January 1920 and published their decisions on 12 February 1920. These decisions worried the occupying Allies, resulting in the Occupation of Constantinople by the British, French and Italian troops on 16 March 1920 and the establishment of a new Turkish nationalist parliament, the Grand National Assembly, in Ankara.\n\nThe Ottoman Minister of Internal Affairs, Damat Ferid Pasha, made the opening speech of parliament due to Mehmed VI's illness. A group of parliamentarians called \"Felâh-ı Vatan\" was established by Mustafa Kemal's friends to acknowledge the decisions taken at the Erzurum Congress and the Sivas Congress. Mustafa Kemal said \"It is the nation's iron fist that writes the Nation's Oath which is the main principle of our independence to the annals of history.\" Decisions taken by this parliament were used as the basis for the new Turkish Republic's claims in the Treaty of Lausanne.\n\nThe Greater and Lesser Tunbs are disputed by the United Arab Emirates against Iran.\n\nGreater Yemen is a theory giving Yemen claim to former territories that were held by various predecessor states that existed between the Himyarite period and 18th century. The areas claimed include parts of modern Saudi Arabia and Oman.\n\nWhen Hong Kong and Macau were British and Portuguese territories, respectively, China considered these two territories to be Chinese territories under British and Portuguese administration. Therefore, Hong Kong people and Macau people descended from Chinese immigrants were entitled to Hong Kong Special Administrative Region passport or Macao Special Administrative Region passport after the two territories became the special administrative regions.\n\nJapan claims the two southernmost islands of the Russian-administered Kuril Islands, the island chain north of Hokkaido, annexed by the Soviet Union following World War II. Japan also claims the South Korean-administered Liancourt Rocks, which are known as Takeshima in Japan and have been claimed since the end of the Second World War.\n\nThe 1909 Gando Convention addressed a territory dispute between China and Joseon Korea in China's favour. Both Korean states now accept the convention border as an administrative boundary. However, because the convention was made by the occupying Empire of Japan, South Korea has disputed its legality and some Koreans claim that Korea extends into \"de facto\" PRC territory, viz. Dandong and Liaoning. The most ambitious claims include all parts of Manchuria that the Goguryeo kingdom controlled.\n\nThe irredentist idea that advocates cultural and political solidarity of Mongols. The proposed territory usually includes the independent state of Mongolia, the Chinese regions of Inner Mongolia (Southern Mongolia) and Dzungaria (in Xinjiang), and the Russian subjects of Buryatia. Sometimes Tuva and the Altai Republic are included as well.\n\nSouth Asia too is another region in which armed irredentist movements have been active for almost a century, in North-East India, Burma and Bangladesh. Most prominent amongst them are the Naga fight for Greater Nagaland, the Chin struggle for a unified Chinland, the Sri Lankan Tamil struggle for a return of their state under Tamil Eelam and other self-determinist movements by the ethnic indigenous peoples of the erstwhile Assam both under the British and post-British Assam under India. Other such movements include Beḻagāva border dispute on Maharashtra and Karnataka border with intentions to unite all Marathi speaking people under one state since the formation of the Karnataka state and dissolution of the bilingual Bombay state.\n\nGreater Bangladesh is an assumption of several Indian intellectuals that the neighbouring country of Bangladesh has an aspiration to unite all Bengali dominated regions under their flag. These include the states of West Bengal, Tripura and Assam as well as the Andaman Islands which are currently part of India and the Burmese State of Rakhine. The theory is principally based on a widespread belief amongst Indian masses that a large number of illegal Bangladeshi immigrants reside in Indian territory. It is alleged that illegal immigration is actively encouraged by some political groups in Bangladesh as well as the state of Bangladesh to convert large parts of India's northeastern states and West Bengal into Muslim-majority areas that would subsequently seek to separate from India and join Muslim-majority Bangladesh.\n\nScholars have reflected that under the guise of anti-Bangladeshi immigrant movement it is actually an anti-Muslim agenda pointed towards Bangladeshi Muslims by false propaganda and widely exaggerated claims on immigrant population. In 1998, Lieutenant General S.K. Sinha, then the Governor of Assam, claimed that massive illegal immigration from Bangladesh was directly linked with \"the long-cherished design of Greater Bangladesh\".\n\nThe call for creation of \"Akhanda Bharata\" or \"Akhand Hindustan\" has on occasion been raised by some Indian right wing Hindutvadi cultural and political organisations, such as the Hindu Mahasabha, Rashtriya Swayamsevak Sangh (RSS), Vishwa Hindu Parishad, Bharatiya Janata Party (BJP). The name of one organisation sharing this goal, the Akhand Hindustan Morcha, bears the term in its name. Other major Indian non-sectarian political parties, such as the Indian National Congress, maintain a position against the partition of India on religious grounds and do not subscribe to a call for Akhand Bharat.\n\nGreater Nepal involves the incorporation of the territories won by the Kingdom of Nepal at it greatest extent back to the Federal Democratic Republic of Nepal.\n\nPakistani irredentism involves the incorporations of Muslim majority lands of British India under Pakistan. This is most notable in the conflict in the Jammu and Kashmir state, a Muslim majority state in India.\n\nIrredentism is commonplace in Africa due to the political boundaries of former European colonial nation-states passing through ethnic boundaries, and recent declarations of independence after civil war. For example, some Ethiopian nationalist circles still claim the former Ethiopian province of Eritrea (internationally recognized as the independent State of Eritrea in 1993 after a 30-year civil war).\n\nIn North Africa, the prime examples of irredentism are the concepts of Greater Morocco and Greater Mauritania. While Mauritania has since relinquished any claims to territories outside its internationally recognized borders, Morocco continues to occupy much of Western Sahara, which it refers to as its \"Southern Provinces\".\n\nGreater Somalia refers to the region in the Horn of Africa in which ethnic Somalis are and have historically represented the predominant population. The territory encompasses The Republic of Somalia, the Ogaden region in Ethiopia, the North Eastern Province in Kenya and southern and eastern Djibouti. Ogaden in eastern Ethiopia has seen military and civic movements seeking to make it part of Somalia. This culminated in the 1977–78 Ogaden War between the two neighbours where the Somali military offensive between July 1977 and March 1978 over the disputed Ethiopian region Ogaden ended when the Somali Armed Forces retreated back across the border and a truce was declared. The Kenyan Northern Frontier District also saw conflict during the Shifta War (1963–1967) when a secessionist conflict in which ethnic Somalis in the Lamu, Garissa, Wajir and Mandera counties (all except Lamu formed part of the former North Eastern Province, abolished in 2013), attempted to join with their fellow Somalis in a \"Greater Somalia\". There has been no similar conflicts in Djibouti, which was previously known as the \"French Somaliland\" during colonisation. Here the apparent struggles for unification manifested itself in political strife that ended when in a referendum to join France as opposed to the Somali Republic succeeded among rumours of widespread vote rigging. and the subsequent death of Somali nationalist Mahmoud Harbi, Vice President of the Government Council, who was killed in a plane crash two years later under suspicious circumstances.\nSome sources say that Somalia has also laid a claim to the Socotra archipelago, which is currently governed by Yemen.\n\nIn the Treaty of Guadalupe Hidalgo (1848) following the Mexican–American War (1845–48), Mexico ceded claims to what is now the Western and Southwestern United States to the United States (see Mexican Cession). The Cortina and Pizaña uprisings of 1859 and 1915 were influenced by irredentist ideas and the \"proximity of the international boundary\". The unsuccessful Pizaña uprising \"was the last major armed protest on the part of Texas-Americans\" (Tejanos). This 1915 uprising and the Plan of San Diego that preceded it marked the high point in Mexican irredentist sentiments.\n\nIn the early years of the Chicano Movement (\"El Movimiento\") in the 1960s and 1970s, some movement figures \"were political nationalists who advocated the secession of the Southwest from the Anglo republic of the United States of America, if not fully, at least locally with regard to Chicano self-determination in local governance, education and means of production\". For example, in the 1970s, Reies Tijerina and his group La Alianza, espoused various separatist, secessionist, or irredentist beliefs. The \"Plan Espiritual de Aztlán\", written during the First Chicano National Youth Conference in 1969, also stated \"the fundamental Chicano nationalist goal of reclaiming Aztlán\"—a reference to ancient Mexican myth—as \"the rightful homeland of the Chicanos\". However, \"Most Chicano nationalists ... did not express the extreme desire for secession from the United States, and the nationalism they expressed weighed more heavily toward the broadly cultural than the explicitly political.\"\n\nToday, there is virtually no Mexican-American support for \"separatist policies of self-determination\". \"Ethnonational irredentism by Mexicans in territories seized by the United States\" following the Mexican–American War \"declined after the failure of several attempted revolts at the end of the nineteenth century, in favor of internal ... struggles for immigrant and racial civil rights\" in the United States. Neither the Mexican government nor any significant Mexican-American group \"makes irredentist claims upon the United States\". In the modern era, there \"has been no evidence of irredentist sentiments among Mexican-Americans, even in such formerly Mexican territories as Southern California, ... nor of disloyalty to the United States, nor of active interest in the politics of Mexico\".\n\n\n"}
{"id": "15227", "url": "https://en.wikipedia.org/wiki?curid=15227", "title": "Inuit languages", "text": "Inuit languages\n\nThe Inuit languages are a closely related group of indigenous American languages traditionally spoken across the North American Arctic and to some extent in the subarctic in Labrador. The related Yupik languages are spoken in western and southern Alaska and in the far east of Russia, but are severely endangered in Russia today and spoken only in a few villages on the Chukchi Peninsula. The Inuit live primarily in three countries: Greenland, Canada (specifically in the Nunatsiavut region of Labrador, the Nunavik region of Quebec, Nunavut, and the Northwest Territories), and the United States (specifically the coast of Alaska).\n\nThe total population of Inuit speaking their traditional languages is difficult to assess with precision, since most counts rely on self-reported census data that may not accurately reflect usage or competence. Greenland census estimates place the number of speakers of varieties of Inuit there at roughly 50,000, while Canadian estimates are at roughly 35,000. These two countries count the bulk of speakers of Inuit language variants, although about 7,500 Alaskans speak varieties of Inuit out of a population of over 13,000 Inuit.\n\nThe Inuit languages have a few hundred speakers in Russia. In addition, an estimated 7,000 Greenlandic Inuit live in European Denmark, the largest group outside Greenland, Canada and Alaska. Thus, the global population of speakers of varieties of Inuit is on the order of nearly 100,000 people.\n\nThe traditional language of the Inuit is a system of closely interrelated dialects that are not readily comprehensible from one end of the Inuit world to the other, and some people do not think of it as a single language but rather as a group of languages. However, there are no clear criteria for breaking the Inuit language into specific member languages since it forms a dialect continuum. Each band of Inuit understands its neighbours, and most likely its neighbours' neighbours; but at some remove, comprehensibility drops to a very low level.\n\nAs a result, Inuit in different places use different words for its own variants and for the entire group of languages, and this ambiguity has been carried into other languages, creating a great deal of confusion over what labels should be applied to it.\n\nIn Greenland the official form of Inuit language, and the official language of the state, is called \"Kalaallisut\". In other languages, it is often called \"Greenlandic\" or some cognate term. The Eskimo languages of Alaska are called \"Inupiatun\", but the variants of the Seward Peninsula are distinguished from the other Alaskan variants by calling them \"Qawiaraq\", or for some dialects, \"Bering Strait Inupiatun\".\n\nIn Canada, the word \"Inuktitut\" is routinely used to refer to all Canadian variants of the Inuit traditional language, and it is under that name that it is recognised as one of the official languages of Nunavut and the Northwest Territories. However, one of the variants of western Nunavut is called \"Inuinnaqtun\" to distinguish itself from the dialects of eastern Canada, while the variants of the Northwest Territories are sometimes called \"Inuvialuktun\" and have in the past sometimes been called \"Inuktun\". In those dialects, the name is sometimes rendered as \"Inuktitun\" to reflect dialectal differences in pronunciation. The Inuit language of Quebec is called \"Inuttitut\" by its speakers, and often by other people, but this is a minor variation in pronunciation. In Labrador, the language is called \"Inuttut\" or, often in official documents, by the more descriptive name \"Labradorimiutut\". Furthermore, Canadians – both Inuit and non-Inuit – sometimes use the word \"Inuktitut\" to refer to \"all\" Inuit language variants, including those of Alaska and Greenland.\n\nThe phrase \"\"Inuit language\"\" is largely limited to professional discourse, since in each area, there is one or more conventional terms that cover all the local variants; or it is used as a descriptive term in publications where readers can't necessarily be expected to know the locally used words.\n\nAlthough many people refer to the Inuit language as \"Eskimo language\", this is a broad term that also includes the Yupik languages, and is in addition strongly discouraged in Canada and diminishing in usage elsewhere. See the article on \"Eskimo\" for more information on this word.\n\nThe language of the Inuit is an Eskimo–Aleut language. It is fairly closely related to the Yupik languages and more remotely to the Aleut language. These cousin languages are all spoken in Western Alaska and Eastern Chukotka, Russia. It is not discernibly related to other indigenous languages of the Americas or northeast Asia, although some have proposed that it is related to the Uralic languages such as Finnish and the Sami languages in the proposed \"Uralo-Siberian\" grouping, or even Indo-European languages as part of the hypothetical \"Nostratic\" superphylum. Some consider it a Paleosiberian language, although that is more a geographic than a linguistic grouping.\n\nEarly forms of the Inuit language were spoken by the Thule people, who overran the Dorset culture that had previously occupied Arctic America at the beginning of the 2nd millennium. By 1300, the Inuit and their language had reached western Greenland, and finally east Greenland roughly at the same time the Viking colonies in southern Greenland disappeared. It is generally believed that it was during this centuries-long eastward migration that the Inuit language became distinct from the Yupik languages spoken in Western Alaska and Chukotka.\n\nUntil 1902, a possible enclave of the Dorset, the \"Sadlermiut\" (in modern Inuktitut spelling \"Sallirmiut\"), existed on Southampton Island. Almost nothing is known about their language, but the few eyewitness accounts tell of them speaking a \"strange dialect\". This suggests that they also spoke an Eskimo–Aleut language, but one quite distinct from the forms spoken in Canada today.\n\nThe Yupik and Inuit languages are very similar syntactically and morphologically. Their common origin can be seen in a number of cognates:\n\nThe western Alaskan variants retain a large number of features present in proto-Inuit language and in Yup'ik, enough so that they might be classed as Yup'ik languages if they were viewed in isolation from the larger Inuit world.\n\nThe Inuit languages are a fairly closely linked set of languages which can be broken up using a number of different criteria. Traditionally, Inuit describe dialect differences by means of place names to describe local idiosyncrasies in language: The dialect of Igloolik versus the dialect of Iqaluit, for example. However, political and sociological divisions are increasingly the principal criteria for describing different variants of the Inuit languages because of their links to different writing systems, literary traditions, schools, media sources and borrowed vocabulary. This makes any partition of the Inuit language somewhat problematic. This article will use labels that try to synthesise linguistic, sociolinguistic and political considerations in splitting up the Inuit dialect spectrum. This scheme is not the only one used or necessarily one used by Inuit themselves, but its labels do try to reflect the usages most seen in popular and technical literature.\n\nIn addition to the territories listed below, some 7,000 Greenlandic speakers are reported to live in mainland Denmark, and according to the 2001 census roughly 200 self-reported Inuktitut native speakers regularly live in parts of Canada which are outside traditional Inuit lands.\n\nOf the roughly 13,000 Alaskan Iñupiat, as few as 3000 may still be able to speak the Iñupiaq, with most of them over the age of 40. Alaskan Inupiat speak four distinct dialects:\n\n\nThe Inuit languages are an official language in the Northwest Territories, and the official and dominant language of Nunavut; it enjoys a high level of official support in Nunavik, a semi-autonomous portion of Quebec; and is still spoken in some parts of Labrador. Generally, Canadians refer to all dialects spoken in Canada as \"Inuktitut\", but the terms \"Inuvialuktun\", \"Inuinnaqtun\", and \"Inuttut\" (also called \"Nunatsiavummiutut\" or \"Labradorimiutut\") have some currency in referring to the variants of specific areas.\n\nGreenland counts approximately 50,000 speakers of the Inuit languages, of whom over 90% speak west Greenlandic dialects at home.\n\nGreenlandic was strongly supported by the Danish Christian mission (conducted by the Danish state church) in Greenland. Several major dictionaries were created, beginning with Poul Egedes's Dictionarium Grönlandico-danico-latinum (1750) and culminating with Samuel Kleinschmidt's (1871) \"Den grønlandske ordbog\" (Transl. \"The Greenlandic Dictionary\") that contained a Greenlandic grammatical system that has formed the basis of modern Greenlandic grammar. Together with the fact that until 1925 Danish was not taught in the public schools, these policies had the consequence that Greenlandic has always and continues to enjoy a very strong position in Greenland, both as a spoken as well as written language.\n\nEastern Canadian Inuit language variants have fifteen consonants and three vowels (which can be long or short).\n\nConsonants are arranged with five places of articulation: bilabial, alveolar, palatal, velar and uvular; and three manners of articulation: voiceless stops, voiced continuants, and nasals, as well as two additional sounds—voiceless fricatives. The Alaskan dialects have an additional manner of articulation, the \"retroflex\", which was present in proto-Inuit language. Retroflexes have disappeared in all the Canadian and Greenlandic dialects. In Natsilingmiutut, the voiced palatal stop derives from a former retroflex.\n\nAlmost all Inuit language variants have only three basic vowels and make a phonological distinction between short and long forms of all vowels. The only exceptions are at the extreme edges of the Inuit world: parts of Greenland, and in western Alaska.\n\nThe Inuit language, like other Eskimo–Aleut languages, has a very rich morphological system, in which a succession of different morphemes are added to root words (like verb endings in European languages) to indicate things that, in languages like English, would require several words to express. (See also: Agglutinative language and Polysynthetic language) All Inuit language words begin with a root morpheme to which other morphemes are suffixed. The language has hundreds of distinct suffixes, in some dialects as many as 700. Fortunately for learners, the language has a highly regular morphology. Although the rules are sometimes very complicated, they do not have exceptions in the sense that English and other Indo-European languages do.\n\nThis system makes words very long, and potentially unique. For example, in central Nunavut Inuktitut:\n\nThis long word is composed of a root word \"tusaa-\" \"to hear\" followed by five suffixes:\n\nThis sort of word construction is pervasive in the Inuit languages and makes it very unlike English. In one large Canadian corpus – the \"Nunavut Hansard\" – 92% of all words appear only once, in contrast to a small percentage in most English corpora of similar size. This makes the application of Zipf's law quite difficult in the Inuit language. Furthermore, the notion of a part of speech can be somewhat complicated in the Inuit languages. Fully inflected verbs can be interpreted as nouns. The word ilisaijuq can be interpreted as a fully inflected verb: \"he studies\", but can also be interpreted as a noun: \"student\". That said, the meaning is probably obvious to a fluent speaker, when put in context.\n\nThe morphology and syntax of the Inuit languages vary to some degree between dialects, and the article \"Inuit grammar\" describes primarily central Nunavut dialects, but the basic principles will generally apply to all of them and to some degree to Yupik languages as well.\n\nBoth the names of places and people tend to be highly prosaic when translated. \"Iqaluit\", for example, is simply the plural of the noun \"iqaluk\" \"fish\" (\"Arctic char\", \"salmon\" or \"trout\" depending on dialect). \"Igloolik\" (\"Iglulik\") means \"place with houses\", a word that could be interpreted as simply \"town\"; \"Inuvik\" is \"place of people\"; \"Baffin Island\", \"Qikiqtaaluk\" in Inuktitut, translates approximately to \"big island\".\n\nAlthough practically all Inuit have legal names based on southern naming traditions, at home and among themselves they still use native naming traditions. There too, names tend to consist of highly prosaic words. The Inuit traditionally believed that by adopting the name of a dead person or a class of things, they could take some of their characteristics or powers, and enjoy a part of their identity. (This is why they were always very willing to accept European names: they believed that this made them equal to the Europeans.)\n\nCommon native names in Canada include \"Ujarak\" (rock), \"Nuvuk\" (headland), \"Nasak\" (hat, or hood), \"Tupiq\" or \"Tupeq\" in Kalaallisut (tent), and \"Qajaq\" (kayak). Inuit also use animal names, traditionally believing that by using those names, they took on some of the characteristics of that animal: \"Nanuq\" or \"Nanoq\" in Kalaallisut (polar-bear), \"Uqalik\" or \"Ukaleq\" in Kalaallisut (Arctic hare), and \"Tiriaq\" or \"Teriaq\" in Kalaallisut (ermine) are favourites. In other cases, Inuit are named after dead people or people in traditional tales, by naming them after anatomical traits those people are believed to have had. Examples include \"Itigaituk\" (has no feet), \"Anana\" or \"Anaana\" (mother), \"Piujuq\" (beautiful) and \"Tulimak\" (rib). Inuit may have any number of names, given by parents and other community members.\n\nIn the 1920s, changes in lifestyle and serious epidemics like tuberculosis made the government of Canada interested in tracking the Inuit of Canada's Arctic. Traditionally Inuit names reflect what is important in Inuit culture: environment, landscape, seascape, family, animals, birds, spirits. However these traditional names were difficult for non-Inuit to parse. Also, the agglutinative nature of Inuit language meant that names seemed long and were difficult for southern bureaucrats and missionaries to pronounce.\n\nThus, in the 1940s, the Inuit were given disc numbers, recorded on a special leather ID tag, like a dog tag. They were required to keep the tag with them always. (Some tags are now so old and worn that the number is polished out.) The numbers were assigned with a letter prefix that indicated location (E = east), community, and then the order in which the census-taker saw the individual. In some ways this state renaming was abetted by the churches and missionaries, who viewed the traditional names and their calls to power as related to shamanism and paganism.\n\nThey encouraged people to take Christian names. So a young woman who was known to her relatives as \"Lutaaq, Pilitaq, Palluq, or Inusiq\" and had been baptised as \"Annie\" was under this system to become Annie E7-121. People adopted the number-names, their family members' numbers, etc., and learned all the region codes (like knowing a telephone area code).\n\nUntil Inuit began studying in the south, many did not know that numbers were not normal parts of Christian and English naming systems. Then in 1969, the government started Project Surname, headed by Abe Okpik, to replace number-names with patrilineal \"family surnames\". But contemporary Inuit carvers and graphic artists still use their disk number as their signature on their works of art.\n\nA popular belief exists that the Inuit have an unusually large number of words for snow. This is not accurate, and results from a misunderstanding of the nature of polysynthetic languages. In fact, the Inuit have only a few base roots for snow: 'qanniq-' ('qanik-' in some dialects), which is used most often like the verb \"to snow\", and 'aput', which means \"snow\" as a substance. Parts of speech work very differently in the Inuit language than in English, so these definitions are somewhat misleading.\n\nThe Inuit languages can form very long words by adding more and more descriptive affixes to words. Those affixes may modify the syntactic and semantic properties of the base word, or may add qualifiers to it in much the same way that English uses adjectives or prepositional phrases to qualify nouns (e.g. \"falling snow\", \"blowing snow\", \"snow on the ground\", \"snow drift\", etc.)\n\nThe \"fact\" that there are many Inuit words for snow has been put forward so often that it has become a journalistic cliché.\n\nThe Inuit use a base-20 counting system.\n\nBecause the Inuit languages are spread over such a large area, divided between different nations and political units and originally reached by Europeans of different origins at different times, there is no uniform way of writing the Inuit language.\n\nCurrently there are six \"standard\" ways to write the languages:\n\nThough all except the syllabics use the Latin alphabet, all of them are a bit different from each other.\nMost Inuktitut in Nunavut and Nunavik is written using a script called Inuktitut syllabics, based on Canadian Aboriginal syllabics. The western part of Nunavut and the Northwest Territories use Latin alphabet usually identified as Inuinnaqtun. In Alaska, two other Latin alphabets are used. Nunatsiavut uses an alphabet devised by German-speaking Moravian missionaries, which included the letter \"kra\". Greenland's Latin alphabet was originally much like the one used in Nunatsiavut, but underwent a spelling reform in 1973 to bring the orthography in line with changes in pronunciation and better reflect the phonemic inventory of the language.\n\nInuktitut syllabics, used in Canada, is based on Cree syllabics, which was devised by the missionary James Evans based on Devanagari a Brahmi script. The present form of Canadian Inuktitut syllabics was adopted by the Inuit Cultural Institute in Canada in the 1970s. The Inuit in Alaska, the Inuvialuit, Inuinnaqtun speakers, and Inuit in Greenland and Labrador use Latin alphabets.\n\nThough presented in syllabic form, syllabics is not a true syllabary, but an abugida, since syllables starting with the same consonant are written with graphically similar letters.\n\nAll of the characters needed for Inuktitut syllabics are available in the Unicode character repertoire, in the blocks Unified Canadian Aboriginal Syllabics.\n\n\n\n\n\n"}
{"id": "15229", "url": "https://en.wikipedia.org/wiki?curid=15229", "title": "Ibn Battuta", "text": "Ibn Battuta\n\nIbn Battuta (; ; fully ; Arabic: ) (February 25, 13041368 or 1369) was a Muslim Moroccan scholar and explorer who widely travelled the medieval world. Over a period of thirty years, Ibn Battuta visited most of the Islamic world and many non-Muslim lands, including Central Asia, Southeast Asia, South Asia and China. Near the end of his life, he dictated an account of his journeys, titled \"A Gift to Those Who Contemplate the Wonders of Cities and the Marvels of Travelling\".\n\nAll that is known about Ibn Battuta's life comes from the autobiographical information included in the account of his travels, which records that he was of Berber descent, born into a family of Islamic legal scholars in Tangier, Morocco, on 24 February 1304, during the reign of the Marinid dynasty. He claimed descent from a Berber tribe known as the Lawata. As a young man, he would have studied at a Sunni Maliki madh'hab (Islamic jurisprudence school), the dominant form of education in North Africa at that time. Maliki Muslims requested Ibn Battuta serve as their religious judge as he was from an area where it was practised.\n\nIn June 1325, at the age of twenty-one, Ibn Battuta set off from his hometown on a \"hajj\", or pilgrimage, to Mecca, a journey that would ordinarily take sixteen months. He would not see Morocco again for twenty-four years.\n\nI set out alone, having neither fellow-traveller in whose companionship I might find cheer, nor caravan whose part I might join, but swayed by an overmastering impulse within me and a desire long-cherished in my bosom to visit these illustrious sanctuaries. So I braced my resolution to quit my dear ones, female and male, and forsook my home as birds forsake their nests. My parents being yet in the bonds of life, it weighed sorely upon me to part from them, and both they and I were afflicted with sorrow at this separation.\nHe travelled to Mecca overland, following the North African coast across the sultanates of Abd al-Wadid and Hafsid. The route took him through Tlemcen, Béjaïa, and then Tunis, where he stayed for two months. For safety, Ibn Battuta usually joined a caravan to reduce the risk of being robbed. He took a bride in the town of Sfax, the first in a series of marriages that would feature in his travels.\n\nIn the early spring of 1326, after a journey of over , Ibn Battuta arrived at the port of Alexandria, at the time part of the Bahri Mamluk empire. He met two ascetic pious men in Alexandria. One was Sheikh Burhanuddin who is supposed to have foretold the destiny of Ibn Battuta as a world traveller saying \"It seems to me that you are fond of foreign travel. You will visit my brother Fariduddin in India, Rukonuddin in Sind and Burhanuddin in China. Convey my greetings to them\". Another pious man Sheikh Murshidi interpreted the meaning of a dream of Ibn Battuta that he was meant to be a world traveller.\n\nHe spent several weeks visiting sites in the area, and then headed inland to Cairo, the capital of the Mamluk Sultanate and an important city. After spending about a month in Cairo, he embarked on the first of many detours within the relative safety of Mamluk territory. Of the three usual routes to Mecca, Ibn Battuta chose the least-travelled, which involved a journey up the Nile valley, then east to the Red Sea port of Aydhab. Upon approaching the town, however, a local rebellion forced him to turn back.\n\nIbn Battuta returned to Cairo and took a second side trip, this time to Mamluk-controlled Damascus. During his first trip he had encountered a holy man who prophesied that he would only reach Mecca by travelling through Syria. The diversion held an added advantage; because of the holy places that lay along the way, including Hebron, Jerusalem, and Bethlehem, the Mamluk authorities spared no efforts in keeping the route safe for pilgrims. Without this help many travellers would be robbed and murdered.\n\nAfter spending the Muslim month of Ramadan in Damascus, he joined a caravan travelling the south to Medina, site of the Mosque of the Islamic prophet Muhammad. After four days in the town, he journeyed on to Mecca, where completing his pilgrimage he took the honorific status of \"El-Hajji\". Rather than returning home, Ibn Battuta decided to continue on, choosing as his next destination the Ilkhanate, a Mongol Khanate, to the northeast.\n\nOn 17 November 1326, following a month spent in Mecca, Ibn Battuta joined a large caravan of pilgrims returning to Iraq across the Arabian Peninsula. The group headed north to Medina and then, travelling at night, turned northeast across the Najd plateau to Najaf, on a journey that lasted about two weeks. In Najaf, he visited the mausoleum of Ali, the Fourth Caliph.\n\nThen, instead of continuing on to Baghdad with the caravan, Ibn Battuta started a six-month detour that took him into Persia. From Najaf, he journeyed to Wasit, then followed the river Tigris south to Basra. His next destination was the town of Isfahan across the Zagros Mountains in Persia. He then headed south to Shiraz, a large, flourishing city spared the destruction wrought by Mongol invaders on many more northerly towns. Finally, he returned across the mountains to Baghdad, arriving there in June 1327. Parts of the city were still ruined from the damage inflicted by Hulago Khan's invading army in 1258.\n\nIn Baghdad, he found Abu Sa'id, the last Mongol ruler of the unified Ilkhanate, leaving the city and heading north with a large retinue. Ibn Battuta joined the royal caravan for a while, then turned north on the Silk Road to Tabriz, the first major city in the region to open its gates to the Mongols and by then an important trading centre as most of its nearby rivals had been razed by the Mongol invaders.\n\nIbn Battuta left again for Baghdad, probably in July, but first took an excursion northwards along the river Tigris. He visited Mosul, where he was the guest of the Ilkhanate governor, and then the towns of Cizre (Jazirat ibn 'Umar) and Mardin in modern-day Turkey. At a hermitage on a mountain near Sinjar, he met a Kurdish mystic who gave him some silver coins. Once back in Mosul, he joined a \"feeder\" caravan of pilgrims heading south to Baghdad, where they would meet up with the main caravan that crossed the Arabian Desert to Mecca. Ill with diarrhoea, he arrived in the city weak and exhausted for his second \"hajj\".\n\nIbn Battuta remained in Mecca for some time (the \"Rihla\" suggests about three years, from September 1327 until autumn 1330). Problems with chronology, however, lead commentators to suggest that he may have left after the 1328 \"hajj\".\n\nAfter the \"hajj\" in either 1328 or 1330, he made his way to the port of Jeddah on the Red Sea coast. From there he followed the coast in a series of boats making slow progress against the prevailing south-easterly winds. Once in Yemen he visited Zabīd and later the highland town of Ta'izz, where he met the Rasulid dynasty king \"(Malik)\" Mujahid Nur al-Din Ali. Ibn Battuta also mentions visiting Sana'a, but whether he actually did so is doubtful. In all likelihood, he went directly from Ta'izz to the important trading port of Aden, arriving around the beginning of 1329 or 1331.\n\nFrom Aden, Ibn Battuta embarked on a ship heading for Zeila on the coast of Somalia. He then moved on to Cape Guardafui further down the Somalia seaboard, spending about a week in each location. Later he would visit Mogadishu, the then pre-eminent city of the \"Land of the Berbers\" (بلد البربر \"Balad al-Barbar\", the medieval Arabic term for the Horn of Africa).\n\nWhen Ibn Battuta arrived in 1331, Mogadishu stood at the zenith of its prosperity. He described it as \"an exceedingly large city\" with many rich merchants, noted for its high-quality fabric that was exported to other countries, including Egypt. Ibn Battuta added that the city was ruled by a Somali Sultan, Abu Bakr ibn Sayx 'Umar, who was originally from Berbera in northern Somalia and spoke both Somali (referred to by Battuta as \"Mogadishan\", the Benadir dialect of Somali) and Arabic with equal fluency. The Sultan also had a retinue of wazirs (ministers), legal experts, commanders, royal eunuchs, and assorted hangers-on at his beck and call.\n\nIbn Battuta continued by ship south to the Swahili Coast, a region then known in Arabic as the \"Bilad al-Zanj\" (\"Land of the Zanj\"), with an overnight stop at the island town of Mombasa. Although relatively small at the time, Mombasa would become important in the following century. After a journey along the coast, Ibn Battuta next arrived in the island town of Kilwa in present-day Tanzania, which had become an important transit centre of the gold trade. He described the city as \"one of the finest and most beautifully built towns; all the buildings are of wood, and the houses are roofed with \"dīs\" reeds\".\n\nIbn Battuta recorded his visit to the Kilwa Sultanate in 1330, and commented favorably on the humility and religion of its ruler, Sultan al-Hasan ibn Sulaiman, a descendant of the legendary Ali ibn al-Hassan Shirazi. He further wrote that the authority of the Sultan extended from Malindi in the north to Inhambane in the south and was particularly impressed by the planning of the city, believing it to be the reason for Kilwa's success along the coast. During this period, he described the construction of the Palace of Husuni Kubwa and a significant extension to the Great Mosque of Kilwa, which was made of coral stones and was the largest Mosque of its kind. With a change in the monsoon winds, Ibn Battuta sailed back to Arabia, first to Oman and the Strait of Hormuz then on to Mecca for the \"hajj\" of 1330 (or 1332).\n\nAfter his third pilgrimage to Mecca, Ibn Battuta decided to seek employment with the Muslim Sultan of Delhi, Muhammad bin Tughluq. In the autumn of 1330 (or 1332), he set off for the Seljuk controlled territory of Anatolia with the intention of taking an overland route to India. He crossed the Red Sea and the Eastern Desert to reach the Nile valley and then headed north to Cairo. From there he crossed the Sinai Peninsula to Palestine and then travelled north again through some of the towns that he had visited in 1326. From the Syrian port of Latakia, a Genoese ship took him (and his companions) to Alanya on the southern coast of modern-day Turkey.\n\nHe then journeyed westwards along the coast to the port of Antalya. In the town he met members of one of the semi-religious \"fityan\" associations. These were a feature of most Anatolian towns in the 13th and 14th centuries. The members were young artisans and had at their head a leader with the title of \"Akhis\". The associations specialised in welcoming travellers. Ibn Battuta was very impressed with the hospitality that he received and would later stay in their hospices in more than 25 towns in Anatolia. From Antalya Ibn Battuta headed inland to Eğirdir which was the capital of the Hamidids. He spent Ramadan (June 1331 or May 1333) in the city.\n\nFrom this point the itinerary across Anatolia in the \"Rihla\" is confused. Ibn Battuta describes travelling westwards from Eğirdir to Milas and then skipping eastward past Eğirdir to Konya. He then continues travelling in an easterly direction, reaching Erzurum from where he skips back to Birgi which lies north of Milas. Historians believe that Ibn Battuta visited a number of towns in central Anatolia, but not in the order that he describes.\n\nFrom Sinope he took a sea route to the Crimean Peninsula, arriving in the Golden Horde realm. He went to the port town of Azov, where he met with the emir of the Khan, then to the large and rich city of Majar. He left Majar to meet with Uzbeg Khan's travelling court (\"Orda\"), which was at the time near Beshtau mountain. From there he made a journey to Bolghar, which became the northernmost point he reached, and noted its unusually (for a subtropics dweller) short nights in summer. Then he returned to the Khan's court and with it moved to Astrakhan.\n\nIbn Battuta recorded that while in Bolghar he wanted to travel further north into the land of darkness. The land is snow-covered throughout (northern Siberia) and the only means of transport is dog-drawn sled. There lived a mysterious people who were reluctant to show themselves. They traded with southern people in a peculiar way. Southern merchants brought various goods and placed them in an open area on the snow in the night, then returned to their tents. Next morning they came to the place again and found their merchandise taken by the mysterious people, but in exchange they found fur-skins which could be used for making valuable coats, jackets, and other winter garments. The trade was done between merchants and the mysterious people without seeing each other. As Ibn Battuta was not a merchant and saw no benefit of going there he abandoned the travel to this land of darkness.\nWhen they reached Astrakhan, Öz Beg Khan had just given permission for one of his pregnant wives, Princess Bayalun, a daughter of Byzantine emperor Andronikos III Palaiologos, to return to her home city of Constantinople to give birth. Ibn Battuta talked his way into this expedition, which would be his first beyond the boundaries of the Islamic world.\n\nArriving in Constantinople towards the end of 1332 (or 1334), he met the Byzantine emperor Andronikos III Palaiologos. He visited the great church of Hagia Sophia and spoke with an Eastern Orthodox priest about his travels in the city of Jerusalem. After a month in the city, Ibn Battuta returned to Astrakhan, then arrived in the capital city Sarai al-Jadid and reported the accounts of his travels to Sultan Öz Beg Khan (r. 1313–1341). Then he continued past the Caspian and Aral Seas to Bukhara and Samarkand, where he visited the court of another Mongolian king, Tarmashirin (r. 1331–1334) of the Chagatai Khanate. From there, he journeyed south to Afghanistan, then crossed into India via the mountain passes of the Hindu Kush. In the \"Rihla\", he mentions these mountains and the history of the range in slave trading. He wrote,\n\nIbn Battuta and his party reached the Indus River on 12 September 1333. From there, he made his way to Delhi and became acquainted with the sultan, Muhammad bin Tughluq.\n\nMuhammad bin Tughluq was renowned as the wealthiest man in the Muslim world at that time. He patronized various scholars, Sufis, qadis, viziers and other functionaries in order to consolidate his rule. As with Mamluk Egypt, the Tughlaq Dynasty was a rare vestigial example of Muslim rule in Asia after the Mongol invasion. On the strength of his years of study in Mecca, Ibn Battuta was appointed a \"qadi\", or judge, by the sultan. However, he found it difficult to enforce Islamic law beyond the sultan's court in Delhi, due to lack of Islamic appeal in India.\n\nIt is uncertain by which route Ibn Battuta entered the Indian subcontinent. He may have entered via the Khyber Pass and Peshawar, or further south. He crossed the Sutlej River near the city of Pakpattan, in modern-day Pakistan, where he paid obeisance at the shrine of Baba Farid, before crossing southwest into Rajput country. From the Rajput Kingdom of Sarsatti, Battuta visited Hansi in India, describing it as \"among the most beautiful cities, the best constructed and the most populated; it is surrounded with a strong wall, and its founder is said to be one of the great infidel kings, called Tara\". Upon his arrival in Sindh, Ibn Battuta mentions the Indian rhinoceros that lived on the banks of the Indus.\n\nThe Sultan was erratic even by the standards of the time and for six years Ibn Battuta veered between living the high life of a trusted subordinate and falling under suspicion of treason for a variety of offences. His plan to leave on the pretext of taking another \"hajj\" was stymied by the Sultan. The opportunity for Battuta to leave Delhi finally arose in 1341 when an embassy arrived from Yuan dynasty China asking for permission to rebuild a Himalayan Buddhist temple popular with Chinese pilgrims.\n\nIbn Battuta was given charge of the embassy but en route to the coast at the start of the journey to China, he and his large retinue were attacked by a group of bandits. Separated from his companions, he was robbed and nearly lost his life. Despite this setback, within ten days he had caught up with his group and continued on to Khambhat in the Indian state of Gujarat. From there, they sailed to Calicut (now known as Kozhikode), where Portuguese explorer Vasco da Gama would land two centuries later. While in Calicut, Battuta was the guest of the ruling Zamorin. While Ibn Battuta visited a mosque on shore, a storm arose and one of the ships of his expedition sank. The other ship then sailed without him only to be seized by a local Sumatran king a few months later.\n\nAfraid to return to Delhi and be seen as a failure, he stayed for a time in southern India under the protection of Jamal-ud-Din, ruler of the small but powerful Nawayath sultanate on the banks of the Sharavathi river next to the Arabian Sea. This area is today known as Hosapattana and lies in the Honavar administrative district of Uttara Kannada. Following the overthrow of the sultanate, Ibn Battuta had no choice but to leave India. Although determined to continue his journey to China, he first took a detour to visit the Maldive Islands where he worked as a judge.\nHe spent nine months on the islands, much longer than he had intended. As a \"Chief Qadi\", his skills were highly desirable in the formerly Buddhist nation that had recently converted to Islam. Half-kidnapped into staying, he became chief judge and married into the royal family of Omar I. He became embroiled in local politics and left when his strict judgments in the laissez-faire island kingdom began to chafe with its rulers. In the \"Rihla\" he mentions his dismay at the local women going about with no clothing above the waist, and the locals taking no notice when he complained. From the Maldives, he carried on to Sri Lanka and visited Sri Pada and Tenavaram temple.\n\nIbn Battuta's ship almost sank on embarking from Sri Lanka, only for the vessel that came to his rescue to suffer an attack by pirates. Stranded onshore, he worked his way back to the Madurai kingdom in India. Here he spent some time in the court of the short-lived Madurai Sultanate under Ghiyas-ud-Din Muhammad Damghani, from where he returned to the Maldives and boarded a Chinese junk, still intending to reach China and take up his ambassadorial post.\n\nHe reached the port of Chittagong in modern-day Bangladesh intending to travel to Sylhet to meet Shah Jalal, who became so renowned that Ibn Battuta, then in Chittagong, made a one-month journey through the mountains of Kamaru near Sylhet to meet him. On his way to Sylhet, Ibn Battuta was greeted by several of Shah Jalal's disciples who had come to assist him on his journey many days before he had arrived. At the meeting in 1345 CE, Ibn Battuta noted that Shah Jalal was tall and lean, fair in complexion and lived by the mosque in a cave, where his only item of value was a goat he kept for milk, butter, and yogurt. He observed that the companions of the Shah Jalal were foreign and known for their strength and bravery. He also mentions that many people would visit the Shah to seek guidance. Ibn Battuta went further north into Assam, then turned around and continued with his original plan.\n\nIn 1345, Ibn Battuta travelled on to Samudra Pasai Sultanate in present-day Aceh, Northern Sumatra, where he notes in his travel log that the ruler of Samudra Pasai was a pious Muslim named Sultan Al-Malik Al-Zahir Jamal-ad-Din, who performed his religious duties with utmost zeal and often waged campaigns against animists in the region. The island of Sumatra, according to Ibn Battuta, was rich in camphor, areca nut, cloves, and tin.\n\nThe \"madh'hab\" he observed was Imam Al-Shafi‘i, whose customs were similar to those he had previously seen in coastal India, especially among the Mappila Muslims, who were also followers of Imam Al-Shafi‘i. At that time Samudra Pasai marked the end of Dar al-Islam, because no territory east of this was ruled by a Muslim. Here he stayed for about two weeks in the wooden walled town as a guest of the sultan, and then the sultan provided him with supplies and sent him on his way on one of his own junks to China.\n\nIbn Battuta first sailed to Malacca on the Malay Peninsula which he called \"Mul Jawi\". He met the ruler of Malacca and stayed as a guest for three days.\n\nIbn Battuta then sailed to a state called Kaylukari in the land of Tawalisi, where he met Urduja, a local princess. Urduja was a brave warrior, and her people are opponents of the Yuan dynasty. She was described as an \"idolater\", but could write the phrase Bismillah in Islamic calligraphy. The locations of Kaylukari and Tawalisi are disputed. Kaylukari might referred to Po Klong Garai in Champa (now southern Vietnam), and Urduja might be an aristocrat of Champa or the Trần dynasty. Filipinos widely believe that Kaylukari was in present-day Pangasinan Province of the Philippines. In modern times, Urduja has been featured in Filipino textbooks and films as a national heroine. Numerous other locations have been proposed, ranging from Java to somewhere in Guangdong Province, China. However, Sir Henry Yule and William Henry Scott consider both Tawilisi and Urduja to be entirely fictitious. (See Tawalisi for details.)\n\nFrom Kaylukari, Ibn Battuta finally reached Quanzhou in Fujian Province, China.\n\nIn the year 1345 Ibn Battuta arrived at Quanzhou in China's Fujian province, then under the rule of the Mongols. One of the first things he noted was that Muslims referred to the city as \"Zaitun\" (meaning olive), but Ibn Battuta could not find any olives anywhere. He mentioned local artists and their mastery in making portraits of newly arrived foreigners; these were for security purposes. Ibn Battuta praised the craftsmen and their silk and porcelain; as well as fruits such as plums and watermelons and the advantages of paper money.\n\nHe described the manufacturing process of large ships in the city of Quanzhou. He also mentioned Chinese cuisine and its usage of animals such as frogs, pigs and even dogs which were sold in the markets, and noted that the chickens in China were larger than those in the west. Scholars however have pointed out numerous errors given in Ibn Battuta's account of China, for example confusing the Yellow River with the Grand Canal and other waterways, as well as believing that porcelain was made from coal.\n\nIn Quanzhou, Ibn Battuta was welcomed by the head of the local Muslim merchants (possibly a fānzhǎng or \"Leader of Foreigners\" ）and Sheikh al-Islam (Imam), who came to meet him with flags, drums, trumpets and musicians. Ibn Battuta noted that the Muslim populace lived within a separate portion in the city where they had their own mosques, bazaars and hospitals. In Quanzhou, he met two prominent Persians, Burhan al-Din of Kazerun and Sharif al-Din from Tabriz (both of whom were influential figures noted in the \"Yuan History\" as \"A-mi-li-ding\" and \"Sai-fu-ding\", respectively). While in Quanzhou he ascended the \"Mount of the Hermit\" and briefly visited a well-known Taoist monk in a cave.\n\nHe then travelled south along the Chinese coast to Guangzhou, where he lodged for two weeks with one of the city's wealthy merchants.\n\nFrom Guangzhou he went north to Quanzhou and then proceeded to the city of Fuzhou, where he took up residence with Zahir al-Din and was proud to meet Kawam al-Din and a fellow countryman named Al-Bushri of Ceuta, who had become a wealthy merchant in China. Al-Bushri accompanied Ibn Battuta northwards to Hangzhou and paid for the gifts that Ibn Battuta would present to the Mongolian Emperor Togon-temür of the Yuan Dynasty.\n\nIbn Battuta said that Hangzhou was one of the largest cities he had ever seen, and he noted its charm, describing that the city sat on a beautiful lake surrounded by gentle green hills. He mentions the city's Muslim quarter and resided as a guest with a family of Egyptian origin. During his stay at Hangzhou he was particularly impressed by the large number of well-crafted and well-painted Chinese wooden ships, with coloured sails and silk awnings, assembling in the canals. Later he attended a banquet of the Yuan Mongol administrator of the city named Qurtai, who according to Ibn Battuta, was very fond of the skills of local Chinese conjurers. Ibn Battuta also mentions locals who worship the Solar deity.\n\nHe described floating through the Grand Canal on a boat watching crop fields, orchids, merchants in black-silk, and women in flowered-silk and priests also in silk. In Beijing, Ibn Battuta referred to himself as the long-lost ambassador from the Delhi Sultanate and was invited to the Yuan imperial court of Togon-temür (who according to Ibn Battuta was worshipped by some people in China). Ibn Batutta noted that the palace of Khanbaliq was made of wood and that the ruler's \"head wife\" (Empress Gi) held processions in her honour.\n\nIbn Battuta also wrote he had heard of \"the rampart of Yajuj and Majuj\" that was \"sixty days' travel\" from the city of Zeitun (Quanzhou); Hamilton Alexander Rosskeen Gibb notes that Ibn Battuta believed that the Great Wall of China was built by Dhul-Qarnayn to contain Gog and Magog as mentioned in the Quran. However, Ibn Battuta, who asked about the wall in China, could find no one who had either seen it or knew of anyone who had seen it.\n\nIbn Battuta travelled from Beijing to Hangzhou, and then proceeded to Fuzhou. Upon his return to Quanzhou, he soon boarded a Chinese junk owned by the Sultan of Samudera Pasai Sultanate heading for Southeast Asia, whereupon Ibn Battuta was unfairly charged a hefty sum by the crew and lost much of what he had collected during his stay in China.\n\nBattuta claimed that the Mongol Khan (Qan) had interned with him in his grave, six slave soldiers and four girl slaves. Silver, gold, weapons, and carpets were put into the grave.\n\nAfter returning to Quanzhou in 1346, Ibn Battuta began his journey back to Morocco. In Kozhikode, he once again considered throwing himself at the mercy of Muhammad bin Tughluq in Delhi, but thought better of it and decided to carry on to Mecca. On his way to Basra he passed through the Strait of Hormuz, where he learned that Abu Sa'id, last ruler of the Ilkhanate Dynasty had died in Persia. Abu Sa'id's territories had subsequently collapsed due to a fierce civil war between the Persians and Mongols.\n\nIn 1348, Ibn Battuta arrived in Damascus with the intention of retracing the route of his first \"hajj\". He then learned that his father had died 15 years earlier and death became the dominant theme for the next year or so. The Black Death had struck and he was on hand as it spread through Syria, Palestine, and Arabia. After reaching Mecca he decided to return to Morocco, nearly a quarter of a century after leaving home. On the way he made one last detour to Sardinia, then in 1349, returned to Tangier by way of Fez, only to discover that his mother had also died a few months before.\n\nAfter a few days in Tangier, Ibn Battuta set out for a trip to the Muslim-controlled territory of al-Andalus on the Iberian Peninsula. King Alfonso XI of Castile and León had threatened to attack Gibraltar, so in 1350, Ibn Battuta joined a group of Muslims leaving Tangier with the intention of defending the port. By the time he arrived, the Black Death had killed Alfonso and the threat of invasion had receded, so he turned the trip into a sight-seeing tour, travelling through Valencia and ending up in Granada.\n\nAfter his departure from al-Andalus he decided to travel through Morocco. On his return home, he stopped for a while in Marrakech, which was almost a ghost town following the recent plague and the transfer of the capital to Fez.\n\nOnce more Ibn Battuta returned to Tangier, but only stayed for a short while. In 1324, two years before his first visit to Cairo, the West African Malian \"Mansa\", or king of kings, Musa had passed through the same city on his own \"hajj\" and caused a sensation with a display of extravagant riches brought from his gold-rich homeland. Although Ibn Battuta never mentioned this visit specifically, when he heard the story it may have planted a seed in his mind as he then decided to cross the Sahara and visit the Muslim kingdoms on its far side.\n\nIn the autumn of 1351, Ibn Battuta left Fez and made his way to the town of Sijilmasa on the northern edge of the Sahara in present-day Morocco. There he bought a number of camels and stayed for four months. He set out again with a caravan in February 1352 and after 25 days arrived at the dry salt lake bed of Taghaza with its salt mines. All of the local buildings were made from slabs of salt by the slaves of the Masufa tribe, who cut the salt in thick slabs for transport by camel. Taghaza was a commercial centre and awash with Malian gold, though Ibn Battuta did not form a favourable impression of the place, recording that it was plagued by flies and the water was brackish.\n\nAfter a ten-day stay in Taghaza, the caravan set out for the oasis of Tasarahla (probably Bir al-Ksaib) where it stopped for three days in preparation for the last and most difficult leg of the journey across the vast desert. From Tasarahla, a Masufa scout was sent ahead to the oasis town of Oualata, where he arranged for water to be transported a distance of four days travel where it would meet the thirsty caravan. Oualata was the southern terminus of the trans-Saharan trade route and had recently become part of the Mali Empire. Altogether, the caravan took two months to cross the of desert from Sijilmasa.\n\nFrom there, Ibn Battuta travelled southwest along a river he believed to be the Nile (it was actually the river Niger), until he reached the capital of the Mali Empire. There he met \"Mansa\" Suleyman, king since 1341. Ibn Battuta disapproved of the fact that female slaves, servants and even the daughters of the sultan went about exposing parts of their bodies not befitting a Muslim. He left the capital in February accompanied by a local Malian merchant and journeyed overland by camel to Timbuktu. Though in the next two centuries it would become the most important city in the region, at that time it was a small city and relatively unimportant. It was during this journey that Ibn Battuta first encountered a hippopotamus. The animals were feared by the local boatmen and hunted with lances to which strong cords were attached. After a short stay in Timbuktu, Ibn Battuta journeyed down the Niger to Gao in a canoe carved from a single tree. At the time Gao was an important commercial center.\n\nAfter spending a month in Gao, Ibn Battuta set off with a large caravan for the oasis of Takedda. On his journey across the desert, he received a message from the Sultan of Morocco commanding him to return home. He set off for Sijilmasa in September 1353, accompanying a large caravan transporting 600 female slaves, and arrived back in Morocco early in 1354.\n\nIbn Battuta's itinerary gives scholars a glimpse as to when Islam first began to spread into the heart of west Africa.\nAfter returning home from his travels in 1354, and at the suggestion of the Marinid ruler of Morocco, Abu Inan Faris, Ibn Battuta dictated an account of his journeys to Ibn Juzayy, a scholar whom he had previously met in Granada. The account is the only source for Ibn Battuta's adventures. The full title of the manuscript may be translated as \"A Gift to Those Who Contemplate the Wonders of Cities and the Marvels of Travelling\" (, \"Tuḥfat an-Nuẓẓār fī Gharāʾib al-Amṣār wa ʿAjāʾib al-Asfār\"). However, it is often simply referred to as \"TheTravels\" (, \"Rihla\"), in reference to a standard form of Arabic literature.\n\nThere is no indication that Ibn Battuta made any notes or had any journal during his twenty-nine years of travelling. When he came to dictate an account of his experiences he had to rely on memory and manuscripts produced by earlier travellers. Ibn Juzayy did not acknowledge his sources and presented some of the earlier descriptions as Ibn Battuta's own observations. When describing Damascus, Mecca, Medina and some other places in the Middle East, he clearly copied passages from the account by the Andalusian Ibn Jubayr which had been written more than 150 years earlier. Similarly, most of Ibn Juzayy's descriptions of places in Palestine were copied from an account by the 13th-century traveller Muhammad al-Abdari.\n\nScholars do not believe that Ibn Battuta visited all the places he described and argue that in order to provide a comprehensive description of places in the Muslim world, he relied on hearsay evidence and made use of accounts by earlier travellers. For example, it is considered very unlikely that Ibn Battuta made a trip up the Volga River from New Sarai to visit Bolghar and there are serious doubts about a number of other journeys such as his trip to Sana'a in Yemen, his journey from Balkh to Bistam in Khorasan and his trip around Anatolia.\n\nIbn Battuta's claim that a Maghrebian called \"Abu'l Barakat the Berber\" converted the Maldives to Islam is contradicted by an entirely different story which says that the Maldives were converted to Islam after miracles were performed by a Tabrizi named Maulana Shaikh Yusuf Shams-ud-din according to the Tarikh, the official history of the Maldives.\n\nSome scholars have also questioned whether he really visited China. Ibn Battuta may have plagiarized entire sections of his descriptions of China lifted from works by other authors like \"Masalik al-absar fi mamalik al-amsar\" by Shihab al-Umari, Sulaiman al-Tajir, and possibly from Al Juwayni, Rashid al din and an Alexander romance. Furthermore, Ibn Battuta's description and Marco Polo's writings share extremely similar sections and themes, with some of the same commentary, e.g. it is unlikely that the 3rd Caliph Uthman ibn Affan had someone with the exact identical name in China who was encountered by Ibn Battuta.\n\nHowever, even if the \"Rihla\" is not fully based on what its author personally witnessed, it provides an important account of much of the 14th-century world. Sex slaves were used by Ibn Battuta such as in Delhi. He wedded and divorced women and had children to sex slaves in Malabar, Delhi, and Bukhara. Ibn Battuta insulted Greeks as \"enemies of Allah\", drunkards and \"swine eaters\", while at the same time in Ephesus he purchased and used a Greek girl who was one of his many slave girls in his \"harem\" through Byzantium, Khorasan, Africa, and Palestine. It was two decades before he again returned to find out what happened to one of his wives and child in Damascus.\n\nIbn Battuta often experienced culture shock in regions he visited where the local customs of recently converted peoples did not fit in with his orthodox Muslim background. Among the Turks and Mongols, he was astonished at the freedom and respect enjoyed by women and remarked that on seeing a Turkish couple in a bazaar one might assume that the man was the woman's servant when he was in fact her husband. He also felt that dress customs in the Maldives, and some sub-Saharan regions in Africa were too revealing.\n\nLittle is known about Ibn Battuta's life after completion of his \"Rihla\" in 1355. He was appointed a judge in Morocco and died in 1368 or 1369.\n\nIbn Battuta's work was unknown outside the Muslim world until the beginning of the 19th century, when the German traveller-explorer Ulrich Jasper Seetzen (1767–1811) acquired a collection of manuscripts in the Middle East, among which was a 94-page volume containing an abridged version of Ibn Juzayy's text. Three extracts were published in 1818 by the German orientalist Johann Kosegarten. A fourth extract was published the following year. French scholars were alerted to the initial publication by a lengthy review published in the \"Journal de Savants\" by the orientalist Silvestre de Sacy.\n\nThree copies of another abridged manuscript were acquired by the Swiss traveller Johann Burckhardt and bequeathed to the University of Cambridge. He gave a brief overview of their content in a book published posthumously in 1819. The Arabic text was translated into English by the orientalist Samuel Lee and published in London in 1829.\n\nIn the 1830s, during the French occupation of Algeria, the Bibliothèque Nationale (BNF) in Paris acquired five manuscripts of Ibn Battuta's travels, in which two were complete. One manuscript containing just the second part of the work is dated 1356 and is believed to be Ibn Juzayy's autograph. The BNF manuscripts were used in 1843 by the Irish-French orientalist Baron de Slane to produce a translation into French of Ibn Battuta's visit to the Sudan. They were also studied by the French scholars Charles Defrémery and Beniamino Sanguinetti. Beginning in 1853 they published a series of four volumes containing a critical edition of the Arabic text together with a translation into French. In their introduction Defrémery and Sanguinetti praised Lee's annotations but were critical of his translation which they claimed lacked precision, even in straightforward passages.\n\nIn 1929, exactly a century after the publication of Lee's translation, the historian and orientalist Hamilton Gibb published an English translation of selected portions of Defrémery and Sanguinetti's Arabic text. Gibb had proposed to the Hakluyt Society in 1922 that he should prepare an annotated translation of the entire \"Rihla\" into English. His intention was to divide the translated text into four volumes, each volume corresponding to one of the volumes published by Defrémery and Sanguinetti. The first volume was not published until 1958. Gibb died in 1971, having completed the first three volumes. The fourth volume was prepared by Charles Beckingham and published in 1994. Defrémery and Sanguinetti's printed text has now been translated into number of other languages.\n\n\n\n\n\n\n\n"}
{"id": "15231", "url": "https://en.wikipedia.org/wiki?curid=15231", "title": "Integrated Services Digital Network", "text": "Integrated Services Digital Network\n\nIntegrated Services Digital Network (ISDN) is a set of communication standards for simultaneous digital transmission of voice, video, data, and other network services over the traditional circuits of the public switched telephone network. It was first defined in 1988 in the CCITT red book. Prior to ISDN, the telephone system was viewed as a way to transport voice, with some special services available for data. The key feature of ISDN is that it integrates speech and data on the same lines, adding features that were not available in the classic telephone system. The ISDN standards define several kinds of access interfaces, such as Basic Rate Interface (BRI), Primary Rate Interface (PRI), Narrowband ISDN (N-ISDN), and Broadband ISDN (B-ISDN).\n\nISDN is a circuit-switched telephone network system, which also provides access to packet switched networks, designed to allow digital transmission of voice and data over ordinary telephone copper wires, resulting in potentially better voice quality than an analog phone can provide. It offers circuit-switched connections (for either voice or data), and packet-switched connections (for data), in increments of 64 kilobit/s. In some countries, ISDN found major market application for Internet access, in which ISDN typically provides a maximum of 128 kbit/s bandwidth in both upstream and downstream directions. Channel bonding can achieve a greater data rate; typically the ISDN B-channels of three or four BRIs (six to eight 64 kbit/s channels) are bonded.\n\nISDN is employed as the network, data-link and physical layers in the context of the OSI model. In common use, ISDN is often limited to usage to Q.931 and related protocols, which are a set of signaling protocols establishing and breaking circuit-switched connections, and for advanced calling features for the user. They were introduced in 1986.\n\nIn a videoconference, ISDN provides simultaneous voice, video, and text transmission between individual desktop videoconferencing systems and group (room) videoconferencing systems.\n\n\"Integrated services\" refers to ISDN's ability to deliver at minimum two simultaneous connections, in any combination of data, voice, video, and fax, over a single line. Multiple devices can be attached to the line, and used as needed. That means an ISDN line can take care of what were expected to be most people's complete communications needs (apart from broadband Internet access and entertainment television) at a much higher transmission rate, without forcing the purchase of multiple analog phone lines. It also refers to integrated switching and transmission in that telephone switching and carrier wave transmission are integrated rather than separate as in earlier technology.\n\nThe entry level interface to ISDN is the Basic Rate Interface (BRI), a 128 kbit/s service delivered over a pair of standard telephone copper wires. The 144 kbit/s payload rate is broken down into two 64 kbit/s bearer channels ('B' channels) and one 16 kbit/s signaling channel ('D' channel or data channel). This is sometimes referred to as 2B+D.\n\nThe interface specifies the following network interfaces:\n\nBRI-ISDN is very popular in Europe but is much less common in North America. It is also common in Japan — where it is known as INS64.\n\nThe other ISDN access available is the Primary Rate Interface (PRI), which is carried over an E1 (2048 kbit/s) in most parts of the world. An E1 is 30 'B' channels of 64 kbit/s, one 'D' channel of 64 kbit/s and a timing and alarm channel of 64 kbit/s. This is often referred to as 30B+2D.\nIn North America PRI service is delivered on one or more T1 carriers (often referred to as 23B+D) of 1544 kbit/s (24 channels). A PRI has 23 'B' channels and 1 'D' channel for signalling (Japan uses a circuit called a J1, which is similar to a T1). Inter-changeably but incorrectly, a PRI is referred to as T1 because it uses the T1 carrier format. A true T1 (commonly called \"Analog T1\" to avoid confusion) uses 24 channels of 64 kbit/s of in-band signaling. Each channel uses 56 kb for data and voice and 8 kb for signaling and messaging. PRI uses out of band signaling which provides the 23 B channels with clear 64 kb for voice and data and one 64 kb 'D' channel for signaling and messaging. In North America, Non-Facility Associated Signalling allows two or more PRIs to be controlled by a single D channel, and is sometimes called \"23B+D + n*24B\". D-channel backup allows for a second D channel in case the primary fails. NFAS is commonly used on a T3.\n\nPRI-ISDN is popular throughout the world, especially for connecting private branch exchanges to the public network.\n\nEven though many network professionals use the term \"ISDN\" to refer to the lower-bandwidth BRI circuit, in North America BRI is relatively uncommon whilst PRI circuits serving PBXs are commonplace.\n\nThe bearer channel (B) is a standard 64 kbit/s voice channel of 8 bits sampled at 8 kHz with G.711 encoding. B-channels can also be used to carry data, since they are nothing more than digital channels.\n\nEach one of these channels is known as a DS0.\n\nMost B channels can carry a 64kbit/s signal, but some were limited to 56K because they traveled over RBS lines. This was commonplace in the 20th century, but has since become less so.\n\nX.25 can be carried over the B or D channels of a BRI line, and over the B channels of a PRI line. X.25 over the D channel is used at many point-of-sale (credit card) terminals because it eliminates the modem setup, and because it connects to the central system over a B channel, thereby eliminating the need for modems and making much better use of the central system's telephone lines.\n\nX.25 was also part of an ISDN protocol called \"Always On/Dynamic ISDN\", or AO/DI. This allowed a user to have a constant multi-link PPP connection to the internet over X.25 on the D channel, and brought up one or two B channels as needed.\n\nIn theory, Frame Relay can operate over the D channel of BRIs and PRIs, but it is seldom, if ever, used.\n\nThere is a second viewpoint: that of the telephone industry, where ISDN is a core technology. A telephone network can be thought of as a collection of wires strung between switching systems. The common electrical specification for the signals on these wires is T1 or E1. Between telephone company switches, the signaling is performed via SS7. Normally, a PBX is connected via a T1 with robbed bit signaling to indicate on-hook or off-hook conditions and MF and DTMF tones to encode the destination number. ISDN is much better because messages can be sent much more quickly than by trying to encode numbers as long (100 ms per digit) tone sequences. This results in faster call setup times. Also, a greater number of features are available and fraud is reduced.\n\nISDN is also used as a smart-network technology intended to add new services to the public switched telephone network (PSTN) by giving users direct access to end-to-end circuit-switched digital services and as a backup or failsafe circuit solution for critical use data circuits.\n\nISDN is used heavily by the broadcast industry as a reliable way of switching low-latency, high-quality, long-distance audio circuits. In conjunction with an appropriate codec using MPEG or various manufacturers' proprietary algorithms, an ISDN BRI can be used to send stereo bi-directional audio coded at 128 kbit/s with 20 Hz – 20 kHz audio bandwidth, although commonly the G.722 algorithm is used with a single 64 kbit/s B channel to send much lower latency mono audio at the expense of audio quality. Where very high quality audio is required multiple ISDN BRIs can be used in parallel to provide a higher bandwidth circuit switched connection. BBC Radio 3 commonly makes use of three ISDN BRIs to carry 320 kbit/s audio stream for live outside broadcasts. ISDN BRI services are used to link remote studios, sports grounds and outside broadcasts into the main broadcast studio. ISDN via satellite is used by field reporters around the world. It is also common to use ISDN for the return audio links to remote satellite broadcast vehicles.\n\nIn many countries, such as the UK and Australia, ISDN has displaced the older technology of equalised analogue landlines, with these circuits being phased out by telecommunications providers. Use of IP-based streaming codecs such as Comrex ACCESS and ipDTL is becoming more widespread in the broadcast sector, using broadband internet to connect remote studios.\n\nISDN-BRI never gained popularity as a general use telephone access technology in Canada and the US, and remains a niche product. The service was seen as \"a solution in search of a problem\", and the extensive array of options and features were difficult for customers to understand and use. ISDN has long been known by derogatory backronyms highlighting these issues, such as It Still Does Nothing, Innovations Subscribers Don't Need,\" and I Still Don't kNow.\"\n\nOnce the concept of \"broadband Internet access\" came to be associated with data rates incoming to the customer at 256 kbit/s or more, and alternatives like ADSL grew in popularity, the consumer market for BRI did not develop. Its only remaining advantage is that, while ADSL has a functional distance limitation and can use ADSL loop extenders, BRI has a greater limit and can use repeaters. As such, BRI may be acceptable for customers who are too remote for ADSL. Widespread use of BRI is further stymied by some small North American CLECs such as CenturyTel having given up on it and not providing Internet access using it. However, AT&T in most states (especially the former SBC/SWB territory) will still install an ISDN BRI line anywhere a normal analog line can be placed and the monthly charge is roughly $55.\n\nISDN-BRI is currently primarily used in industries with specialized and very specific needs. High-end videoconferencing hardware made by companies such as Sony, Polycom, Tandberg, and LifeSize via the LifeSize Networker can bond up to 8 B-channels together (using a BRI circuit for every 2 channels) to provide digital, circuit-switched video connections to almost anywhere in the world. This is very expensive, and is being replaced by IP-based conferencing, but where cost concern is less of an issue than predictable quality and where a QoS-enabled IP does not exist, BRI is the preferred choice.\n\nMost modern non-VoIP PBXs use ISDN-PRI circuits. These are connected via T1 lines with the central office switch, replacing older analog two-way and direct inward dialing (DID) trunks. PRI is capable of delivering Calling Line Identification (CLID) in both directions so that the telephone number of an extension, rather than a company's main number, can be sent. It is still commonly used in recording studios, when a voice-over actor is in one studio (possibly telecommuting from home), but the director and producer are in a studio at another location. The ISDN protocol delivers channelized, not-over-the-Internet service, powerful call setup and routing features, faster setup and tear down, superior audio fidelity as compared to POTS (plain old telephone service), lower delay and, at higher densities, lower cost.\n\nIn 2013, Verizon announced it would no longer take orders for ISDN service in the Northeastern United States.\n\nTelstra provides the business customer with the ISDN services. There are five types of ISDN services which are ISDN2, ISDN2 Enhanced, ISDN10, ISDN20 and ISDN30. Telstra changed the minimum monthly charge for voice and data calls. In general, there are two group of ISDN service types; The Basic Rate services – ISDN 2 or ISDN 2 Enhanced. Another group of types are the Primary Rate services, ISDN 10/20/30 . Telstra announced that the new sales of ISDN product would be unavailable as of 31 January 2018. The final exit date of ISDN service and migration to the new service would be confirmed by 2022. \n\nBharat Sanchar Nigam Limited, Reliance Communications and Bharti Airtel are the largest communication service providers, and offer both ISDN BRI and PRI services across the country. Reliance Communications and Bharti Airtel uses the DLC technology for providing these services. With the introduction of broadband technology, the load on bandwidth is being absorbed by ADSL. ISDN continues to be an important backup network for point-to-point leased line customers such as banks, Eseva Centers, Life Insurance Corporation of India, and SBI ATMs.\n\nOn April 19, 1988, Japanese telecommunications company NTT began offering nationwide ISDN services trademarked INS Net 64, and INS Net 1500, a fruition of NTT's independent research and trial from the 1970s of what it referred to the INS (Information Network System).\n\nPreviously, in April 1985, Japanese digital telephone exchange hardware made by Fujitsu was used to experimentally deploy the world's first I interface ISDN. The I interface, unlike the older and incompatible Y interface, is what modern ISDN services use today.\n\nSince 2000, NTT's ISDN offering have been known as FLET's ISDN, incorporating the \"FLET's\" brand that NTT uses for all of its ISP offerings.\n\nIn Japan, the number of ISDN subscribers dwindled as alternative technologies such as ADSL, cable Internet access, and fiber to the home gained greater popularity. On November 2, 2010, NTT announced plans to migrate their backend from PSTN to the IP network from around 2020 to around 2025. For this migration, ISDN services will be retired, and fiber optic services are recommended as an alternative.\n\nIn the United Kingdom, British Telecom (BT) provides ISDN2e (BRI) as well as ISDN30 (PRI). Until April 2006, they also offered services named Home Highway and Business Highway, which were BRI ISDN-based services that offered integrated analogue connectivity as well as ISDN. Later versions of the Highway products also included built-in USB sockets for direct computer access. Home Highway was bought by many home users, usually for Internet connection, although not as fast as ADSL, because it was available before ADSL and in places where ADSL does not reach.\n\nIn early 2015, BT announced their intention to retire the UK's ISDN infrastructure by 2025.\n\nFrance Telecom offers ISDN services under their product name Numeris (2 B+D), of which a professional Duo and home Itoo version is available. ISDN is generally known as RNIS in France and has widespread availability. The introduction of ADSL is reducing ISDN use for data transfer and Internet access, although it is still common in more rural and outlying areas, and for applications such as business voice and point-of-sale terminals.\n\nIn Germany, ISDN was very popular with an installed base of 25 million channels (29% of all subscriber lines in Germany as of 2003 and 20% of all ISDN channels worldwide). Due to the success of ISDN, the number of installed analog lines was decreasing. Deutsche Telekom (DTAG) offered both BRI and PRI. Competing phone companies often offered ISDN only and no analog lines. However, these operators generally offered free hardware that also allows the use of POTS equipment, such as NTBAs with integrated terminal adapters. Because of the widespread availability of ADSL services, ISDN was primarily used for voice and fax traffic.\n\nUntil 2007 ISDN (BRI) and ADSL/VDSL were often bundled on the same line, mainly because the combination of ADSL with an analog line had no cost advantage over a combined ISDN-ADSL line. This advantage diminished when vendors of ISDN technology stopped manufacturing it and spare parts became hard to come by. Since then phone companies started introducing cheaper ADSL-only products using VoIP for telephony.\n\nSince the introduction of VDSL2 using outdoor MSANs, ISDN has become obsolete. Today new ISDN lines are not available anymore in Germany and existing ISDN lines will be phased out by 2018 and replaced by G.992.3 Annex J all-digital-mode ADSL.\n\nOTE, the incumbent telecommunications operator, offers ISDN BRI (BRA) services in Greece. Following the launch of ADSL in 2003, the importance of ISDN for data transfer began to decrease and is today limited to niche business applications with point-to-point requirements.\n\nA study of the German Department of Science shows the following spread of ISDN-channels per 1,000 inhabitants in the year 2005:\n\nIn ISDN, there are two types of channels, \"B\" (for \"bearer\") and \"D\" (for \"data\"). \"B channels\" are used for data (which may include voice), and \"D channels\" are intended for signaling and control (but can also be used for data).\n\nThere are two ISDN implementations. Basic Rate Interface (BRI), also called basic rate access (BRA) — consists of two B channels, each with bandwidth of 64 kbit/s, and one D channel with a bandwidth of 16 kbit/s. Together these three channels can be designated as 2B+D. Primary Rate Interface (PRI), also called primary rate access (PRA) in Europe — contains a greater number of B channels and a D channel with a bandwidth of 64 kbit/s. The number of B channels for PRI varies according to the nation: in North America and Japan it is 23B+1D, with an aggregate bit rate of 1.544 Mbit/s (T1); in Europe, India and Australia it is 30B+2D, with an aggregate bit rate of 2.048 Mbit/s (E1). Broadband Integrated Services Digital Network (BISDN) is another ISDN implementation and it is able to manage different types of services at the same time. It is primarily used within network backbones and employs ATM.\n\nAnother alternative ISDN configuration can be used in which the B channels of an ISDN BRI line are bonded to provide a total duplex bandwidth of 128 kbit/s. This precludes use of the line for voice calls while the internet connection is in use. The B channels of several BRIs can be bonded, a typical use is a 384K videoconferencing channel.\n\nUsing bipolar with eight-zero substitution encoding technique, call data is transmitted over the data (B) channels, with the signaling (D) channels used for call setup and management. Once a call is set up, there is a simple 64 kbit/s synchronous bidirectional data channel (actually implemented as two simplex channels, one in each direction) between the end parties, lasting until the call is terminated. There can be as many calls as there are bearer channels, to the same or different end-points. Bearer channels may also be multiplexed into what may be considered single, higher-bandwidth channels via a process called B channel BONDING, or via use of Multi-Link PPP \"bundling\" or by using an H0, H11, or H12 channel on a PRI.\n\nThe D channel can also be used for sending and receiving X.25 data packets, and connection to X.25 packet network, this is specified in X.31. In practice, X.31 was only commercially implemented in the UK, France, Japan and Germany.\n\nA set of \"reference points\" are defined in the ISDN standard to refer to certain points between the telco and the end user ISDN equipment.\n\nMost NT-1 devices can perform the functions of the NT2 as well, and so the S and T reference points are generally collapsed into the S/T reference point.\n\nIn North America, the NT1 device is considered customer premises equipment (CPE) and must be maintained by the customer, thus, the U interface is provided to the customer. In other locations, the NT1 device is maintained by the telco, and the S/T interface is provided to the customer. In India, service providers provide U interface and an NT1 may be supplied by Service provider as part of service offering.\n\nAmong the kinds of data that can be moved over the 64 kbit/s channels are pulse-code modulated voice calls, providing access to the traditional voice PSTN. This information can be passed between the network and the user end-point at call set-up time. In North America, ISDN is now used mostly as an alternative to analog connections, most commonly for Internet access. Some of the services envisioned as being delivered over ISDN are now delivered over the Internet instead. In Europe, and in Germany in particular, ISDN has been successfully marketed as a phone with features, as opposed to a POTS phone with few or no features. Meanwhile, features that were first available with ISDN (such as Three-Way Calling, Call Forwarding, Caller ID, etc.) are now commonly available for ordinary analog phones as well, eliminating this advantage of ISDN. Another advantage of ISDN was the possibility of multiple simultaneous calls (one call per B channel), e.g. for big families, but with the increased popularity and reduced prices of mobile telephony this has become less interesting as well, making ISDN unappealing to the private customer. However, ISDN is typically more reliable than POTS, and has a significantly faster call setup time compared with POTS, and IP connections over ISDN typically have some 30–35ms round trip time, as opposed to 120–180ms (both measured with otherwise unused lines) over 56k or V.34/V.92 modems, making ISDN more reliable and more efficient for telecommuters.\n\nWhere an analog connection requires a modem, an ISDN connection requires a terminal adapter (TA). The function of an ISDN terminal adapter is often delivered in the form of a PC card with an S/T interface, and single-chip solutions seem to exist, considering the plethora of combined ISDN- and ADSL-routers.\n\nISDN is commonly used in radio broadcasting. Since ISDN provides a high quality connection this assists in delivering good quality audio for transmission in radio. Most radio studios are equipped with ISDN lines as their main form of communication with other studios or standard phone lines. Equipment made by companies such as Telos/Omnia (the popular Zephyr codec), Comrex, Tieline and others are used regularly by radio broadcasters. Almost all live sports broadcasts on radio are backhauled to their main studios via ISDN connections.\n\nThe following is an example of a Primary Rate (PRI) ISDN call showing the Q.921/LAPD and the Q.931/Network message intermixed (i.e. exactly what was exchanged on the D-channel). The call is originating from the switch where the trace was taken and goes out to some other switch, possibly an end-office LEC, who terminates the call.\n\nThe first line format is <time> <D-channel> <Transmitted/Received> <LAPD/ISDN message ID>. If the message is an ISDN level message, then a decoding of the message is attempted showing the various Information Elements that make up the message. All ISDN messages are tagged with an ID number relative to the switch that started the call (local/remote). Following this optional decoding is a dump of the bytes of the message in <offset> <hex> ... <hex> <ascii> ... <ascii> format.\n\nThe RR messages at the beginning prior to the call are the keep alive messages. SETUP message indicate the start of the call. Each message is acknowledged by the other side with a RR.\n\n\nSpecifications defining the physical layer and part of the data link layers of ISDN:\n\nFrom the point of view of the OSI architecture, an ISDN line has a stack of three protocols\n\n"}
{"id": "15235", "url": "https://en.wikipedia.org/wiki?curid=15235", "title": "Genomic imprinting", "text": "Genomic imprinting\n\nGenomic imprinting is an epigenetic phenomenon that causes genes to be expressed in a parent-of-origin-specific manner. Forms of genomic imprinting have been demonstrated in fungi, plants and animals. As of 2014, there are about 150 imprinted genes known in the mouse and about half that in humans.\n\nGenomic imprinting is an inheritance process independent of the classical Mendelian inheritance. It is an epigenetic process that involves DNA methylation and histone methylation without altering the genetic sequence. These epigenetic marks are established (\"imprinted\") in the germline (sperm or egg cells) of the parents and are maintained through mitotic cell divisions in the somatic cells of an organism.\n\nAppropriate imprinting of certain genes is important for normal development. Human diseases involving genomic imprinting include Angelman syndrome and Prader–Willi syndrome.\n\nIn diploid organisms (like humans), the somatic cells possess two copies of the genome, one inherited from the father and one from the mother. Each autosomal gene is therefore represented by two copies, or alleles, with one copy inherited from each parent at fertilization. For the vast majority of autosomal genes, expression occurs from both alleles simultaneously. In mammals, however, a small proportion (<1%) of genes are imprinted, meaning that gene expression occurs from only one allele (some recent studies have questioned this assertion, claiming that the number of regions of parent-of-origin methylation in, for example, the human genome, is much larger than previously thought). The expressed allele is dependent upon its parental origin. For example, the gene encoding insulin-like growth factor 2 (IGF2/Igf2) is only expressed from the allele inherited from the father.\n\nThe term \"imprinting\" was first used to describe events in the insect \"Pseudococcus nipae\". In Pseudococcids (mealybugs) (Hemiptera, Coccoidea) both the male and female develop from a fertilised egg. In females, all chromosomes remain euchromatic and functional. In embryos destined to become males, one haploid set of chromosomes becomes heterochromatinised after the sixth cleavage division and remains so in most tissues; males are thus functionally haploid.\n\nThat imprinting might be a feature of mammalian development was suggested in breeding experiments in mice carrying reciprocal chromosomal translocations. Nucleus transplantation experiments in mouse zygotes in the early 1980s confirmed that normal development requires the contribution of both the maternal and paternal genomes. The vast majority of mouse embryos derived from parthenogenesis (called parthenogenones, with two maternal or egg genomes) and androgenesis (called androgenones, with two paternal or sperm genomes) die at or before the blastocyst/implantation stage. In the rare instances that they develop to postimplantation stages, gynogenetic embryos show better embryonic development relative to placental development, while for androgenones, the reverse is true. Nevertheless, for the latter, only a few have been described (in a 1984 paper).\n\nNo naturally occurring cases of parthenogenesis exist in mammals because of imprinted genes. However, in 2004, experimental manipulation by Japanese researchers of a paternal methylation imprint controlling the \"Igf2\" gene led to the birth of a mouse (named Kaguya) with two maternal sets of chromosomes, though it is not a true parthenogenone since cells from two different female mice were used. The researchers were able to succeed by using one egg from an immature parent, thus reducing maternal imprinting, and modifying it to express the gene Igf2, which is normally only expressed by the paternal copy of the gene.\n\nParthenogenetic/gynogenetic embryos have twice the normal expression level of maternally derived genes, and lack expression of paternally expressed genes, while the reverse is true for androgenetic embryos. It is now known that there are at least 80 imprinted genes in humans and mice, many of which are involved in embryonic and placental growth and development. Hybrid offspring of two species may exhibit unusual growth due to the novel combination of imprinted genes.\n\nVarious methods have been used to identify imprinted genes. In swine, Bischoff \"et al.\" 2009 compared transcriptional profiles using short-oligonucleotide microarrays to survey differentially expressed genes between parthenotes (2 maternal genomes) and control fetuses (1 maternal, 1 paternal genome). An intriguing study surveying the transcriptome of murine brain tissues revealed over 1300 imprinted gene loci (approximately 10-fold more than previously reported) by RNA-sequencing from F1 hybrids resulting from reciprocal crosses. The result however has been challenged by others who claimed that this is an overestimation by an order of magnitude due to flawed statistical analysis.\n\nIn domesticated livestock, single-nucleotide polymorphisms in imprinted genes influencing foetal growth and development have been shown to be associated with economically important production traits in cattle, sheep and pigs.\n\nAt the same time as the generation of the gynogenetic and androgenetic embryos discussed above, mouse embryos were also being generated that contained only small regions that were derived from either a paternal or maternal source. The generation of a series of such uniparental disomies, which together span the entire genome, allowed the creation of an imprinting map. Those regions which when inherited from a single parent result in a discernible phenotype contain imprinted gene(s). Further research showed that within these regions there were often numerous imprinted genes. Around 80% of imprinted genes are found in clusters such as these, called imprinted domains, suggesting a level of co-ordinated control. More recently, genome-wide screens to identify imprinted genes have used differential expression of mRNAs from control fetuses and parthenogenetic or androgenetic fetuses hybridized to expression arrays, allele-specific gene expression using SNP genotyping arrays, transcriptome sequencing, and in silico prediction pipelines.\n\nImprinting is a dynamic process. It must be possible to erase and re-establish imprints through each generation so that genes that are imprinted in an adult may still be expressed in that adult's offspring. (For example, the maternal genes that control insulin production will be imprinted in a male but will be expressed in any of the male's offspring that inherit these genes.) The nature of imprinting must therefore be epigenetic rather than DNA sequence dependent. In germline cells the imprint is erased and then re-established according to the sex of the individual, i.e. in the developing sperm (during spermatogenesis), a paternal imprint is established, whereas in developing oocytes (oogenesis), a maternal imprint is established. This process of erasure and reprogramming is necessary such that the germ cell imprinting status is relevant to the sex of the individual. In both plants and mammals there are two major mechanisms that are involved in establishing the imprint; these are DNA methylation and histone modifications.\n\nRecently, a new study has suggested a novel inheritable imprinting mechanism in humans that would be specific of placental tissue and that is independent of DNA methylation (the main and classical mechanism for genomic imprinting). Among the hypothetical explanations for this exclusively human phenomenon, two possible mechanisms have been proposed: either a histone modification that confers imprinting at novel placental-specific imprinted \"loci\" or, alternatively, a recruitment of DNMTs to these loci by a specific and unknown transcription factor that would be expressed during early trophoblast differentiation.\n\nThe grouping of imprinted genes within clusters allows them to share common regulatory elements, such as non-coding RNAs and differentially methylated regions (DMRs). When these regulatory elements control the imprinting of one or more genes, they are known as imprinting control regions (ICR). The expression of non-coding RNAs, such as \"Air\" on mouse chromosome 17 and KCNQ1OT1 on human chromosome 11p15.5, have been shown to be essential for the imprinting of genes in their corresponding regions.\n\nDifferentially methylated regions are generally segments of DNA rich in cytosine and guanine nucleotides, with the cytosine nucleotides methylated on one copy but not on the other. Contrary to expectation, methylation does not necessarily mean silencing; instead, the effect of methylation depends upon the default state of the region.\n\nThe control of expression of specific genes by genomic imprinting is unique to therian mammals (placental mammals and marsupials) and flowering plants. Imprinting of whole chromosomes has been reported in mealybugs (Genus: \"Pseudococcus\"). and a fungus gnat (\"Sciara\"). It has also been established that X-chromosome inactivation occurs in an imprinted manner in the extra-embryonic tissues of mice and all tissues in marsupials, where it is always the paternal X-chromosome which is silenced.\n\nThe majority of imprinted genes in mammals have been found to have roles in the control of embryonic growth and development, including development of the placenta. Other imprinted genes are involved in post-natal development, with roles affecting suckling and metabolism.\n\nA widely accepted hypothesis for the evolution of genomic imprinting is the \"parental conflict hypothesis\". Also known as the kinship theory of genomic imprinting, this hypothesis states that the inequality between parental genomes due to imprinting is a result of the differing interests of each parent in terms of the evolutionary fitness of their genes. The father's genes that encode for imprinting gain greater fitness through the success of the offspring, at the expense of the mother. The mother's evolutionary imperative is often to conserve resources for her own survival while providing sufficient nourishment to current and subsequent litters. Accordingly, paternally expressed genes tend to be growth-promoting whereas maternally expressed genes tend to be growth-limiting. In support of this hypothesis, genomic imprinting has been found in all placental mammals, where post-fertilisation offspring resource consumption at the expense of the mother is high; although it has also been found in oviparous birds where there is relatively little post-fertilisation resource transfer and therefore less parental conflict.\n\nHowever, our understanding of the molecular mechanisms behind genomic imprinting show that it is the maternal genome that controls much of the imprinting of both its own and the paternally-derived genes in the zygote, making it difficult to explain why the maternal genes would willingly relinquish their dominance to that of the paternally-derived genes in light of the conflict hypothesis.\n\nAnother hypothesis proposed is that some imprinted genes act coadaptively to improve both fetal development and maternal provisioning for nutrition and care. In it a subset of paternally expressed genes are co-expressed in both the placenta and the mother's hypothalamus. This would come about through selective pressure from parent-infant coadaptation to improve infant survival. Paternally expressed 3 (Peg3) is a gene for which this hypothesis may apply.\n\nOthers have approached their study of the origins of genomic imprinting from a different side, arguing that natural selection is operating on the role of epigenetic marks as machinery for homologous chromosome recognition during meiosis, rather than on their role in differential expression. This argument centers on the existence of epigenetic effects on chromosomes that do not directly affect gene expression, but do depend on which parent the chromosome originated from. This group of epigenetic changes that depend on the chromosome's parent of origin (including both those that affect gene expression and those that do not) are called parental origin effects, and include phenomena such as paternal X inactivation in the marsupials, nonrandom parental chromatid distribution in the ferns, and even mating type switching in yeast. This diversity in organisms that show parental origin effects has prompted theorists to place the evolutionary origin of genomic imprinting before the last common ancestor of plants and animals, over a billion years ago.\n\nNatural selection for genomic imprinting requires genetic variation in a population. A hypothesis for the origin of this genetic variation states that the host-defense system responsible for silencing foreign DNA elements, such as genes of viral origin, mistakenly silenced genes whose silencing turned out to be beneficial for the organism. There appears to be an over-representation of retrotransposed genes, that is to say genes that are inserted into the genome by viruses, among imprinted genes. It has also been postulated that if the retrotransposed gene is inserted close to another imprinted gene, it may just acquire this imprint.\n\nImprinting may cause problems in cloning, with clones having DNA that is not methylated in the correct positions. It is possible that this is due to a lack of time for reprogramming to be completely achieved. When a nucleus is added to an egg during somatic cell nuclear transfer, the egg starts dividing in minutes, as compared to the days or months it takes for reprogramming during embryonic development. If time is the responsible factor, it may be possible to delay cell division in clones, giving time for proper reprogramming to occur.\n\nAn allele of the \"callipyge\" (from the Greek for \"beautiful buttocks\"), or CLPG, gene in sheep produces large buttocks consisting of muscle with very little fat. The large-buttocked phenotype only occurs when the allele is present on the copy of chromosome 18 inherited from a sheep's father and is \"not\" on the copy of chromosome 18 inherited from that sheep's mother.\n\nIn vitro fertilisation, including ICSI, is associated with an increased risk of imprinting disorders, with an odds ratio of 3.7 (95% confidence interval 1.4 to 9.7).\n\nThe first imprinted genetic disorders to be described in humans were the reciprocally inherited Prader-Willi syndrome and Angelman syndrome. Both syndromes are associated with loss of the chromosomal region 15q11-13 (band 11 of the long arm of chromosome 15). This region contains the paternally expressed genes SNRPN and NDN and the maternally expressed gene UBE3A.\n\nDIRAS3 is a paternally expressed and maternally imprinted gene located on chromosome 1 in humans. Reduced DIRAS3 expression is linked to an increased risk of ovarian and breast cancers; in 41% of breast and ovarian cancers the protein encoded by DIRAS3 is not expressed, suggesting that it functions as a tumor suppressor gene Therefore, if uniparental disomy occurs and a person inherits both chromosomes from the mother, the gene will not be expressed and the individual is put at a greater risk for breast and ovarian cancer.\n\nOther conditions involving imprinting include Beckwith-Wiedemann syndrome, Silver-Russell syndrome, and pseudohypoparathyroidism.\n\nTransient neonatal diabetes mellitus can also involve imprinting.\n\nThe \"imprinted brain theory\" argues that unbalanced imprinting may be a cause of autism and psychosis.\n\nIn insects, imprinting affects entire chromosomes. In some insects the entire paternal genome is silenced in male offspring, and thus is involved in sex determination. The imprinting produces effects similar to the mechanisms in other insects that eliminate paternally inherited chromosomes in male offspring, including arrhenotoky.\n\nIn placental species, parent-offspring conflict can result in the evolution of strategies, such as genomic imprinting, for embryos to subvert maternal nutrient provisioning. Despite several attempts to find it, genomic imprinting has not been found in the platypus, reptiles, birds or fish. The absence of genomic imprinting in a placental reptile, the southern grass skink, is interesting as genomic imprinting was thought to be associated with the evolution of viviparity and placental nutrient transport.\n\nStudies in domestic livestock, such as dairy and beef cattle, have implicated imprinted genes (e.g. IGF2) in a range of economic traits, including dairy performance in Holstein-Friesian cattle.\n\nA similar imprinting phenomenon has also been described in flowering plants (angiosperms). During fertilisation of the egg cell, a second, separate fertilization event gives rise to the endosperm, an extraembryonic structure that nourishes the embryo in a manner analogous to the mammalian placenta. Unlike the embryo, the endosperm is often formed from the fusion of two maternal cells with a male gamete. This results in a triploid genome. The 2:1 ratio of maternal to paternal genomes appears to be critical for seed development. Some genes are found to be expressed from both maternal genomes while others are expressed exclusively from the lone paternal copy. It has been suggested that these imprinted genes are responsible for the triploid block effect in flowering plants that prevents hybridization between diploids and autotetraploids.\n\n\n"}
{"id": "15236", "url": "https://en.wikipedia.org/wiki?curid=15236", "title": "ICANN", "text": "ICANN\n\nThe Internet Corporation for Assigned Names and Numbers (ICANN ) is a nonprofit organization responsible for coordinating the maintenance and procedures of several databases related to the namespaces and numerical spaces of the Internet, ensuring the network's stable and secure operation. ICANN performs the actual technical maintenance work of the Central Internet Address pools and DNS root zone registries pursuant to the Internet Assigned Numbers Authority (IANA) function contract. The contract regarding the IANA stewardship functions between ICANN and the National Telecommunications and Information Administration (NTIA) of the United States Department of Commerce ended on October 1, 2016, formally transitioning the functions to the global multistakeholder community.\n\nMuch of its work has concerned the Internet's global Domain Name System (DNS), including policy development for internationalization of the DNS system, introduction of new generic top-level domains (TLDs), and the operation of root name servers. The numbering facilities ICANN manages include the Internet Protocol address spaces for IPv4 and IPv6, and assignment of address blocks to regional Internet registries. ICANN also maintains registries of Internet Protocol identifiers.\n\nICANN's primary principles of operation have been described as helping preserve the operational stability of the Internet; to promote competition; to achieve broad representation of the global Internet community; and to develop policies appropriate to its mission through bottom-up, consensus-based processes.\n\nICANN's creation was announced publicly on September 17, 1998, and it formally came into being on September 30, 1998, incorporated in the U.S. state of California. Originally headquartered in Marina del Rey in the same building as the University of Southern California's Information Sciences Institute (ISI), its offices are now in the Playa Vista neighborhood of Los Angeles.\n\nBefore the establishment of ICANN, the IANA function of administering registries of Internet protocol identifiers (including the distributing top-level domains and IP addresses) was performed by Jon Postel, a Computer Science researcher who had been involved in the creation of ARPANET, first at UCLA and then at USC-ISI. In 1997 Postel testified before Congress that this had come about as a \"side task\" to this research work. The Information Sciences Institute was funded by the U.S. Department of Defense, as was SRI International's Network Information Center, which also performed some assigned name functions.\n\nAs the Internet grew and expanded globally, the U.S. Department of Commerce initiated a process to establish a new organization to perform the IANA functions. On January 30, 1998, the National Telecommunications and Information Administration (NTIA), an agency of the U.S. Department of Commerce, issued for comment, \"A Proposal to Improve the Technical Management of Internet Names and Addresses.\" The proposed rule making, or \"Green Paper\", was published in the Federal Register on February 20, 1998, providing opportunity for public comment. NTIA received more than 650 comments as of March 23, 1998, when the comment period closed.\n\nThe Green Paper proposed certain actions designed to privatize the management of Internet names and addresses in a manner that allows for the development of competition and facilitates global participation in Internet management. The Green Paper proposed for discussion a variety of issues relating to DNS management including private sector creation of a new not-for-profit corporation (the \"new corporation\") managed by a globally and functionally representative board of directors. ICANN was formed in response to this policy. ICANN managed the Internet Assigned Numbers Authority (IANA) under contract to the United States Department of Commerce (DOC) and pursuant to an agreement with the IETF.\n\nICANN was incorporated in California on September 30, 1998, with entrepreneur and philanthropist Esther Dyson as founding chairwoman. It is a nonprofit public benefit corporation \"organized under the California Nonprofit Public Benefit Corporation Law for charitable and public purposes.\" ICANN was established in California due to the presence of Jon Postel, who was a founder of ICANN and was set to be its first Chief Technology Officer prior to his unexpected death. ICANN formerly operated from the same Marina del Rey building where Postel formerly worked, which is home to an office of the Information Sciences Institute at the University of Southern California. However, ICANN's headquarters is now located in the nearby Playa Vista neighborhood of Los Angeles.\n\nPer its original by-laws, primary responsibility for policy formation in ICANN was to be delegated to three supporting organizations (Address Supporting Organization, Domain Name Supporting Organization, and Protocol Supporting Organization), each of which was to develop and recommend substantive policies and procedures for the management of the identifiers within their respective scope. They were also required to be financially independent from ICANN. As expected, the Regional Internet Registries and the IETF agreed to serve as the Address Supporting Organization and Protocol Supporting Organization respectively, and ICANN issued a call for interested parties to propose the structure and composition of the Domain Name Supporting Organization. In March 1999, the ICANN Board, based in part on the DNSO proposals received, decided instead on an alternate construction for the DNSO which delineated specific constituencies bodies within ICANN itself, thus adding primary responsibility for DNS policy development to ICANN's existing duties of oversight and coordination.\n\nOn July 26, 2006, the United States government renewed the contract with ICANN for performance of the IANA function for an additional one to five years. The context of ICANN's relationship with the U.S. government was clarified on September 29, 2006 when ICANN signed a new Memorandum of Understanding with the United States Department of Commerce (DOC). This document gave the DOC oversight over some of the ICANN operations.\n\nDuring July 2008, the DOC reiterated an earlier statement that it has \"no plans to transition management of the authoritative root zone file to ICANN\". The letter also stresses the separate roles of the IANA and VeriSign.\n\nOn September 30, 2009, ICANN signed an agreement with the DOC (known as the \"Affirmation of Commitments\") that confirmed ICANN's commitment to a multistakeholder governance model, but did not remove it from DOC oversight and control.\n\nOn March 10, 2016, ICANN and the DOC signed a historic, culminating agreement to finally remove ICANN and IANA from the control and oversight of the DOC. On October 1, 2016, ICANN was freed from U.S. government oversight.\n\nOn March 18, 2002, publicly elected At-Large Representative for North America board member Karl Auerbach sued ICANN in Superior Court in California to gain access to ICANN's accounting records without restriction. Auerbach won.\n\nDuring September and October 2003, ICANN played a crucial role in the conflict over VeriSign's \"wild card\" DNS service Site Finder. After an open letter from ICANN issuing an ultimatum to VeriSign, later endorsed by the Internet Architecture Board, the company voluntarily ended the service on October 4, 2003. After this action, VeriSign filed a lawsuit against ICANN on February 27, 2004, claiming that ICANN had exceeded its authority. By this lawsuit, VeriSign sought to reduce ambiguity about ICANN's authority. The antitrust component of VeriSign's claim was dismissed during August 2004. VeriSign's challenge that ICANN overstepped its contractual rights is currently outstanding. A proposed settlement already approved by ICANN's board would resolve VeriSign's challenge to ICANN in exchange for the right to increase pricing on .com domains. At the meeting of ICANN in Rome, which took place from March 2 to 6, 2004, ICANN agreed to ask approval of the U.S. Department of Commerce for the Waiting List Service of VeriSign.\n\nOn May 17, 2004, ICANN published a proposed budget for the year 2004–05. It included proposals to increase the openness and professionalism of its operations, and greatly increased its proposed spending from US$8.27 million to $15.83 million. The increase was to be funded by the introduction of new top-level domains, charges to domain registries, and a fee for some domain name registrations, renewals and transfers (initially USD 0.20 for all domains within a country-code top-level domain, and USD 0.25 for all others). The Council of European National Top Level Domain Registries (CENTR), which represents the Internet registries of 39 countries, rejected the increase, accusing ICANN of a lack of financial prudence and criticizing what it describes as ICANN's \"unrealistic political and operational targets\". Despite the criticism, the registry agreement for the top-level domains jobs and travel includes a US$2 fee on every domain the licensed companies sell or renew.\n\nAfter a second round of negotiations during 2004, the TLDs eu, , travel, jobs, mobi, and cat were introduced during 2005.\nOn February 28, 2006, ICANN's board approved a settlement with VeriSign in the lawsuit resulting from SiteFinder that involved allowing VeriSign (the registry) to raise its registration fees by up to 7% a year. This was criticised by a few members of the U.S. House of Representatives' Small Business Committee.\n\nDuring February 2007, ICANN began procedures to end accreditation of one of their registrars, RegisterFly amid charges and lawsuits involving fraud, and criticism of ICANN's management of the situation. ICANN has been the subject of criticism as a result of its handling of RegisterFly, and the harm caused to thousands of clients as a result of what has been termed ICANN's \"laissez faire attitude toward customer allegations of fraud\".\n\nOn May 23, 2008, ICANN issued enforcement notices against ten accredited registrars and announced this through a press release entitled \"'Worst Spam Offenders' Notified by ICANN, Compliance system working to correct Whois and other issues.\" This was largely in response to a report issued by KnujOn, called \"The 10 Worst Registrars\" in terms of spam advertised junk product sites and compliance failure. The mention of the word \"spam\" in the title of the ICANN memo is somewhat misleading since ICANN does not address issues of spam or email abuse. Website content and usage are not within ICANN's mandate. However, the KnujOn report details how various registrars have not complied with their contractual obligations under the Registrar Accreditation Agreement (RAA). The main point of the KnujOn research was to demonstrate the relationships between compliance failure, illicit product traffic, and spam. The report demonstrated that out of 900 ICANN accredited registrars, fewer than 20 held 90% of the web domains advertised in spam. These same registrars were also most frequently cited by KnujOn as failing to resolve complaints made through the Whois Data Problem Reporting System (WDPRS).\n\nOn June 26, 2008, the ICANN Board started a new process of TLD naming policy to take a \"significant step forward on the introduction of new generic top-level domains.\" This program envisions the availability of many new or already proposed domains, as well a new application and implementation process.\n\nOn October 1, 2008, ICANN issued breach notices against Joker and Beijing Innovative Linkage Technology Ltd. after further researching reports and complaints issued by KnujOn. These notices gave the registrars 15 days to fix their Whois investigation efforts.\n\nIn 2010, ICANN approved a major review of its policies with respect to accountability, transparency, and public participation by the Berkman Center for Internet and Society at Harvard University. This external review was an assistance of the work of ICANN's Accountability and Transparency Review team.\n\nOn February 3, 2011, ICANN announced that it had distributed the last batch of its remaining IPv4 addresses to the world's five Regional Internet Registries, the organizations that manage IP addresses in different regions. These registries began assigning the final IPv4 addresses within their regions until they ran out completely.\n\nOn June 20, 2011, the ICANN board voted to end most restrictions on the names of generic top-level domains (gTLD). Companies and organizations became able to choose essentially arbitrary top-level Internet domain names. The use of non-Latin characters (such as Cyrillic, Arabic, Chinese, etc.) is also allowed in gTLDs. ICANN began accepting applications for new gTLDS on January 12, 2012. The initial price to apply for a new gTLD was set at $185,000 and the annual renewal fee is $25,000.\n\nFollowing the 2013 NSA spying scandal, ICANN endorsed the Montevideo Statement, although no direct connection between these can be proven.\n\nOn October 1, 2016, ICANN ended its contract with the United States Department of Commerce National Telecommunications and Information Administration (NTIA) and entered the private sector.\n\nThe EU attempts to enforce its GDPR (active since May 25, 2018) shall impact on ICANN operations, which the latter tried to fix through last-minute changes. The deadline coincided with French President Macron's hosting of megatech bosses such as Facebook CEO Mark Zuckerberg, Microsoft CEO Satya Nadella, Uber CEO Dara Khosrowshahi, IBM CEO Ginni Rometty, Intel CEO Brian Krzanich, Samsung President Young Sohn, and SAP CEO Bill McDermott.\n\nFrom its founding to the present, ICANN has been formally organized as a nonprofit corporation \"for charitable and public purposes\" under the California Nonprofit Public Benefit Corporation Law. It is managed by a 16-member board of directors composed of eight members selected by a nominating committee on which all the constituencies of ICANN are represented; six representatives of its Supporting Organizations, sub-groups that deal with specific sections of the policies under ICANN's purview; an at-large seat filled by an at-large organization; and the President / CEO, appointed by the board.\n\nThere are currently three supporting organizations: the Generic Names Supporting Organization (GNSO) deals with policy making on generic top-level domains (gTLDs); the Country Code Names Supporting Organization (ccNSO) deals with policy making on country-code top-level domains (ccTLDs); the Address Supporting Organization (ASO) deals with policy making on IP addresses.\n\nICANN also relies on some advisory committees and other advisory mechanisms to receive advice on the interests and needs of stakeholders that do not directly participate in the Supporting Organizations. These include the Governmental Advisory Committee (GAC), which is composed of representatives of a large number of national governments from all over the world; the At-Large Advisory Committee (ALAC), which is composed of individual Internet users from around the world selected by each of the Regional At-Large Organizations (RALO) and Nominating Committee; the Root Server System Advisory Committee, which provides advice on the operation of the DNS root server system; the Security and Stability Advisory Committee (SSAC), which is composed of Internet experts who study security issues pertaining to ICANN's mandate; and the Technical Liaison Group (TLG), which is composed of representatives of other international technical organizations that focus, at least in part, on the Internet.\n\nThe Governmental Advisory Committee has representatives from 111 states (108 UN members, the Holy See, Cook Islands, Niue and Taiwan), Hong Kong, Bermuda, Montserrat, the European Commission and the African Union Commission.\n\nIn addition the following organizations are GAC Observers:\n\n\nIn the Memorandum of understanding that set up the relationship between ICANN and the U.S. government, ICANN was given a mandate requiring that it operate \"in a bottom up, consensus driven, democratic manner.\" However, the attempts that ICANN have made to establish an organizational structure that would allow wide input from the global Internet community did not produce results amenable to the current Board. As a result, the At-Large constituency and direct election of board members by the global Internet community were soon abandoned.\n\nICANN holds periodic public meetings rotated between continents for the purpose of encouraging global participation in its processes. Resolutions of the ICANN Board, preliminary reports, and minutes of the meetings, are published on the ICANN website, sometimes in real time. However, there are criticisms from ICANN constituencies including the Noncommercial Users Constituency (NCUC) and the At-Large Advisory Committee (ALAC) that there is not enough public disclosure and that too many discussions and decisions take place out of sight of the public.\n\nDuring the early 2000s, there had been speculation that the United Nations might assume control of ICANN, followed by a negative reaction from the U.S. government and worries about a division of the Internet. The World Summit on the Information Society in Tunisia during November 2005 agreed not to get involved in the day-to-day and technical operations of ICANN. However it also agreed to establish an international Internet Governance Forum, with a consultative role on the future governance of the Internet. ICANN's Government Advisory Committee is currently established to provide advice to ICANN regarding public policy issues and has participation by many of the world's governments.\n\nSome have attempted to argue that ICANN was never given the authority to decide policy, e.g., choose new TLDs or exclude other interested parties who refuse to pay ICANN's US$185,000 fee, but was to be a technical caretaker. Critics suggest that ICANN should not be allowed to impose business rules on market participants, and that all TLDs should be added on a first-come, first-served basis and the market should be the arbiter of who succeeds and who does not.\n\nOne task that ICANN was asked to do was to address the issue of domain name ownership resolution for generic top-level domains (gTLDs). ICANN's attempt at such a policy was drafted in close cooperation with the World Intellectual Property Organization (WIPO), and the result has now become known as the Uniform Dispute Resolution Policy (UDRP). This policy essentially attempts to provide a mechanism for rapid, cheap and reasonable resolution of domain name conflicts, avoiding the traditional court system for disputes by allowing cases to be brought to one of a set of bodies that arbitrate domain name disputes. According to ICANN policy, a domain registrant must agree to be bound by the UDRP—they cannot get a domain name without agreeing to this.\n\nExamination of the UDRP decision patterns has caused some to conclude that compulsory domain name arbitration is less likely to give a fair hearing to domain name owners asserting defenses under the First Amendment and other laws, compared to the federal courts of appeal in particular.\n\nIn 2013, the initial report of ICANN's Expert Working Group has recommended that the present form of Whois, a utility that allows anyone to know who has registered a domain name on the Internet, should be \"abandoned\". It recommends it be replaced with a system that keeps most registration information secret (or \"gated\") from most Internet users, and only discloses information for \"permissible purposes\". ICANN's list of permissible purposes includes domain name research, domain name sale and purchase, regulatory enforcement, personal data protection, legal actions, and abuse mitigation. Whois has been a key tool of investigative journalists interested in determining who was disseminating information on the Internet. The use of whois by the free press is not included in the list of permissible purposes in the initial report.\n\nSince its creation, ICANN has been the subject of criticism and controversy. In 2000, professor Michael Froomkin of the University of Miami School of Law argued that ICANN's relationship with the U.S. Department of Commerce is illegal, in violation of either the Constitution or federal statutes. In 2009, the new \"Affirmation of Commitments\" agreement between ICANN and the U.S. Department of Commerce, that aimed to create international oversight, ran into criticism.\n\nDuring December 2011, the Federal Trade Commission stated ICANN had long failed to provide safeguards that protect consumers from online swindlers.\n\nAlso during 2011, seventy-nine companies, including The Coca-Cola Company, Hewlett-Packard, Samsung and others, signed a petition against ICANN's new TLD program (sometimes referred to as a \"commercial landgrab\"), in a group organized by the Association of National Advertisers. As of September 2014, this group, the Coalition for Responsible Internet Domain Oversight, that opposes the rollout of ICANN's TLD expansion program, has been joined by 102 associations and 79 major companies. Partly as a response to this criticism, ICANN initiated an effort to protect trademarks in domain name registrations, which eventually culminated in the establishment of the Trademark Clearinghouse.\n\nOne controversial proposal, resulting from a September 2011 summit between India, Brazil, and South Africa (IBSA), would seek to move Internet governance into a \"UN Committee on Internet-Related Policy\" (UN-CIRP). The action was a reaction to a perception that the principles of the 2005 Tunis Agenda for the Information Society have not been met. The statement proposed the creation of a new political organization operating as a component of the United Nations to provide policy recommendations for the consideration of technical organizations such as ICANN and international bodies such as the ITU. Subsequent to public criticisms, the Indian government backed away from the proposal.\n\nOn October 7, 2013 the Montevideo Statement on the Future of Internet Cooperation was released by the managers of a number of organizations involved in coordinating the Internet's global technical infrastructure, loosely known as the \"I*\" (or \"I-star\") group. Among other things, the statement \"expressed strong concern over the undermining of the trust and confidence of Internet users globally due to recent revelations of pervasive monitoring and surveillance\" and \"called for accelerating the globalization of ICANN and IANA functions, towards an environment in which all stakeholders, including all governments, participate on an equal footing\". This desire to reduce United States association with the internet is considered a reaction to the ongoing NSA surveillance scandal. The statement was signed by the managers of the Internet Corporation for Assigned Names and Numbers (ICANN), the Internet Engineering Task Force, the Internet Architecture Board, the World Wide Web Consortium, the Internet Society, and the five regional Internet address registries (African Network Information Center, American Registry for Internet Numbers, Asia-Pacific Network Information Centre, Latin America and Caribbean Internet Addresses Registry, and Réseaux IP Européens Network Coordination Centre).\n\nDuring October 2013, Fadi Chehadé, current President and CEO of ICANN, met with Brazilian President Dilma Rousseff in Brasilia. Upon Chehadé's invitation, the two announced that Brazil would host an international summit on Internet governance during April 2014. The announcement came after the 2013 disclosures of mass surveillance by the U.S. government, and President Rousseff's speech at the opening session of the 2013 United Nations General Assembly, where she strongly criticized the American surveillance program as a \"breach of international law\". The \"Global Multistakeholder Meeting on the Future of Internet Governance (NET mundial)\" will include representatives of government, industry, civil society, and academia. At the IGF VIII meeting in Bali in October 2013 a commenter noted that Brazil intends the meeting to be a \"summit\" in the sense that it will be high level with decision-making authority. The organizers of the \"NET mundial\" meeting have decided that an online forum called \"/1net\", set up by the I* group, will be a major conduit of non-governmental input into the three committees preparing for the meeting in April.\n\nThe Obama administration that had joined critics of ICANN during 2011 announced in March 2014 that they intended to transition away from oversight of the IANA functions contract. The current contract that the United States Department of Commerce has with ICANN expired in 2015, in its place the NTIA will transition oversight of the IANA functions to the 'global multistakeholder community'.\n\nThe NetMundial Initiative is a plan for international governance of the Internet that was first proposed at the Global Multistakeholder Meeting on the Future of Internet Governance (GMMFIG) conference (April 23–24, 2014)\nand later developed into the NetMundial Initiative by ICANN CEO Fadi Chehadé along with representatives of the World Economic Forum (WEF)\nand the Brazilian Internet Steering Committee (Comitê Gestor da Internet no Brasil), commonly referred to as \"CGI.br\".\n\nThe meeting produced a nonbinding statement in favor of consensus-based decision-making. It represented a compromise and did not harshly condemn mass surveillance or include the words \"net neutrality\", despite initial endorsement for that from Brazil. The final resolution says ICANN should be controlled internationally by September 2015.\nA minority of governments, including Russia, China, Iran and India, were unhappy with the final resolution and wanted multilateral management for the Internet, rather than broader multistakeholder management.\n\nA month later, the Panel on Global Internet Cooperation and Governance Mechanisms (convened by the Internet Corporation for Assigned Names and Numbers (ICANN) and the World Economic Forum (WEF) with assistance from The Annenberg Foundation), endorsed and included the NetMundial statement in its own report.\n\nDuring June 2014, France strongly attacked ICANN, saying ICANN is not a fit venue for Internet governance and that alternatives should be sought.\n\nICANN has received more than $60 million from gTLD auctions, and has accepted the controversial domain name \".sucks\" (referring to the primarily US slang for being inferior or objectionable). .sucks domains are owned and controlled by the Vox Populi Registry which won the rights for .sucks gTLD in November 2014.\n\nThe .sucks domain registrar has been described as \"predatory, exploitive and coercive\" by the Intellectual Property Constituency that advises the ICANN board. When the .sucks registry announced their pricing model, \"most brand owners were upset and felt like they were being penalized by having to pay more to protect their brands.\" Because of the low utility of the \".sucks\" domain, most fees come from \"Brand Protection\" customers registering their trademarks to prevent domains being registered.\n\nCanadian brands had complained that they were being charged \"exorbitant\" prices to register their trademarks as premium names. FTC chair Edith Ramirez has written to ICANN to say the agency will take action against the .sucks owner if \"we have reason to believe an entity has engaged in deceptive or unfair practices in violation of Section 5 of the FTC Act\". The Register reported that intellectual property lawyers are infuriated that \"the dot-sucks registry was charging trademark holders $2,500 for .sucks domains and everyone else $10.\"\n\nU.S. Representative Bob Goodlatte has said that trademark holders are \"being shaken down\" by the registry's fees. Jay Rockefeller says that .sucks is a \"a predatory shakedown scheme\" and \"Approving '.sucks', a gTLD with little or no public interest value, will have the effect of undermining the credibility ICANN has slowly been building with skeptical stakeholders.\"\n\nOn July 30, 2018, whistleblower and writer for the Register Kieran McCarthy exposed the systematic refusal of ICANN to create the .islam and .halal gTLDs through the willful breaking of the organization's own bylaws. McCarthy coined the 6-year refusal as \"the internet's very own Muslim ban\" after ICANN kowtowed to Middle Eastern governments by not approving the domain name additions.\n\n\n\n"}
{"id": "15237", "url": "https://en.wikipedia.org/wiki?curid=15237", "title": "Iterative method", "text": "Iterative method\n\nIn computational mathematics, an iterative method is a mathematical procedure that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the \"n\"-th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common. \n\nIn contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors, direct methods would deliver an exact solution (like solving a linear system of equations formula_1 by Gaussian elimination). Iterative methods are often the only choice for nonlinear equations. However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.\n\nIf an equation can be put into the form \"f\"(\"x\") = \"x\", and a solution x is an attractive fixed point of the function \"f\", then one may begin with a point \"x\" in the basin of attraction of x, and let \"x\" = \"f\"(\"x\") for \"n\" ≥ 1, and the sequence {\"x\"} will converge to the solution x. Here \"x\" is the \"n\"th approximation or iteration of \"x\" and \"x\" is the next or \"n\" + 1 iteration of \"x\". Alternately, superscripts in parentheses are often used in numerical methods, so as not to interfere with subscripts with other meanings. (For example, \"x\" = \"f\"(\"x\").) If the function \"f\" is continuously differentiable, a sufficient condition for convergence is that the spectral radius of the derivative is strictly bounded by one in a neighborhood of the fixed point. If this condition holds at the fixed point, then a sufficiently small neighborhood (basin of attraction) must exist.\n\nIn the case of a system of linear equations, the two main classes of iterative methods are the stationary iterative methods, and the more general Krylov subspace methods.\n\nStationary iterative methods solve a linear system with an operator approximating the original one; and based on a measurement of the error in the result (the residual), form a \"correction equation\" for which this process is repeated. While these methods are simple to derive, implement, and analyze, convergence is only guaranteed for a limited class of matrices. \n\nAn \"iterative method\" is defined by\nand for a given linear system formula_3 with exact solution formula_4 the \"error\" by\nAn iterative method is called \"linear\" if there exists a matrix formula_6 such that\nand this matrix is called \"iteration matrix\".\nAn iterative method with a given iteration matrix formula_8 is called \"convergent\" if the following holds\n\nAn important theorem states that for a given iterative method and its iteration matrix formula_8 it is convergent if and only if its spectral radius formula_11 is smaller than unity, that is,\n\nThe basic iterative methods work by a splitting up the matrix formula_13 into\nand here the matrix formula_15 should be easily invertible.\nThe iterative methods are now defined as\nFrom this follows that the iteration matrix is given by\n\nBasic examples of stationary iterative methods use a splitting of the matrix formula_13 such as\nwhere formula_20 is only the diagonal part of formula_13, and formula_22 is the strict lower triangular part of formula_13.\nRespectively, formula_24 is the upper triangular part of formula_13.\nLinear stationary iterative methods are also called relaxation methods.\n\nKrylov subspace methods work by forming a basis of the sequence of successive matrix powers times the initial residual (the Krylov sequence). \nThe approximations to the solution are then formed by minimizing the residual over the subspace formed. \nThe prototypical method in this class is the conjugate gradient method (CG) which assumes that the system matrix formula_13 is symmetric positive-definite.\nFor symmetric (and possibly indefinite) formula_13 one works with the minimal residual method (MINRES).\nIn the case of not even symmetric matrices methods, such as the generalized minimal residual method (GMRES) and the biconjugate gradient method (BiCG), have been derived.\n\nSince these methods form a basis, it is evident that the method converges in \"N\" iterations, where \"N\" is the system size. However, in the presence of rounding errors this statement does not hold; moreover, in practice \"N\" can be very large, and the iterative process reaches sufficient accuracy already far earlier. The analysis of these methods is hard, depending on a complicated function of the spectrum of the operator.\n\nThe approximating operator that appears in stationary iterative methods can also be incorporated in Krylov subspace methods such as GMRES (alternatively, preconditioned Krylov methods can be considered as accelerations of stationary iterative methods), where they become transformations of the original operator to a presumably better conditioned one. The construction of preconditioners is a large research area.\n\nProbably the first iterative method for solving a linear system appeared in a letter of Gauss to a student of his. He proposed solving a 4-by-4 system of equations by repeatedly solving the component in which the residual was the largest. \n\nThe theory of stationary iterative methods was solidly established with the work of D.M. Young starting in the 1950s. The Conjugate Gradient method was also invented in the 1950s, with independent developments by Cornelius Lanczos, Magnus Hestenes and Eduard Stiefel, but its nature and applicability were misunderstood at the time. Only in the 1970s was it realized that conjugacy based methods work very well for partial differential equations, especially the elliptic type.\n\n\n"}
{"id": "15238", "url": "https://en.wikipedia.org/wiki?curid=15238", "title": "International judicial institution", "text": "International judicial institution\n\nInternational judicial institutions can be divided into courts, arbitral tribunals and quasi-judicial institutions. Courts are permanent bodies, with near the same composition for each case. Arbitral tribunals, by contrast, are constituted anew for each case. Both courts and arbitral tribunals can make binding decisions. Quasi-judicial institutions, by contrast, make rulings on cases, but these rulings are not in themselves legally binding; the main example is the individual complaints mechanisms available under the various UN human rights treaties.\n\nInstitutions can also be divided into global and regional institutions.\n\nThe listing below incorporates both currently existing institutions, defunct institutions that no longer exist, institutions which never came into existence due to non-ratification of their constitutive instruments, and institutions which do not yet exist, but for which constitutive instruments have been signed. It does not include mere proposed institutions for which no instrument was ever signed.\n\n\n\n\n\n\n"}
{"id": "15239", "url": "https://en.wikipedia.org/wiki?curid=15239", "title": "International Prize Court", "text": "International Prize Court\n\nThe International Prize Court was an international court proposed at the beginning of the 20th century, to hear prize cases. An international agreement to create it, the \"Convention Relative to the Creation of an International Prize Court\", was made at the Second Hague Conference in 1907 but never came into force.\n\nThe capturing of prizes (enemy equipment, vehicles, and especially ships) during wartime is a tradition that goes back as far as organized warfare itself. The International Prize Court was to hear appeals from national courts concerning prize cases. Even as a draft, the convention was innovative for the time, in being both the first ever treaty for a truly international court (as opposed to a mere arbitral tribunal), and in providing individuals with access to the court, going against the prevailing doctrines of international law at the time, according to which only states had rights and duties under international law. The Convention was opposed, particularly by elements within the United States and the United Kingdom, as a violation of national sovereignty.\n\nThe 1907 convention was modified by the \"Additional Protocol to the Convention Relative to the Creation of an International Prize Court\", done at the Hague on October 18, 1910. The protocol was an attempt to resolve some concerns expressed by the United States at the court, which felt it to be in violation of its constitutional provision that provides for the U.S. Supreme Court being the final judicial authority. However, neither the convention nor the subsequent protocol ever entered into force, since only Nicaragua ratified the agreements. As a result, the court never came into existence.\n\nA number of ideas from the International Prize Court proposal can be seen in present day international courts, such as its provision for judges \"ad hoc\", later adopted in the Permanent Court of International Justice and the subsequent International Court of Justice.\n\n\n"}
{"id": "15240", "url": "https://en.wikipedia.org/wiki?curid=15240", "title": "Imam", "text": "Imam\n\nImam (; '; plural: ') is an Islamic leadership position. \n\nIt is most commonly used as the title of a worship leader of a mosque and Muslim community among Sunni Muslims. In this context, imams may lead Islamic worship services, serve as community leaders, and provide religious guidance. \n\nFor Shi'a Muslims, the imam has a more central meaning and role in Islam through the concept of imamah; the term is only applicable to those members of \"Ahl al-Bayt\", the house of the Islamic prophet Muhammad, designated as infallibles.\n\nThe Sunni branch of Islam does not have imams in the same sense as the Shi'a, an important distinction often overlooked by those outside of the Islamic faith. In everyday terms, the imam for Sunni Muslims is the one who leads Islamic formal (Fard) prayers, even in locations besides the mosque, whenever prayers are done in a group of two or more with one person leading (imam) and the others following by copying his ritual actions of worship. Friday sermon is most often given by an appointed imam. All mosques have an imam to lead the (congregational) prayers, even though it may sometimes just be a member from the gathered congregation rather than an officially appointed salaried person. The position of women as imams is controversial. The person that should be chosen, according to Hadith, is one who has most knowledge of the Quran and Sunnah (prophetic tradition) and is of good character.\n\nThe term is also used for a recognized religious scholar or authority in Islam, often for the founding scholars of the four Sunni madhhabs, or schools of jurisprudence (\"fiqh\"). It may also refer to the Muslim scholars who created the analytical sciences related to Hadith or it may refer to the heads of Muhammad's family in their generational times.\nThe Position of Imams In Turkey\n\nImams are appointed by the state to work at mosques and they are required to be graduates of an İmam Hatip high school or have a university degree in Theology. This is an official position regulated by the Presidency of Religious Affairs in Turkey and only males are appointed to this position while female officials under the same state organisation work as preachers and Qur'an course tutors, religious services experts. These officials are supposed to belong to the Hanafi school of the Sunni sect.\n\nA central figure in an Islamic movement is also called as an Imam like the Imam Nabhawi in Syria and Ahmad Raza Khan in India and Pakistan is also called as the Imam for Sunni Muslims.\n\nIn the Shi'a context, an imam is not only presented as the man of God \"par excellence\", but as participating fully in the names, attributes, and acts that theology usually reserves for God alone. Imams have a meaning more central to belief, referring to leaders of the community. Twelver and Ismaili Shi'a believe that these imams are chosen by God to be perfect examples for the faithful and to lead all humanity in all aspects of life. They also believe that all the imams chosen are free from committing any sin, impeccability which is called \"ismah\". These leaders must be followed since they are appointed by God.\n\nHere follows a list of the Twelvers imams:\n\nFatimah, also Fatimah al-Zahraa, daughter of Muhammed (615–632), is also considered infallible but not an Imam. The Shi'a believe that the last Imam, the 12th Imam Mahdi will one day emerge on Qiyamah.\n\nSee Imamah (Ismaili doctrine) and List of Ismaili imams for Ismaili imams.\n\nSee details under Zaidiyyah, Islamic history of Yemen and Imams of Yemen.\n\nAt times, imams have held both secular and religious authority. This was the case in Oman among the Kharijite or Ibadi sects. At times, the imams were elected. At other times the position was inherited, as with the Yaruba dynasty from 1624 and 1742. See List of rulers of Oman, the Rustamid dynasty: 776–909, Nabhani dynasty: 1154–1624, the Yaruba dynasty: 1624–1742, the Al Said: 1744–present for further information. The Imamate of Futa Jallon (1727-1896) was a Fulani state in West Africa where secular power alternated between two lines of hereditary Imams, or \"almami\".\nIn the Zaidi Shiite sect, imams were secular as well as spiritual leaders who held power in Yemen for more than a thousand years. In 897, a Zaidi ruler, al-Hadi ila'l-Haqq Yahya, founded a line of such imams, a theocratic form of government which survived until the second half of the 20th century. (See details under Zaidiyyah, History of Yemen, Imams of Yemen.)\n\nRuhollah Khomeini is officially referred to as Imam in Iran. Several Iranian places and institutions are named \"Imam Khomeini\", including a city, an international airport, a hospital, and a university.\n\n\n\n"}
{"id": "15242", "url": "https://en.wikipedia.org/wiki?curid=15242", "title": "Instrument flight rules", "text": "Instrument flight rules\n\nInstrument flight rules (IFR) is one of two sets of regulations governing all aspects of civil aviation aircraft operations; the other is visual flight rules (VFR).\n\nThe U.S. Federal Aviation Administration's (FAA) \"Instrument Flying Handbook\" defines IFR as: \"Rules and regulations established by the FAA to govern flight under conditions in which flight by outside visual reference is not safe. IFR flight depends upon flying by reference to instruments in the flight deck, and navigation is accomplished by reference to electronic signals.\" It is also a term used by pilots and controllers to indicate the type of flight plan an aircraft is flying, such as an IFR or VFR flight plan.\n\nTo put instrument flight rules into context, a brief overview of visual flight rules (VFR) is necessary. It is possible and fairly straightforward, in relatively clear weather conditions, to fly a plane solely by reference to outside visual cues, such as the horizon to maintain orientation, nearby buildings and terrain features for navigation, and other aircraft to maintain separation. This is known as operating the aircraft under VFR, and is the most common mode of operation for small aircraft. However, it is safe to fly VFR only when these outside references can be clearly seen from a sufficient distance; when flying through or above clouds, or in fog, rain, dust or similar low-level weather conditions, these references can be obscured. Thus, cloud ceiling and flight visibility are the most important variables for safe operations during all phases of flight.<ref name='NASA/CR-2000-210288'> \n</ref> The minimum weather conditions for ceiling and visibility for VFR flights are defined in FAR Part 91.155, and vary depending on the type of airspace in which the aircraft is operating, and on whether the flight is conducted during daytime or nighttime. However, typical daytime VFR minimums for most airspace is 3 statute miles of flight visibility and a distance from clouds of 500' below, 1,000' above, and 2,000' feet horizontally. Flight conditions reported as equal to or greater than these VFR minimums are referred to as visual meteorological conditions (VMC).\n\nAny aircraft operating under VFR must have the required equipment on board, as described in FAR Part 91.205 (which includes some instruments necessary for IFR flight). VFR pilots \"may\" use cockpit instruments as secondary aids to navigation and orientation, but are not required to; the view outside of the aircraft is the primary source for keeping the aircraft straight and level (orientation), flying to the intended destination (navigation), and not hitting anything (separation).\n\nVisual flight rules are generally simpler than instrument flight rules, and require significantly less training and practice. VFR provides a great degree of freedom, allowing pilots to go where they want, when they want, and allows them a much wider latitude in determining how they get there.\n\nWhen operation of an aircraft under VFR is not safe, because the visual cues outside the aircraft are obscured by weather, instrument flight rules must be used instead. IFR permits an aircraft to operate in instrument meteorological conditions (IMC), which is essentially any weather condition less than VMC but in which aircraft can still operate safely. Use of instrument flight rules is also required when flying in \"Class A\" airspace regardless of weather conditions. Class A airspace extends from 18,000 feet above mean sea level to flight level 600 (60,000 feet pressure altitude) above the contiguous 48 United States and overlying the waters within 12 miles thereof. Flight in Class A airspace requires pilots and aircraft to be instrument equipped and rated and to be operating under Instrument Flight Rules (IFR). In many countries commercial airliners and their pilots must operate under IFR as the majority of flights enter Class A airspace; however, aircraft operating as commercial airliners must operate under IFR even if the flight plan does not take the craft into Class A airspace, such as with smaller regional flights. Procedures and training are significantly more complex compared to VFR instruction, as a pilot must demonstrate competency in conducting an entire cross-country flight solely by reference to instruments.\n\nInstrument pilots must meticulously evaluate weather, create a detailed flight plan based around specific instrument departure, en route, and arrival procedures, and dispatch the flight.\n\nThe distance by which an aircraft avoids obstacles or other aircraft is termed \"separation\". The most important concept of IFR flying is that separation is maintained regardless of weather conditions. In controlled airspace, air traffic control (ATC) separates IFR aircraft from obstacles and other aircraft using a flight \"clearance\" based on route, time, distance, speed, and altitude. ATC monitors IFR flights on radar, or through aircraft position reports in areas where radar coverage is not available. Aircraft position reports are sent as voice radio transmissions. In the United States, a flight operating under IFR is required to provide position reports unless ATC advises a pilot that the plane is in radar contact. The pilot must resume position reports after ATC advises that radar contact has been lost, or that radar services are terminated.\n\nIFR flights in controlled airspace require an ATC \"clearance\" for each part of the flight. A clearance always specifies a \"clearance limit\", which is the farthest the aircraft can fly without a new clearance. In addition, a clearance typically provides a heading or route to follow, altitude, and communication parameters, such as frequencies and transponder codes.\n\nIn uncontrolled airspace, ATC clearances are unavailable. In some states a form of separation is provided to certain aircraft in uncontrolled airspace as far as is practical (often known under ICAO as an advisory service in class G airspace), but separation is not mandated nor widely provided.\n\nDespite the protection offered by flight in controlled airspace under IFR, the ultimate responsibility for the safety of the aircraft rests with the pilot in command, who can refuse clearances.\n\nIt is essential to differentiate between flight plan type (VFR or IFR) and weather conditions (VMC or IMC). While current and forecast weather may be a factor in deciding which type of flight plan to file, weather conditions themselves do not affect one's filed flight plan. For example, an IFR flight that encounters visual meteorological conditions (VMC) en route does not automatically change to a VFR flight, and the flight must still follow all IFR procedures regardless of weather conditions. In the US, weather conditions are forecast broadly as VFR, MVFR (Marginal Visual Flight Rules), IFR, or LIFR (Low Instrument Flight Rules).\n\nThe main purpose of IFR is the safe operation of aircraft in instrument meteorological conditions (IMC). The weather is considered to be MVFR or IMC when it does not meet the minimum requirements for visual meteorological conditions (VMC). To operate safely in IMC (\"actual instrument conditions\"), a pilot controls the aircraft relying on flight instruments and ATC provides separation.\n\nIt is important not to confuse IFR with IMC. A significant amount of IFR flying is conducted in Visual Meteorological Conditions (VMC). Anytime a flight is operating in VMC and in a volume of airspace in which VFR traffic can operate, the crew is responsible for seeing and avoiding VFR traffic; however, because the flight is conducted under Instrument Flight Rules, ATC still provides separation services from other IFR traffic, and can in many cases also advise the crew of the location of VFR traffic near the flight path.\n\nAlthough dangerous and illegal, a certain amount of VFR flying is conducted in IMC. A scenario is a VFR pilot taking off in VMC conditions, but encountering deteriorating visibility while en route. Continued VFR flight into IMC can lead to spatial disorientation of the pilot which is the cause of a significant number of general aviation crashes. VFR flight into IMC is distinct from \"VFR-on-top\", an IFR procedure in which the aircraft operates in VMC using a hybrid of VFR and IFR rules, and \"VFR over the top\", a VFR procedure in which the aircraft takes off and lands in VMC but flies above an intervening area of IMC. Also possible in many countries is \"Special VFR\" flight, where an aircraft is explicitly granted permission to operate VFR within the controlled airspace of an airport in conditions technically less than VMC; the pilot asserts they have the necessary visibility to fly despite the weather, must stay in contact with ATC, and cannot leave controlled airspace while still below VMC minimums.\n\nDuring flight under IFR, there are no visibility requirements, so flying through clouds (or other conditions where there is zero visibility outside the aircraft) is legal and safe. However, there are still minimum weather conditions that must be present in order for the aircraft to take off or to land; these vary according to the kind of operation, the type of navigation aids available, the location and height of terrain and obstructions in the vicinity of the airport, equipment on the aircraft, and the qualifications of the crew. For example, Reno-Tahoe International Airport (KRNO) in a mountainous region has significantly different instrument approaches for aircraft landing on the same runway surface, but from opposite directions. Aircraft approaching from the north must make visual contact with the airport at a higher altitude than when approaching from the south because of rapidly rising terrain south of the airport. This higher altitude allows a flight crew to clear the obstacle if a landing is aborted. In general, each specific instrument approach specifies the minimum weather conditions to permit landing.\n\nAlthough large airliners, and increasingly, smaller aircraft, carry their own terrain awareness and warning system (TAWS), these are primarily backup systems providing a last layer of defense if a sequence of errors or omissions causes a dangerous situation.\n\nBecause IFR flights often take place without visual reference to the ground, a means of navigation other than looking outside the window is required. A number of navigational aids are available to pilots, including ground-based systems such as DME/VORs and NDBs as well as the satellite-based GPS/GNSS system. Air traffic control may assist in navigation by assigning pilots specific headings (\"radar vectors\"). The majority of IFR navigation is given by ground- and satellite-based systems, while radar vectors are usually reserved by ATC for sequencing aircraft for a busy approach or transitioning aircraft from takeoff to cruise, among other things.\n\nAutopilot allows automatic piloting.\n\nModern flight management systems have evolved to allow a crew to plan a flight as to route and altitude and to specific time of arrival at specific locations. This capability is used in several trial projects experimenting with \"four-dimensional\" approach clearances for commercial aircraft, with time as the fourth dimension. These clearances allow ATC to optimize the arrival of aircraft at major airports, which increases airport capacity and uses less fuel providing monetary and environmental benefits to airlines and the public.\n\n\nSpecific procedures allow IFR aircraft to transition safely through every stage of flight. These procedures specify how an IFR pilot should respond, even in the event of a complete radio failure, and loss of communications with ATC, including the expected aircraft course and altitude.\n\nDepartures are described in an IFR clearance issued by ATC prior to takeoff. The departure clearance may contain an assigned heading, one or more waypoints, and an initial altitude to fly. The clearance can also specify a departure procedure (DP) or standard instrument departure (SID) that should be followed unless \"NO DP\" is specified in the notes section of the filed flight plan.\n\nHere is an example of an IFR clearance for a Cessna aircraft traveling from Palo Alto airport (KPAO) to Stockton airport (KSCK).\n\nDetailed explanation:\n\n\n\n\n\n\n\n\n\n\nThe clearance scheme, used by ATC, can be easily remembered using the acronym\n\nEn route flight is described by IFR charts showing navigation aids, fixes, and standard routes called \"airways\". Aircraft with appropriate navigational equipment such as GPS, are also often cleared for a \"direct-to\" routing, where only the destination, or a few navigational waypoints are used to describe the route that the flight will follow. ATC will assign altitudes in its initial clearance or amendments thereto, and navigational charts indicate minimum safe altitudes for airways.\n\nThe approach portion of an IFR flight may begin with a standard terminal arrival route (STAR), describing common routes to fly to arrive at an initial approach fix (IAF) from which an instrument approach commences. \nAn instrument approach terminates either by the pilot acquiring sufficient visual reference to proceed to the runway, or with a missed approach because the required visual reference is not seen in time.\n\nTo fly under IFR, a pilot must have an instrument rating and must be \"current\" (meet recency of experience requirements).\nIn the United States, to file and fly under IFR, a pilot must be instrument-rated and, within the preceding six months, have flown six instrument approaches, as well as holding procedures and course interception and tracking with navaids. Flight under IFR beyond six months after meeting these requirements is not permitted; however, currency may be reestablished within the next six months by completing the requirements above. Beyond the twelfth month, examination (\"instrument proficiency check\") by an instructor is required.\n\nPracticing instrument approaches can be done either in the instrument meteorological conditions or in visual meteorological conditions – in the latter case, a safety pilot is required so that the pilot practicing instrument approaches can wear a view-limiting device which restricts his field of view to the instrument panel. A safety pilot's primary duty is to observe and avoid other traffic.\n\nFor all ILS Cat II or Cat III approaches, additional crew training is required and a certain number of low visibility approaches must either be performed or simulated within a fixed time for pilots to be 'current' in performing them.\n\nIn the UK, an IR (UK restricted) - formerly the \"IMC rating\" - which permits flight under IFR in airspace classes B to G in instrument meteorological conditions, a non-instrument-rated pilot can also elect to fly under IFR in visual meteorological conditions outside controlled airspace. Compared to the rest of the world, the UK's flight crew licensing regime is somewhat unusual in its licensing for meteorological conditions and airspace, rather than flight rules.\n\nThe aircraft must be equipped and type-certified for instrument flight, and the related navigational equipment must have been inspected or tested within a specific period of time prior to the instrument flight.\n\nIn the United States, instruments required for IFR flight in addition to those that are required for VFR flight are: heading indicator, sensitive altimeter adjustable for barometric pressure, clock with a sweep-second pointer or digital equivalent, attitude indicator, radios and suitable avionics for the route to be flown, alternator or generator, gyroscopic rate-of-turn indicator that is either a turn coordinator or the turn and bank indicator. From 1999 single-engine helicopters could not be FAA-certified for IFR, and Helicopter Association International estimates that 326 lives were lost in 133 accidents that would likely not have happened if those helicopters had been flying under IFR.\n\n\n"}
{"id": "15245", "url": "https://en.wikipedia.org/wiki?curid=15245", "title": "Ismail Khan", "text": "Ismail Khan\n\nMohammad Ismail Khan (Persian|Pashto: محمد اسماعیل خان) (born 1946) is a politician in Afghanistan, who served as Minister of Water and Energy from 2005 to 2013. He was previously the Governor of Herat Province. He is widely known as a warlord because of his rise to power during the Soviet–Afghan War when he controlled a large sized mujahideen force, mainly his fellow Tajiks from western Afghanistan. He is a key member of the political party Jamiat-e Islami and was a member of the now defunct United National Front party.\n\nKhan was born in or about 1946 in the Shindand District of Herat Province in Afghanistan. His family are Tajiks from the Chahar-Mahal neighbourhood of Shindand.\n\nIn early 1979 Ismail Khan was a Captain in the Afghan National Army based in the western city of Herat. In early March of that year, there was a protest in front of the Communist governor's palace against the arrests and assassinations being carried out in the countryside. The governor's troops opened fire on the demonstrators, who proceeded to storm the palace and hunt down Soviet advisers. The Herat garrison mutinied and joined the revolt, with Ismail Khan and other officers distributing all available weapons to the insurgents. Hundreds of civil workers and people not dressed in traditional Muslim clothes were murdered. A garrison of Soviet advisors was overtaken and all of its inhabitants: Soviet advisors along with their wives and children were massacred. The mob put severed heads of the victims on sticks and paraded them through the city of Herat. The government led by Nur Mohammed Taraki responded, pulverizing the city using Soviet supplied bombers and killing an estimated 24,000 citizens in less than a week. This event marked the opening salvo of the rebellion which led to the Soviet military intervention in Afghanistan in December 1979. Ismail Khan escaped to the countryside where he began to assemble a local rebel force.\n\nDuring the ensuing war, he became the leader of the western command of Burhanuddin Rabbani's Jamiat-e-Islami, political party associated with neighboring Pakistan's Jamaat-e-Islami. With Ahmad Shah Massoud, he was one of the most respected mujahideen leaders. In 1992, two years after the Soviet withdrawal from Afghanistan, the mujahideen captured Herat and Ismail Khan became Governor.\n\nIn 1995, he successfully defended his province against the Taliban, in cooperation with defense minister Ahmad Shah Massoud. Khan even tried to attack the Taliban stronghold of Kandahar, but was repulsed. Later in September, an ally of the Jamiat, Uzbek General Abdul Rashid Dostum changed sides, and attacked Herat. Ismail Khan was forced to flee to neighboring Iran with 8,000 men and the Taliban took over Herat Province.\n\nTwo years later, while organizing opposition to the Taliban in Faryab area, he was betrayed and captured by Abdul Majid Rouzi who had defected to the Taliban along with Abdul Malik Pahlawan, then one of Dostum's deputies. Then in March 1999 he escaped from Kandahar prison. During the U.S. intervention in Afghanistan, he fought against the Taliban within the United Islamic Front for the Salvation of Afghanistan (Northern Alliance) and thus regained his position as Governor of Herat after they were victorious in December 2001.\n\nAfter returning to Herat, Ismail Khan quickly consolidated his control over the region. He took over control of the city from the local ulema and quickly established control over the trade route between Herat and Iran, a large source of revenue. As Emir of Herat, Ismail Khan exercised great autonomy, providing social welfare for Heratis, expanding his power into neighbouring provinces, and maintaining direct international contacts. Although hated by the educated in Herat and often accused of human rights abuses, Ismail Khan's regime provided security, paid government employees, and made investments in public services. However, during his tenure as governor, Ismail Khan was accused of ruling his province like a private fiefdom, leading to increasing tensions with the Afghan Transitional Administration. In particular, he refused to pass on to the government the revenues gained from custom taxes on goods from Iran and Turkmenistan.\n\nOn 13 August 2003, President Karzai removed Governor Ismail Khan from his command of the 4th Corps. This was announced as part of a programme removing the ability of officials to hold both civilian and military posts.\n\nIsmail Khan was ultimately removed from power in March 2004 due to pressure by neighbouring warlords and the central Afghan government. Various sources have presented different versions of the story, and the exact dynamics cannot be known with certainty. What is known is that Ismail Khan found himself at odds with a few regional commanders who, although theoretically his subordinates, attempted to remove him from power. Ismail Khan claims that these efforts began with a botched assassination attempt. Afterwards, these commanders moved their forces near Herat. Ismail Khan, unpopular with the Herati military class, was slow to mobilise his forces, perhaps waiting for the threat to Herat to become existential as a means to motivate his forces. However, the conflict was stopped with the intervention of International Security Assistance Force forces and soldiers of the Afghan National Army, freezing the conflict in its tracks. Ismail Khan's forces even fought skirmishes with the Afghan National Army, in which his son, Mirwais Sadiq was killed. Because Ismail Khan was contained by the Afghan National Army, the warlords who opposed him were quickly able to occupy strategic locations unopposed. Ismail Khan was forced to give up his governorship and to go to Kabul, where he served in Hamid Karzai's cabinet as the Minister of Energy.\n\nIn 2005 Ismail Khan became the Minister of Water and Energy.\n\nIn late 2012, the Government of Afghanistan accused Ismail Khan of illegally distributing weapons to his supporters. About 40 members of the country's Parliament requested Ismail Khan to answer their queries. The government believes that Khan is attempting to create some kind of disruption in the country.\n\nOn September 27, 2009, Ismail Khan survived a suicide blast that killed 4 of his bodyguards in Herat, in western Afghanistan. He was driving to Herat Airport when a powerful explosion occurred on the way there. Taliban spokesman, Zabiullah Mujahid, claimed responsibility and said the target was Khan.\n\nGuantanamo captive Abdul Razzaq Hekmati requested Ismail Khan's testimony, when he was called before a Combatant Status Review Tribunal. \nIsmail Khan, like Afghan Minister of Defense Rahim Wardak, was one of the high-profile Afghans that those conducting the Tribunals ruled were \"not reasonably available\" to give a statement on a captive's behalf because they could not be located.\n\nHekmati had played a key role in helping Ismail Khan escape from the Taliban in 1999.\nHekmati stood accused of helping Taliban leaders escape from the custody of Hamid Karzai's government.\n\nCarlotta Gall and Andy Worthington interviewed Ismail Khan for a new \"The New York Times\" article after Hekmati died of cancer in Guantanamo. \nAccording to the \"New York Times\"\nIsmail Khan said he personally buttonholed the American ambassador to tell him that Hekmati was innocent, and should be released. In contrast, Hekmati was told that the State Department had been unable to locate Khan.\n\nIsmail Khan is a controversial figure. Reporters Without Borders has charged him with muzzling the press and ordering attacks on journalists. Also Human Rights Watch has accused him of human rights abuses.\n\nNevertheless, he remains a popular figure for some in Afghanistan. Unlike other mujahideen commanders, Khan has not been linked to large-scale massacres and atrocities such as those committed after the capture of Kabul in 1992. Following news of his dismissal, rioting broke out in the streets of Herat, and President Karzai had to ask him to make a personal appeal for calm.\n\n"}
{"id": "15250", "url": "https://en.wikipedia.org/wiki?curid=15250", "title": "Indigo", "text": "Indigo\n\nIndigo is a deep and rich color close to the color wheel blue (a primary color in the RGB color space), as well as to some variants of ultramarine. It is traditionally regarded as a color in the visible spectrum, as well as one of the seven colors of the rainbow: the color between violet and blue; however, sources differ as to its actual position in the electromagnetic spectrum.\n\nThe color indigo is named after the indigo dye derived from the plant \"Indigofera tinctoria\" and related species.\n\nThe first known recorded use of indigo as a color name in English was in 1289.\n\nSpecies of \"Indigofera\" were cultivated in East Asia, Egypt, India, and Peru in antiquity. The earliest direct evidence for the use of indigo dates to around 4000 BC and comes from Huaca Prieta, in contemporary Peru. Pliny the Elder mentions India as the source of the dye after which it was named. It was imported from there in small quantities via the Silk Road.\n\nThe Ancient Greek term for the dye was (\"Indian dye\"), which, adopted to Latin as \"indicum\" and via Portuguese gave rise to the modern word \"indigo\". \n\nSpanish explorers discovered an American species of indigo and began to cultivate the product in Guatemala. The English and French subsequently began to encourage indigo cultivation in their colonies in the West Indies.\n\nBlue dye can be made from two different types of plants: the indigo plant, which produces the best results, and from the woad plant \"Isatis tinctoria\", also known as pastel. For a long time woad was the main source of blue dye in Europe. Woad was replaced by true indigo as trade routes opened up, and both plant sources have now been largely replaced by synthetic dyes.\n\nThe Early Modern English word \"indigo\" referred to the dye, not to the color (hue) itself, and \"indigo\" is not traditionally part of the basic color-naming system. Modern sources place indigo in the electromagnetic spectrum between 420 and 450 nanometers, which lies on the short-wave side of color wheel (RGB) blue, towards (spectral) violet.\n\nHowever, the correspondence of this definition with colors of actual indigo dyes is disputed. Optical scientists Hardy and Perrin list indigo as between 445 and 464 nm wavelength, which occupies a spectrum segment from roughly the color wheel (RGB) blue extending to the long-wave side, towards azure.\n\nIsaac Newton introduced indigo as one of the seven base colors of his work. In the mid-1660s, when Newton bought a pair of prisms at a fair near Cambridge, the East India Company had begun importing indigo dye into England, supplanting the homegrown woad as source of blue dye. In a pivotal experiment in the history of optics, the young Newton shone a narrow beam of sunlight through a prism to produce a rainbow-like band of colors on the wall. In describing this optical spectrum, Newton acknowledged that the spectrum had a continuum of colors, but named seven: \"The originall or primary colours are Red, yellow, Green, Blew, & a violet purple; together with Orang, Indico, & an indefinite varietie of intermediate gradations.\" He linked the seven prismatic colors to the seven notes of a western major scale, as shown in his color wheel, with orange and indigo as the semitones. Having decided upon seven colors, he asked a friend to repeatedly divide up the spectrum that was projected from the prism onto the wall:\n\nI desired a friend to draw with a pencil lines cross the image, or pillar of colours, where every one of the seven aforenamed colours was most full and brisk, and also where he judged the truest confines of them to be, whilst I held the paper so, that the said image might fall within a certain compass marked on it. And this I did, partly because my own eyes are not very critical in distinguishing colours, partly because another, to whom I had not communicated my thoughts about this matter, could have nothing but his eyes to determine his fancy in making those marks.\n\nIndigo is therefore counted as one of the traditional colors of the rainbow, the order of which is given by the mnemonic \"Roy G. Biv\". James Clerk Maxwell and Hermann von Helmholtz accepted indigo as an appropriate name for the color flanking violet in the spectrum.\n\nLater scientists conclude that Newton named the colors differently from current usage.\nAccording to Gary Waldman, \"A careful reading of Newton's work indicates that the color he called indigo, we would normally call blue; his blue is then what we would name blue-green, cyan or light blue.\" If this is true, Newton's seven spectral colors would have been:\nRed: Orange: Yellow: Green: Blue: Indigo: Violet:\n\nThe human eye does not readily differentiate hues in the wavelengths between what we today call blue and violet. If this is where Newton meant indigo to lie, most individuals would have difficulty distinguishing indigo from its neighbors. According to Isaac Asimov, \"It is customary to list indigo as a color lying between blue and violet, but it has never seemed to me that indigo is worth the dignity of being considered a separate color. To my eyes it seems merely deep blue.\"\n\nModern color scientists typically divide the spectrum between violet and blue at about 450 nm, with no indigo.\n\nLike many other colors (orange, rose, and violet are the best-known), indigo gets its name from an object in the natural world—the plant named indigo once used for dyeing cloth (see also Indigo dye).\n\nThe color \"electric indigo\" is a bright and saturated color between the traditional indigo and violet. This is the brightest color indigo that can be approximated on a computer screen; it is a color located between the (primary) blue and the color violet of the RGB color wheel.\n\nThe web color \"blue violet\" or \"deep indigo\" is a tone of indigo brighter than pigment indigo, but not as bright as electric indigo.\n\nThe color \"pigment indigo\" is equivalent to the web color indigo and approximates the color indigo that is usually reproduced in pigments and colored pencils.\n\nThe color of indigo dye is a different color from either spectrum indigo or pigment indigo. This is the actual color of the dye. A vat full of this dye is a darker color, approximating the web color midnight blue.\n\nBelow are displayed these four major tones of indigo.\n\n\"Electric indigo\" is brighter than the pigment indigo reproduced below. When plotted on the CIE chromaticity diagram, this color is at 435 nanometers, in the middle of the portion of the spectrum traditionally considered indigo, i.e., between 450 and 420 nanometers. This color is only an approximation of spectral indigo, since actual spectral colors are outside the gamut of the sRGB color system.\nAt right is displayed the web color \"blue-violet\", a color intermediate in brightness between electric indigo and pigment indigo. It is also known as \"deep indigo\".\n\nThe color box at right displays the web color indigo, the color indigo as it would be reproduced by artists' paints as opposed to the brighter indigo above (electric indigo) that is possible to reproduce on a computer screen. Its hue is closer to violet than to indigo dye for which the color is named. Pigment indigo can be obtained by mixing 55% pigment cyan with about 45% pigment magenta.\n\nCompare the subtractive colors to the additive colors in the two primary color charts in the article on primary colors to see the distinction between electric colors as reproducible from light on a computer screen (additive colors) and the pigment colors reproducible with pigments (subtractive colors); the additive colors are significantly brighter because they are produced from light instead of pigment.\n\nWeb color indigo represents the way the color indigo was always reproduced in pigments, paints, or colored pencils in the 1950s. By the 1970s, because of the advent of psychedelic art, artists became used to brighter pigments, and pigments called \"bright indigo\" or \"bright blue-violet\" that are the pigment equivalent of the electric indigo reproduced in the section above became available in artists' pigments and colored pencils.\n'Tropical Indigo' is the color that is called \"añil\" in the \"Guía de coloraciones\" (Guide to colorations) by Rosa Gallego and\nJuan Carlos Sanz, a color dictionary published in 2005 that is widely popular in the Hispanophone realm.\n\n\"Indigo dye\" is a greenish dark blue color.\n\n\nLiterature\n\nMarina Warner's novel \"Indigo\" (1992) is a retelling of Shakespeare's \"The Tempest\" and features the production of indigo dye by Sycorax.\n\n\n\n\n\nThe French Army adopted dark blue indigo at the time of the French Revolution, as a replacement for the white uniforms previously worn by the Royal infantry regiments. In 1806, Napoleon decided to restore the white coats because of shortages of indigo dye imposed by the British continental blockade. However, the greater practicability of the blue color led to its retention, and indigo remained the dominant color of French military coats until 1914.\n\nThe spiritualist applications use electric indigo, because the color is positioned between blue and violet on the spectrum.\n\n"}
{"id": "15251", "url": "https://en.wikipedia.org/wiki?curid=15251", "title": "International Monetary Fund", "text": "International Monetary Fund\n\nThe International Monetary Fund (IMF) is an international organization headquartered in Washington, D.C., consisting of \"189 countries working to foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world.\" Formed in 1944 at the Bretton Woods Conference primarily by the ideas of Harry Dexter White and John Maynard Keynes, it came into formal existence in 1945 with 29 member countries and the goal of reconstructing the international payment system. It now plays a central role in the management of balance of payments difficulties and international financial crises. Countries contribute funds to a pool through a quota system from which countries experiencing balance of payments problems can borrow money. , the fund had SDR477 billion (about $666 billion).\n\nThrough the fund, and other activities such as the gathering of statistics and analysis, surveillance of its members' economies and the demand for particular policies, the IMF works to improve the economies of its member countries. The organisation's objectives stated in the Articles of Agreement are: to promote international monetary co-operation, international trade, high employment, exchange-rate stability, sustainable economic growth, and making resources available to member countries in financial difficulty.\nIMF funds come from two major sources:quotas and loans. Quotas, which are pooled funds of member nations, generate most IMF funds. The size of a member's quota depends on its economic and financial importance in the world. Nations with larger economic importance have larger quotas. The quotas are increased periodically as a means of boosting the IMF's resources.\n\nThe current Managing Director (MD) and Chairwoman of the International Monetary Fund is noted French lawyer and former politician, Christine Lagarde, who has held the post since 5 July 2011.\n\nAccording to the IMF itself, it works to foster global growth and economic stability by providing policy advice and financing the members by working with developing nations to help them achieve macroeconomic stability and reduce poverty. The rationale for this is that private international capital markets function imperfectly and many countries have limited access to financial markets. Such market imperfections, together with balance-of-payments financing, provide the justification for official financing, without which many countries could only correct large external payment imbalances through measures with adverse economic consequences. The IMF provides alternate sources of financing.\n\nUpon the founding of the IMF, its three primary functions were: to oversee the fixed exchange rate arrangements between countries, thus helping national governments manage their exchange rates and allowing these governments to prioritize economic growth, and to provide short-term capital to aid the balance of payments. This assistance was meant to prevent the spread of international economic crises. The IMF was also intended to help mend the pieces of the international economy after the Great Depression and World War II. As well, to provide capital investments for economic growth and projects such as infrastructure.\n\nThe IMF's role was fundamentally altered by the floating exchange rates post-1971. It shifted to examining the economic policies of countries with IMF loan agreements to determine if a shortage of capital was due to economic fluctuations or economic policy. The IMF also researched what types of government policy would ensure economic recovery. A particular concern of the IMF was to prevent financial crisis, such as those in Mexico 1982, Brazil in 1987, East Asia in 1997–98 and Russia in 1998, from spreading and threatening the entire global financial and currency system. The challenge was to promote and implement policy that reduced the frequency of crises among the emerging market countries, especially the middle-income countries which are vulnerable to massive capital outflows. Rather than maintaining a position of oversight of only exchange rates, their function became one of surveillance of the overall macroeconomic performance of member countries. Their role became a lot more active because the IMF now manages economic policy rather than just exchange rates.\n\nIn addition, the IMF negotiates conditions on lending and loans under their policy of conditionality, which was established in the 1950s. Low-income countries can borrow on concessional terms, which means there is a period of time with no interest rates, through the Extended Credit Facility (ECF), the Standby Credit Facility (SCF) and the Rapid Credit Facility (RCF). Nonconcessional loans, which include interest rates, are provided mainly through Stand-By Arrangements (SBA), the Flexible Credit Line (FCL), the Precautionary and Liquidity Line (PLL), and the Extended Fund Facility. The IMF provides emergency assistance via the Rapid Financing Instrument (RFI) to members facing urgent balance-of-payments needs.\n\nThe IMF is mandated to oversee the international monetary and financial system and monitor the economic and financial policies of its member countries. This activity is known as surveillance and facilitates international co-operation. Since the demise of the Bretton Woods system of fixed exchange rates in the early 1970s, surveillance has evolved largely by way of changes in procedures rather than through the adoption of new obligations. The responsibilities changed from those of guardian to those of overseer of members' policies.\n\nThe Fund typically analyses the appropriateness of each member country's economic and financial policies for achieving orderly economic growth, and assesses the consequences of these policies for other countries and for the global economy.\n\nIn 1995 the International Monetary Fund began work on data dissemination standards with the view of guiding IMF member countries to disseminate their economic and financial data to the public. The International Monetary and Financial Committee (IMFC) endorsed the guidelines for the dissemination standards and they were split into two tiers: The General Data Dissemination System (GDDS) and the Special Data Dissemination Standard (SDDS).\n\nThe executive board approved the SDDS and GDDS in 1996 and 1997 respectively, and subsequent amendments were published in a revised \"Guide to the General Data Dissemination System\". The system is aimed primarily at statisticians and aims to improve many aspects of statistical systems in a country. It is also part of the World Bank Millennium Development Goals and Poverty Reduction Strategic Papers.\n\nThe primary objective of the GDDS is to encourage member countries to build a framework to improve data quality and statistical capacity building to evaluate statistical needs, set priorities in improving the timeliness, transparency, reliability and accessibility of financial and economic data. Some countries initially used the GDDS, but later upgraded to SDDS.\n\nSome entities that are not themselves IMF members also contribute statistical data to the systems:\n\nIMF conditionality is a set of policies or conditions that the IMF requires in exchange for financial resources. The IMF does require collateral from countries for loans but also requires the government seeking assistance to correct its macroeconomic imbalances in the form of policy reform. If the conditions are not met, the funds are withheld. The concept of conditionality was introduced in a 1952 Executive Board decision and later incorporated into the Articles of Agreement.\n\nConditionality is associated with economic theory as well as an enforcement mechanism for repayment. Stemming primarily from the work of Jacques Polak, the theoretical underpinning of conditionality was the \"monetary approach to the balance of payments\".\n\nSome of the conditions for structural adjustment can include:\n\nThese conditions are known as the Washington Consensus.\n\nThese loan conditions ensure that the borrowing country will be able to repay the IMF and that the country will not attempt to solve their balance-of-payment problems in a way that would negatively impact the international economy. The incentive problem of moral hazard—when economic agents maximise their own utility to the detriment of others because they do not bear the full consequences of their actions—is mitigated through conditions rather than providing collateral; countries in need of IMF loans do not generally possess internationally valuable collateral anyway.\n\nConditionality also reassures the IMF that the funds lent to them will be used for the purposes defined by the Articles of Agreement and provides safeguards that country will be able to rectify its macroeconomic and structural imbalances. In the judgment of the IMF, the adoption by the member of certain corrective measures or policies will allow it to repay the IMF, thereby ensuring that the resources will be available to support other members.\n\n, borrowing countries have had a very good track record for repaying credit extended under the IMF's regular lending facilities with full interest over the duration of the loan. This indicates that IMF lending does not impose a burden on creditor countries, as lending countries receive market-rate interest on most of their quota subscription, plus any of their own-currency subscriptions that are loaned out by the IMF, plus all of the reserve assets that they provide the IMF.\n\nThe IMF was originally laid out as a part of the Bretton Woods system exchange agreement in 1944. During the Great Depression, countries sharply raised barriers to trade in an attempt to improve their failing economies. This led to the devaluation of national currencies and a decline in world trade.\nThis breakdown in international monetary co-operation created a need for oversight. The representatives of 45 governments met at the Bretton Woods Conference in the Mount Washington Hotel in Bretton Woods, New Hampshire, in the United States, to discuss a framework for postwar international economic co-operation and how to rebuild Europe.\n\nThere were two views on the role the IMF should assume as a global economic institution. American delegate Harry Dexter White foresaw an IMF that functioned more like a bank, making sure that borrowing states could repay their debts on time. Most of White's plan was incorporated into the final acts adopted at Bretton Woods. British economist John Maynard Keynes imagined that the IMF would be a cooperative fund upon which member states could draw to maintain economic activity and employment through periodic crises. This view suggested an IMF that helped governments and to act as the United States government had during the New Deal in response to World War II.\nThe IMF formally came into existence on 27 December 1945, when the first 29 countries ratified its Articles of Agreement. By the end of 1946 the IMF had grown to 39 members. On 1 March 1947, the IMF began its financial operations, and on 8 May France became the first country to borrow from it.\n\nThe IMF was one of the key organisations of the international economic system; its design allowed the system to balance the rebuilding of international capitalism with the maximisation of national economic sovereignty and human welfare, also known as embedded liberalism. The IMF's influence in the global economy steadily increased as it accumulated more members. The increase reflected in particular the attainment of political independence by many African countries and more recently the 1991 dissolution of the Soviet Union because most countries in the Soviet sphere of influence did not join the IMF.\n\nThe Bretton Woods system prevailed until 1971, when the United States government suspended the convertibility of the US$ (and dollar reserves held by other governments) into gold. This is known as the Nixon Shock. The changes to the IMF articles of agreement reflecting these changes were ratified by the 1976 Jamaica Accords.\n\nThe IMF provided two major lending packages in the early 2000s to Argentina (during the 1998–2002 Argentine great depression) and Uruguay (after the 2002 Uruguay banking crisis). However, by the mid-2000s, IMF lending was at its lowest share of world GDP since the 1970s.\n\nIn May 2010, the IMF participated, in 3:11 proportion, in the first Greek bailout that totalled €110 billion, to address the great accumulation of public debt, caused by continuing large public sector deficits. As part of the bailout, the Greek government agreed to adopt austerity measures that would reduce the deficit from 11% in 2009 to \"well below 3%\" in 2014. The bailout did not include debt restructuring measures such as a haircut, to the chagrin of the Swiss, Brazilian, Indian, Russian, and Argentinian Directors of the IMF, with the Greek authorities themselves (at the time, PM George Papandreou and Finance Minister Giorgos Papakonstantinou) ruling out a haircut.\n\nA second bailout package of more than €100 billion was agreed over the course of a few months from October 2011, during which time Papandreou was forced from office. The so-called Troika, of which the IMF is part, are joint managers of this programme, which was approved by the Executive Directors of the IMF on 15 March 2012 for SDR23.8 billion, and which saw private bondholders take a haircut of upwards of 50%. In the interval between May 2010 and February 2012 the private banks of Holland, France and Germany reduced exposure to Greek debt from €122 billion to €66 billion.\n\n, the largest borrowers from the IMF in order were Greece, Portugal, Ireland, Romania, and Ukraine.\n\nOn 25 March 2013, a €10 billion international bailout of Cyprus was agreed by the Troika, at the cost to the Cypriots of its agreement: to close the country's second-largest bank; to impose a one-time bank deposit levy on Bank of Cyprus uninsured deposits. No insured deposit of €100k or less were to be affected under the terms of a novel bail-in scheme.\n\nThe topic of sovereign debt restructuring was taken up by the IMF in April 2013 for the first time since 2005, in a report entitled \"Sovereign Debt Restructuring: Recent Developments and Implications for the Fund's Legal and Policy Framework\". The paper, which was discussed by the board on 20 May, summarised the recent experiences in Greece, St Kitts and Nevis, Belize, and Jamaica. An explanatory interview with Deputy Director Hugh Bredenkamp was published a few days later, as was a deconstruction by Matina Stevis of the \"Wall Street Journal\".\n\nIn the October 2013 Fiscal Monitor publication, the IMF suggested that a capital levy capable of reducing Euro-area government debt ratios to \"end-2007 levels\" would require a very high tax rate of about 10%.\n\nThe Fiscal Affairs department of the IMF, headed at the time by Acting Director Sanjeev Gupta, produced a January 2014 report entitled \"Fiscal Policy and Income Inequality\" that stated that \"Some taxes levied on wealth, especially on immovable property, are also an option for economies seeking more progressive taxation ... Property taxes are equitable and efficient, but underutilized in many economies ... There is considerable scope to exploit this tax more fully, both as a revenue source and as a redistributive instrument.\"\n\nAt the end of March 2014, the IMF secured an $18 billion bailout fund for the provisional government of Ukraine in the aftermath of the 2014 Ukrainian revolution.\n\nNot all member countries of the IMF are sovereign states, and therefore not all \"member countries\" of the IMF are members of the United Nations. Amidst \"member countries\" of the IMF that are not member states of the UN are non-sovereign areas with special jurisdictions that are officially under the sovereignty of full UN member states, such as Aruba, Curaçao, Hong Kong, and Macau, as well as Kosovo. The corporate members appoint \"ex-officio\" voting members, who are listed below. All members of the IMF are also International Bank for Reconstruction and Development (IBRD) members and vice versa.\n\nFormer members are Cuba (which left in 1964), and the Republic of China (Taiwan), which was ejected from the UN in 1980 after losing the support of then United States President Jimmy Carter and was replaced by the People's Republic of China. However, \"Taiwan Province of China\" is still listed in the official IMF indices.\n\nApart from Cuba, the other UN states that do not belong to the IMF are Andorra, Liechtenstein, Monaco and North Korea.\n\nThe former Czechoslovakia was expelled in 1954 for \"failing to provide required data\" and was readmitted in 1990, after the Velvet Revolution. Poland withdrew in 1950—allegedly pressured by the Soviet Union—but returned in 1986.\n\nAny country may apply to be a part of the IMF. Post-IMF formation, in the early postwar period, rules for IMF membership were left relatively loose. Members needed to make periodic membership payments towards their quota, to refrain from currency restrictions unless granted IMF permission, to abide by the Code of Conduct in the IMF Articles of Agreement, and to provide national economic information. However, stricter rules were imposed on governments that applied to the IMF for funding.\n\nThe countries that joined the IMF between 1945 and 1971 agreed to keep their exchange rates secured at rates that could be adjusted only to correct a \"fundamental disequilibrium\" in the balance of payments, and only with the IMF's agreement.\n\nSome members have a very difficult relationship with the IMF and even when they are still members they do not allow themselves to be monitored.\n\nMember countries of the IMF have access to information on the economic policies of all member countries, the opportunity to influence other members' economic policies, technical assistance in banking, fiscal affairs, and exchange matters, financial support in times of payment difficulties, and increased opportunities for trade and investment.\n\nThe Board of Governors consists of one governor and one alternate governor for each member country. Each member country appoints its two governors. The Board normally meets once a year and is responsible for electing or appointing executive directors to the Executive Board. While the Board of Governors is officially responsible for approving quota increases, Special Drawing Right allocations, the admittance of new members, compulsory withdrawal of members, and amendments to the Articles of Agreement and By-Laws, in practice it has delegated most of its powers to the IMF's Executive Board.\n\nThe Board of Governors is advised by the International Monetary and Financial Committee and the Development Committee. The International Monetary and Financial Committee has 24 members and monitors developments in global liquidity and the transfer of resources to developing countries. The Development Committee has 25 members and advises on critical development issues and on financial resources required to promote economic development in developing countries. They also advise on trade and environmental issues.\n\nThe Board of Governors reports directly to the Managing Director of the IMF, Christine Lagarde.\n\n24 Executive Directors make up the Executive Board. The Executive Directors represent all 189 member countries in a geographically based roster. Countries with large economies have their own Executive Director, but most countries are grouped in constituencies representing four or more countries.\n\nFollowing the \"2008 Amendment on Voice and Participation\" which came into effect in March 2011, eight countries each appoint an Executive Director: the United States, Japan, China, Germany, France, the United Kingdom, Russia, and Saudi Arabia. The remaining 16 Directors represent constituencies consisting of 4 to 22 countries. The Executive Director representing the largest constituency of 22 countries accounts for 1.55% of the vote. This Board usually meets several times each week. The Board membership and constituency is scheduled for periodic review every eight years.\n\nThe IMF is led by a managing director, who is head of the staff and serves as Chairman of the Executive Board. The managing director is assisted by a First Deputy managing director and three other Deputy Managing Directors. Historically the IMF's managing director has been European and the president of the World Bank has been from the United States. However, this standard is increasingly being questioned and competition for these two posts may soon open up to include other qualified candidates from any part of the world.\n\nIn 2011 the world's largest developing countries, the BRIC nations, issued a statement declaring that the tradition of appointing a European as managing director undermined the legitimacy of the IMF and called for the appointment to be merit-based.\n\nPrevious managing director Dominique Strauss-Kahn was arrested in connection with charges of sexually assaulting a New York hotel room attendant and resigned on 18 May. The charges were later dropped. On 28 June 2011 Christine Lagarde was confirmed as managing director of the IMF for a five-year term starting on 5 July 2011. She was re-elected by consensus for a second five-year term, starting 5 July 2016, being the only candidate nominated for the post of Managing Director.\n\nVoting power in the IMF is based on a quota system. Each member has a number of basic votes (each member's number of basic votes equals 5.502% of the total votes), plus one additional vote for each Special Drawing Right (SDR) of 100,000 of a member country's quota. The Special Drawing Right is the unit of account of the IMF and represents a claim to currency. It is based on a basket of key international currencies. The basic votes generate a slight bias in favour of small countries, but the additional votes determined by SDR outweigh this bias. Changes in the voting shares require approval by a supermajority of 85% of voting power.\n\nIn December 2015, the United States Congress adopted a legislation authorising the 2010 Quota and Governance Reforms. As a result,\n\nThe IMF's quota system was created to raise funds for loans. Each IMF member country is assigned a quota, or contribution, that reflects the country's relative size in the global economy. Each member's quota also determines its relative voting power. Thus, financial contributions from member governments are linked to voting power in the organisation.\n\nThis system follows the logic of a shareholder-controlled organisation: wealthy countries have more say in the making and revision of rules. Since decision making at the IMF reflects each member's relative economic position in the world, wealthier countries that provide more money to the IMF have more influence than poorer members that contribute less; nonetheless, the IMF focuses on redistribution.\n\nQuotas are normally reviewed every five years and can be increased when deemed necessary by the Board of Governors. IMF voting shares are relatively inflexible: countries that grow economically have tended to become under-represented as their voting power lags behind. Currently, reforming the representation of developing countries within the IMF has been suggested. These countries' economies represent a large portion of the global economic system but this is not reflected in the IMF's decision making process through the nature of the quota system. Joseph Stiglitz argues, \"There is a need to provide more effective voice and representation for developing countries, which now represent a much larger portion of world economic activity since 1944, when the IMF was created.\" In 2008, a number of quota reforms were passed including shifting 6% of quota shares to dynamic emerging markets and developing countries.\n\nThe IMF's membership is divided along income lines: certain countries provide the financial resources while others use these resources. Both developed country \"creditors\" and developing country \"borrowers\" are members of the IMF. The developed countries provide the financial resources but rarely enter into IMF loan agreements; they are the creditors. Conversely, the developing countries use the lending services but contribute little to the pool of money available to lend because their quotas are smaller; they are the borrowers. Thus, tension is created around governance issues because these two groups, creditors and borrowers, have fundamentally different interests.\n\nThe criticism is that the system of voting power distribution through a quota system institutionalises borrower subordination and creditor dominance. The resulting division of the IMF's membership into borrowers and non-borrowers has increased the controversy around conditionality because the borrowers are interested in increasing loan access while creditors want to maintain reassurance that the loans will be repaid.\n\nA recent source revealed that the average overall use of IMF credit per decade increased, in real terms, by 21% between the 1970s and 1980s, and increased again by just over 22% from the 1980s to the 1991–2005 period. Another study has suggested that since 1950 the continent of Africa alone has received $300 billion from the IMF, the World Bank, and affiliate institutions.\n\nA study by Bumba Mukherjee found that developing democratic countries benefit more from IMF programs than developing autocratic countries because policy-making, and the process of deciding where loaned money is used, is more transparent within a democracy. One study done by Randall Stone found that although earlier studies found little impact of IMF programs on balance of payments, more recent studies using more sophisticated methods and larger samples \"usually found IMF programs improved the balance of payments\".\n\nThe Exceptional Access Framework was created in 2003 when John B. Taylor was Under Secretary of the US Treasury for International Affairs. The new Framework became fully operational in February 2003 and it was applied in the subsequent decisions on Argentina and Brazil. Its purpose was to place some sensible rules and limits on the way the IMF makes loans to support governments with debt problem—especially in emerging markets—and thereby move away from the bailout mentality of the 1990s. Such a reform was essential for ending the crisis atmosphere that then existed in emerging markets. The reform was closely related to, and put in place nearly simultaneously with, the actions of several emerging market countries to place collective action clauses in their bond contracts.\n\nIn 2010, the framework was abandoned so the IMF could make loans to Greece in an unsustainable and political situation.\n\nThe topic of sovereign debt restructuring was taken up by IMF staff in April 2013 for the first time since 2005, in a report entitled \"Sovereign Debt Restructuring: Recent Developments and Implications for the Fund's Legal and Policy Framework\". The paper, which was discussed by the board on 20 May, summarised the recent experiences in Greece, St Kitts and Nevis, Belize and Jamaica. An explanatory interview with Deputy Director Hugh Bredenkamp was published a few days later, as was a deconstruction by Matina Stevis of the \"Wall Street Journal\".\n\nThe staff was directed to formulate an updated policy, which was accomplished on 22 May 2014 with a report entitled \"The Fund's Lending Framework and Sovereign Debt: Preliminary Considerations\", and taken up by the Executive Board on 13 June. The staff proposed that \"in circumstances where a (Sovereign) member has lost market access and debt is considered sustainable ... the IMF would be able to provide Exceptional Access on the basis of a debt operation that involves an extension of maturities\", which was labelled a \"reprofiling operation\". These reprofiling operations would \"generally be less costly to the debtor and creditors—and thus to the system overall—relative to either an upfront debt reduction operation or a bail-out that is followed by debt reduction ... (and) would be envisaged only when both (a) a member has lost market access and (b) debt is assessed to be sustainable, but not with high probability ... Creditors will only agree if they understand that such an amendment is necessary to avoid a worse outcome: namely, a default and/or an operation involving debt reduction ... Collective action clauses, which now exist in most—but not all—bonds, would be relied upon to address collective action problems.\"\n\nGlobalization encompasses three institutions: global financial markets and transnational companies, national governments linked to each other in economic and military alliances led by the United States, and rising \"global governments\" such as World Trade Organization (WTO), IMF, and World Bank. Charles Derber argues in his book \"People Before Profit,\" \"These interacting institutions create a new global power system where sovereignty is globalized, taking power and constitutional authority away from nations and giving it to global markets and international bodies\". Titus Alexander argues that this system institutionalises global inequality between western countries and the Majority World in a form of global apartheid, in which the IMF is a key pillar.\n\nThe establishment of globalised economic institutions has been both a symptom of and a stimulus for globalisation. The development of the World Bank, the IMF regional development banks such as the European Bank for Reconstruction and Development (EBRD), and multilateral trade institutions such as the WTO signals a move away from the dominance of the state as the primary actor analyzed in international affairs. Globalization has thus been transformative in terms of a reconceptualising of state sovereignty.\n\nFollowing United States President Bill Clinton's administration's aggressive financial deregulation campaign in the 1990s, globalisation leaders overturned longstanding restrictions by governments that limited foreign ownership of their banks, deregulated currency exchange, and eliminated restrictions on how quickly money could be withdrawn by foreign investors.\n\nAn IMF report from May 2015 estimated that the world's governments directly and indirectly subsidise fossil fuel companies with $5.3tn (£3.4tn) a year. The measurement accounts for the unpaid costs that polluters impose on governments by the burning of coal, oil, and gas. The projected impacts of fossil fuel subsidies on populations—air pollution, health problems, floods, droughts, and storms driven by climate change—account for over half of the reported global expenditure.\n\nOverseas Development Institute (ODI) research undertaken in 1980 included criticisms of the IMF which support the analysis that it is a pillar of what activist Titus Alexander calls global apartheid.\n\nODI conclusions were that the IMF's very nature of promoting market-oriented approaches attracted unavoidable criticism. On the other hand, the IMF could serve as a scapegoat while allowing governments to blame international bankers. The ODI conceded that the IMF was insensitive to political aspirations of LDCs, while its policy conditions were inflexible.\n\nArgentina, which had been considered by the IMF to be a model country in its compliance to policy proposals by the Bretton Woods institutions, experienced a catastrophic economic crisis in 2001, which some believe to have been caused by IMF-induced budget restrictions—which undercut the government's ability to sustain national infrastructure even in crucial areas such as health, education, and security—and privatisation of strategically vital national resources. Others attribute the crisis to Argentina's misdesigned fiscal federalism, which caused subnational spending to increase rapidly. The crisis added to widespread hatred of this institution in Argentina and other South American countries, with many blaming the IMF for the region's economic problems. The current—as of early 2006—trend toward moderate left-wing governments in the region and a growing concern with the development of a regional economic policy largely independent of big business pressures has been ascribed to this crisis.\n\nIn 2006, a senior ActionAid policy analyst Akanksha Marphatia stated that IMF policies in Africa undermine any possibility of meeting the Millennium Development Goals (MDGs) due to imposed restrictions that prevent spending on important sectors, such as education and health.\n\nIn an interview (2008-05-19), the former Romanian Prime Minister Călin Popescu-Tăriceanu claimed that \"Since 2005, IMF is constantly making mistakes when it appreciates the country's economic performances\". Former Tanzanian President Julius Nyerere, who claimed that debt-ridden African states were ceding sovereignty to the IMF and the World Bank, famously asked, \"Who elected the IMF to be the ministry of finance for every country in the world?\"\n\nFormer chief economist of IMF and former Reserve Bank of India (RBI) Governor Raghuram Rajan who predicted Financial crisis of 2007–08 criticised IMF for remaining a sideline player to the Developed world. He criticised IMF for praising the monetary policies of the US, which he believed were wreaking havoc in emerging markets. He had been critical of the ultra-loose money policies of the Western nations and IMF.\n\nCountries such as Zambia have not received proper aid with long-lasting effects, leading to concern from economists. Since 2005, Zambia (as well as 29 other African countries) did receive debt write-offs, which helped with the country's medical and education funds. However, Zambia returned to a debt of over half its GDP in less than a decade. American economist William Easterly, skeptical of the IMF's methods, had initially warned that \"debt relief would simply encourage more reckless borrowing by crooked governments unless it was accompanied by reforms to speed up economic growth and improve governance,\" according to \"The Economist\".\n\nThe IMF has been criticised for being \"out of touch\" with local economic conditions, cultures, and environments in the countries they are requiring policy reform. The economic advice the IMF gives might not always take into consideration the difference between what spending means on paper and how it is felt by citizens.\n\nJeffrey Sachs argues that the IMF's \"usual prescription is 'budgetary belt tightening to countries who are much too poor to own belts'\". Sachs wrote that the IMF's role as a generalist institution specialising in macroeconomic issues needs reform. Conditionality has also been criticised because a country can pledge collateral of \"acceptable assets\" to obtain waivers—if one assumes that all countries are able to provide \"acceptable collateral\".\n\nOne view is that conditionality undermines domestic political institutions. The recipient governments are sacrificing policy autonomy in exchange for funds, which can lead to public resentment of the local leadership for accepting and enforcing the IMF conditions. Political instability can result from more leadership turnover as political leaders are replaced in electoral backlashes. IMF conditions are often criticised for reducing government services, thus increasing unemployment.\n\nAnother criticism is that IMF programs are only designed to address poor governance, excessive government spending, excessive government intervention in markets, and too much state ownership. This assumes that this narrow range of issues represents the only possible problems; everything is standardised and differing contexts are ignored. A country may also be compelled to accept conditions it would not normally accept had they not been in a financial crisis in need of assistance.\n\nOn top of that, regardless of what methodologies and data sets used, it comes to same the conclusion of exacerbating income inequality. With Gini coefficient, it became clear that countries with IMF programs face increased income inequality.\n\nIt is claimed that conditionalities retard social stability and hence inhibit the stated goals of the IMF, while Structural Adjustment Programs lead to an increase in poverty in recipient countries. The IMF sometimes advocates \"austerity programmes\", cutting public spending and increasing taxes even when the economy is weak, to bring budgets closer to a balance, thus reducing budget deficits. Countries are often advised to lower their corporate tax rate. In \"Globalization and Its Discontents\", Joseph E. Stiglitz, former chief economist and senior vice-president at the World Bank, criticises these policies. He argues that by converting to a more monetarist approach, the purpose of the fund is no longer valid, as it was designed to provide funds for countries to carry out Keynesian reflations, and that the IMF \"was not participating in a conspiracy, but it was reflecting the interests and ideology of the Western financial community\".\n\nInternational politics play an important role in IMF decision making. The clout of member states is roughly proportional to its contribution to IMF finances. The United States has the greatest number of votes and therefore wields the most influence. Domestic politics often come into play, with politicians in developing countries using conditionality to gain leverage over the opposition to influence policy.\n\nThe IMF is only one of many international organisations, and it is a generalist institution that deals only with macroeconomic issues; its core areas of concern in developing countries are very narrow. One proposed reform is a movement towards close partnership with other specialist agencies such as UNICEF, the Food and Agriculture Organization (FAO), and the United Nations Development Program (UNDP).\n\nJeffrey Sachs argues in \"The End of Poverty\" that the IMF and the World Bank have \"the brightest economists and the lead in advising poor countries on how to break out of poverty, but the problem is development economics\". Development economics needs the reform, not the IMF. He also notes that IMF loan conditions should be paired with other reforms—e.g., trade reform in developed nations, debt cancellation, and increased financial assistance for investments in basic infrastructure. IMF loan conditions cannot stand alone and produce change; they need to be partnered with other reforms or other conditions as applicable.\n\nThe scholarly consensus is that IMF decision-making is not simply technocratic, but also guided by political and economic concerns. The United States is the IMF's most powerful member, and its influence reaches even into decision-making concerning individual loan agreements. The United States has historically been openly opposed to losing what Treasury Secretary Jacob Lew described in 2015 as its \"leadership role\" at the IMF, and the United States' \"ability to shape international norms and practices\".\n\nReforms to give more powers to emerging economies were agreed by the G20 in 2010. The reforms could not pass, however, until they were ratified by the US Congress, since 85% of the Fund's voting power was required for the reforms to take effect, and the Americans held more than 16% of voting power at the time. After repeated criticism, the United States finally ratified the voting reforms at the end of 2015. The OECD countries maintained their overwhelming majority of voting share, and the United States in particular retained its share at over 16%.\n\nThe role of the Bretton Woods institutions has been controversial since the late Cold War, because of claims that the IMF policy makers supported military dictatorships friendly to American and European corporations, but also other anti-communist and Communist regimes (such as Mobutu's Zaire and Ceaușescu's Romania, respectively). Critics also claim that the IMF is generally apathetic or hostile to human rights, and labour rights. The controversy has helped spark the anti-globalization movement.\n\nAn example of IMF's support for a dictatorship was its ongoing support for Mobutu's rule in Zaire, although its own envoy, Erwin Blumenthal, provided a sobering report about the entrenched corruption and embezzlement and the inability of the country to pay back any loans.\n\nArguments in favour of the IMF say that economic stability is a precursor to democracy; however, critics highlight various examples in which democratised countries fell after receiving IMF loans.\n\nA 2017 study found no evidence of IMF lending programs undermining democracy in borrowing countries. To the contrary, it found \"evidence for modest but definitively positive conditional differences in the democracy scores of participating and non-participating countries.\"\n\nA number of civil society organisations have criticised the IMF's policies for their impact on access to food, particularly in developing countries. In October 2008, former United States president Bill Clinton delivered a speech to the United Nations on World Food Day, criticising the World Bank and IMF for their policies on food and agriculture:\n\nThe FPIF remarked that there is a recurring pattern: \"the destabilization of peasant producers by a one-two punch of IMF-World Bank structural adjustment programs that gutted government investment in the countryside followed by the massive influx of subsidized U.S. and European Union agricultural imports after the WTO's Agreement on Agriculture pried open markets.\"\n\nA 2009 study concluded that the strict conditions resulted in thousands of deaths in Eastern Europe by tuberculosis as public health care had to be weakened. In the 21 countries to which the IMF had given loans, tuberculosis deaths rose by 16.6%.\n\nIn 2009, a book by Rick Rowden titled \"The Deadly Ideas of Neoliberalism: How the IMF has Undermined Public Health and the Fight Against AIDS\", claimed that the IMF's monetarist approach towards prioritising price stability (low inflation) and fiscal restraint (low budget deficits) was unnecessarily restrictive and has prevented developing countries from scaling up long-term investment in public health infrastructure. The book claimed the consequences have been chronically underfunded public health systems, leading to demoralising working conditions that have fuelled a \"brain drain\" of medical personnel, all of which has undermined public health and the fight against HIV/AIDS in developing countries.\n\nIn 2016, the IMF's research department published a report titled \"Neoliberalism: Oversold?\" which, while praising some aspects of the \"neoliberal agenda,\" claims that the organisation has been \"overselling\" fiscal austerity policies and financial deregulation, which they claim has exacerbated both financial crises and economic inequality around the world.\n\nIMF policies have been repeatedly criticised for making it difficult for indebted countries to say no to environmentally harmful projects that nevertheless generate revenues such as oil, coal, and forest-destroying lumber and agriculture projects. Ecuador, for example, had to defy IMF advice repeatedly to pursue the protection of its rainforests, though paradoxically this need was cited in the IMF argument to provide support to Ecuador. The IMF acknowledged this paradox in the 2010 report that proposed the IMF Green Fund, a mechanism to issue special drawing rights directly to pay for climate harm prevention and potentially other ecological protection as pursued generally by other environmental finance.\n\nWhile the response to these moves was generally positive possibly because ecological protection and energy and infrastructure transformation are more politically neutral than pressures to change social policy. Some experts voiced concern that the IMF was not representative, and that the IMF proposals to generate only US$200 billion a year by 2020 with the SDRs as seed funds, did not go far enough to undo the general incentive to pursue destructive projects inherent in the world commodity trading and banking systems—criticisms often levelled at the World Trade Organization and large global banking institutions.\n\nIn the context of the European debt crisis, some observers noted that Spain and California, two troubled economies within Europe and the United States, and also Germany, the primary and politically most fragile supporter of a euro currency bailout would benefit from IMF recognition of their leadership in green technology, and directly from Green Fund-generated demand for their exports, which could also improve their credit ratings.\n\nBoth Lagarde and her two predecessors Strauss-Kahn and Rato have been investigated by the authorities and have either faced trial or are scheduled to go on trial for a variety of offences.\n\nLagarde had been accused of giving preferential treatment to businessman-turned-politician Bernard Tapie as he pursued a legal challenge against the French government. At the time, Lagarde was the French economic minister. Within hours of her conviction, in which she escaped any punishment, the fund's 24-member executive board put to rest any speculation that she might have to resign, praising her \"outstanding leadership\" and the \"wide respect\" she commands around the world.\n\nRato was arrested on 16 April 2015 for alleged fraud, embezzlement and money laundering. On 23 February 2017, Rato was found guilty of embezzlement and sentenced to 4 years' imprisonment. In September 2018, the sentence was confirmed by the Supreme Court of Spain.\n\nIn March 2011 the Ministers of Economy and Finance of the African Union proposed to establish an African Monetary Fund.\n\nAt the 6th BRICS summit in July 2014 the BRICS nations (Brazil, Russia, India, China, and South Africa) announced the BRICS Contingent Reserve Arrangement (CRA) with an initial size of US$100 billion, a framework to provide liquidity through currency swaps in response to actual or potential short-term balance-of-payments pressures.\n\nIn 2014, the China-led Asian Infrastructure Investment Bank was established.\n\n\"Life and Debt\", a documentary film, deals with the IMF's policies' influence on Jamaica and its economy from a critical point of view. \"Debtocracy\", a 2011 independent Greek documentary film, also criticises the IMF. Portuguese musician 's 1982 album is inspired by the IMF's intervention in Portugal through monitored stabilisation programs in 1977–78. In the 2015 film, \"Our Brand Is Crisis\", the IMF is mentioned as a point of political contention.\n\n\n\n"}
{"id": "15252", "url": "https://en.wikipedia.org/wiki?curid=15252", "title": "Islands of the Clyde", "text": "Islands of the Clyde\n\nThe Islands of the Firth of Clyde are the fifth largest of the major Scottish island groups after the Inner and Outer Hebrides, Orkney and Shetland. They are situated in the Firth of Clyde between Ayrshire and Argyll and Bute. There are about forty islands and skerries, of which only four are inhabited and only nine larger than . The largest and most populous are Arran and Bute, and Great Cumbrae and Holy Isle are also served by dedicated ferry routes. Unlike the four larger Scottish archipelagos, none of the isles in this group are connected to one another or to the mainland by bridges.\n\nThe geology and geomorphology of the area is complex and the islands and the surrounding sea lochs each have distinctive features. The influence of the Atlantic Ocean and the North Atlantic Drift create a mild, damp oceanic climate.\n\nThe larger islands have been continuously inhabited since Neolithic times, were influenced by the emergence of the kingdom of Dál Riata from 500 AD and then absorbed into the emerging Kingdom of Alba under Kenneth MacAlpin. They experienced Viking incursions during the Early Middle Ages and then became part of the Kingdom of Scotland in the 13th century. There is a diversity of wildlife, including three species of rare endemic tree.\n\nThe Highland Boundary Fault runs past Bute and through the northern part of Arran, so from a geological perspective some of the islands are in the Highlands and some in the Central Lowlands. As a result, Arran is sometimes referred to as \"Scotland in miniature\" and the island is a popular destination for geologists, who come to see intrusive igneous landforms such as sills and dykes as well as sedimentary and metasedimentary rocks ranging widely in age. Visiting in 1787, the geologist James Hutton found his first example of an unconformity there and this spot is one of the most famous places in the study of geology. A group of weakly metamorphosed rocks that form the Highland Border Complex lie discontinuously along the Highland Boundary Fault. One of the most prominent exposures is along Loch Fad on Bute. Ailsa Craig, which lies some south of Arran, has been quarried for a rare type of micro-granite containing riebeckite known as \"Ailsite\" which is used to make curling stones. As of 2004, 60 to 70% of all curling stones in use were made from granite from the island.\n\nIn common with the rest of Scotland the Firth of Clyde was covered by ice sheets during the Pleistocene ice ages and the landscape is much affected by glaciation. Arran's highest peaks may have been nunataks at this time. After the last retreat of the ice sea level changes and the isostatic rise of land makes charting post glacial coastlines a complex task but the resultant clifflines behind raised beaches are a prominent feature of the entire coastline.\n\nThe soils of the islands reflect the diverse geology. Bute has the most productive land, and a pattern of deposits that is typical of the southwest of Scotland. There is a mixture of boulder clay and other glacial deposits in the eroded valleys, and raised beach and marine deposits elsewhere, especially to the south and west which result in a machair landscape in places, inland from the sandy bays, such as Stravanan.\n\nThe Firth of Clyde, in which these island lie, is north of the Irish Sea and has numerous branching inlets, some of them substantial features in their own right. These include Loch Goil, Loch Long, Gare Loch, Loch Fyne and the estuary of the River Clyde. In places the effect of glaciation on the seabed is pronounced. For example, the Firth is deep between Arran and Bute, although they are only apart. The islands are all exposed to wind and tide and various lighthouses, such as those on Ailsa Craig, Pladda and Davaar act as an aid to navigation.\n\nThe Firth of Clyde lies between 55 and 56 degrees north, at the same latitude as Labrador in Canada and north of the Aleutian Islands, but the influence of the North Atlantic Drift—the northern extension of the Gulf Stream—ameliorates the winter weather and the area enjoys a mild, damp oceanic climate. Temperatures are generally cool, averaging about in January and in July at sea level. Snow seldom lies at sea level and frosts are generally less frequent than the mainland. In common with most islands of the west coast of Scotland, rainfall is generally high at between per annum on Bute, the Cumbraes and in the south of Arran and per annum in the north of Arran. The Arran mountains are wetter still with the summits receiving over annually. May, June and July are the sunniest months, with upwards of 200 hours of bright sunshine being recorded on average, southern Bute benefiting from a particularly high level of sunny days.\n\nMesolithic humans arrived in the Firth of the Clyde during the fourth millennium BC, probably from Ireland. This was followed by a wave of Neolithic peoples using the same route and there is some evidence that the Firth of Clyde was a significant route via which mainland Scotland was colonised at this time. A particular style of megalithic structure developed in Argyll, the Clyde estuary and elsewhere in western Scotland that has become known as the Clyde cairn. They are rectangular or trapezoidal in shape with a small enclosing chamber faced with large slabs of stone set on end and sometimes subdivided into smaller compartments. A forecourt area may have been used for displays or rituals associated with the interment of the dead, who were placed inside the chambers. They are concentrated in Arran, Bute and Kintyre and it is likely that the Clyde cairns were the earliest forms of Neolithic monument constructed by incoming settlers although few of the 100 or so examples have been given a radiocarbon dating. An example at Monamore on Arran has been dated to 3160 BC, although it was almost certainly built earlier than that, possibly c. 4000BC. There are also numerous standing stones dating from prehistoric times, including six stone circles on Machrie Moor, Arran and other examples on Great Cumbrae and Bute.\n\nBronze Age settlers also constructed megaliths at various sites, many of them dating from the second millennium BC, although the chambered cairns were replaced by burial cists, found on for example, Inchmarnock. Settlement evidence, especially from the early part of this era is however poor. The Queen of the Inch necklace is an article of jewellery made of jet found on Bute that dates from circa 2000 BC. During the early Iron Age Brythonic culture held sway, there being no evidence that the Roman occupation of southern Scotland extended to these islands.\n\nDuring the 2nd century AD Irish influence was at work in the region and by the 6th century the kingdom of Dál Riata was established. Unlike the P-Celtic speaking Brythons, these Gaels spoke a form of Gaelic that still survives in the Hebrides. Through the efforts of Saint Ninian and others Christianity slowly supplanted Druidism. Dál Riata flourished from the time of Fergus Mór in the late fifth century until the Viking incursions that commenced in the late eighth century. Islands close to the shores of modern Ayrshire would have remained part of the Kingdom of Strathclyde during this period, whilst the main islands became part of the emerging Kingdom of Alba founded by Kenneth MacAlpin (Cináed mac Ailpín).\n\nThe Islands of the Clyde historically formed the border zone between the Norse \"Suðreyjar\" and Scotland. As such many of these islands fell under Norse hegemony between the 9th and 13th centuries.\n\nThe islands of the Clyde may well have formed the power base of Somhairle mac Giolla Brighde and his descendants by the last half of the 12th century. At about this time period, the authority of the Steward of Scotland seems to have encroached into the region; and there is reason to suspect that, by the turn of the 13th century, the islands were consumed by the expanding Stewart lordship. The western extension of Scottish authority appears to have been one of the factors behind a Norwegian invasion of the region in 1230, in which the invaders seized Rothesay Castle.\n\nIn 1263 Norwegian troops commanded by Haakon Haakonarson repeated the feat but the ensuing Battle of Largs between Scots and Norwegian forces, which took place on the shores of the Firth of Clyde, was inconclusive as a military contest. This marked an ultimately terminal weakening of Norwegian power in Scotland. Haakon retreated to Orkney, where he died in December 1263, entertained on his death bed by recitations of the sagas. Following this ill-fated expedition, all rights that the Norwegian Crown \"had of old therein\" in relation to the islands were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth.\n\nFrom the mid-thirteenth century to the present day all of the islands of the Clyde have remained part of Scotland.\n\nFrom the commencement of the early medieval period until 1387 all of these isles were part of the Diocese of Sodor and Man, based at Peel, on the Isle of Man. Thereafter, the seat of the Bishopric of the Isles was relocated to the north, firstly to Snizort on Skye and then Iona, a state of affairs which continued until the 16th century Scottish Reformation.\n\nThe century following 1750 was time of significant change. New forms of transport, industry and agriculture brought sweeping changes, and an end to traditional ways of life that had endured for centuries. The aftermath of the Battle of Culloden marked the beginning of the end for the clan system and whilst there were marked improvements in living standards for some, these transformations came at a cost for others. In the early 19th century Alexander, 10th Duke of Hamilton (1767–1852) embarked on a programme of clearances that had a devastating effect on Arran's population. Whole villages were removed and the Gaelic culture of the island dealt a terminal blow. A memorial to this early form of ethnic cleansing has been constructed on the shore at Lamlash, paid for by a Canadian descendant of the emigrants.\n\nFrom the 1850s to the late 20th century the Clyde Puffer, made famous by the \"Vital Spark\", was the workhorse of the islands, carrying all kinds of produce and products to and from the islands. The Caledonian Steam Packet Company (CSP) was formed in May 1889 to operate steamer services to and from Gourock for the Caledonian Railway and soon expanded by taking over rival steamer operators. David MacBrayne Ltd operated the Glasgow to Ardrishaig steamer service, as part of the \"Royal Route\" to Oban. During the 20th century many of the islands were developed as tourist resorts for Glaswegians who went \"Doon the Watter\", in parallel to mainland resorts such as Largs and Troon.\nIn 1973 CSP and MacBraynes commenced joint Clyde and West Highland operations under the new name of Caledonian MacBrayne. A publicly owned company, they serve Great Cumbrae, Arran and Bute as well as running mainland-to-mainland ferries across the firth. Private companies operate services from Arran to Holy Isle and from McInroy's Point (Gourock) to Hunter's Quay on the Cowal peninsula.\n\nThe majority of the islands at one time made up the traditional County of Bute. Today the islands are split more or less equally between the modern unitary authorities of Argyll and Bute and North Ayrshire with only Ailsa Craig and Lady Isle in South Ayrshire falling outwith these two council areas.\n\nThe following table gives a list of the islands of the Firth of Clyde with an area greater than 40 hectares (approximately 100 acres) plus adjacent smaller uninhabited islets, tidal islets only separated at higher stages of the tide, and skerries which are only exposed at lower stages of the tide.\n\nSix islands were inhabited in 2001 including Davaar and Sanda with 2 and 1 residents respectively. By the time of the 2011 census neither had a usually resident population.\n\nSome islets lie remote from the larger islands and are listed separately here by location.\n\nGare Loch is a small loch which hosts the Faslane Naval Base, the home of the UK's Trident nuclear submarines. At its southern end, the loch opens into the Firth of Clyde, via the Rhu narrows. It contains two islets: Green Island and Perch Rock.\n\nThe Kilbrannan Sound, which lies between Arran and the Kintyre peninsula, contains several islets: An Struthlag, Cour Island, Eilean Carrach (Carradale), Eilean Carrach (Skipness), Eilean Grianain, Eilean Sunadale, Gull Isle, Island Ross and Thorn Isle. In the late 11th century Magnus Barefoot, King of Norway, made an arrangement with King Malcolm III of Scotland that he could take possession of land on the west coast around which a ship could sail. He had his longship dragged across the long isthmus in the north of Kintyre between East Loch Tarbert and West Loch Tarbert as part of a campaign to increase his possessions. Magnus declared that Kintyre had \"better land than the best of the Hebrides\", and by taking command of his ship's tiller and \"sailing\" across the isthmus he was able to claim the entire peninsula was an island, which remained under Norse rule for more than a dozen years as a result.\n\nLoch Fyne, which extends inland from the Sound of Bute is the longest of Scotland's sea lochs and contains several islets and skerries. These are Duncuan Island, Eilean Ardgaddan, Eilean a' Bhuic, Eilean Aoghainn, Eilean a' Chomhraig, Eilean an Dúnain, Eilean Buidhe (Ardmarnock), Eilean Buidhe (Portavadie), Eilean Fraoch, Eilean Math-ghamhna, Eilean Mór, Glas Eilean, Heather Island, Inverneil Island, Kilbride Island and Liath Eilean.\n\nThe North Ayrshire islets of Broad Rock, East Islet, Halftide Rock, High Rock and North Islet are all found surrounding Horse Isle. Lady Isle, which lies off the South Ayrshire coast near Troon once housed \"ane old chapell with an excellent spring of water\". However, in June 1821 someone set fire to the \"turf and pasture\", and permanently destroyed the island's grazing, with gales blowing much of the island's soil into the sea.\n\nNeither Loch Goil nor Loch Long, which are fjord-like arms of the firth to the north, contain islands.\n\nThe following are places along that shores of the Firth of Clyde that are not islands and have misleading names, \"eilean\" being Gaelic for \"island\": Eilean na Beithe, Portavadie; Eilean Beag, Cove; Eilean Dubh, Dalchenna, Loch Fyne; Eilean nan Gabhar, Melldalloch, Kyles of Bute; Barmore Island, just north of Tarbert, Kintyre; Eilean Aoidh, south of Portavadie; Eilean Leathan, Kilbrannan Sound just south of Torrisdale Bay; Island Muller, Kilbrannan Sound north of Campbeltown.\n\nThere are populations of red deer, red squirrel, badger, otter, adder and common lizard. Offshore there are harbour porpoises, basking sharks and various species of dolphin. Davaar is home to a population of wild goats.\n\nOver 200 species of bird have been recorded in the area including black guillemot, eider, peregrine falcon and the golden eagle. In 1981 there were 28 ptarmigan on Arran, but in 2009 it was reported that extensive surveys had been unable to record any. Similarly, the red-billed chough no longer breeds on the island.\n\nArran also has three rare endemic species of tree, the Arran Whitebeams. These are the Scottish or Arran whitebeam, the cut-leaved whitebeam and the Catacol whitebeam, which are amongst the most endangered tree species in the world. They are found in a protected national nature reserve, and are monitored by staff from Scottish Natural Heritage. Only 283 Arran whitebeam and 236 cut-leaved whitebeam were recorded as mature trees in 1980. The Catacol whitebeam was discovered in 2007 and steps have been taken to protect the two known specimens.\n\nThe Roman historian Tacitus refers to the \"Clota\" meaning the Clyde. The derivation is not certain but probably from the Brythonic \"Clouta\" which became \"Clut\" in Old Welsh. The name's literal meaning is \"wash\" but probably refers to the idea of a river goddess being \"the washer\" or \"strongly flowing one\". Bute's derivation is also uncertain. \"Bót\" is the Norse name and this is the Old Irish word for \"fire\", possibly a reference to signal fires. The etymology of Arran is no more clear—Haswell-Smith (2004) offers a Brythonic derivation and a meaning of \"high place\" although Watson (1926) suggests it may be pre-Celtic.\n\n\n"}
{"id": "15253", "url": "https://en.wikipedia.org/wiki?curid=15253", "title": "International Bank Account Number", "text": "International Bank Account Number\n\nThe International Bank Account Number (IBAN) is an internationally agreed system of identifying bank accounts across national borders to facilitate the communication and processing of cross border transactions with a reduced risk of transcription errors. It was originally adopted by the European Committee for Banking Standards (ECBS), and later as an international standard under ISO 13616:1997. The current standard is ISO 13616:2007, which indicates SWIFT as the formal registrar. Initially developed to facilitate payments within the European Union, it has been implemented by most European countries and numerous countries in the other parts of the world, mainly in the Middle East and in the Caribbean. As of February 2016, 69 countries were using the IBAN numbering system.\n\nThe IBAN consists of up to 34 alphanumeric characters comprising: a country code; two check digits; and a number that includes the domestic bank account number, branch identifier, and potential routing information. The check digits enable a check of the bank account number to confirm its integrity before submitting a transaction.\n\nBefore IBAN, differing national standards for bank account identification (i.e. bank, branch, routing codes, and account number) were confusing for some users. This often led to necessary routing information being missing from payments. Routing information as specified by ISO 9362 (also known as Business Identifier Codes (BIC code), SWIFT ID or SWIFT code, and SWIFT-BIC) does not require a specific format for the transaction so the identification of accounts and transaction types is left to agreements of the transaction partners. It also does not contain check digits, so errors of transcription were not detectable and it was not possible for a sending bank to validate the routing information prior to submitting the payment. Routing errors caused delayed payments and incurred extra costs to the sending and receiving banks and often to intermediate routing banks.\n\nIn 1997, to overcome these difficulties, the International Organization for Standardization (ISO) published ISO 13616:1997. This proposal had a degree of flexibility, which the European Committee for Banking Standards (ECBS) believed would make it unworkable, and they produced a \"slimmed down\" version of the standard which, amongst other things, permitted only upper-case letters and required that the IBAN for each country have a fixed length. ISO 13616:1997 was subsequently withdrawn and replaced by ISO 13616:2003. The standard was revised again in 2007 when it was split into two parts. ISO 13616-1:2007 \"specifies the elements of an international bank account number (IBAN) used to facilitate the processing of data internationally in data interchange, in financial environments as well as within and between other industries\" but \"does not specify internal procedures, file organization techniques, storage media, languages, etc. to be used in its implementation\". ISO 13616-2:2007 describes \"the Registration Authority (RA) responsible for the registry of IBAN formats that are compliant with ISO 13616-1 [and] the procedures for registering ISO 13616-compliant IBAN formats\". The official IBAN registrar under ISO 13616-2:2007 is SWIFT.\n\nIBAN imposes a flexible but regular format sufficient for account identification and contains validation information to avoid errors of transcription. It carries all the routing information needed to get a payment from one bank to another wherever it may be; it contains key bank account details such as country code, branch codes (known as sort codes in the UK and Ireland) and account numbers, and it contains \"check digits\" which can be validated at source according to a single standard procedure. Where used, IBANs have reduced trans-national money transfer errors to under 0.1% of total payments.\n\nThe IBAN consists of up to 34 alphanumeric characters, as follows:\n\nThe check digits enable a sanity check of the bank account number to confirm its integrity before submitting a transaction.\n\nThe IBAN should not contain spaces when transmitted electronically. When printed it is expressed in groups of four characters separated by a single space, the last group being of variable length as shown in the example below:\n\nPermitted IBAN characters are the digits \"0\" to \"9\" and the 26 Latin alphabetic characters \"A\" to \"Z\". This applies even in countries (e.g., Thailand) where these characters are not used in the national language.\n\nThe Basic Bank Account Number (BBAN) format is decided by the national central bank or designated payment authority of each country. There is no consistency between the formats adopted. The national authority may register its BBAN format with SWIFT, but is not obliged to do so. It may adopt IBAN without registration. SWIFT also acts as the registration authority for the SWIFT system, which is used by most countries that have not adopted IBAN. A major difference between the two systems is that under SWIFT there is no requirement that BBANs used within a country be of a pre-defined length.\n\nThe BBAN must be of a fixed length for the country and comprise case-insensitive alphanumeric characters. It includes the domestic bank account number, branch identifier, and potential routing information. Each country can have a different national routing/account numbering system, up to a maximum of 30 alphanumeric characters.\n\nThe check digits enable the sending bank (or its customer) to perform a sanity check of the routing destination and account number from a single string of data at the time of data entry. This check is guaranteed to detect any instances where a single character has been omitted, duplicated, mistyped or where two characters have been transposed. Thus routing and account number errors are virtually eliminated.\n\nOne of the design aims of the IBAN was to enable as much validation as possible to be done at the point of data entry. In particular, the computer program that accepts an IBAN will be able to validate:\n\nThe check digits are calculated using MOD-97-10 as per ISO/IEC 7064:2003 (abbreviated to \"mod-97\" in this article), which specifies a set of check character systems capable of protecting strings against errors which occur when people copy or key data. In particular, the standard states that the following can be detected:\n\nThe underlying rules for IBANs is that the account-servicing financial institution should issue an IBAN, as there are a number of areas where different IBANs could be generated from the same account and branch numbers that would satisfy the generic IBAN validation rules. In particular cases where 00 is a valid check digit, 97 will not be a valid check digit, likewise, if 01 is a valid check digit, 98 will not be a valid check digit, similarly with 02 and 99.\n\nThe UN CEFACT TBG5 has published a free IBAN validation service in 32 languages for all 57 countries that have adopted the IBAN standard. They have also published the Javascript source code of the verification algorithm.\n\nAn English language IBAN checker for ECBS member country bank accounts is available on its website.\n\nAn IBAN is validated by converting it into an integer and performing a basic \"mod-97\" operation (as described in ISO 7064) on it. If the IBAN is valid, the remainder equals 1. The algorithm of IBAN validation is as follows:\n\n\nIf the remainder is 1, the check digit test is passed and the IBAN might be valid.\n\nExample (fictitious United Kingdom bank, sort code 12-34-56, account number 98765432):\n\nAccording to the ECBS \"generation of the IBAN shall be the exclusive responsibility of the bank/branch servicing the account\". The ECBS document replicates part of the ISO/IEC 7064:2003 standard as a method for generating check digits in the range 02 to 98. Check digits in the ranges 00 to 96, 01 to 97, and 03 to 99 will also provide validation of an IBAN, but the standard is silent as to whether or not these ranges may be used.\n\nThe preferred algorithm is:\n\n\nAny computer programming language or software package that is used to compute \"D\" mod \"97\" directly must have the ability to handle integers of more than 30 digits. In practice, this can only be done by software that either supports arbitrary-precision arithmetic or that can handle 220 bit (unsigned) integers, features that are often not standard. If the application software in use does not provide the ability to handle integers of this size, the modulo operation can be performed in a piece-wise manner (as is the case with the UN CEFACT TBG5 Javascript program).\n\nPiece-wise calculation can be done in many ways. One such way is as follows:\n\n\nThe result of the final calculation in step 2 will be \"D\" mod 97 = \"N\" mod \"97\".\n\nIn this example, the above algorithm for \"D\" mod 97 will be applied to \"D\" = 3214282912345698765432161182. (The digits are colour-coded to aid the description below.) If the result is one, the IBAN corresponding to \"D\" passes the check digit test.\n\n\nFrom step 8, the final result is \"D\" mod 97 = 1 and the IBAN has passed this check digit test.\n\nInternational bank transactions use either an IBAN or the ISO 9362 Business Identifier Code system (BIC or SWIFT code) in conjunction with the BBAN (Basic Bank Account Number).\n\nThe banks of most countries in Europe publish account numbers using both the IBAN format and the nationally recognised identifiers, this being mandatory within the European Economic Area.\n\nDay-to-day administration of banking in British Overseas Territories varies from territory to territory; some, such as South Georgia and the South Sandwich Islands, have too small a population to warrant a banking system while others, such as Bermuda, have a thriving financial sector. The use of the IBAN is up to the local government—Gibraltar, being part of the European Union is required to use the IBAN, as are the Crown dependencies, which use the British clearing system, and the British Virgin Islands have chosen to do so. , no other British Overseas Territories have chosen to use the IBAN. Banks in the Caribbean Netherlands also do not use the IBAN.\n\nThe IBAN designation scheme was chosen as the foundation for electronic straight-through processing in the European Economic Area. The European Parliament mandated that a bank charge needs to be the same amount for domestic credit transfers as for cross-border credit transfers regulated in decision 2560/2001 (updated in 924/2009). This regulation took effect in 2003. Only payments in euro up to €12,500 to a bank account designated by its IBAN were covered by the regulation.\n\nThe Euro Payments regulation has been the foundation for the decision to create a Single Euro Payments Area (SEPA). The European Central Bank has created the TARGET2 interbank network that unifies the technical infrastructure of the 26 central banks of the European Union (although Sweden and the UK have opted out). SEPA is a self-regulatory initiative by the banking sector of Europe as represented in the European Payments Council (EPC). The European Union made the scheme mandatory through the Payment Services Directive published in 2007. Since January 2008, all countries must support SEPA credit transfer, and SEPA direct debit must be supported since November 2009. The regulation on SEPA payments increases the charge cap (same price for domestic payments as for cross-border payments) to €50,000.\n\nWith a further decision of the European Parliament, the IBAN scheme for bank accounts fully replaced the domestic numbering schemes from 31 December 2012. On 16 December 2010, the European Commission published proposed regulations that will make IBAN support mandatory for domestic credit transfer by 2013 and for domestic direct debit by 2014 (with a 12 and 24 months transition period respectively). Some countries have already replaced their traditional bank account scheme by IBAN. This includes Switzerland where IBAN was introduced for national credit transfer on 1 January 2006 and the support for the old bank account numbers has not been required from 1 January 2010.\n\nBased on a 20 December 2011 memorandum, the EU parliament resolved the mandatory dates for the adoption of the IBAN on 14 February 2012. From 1 February 2014, all national systems for credit transfer and direct debit must be abolished and replaced by an IBAN-based system. This will be extended to all cross-border SEPA transactions from 1 February 2016 (Article 5 Section 7). After these dates the IBAN will be sufficient to identify an account for home and foreign financial transactions in SEPA countries and banks will no longer be permitted to require that the customer supply the BIC for the beneficiary's bank.\n\nIn the run-up to the 1 February 2014 deadline, it became apparent that many old bank account numbers had not been allocated IBANs—an issue that has to be addressed on a country-by-country basis. In Germany, for example, Deutsche Bundesbank and the German Banking Industry Committee require that all holders of German bank codes (\"Bankleitzahl\") publish the specifics of their IBAN generation format taking into account not only the generation of check digits but also the handling of legacy bank codes, thereby enabling third parties to generate IBANs independently of the bank. The first such catalogue was published in June 2013 as a variant of the old bank code catalog (\"Bankleitzahlendatei\").\n\nBanks in numerous non-European countries including most states of the Middle East, North Africa and the Caribbean have implemented the IBAN format for account identification. In some countries the IBAN is used on an \"ad hoc\" basis, an example being Ukraine where account numbers used for international transfers of four of the national banks have additional aliases that follow the IBAN format as a precursor to formal SWIFT registration.\n\nThe degree to which bank verifies the validity of a recipient's bank account number depends of the configuration of the transmitting bank's software—many major software packages supply bank account validation as a standard function. Some banks outside Europe may not recognize IBAN, though this is expected to diminish with time. Non-European banks usually accept IBANs for accounts in Europe, although they might not treat IBANs differently from other foreign bank account numbers. In particular, they might not check the IBAN's validity prior to sending the transfer.\n\nBanks in the United States do not use IBAN as account numbers for U.S. accounts. Any adoption of the IBAN standard by U.S. banks would likely be initiated by ANSI ASC X9, the U.S. financial services standards development organization: a working group (WGAB20) was established as an X9 subcommittee to generate an IBAN construction for U.S. bank accounts.\n\nCanadian financial institutions have not adopted IBAN and use routing numbers issued by Payments Canada for domestic transfers, and SWIFT for international transfers. There is no formal governmental or private sector regulatory requirement in Canada for the major banks to use IBAN.\n\nAustralia and New Zealand do not use IBAN. They use Bank State Branch codes for domestic transfers and SWIFT for international transfers.\n\nThis table summarises the IBAN formats by country:\n\nIn addition to the above list, Nordea has catalogued IBANs for countries listed below.\n\nIn this list\nAddition list of countries, in the process of introducing the IBAN retrieved from SWIFT partner website are listed below.\n\nIn this list\n\nThere is criticism about the length and readability of IBAN. Printed on paper the IBAN is often difficult to read. Therefore, it is popular to group the IBAN with four symbols. However, for electronic documents (e.g. PDF invoice) the copy and paste of grouped IBAN can result in errors with online banking forms. However, most modern bank institutes allow and detect the copy and paste of both grouped and ungrouped IBAN.\n\n\n"}
{"id": "15254", "url": "https://en.wikipedia.org/wiki?curid=15254", "title": "Infinitive", "text": "Infinitive\n\nInfinitive (abbreviated ) is a grammatical term referring to certain verb forms existing in many languages, most often used as non-finite verbs. As with many linguistic concepts, there is not a single definition applicable to all languages. The word is derived from Late Latin \"[modus] infinitivus\", a derivative of \"infinitus\" meaning \"unlimited\".\n\nIn traditional descriptions of English, the infinitive is the basic dictionary form of a verb when used non-finitely, with or without the particle \"to\". Thus \"to go\" is an infinitive, as is \"go\" in a sentence like \"I must go there\" (but not in \"I go there\", where it is a finite verb). The form without \"to\" is called the bare infinitive, and the form with \"to\" is called the full infinitive or \"to\"-infinitive.\n\nIn many other languages the infinitive is a single word, often with a characteristic inflective ending, like \"morir\" (\"(to) die\") in Spanish, \"manger\" (\"(to) eat\") in French, \"portare\" (\"(to) carry\") in Latin, \"lieben\" (\"(to) love\") in German, etc. However, some languages have no infinitive forms. Many Native American languages, and some languages in Africa and Australia do not have direct equivalents to infinitives or verbal nouns. Instead, they use finite verb forms in ordinary clauses or various special constructions.\n\nBeing a verb, an infinitive may take objects and other complements and modifiers to form a verb phrase (called an infinitive phrase). Like other non-finite verb forms (like participles, converbs, gerunds and gerundives), infinitives do not generally have an expressed subject; thus an infinitive verb phrase also constitutes a complete non-finite clause, called an infinitive (infinitival) clause. Such phrases or clauses may play a variety of roles within sentences, often being nouns (for example being the subject of a sentence or being a complement of another verb), and sometimes being adverbs or other types of modifier. Many verb forms known as infinitives differ from gerunds (verbal nouns) in that they do not inflect for case or occur in adpositional phrases. Instead, infinitives often originate in earlier inflectional forms of verbal nouns. Unlike finite verbs, infinitives are not usually inflected for tense, person, etc. either, although some degree of inflection sometimes occurs; for example Latin has distinct active and passive infinitives.\n\nAn \"infinitive phrase\" is a verb phrase constructed with the verb in infinitive form. This consists of the verb together with its objects and other complements and modifiers. Some examples of infinitive phrases in English are given below – these may be based on either the full infinitive (introduced by the particle \"to\") or the bare infinitive (without the particle \"to\").\n\nInfinitive phrases often have an implied grammatical subject making them effectively clauses rather than phrases. Such \"infinitive clauses\" or \"infinitival clauses\", are one of several kinds of non-finite clause. They can play various grammatical roles like a constituent of a larger clause or sentence; for example it may form a noun phrase or adverb. Infinitival clauses may be embedded within each other in complex ways, like in the sentence:\nHere the infinitival clause \"to get married\" is contained within the finite dependent clause \"that Brett Favre is going to get married\"; this in turn is contained within another infinitival clause, which is contained in the finite independent clause (the whole sentence).\n\nThe grammatical structure of an infinitival clause may differ from that of a corresponding finite clause. For example, in German, the infinitive form of the verb usually goes to the end of its clause, whereas a finite verb (in an independent clause) typically comes in second position.\n\nFollowing certain verbs or prepositions, infinitives commonly \"do\" have an expressed subject, e.g., \nAs these examples illustrate, the subject of the infinitive is in the objective case (them, him) in contrast to the nominative case that would be used with a finite verb, e.g., \"They ate their dinner.\" \nSuch accusative and infinitive constructions are present in Latin and Ancient Greek, as well as many modern languages. The unusual case for the subject of an infinitive is an example of exceptional case-marking, where the infinitive clause's role being an object of a verb or preposition (want, for) overpowers the pronoun's subjective role within the clause.\n\nIn some languages, infinitives may be marked for grammatical categories like voice, aspect, and to some extent tense. This may be done by inflection, like with the Latin perfect and passive infinitives, or by periphrasis (with the use of auxiliary verbs), like with the Latin future infinitives or the English perfect and progressive infinitives.\n\nLatin has present, perfect and future infinitives, with active and passive forms of each. For details see .\n\nEnglish has infinitive constructions that are marked (periphrastically) for aspect: perfect, progressive (continuous), or a combination of the two (perfect progressive). These can also be marked for passive voice (as can the plain infinitive):\nFurther constructions can be made with other auxiliary-like expressions, like \"(to) be going to eat\" or \"(to) be about to eat\", which have future meaning. For more examples of the above types of construction, see .\n\nPerfect infinitives are also found in other European languages that have perfect forms with auxiliaries similarly to English. For example, \"avoir mangé\" means \"(to) have eaten\" in French.\n\nRegarding English, the term \"infinitive\" is traditionally applied to the unmarked form of the verb (the \"plain form\") when it forms a non-finite verb, whether or not introduced by the particle \"to\". Hence \"sit\" and \"to sit\", as used in the following sentences, would each be considered an infinitive:\n\nThe form without \"to\" is called the \"bare infinitive\"; the form introduced by \"to\" is called the \"full infinitive\" or \"to-infinitive\".\n\nThe other non-finite verb forms in English are the gerund or present participle (the \"-ing\" form), and the past participle – these are not considered infinitives. Moreover, the unmarked form of the verb is not considered an infinitive when it is forms a finite verb: like a present indicative (\"I \"sit\" every day\"), subjunctive (\"I suggest that he \"sit\"\"), or imperative (\"\"Sit\" down!\"). (For some irregular verbs the form of the infinitive coincides additionally with that of the past tense and/or past participle, like in the case of \"put\".)\n\nCertain auxiliary verbs are defective in that they do not have infinitives (or any other non-finite forms). This applies to the modal verbs (\"can\", \"must\", etc.), as well as certain related auxiliaries like the \"had\" of \"had better\" and the \"used\" of \"used to\". (Periphrases can be employed instead in some cases, like \"(to) be able to\" for \"can\", and \"(to) have to\" for \"must\".) It also applies to the auxiliary \"do\", like used in questions, negatives and emphasis like described under \"do\"-support. (Infinitives are negated by simply preceding them with \"not\". Of course the verb \"do\" when forming a main verb can appear in the infinitive.) However, the auxiliary verbs \"have\" (used to form the perfect) and \"be\" (used to form the passive voice and continuous aspect) both commonly appear in the infinitive: \"I should have finished by now\"; \"It's thought to have been a burial site\"; \"Let him be released\"; \"I hope to be working tomorrow.\"\n\nHuddleston and Pullum's \"Cambridge Grammar of the English Language\" (2002) does not use the notion of the \"infinitive\" (\"there is no form in the English verb paradigm called 'the infinitive'\"), only that of the \"infinitival clause\", noting that English uses the same form of the verb, the \"plain form\", in infinitival clauses that it uses in imperative and present-subjunctive clauses.\n\nA matter of controversy among prescriptive grammarians and style writers has been the appropriateness of separating the two words of the \"to\"-infinitive (as in \"I expect \"to\" happily \"sit\" here\"). For details of this, see split infinitive. Opposing linguistic theories typically do not consider the \"to\"-infinitive a distinct constituent, instead regarding the scope of the particle \"to\" as an entire verb phrase; thus, \"to buy a car\" is parsed like \"<nowiki>to [buy [a car]]</nowiki>\", not like \"<nowiki>[to buy] [a car]</nowiki>\".\n\nThe bare infinitive and the \"to\"-infinitive have a variety of uses in English. The two forms are mostly in complementary distribution – certain contexts call for one, and certain contexts for the other; they are not normally interchangeable, except in occasional instances like after the verb \"help\", where either can be used.\n\nThe main uses of infinitives (or infinitive phrases) are like follows:\n\nThe infinitive is also the usual dictionary form or citation form of a verb. The form listed in dictionaries is the bare infinitive, although the \"to\"-infinitive is often used in referring to verbs or in defining other verbs: \"The word 'amble' means 'to walk slowly'\"; \"How do we conjugate the verb \"to go\"?\"\n\nFor further detail and examples of the uses of infinitives in English, see Bare infinitive and \"To\"-infinitive in the article on uses of English verb forms.\n\nThe original Proto-Germanic ending of the infinitive was \"-an\", with verbs derived from other words ending in \"-jan\" or \"-janan\".\n\nIn German it is \"-en\" (\"sagen\"), with \"-eln\" or \"-ern\" endings on a few words based on -l or -r roots (\"segeln\", \"ändern\"). The use of \"zu\" with infinitives is similar to English \"to\", but is less frequent than in English. German infinitives can form nouns, often expressing abstractions of the action, in which case they are of neuter gender: \"das Essen\" means \"the eating\", but also \"the food\".\n\nIn Dutch infinitives also end in \"-en\" (\"zeggen\" — \"to say\"), sometimes used with \"te\" similar to English \"to\", e.g., \"Het is niet moeilijk te begrijpen\" → \"It is not hard to understand.\" The few verbs with stems ending in \"-a\" have infinitives in -n (\"gaan\" — \"to go\", \"slaan\" — \"to hit\"). Afrikaans has lost the distinction between the infinitive and present forms of verbs, with the exception of the verbs \"wees\" (to be), which admits the present form \"is\", and the verb \"hê\" (to have), whose present form is \"het\".\n\nIn North Germanic languages the final \"-n\" was lost from the infinitive as early as 500–540 AD, reducing the suffix to \"-a\". Later it has been further reduced to \"-e\" in Danish and some Norwegian dialects (including the written majority language bokmål). In the majority of Eastern Norwegian dialects and a few bordering Western Swedish dialects the reduction to \"-e\" was only partial, leaving some infinitives in \"-a\" and others in \"-e\" (å laga vs. å kaste). In northern parts of Norway the infinitive suffix is completely lost (å lag’ vs. å kast’) or only the \"-a\" is kept (å laga vs. å kast’). The infinitives of these languages are inflected for passive voice through the addition of \"-s\" or \"-st\" to the active form. This suffix appearance in Old Norse was a contraction of \"mik\" (“me”, forming \"-mk\") or \"sik\" (reflexive pronoun, forming \"-sk\") and was originally expressing reflexive actions: (hann) \"kallar\" (“(he) calls”) + \"-sik\" (“himself”) > (hann) \"kallask\" (“(he) calls himself”). The suffixes \"-mk\" and \"-sk\" later merged to \"-s\", which evolved to \"-st\" in the western dialects. The loss or reduction of \"-a\" in active voice in Norwegian did not occur in the passive forms (\"-ast\", \"-as\"), except for some dialects that have \"-es\". The other North Germanic languages have the same vowel in both forms.\n\nThe formation of the infinitive in the Romance languages reflects that in their ancestor, Latin, almost all verbs had an infinitive ending with \"-re\" (preceded by one of various thematic vowels). For example, in Italian infinitives end in \"-are\", \"-ere\", \"-rre\" (rare), or \"-ire\" (which is still identical to the Latin forms), and in \"-arsi\", \"-ersi\", \"-rsi\", \"-irsi\" for the reflexive forms. In Spanish and Portuguese, infinitives end in \"-ar\", \"-er\", or \"-ir\" (Spanish also has reflexive forms in \"-arse\", \"-erse\", \"-irse\"), while similarly in French they typically end in \"-re\", \"-er\", \"oir\", and \"-ir\". In Romanian, both short and long-form infinitives exist; the so-called \"long infinitives\" end in \"-are, -ere, -ire\" and in modern speech are used exclusively as verbal nouns. Verbs that cannot be converted into the nominal long infinitive are very rare). The \"short infinitives\" used in verbal contexts (e.g., after an auxiliary verb) have the endings \"-a\",\"-ea\", \"-e\", and \"-i\" (basically removing the ending in \"-re\"). In Romanian, the infinitive is usually replaced by a clause containing the conjunction \"să\" plus the subjunctive mood. The only verb that is modal in common modern Romanian is the verb \"a putea\", to be able to. However, in popular speech the infinitive after \"a putea\" is also increasingly replaced by the subjunctive.\n\nIn all Romance languages, infinitives can also form nouns.\n\nLatin infinitives challenged several of the generalizations about infinitives. They did inflect for voice (\"amare\", \"to love\", \"amari\", to be loved) and for tense (\"amare\", \"to love\", \"amavisse\", \"to have loved\"), and allowed for an overt expression of the subject (\"video Socratem currere\", \"I see Socrates running\"). See .\n\nRomance languages inherited from Latin the possibility of an overt expression of the subject (as in Italian \"vedo Socrate correre\"). Moreover, the \"inflected infinitive\" (or \"personal infinitive\") found in Portuguese and Galician inflects for person and number. These, alongside Sardinian, are the only Indo-European languages that allow infinitives to take person and number endings. This helps to make infinitive clauses very common in these languages; for example, the English finite clause \"in order that you/she/we have...\" would be translated to Portuguese like \"para teres/ela ter/termos...\" (Portuguese is a null-subject language). The Portuguese personal infinitive has no proper tenses, only aspects (imperfect and perfect), but tenses can be expressed using periphrastic structures. For instance, \"even though you sing/have sung/are going to sing\" could be translated to \"apesar de cantares/teres cantado/ires cantar\".\n\nOther Romance languages (including Spanish, Romanian, Catalan, and some Italian dialects) allow uninflected infinitives to combine with overt nominative subjects. For example, Spanish \"al abrir yo los ojos\" (\"when I opened my eyes\") or \"sin yo saberlo\" (\"without my knowing about it\").\n\nIn Ancient Greek the infinitive has four tenses (present, future, aorist, perfect) and three voices (active, middle, passive). Present and perfect have the same infinitive for both middle and passive, while future and aorist have separate middle and passive forms.\n\nThematic verbs form present active infinitives by adding to the stem the thematic vowel and the infinitive ending , and contracts to , e.g., . Athematic verbs, and perfect actives and aorist passives, add the suffix instead, e.g., . In the middle and passive, the present middle infinitive ending is , e.g., and most tenses of thematic verbs add an additional between the ending and the stem, e.g., .\n\nThe infinitive \"per se\" does not exist in Modern Greek. To see this, consider the ancient Greek \"ἐθέλω γράφειν\" “I want to write”. In modern Greek this become \"θέλω να γράψω\" “I want that I write”. In modern Greek, the infinitive has thus changed form and function and is used mainly in the formation of periphrastic tense forms and not with an article or alone. Instead of the Ancient Greek infinitive system \"γράφειν, γράψειν, γράψαι, γεγραφέναι\", Modern Greek uses only the form \"γράψει\", a development of the ancient Greek aorist infinitive \"γράψαι\". This form is also invariable. The modern Greek infinitive has only two forms according to voice: for example, \"γράψει\" for the active voice and \"γραφ(τ)εί\" for the passive voice (coming from the ancient passive aorist infinitive \"γραφῆναι\").\n\nThe infinitive in Russian usually ends in \"-t’\" (ть) preceded by a thematic vowel, or \"-ti\" (ти), if not preceded by one; some verbs have a stem ending in a consonant and change the \"t\" to \"č’\", like \"*mogt’ → moč’\" (*могть → мочь) \"can\". Some other Balto-Slavic languages have the infinitive typically ending in, for example, \"-ć\" (sometimes \"-c\") in Polish, \"-t’\" in Slovak, \"-t\" (formerly \"-ti\") in Czech and Latvian (with a handful ending in -s on the latter), \"-ty\" (-ти) in Ukrainian, -ць (\"-ts\"') in Belarusian. Lithuanian infinitives end in -\"ti\", Slovenian end on -\"ti\" or -\"či\", and Croatian on -\"ti\" or -\"ći\".\n\nSerbian officially retains infinitives -\"ti\" or -\"ći\", but is more flexible than the other slavic languages in breaking the infinitive through a clause. The infinitive nevertheless remains the dictionary form. \n\nBulgarian and Macedonian have lost the infinitive altogether except in a handful of frozen expressions where it is the same as the 3rd person singular aorist form. Almost all expressions where an infinitive may be used in Bulgarian are listed here; neverthess in all cases a subordinate clause is the more usual form. For that reason, the present first-person singular conjugation is the dictionary form in Bulgarian, while Macedonian uses the third person singular form of the verb in present tense.\n\nHebrew has \"two\" infinitives, the infinitive absolute and the infinitive construct. The infinitive construct is used after prepositions and is inflected with pronominal endings to indicate its subject or object: \"bikhtōbh hassōphēr\" \"when the scribe wrote\", \"ahare lekhtō\" \"after his going\". When the infinitive construct is preceded by (\"lə-\", \"li-\", \"lā-\", \"lo-\") \"to\", it has a similar meaning to the English \"to\"-infinitive, and this is its most frequent use in Modern Hebrew. The infinitive absolute is used for verb focus and emphasis, like in \"mōth yāmūth\" (literally \"a dying he will die\"; figuratively, \"he shall indeed/surely die\"). This usage is commonplace in the Bible, but in Modern Hebrew it is restricted to high-flown literary works.\n\nNote, however, that the \"to\"-infinitive of Hebrew is not the dictionary form; that is the third person singular perfect form.\n\nThe Finnish grammatical tradition includes many non-finite forms that are generally labeled as (numbered) infinitives although many of these are functionally converbs. To form the so-called first infinitive, the strong form of the root (without consonant gradation or epenthetic 'e') is used, and these changes occur:\n\nAs such, it is inconvenient for dictionary use, because the imperative would be closer to the root word. Nevertheless, dictionaries use the first infinitive.\n\nThere are also four other infinitives, plus a \"long\" form of the first:\nNote that all of these must change to reflect vowel harmony, so the fifth infinitive (with a third-person suffix) of \"hypätä\" \"jump\" is \"hyppäämäisillään\" \"he was about to jump\", not \"*hyppäämaisillaan\".\n\nThe Seri language of northwestern Mexico has infinitival forms used in two constructions (with the verb meaning 'want' and with the verb meaning 'be able'). The infinitive is formed by adding a prefix to the stem: either \"iha-\" (plus a vowel change of certain vowel-initial stems) if the complement clause is transitive, or \"ica-\" (and no vowel change) if the complement clause is intransitive. The infinitive shows agreement in number with the controlling subject. Examples are: \"icatax ihmiimzo\" 'I want to go', where \"icatax\" is the singular infinitive of the verb 'go' (singular root is \"-atax\"), and \"icalx hamiimcajc\" 'we want to go', where \"icalx\" is the plural infinitive. Examples of the transitive infinitive: \"ihaho\" 'to see it/him/her/them' (root \"-aho\"), and \"ihacta\" 'to look at it/him/her/them' (root \"-oocta\").\n\nIn languages without an infinitive, the infinitive is translated either as a \"that\"-clause or as a verbal noun. For example, in Literary Arabic the sentence \"I want to write a book\" is translated as either \"urīdu an aktuba kitāban\" (lit. \"I want that I write a book\", with a verb in the subjunctive mood) or \"urīdu kitābata kitābin\" (lit. \"I want the writing of a book\", with the \"masdar\" or verbal noun), and in Levantine Colloquial Arabic \"biddi aktub kitāb\" (subordinate clause with verb in subjunctive).\n\nEven in languages that have infinitives, similar constructions are sometimes necessary where English would allow the infinitive. For example, in French the sentence \"I want you to come\" translates to \"Je veux que vous veniez\" (lit. \"I want that you come\", with \"come\" being in the subjunctive mood). However, \"I want to come\" is simply \"Je veux venir\", using the infinitive, just as in English. In Russian, sentences such as \"I want you to leave\" do not use an infinitive. Rather, they use the conjunction чтобы \"in order to/so that\" with the past tense form (most probably remnant of subjunctive) of the verb: \"Я хочу, чтобы вы ушли\" (literally, \"I want so that you left\").\n\n"}
{"id": "15256", "url": "https://en.wikipedia.org/wiki?curid=15256", "title": "Immaculate Conception", "text": "Immaculate Conception\n\nIn Christian theology, the Immaculate Conception is the conception of the Virgin Mary free from original sin by virtue of the merits of her son Jesus. The Catholic Church teaches that God acted upon Mary in the first moment of her conception, keeping her \"immaculate\".\n\nThe Immaculate Conception is commonly confused with the virgin birth of Jesus, the latter being, rather, the doctrine of the Incarnation. While virtually all Christians believe in the virgin birth of Jesus, it is principally Roman Catholics, along with various other Christian denominations, who believe in the doctrine of the Immaculate Conception.\n\nAlthough the belief that Mary was sinless, or conceived without original sin, has been widely held since Late Antiquity, the doctrine was not dogmatically defined in the Catholic Church until 1854 when Pope Pius IX, declared \"ex cathedra\", i.e., using papal infallibility, in his papal bull \"Ineffabilis Deus\", the Immaculate Conception to be doctrine. The Catholic Church celebrates the Solemnity of the Immaculate Conception on December 8; in many Catholic countries, it is a holy day of obligation or patronal feast, and in some a national public holiday.\n\nThe defined dogma of the Immaculate Conception states:\nThe definition concerns original sin only, and it makes no declaration about the Church's belief that the Blessed Virgin was sinless in the sense of freedom from actual or personal sin. \nThe doctrine teaches that from her conception Mary, being always free from original sin, received the sanctifying grace that would normally come with baptism after birth.\n\nThe Encyclical \"Mystici Corporis\" from Pope Pius XII (1943) in addition holds that Mary was also sinless personally, \"free from all sin, original or personal\". In this, Pius XII repeats a position already expressed by the Council of Trent, which decreed \"If anyone shall say that a man once justified can sin no more, nor lose grace, and that therefore he who falls and sins was never truly justified; or, on the contrary, that throughout his whole life he can avoid all sins even venial sins, except by a special privilege of God, as the Church holds in regard to the Blessed Virgin: let him be anathema.\"\nWhen defining the dogma in \"Ineffabilis Deus\", Pope Pius IX explicitly affirmed that Mary was redeemed in a manner more sublime. He stated that Mary, rather than being cleansed after sin, was completely prevented from contracting original sin in view of the foreseen merits of Jesus Christ, the Savior of the human race. In , Mary proclaims: \"My spirit has rejoiced in God my Saviour.\" This is referred to as Mary's pre-redemption by Christ. Since the Second Council of Orange against semi-pelagianism, the Catholic Church has taught that even had man never sinned in the Garden of Eden and was sinless, he would still require God's grace to remain sinless.\n\nThe doctrine of the immaculate conception (Mary being conceived free from original sin) is not to be confused with the virginal conception of her son Jesus. Catholics believe that Mary was conceived of both parents, traditionally known by the names of Saint Joachim and Saint Anne. In 1677, the Holy See condemned the error of Imperiali who taught that St. Anne in the conception and birth of Mary remained virgin, which had been a belief surfacing occasionally since the 4th century. The Church celebrates the Feast of the Immaculate Conception (when Mary was conceived free from original sin) on 8 December, exactly nine months before celebrating the Nativity of Mary. The feast of the Annunciation (which commemorates the virginal conception and the Incarnation of Jesus) is celebrated on 25 March, nine months before Christmas Day.\n\nA feast of the Conception of the Most Holy and All Pure Mother of God was celebrated in Syria on 8 December perhaps as early as the 5th century. The title of \"achrantos\" (spotless, immaculate, all-pure) refers to the holiness of Mary, not specifically to the holiness of her conception.\n\nMary's complete sinlessness and concomitant exemption from any taint from the first moment of her existence was a doctrine familiar to Greek theologians of Byzantium. Beginning with St. Gregory Nazianzen, his explanation of the \"purification\" of Jesus and Mary at the circumcision (Luke 2:22) prompted him to consider the primary meaning of \"purification\" in Christology (and by extension in Mariology) to refer to a perfectly sinless nature that manifested itself in glory in a moment of grace (e.g., Jesus at his Baptism). St. Gregory Nazianzen designated Mary as \"prokathartheisa\" (prepurified). Gregory likely attempted to solve the riddle of the Purification of Jesus and Mary in the Temple through considering the human natures of Jesus and Mary as equally holy and therefore both purified in this manner of grace and glory. Gregory's doctrines surrounding Mary's purification were likely related to the burgeoning commemoration of the Mother of God in and around Constantinople very close to the date of Christmas. Nazianzen's title of Mary at the Annunciation as \"prepurified\" was subsequently adopted by all theologians interested in his Mariology to justify the Byzantine equivalent of the Immaculate Conception. This is especially apparent in the Fathers St. Sophronios of Jerusalem and St. John Damascene, who will be treated below in this article at the section on Church Fathers. About the time of Damascene, the public celebration of the \"Conception of St. Ann\" (i.e., of the \"Theotokos\" in her womb) was becoming popular. After this period, the \"purification\" of the perfect natures of Jesus and Mary would not only mean moments of grace and glory at the Incarnation and Baptism and other public Byzantine liturgical feasts, but purification was eventually associated with the feast of Mary's very conception (along with her Presentation in the Temple as a toddler) by Orthodox authors of the 2nd millennium (e.g., St. Nicholas Cabasilas and Joseph Bryennius).\n\nIt is admitted that the doctrine as defined by Pius IX was not explicitly mooted before the 12th century. It is also agreed that \"no direct or categorical and stringent proof of the dogma can be brought forward from Scripture\". But it is claimed that the doctrine is implicitly contained in the teaching of the Fathers. Their expressions on the subject of the sinlessness of Mary are, it is pointed out, so ample and so absolute that they must be taken to include original sin as well as actual. Thus in the first five centuries such epithets as \"in every respect holy\", \"in all things unstained\", \"super-innocent\", and \"singularly holy\" are applied to her; she is compared to Eve before the fall, as ancestress of a redeemed people; she is \"the earth before it was accursed\". The well-known words of St. Augustine (d. 430) may be cited: \"As regards the mother of God,\" he says, \"I will not allow any question whatever of sin.\" It is true that he is here speaking directly of actual or personal sin. But his argument is that all men are sinners; that they are so through original depravity; that this original depravity may be overcome by the grace of God, and he adds that he does not know but that Mary may have had sufficient grace to overcome sin \"of every sort\" (\"omni ex parte\").\n\nAlthough the doctrine of Mary's Immaculate Conception appears only later among Latin (and particularly Frankish) theologians, it became ever more manifest among Byzantine theologians reliant on Gregory Nazianzen's Mariology in the Medieval or Byzantine East. Although hymnographers and scholars, like the Emperor Justinian I, were accustomed to call Mary \"prepurified\" in their poetic and credal statements, the first point of departure for more fully commenting on Nazianzen's meaning occurs in Sophronius of Jerusalem. In other places Sophronius explains that the Theotokos was already immaculate, when she was \"purified\" at the Annunciation and goes so far as to note that John the Baptist is literally \"holier than all 'Men' born of woman\" since Mary's surpassing holiness signifies that she was holier than even John after his sanctification in utero. Sophronius' teaching is augmented and incorporated by St. John Damascene (d. 749/750). John, besides many passages wherein he extolls the Theotokos for her purification at the Annunciation, grants her the unique honor of \"purifying the waters of baptism by touching them\". This honor was most famously and firstly attributed to Christ, especially in the legacy of Nazianzen. As such, Nazianzen's assertion of parallel holiness between the prepurified Mary and purified Jesus of the New Testament is made even more explicit in Damascene in his discourse on Mary's holiness to also imitate Christ's baptism at the Jordan. The Damascene's hymnongraphy and \"De fide Orthodoxa\" explicitly use Mary's \"pre-purification\" as a key to understanding her absolute holiness and unsullied human nature. In fact, Damascene (along with Nazianzen) serves as the source for nearly all subsequent promotion of Mary's complete holiness from her Conception by the \"all pure seed\" of Joachim and the womb \"wider than heaven\" of St. Ann.\n\nBernard of Clairvaux in the 12th century raised the question of the Immaculate Conception. A feast of the Conception of the Blessed Virgin had already begun to be celebrated in some churches of the West. St Bernard blames the canons of the metropolitan church of Lyon for instituting such a festival without the permission of the Holy See. In doing so, he takes occasion to repudiate altogether the view that the conception of Mary was sinless, calling it a \"novelty.\" Some doubt, however, whether he was using the term \"conception\" in the same sense in which it is used in the definition of Pope Pius IX. Bernard would seem to have been speaking of conception in the active sense of the mother's cooperation, for in his argument he says: \"How can there be absence of sin where there is concupiscence (\"libido\")?\" and stronger expressions follow, which could be interpreted to indicate that he was speaking of the mother and not of the child. Yet, Bernard also decries those who support the feast for trying to \"add to the glories of Mary,\" which proves he was indeed talking about Mary.\n\nSaint Thomas Aquinas rejected the Immaculate Conception, saying if the Virgin Mary had been sanctified before her conception, she would not have needed the redemption of Christ concluding that \"Blessed Virgin was sanctified after animation\".. Aquinas distinguished between first time of conception, animation and birth: with the word \"conception\" is meant the first time of life of the human body, caused by the parents'union (except for the Virgin birth of Jesus), whereas the term \"animation\" indicates the following time where a soul is believed to be created by God, and then by Him unified together with the human body. The person exists only when the body has received its own soul. Latin \"Summa theologica\" affirms:\nSaint Bonaventure (d. 1274), second only to Saint Thomas in his influence on the Christian schools of his age, hesitated to accept it for a similar reason. He believed that Mary was completely free from sin, but that she was not given this grace at the instant of her conception.\n\nThe celebrated John Duns Scotus (d. 1308), a Friar Minor like Saint Bonaventure, argued, on the contrary, that from a rational point of view it was certainly as little derogatory to the merits of Christ to assert that Mary was by him preserved from all taint of sin, as to say that she first contracted it and then was delivered. Proposing a solution to the theological problem of reconciling the doctrine with that of universal redemption in Christ, he argued that Mary's immaculate conception did not remove her from redemption by Christ; rather it was the result of a more perfect redemption granted her because of her special role in salvation history.\n\nThe arguments of Scotus, combined with a better acquaintance with the language of the early Fathers, gradually prevailed in the schools of the Western Church. In 1387 the university of Paris strongly condemned the opposite view.\n\nScotus's arguments remained controversial, however, particularly among the Dominicans, who were willing enough to celebrate Mary's \"sanctificatio\" (being made free from sin) but, following the Dominican Thomas Aquinas' arguments, continued to insist that her sanctification could not have occurred until after her conception.\n\nPopular opinion remained firmly behind the celebration of Mary's conception. In 1439, the Council of Basel, which is not reckoned an ecumenical council, stated that belief in the immaculate conception of Mary is in accord with the Catholic faith. By the end of the 15th century the belief was widely professed and taught in many theological faculties, but such was the influence of the Dominicans, and the weight of the arguments of Thomas Aquinas (who had been canonised in 1323 and declared \"Doctor Angelicus\" of the Church in 1567) that the Council of Trent (1545–1563)—which might have been expected to affirm the doctrine—instead declined to take a position.\n\nThe papal bull defining the dogma, \"Ineffabilis Deus\" (1854), mentioned in particular the patrististic interpretation of as referring to a woman, Mary, who would be eternally at enmity with the evil serpent and completely triumphing over him. It said the Fathers saw foreshadowings of Mary's \"wondrous abundance of divine gifts and original innocence\" \"in that ark of Noah, which was built by divine command and escaped entirely safe and sound from the common shipwreck of the whole world; in the ladder which Jacob saw reaching from the earth to heaven, by whose rungs the angels of God ascended and descended, and on whose top the Lord himself leaned; in that bush which Moses saw in the holy place burning on all sides, which was not consumed or injured in any way but grew green and blossomed beautifully; in that impregnable tower before the enemy, from which hung a thousand bucklers and all the armor of the strong; in that garden enclosed on all sides, which cannot be violated or corrupted by any deceitful plots; in that resplendent city of God, which has its foundations on the holy mountains; in that most august temple of God, which, radiant with divine splendours, is full of the glory of God; and in very many other biblical types of this kind.\"\n\nThe bull recounts that the Fathers interpreted the angel's address to Mary, \"highly favoured one\" or \"full of grace\", as indicating that \"she was never subject to the curse and was, together with her Son, the only partaker of perpetual benediction\"; they \"frequently compare her to Eve while yet a virgin, while yet innocence, while yet incorrupt, while not yet deceived by the deadly snares of the most treacherous serpent\".\n\nThe theological underpinnings of Immaculate Conception had been the subject of debate during the Middle Ages with opposition provided by figures such as Saint Thomas Aquinas, a Dominican. However, supportive arguments by Franciscans William of Ware and Duns Scotus, and general belief among Catholics made the doctrine more acceptable, so that the Council of Basel supported it in the 15th century, but the Council of Trent sidestepped the question. Pope Sixtus IV, a Franciscan, had tried to pacify the situation by forbidding either side from criticizing each other, and placed the feast of the Immaculate Conception on the Roman Calendar in 1477, but Pope Pius V, a Dominican, changed it to the feast of the Conception of Mary. Clement XI made the feast universal in 1708, but still did not call it the feast of the Immaculate Conception. Popular and theological support for the concept continued to grow and by the 18th century it was widely depicted in art.\n\nDuring the reign of Pope Gregory XVI the bishops in various countries began to press for a definition as dogma of the teaching of Mary's immaculate conception.\n\nIn 1839, Mariano Spada (1796–1872), professor of theology at the Roman College of Saint Thomas, published \"Esame Critico sulla dottrina dell' Angelico Dottore S. Tommaso di Aquino circa il Peccato originale, relativamente alla Beatissima Vergine Maria\" [\"A critical examination of the doctrine of St. Thomas Aquinas, the Angelic Doctor, regarding original sin with respect to the Most Blessed Virgin Mary\"], in which Aquinas is interpreted not as treating the question of the Immaculate Conception later formulated in the papal bull \"Ineffabilis Deus\" but rather the sanctification of Mary within the womb of Saint Anne. Spada furnished an interpretation whereby Pius IX was relieved of the problem of seeming to foster a doctrine not in agreement with the Aquinas' teaching. Pope Pius IX would later appoint Spada Master of the Sacred Palace in 1867.\n\nPius IX, at the beginning of his pontificate, and again after 1851, appointed commissions to investigate the whole subject, and he was advised that the doctrine was one which could be defined and that the time for a definition was opportune.\n\nIt was not until 1854 that Pope Pius IX, with the support of the overwhelming majority of Roman Catholic bishops, whom he had consulted between 1851–1853, promulgated the papal bull \"Ineffabilis Deus\" (Latin for \"Ineffable God\"), which defined \"ex cathedra\" the dogma of the Immaculate Conception:\n\nThe dogma was defined in accordance with the conditions of papal infallibility, which would be defined in 1870 by the First Vatican Council.\n\nThe papal definition of the dogma declares with absolute certainty and authority that Mary possessed sanctifying grace from the first instant of her existence and was free from the lack of grace caused by the original sin at the beginning of human history. Mary's salvation was won by her son Jesus Christ through his passion, death, and resurrection and was not due to her own merits.\n\nGeorge Sale in 1734 proposed that the doctrine of immaculate conception of Mary may be alluded to in the text of the Qur'an. \nThus, commenting in 1734 on the passage \"I have called her Mary; and I commend her to thy protection, and also her issue, against Satan driven away with stones\", Sale stated: \"It is not improbable that the pretended immaculate conception of the virgin Mary is intimated in this passage. For according to a tradition of Mohammed, every person that comes into the world, is touched at his birth by the devil, and therefore cries out, Mary and her son only excepted; between whom, and the evil spirit God placed a veil, so that his touch did not reach them. And for this reason they say, neither of them were guilty of any sin, like the rest of the children of Adam.\"\nEdward Gibbon in volume 5 of his \"Decline and Fall of the Roman Empire\", published in 1788, wrote: \"The Latin Church has not disdained to borrow from the Koran the immaculate conception of his virgin mother.\" That he was speaking of her immaculate conception by her mother, not of her own virginal conception of Jesus, is shown by his footnote: \"In the xiith century the immaculate conception was condemned by St. Bernard as a presumptuous novelty.\" \nIn the aftermath of the definition of the dogma in 1854, this charge was repeated in an article published in 1865: \"Strange as it may appear, that the doctrine which the church of Rome has promulgated, with so much pomp and ceremony, 'for the destruction of all heresies, and the confirmation of the faith of her adherents', should have its origin in the Mohametan Bible; yet the testimony of such authorities as Gibbon, and Sale, and Forster, and Gagnier, and Maracci, leave no doubt as to the marvellous fact.\"\n\nWithout making Islamic belief the origin of the doctrine defined in 1854, a similarity between the two has been noted also by Roman Catholic writers such as Thomas Patrick Hughes, William Bernard Ullathorne, and Giancarlo Finazzo.\n\n\"The English Commentary of the Holy Quran\" argues that this interpretation is misleading because Islam does not embrace the concept of original sin to begin with, so Mary could not have been exempt from it.\n\nMoreover, Hannah's prayer in the Quran for her child to remain protected from Satan (Shayṭān) was said \"after\" it had already been born, not before and expresses a natural concern any righteous parent would have.\n\nA \"hadith\" nevertheless states that the only children born without the \"touch of Satan\" were Mary and Jesus- \nThe specific mention of Mary and Jesus in this hadith has been argued as taken to \"represent a class of people\", in keeping with the Arabic language and the Quranic verse \"[O Satan,] surely thou shalt have no power over My servants, except such of the erring ones as choose to follow thee\" (15:42).\n\nFor the Roman Catholic Church the dogma of the Immaculate Conception gained additional significance from the reputed apparitions of Our Lady of Lourdes in 1858. At Lourdes a 14-year-old girl, Bernadette Soubirous, claimed that a beautiful woman appeared to her and said, \"I am the Immaculate Conception\". Many believe the woman to have been the Blessed Virgin Mary and pray to her as such.\n\nPope Pius IX defined the dogma of the Immaculate Conception \"not so much because of proofs in Scripture or ancient tradition, but due to a profound \"sensus fidelium\" and the Magisterium\".\n\nSpeaking of the witness of the Church Fathers in claiming for Mary titles such as \"Free from all contagion of sin\", Pope Pius XII wrote:\nThe Roman Catholic tradition has a well-established philosophy for the study of the Immaculate Conception and the veneration of the Blessed Virgin Mary in the field of Mariology, with Pontifical schools such as the Marianum specifically devoted to this.\n\nAccording to Bernard Ullathorne, a 19th-century English Roman Catholic prelate, \"the expressions – The Immaculate Conception – The Immaculate Preservation – The Immunity – and Exception from original sin, are all phrases which bear the same signification, and are used equally to express one and the same mystery.\"\n\nA number of countries are considered to be under the patronage of the Immaculate Conception by pontifical decree. These include Argentina, Brazil, Korea, Nicaragua, Paraguay, the Philippines, Spain (including the old kingdoms and the present state), the United States and Uruguay. By royal decree under the House of Bragança, she is the principal Patroness of Portugal.\n\nBy 750, the feast of her conception (December 8) was widely celebrated in the Byzantine East, under the name of the Conception (active) of Saint Anne. In the West it was known as the feast of the Conception (passive) of Mary, and was associated particularly with the Normans, whether these introduced it directly from the East or took it from English usage. The spread of the feast, by now with the adjective \"Immaculate\" attached to its title, met opposition on the part of some, on the grounds that sanctification was possible only after conception. Critics included Saints Bernard of Clairvaux, Albertus Magnus and Thomas Aquinas. Other theologians defended the expression \"Immaculate Conception\", pointing out that sanctification could be conferred at the first moment of conception in view of the foreseen merits of Christ, a view held especially by Franciscans.\n\nWilliam of Ware and Blessed John Duns Scotus pointed out that Mary’s Immaculate Conception enhances Jesus’ redemptive work. One of the chief proponents of the doctrine was the Hungarian Franciscan Pelbartus Ladislaus of Temesvár.\nOn 28 February 1476, Pope Sixtus IV, authorized those dioceses that wished to introduce the feast to do so, and introduced it to his own diocese of Rome in 1477, with a specially composed Mass and Office of the feast. With his bull \"Cum praeexcelsa\" of 28 February 1477, in which he referred to the feast as that of the Conception of Mary, without using the word \"Immaculate\", he granted indulgences to those who would participate in the specially composed Mass or Office on the feast itself or during its octave, and he used the word \"immaculate\" of Mary, but applied instead the adjective \"miraculous\" to her conception. On 4 September 1483, referring to the feast as that of \"the Conception of Immaculate Mary ever Virgin\", he condemned both those who called it mortally sinful and heretical to hold that the \"glorious and immaculate mother of God was conceived \"without\" the stain of original sin\" and those who called it mortally sinful and heretical to hold that \"the glorious Virgin Mary was conceived \"with\" original sin\", since, he said, \"up to this time there has been no decision made by the Roman Church and the Apostolic See.\" This decree was reaffirmed by the Council of Trent.\n\nPope Pius V, while including the feast in the Tridentine Calendar, removed the adjective \"Immaculate\" and suppressed the existing special Mass for the feast, directing that the Mass for the Nativity of Mary (with the word \"Nativity\" replaced by \"Conception\") be used instead. Part of that earlier Mass was revived in the Mass that Pope Pius IX ordered to be used on the feast and that is still in use.\n\nOn 6 December 1708, Pope Clement XI made the feast of the Conception of Mary, at that time still with the Nativity of Mary formula for the Mass, a Holy Day of Obligation. Until Pope Pius X reduced in 1911 the number of Holy Days of Obligation to 8, there were in the course of the year 36 such days, apart from Sundays. Writers such as Sarah Jane Boss interpret the existence of the feast as a strong indication of the Church's traditional belief in the Immaculate Conception.\n\nFor differing reasons, belief in Mary's immaculate conception in the Catholic doctrinal form is not part of the official doctrines of the Eastern Orthodox, Oriental Orthodox, Anglican and Protestant churches.\n\nContemporary Eastern Orthodox Christians often object to the dogmatic declaration of her immaculate conception as an \"over-elaboration\" of the faith and because they see it as too closely connected with a particular interpretation of the doctrine of ancestral sin. All the same, the historical and authentic tradition of Mariology in Byzantium took its historical point of departure from Sophronios, Damascene, and their imitators. The most famous Eastern Orthodox theologian to imply Mary's Immaculate Conception was St. Gregory Palamas. Though many passages from his works were long known to extol and attribute to Mary a Christlike holiness in her human nature, traditional objections to Palamas' disposition toward the Immaculate Conception typically rely on a poor understanding of his doctrine of \"the purification of Mary\" at the Annunciation. Not only did he explicitly cite St. Gregory Nazianzen for his understanding of Jesus' purification at His baptism and Mary's at the Annunciation, but Theophanes of Nicaea, Joseph Bryennius, and Gennadios Scholarios all explicitly placed Mary's Conception as the first moment of her all-immaculate participation in the divine energies to such a degree that she was always completely without spot and graced. In addition to Emperor Manuel II and Gennadius Scholarius, St. Mark of Ephesus also fervently defended Mary's title as \"prepurified\" against the Dominican Manuel Calecas, who was perhaps promoting thomistic Mariology that denied Mary's all-holiness from the first moment of her existence.\n\nIn the tradition of Ethiopian Orthodoxy, the Kebra Nagast says:\nWhile Old Catholics do not reject the Immaculate Conception of Mary, and some of their parishes venerate Mary as immaculately conceived and celebrate the feast of her Immaculate Conception, they do not accept its definition as a dogma, since they reject papal infallibility and, with it, the Pope's authority to define dogma.\n\nMartin Luther, who initiated the Protestant Reformation, said: \"Mother Mary, like us, was born in sin of sinful parents, but the Holy Spirit covered her, sanctified and purified her so that this child was born of flesh and blood, but not with sinful flesh and blood. The Holy Spirit permitted the Virgin Mary to remain a true, natural human being of flesh and blood, just as we. However, he warded off sin from her flesh and blood so that she became the mother of a pure child, not poisoned by sin as we are. For in that moment when she conceived, she was a holy mother filled with the Holy Spirit and her fruit is a holy pure fruit, at once God and truly man, in one person.\" Some Lutherans, such as the members of the Anglo-Lutheran Catholic Church, support the doctrine.\n\nMost Protestants reject the doctrine because they do not consider the development of dogmatic theology to be authoritative apart from biblical exegesis, and because the doctrine of the Immaculate Conception is not taught in the Bible. The formal pronouncement of Mary's Immaculate Conception by the Catholic Church in 1854 further alienated some Protestant churches largely due to its implication that not all have sinned.\n\nBelief in Mary's immaculate conception is not a doctrine within Anglicanism, although it is shared by many Anglo-Catholics. In the Church of England's \"Common Worship\" prayer book, 8 December is designated a Lesser Festival of the \"Conception of the Blessed Virgin Mary\" (without the adjective \"immaculate\").\n\nThe report \"Mary: Faith and Hope in Christ\", by the Anglican-Roman Catholic International Commission, concluded that the teaching about Mary in the two definitions of the Assumption and the Immaculate Conception can be said to be consonant with the teaching of the Scriptures and the ancient common traditions. But the report expressed concerns that the Roman Catholic dogmatic definitions of these concepts implies them to be \"revealed by God\", stating: \"The question arises for Anglicans, however, as to whether these doctrines concerning Mary are revealed by God in a way which must be held by believers as a matter of faith.\"\n\nOther than Anglo-Catholics, most Anglicans reject the doctrine that Mary was sinless and conceived without original sin, often citing that it is not within the Holy Scripture and is against the redemptive role and purpose of Jesus Christ merited for all human beings.\n\nThe Roman Missal and the Roman Rite Liturgy of the Hours naturally includes references to Mary's immaculate conception in the feast of the Immaculate Conception. An example is the antiphon that begins: \"Tota pulchra es, Maria, et macula originalis non est in te\" (\"You are all beautiful, Mary, and the original stain [of sin] is not in you.\" It continues: \"Your clothing is white as snow, and your face is like the sun. You are all beautiful, Mary, and the original stain [of sin] is not in you. You are the glory of Jerusalem, you are the joy of Israel, you give honour to our people. You are all beautiful, Mary.\") On the basis of the original Gregorian chant music, polyphonic settings have been composed by Anton Bruckner, Pablo Casals, Maurice Duruflé, Grzegorz Gerwazy Gorczycki, , José Maurício Nunes Garcia, and .\n\nOther prayers honouring Mary's immaculate conception are in use outside the formal liturgy. The Immaculata prayer, composed by Saint Maximillian Kolbe, is a prayer of entrustment to Mary as the Immaculata. A novena of prayers, with a specific prayer for each of the nine days has been composed under the title of the Immaculate Conception Novena.\n\nAve Maris Stella is the vesper hymn of the feast of the Immaculate Conception. The hymn \"Immaculate Mary\", addressed to Mary as the Immaculately Conceived One, is closely associated with Lourdes.\n\nDuring the Medieval period, the conception of Mary was symbolically depicted in the Meeting at the Golden Gate and was an early scene in the many cycles of the \"Life of the Virgin\", as a counterpart of the Annunciation showing the conception of Jesus. To some medieval viewers, the kiss was a literal representation of the moment of Mary's conception, while for others it was a symbolic representation. The 14th and 15th centuries were the highpoint of these depictions. Gradually more allegorical depictions of the Immaculate Conception, featuring an adult Mary, replaced this scene in representing the doctrine. \n\nThe 1476 extension of the feast of the Immaculate Conception to the entire Latin Church reduced the likelihood of controversy for the artist or patron in depicting an image, so that emblems depicting \"The Immaculate Conception\" began to appear.\n\nMany artists in the 15th century faced the problem of how to depict an abstract idea such as the Immaculate Conception, and the problem was not fully solved for 150 years. The Italian Renaissance artist Piero di Cosimo was among those artists who tried new solutions, but none of these became generally adopted so that the subject matter would be immediately recognisable to the faithful.\n\nThe definitive iconography for the Immaculate Conception, drawing on the emblem tradition, seems to have been finally established by the master and then father-in-law of Diego Velázquez, the painter and theorist Francisco Pacheco. Pacheco's iconography influenced other Spanish artists or artists active in Spain such as El Greco, Bartolomé Murillo, Diego Velázquez, and Francisco Zurbarán, who each produced a number of artistic masterpieces based on the use of these same symbols.\n\nThe popularity of this particular representation of \"The Immaculate Conception\" spread across the rest of Europe, and has since remained the best known artistic depiction of the concept: in a heavenly realm, moments after her creation, the spirit of Mary (in the form of a young woman) looks up in awe at (or bows her head to) God. The moon is under her feet and a halo of twelve stars surround her head, possibly a reference to \"a woman clothed with the sun\" from Revelation 12:1–2. Additional imagery may include clouds, a golden light, and putti. In some paintings the putti are holding lilies and roses, flowers often associated with Mary.\n\n\n\n"}
{"id": "15260", "url": "https://en.wikipedia.org/wiki?curid=15260", "title": "Islands of the North Atlantic", "text": "Islands of the North Atlantic\n\nIONA (Islands of the North Atlantic) is an acronym suggested in 1980 by Sir John Biggs-Davison to refer to a loose linkage of England, Wales, Scotland, Ireland, the Isle of Man and Channel Islands, similar to the present day British-Irish Council. Its intended purpose was as a more politically acceptable alternative to British Isles, which is disliked by many people in Ireland.\n\nThe neologism has been criticised on the grounds that it excludes most of the islands in the North Atlantic, and also that the only island referred to by the term that is actually in the North Atlantic Ocean is Ireland. Great Britain is in fact in between the Irish Sea and The North Sea. It has been used particularly in the context of the Northern Irish peace process during the negotiation of the Good Friday Agreement, as a neutral name for the proposed council.\n\nOne feature of this name is that IONA has the same spelling as the island of Iona which is off the coast of Scotland but with which Irish people have strong cultural associations. It is therefore a name with which people of both main islands might identify. Taoiseach Bertie Ahern noted the symbolism in a 2006 address in Edinburgh:[The Island of] Iona is a powerful symbol of relationships between these islands, with its ethos of service not dominion. Iona also radiated out towards the Europe of the Dark Ages, not to mention Pagan England at Lindisfarne. The British-Irish Council is the expression of a relationship that at the origin of the Anglo-Irish process in 1981 was sometimes given the name Iona, islands of the North Atlantic, and sometimes Council of the Isles, with its evocation of the Lords of the Isles of the 14th and 15th centuries who spanned the North Channel. In the 17th century, Highland warriors and persecuted Presbyterian Ministers criss-crossed the North Channel.\nIn a Dáil Éireann debate, Proinsias De Rossa was less enthusiastic: The acronym IONA is a useful way of addressing the coming together of these two islands. However, the island of Iona is probably a green heaven in that nobody lives on it and therefore it cannot be polluted in any way.\n\nOutside the Northern Ireland peace process the term IONA is used by the World Universities Debating Championship and in inter-varsity debating competitions throughout Britain and Ireland. In this context IONA is one of the regions which appoint a representative onto the committee of the World Universities Debating Council. Greenland, the Faroe Islands and Iceland would be included in the definition of IONA used in this context, while Newfoundland and Prince Edward Island would be in North America. However, none of these islands have yet participated in the World Universities Debating Championships. Otherwise, the term has achieved very little popular usage in any context.\n\n\n"}
{"id": "15261", "url": "https://en.wikipedia.org/wiki?curid=15261", "title": "Intel DX4", "text": "Intel DX4\n\nThe IntelDX4 is a clock-tripled i486 microprocessor with 16 KB L1 cache. Intel named it DX4 (rather than \"DX3\") as a consequence of litigation with AMD over trademarks. The product was officially named the IntelDX4, but OEMs continued using the i486 naming convention.\n\nIntel produced IntelDX4s with two clock speed steppings: A 75 MHz version (3× 25 MHz multiplier), and a 100 MHz version (3× 33.3 MHz). Both chips were released in March 1994. A version of the IntelDX4 featuring write-back cache was released in October 1994. The original write-through versions of the chip are marked with a laser embossed \"&E\", while the write-back enabled versions are marked \"&EW\". i486 OverDrive editions of the IntelDX4 had locked multipliers, and therefore can only run at 3× the external clock-speed. The 100 MHz model of the processor had an iCOMP rating of 435, while the 75 MHz processor had a rating of 319. The IntelDX4 was an OEM-only product, but the DX4 Overdrive could be purchased at a retail store.\n\nThe IntelDX4 microprocessor is mostly pin-compatible with the 80486, but requires a lower 3.3 V supply. Normal 80486 and DX2 processors use a 5 V supply; plugging a DX4 into an unmodified socket will destroy it. Motherboards lacking support for the 3.3 V CPUs can sometimes make use of them using a voltage regulator (VRM) that fits between the socket and the CPU.\n"}
{"id": "15264", "url": "https://en.wikipedia.org/wiki?curid=15264", "title": "Iapetus (disambiguation)", "text": "Iapetus (disambiguation)\n\nIapetus is a Titan in Greek mythology.\nIapetus may also refer to:\n"}
{"id": "15266", "url": "https://en.wikipedia.org/wiki?curid=15266", "title": "Interactive Fiction Competition", "text": "Interactive Fiction Competition\n\nThe Interactive Fiction Competition (also known as IFComp) is one of several annual competitions for works of interactive fiction. It has been held since 1995. It is intended for fairly short games, as judges are only allowed to spend two hours playing a game before deciding how many points to award it. The competition has been described as the \"Super Bowl\" of interactive fiction.\n\nThe competition is organized by \"Stephen Granade\". Although the first competition had separate sections for Inform and TADS games, subsequent competitions have not been divided into sections and are open to games produced by any method, provided that the software used to play the game is freely available. Anyone can judge the games, and anyone can donate a prize. Almost always, there are enough prizes donated that anyone who enters will get one. Entries are required to be released as freeware or public domain, reflecting the general non-profit ethos of the IF community.\n\nIn addition to the main competition, the entries take part in the Miss Congeniality contest, where the participating authors vote for three games (not including their own). This was started in 1998 to distribute that year's surplus prizes; this additional contest has remained unchanged since then, even without the original reason for its existence.\n\nIn 2016, operation of the competition was taken over by the Interactive Fiction Technology Foundation.\n\nThe competition differs from the XYZZY Awards, as authors must specifically submit games to the Interactive Fiction Competition, but all games released in the past year are eligible for the XYZZY Awards. Many games win awards in both competitions.\n\nThe following is a list of first place winners to date:\n\nA reviewer for \"The A.V. Club\" said of the 2008 competition, \"Once again, the IF Competition delivers some of the best writing in games.\" The 2008 competition was described as containing \"some real standouts both in quality of puzzles and a willingness to stretch the definition of text adventures/interactive fiction.\"\n\n\n"}
{"id": "15267", "url": "https://en.wikipedia.org/wiki?curid=15267", "title": "Immunity", "text": "Immunity\n\nImmunity may refer to:\n\n\n\n"}
{"id": "15268", "url": "https://en.wikipedia.org/wiki?curid=15268", "title": "Inquests in England and Wales", "text": "Inquests in England and Wales\n\nInquests in England and Wales are held into sudden and unexplained deaths and also into the circumstances of discovery of a certain class of valuable artefacts known as \"treasure trove\". In England and Wales, inquests are the responsibility of a coroner, who operates under the jurisdiction of the Coroners and Justice Act 2009.\n\nThere is a general duty upon every person to report a death to the coroner if an inquest is likely to be required. However, this duty is largely unenforceable in practice and the duty falls on the responsible registrar. The registrar must report a death where:\n\nThe coroner must hold an inquest where the death was:\n\nWhere the cause of death is unknown, the coroner may order a post mortem examination in order to determine whether the death was violent. If the death is found to be non-violent, an inquest is unnecessary.\n\nIn 2004 in England and Wales, there were 514,000 deaths of which 225,500 were referred to the coroner. Of those, 115,800 resulted in post-mortem examinations and there were 28,300 inquests, 570 with a jury. In 2014 the Royal College of Pathologists claimed that up to 10,000 deaths a year recorded as being from natural causes should have been investigated by inquests. They were particularly concerned about people whose death occurred as a result of medical errors. \"We believe a medical examiner would have been alerted to what was going on in Mid-Staffordshire long before this long list of avoidable deaths reached the total it did,\" said Archie Prentice, the pathologists' president.\n\nA coroner must summon a jury for an inquest if the death was not a result of natural causes and occurred when the deceased was in state custody (for example in prison, police custody, or whilst detained under the Mental Health Act 1983); or if it was the result of an act or omission of a police officer; or if it was a result of a notifiable accident, poisoning or disease. The senior coroner can also call a jury at his or her own discretion. This discretion has been heavily litigated in light of the Human Rights Act 1998, which means that juries are required now in a broader range of situations than expressly required by statute.\n\nThe purpose of the inquest is to answer four questions:\n\nEvidence must be solely for the purpose of answering these questions and no other evidence is admitted. It is not for the inquest to ascertain \"how the deceased died\" or \"in what broad circumstances\", but \"how the deceased came by his death\", a more limited question. Moreover, it is not the purpose of the inquest to determine, or appear to determine, criminal or civil liability, to apportion guilt or attribute blame. For example, where a prisoner hanged himself in a cell, he came by his death by hanging and it was not the role of the inquest to enquire into the broader circumstances such as the alleged neglect of the prison authorities that might have contributed to his state of mind or given him the opportunity. However, the inquest should set out as many of the facts as the public interest requires.\n\nUnder the terms of article 2 of the European Convention of Human Rights, governments are required to \"establish a framework of laws, precautions, procedures and means of enforcement which will, to the greatest extent reasonably practicable, protect life\". The European Court of Human Rights has interpreted this as mandating independent official investigation of any death where public servants may be implicated. Since the Human Rights Act 1998 came into force, in those cases alone, the inquest is now to consider the broader question \"by what means and in what circumstances\".\n\nIn disasters, such as the 1987 King's Cross fire, a single inquest may be held into several deaths. However, when several protesters were shot and killed by police in Mitchelstown in 1887, the findings of a common inquest were quashed because the killings had taken place at different times and in different places.\n\nInquests are governed by the Rules. The coroner gives notice to near relatives, those entitled to examine witnesses and those whose conduct is likely to be scrutinised. Inquests are held in public except where there are real issues of national security.\n\nIndividuals with an interest in the proceedings, such as relatives of the deceased, individuals appearing as witnesses, and organisations or individuals who may face some responsibility in the death of the individual, may be represented by lawyers at the discretion of the coroner. Witnesses may be compelled to testify subject to the privilege against self-incrimination.\n\nThe following verdicts are not mandatory but are strongly recommended:\n\nIn 2004, 37% of inquests recorded an outcome of death by accident / misadventure, 21% by natural causes, 13% suicide, 10% open verdicts, and 19% other outcomes.\n\nSince 2004 it has been possible for the coroner to record a narrative verdict, recording the circumstances of a death without apportioning blame or liability. Since 2009, other possible verdicts have included \"alcohol/drug related death\" and \"road traffic collision\". The civil standard of proof, on the balance of probabilities, is needed for most verdicts, except unlawful killing and suicide where the criminal standard of beyond reasonable doubt is required.\n\nOwing in particular to the failures to notice the serial murder committed by Harold Shipman, the Coroners and Justice Act 2009 modernised the system with:\n\n\n\n"}
{"id": "15270", "url": "https://en.wikipedia.org/wiki?curid=15270", "title": "Index", "text": "Index\n\nIndex may refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "15271", "url": "https://en.wikipedia.org/wiki?curid=15271", "title": "Information retrieval", "text": "Information retrieval\n\nInformation retrieval (IR) is the activity of obtaining information system resources relevant to an information need from a collection of information resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for metadata that describe data, and for databases of texts, images or sounds.\n\nAutomated information retrieval systems are used to reduce what has been called information overload. An IR system is a software that provide access to books, journals and other documents, stores them and manages the document. Web search engines are the most visible IR applications.\n\nAn information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.\n\nAn object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.\n\nDepending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\n\nMost IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.\n\nThe idea of using computers to search for relevant pieces of information was popularized in the article \"As We May Think\" by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\n\nIn 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.\n\nFor effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\n\n\n\nThe evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.\n\n\n\n\n\n\n"}
{"id": "15272", "url": "https://en.wikipedia.org/wiki?curid=15272", "title": "List of Italian-language poets", "text": "List of Italian-language poets\n\nList of poets who wrote in Italian (or Italian dialects).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "15274", "url": "https://en.wikipedia.org/wiki?curid=15274", "title": "International Criminal Tribunal for the former Yugoslavia", "text": "International Criminal Tribunal for the former Yugoslavia\n\nThe International Tribunal for the Prosecution of Persons Responsible for Serious Violations of International Humanitarian Law Committed in the Territory of the Former Yugoslavia since 1991, more commonly referred to as the International Criminal Tribunal for the former Yugoslavia (ICTY), was a body of the United Nations established to prosecute serious crimes committed during the Yugoslav Wars, and to try their perpetrators. The tribunal was an ad hoc court located in The Hague, Netherlands.\n\nThe Court was established by Resolution 827 of the United Nations Security Council, which was passed on 25 May 1993. It had jurisdiction over four clusters of crimes committed on the territory of the former Yugoslavia since 1991: grave breaches of the Geneva Conventions, violations of the laws or customs of war, genocide, and crimes against humanity. The maximum sentence it could impose was life imprisonment. Various countries signed agreements with the UN to carry out custodial sentences.\n\nA total of 161 persons were indicted; the final indictments were issued in December 2004, the last of which were confirmed and unsealed in the spring of 2005. The final fugitive, Goran Hadžić, was arrested on 20 July 2011. The final judgment was issued on 29 November 2017 and the institution formally ceased to exist on 31 December 2017.\n\nResidual functions of the ICTY, including oversight of sentences and consideration of any appeal proceedings initiated since 1 July 2013, are under the jurisdiction of a successor body, the Mechanism for International Criminal Tribunals.\n\nUnited Nations Security Council Resolution 808 of 22 February 1993 decided that \"an international tribunal shall be established for the prosecution of persons responsible for serious violations of international humanitarian law committed in the territory of the former Yugoslavia since 1991\", and calling on the Secretary-General to \"submit for consideration by the Council ... a report on all aspects of this matter, including specific proposals and where appropriate options ... taking into account suggestions put forward in this regard by Member States\".\n\nThe Court was originally proposed by German Foreign Minister Klaus Kinkel. By 25 May 1993, the international community had tried to pressure the leaders of the former Yugoslavian republics diplomatically, militarily, politically, economically, and – with Resolution 827 – through juridical means. Resolution 827 of 25 May 1993 approved S/25704 report of the Secretary-General and adopted the Statute of the International Tribunal annexed to it, formally creating the ICTY. It would have jurisdiction over four clusters of crime committed on the territory of the former SFR Yugoslavia since 1991: grave breaches of the Geneva Conventions, violations of the laws or customs of war, genocide, and crime against humanity. The maximum sentence it could impose was life imprisonment.\n\nIn 1993, the ICTY built its internal infrastructure. 17 states have signed an agreement with the ICTY to carry out custodial sentences.\n\n1993–1994: In the first year of its existence, the Tribunal laid the foundations for its existence as a judicial organ. The Tribunal established the legal framework for its operations by adopting the rules of procedure and evidence, as well as its rules of detention and directive for the assignment of defense counsel. Together these rules established a legal aid system for the Tribunal. As the ICTY is part of the United Nations and as it was the first \"international\" court for \"criminal\" justice, the development of a juridical infrastructure was considered quite a challenge. However after the first year the first ICTY judges had drafted and adopted all the rules for court proceedings.\n\n1994–1995: The ICTY established its offices within the Aegon Insurance Building in The Hague (which was, at the time, still partially in use by Aegon) and detention facilities in Scheveningen in The Hague (the Netherlands). The ICTY hired now many staff members. By July 1994 there were sufficient staff members in the office of the prosecutor to begin field investigations and by November 1994 the first indictment was presented and confirmed. In 1995, the entire staff numbered more than 200 persons and came from all over the world. Moreover, some governments assigned their legally trained people to the ICTY.\n\nIn 1994 the first indictment was issued against the Bosnian-Serb concentration camp commander Dragan Nikolić. This was followed on 13 February 1995 by two indictments comprising 21 individuals which were issued against a group of 21 Bosnian-Serbs charged with committing atrocities against Muslim and Croat civilian prisoners. While the war in the former Yugoslavia was still raging, the ICTY prosecutors showed that an international court was viable. However, no accused was arrested.\n\nThe court confirmed eight indictments against 46 individuals and issued arrest warrants. Bosnian Serb indictee Duško Tadić became the subject of the Tribunal's first trial. Tadić was arrested by German police in Munich in 1994 for his alleged actions in the Prijedor region in Bosnia-Herzegovina (especially his actions in the Omarska, Trnopolje and Keraterm detention camps). He made his first appearance before the ICTY Trial Chamber on 26 April 1995, and pleaded not guilty to all of the charges in the indictment.\n\n1995–1996: Between June 1995 and June 1996, 10 public indictments had been confirmed against a total of 33 individuals. Six of the newly indicted persons were transferred in the Tribunal's detention unit. In addition to Duško Tadic, by June 1996 the tribunal had Tihomir Blaškić, Dražen Erdemović, Zejnil Delalić, Zdravko Mucić, Esad Landžo and Hazim Delić in custody. Erdemović became the first person to enter a guilty plea before the tribunal's court. Between 1995 and 1996, the ICTY dealt with miscellaneous cases involving several detainees, which never reached the trial stage.\n\nIn 2004, the ICTY published a list of five accomplishments \"in justice and law\":\n\n\nThe United Nations Security Council passed resolutions 1503 in August 2003 and 1534 in March 2004, which both called for the completion of all cases at both the ICTY and its sister tribunal, the International Criminal Tribunal for Rwanda (ICTR) by 2010.\n\nIn December 2010, the Security Council adopted Resolution 1966, which established the Mechanism for International Criminal Tribunals (MICT), a body intended to gradually assume residual functions from both the ICTY and the ICTR as they wound down their mandate. Resolution 1966 called upon the Tribunal to finish its work by 31 December 2014 to prepare for its closure and transfer of its responsibilities.\n\nIn a \"Completion Strategy Report\" issued in May 2011, the ICTY indicated it aimed to complete all trials by the end of 2012 and all appeals by 2015, with the exception of Radovan Karadžić whose trial was expected to end in 2014 and Ratko Mladić and Goran Hadžić, who were at large at that time and were not arrested until later that year.\n\nThe MICT's ICTY branch began functioning on 1 July 2013. Per the Transitional Arrangements adopted by the UN Security Council, the ICTY was to conduct and complete all outstanding first instance trials, including those of Karadžić, Mladić and Hadžić. The ICTY would also conduct and complete all appeal proceedings for which the notice of appeal against the judgement or sentence was filed before 1 July 2013. The MICT will handle any appeals for which notice is filed after that date.\n\nThe final ICTY trial to be completed in the first instance was that of Ratko Mladić, who was convicted on 22 November 2017. The final case to be considered by the ICTY was an appeal proceeding encompassing six individuals, whose sentences were upheld on 29 November 2017.\n\nWhile operating, the Tribunal employed around 900 staff. Its organisational components were Chambers, Registry and the Office of the Prosecutor (OTP).\nThe Prosecutor was responsible for investigating crimes, gathering evidence and prosecutions and was head of the Office of the Prosecutor (OTP). The Prosecutor was appointed by the UN Security Council upon nomination by the UN Secretary-General.\n\nThe last prosecutor was Serge Brammertz. Previous Prosecutors have been Ramón Escovar Salom of Venezuela (1993–1994), however, he never took up that office, Richard Goldstone of South Africa (1994–1996), Louise Arbour of Canada (1996–1999), and Carla Del Ponte of Switzerland (1999–2007). Richard Goldstone, Louise Arbour and Carla Del Ponte also simultaneously served as the Prosecutor of the International Criminal Tribunal for Rwanda until 2003. Graham Blewitt [Australia] served as the Deputy Prosecutor from 1994 until 2004. David Tolbert, the President of the International Center for Transitional Justice, was also appointed Deputy Prosecutor of the ICTY in 2004.\n\nChambers encompassed the judges and their aides. The Tribunal operated three Trial Chambers and one Appeals Chamber. The President of the Tribunal was also the presiding Judge of the Appeals Chamber.\n\nAt the time of the court's dissolution, there were seven permanent judges and one \"ad hoc\" judge who served on the Tribunal. A total of 86 judges have been appointed to the Tribunal from 52 United Nations member states. Of those judges, 51 were permanent judges, 36 were \"ad litem\" judges, and one was an \"ad hoc\" judge. Note that one judge served as both a permanent and \"ad litem\" judge, and another served as both a permanent and \"ad hoc\" judge.\n\nUN member and observer states could each submit up to two nominees of different nationalities to the UN Secretary-General. The UN Secretary-General submitted this list to the UN Security Council which selected from 28 to 42 nominees and submitted these nominees to the UN General Assembly. The UN General Assembly then elected 14 judges from that list. Judges served for four years and were eligible for re-election. The UN Secretary-General appointed replacements in case of vacancy for the remainder of the term of office concerned.\n\nOn 21 October 2015, Judge Carmel Agius of Malta was elected President of the ICTY and Liu Daqun of China was elected Vice-President; they have assumed their positions on 17 November 2015. His predecessors were Antonio Cassese of Italy (1993–1997), Gabrielle Kirk McDonald of the United States (1997–1999), Claude Jorda of France (1999–2002), Theodor Meron of the United States (2002–2005), Fausto Pocar of Italy (2005–2008), Patrick Robinson of Jamaica (2008–2011), and Theodor Meron (2011–2015).\n\nThe Registry was responsible for handling the administration of the Tribunal; activities included keeping court records, translating court documents, transporting and accommodating those who appear to testify, operating the Public Information Section, and such general duties as payroll administration, personnel management and procurement. It was also responsible for the Detention Unit for indictees being held during their trial and the Legal Aid program for indictees who cannot pay for their own defence. It was headed by the Registrar, a position occupied over the years by Theo van Boven of the Netherlands (February 1994 to December 1994), Dorothée de Sampayo Garrido-Nijgh of the Netherlands (1995–2000), Hans Holthuis of the Netherlands (2001–2009), and John Hocking of Australia (May 2009 to December 2017).\n\nThose defendants on trial and those who were denied a provisional release were detained at the United Nations Detention Unit on the premises of the Penitentiary Institution Haaglanden, location Scheveningen in Belgisch Park, a suburb of The Hague, located some 3 km by road from the courthouse. The indicted were housed in private cells which had a toilet, shower, radio, satellite TV, personal computer (without internet access) and other luxuries. They were allowed to phone family and friends daily and could have conjugal visits. There was also a library, a gym and various rooms used for religious observances. The inmates were allowed to cook for themselves. All of the inmates mixed freely and were not segregated on the basis of nationality. As the cells were more akin to a university residence instead of a jail, some had derisively referred to the ICT as the \"Hague Hilton\". The reason for this luxury relative to other prisons is that the first president of the court wanted to emphasise that the indictees were innocent until proven guilty.\n\nThe Tribunal indicted 161 individuals between 1997 and 2004 and completed proceedings with them as follows:\n\nThe indictees ranged from common soldiers to generals and police commanders all the way to prime ministers. Slobodan Milošević was the first sitting head of state indicted for war crimes. Other \"high level\" indictees included Milan Babić, former President of the Republika Srpska Krajina; Ramush Haradinaj, former Prime Minister of Kosovo; Radovan Karadžić, former President of the Republika Srpska; Ratko Mladić, former Commander of the Bosnian Serb Army; and Ante Gotovina, former General of the Croatian Army.\n\nThe very first hearing at the ICTY was referral request in the Tadić case on 8 November 1994. Croat Serb General and former President of the Republic of Serbian Krajina Goran Hadžić was the last fugitive wanted by the Tribunal to be arrested on 20 July 2011.\n\nAn additional 23 individuals have been the subject of contempt proceedings.\n\nSkeptics argued that an international court could not function while the war in the former Yugoslavia was still going on. This would be a huge undertaking for any court, but for the ICTY it would be an even greater one, as the new tribunal still needed judges, a prosecutor, a registrar, investigative and support staff, an extensive interpretation and translation system, a legal aid structure, premises, equipment, courtrooms, detention facilities, guards and all the related funding.\n\nCriticisms of the court include:\n\nSupporters of the work of the ICTY responded to critics in various publications. In a response to David Harland's \"Selective Justice\", Jelena Subotić, an assistant professor of political science at Georgia State University and author of \"Hijacked Justice: Dealing with the Past in the Balkans\", responded that the critics of the Tribunal miss the point, \"which is not to deliver justice for past wrongs equally for 'all sides', fostering reconciliation, but to carefully measure each case on its own merits ... We should judge the work of the tribunal by its legal expertise, not by the political outcomes we desire.\"\n\nMarko Hoare claims the accusations of the tribunal's \"selective justice\" stem from Serbian nationalist propaganda. He wrote: \"This is, of course, the claim that hardline Serb nationalists and supporters of Slobodan Milosevic have been making for about the last two decades. Instead of carrying out any research into the actual record of the ICTY in order to support his thesis, Harland simply repeats a string of cliches of the kind that frequently appear in anti-Hague diatribes by Serb nationalists.\"\n\n\n"}
{"id": "15275", "url": "https://en.wikipedia.org/wiki?curid=15275", "title": "ISO 216", "text": "ISO 216\n\nISO 216 specifies international standard (ISO) paper sizes used in most countries in the world today, although not in Canada, the United States, Mexico, Colombia, or the Dominican Republic. The standard defines the \"A\" and \"B\" series of paper sizes, including A4, the most commonly available paper size worldwide. Two supplementary standards, ISO 217 and ISO 269, define related paper sizes; the ISO 269 \"C\" series is commonly listed alongside the A and B sizes.\n\nAll ISO 216, ISO 217 and ISO 269 paper sizes (except some envelopes) have the same aspect ratio, :1, within rounding to millimetres. This ratio has the unique property that when cut or folded in half widthways, the halves also have the same aspect ratio. Each ISO paper size is one half of the area of the next larger size in the same series.\n\nIn 1786, the German scientist Georg Christoph Lichtenberg described the advantages of basing a paper size on an aspect ratio of in a letter to Johann Beckmann. The formats that became ISO paper sizes A2, A3, B3, B4, and B5 were developed in France. They were listed in a 1798 law on taxation of publications that was based in part on page sizes.\nThe main advantage of this system is its scaling. Rectangular paper with an aspect ratio of has the unique property that, when cut or folded in half midway between its shorter sides, each half has the same aspect ratio and half the area of the whole sheet before it was divided. Equivalently, if one lays two same-sized sheets paper with an aspect ratio of side-by-side along their longer side, they form a larger rectangle with the aspect ratio of and double the area of each individual sheet.\n\nThe ISO system of paper sizes exploits these properties of the aspect ratio. In each series of sizes (for example, series A), the largest size is numbered 0 (for example, A0), and each successive size (for example, A1, A2, etc.) has half the area of the preceding sheet and can be cut by halving the length of the preceding size sheet. The new measurement is rounded down to the nearest millimetre. A folded brochure can be made by using a sheet of the next larger size (for example, an A4 sheet is folded in half to make a brochure with size A5 pages. An office photocopier or printer can be designed to reduce a page from A4 to A5 or to enlarge a page from A4 to A3. Similarly, two sheets of A4 can be scaled down to fit one A4 sheet without excess empty paper.\n\nThis system also simplifies calculating the weight of paper. Under ISO 536, paper's grammage is defined as a sheet's weight in grams (g) per area in square metres (abbreviated g/m or gsm). Since an A0 sheet has an area of 1 m, its weight in grams is the same as its grammage. One can derive the grammage of other sizes by arithmetic division in g/m. A standard A4 sheet made from 80 g/m paper weighs 5 g, as it is (four halvings, ignoring rounding) of an A0 page. Thus the weight, and the associated postage rate, can be easily approximated by counting the number of sheets used.\n\nISO 216 and its related standards were first published between 1975 and 1995:\n\nPaper in the A series format has an aspect ratio of (≈ 1.414, when rounded). A0 is defined so that it has an area of 1 square metre before rounding to the nearest millimeter. Successive paper sizes in the series (A1, A2, A3, etc.) are defined by halving the area of the preceding paper size and rounding down, so that the long side of A(\"n\"+1) is the same length as the short side of A\"n\". Hence, each next size is roughly half of the prior size. So, an A1 page can fit 2 A2 pages inside the same area.\n\nThe most used of this series is the size A4, which is and thus almost exactly in area. For comparison, the letter paper size commonly used in North America () is about 6 mm (0.24 in) wider and 18 mm (0.71 in) shorter than A4. Then, the size of A5 paper is half of A4, as 148 x 210 mm (5.8 x 8.3 in). \n\nThe geometric rationale for using the square root of 2 is to maintain the aspect ratio of each subsequent rectangle after cutting or folding an A-series sheet in half, perpendicular to the larger side. Given a rectangle with a longer side, , and a shorter side, , ensuring that its aspect ratio, , will be the same as that of a rectangle half its size, , which means that , which reduces to ; in other words, an aspect ratio of 1:.\n\nThe formula that gives the larger border of the paper size A\"n\" in metres and without rounding off is the geometric sequence:\nThe paper size A\"n\" thus has the dimension\nand area (before rounding)\n\nThe measurement in millimetres of the long side of A\"n\" can be calculated as\n(brackets represent the floor function).\n\nThe B series is defined in the standard as follows: \"A subsidiary series of sizes is obtained by placing the geometrical means between adjacent sizes of the A series in sequence.\" The use of the geometric mean makes each step in size: B0, A0, B1, A1, B2 … smaller than the previous one by the same factor. As with the A series, the lengths of the B series have the ratio , and folding one in half (and rounding down to the nearest millimeter) gives the next in the series. The shorter side of B0 is exactly 1 metre.\n\nThe measurement in millimetres of the long side of B\"n\" can be calculated as\n\nThere is also an incompatible Japanese B series which the JIS defines to have 1.5 times the area of the corresponding JIS A series (which is identical to the ISO A series). Thus, the lengths of JIS B series paper are ≈ 1.22 times those of A-series paper. By comparison, the lengths of ISO B series paper are ≈ 1.19 times those of A-series paper.\n\nThe C series formats are geometric means between the B series and A series formats with the same number (e.g., C2 is the geometric mean between B2 and A2). The width to height ratio is as in the A and B series. The C series formats are used mainly for envelopes. An A4 page will fit into a C4 envelope. C series envelopes follow the same ratio principle as the A series pages. For example, if an A4 page is folded in half so that it is A5 in size, it will fit into a C5 envelope (which will be the same size as a C4 envelope folded in half). The lengths of ISO C series paper are therefore ≈ 1.09 times those of A-series paper.\n\nA, B, and C paper fit together as part of a geometric progression, with ratio of successive side lengths of , though there is no size half-way between B\"n\" and A(\"n\" − 1): A4, C4, B4, \"D4\", A3, …; there is such a D-series in the Swedish extensions to the system.\n\nThe measurement in millimetres of the long side of C\"n\" can be calculated as\n\nThe tolerances specified in the standard are:\n\nThese are related to comparison between series A, B and C.\n\nThe ISO 216 formats are organized around the ratio 1:; two sheets next to each other together have the same ratio, sideways. In scaled photocopying, for example, two A4 sheets reduced to A5 size fit exactly onto one A4 sheet, and an A4 sheet in magnified size onto an A3 sheet; in each case, there is neither waste nor want.\n\nThe principal countries not generally using the ISO paper sizes are the United States and Canada, which use the Letter, Legal and Executive system. Although they have also officially adopted the ISO 216 paper format, Mexico, Panama, Venezuela, Colombia, the Philippines, and Chile also use mostly U.S. paper sizes.\n\nRectangular sheets of paper with the ratio 1: are popular in paper folding, such as origami, where they are sometimes called \"A4 rectangles\" or \"silver rectangles\". In other contexts, the term \"silver rectangle\" can also refer to a rectangle in the proportion 1:(1 + ), known as the silver ratio.\n\nAn important adjunct to the ISO paper sizes, particularly the A series, are the technical drawing line widths specified in ISO 128, and the matching technical pen widths of 0.13, 0.18, 0.25, 0.35, 0.5, 0.7, 1.0, 1.40, and 2.0 mm, as specified in . Color codes are assigned to each size to facilitate easy recognition by the drafter. These sizes increase by a factor of , so that particular pens can be used on particular sizes of paper, and then the next smaller or larger size can be used to continue the drawing after it has been reduced or enlarged, respectively. For example, a continuous thick line on A0 size paper shall be drawn with a 0.7 mm pen, the same line on A1 paper shall be drawn with a 0.5 mm pen, and finally on A2, A3, or A4 paper it shall be drawn with a 0.35 mm pen.\n\nThe earlier DIN 6775 standard upon which ISO 9175-1 is based also specified a term and symbol for easy identification of pens and drawing templates compatible with the standard, called , which may still be found on some technical drafting equipment.\n\n\n"}
{"id": "15276", "url": "https://en.wikipedia.org/wiki?curid=15276", "title": "ISO 3864", "text": "ISO 3864\n\nISO 3864 specifies international standards for safety symbols. These labels are graphical, to overcome language barriers. The standard is split into four parts:\nThese are the colors specified in ISO Standard 3864-4 in RAL colour standard.\nThe corresponding American standard is ANSI Z535, which is incompatible with the ISO standard. ANSI standard ANSI Z535.6-2006 defines an optional accompanying text in one or more languages.\n\nISO 3864 is extended by ISO 7010, which provides a set of symbols based on the principles and properties specified in ISO 3864.\n\n"}
{"id": "15281", "url": "https://en.wikipedia.org/wiki?curid=15281", "title": "Isaac Abendana", "text": "Isaac Abendana\n\nIsaac Abendana (ca. 1640 – 1699) was the younger brother of Jacob Abendana, and became \"hakam\" of the Spanish Portuguese Synagogue in London after his brother died. \n\nAbendana moved to England before his brother, in 1662, and taught Hebrew at Cambridge University. He completed an unpublished Latin translation of the \"Mishnah\" for the university in 1671. \n\nWhile he was at Cambridge, Abendana sold Hebrew books to the Bodleian Library of Oxford, and in 1689 he took a teaching position in Magdalen College. In Oxford, he wrote a series of Jewish almanacs for Christians, which he later collected and compiled as the \"Discourses on the Ecclesiastical and Civil Polity of the Jews\" (1706). Like his brother, he maintained an extensive correspondence with leading Christian scholars of his time, most notably with the philosopher Ralph Cudworth, master of Christ's College, Cambridge. \n"}
{"id": "15284", "url": "https://en.wikipedia.org/wiki?curid=15284", "title": "List of intelligence agencies", "text": "List of intelligence agencies\n\nThis is a list of intelligence agencies. It includes only currently operational institutions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWildlife Crime Control Bureau\n\n\n\nGeneral Staff of the Armed Forces of the Islamic Republic of Iran:\n\nIslamic Republic of Iran Army:\n\nIslamic Revolutionary Guard Corps:\n\nLaw Enforcement Force of the Islamic Republic of Iran:\n\nMinistry of Defence and Armed Forces Logistics:\n\nKurdistan Region\n\nForeign & Domestic Military Intelligence (Defence Forces)\nDomestic Police Intelligence (\"Garda Síochána\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the Presidents Office\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial Telecommunication Service, abbreviated STS\nServiciul de Protecţie şi Pază, abbreviated SPP\n(DGIA) – \"Direcția Generală de Informații a Apărării\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMISS (formerly DMI - Defense Military Intelligence) now Military Intelligence Security Services\n\n\nDomestic intelligence\n\nForeign intelligence\n\nSignals intelligence\n\n\n\n\n\n\n\n\n"}
{"id": "15285", "url": "https://en.wikipedia.org/wiki?curid=15285", "title": "Internet Engineering Task Force", "text": "Internet Engineering Task Force\n\nThe Internet Engineering Task Force (IETF) is an open standards organization, which develops and promotes voluntary Internet standards, in particular the standards that comprise the Internet protocol suite (TCP/IP). It has no formal membership or membership requirements. All participants and managers are volunteers, though their work is usually funded by their employers or sponsors. \n\nThe IETF started out as an activity supported by the U.S. federal government, but since 1993 it has operated as a standards development function under the auspices of the Internet Society, an international membership-based non-profit organization.\n\nThe IETF is organized into a large number of working groups and informal discussion groups (BoFs, or Birds of a Feather), each dealing with a specific topic and operates in a bottom-up task creation mode, largely driven by these working groups. Each working group has an appointed chairperson (or sometimes several co-chairs), along with a charter that describes its focus, and what and when it is expected to produce. It is open to all who want to participate, and holds discussions on an open mailing list or at IETF meetings, where the entry fee in July 2014 was USD $650 per person.. Midst 2018 the fees are: early bird $700, late payment $875, student $150 and a one day pass for $375.\n\nRough consensus is the primary basis for decision making. There are no formal voting procedures. Because the majority of the IETF's work is done via mailing lists, meeting attendance is not required for contributors. Each working group is intended to complete work on its topic and then disband. In some cases, the WG will instead have its charter updated to take on new tasks as appropriate.\n\nThe working groups are organized into areas by subject matter. Current areas are Applications, General, Internet, Operations and Management, Real-time Applications and Infrastructure, Routing, Security, and Transport. Each area is overseen by an \"area director\" (AD), with most areas having two co-ADs. The ADs are responsible for appointing working group chairs. The area directors, together with the IETF Chair, form the Internet Engineering Steering Group (IESG), which is responsible for the overall operation of the IETF. \n\nThe IETF is overseen by the Internet Architecture Board (IAB), which oversees its external relationships, and relations with the RFC Editor. The IAB is also jointly responsible for the IETF Administrative Oversight Committee (IAOC), which oversees the IETF Administrative Support Activity (IASA), which provides logistical, etc. support for the IETF. The IAB also manages the Internet Research Task Force (IRTF), with which the IETF has a number of cross-group relations.\n\nA Nominating Committee (NomCom) of ten randomly chosen volunteers who participate regularly at meetings is vested with the power to appoint, reappoint, and remove members of the IESG, IAB, IASA, and the IAOC. To date, no one has been removed by a NomCom, although several people have resigned their positions, requiring replacements.\n\nIn 1993 the IETF changed from an activity supported by the U.S. government to an independent, international activity associated with the Internet Society, an international membership-based non-profit organization. Because the IETF itself does not have members, nor is it an organization \"per se\", the Internet Society provides the financial and legal framework for the activities of the IETF and its sister bodies (IAB, IRTF, …). IETF activities are funded by meeting fees, meeting sponsors and by the Internet Society via its organizational membership and the proceeds of the Public Interest Registry. \n\nIn December 2005 the IETF Trust was established to manage the copyrighted materials produced by the IETF.\n\nThe first IETF meeting was attended by 21 U.S.-government-funded researchers on 16 January 1986. It was a continuation of the work of the earlier GADS Task Force. Representatives from non-governmental entities were invited to attend starting with the fourth IETF meeting in October 1986. Since that time all IETF meetings have been open to the public.\n\nInitially, the IETF met quarterly, but from 1991, it has been meeting three times a year. The initial meetings were very small, with fewer than 35 people in attendance at each of the first five meetings. The maximum attendance during the first 13 meetings was only 120 attendees. This occurred at the 12th meeting held during January 1989. These meetings have grown in both participation and scope a great deal since the early 1990s; it had a maximum attendance of 2,810 at the December 2000 IETF held in San Diego, CA. Attendance declined with industry restructuring during the early 2000s, and is currently around 1,200.\n\nThe location for IETF meetings vary greatly. A list of past and future meeting locations can be found on the IETF meetings page. The IETF strives to hold its meetings near where most of the IETF volunteers are located. For many years, the goal was three meetings a year, with two in North America and one in either Europe or Asia, alternating between them every other year. The current goal is to hold three meetings in North America, two in Europe and one in Asia during a two-year period. However, corporate sponsorship of the meetings is also an important factor and the schedule has been modified from time to time in order to decrease operational costs.\n\nThe IETF also organizes hackathons during the IETF meetings. The focus is on implementing code that will improve standards in terms of quality and interoperability.\n\nThe details of IETF operations have changed considerably as the organization has grown, but the basic mechanism remains publication of proposed specifications, development based on the proposals, review and independent testing by participants, and republication as a revised proposal, a draft proposal, or eventually as an Internet Standard. IETF standards are developed in an open, all-inclusive process in which any interested individual can participate. All IETF documents are freely available over the Internet and can be reproduced at will. Multiple, working, useful, interoperable implementations are the chief requirement before an IETF proposed specification can become a standard. Most specifications are focused on single protocols rather than tightly interlocked systems. This has allowed the protocols to be used in many different systems, and its standards are routinely re-used by bodies which create full-fledged architectures (e.g. 3GPP IMS).\n\nBecause it relies on volunteers and uses \"rough consensus and running code\" as its touchstone, results can be slow whenever the number of volunteers is either too small to make progress, or so large as to make consensus difficult, or when volunteers lack the necessary expertise. For protocols like SMTP, which is used to transport e-mail for a user community in the many hundreds of millions, there is also considerable resistance to any change that is not fully backwards compatible, except for IPv6. Work within the IETF on ways to improve the speed of the standards-making process is ongoing but, because the number of volunteers with opinions on it is very great, consensus on improvements has been slow to develop.\n\nThe IETF cooperates with the W3C, ISO/IEC, ITU, and other standards bodies.\n\nStatistics are available that show who the top contributors by RFC publication are. While the IETF only allows for participation by individuals, and not by corporations or governments, sponsorship information is available from these statistics.\n\nThe IETF Chairperson is selected by the Nominating Committee (NomCom) process for a 2-year renewable term. Before 1993, the IETF Chair was selected by the IAB.\n\nA list of the past and current Chairs of the IETF follows:\nIt works on a broad range of networking technologies which provide foundation for the Internet's growth and evolution.\n\nIt aims to improve the efficiency in management of networks as they grow in size and complexity. The IETF is also standardizing protocols for autonomic networking that enables networks to be self managing.\n\nIt is a network of physical objects or things that are embedded with electronics, sensors, software and also enables objects to exchange data with operator, manufacturer and other connected devices. Several IETF working groups are developing protocols that are directly relevant to IoT.\n\nIts development provides the ability of internet applications to send data over the Internet. There are some well established transport protocols such as TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) which are continuously getting extended and refined to meet the needs of the global Internet.\n\nIt divides its work into number of areas which have Working groups that have a relation to an area's focus. Area Directors handle the primary task of area management. Area Directors may be advised by one or more Directorates. The area structure is defined by the Internet Engineering Steering Group. The Nominations Committee can be used to add new members.\n\nIn October 2018, Microsoft and Google engineers introduced a plan to create the Token Binding Protocol in order to stop OAuth token vulnerabilities called \"replay attacks\".\n\n"}
{"id": "15286", "url": "https://en.wikipedia.org/wiki?curid=15286", "title": "ISM band", "text": "ISM band\n\nThe industrial, scientific and medical (ISM) radio bands are radio bands (portions of the radio spectrum) reserved internationally for the use of radio frequency (RF) energy for industrial, scientific and medical purposes other than telecommunications. \nExamples of applications in these bands include radio-frequency process heating, microwave ovens, and medical diathermy machines. The powerful emissions of these devices can create electromagnetic interference and disrupt radio communication using the same frequency, so these devices were limited to certain bands of frequencies. In general, communications equipment operating in these bands must tolerate any interference generated by ISM applications, and users have no regulatory protection from ISM device operation.\n\nDespite the intent of the original allocations, in recent years the fastest-growing uses of these bands have been for short-range, low power wireless communications systems, since these bands are often approved for such devices that can be used without a government license, as would otherwise be required for transmitters; ISM frequencies are often chosen for that purpose as they already have serious interference. Cordless phones, Bluetooth devices, near field communication (NFC) devices, garage door openers, baby monitors and wireless computer networks (WiFi) all may use the ISM frequencies, although these low power emitters are not considered ISM.\n\nThe ISM bands are defined by the ITU Radio Regulations (article 5) in footnotes 5.138, 5.150, and 5.280 of the Radio Regulations. Individual countries' use of the bands designated in these sections may differ due to variations in national radio regulations. Because communication devices using the ISM bands must tolerate any interference from ISM equipment, unlicensed operations are typically permitted to use these bands, since unlicensed operation typically needs to be tolerant of interference from other devices anyway. The ISM bands share allocations with unlicensed and licensed operations; however, due to the high likelihood of harmful interference, licensed use of the bands is typically low. In the United States, uses of the ISM bands are governed by Part 18 of the Federal Communications Commission (FCC) rules, while Part 15 contains the rules for unlicensed communication devices, even those that share ISM frequencies. In Europe, the ETSI is responsible for regulating the use of Short Range Devices, some of which operate in ISM bands.\n\nThe allocation of radio frequencies is provided according to \"Article 5\" of the ITU Radio Regulations (edition 2012).\n\nIn order to improve harmonisation in spectrum utilisation, the majority of service-allocations stipulated in this document were incorporated in national Tables of Frequency Allocations and Utilisations which is within the responsibility of the appropriate national administration. The allocation might be primary, secondary, exclusive, and shared.\n\nType A (footnote 5.138) = frequency bands are designated for \"ISM applications\". The use of these frequency bands for ISM applications shall be subject to special authorization by the administration concerned, in agreement with other administrations whose radiocommunication services might be affected. In applying this provision, administrations shall have due regard to the latest relevant ITU-R Recommendations.\n\nType B (footnote 5.150) = frequency bands are also designated for ISM applications. Radiocommunication services operating within these bands must accept harmful interference which may be caused by these applications.\n\nITU RR, Footnote 5.280 = In Germany, Austria, Bosnia and Herzegovina, Croatia, Macedonia, Liechtenstein, Montenegro, Portugal, Serbia, Slovenia and Switzerland, the band 433.05-434.79 MHz (center frequency 433.92 MHz) is designated for \"ISM applications\". Radiocommunication services of these countries operating within this band must accept harmful interference which may be caused by these applications.\n\nFootnote AU, Australia is part of ITU Region 3 the band 433.05 to 434.79 MHz is not a designated ISM band in Australia, however the operation of low powered devices in the radiofrequency band 433.05 to 434.79 MHz is supported through Radiocommunications class licence for low interference potential devices (LIPDs).\n\nThe ISM bands were first established at the International Telecommunications Conference of the ITU in Atlantic City, 1947. The American delegation specifically proposed several bands, including the now commonplace 2.4 GHz band, to accommodate the then nascent process of microwave heating; however, FCC annual reports of that time suggest that much preparation was done ahead of these presentations.\n\nFrom the proceedings:\n\"The delegate of the United States, referring to his request that the frequency 2450 Mc/s be allocated for I.S.M., indicated that there was in existence in the United States, and working on this frequency a diathermy machine and an electronic cooker, and that the latter might eventually be installed in transatlantic ships and airplanes. There was therefore some point in attempting to reach world agreement on this subject.\"\n\nRadio frequencies in the ISM bands have been used for communication purposes, although such devices may experience interference from non-communication sources. In the United States, as early as 1958 Class D Citizens Band, a Part 95 service, was allocated to frequencies that are also allocated to ISM. [1]\n\nIn the U.S., the FCC first made unlicensed spread spectrum available in the ISM bands in rules adopted on May 9, 1985.\n\nMany other countries later developed similar regulations, enabling use of this technology. The FCC action was proposed by Michael Marcus of the FCC staff in 1980 and the subsequent regulatory action took five more years. It was part of a broader proposal to allow civil use of spread spectrum technology and was opposed at the time by mainstream equipment manufacturers and many radio system operators.\n\nThe original ISM specifications envisioned that the bands would be used primarily for noncommunication purposes, such as heating. The bands are still widely used for these purposes. For many people, the most commonly encountered ISM device is the home microwave oven operating at 2.45 GHz which uses microwaves to cook food. Industrial heating is another big application area; such as induction heating, microwave heat treating, plastic softening, and plastic welding processes. In medical settings, shortwave and microwave diathermy machines use radio waves in the ISM bands to apply deep heating to the body for relaxation and healing. More recently hyperthermia therapy uses microwaves to heat tissue to kill cancer cells. \n\nHowever, as detailed below, the increasing congestion of the radio spectrum, the increasing sophistication of microelectronics, and the attraction of unlicensed use, has in recent decades led to an explosion of uses of these bands for short range communication systems for wireless devices, which are now by far the largest uses of these bands. These are sometimes called \"non ISM\" uses since they do not fall under the originally envisioned \"industrial\", \"scientific\", and \"medical\" application areas. One of the largest applications has been wireless networking (WiFi). The IEEE 802.11 wireless networking protocols, the standards on which almost all wireless systems are based, use the ISM bands. Virtually all laptops, tablet computers, computer printers and cellphones now have 802.11 wireless modems using the 2.4 and 5.7 GHz ISM bands. Bluetooth is another networking technology using the 2.4 GHz band, which can be problematic given the probability of interference. Near field communication devices such as proximity cards and contactless smart cards use the lower frequency 13 and 27 MHz ISM bands. Other short range devices using the ISM bands are: wireless microphones, baby monitors, garage door openers, wireless doorbells, keyless entry systems for vehicles, radio control channels for UAVs (drones), wireless surveillance systems, RFID systems for merchandise, and wild animal tracking systems. \n\nSome electrodeless lamp designs are ISM devices, which use RF emissions to excite fluorescent tubes. Sulfur lamps are commercially available plasma lamps, which use a 2.45 GHz magnetron to heat sulfur into a brightly glowing plasma.\n\nLong-distance wireless power systems have been proposed and experimented with which would use high-power transmitters and rectennas, in lieu of overhead transmission lines and underground cables, to send power to remote locations. NASA has studied using microwave power transmission on 2.45 GHz to send energy collected by solar power satellites back to the ground.\n\nAlso in space applications, a Helicon Double Layer ion thruster is a prototype spacecraft propulsion engine which uses a 13.56 MHz transmission to break down and heat gas into plasma.\n\nIn recent years ISM bands have also been shared with (non-ISM) license-free error-tolerant communications applications such as wireless sensor networks in the 915 MHz and 2.450 GHz bands, as well as wireless LANs and cordless phones in the 915 MHz, 2.450 GHz, and 5.800 GHz bands. Because unlicensed devices are required to be tolerant of ISM emissions in these bands, unlicensed low power users are generally able to operate in these bands without causing problems for ISM users. ISM equipment does not necessarily include a radio receiver in the ISM band (e.g. a microwave oven does not have a receiver).\n\nIn the United States, according to 47 CFR Part 15.5, low power communication devices must accept interference from licensed users of that frequency band, and the Part 15 device must not cause interference to licensed users. Note that the 915 MHz band should not be used in countries outside Region 2, except those that specifically allow it, such as Australia and Israel, especially those that use the GSM-900 band for cellphones. The ISM bands are also widely used for Radio-frequency identification (RFID) applications with the most commonly used band being the 13.56 MHz band used by systems compliant with ISO/IEC 14443 including those used by biometric passports and contactless smart cards.\n\nIn Europe, the use of the ISM band is covered by Short Range Device regulations issued by European Commission, based on technical recommendations by CEPT and standards by ETSI. In most of Europe, LPD433 band is allowed for license-free voice communication in addition to PMR446.\n\nWireless LAN devices use wavebands as follows:\n\nIEEE 802.15.4, ZigBee and other personal area networks may use the and ISM bands because of frequency sharing between different allocations.\n\nWireless LANs and cordless phones can also use bands other than those shared with ISM, but such uses require approval on a country by country basis. DECT phones use allocated spectrum outside the ISM bands that differs in Europe and North America. Ultra-wideband LANs require more spectrum than the ISM bands can provide, so the relevant standards such as IEEE 802.15.4a are designed to make use of spectrum outside the ISM bands. Despite the fact that these additional bands are outside the official ITU-R ISM bands, because they are used for the same types of low power personal communications, they are sometimes incorrectly referred to as ISM bands as well.\n\nAlso note that several brands of radio control equipment use the band range for low power remote control of toys, from gas powered cars to miniature aircraft.\n\nWorldwide Digital Cordless Telecommunications or WDCT is a technology that uses the radio spectrum.\n\nGoogle's Project Loon uses ISM bands (specifically 2.4 and 5.8 GHz bands) for balloon-to-balloon and balloon-to-ground communications.\n\nPursuant to 47 CFR Part 97 some ISM bands are used by licensed amateur radio operators for communication - including amateur television.\n\n\n"}
{"id": "15287", "url": "https://en.wikipedia.org/wiki?curid=15287", "title": "Series (mathematics)", "text": "Series (mathematics)\n\nIn mathematics, a series is, roughly speaking, a description of the operation of adding infinitely many quantities, one after the other, to a given starting quantity. The study of series is a major part of calculus and its generalization, mathematical analysis. Series are used in most areas of mathematics, even for studying finite structures (such as in combinatorics), through generating functions. In addition to their ubiquity in mathematics, infinite series are also widely used in other quantitative disciplines such as physics, computer science, statistics and finance.\n\nFor a long time, the idea that such a potentially infinite summation could produce a finite result was considered paradoxical. This paradox was resolved using the concept of a limit during the 19th century. Zeno's paradox of Achilles and the tortoise illustrates this counterintuitive property of infinite sums: Achilles runs after a tortoise, but when he reaches the position of the tortoise at the beginning of the race, the tortoise has reached a second position; when he reaches this second position, the tortoise is at a third position, and so on. Zeno concluded that Achilles could \"never\" reach the tortoise, and thus that movement does not exist. Zeno divided the race into infinitely many sub-races, each requiring a finite amount of time, so that the total time for Achilles to catch the tortoise is given by a series. The resolution of the paradox is that, although the series has an infinite number of terms, it has a finite sum, which gives the time necessary for Achilles to catch up with the tortoise.\n\nIn modern terminology, any (ordered) infinite sequence formula_1 of terms (that is, numbers, functions, or anything that can be added) defines a series, which is the operation of adding the formula_2 one after the other. To emphasize that there are an infinite number of terms, a series may be called an infinite series. Such a series is represented (or denoted) by an expression like\nor, using the summation sign,\n\nThe infinite sequence of additions implied by a series cannot be effectively carried on (at least in a finite amount of time). However, if the set to which the terms and their finite sums belong has a notion of limit, it is sometimes possible to assign a value to a series, called the \"sum\" of the series. This value is the limit as tends to infinity (if the limit exists) of the finite sums of the first terms of the series, which are called the \"th partial sums\" of the series. That is,\nWhen this limit exists, one says that the series is \"convergent\" or \"summable\", or that the sequence formula_1 is summable. In this case, the limit is called the \"sum of the series\". Otherwise, the series is said to be \"divergent\".\n\nGenerally, the terms of a series come from a ring, often the field formula_7 of the real numbers or the field formula_8 of the complex numbers. In this case, the set of all series is itself a ring (and even an associative algebra), in which the addition consists of adding the series term by term, and the multiplication is the Cauchy product.\n\nAn infinite series or simply a series is an infinite sum, represented by an infinite expression of the form\nwhere formula_10 is any ordered sequence of terms, such as numbers, functions, or anything else that can be added (an abelian group). This is an expression that is obtained from the list of terms formula_11 by laying them side by side, and conjoining them with the symbol \"+\". A series may also be represented by using summation notation, such as\n\nIf an abelian group \"A\" of terms has a concept of limit (for example, if it is a metric space), then some series, the convergent series, can be interpreted as having a value in \"A\", called the \"sum of the series\". This includes the common cases from calculus in which the group is the field of real numbers or the field of complex numbers. Given a series formula_13 its \"k\"th partial sum is\nBy definition, the series formula_15 \"converges\" to the limit (or simply \"sums\" to ), if the sequence of its partial sums has a limit . In this case, one usually writes\nA series is said to be \"convergent\" if it converges to some limit or \"divergent\" when it does not. The value of this limit, if it exists, is then the value of the series.\n\nA series is said to converge or to \"be convergent\" when the sequence of partial sums has a finite limit. If the limit of is infinite or does not exist, the series is said to diverge. When the limit of partial sums exists, it is called the value (or sum) of the series\n\nAn easy way that an infinite series can converge is if all the \"a\" are zero for \"n\" sufficiently large. Such a series can be identified with a finite sum, so it is only infinite in a trivial sense.\n\nWorking out the properties of the series that converge even if infinitely many terms are non-zero is the essence of the study of series. Consider the example\n\nIt is possible to \"visualize\" its convergence on the real number line: we can imagine a line of length 2, with successive segments marked off of lengths 1, ½, ¼, etc. There is always room to mark the next segment, because the amount of line remaining is always the same as the last segment marked: when we have marked off ½, we still have a piece of length ½ unmarked, so we can certainly mark the next ¼. This argument does not prove that the sum is \"equal\" to 2 (although it is), but it does prove that it is \"at most\" 2. In other words, the series has an upper bound. Given that the series converges, proving that it is equal to 2 requires only elementary algebra. If the series is denoted , it can be seen that\n\nTherefore,\n\nThe idiom can be extended to other, equivalent notions of series. For instance, a recurring decimal, as in\n\nencodes the series\n\nSince these series always converge to real numbers (because of what is called the completeness property of the real numbers), to talk about the series in this way is the same as to talk about the numbers for which they stand. In particular, the decimal expansion 0.111… can be identified with /. This leads to an argument that , which only relies on the fact that the limit laws for series preserve the arithmetic operations; this argument is presented in the article 0.999...\n\n\nand\n\n\nPartial summation takes as input a sequence, { \"a\" }, and gives as output another sequence, { \"S\" }. It is thus a unary operation on sequences. Further, this function is linear, and thus is a linear operator on the vector space of sequences, denoted Σ. The inverse operator is the finite difference operator, Δ. These behave as discrete analogs of integration and differentiation, only for series (functions of a natural number) instead of functions of a real variable. For example, the sequence {1, 1, 1, ...} has series {1, 2, 3, 4, ...} as its partial summation, which is analogous to the fact that formula_49\n\nIn computer science it is known as prefix sum.\n\nSeries are classified not only by whether they converge or diverge, but also by the properties of the terms a (absolute or conditional convergence); type of convergence of the series (pointwise, uniform); the class of the term a (whether it is a real number, arithmetic progression, trigonometric function); etc.\n\nWhen \"a\" is a non-negative real number for every \"n\", the sequence \"S\" of partial sums is non-decreasing. It follows that a series ∑\"a\" with non-negative terms converges if and only if the sequence \"S\" of partial sums is bounded.\n\nFor example, the series\n\nis convergent, because the inequality\n\nand a telescopic sum argument implies that the partial sums are bounded by 2. The exact value of the original series is the Basel problem.\n\nA series\nis said to converge absolutely if the series of absolute values\nconverges. This is sufficient to guarantee not only that the original series converges to a limit, but also that any reordering of it converges to the same limit.\n\nA series of real or complex numbers is said to be conditionally convergent (or semi-convergent) if it is convergent but not absolutely convergent. A famous example is the alternating series\n\nwhich is convergent (and its sum is equal to ln 2), but the series formed by taking the absolute value of each term is the divergent harmonic series. The Riemann series theorem says that any conditionally convergent series can be reordered to make a divergent series, and moreover, if the \"a\" are real and \"S\" is any real number, that one can find a reordering so that the reordered series converges with sum equal to \"S\".\n\nAbel's test is an important tool for handling semi-convergent series. If a series has the form\n\nwhere the partial sums \"B\" = are bounded, \"λ\" has bounded variation, and exists:\n\nthen the series is convergent. This applies to the pointwise convergence of many trigonometric series, as in\n\nwith 0 < \"x\" < 2π. Abel's method consists in writing \"b\" = \"B\" − \"B\", and in performing a transformation similar to integration by parts (called summation by parts), that relates the given series to the absolutely convergent series\n\n\nA series of real- or complex-valued functions\n\nconverges pointwise on a set \"E\", if the series converges for each \"x\" in \"E\" as an ordinary series of real or complex numbers. Equivalently, the partial sums\nconverge to \"ƒ\"(\"x\") as \"N\" → ∞ for each \"x\" ∈ \"E\".\n\nA stronger notion of convergence of a series of functions is called uniform convergence. The series converges uniformly if it converges pointwise to the function \"ƒ\"(\"x\"), and the error in approximating the limit by the \"N\"th partial sum,\ncan be made minimal \"independently\" of \"x\" by choosing a sufficiently large \"N\".\n\nUniform convergence is desirable for a series because many properties of the terms of the series are then retained by the limit. For example, if a series of continuous functions converges uniformly, then the limit function is also continuous. Similarly, if the \"ƒ\" are integrable on a closed and bounded interval \"I\" and converge uniformly, then the series is also integrable on \"I\" and can be integrated term-by-term. Tests for uniform convergence include the Weierstrass' M-test, Abel's uniform convergence test, Dini's test, and the Cauchy criterion.\n\nMore sophisticated types of convergence of a series of functions can also be defined. In measure theory, for instance, a series of functions converges almost everywhere if it converges pointwise except on a certain set of measure zero. Other modes of convergence depend on a different metric space structure on the space of functions under consideration. For instance, a series of functions converges in mean on a set \"E\" to a limit function \"ƒ\" provided\nas \"N\" → ∞.\n\nA power series is a series of the form\n\nThe Taylor series at a point \"c\" of a function is a power series that, in many cases, converges to the function in a neighborhood of \"c\". For example, the series\nis the Taylor series of formula_65 at the origin and converges to it for every \"x\".\n\nUnless it converges only at \"x\"=\"c\", such a series converges on a certain open disc of convergence centered at the point \"c\" in the complex plane, and may also converge at some of the points of the boundary of the disc. The radius of this disc is known as the radius of convergence, and can in principle be determined from the asymptotics of the coefficients \"a\". The convergence is uniform on closed and bounded (that is, compact) subsets of the interior of the disc of convergence: to wit, it is uniformly convergent on compact sets.\n\nHistorically, mathematicians such as Leonhard Euler operated liberally with infinite series, even if they were not convergent. When calculus was put on a sound and correct foundation in the nineteenth century, rigorous proofs of the convergence of series were always required.\n\nWhile many uses of power series refer to their sums, it is also possible to treat power series as \"formal sums\", meaning that no addition operations are actually performed, and the symbol \"+\" is an abstract symbol of conjunction which is not necessarily interpreted as corresponding to addition. In this setting, the sequence of coefficients itself is of interest, rather than the convergence of the series. Formal power series are used in combinatorics to describe and study sequences that are otherwise difficult to handle, for example, using the method of generating functions. The Hilbert–Poincaré series is a formal power series used to study graded algebras.\n\nEven if the limit of the power series is not considered, if the terms support appropriate structure then it is possible to define operations such as addition, multiplication, derivative, antiderivative for power series \"formally\", treating the symbol \"+\" as if it corresponded to addition. In the most common setting, the terms come from a commutative ring, so that the formal power series can be added term-by-term and multiplied via the Cauchy product. In this case the algebra of formal power series is the total algebra of the monoid of natural numbers over the underlying term ring. If the underlying term ring is a differential algebra, then the algebra of formal power series is also a differential algebra, with differentiation performed term-by-term.\n\nLaurent series generalize power series by admitting terms into the series with negative as well as positive exponents. A Laurent series is thus any series of the form\nIf such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points. The series converges uniformly on compact subsets of the interior of the annulus of convergence.\n\nA Dirichlet series is one of the form\n\nwhere \"s\" is a complex number. For example, if all \"a\" are equal to 1, then the Dirichlet series is the Riemann zeta function\n\nLike the zeta function, Dirichlet series in general play an important role in analytic number theory. Generally a Dirichlet series converges if the real part of \"s\" is greater than a number called the abscissa of convergence. In many cases, a Dirichlet series can be extended to an analytic function outside the domain of convergence by analytic continuation. For example, the Dirichlet series for the zeta function converges absolutely when Re \"s\" > 1, but the zeta function can be extended to a holomorphic function defined on formula_69  with a simple pole at 1.\n\nThis series can be directly generalized to general Dirichlet series.\n\nA series of functions in which the terms are trigonometric functions is called a trigonometric series:\nThe most important example of a trigonometric series is the Fourier series of a function.\n\nGreek mathematician Archimedes produced the first known summation of an infinite series with a\nmethod that is still used in the area of calculus today. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of π.\n\nMathematicians from Kerala, India studied infinite series around 1350 CE.\n\nIn the 17th century, James Gregory worked in the new decimal system on infinite series and published several Maclaurin series. In 1715, a general method for constructing the Taylor series for all functions for which they exist was provided by Brook Taylor. Leonhard Euler in the 18th century, developed the theory of hypergeometric series and q-series.\n\nThe investigation of the validity of infinite series is considered to begin with Gauss in the 19th century. Euler had already considered the hypergeometric series\n\non which Gauss published a memoir in 1812. It established simpler criteria of convergence, and the questions of remainders and the range of convergence.\n\nCauchy (1821) insisted on strict tests of convergence; he showed that if two series are convergent their product is not necessarily so, and with him begins the discovery of effective criteria. The terms \"convergence\" and \"divergence\" had been introduced long before by Gregory (1668). Leonhard Euler and Gauss had given various criteria, and Colin Maclaurin had anticipated some of Cauchy's discoveries. Cauchy advanced the theory of power series by his expansion of a complex function in such a form.\n\nAbel (1826) in his memoir on the binomial series\n\ncorrected certain of Cauchy's conclusions, and gave a completely\nscientific summation of the series for complex values of formula_73 and formula_74. He showed the necessity of considering the subject of continuity in questions of convergence.\n\nCauchy's methods led to special rather than general criteria, and\nthe same may be said of Raabe (1832), who made the first elaborate\ninvestigation of the subject, of De Morgan (from 1842), whose\nlogarithmic test DuBois-Reymond (1873) and Pringsheim (1889) have\nshown to fail within a certain region; of Bertrand (1842), Bonnet\n(1843), Malmsten (1846, 1847, the latter without integration);\nStokes (1847), Paucker (1852), Chebyshev (1852), and Arndt\n(1853).\n\nGeneral criteria began with Kummer (1835), and have been\nstudied by Eisenstein (1847), Weierstrass in his various\ncontributions to the theory of functions, Dini (1867),\nDuBois-Reymond (1873), and many others. Pringsheim's memoirs (1889) present the most complete general theory.\n\nThe theory of uniform convergence was treated by Cauchy (1821), his\nlimitations being pointed out by Abel, but the first to attack it\nsuccessfully were Seidel and Stokes (1847–48). Cauchy took up the\nproblem again (1853), acknowledging Abel's criticism, and reaching\nthe same conclusions which Stokes had already found. Thomae used the\ndoctrine (1866), but there was great delay in recognizing the\nimportance of distinguishing between uniform and non-uniform\nconvergence, in spite of the demands of the theory of functions.\n\nA series is said to be semi-convergent (or conditionally convergent) if it is convergent but not absolutely convergent.\n\nSemi-convergent series were studied by Poisson (1823), who also gave a general form for the remainder of the Maclaurin formula. The most important solution of the problem is due, however, to Jacobi (1834),\nwho attacked the question of the remainder from a different standpoint and reached a different formula. This expression was also worked out, and another one given, by Malmsten (1847). Schlömilch (\"Zeitschrift\", Vol.I, p. 192, 1856) also improved Jacobi's remainder, and showed the relation between the remainder and Bernoulli's function\n\nGenocchi (1852) has further contributed to the theory.\n\nAmong the early writers was Wronski, whose \"loi suprême\" (1815) was hardly recognized until Cayley (1873) brought it into\nprominence.\n\nFourier series were being investigated\nas the result of physical considerations at the same time that\nGauss, Abel, and Cauchy were working out the theory of infinite\nseries. Series for the expansion of sines and cosines, of multiple\narcs in powers of the sine and cosine of the arc had been treated by\nJacob Bernoulli (1702) and his brother Johann Bernoulli (1701) and still\nearlier by Vieta. Euler and Lagrange simplified the subject,\nas did Poinsot, Schröter, Glaisher, and Kummer.\n\nFourier (1807) set for himself a different problem, to\nexpand a given function of \"x\" in terms of the sines or cosines of\nmultiples of \"x\", a problem which he embodied in his \"Théorie analytique de la chaleur\" (1822). Euler had already given the\nformulas for determining the coefficients in the series;\nFourier was the first to assert and attempt to prove the general\ntheorem. Poisson (1820–23) also attacked the problem from a\ndifferent standpoint. Fourier did not, however, settle the question\nof convergence of his series, a matter left for Cauchy (1826) to\nattempt and for Dirichlet (1829) to handle in a thoroughly\nscientific manner (see convergence of Fourier series). Dirichlet's treatment (\"Crelle\", 1829), of trigonometric series was the subject of criticism and improvement by\nRiemann (1854), Heine, Lipschitz, Schläfli, and\ndu Bois-Reymond. Among other prominent contributors to the theory of\ntrigonometric and Fourier series were Dini, Hermite, Halphen,\nKrause, Byerly and Appell.\n\nAsymptotic series, otherwise asymptotic expansions, are infinite series whose partial sums become good approximations in the limit of some point of the domain. In general they do not converge. But they are useful as sequences of approximations, each of which provides a value close to the desired answer for a finite number of terms. The difference is that an asymptotic series cannot be made to produce an answer as exact as desired, the way that convergent series can. In fact, after a certain number of terms, a typical asymptotic series reaches its best approximation; if more terms are included, most such series will produce worse answers.\n\nUnder many circumstances, it is desirable to assign a limit to a series which fails to converge in the usual sense. A summability method is such an assignment of a limit to a subset of the set of divergent series which properly extends the classical notion of convergence. Summability methods include Cesàro summation, (\"C\",\"k\") summation, Abel summation, and Borel summation, in increasing order of generality (and hence applicable to increasingly divergent series).\n\nA variety of general results concerning possible summability methods are known. The Silverman–Toeplitz theorem characterizes \"matrix summability methods\", which are methods for summing a divergent series by applying an infinite matrix to the vector of coefficients. The most general method for summing a divergent series is non-constructive, and concerns Banach limits.\n\nThe notion of series can be easily extended to the case of a Banach space. If \"x\" is a sequence of elements of a Banach space \"X\", then the series Σ\"x\" converges to \"x\" ∈ \"X\" if the sequence of partial sums of the series tends to \"x\"; to wit,\nas \"N\" → ∞.\n\nMore generally, convergence of series can be defined in any abelian Hausdorff topological group. Specifically, in this case, Σ\"x\" converges to \"x\" if the sequence of partial sums converges to \"x\".\n\nDefinitions may be given for sums over an arbitrary index set . There are two main differences with the usual notion of series: first, there is no specific order given on the set ; second, this set may be uncountable. The notion of convergence needs to be strengthened, because the concept of conditional convergence depends on the ordering of the index set.\n\nIf formula_77 is a function from an index set to a set , then the \"series\" associated to formula_78 is the formal sum of the elements formula_79 over the index elements formula_80 denoted by the\n\nWhen the index set is the natural numbers formula_82, the function formula_83 is a sequence denoted by formula_84. A series indexed on the natural numbers is an ordered formal sum and so we rewrite formula_85 as formula_86 in order to emphasize the ordering induced by the natural numbers. Thus, we obtain the common notation for a series indexed by the natural numbers\n\nWhen summing a family {\"a\"}, \"i\" ∈ \"I\", of non-negative numbers, one may define\n\nWhen the supremum is finite, the set of \"i\" ∈ \"I\" such that \"a\" > 0 is countable. Indeed, for every \"n\" ≥ 1, the set formula_89 is finite, because\n\nIf \"I\"  is countably infinite and enumerated as \"I\" = {\"i\", \"i\"...} then the above defined sum satisfies\n\nprovided the value ∞ is allowed for the sum of the series.\n\nAny sum over non-negative reals can be understood as the integral of a non-negative function with respect to the counting measure, which accounts for the many similarities between the two constructions.\n\nLet \"a\" : \"I\" → \"X\", where \"I\"  is any set and \"X\"  is an abelian Hausdorff topological group. Let \"F\"  be the collection of all finite subsets of \"I\". Note that \"F\"  is a directed set ordered under inclusion with union as join. Define the sum \"S\"  of the family \"a\" as the limit\n\nif it exists and say that the family \"a\" is unconditionally summable. Saying that the sum \"S\"  is the limit of finite partial sums means that for every neighborhood \"V\"  of 0 in \"X\", there is a finite subset \"A\" of \"I\"  such that\n\nBecause \"F\"  is not totally ordered, this is not a limit of a sequence of partial sums, but rather of a net.\n\nFor every \"W\", neighborhood of 0 in \"X\", there is a smaller neighborhood \"V\"  such that \"V\" − \"V\" ⊂ \"W\". It follows that the finite partial sums of an unconditionally summable family \"a\", \"i\" ∈ \"I\", form a \"Cauchy net\", that is, for every \"W\", neighborhood of 0 in \"X\", there is a finite subset \"A\" of \"I\"  such that\n\nWhen \"X\"  is complete, a family \"a\" is unconditionally summable in \"X\"  if and only if the finite sums satisfy the latter Cauchy net condition. When \"X\"  is complete and \"a\", \"i\" ∈ \"I\", is unconditionally summable in \"X\", then for every subset \"J\" ⊂ \"I\", the corresponding subfamily \"a\", \"j\" ∈ \"J\", is also unconditionally summable in \"X\".\n\nWhen the sum of a family of non-negative numbers, in the extended sense defined before, is finite, then it coincides with the sum in the topological group \"X\" = R.\n\nIf a family \"a\" in \"X\"  is unconditionally summable, then for every \"W\", neighborhood of 0 in \"X\", there is a finite subset \"A\" of \"I\"  such that \"a\" ∈ \"W\"  for every \"i\" not in \"A\". If \"X\"  is first-countable, it follows that the set of \"i\" ∈ \"I\"  such that \"a\" ≠ 0 is countable. This need not be true in a general abelian topological group (see examples below).\n\nSuppose that \"I\" = N. If a family \"a\", \"n\" ∈ N, is unconditionally summable in an abelian Hausdorff topological group \"X\", then the series in the usual sense converges and has the same sum,\n\nBy nature, the definition of unconditional summability is insensitive to the order of the summation. When ∑\"a\" is unconditionally summable, then the series remains convergent after any permutation \"σ\" of the set N of indices, with the same sum,\n\nConversely, if every permutation of a series ∑\"a\" converges, then the series is unconditionally convergent. When \"X\"  is complete, then unconditional convergence is also equivalent to the fact that all subseries are convergent; if \"X\"  is a Banach space, this is equivalent to say that for every sequence of signs \"ε\" = ±1, the series\n\nconverges in \"X\". If \"X\"  is a Banach space, then one may define the notion of absolute convergence. A series ∑\"a\" of vectors in \"X\"  converges absolutely if\n\nIf a series of vectors in a Banach space converges absolutely then it converges unconditionally, but the converse only holds in finite-dimensional Banach spaces (theorem of ).\n\nConditionally convergent series can be considered if \"I\" is a well-ordered set, for example, an ordinal number \"α\". One may define by transfinite recursion:\n\nand for a limit ordinal \"α\",\n\nif this limit exists. If all limits exist up to \"α\", then the series converges.\n\n\n"}
{"id": "15289", "url": "https://en.wikipedia.org/wiki?curid=15289", "title": "Interrupt", "text": "Interrupt\n\nIn system programming, an interrupt is a signal to the processor emitted by hardware or software indicating an event that needs immediate attention. An interrupt alerts the processor to a high-priority condition requiring the interruption of the current code the processor is executing. The processor responds by suspending its current activities, saving its state, and executing a function called an \"interrupt handler\" (or an interrupt service routine, ISR) to deal with the event. This interruption is temporary, and, after the interrupt handler finishes, the processor resumes normal activities. There are two types of interrupts: hardware interrupts and software interrupts.\n\nHardware interrupts are used by devices to communicate that they require attention from the operating system. Internally, hardware interrupts are implemented using electronic alerting signals that are sent to the processor from an external device, which is either a part of the computer itself, such as a disk controller, or an external peripheral. For example, pressing a key on the keyboard or moving the mouse triggers hardware interrupts that cause the processor to read the keystroke or mouse position. Unlike the software type (described below), hardware interrupts are asynchronous and can occur in the middle of instruction execution, requiring additional care in programming. The act of initiating a hardware interrupt is referred to as an interrupt request (IRQ).\n\nA software interrupt is caused either by an exceptional condition in the processor itself, or a special instruction in the instruction set which causes an interrupt when it is executed. The former is often called a \"trap\" or \"exception\" and is used for errors or events occurring during program execution that are exceptional enough that they cannot be handled within the program itself. For example, a divide-by-zero exception will be thrown if the processor's arithmetic logic unit is commanded to divide a number by zero as this instruction is an error and impossible. The operating system will catch this exception, and can decide what to do about it: usually aborting the process and displaying an error message. Software interrupt instructions can function similarly to subroutine calls and are used for a variety of purposes, such as to request services from device drivers, like interrupts sent to and from a disk controller to request reading or writing of data to and from the disk.\n\nEach interrupt has its own interrupt handler. The number of hardware interrupts is limited by the number of interrupt request (IRQ) lines to the processor, but there may be hundreds of different software interrupts. Interrupts are a commonly used technique for computer multitasking, especially in real-time computing. Such a system is said to be interrupt-driven.\n\nInterrupts are similar to signals, the difference being that signals are used for inter-process communication (IPC), mediated by the kernel (possibly via system calls) and handled by processes, while interrupts are mediated by the processor and handled by the kernel. The kernel may pass an interrupt as a signal to the process that caused it (typical examples are SIGSEGV, SIGBUS, SIGILL and SIGFPE).\n\nHardware interrupts were introduced as an optimization, eliminating unproductive waiting time in polling loops, waiting for external events. The first system to use this approach was the DYSEAC, completed in 1954, although earlier systems provided error trap functions. Interrupts may be implemented in hardware as a distinct system with control lines, or they may be integrated into the memory subsystem.\n\nIf implemented in hardware, an interrupt controller circuit such as the IBM PC's Programmable Interrupt Controller (PIC) may be connected between the interrupting device and the processor's interrupt pin to multiplex several sources of interrupt onto the one or two CPU lines typically available. If implemented as part of the memory controller, interrupts are mapped into the system's memory address space.\n\nInterrupts can be categorized into these different types:\n\nProcessors typically have an internal \"interrupt mask\" which allows software to ignore all external hardware interrupts while it is set. Setting or clearing this mask may be faster than accessing an interrupt mask register (IMR) in a PIC or disabling interrupts in the device itself. In some cases, such as the x86 architecture, disabling and enabling interrupts on the processor itself act as a memory barrier; however, it may actually be slower.\n\nAn interrupt that leaves the machine in a well-defined state is called a \"precise interrupt\". Such an interrupt has four properties:\n\nAn interrupt that does not meet these requirements is called an \"imprecise interrupt\".\n\nThe phenomenon where the overall system performance is severely hindered by excessive amounts of processing time spent handling interrupts is called an interrupt storm.\n\nA \"level-triggered interrupt\" is an interrupt signaled by maintaining the interrupt line at a high or low logic level. A device wishing to signal a level-triggered interrupt drives the interrupt request line to its active level (high or low), and then holds it at that level until it is serviced. It ceases asserting the line when the CPU commands it to or otherwise handles the condition that caused it to signal the interrupt.\n\nTypically, the processor samples the interrupt input at predefined times during each bus cycle such as state T2 for the Z80 microprocessor. If the interrupt isn't active when the processor samples it, the CPU doesn't see it. One possible use for this type of interrupt is to minimize spurious signals from a noisy interrupt line: a spurious pulse will often be so short that it is not noticed.\n\nMultiple devices may share a level-triggered interrupt line if they are designed to. The interrupt line must have a pull-down or pull-up resistor so that when not actively driven it settles to its inactive state. Devices actively assert the line to indicate an outstanding interrupt, but let the line float (do not actively drive it) when not signalling an interrupt. The line is then in its asserted state when any (one or more than one) of the sharing devices is signalling an outstanding interrupt.\n\nLevel-triggered interrupt is favored by some because it is easy to share the interrupt request line without losing the interrupts, when multiple shared devices interrupt at the same time. Upon detecting assertion of the interrupt line, the CPU must search through the devices sharing the interrupt request line until one who triggered the interrupt is detected. After servicing this device, the CPU may recheck the interrupt line status to determine whether any other devices also need service. If the line is now de-asserted, the CPU avoids checking the remaining devices on the line. Since some devices interrupt more frequently than others, and other device interrupts are particularly expensive, a careful ordering of device checks is employed to increase efficiency. The original PCI standard mandated level-triggered interrupts because of this advantage of sharing interrupts. \n\nThere are also serious problems with sharing level-triggered interrupts. As long as any device on the line has an outstanding request for service the line remains asserted, so it is not possible to detect a change in the status of any other device. Deferring servicing a low-priority device is not an option, because this would prevent detection of service requests from higher-priority devices. If there is a device on the line that the CPU does not know how to service, then any interrupt from that device permanently blocks all interrupts from the other devices.\n\nAn \"edge-triggered interrupt\" is an interrupt signalled by a level transition on the interrupt line, either a falling edge (high to low) or a rising edge (low to high). A device, wishing to signal an interrupt, drives a pulse onto the line and then releases the line to its inactive state. If the pulse is too short to be detected by polled I/O then special hardware may be required to detect the edge.\n\nMultiple devices may share an edge-triggered interrupt line if they are designed to. The interrupt line must have a pull-down or pull-up resistor so that when not actively driven it settles to one particular state. Devices signal an interrupt by briefly driving the line to its non-default state, and let the line float (do not actively drive it) when not signalling an interrupt. This type of connection is also referred to as open collector. The line then carries all the pulses generated by all the devices. (This is analogous to the pull cord on some buses and trolleys that any passenger can pull to signal the driver that they are requesting a stop.) However, interrupt pulses from different devices may merge if they occur close in time. To avoid losing interrupts the CPU must trigger on the trailing edge of the pulse (e.g. the rising edge if the line is pulled up and driven low). After detecting an interrupt the CPU must check all the devices for service requirements.\n\nEdge-triggered interrupts do not suffer the problems that level-triggered interrupts have with sharing. Service of a low-priority device can be postponed arbitrarily, and interrupts will continue to be received from the high-priority devices that are being serviced. If there is a device that the CPU does not know how to service, it may cause a spurious interrupt, or even periodic spurious interrupts, but it does not interfere with the interrupt signalling of the other devices. However, it is fairly easy for an edge triggered interrupt to be missed - for example if interrupts have to be masked for a period - and unless there is some type of hardware latch that records the event it is impossible to recover. Such problems caused many \"lockups\" in early computer hardware because the processor did not know it was expected to do something. More modern hardware often has one or more interrupt status registers that latch the interrupt requests; well written edge-driven interrupt software often checks such registers to ensure events are not missed.\n\nThe elderly Industry Standard Architecture (ISA) bus uses edge-triggered interrupts, but does not mandate that devices be able to share them. The parallel port also uses edge-triggered interrupts. Many older devices assume that they have exclusive use of their interrupt line, making it electrically unsafe to share them. However, ISA motherboards include pull-up resistors on the IRQ lines, so well-behaved devices share ISA interrupts just fine.\n\nThere are 3 ways multiple devices \"sharing the same line\" can be raised. First is by exclusive conduction (switching) or exclusive connection (to pins). Next is by bus (all connected to same line listening): cards on a bus must know when they are to talk and not talk (ie, the ISA bus). Talking can be triggered two ways: by accumulation latch or by logic gate. Logic gates expect a continual data flow which is monitored for key signals. Accumulators only trigger when the remote side excites the gate beyond a threshold, thus no negotiated speed is required. Each has its speed versus distance advantages. A trigger, generally, is the method in which excitation is detected: rising edge, falling edge, threshold (oscilloscope can trigger upon a wide variety of shapes and conditions).\n\nTriggering for software interrupts must be built into the softwares (both in OS and app). A 'C' app has a trigger table (a table of functions) in its header, which both the app and OS know of and use appropriately that is not related to hardware. However do not confuse this with hardware interrupts which signal the CPU (the CPU enacts software from a table of functions, similarly to software interrupts).\n\nSome systems use a hybrid of level-triggered and edge-triggered signalling. The hardware not only looks for an edge, but it also verifies that the interrupt signal stays active for a certain period of time.\n\nA common use of a hybrid interrupt is for the NMI (non-maskable interrupt) input. Because NMIs generally signal major – or even catastrophic – system events, a good implementation of this signal tries to ensure that the interrupt is valid by verifying that it remains active for a period of time. This 2-step approach helps to eliminate false interrupts from affecting the system.\n\nA \"message-signalled interrupt\" does not use a physical interrupt line. Instead, a device signals its request for service by sending a short message over some communications medium, typically a computer bus. The message might be of a type reserved for interrupts, or it might be of some pre-existing type such as a memory write.\n\nMessage-signalled interrupts behave very much like edge-triggered interrupts, in that the interrupt is a momentary signal rather than a continuous condition. Interrupt-handling software treats the two in much the same manner. Typically, multiple pending message-signalled interrupts with the same message (the same virtual interrupt line) are allowed to merge, just as closely spaced edge-triggered interrupts can merge.\n\nMessage-signalled interrupt vectors can be shared, to the extent that the underlying communication medium can be shared. No additional effort is required.\n\nBecause the identity of the interrupt is indicated by a pattern of data bits, not requiring a separate physical conductor, many more distinct interrupts can be efficiently handled. This reduces the need for sharing. Interrupt messages can also be passed over a serial bus, not requiring any additional lines.\n\nPCI Express, a serial computer bus, uses message-signalled interrupts exclusively.\n\nIn a push button analogy applied to computer systems, the term \"doorbell\" or \"doorbell interrupt\" is often used to describe a mechanism whereby a software system can signal or notify a computer hardware device that there is some work to be done. Typically, the software system will place data in some well-known and mutually agreed upon memory location(s), and \"ring the doorbell\" by writing to a different memory location. This different memory location is often called the doorbell region, and there may even be multiple doorbells serving different purposes in this region. It is this act of writing to the doorbell region of memory that \"rings the bell\" and notifies the hardware device that the data are ready and waiting. The hardware device would now know that the data are valid and can be acted upon. It would typically write the data to a hard disk drive, or send them over a network, or encrypt them, etc. \n\nThe term \"doorbell interrupt\" is usually a misnomer. Its similar to an interrupt, because it causes some work to be done by the device; however, the doorbell region is sometimes implemented as a polled region, sometimes the doorbell region writes through to physical device registers, and sometimes the doorbell region is hardwired directly to physical device registers. When either writing through or directly to physical device registers, this may cause a real interrupt to occur at the device's central processor unit (CPU), if it has one.\n\nDoorbell interrupts can be compared to Message Signaled Interrupts, as they have some similarities.\n\nMultiple devices sharing an interrupt line (of any triggering style) all act as spurious interrupt sources with respect to each other. With many devices on one line the workload in servicing interrupts grows in proportion to the square of the number of devices. It is therefore preferred to spread devices evenly across the available interrupt lines. Shortage of interrupt lines is a problem in older system designs where the interrupt lines are distinct physical conductors. Message-signalled interrupts, where the interrupt line is virtual, are favored in new system architectures (such as PCI Express) and relieve this problem to a considerable extent.\n\nSome devices with a poorly designed programming interface provide no way to determine whether they have requested service. They may lock up or otherwise misbehave if serviced when they do not want it. Such devices cannot tolerate spurious interrupts, and so also cannot tolerate sharing an interrupt line. ISA cards, due to often cheap design and construction, are notorious for this problem. Such devices are becoming much rarer, as hardware logic becomes cheaper and new system architectures mandate shareable interrupts.\n\nInterrupts provide low overhead and good latency at low load, but degrade significantly at high interrupt rate unless care is taken to prevent several pathologies. These are various forms of livelocks, when the system spends all of its time processing interrupts to the exclusion of other required tasks. Under extreme conditions, a large number of interrupts (like very high network traffic) may completely stall the system. To avoid such problems, an operating system must schedule network interrupt handling as carefully as it schedules process execution.\n\nWith multi-core processors, additional performance improvements in interrupt handling can be achieved through receive-side scaling (RSS) when multiqueue NICs are used. Such NICs provide multiple receive queues associated to separate interrupts; by routing each of those interrupts to different cores, processing of the interrupt requests triggered by the network traffic received by a single NIC can be distributed among multiple cores. Distribution of the interrupts among cores can be performed automatically by the operating system, or the routing of interrupts (usually referred to as \"IRQ affinity\") can be manually configured.\n\nA purely software-based implementation of the receiving traffic distribution, known as \"receive packet steering\" (RPS), distributes received traffic among cores later in the data path, as part of the interrupt handler functionality. Advantages of RPS over RSS include no requirements for specific hardware, more advanced traffic distribution filters, and reduced rate of interrupts produced by a NIC. As a downside, RPS increases the rate of inter-processor interrupts (IPIs). \"Receive flow steering\" (RFS) takes the software-based approach further by accounting for application locality; further performance improvements are achieved by processing interrupt requests by the same cores on which particular network packets will be consumed by the targeted application.\n\nTypical uses of interrupts include the following: system timers, disk I/O, power-off signals, and traps. Other interrupts exist to transfer data bytes using UARTs or Ethernet; sense key-presses; control motors; or anything else the equipment must do.\n\nAnother typical use is to generate periodic interrupts by dividing the output of a crystal oscillator and having an interrupt handler count the interrupts in order for a processor to keep time. These periodic interrupts are often used by the OS's task scheduler to reschedule the priorities of running processes. Some older computers generated periodic interrupts from the power line frequency because it was controlled by the utilities to eliminate long-term drift of electric clocks.\n\nFor example, a disk interrupt signals the completion of a data transfer from or to the disk peripheral; a process waiting to read or write a file starts up again. As another example, a power-off interrupt predicts or requests a loss of power, allowing the computer equipment to perform an orderly shut-down. Also, interrupts are used in typeahead features for buffering events like keystrokes.\n\nInterrupts are used to allow emulation of instructions which are unimplemented on certain models in a computer line. For example floating point instructions may be implemented in hardware on some systems and emulated on lower-cost systems. Execution of an unimplemented instruction will cause an interrupt. The operating system interrupt handler will recognize the occurrence on an unimplemented instruction, interpret the instruction in a software routine, and then return to the interrupting program as if the instruction had been executed. This provides application software portability across the entire line.\n\n"}
{"id": "15290", "url": "https://en.wikipedia.org/wiki?curid=15290", "title": "Intercalation (timekeeping)", "text": "Intercalation (timekeeping)\n\nIntercalation or embolism in timekeeping is the insertion of a leap day, week, or month into some calendar years to make the calendar follow the seasons or moon phases. Lunisolar calendars may require intercalations of both days and months.\n\nThe solar or tropical year does not have a whole number of days (it is about 365.24 days), but a calendar year must have a whole number of days. The most common way to reconcile the two is to vary the number of days in the calendar year.\n\nIn solar calendars, this is done by adding to a common year of 365 days, an extra day (\"leap day\" or \"intercalary day\") about every four years, causing a leap year to have 366 days (Julian, Gregorian and Indian national calendars).\n\nThe Decree of Canopus, which was issued by the pharaoh Ptolemy III Euergetes of Ancient Egypt in 239 BCE, decreed a solar leap day system; an Egyptian leap year was not adopted until 25 BC, when the Roman Emperor Augustus successfully instituted a reformed Alexandrian calendar.\n\nIn the Julian calendar, as well as in the Gregorian calendar, which improved upon it, intercalation is done by adding an extra day to February in each leap year. In the Julian calendar this was done every four years. In the Gregorian, years divisible by 100 but not 400 were exempted in order to improve accuracy. Thus, 2000 was a leap year; 1700, 1800, and 1900 were not.\n\nEpagomenal days are days within a solar calendar that are outside any regular month. Usually five epagomenal days are included within every year (Egyptian, Coptic, Ethiopian, Mayan Haab' and French Republican Calendars), but a sixth epagomenal day is intercalated every four years in some (Coptic, Ethiopian and French Republican calendars).\n\nThe Bahá'í calendar includes enough epagomenal days (usually 4 or 5) before the last month (, \"ʿalāʾ\") to ensure that the following year starts on the March equinox. These are known as the Ayyám-i-Há.\n\nThe solar year does not have a whole number of lunar months (it is about 12.37 lunations), so a lunisolar calendar must have a variable number of months in a year. Regular years have 12 months, but embolismic years insert a 13th \"intercalary\" or \"leap\" or \"embolismic\" month every second or third year (see blue moon). Whether to insert an intercalary month in a given year may be determined using regular cycles such as the 19-year Metonic cycle (Hebrew calendar and in the determination of Easter) or using calculations of lunar phases (Hindu lunisolar and Chinese calendars). The Buddhist calendar adds both an intercalary day and month on a usually regular cycle. The Jewish year is arranged according to a luni-solar system.\n\nThe tabular Islamic calendar usually has 12 lunar months that alternate between 30 and 29 days every year, but an intercalary day is added to the last month of the year 11 times within a 30-year cycle. Some historians also linked the pre-Islamic practice of Nasi' to intercalation.\n\nThe Solar Hijri calendar is based on solar calculations and is similar to the Gregorian calendar in its structure, and hence the intercalation, with the exception that the year date starts with the Hegira.\n\nThe International Earth Rotation and Reference Systems Service can insert or remove leap seconds from the last day of any month (June and December are preferred). These are sometimes described as intercalary.\n\nISO 8601 includes a specification for a 52/53-week year. Any year that has 53 Thursdays has 53 weeks; this extra week may be regarded as intercalary.\n\n"}
{"id": "15291", "url": "https://en.wikipedia.org/wiki?curid=15291", "title": "Intercourse", "text": "Intercourse\n\nIntercourse may refer to:\n\n\n\n\n\n\n"}
{"id": "15292", "url": "https://en.wikipedia.org/wiki?curid=15292", "title": "Ink", "text": "Ink\n\nInk is a liquid or paste that contains pigments or dyes and is used to color a surface to produce an image, text, or design. Ink is used for drawing or writing with a pen, brush, or quill. Thicker inks, in paste form, are used extensively in letterpress and lithographic printing.\n\nInk can be a complex medium, composed of solvents, pigments, dyes, resins, lubricants, solubilizers, surfactants, particulate matter, fluorescents, and other materials. The components of inks serve many purposes; the ink's carrier, colorants, and other additives affect the flow and thickness of the ink and its dry appearance.\n\nIn 2011 worldwide consumption of printing inks generated revenues of more than 20 billion US dollars. Demand by traditional print media is shrinking, on the other hand more and more printing inks are consumed for packagings.\n\n Many ancient cultures around the world have independently discovered and formulated inks for the purposes of writing and drawing. The knowledge of the inks, their recipes and the techniques for their production comes from archaeological analysis or from written text itself.\n\nInk was used in Ancient Egypt for writing and drawing on papyrus from at least the 26th century BC.\n\nThe history of Chinese inks can be traced to the 23rd century BC, with the utilization of natural plant (plant dyes), animal, and mineral inks based on such materials as graphite that were ground with water and applied with ink brushes. Evidence for the earliest Chinese inks, similar to modern inksticks, is around 256 BC in the end of the Warring States period and produced from soot and animal glue. The best inks for drawing or painting on paper or silk are produced from the resin of the pine tree. They must be between 50 and 100 years old. The Chinese inkstick is produced with a fish glue, whereas Japanese glue (膠 \"nikawa\") is from cow or stag.\n\nThe process of making India ink was known in China as early as the middle of the 3rd millennium BC, during Neolithic China. India ink was first invented in China, although the source of materials to make the carbon pigment in India ink was later often traded from India, thus the term \"India ink\" was coined. The traditional Chinese method of making the ink was to grind a mixture of hide glue, carbon black, lampblack, and bone black pigment with a pestle and mortar, then pouring it into a ceramic dish where it could dry. To use the dry mixture, a wet brush would be applied until it reliquified. The manufacture of India ink was well-established by the Cao Wei Dynasty (220–265 AD). Indian documents written in Kharosthi with ink have been unearthed in Chinese Turkestan. The practice of writing with ink and a sharp pointed needle was common in early South India. Several Buddhist and Jain sutras in India were compiled in ink.\n\nIn ancient Rome, atramentum was used; in an article for the \"Christian Science Monitor\", Sharon J. Huntington describes these other historical inks:\n\nAbout 1,600 years ago, a popular ink recipe was created. The recipe was used for centuries. Iron salts, such as ferrous sulfate (made by treating iron with sulfuric acid), were mixed with tannin from gallnuts (they grow on trees) and a thickener. When first put to paper, this ink is bluish-black. Over time it fades to a dull brown.\n\nScribes in medieval Europe (about AD 800 to 1500) wrote principally on parchment or vellum. One 12th century ink recipe called for hawthorn branches to be cut in the spring and left to dry. Then the bark was pounded from the branches and soaked in water for eight days. The water was boiled until it thickened and turned black. Wine was added during boiling. The ink was poured into special bags and hung in the sun. Once dried, the mixture was mixed with wine and iron salt over a fire to make the final ink.\n\nThe reservoir pen, which may have been the first fountain pen, dates back to 953, when Ma'ād al-Mu'izz, the caliph of Egypt, demanded a pen that would not stain his hands or clothes, and was provided with a pen that held ink in a reservoir.\n\nIn the 15th century, a new type of ink had to be developed in Europe for the printing press by Johannes Gutenberg. According to Martyn Lyons in his book \"Books: A Living History\", Gutenberg's dye was indelible, oil-based, and made from the soot of lamps (lamp-black) mixed with varnish and egg white. Two types of ink were prevalent at the time: the Greek and Roman writing ink (soot, glue, and water) and the 12th century variety composed of ferrous sulfate, gall, gum, and water. Neither of these handwriting inks could adhere to printing surfaces without creating blurs. Eventually an oily, varnish-like ink made of soot, turpentine, and walnut oil was created specifically for the printing press.\n\nInk formulas vary, but commonly involve two components:\n\nInks generally fall into four classes:\n\nPigment inks are used more frequently than dyes because they are more color-fast, but they are also more expensive, less consistent in color, and have less of a color range than dyes.\n\nPigments are solid, opaque particles suspended in ink to provide color. Pigment molecules typically link together in crystalline structures that are 0.1–2 µm in size and comprise 5–30 percent of the ink volume. Qualities such as hue, saturation, and lightness vary depending on the source and type of pigment.\n\nDye-based inks are generally much stronger than pigment-based inks and can produce much more color of a given density per unit of mass. However, because dyes are dissolved in the liquid phase, they have a tendency to soak into paper, making the ink less efficient and potentially allowing the ink to bleed at the edges of an image.\n\nTo circumvent this problem, dye-based inks are made with solvents that dry rapidly or are used with quick-drying methods of printing, such as blowing hot air on the fresh print. Other methods include harder paper sizing and more specialized paper coatings. The latter is particularly suited to inks used in non-industrial settings (which must conform to tighter toxicity and emission controls), such as inkjet printer inks. Another technique involves coating the paper with a charged coating. If the dye has the opposite charge, it is attracted to and retained by this coating, while the solvent soaks into the paper. Cellulose, the wood-derived material most paper is made of, is naturally charged, and so a compound that complexes with both the dye and the paper's surface aids retention at the surface. Such a compound is commonly used in ink-jet printing inks.\n\nAn additional advantage of dye-based ink systems is that the dye molecules can interact with other ink ingredients, potentially allowing greater benefit as compared to pigmented inks from optical brighteners and color-enhancing agents designed to increase the intensity and appearance of dyes.\n\nA more recent development in dye-based inks are dyes that react with cellulose to permanently color the paper. Such inks are not affected by water, alcohol, and other solvents. As such, their use is recommended to prevent frauds that involve removing signatures, such as check washing. This kind of ink is most commonly found in gel inks and in certain fountain pen inks.\n\nThere is a misconception that ink is non-toxic even if swallowed. Once ingested, ink can be hazardous to one's health. Certain inks, such as those used in digital printers, and even those found in a common pen can be harmful. Though ink does not easily cause death, repeated skin contact or ingestion can cause effects such as severe headaches, skin irritation, or nervous system damage. These effects can be caused by solvents, or by pigment ingredients such as \"p\"-Anisidine, which helps create some inks' color and shine.\n\nThree main environmental issues with ink are:\n\nSome regulatory bodies have set standards for the amount of heavy metals in ink. There is a trend toward vegetable oils rather than petroleum oils in recent years in response to a demand for better environmental sustainability performance.\n\nInk uses up non-renewable oils and metals, which have a negative impact on the environment.\n\nCarbon inks were commonly made from lampblack or soot and a binding agent such as gum arabic or animal glue. The binding agent keeps carbon particles in suspension and adhered to paper. Carbon particles do not fade over time even when bleached or when in sunlight. One benefit is that carbon ink does not harm paper. Over time, the ink is chemically stable and therefore does not threaten the paper's strength. Despite these benefits, carbon ink is not ideal for permanence and ease of preservation. Carbon ink tends to smudge in humid environments and can be washed off surfaces. The best method of preserving a document written in carbon ink is to store it in a dry environment (Barrow 1972).\n\nRecently, carbon inks made from carbon nanotubes have been successfully created. They are similar in composition to traditional inks in that they use a polymer to suspend the carbon nanotubes. These inks can be used in inkjet printers and produce electrically conductive patterns.\n\nIron gall inks became prominent in the early 12th century; they were used for centuries and were widely thought to be the best type of ink. However, iron gall ink is corrosive and damages paper over time (Waters 1940). Items containing this ink can become brittle and the writing fades to brown. The original scores of Johann Sebastian Bach are threatened by the destructive properties of iron gall ink. The majority of his works are held by the German State Library, and about 25% of those are in advanced stages of decay (American Libraries 2000). The rate at which the writing fades is based on several factors, such as proportions of ink ingredients, amount deposited on the paper, and paper composition (Barrow 1972:16). Corrosion is caused by acid catalysed hydrolysis and iron(II)-catalysed oxidation of cellulose (Rouchon-Quillet 2004:389).\n\nTreatment is a controversial subject. No treatment undoes damage already caused by acidic ink. Deterioration can only be stopped or slowed. Some think it best not to treat the item at all for fear of the consequences. Others believe that non-aqueous procedures are the best solution. Yet others think an aqueous procedure may preserve items written with iron gall ink. Aqueous treatments include distilled water at different temperatures, calcium hydroxide, calcium bicarbonate, magnesium carbonate, magnesium bicarbonate, and calcium phytate. There are many possible side effects from these treatments. There can be mechanical damage, which further weakens the paper. Paper color or ink color may change, and ink may bleed. Other consequences of aqueous treatment are a change of ink texture or formation of plaque on the surface of the ink (Reibland & de Groot 1999).\n\nIron gall inks require storage in a stable environment, because fluctuating relative humidity increases the rate that formic acid, acetic acid, and furan derivatives form in the material the ink was used on. Sulfuric acid acts as a catalyst to cellulose hydrolysis, and iron (II) sulfate acts as a catalyst to cellulose oxidation. These chemical reactions physically weaken the paper, causing brittleness.\n\n\"Indelible\" means \"unremovable\". Some types of indelible ink have a very short shelf life because of the quickly evaporating solvents used. India, Mexico, Indonesia, Malaysia and other developing countries have used indelible ink in the form of electoral stain to prevent electoral fraud. The Indian Scientist Dr. M.L. Goel is the founding father of indelible ink in India and gave the secret formula to NPL (National Physical Laboratory) of India.\n\nThe Election Commission in India has used indelible ink for many elections. Indonesia used it in its last election in Aceh. In Mali, the ink is applied to the fingernail. Indelible ink itself is not infallible as it can be used to commit electoral fraud by marking opponent party members before they have chances to cast their votes. There are also reports of \"indelible\" ink washing off voters' fingers in Afghanistan.\n\n\n\n\n\n"}
{"id": "15294", "url": "https://en.wikipedia.org/wiki?curid=15294", "title": "Islamabad Capital Territory", "text": "Islamabad Capital Territory\n\nIslamabad Capital Territory (, or ICT) is the one and only federal territory of Pakistan. The territory is bounded by Punjab on the south, west and east and by Khyber Pakhtunkhwa on the north. The territory includes Islamabad, the federal capital of Pakistan, which covers 906 km (349.8 mi) out of the total of 1165.5 km (450 mi). The territory is represented in the National Assembly constituencies NA-52, NA-53 and NA-54.\n\nIn 1960, land was transferred from Rawalpindi District of Punjab province to establish Pakistan's new capital. According to the 1960 master plan, the Capital Territory included Rawalpindi, and was to be composed of the following parts:\n\nHowever, Rawalpindi was eventually excluded from the Islamabad master plan in the 1980s. \n\nIslamabad is subdivided into five zones:\n\nIslamabad Capital Territory comprises Islamabad urban and rural areas. The Islamabad Rural consists of 23 Union Councils, comprising 133 villages, while Islamabad Urban has 27 Union Councils.\n\nThe climate of Islamabad has a humid subtropical climate (Köppen: Cwa), with five seasons: Winter (November–February), Spring (March and April), Summer (May and June), Rainy Monsoon (July and August) and Autumn (September and October). The hottest month is June, where average highs routinely exceed . Wettest month is July, with heavy rainfalls and evening thunderstorms with the possibility of cloudburst and flooding. Coolest month is January. Islamabad's micro-climate is regulated by three artificial reservoirs: Rawal, Simli, and Khanpur Dam. Last one is located on the Haro River near the town of Khanpur, about from Islamabad. Simli Dam is north of Islamabad. of the city consists of Margalla Hills National Park. Loi Bher Forest is situated along the Islamabad Highway, covering an area of . Highest monthly rainfall of was recorded during July 1995. Winters generally feature dense fog in the mornings and sunny afternoons. In the city, temperatures stay mild, with snowfall over the higher elevations points on nearby hill stations, notably Murree and Nathia Gali. The temperatures range from in January to in June. The highest recorded temperature was on 23 June 2005 while the lowest temperature was on 17 January 1967. The city has \"recorded\" snowfall. On 23 July 2001, Islamabad received a record breaking of rainfall in just 10 hours. It was the heaviest rainfall in Islamabad in the past 100 years and the highest rainfall in 24 hours as well.\n\nThe main administrative authority of the city is Islamabad Capital Territory (ICT) Administration with some help from Capital Development Authority (CDA), which oversees the planning, development, construction, and administration of the city. Islamabad Capital Territory is divided into eight zones: Administrative Zone, Commercial District, Educational Sector, Industrial Sector, Diplomatic Enclave, Residential Areas, Rural Areas and Green Area. \n\nIslamabad city is divided into five major zones: Zone I, Zone II, Zone III, Zone IV, and Zone V. Out of these, Zone IV is the largest in area. All sectors of ghouri town (1, 2, 3, VIP, 5, 4-A, 4-B, 4-C, 5-A, 5-B and sector 7) are located in this zone. Zone I consists mainly of all the developed residential sectors, while Zone II consists of the under-developed residential sectors. Each residential sector is identified by a letter of the alphabet and a number, and covers an area of approximately 4 square kilometers. The sectors are lettered from A to I, and each sector is divided into four numbered sub-sectors.\n\nSeries A, B, and C are still underdeveloped. The D series has seven sectors (D-11 to D-17), of which only sector D-12 is completely developed. This series is located at the foot of Margalla Hills. The E Sectors are named from E-7 to E-17. Many foreigners and diplomatic personnel are housed in these sectors. In the revised Master Plan of the city, CDA has decided to develop a park on the pattern of Fatima Jinnah Park in sector E-14. Sectors E-8 and E-9 contain the campuses of Bahria University, Air University, and the National Defence University. The F and G series contains the most developed sectors. F series contains sectors F-5 to F-17; some sectors are still under-developed. F-5 is an important sector for the software industry in Islamabad, as the two software technology parks are located here. The entire F-9 sector is covered with Fatima Jinnah Park. The Centaurus complex will be one of the major landmarks of the F-8 sector. G sectors are numbered G-5 through G-17. Some important places include the Jinnah Convention Center and Serena Hotel in G-5, the Red Mosque in G-6, and the Pakistan Institute of Medical Sciences, the largest medical complex in the capital, located in G-8.\n\nThe H sectors are numbered H-8 through H-17. The H sectors are mostly dedicated to educational and health institutions. National University of Sciences and Technology covers a major portion of sector H-12. The I sectors are numbered from I-8 to I-18. With the exception of I-8, which is a well-developed residential area, these sectors are primarily part of the industrial zone. Currently two sub-sectors of I-9 and one sub-sector of I-10 are used as industrial areas. CDA is planning to set up Islamabad Railway Station in Sector I-18 and Industrial City in sector I-17. Zone III consists primarily of the Margalla Hills and Margalla Hills National Park. Rawal Lake is in this zone. Zone IV and V consist of Islamabad Park, and rural areas of the city. The Soan River flows into the city through Zone V.\n\nWhile urban Islamabad is home to people from all over Pakistan as well as expatriates, in the rural areas a number of Pothohari speaking tribal communities can still be recognized.\n\nWhen the master plan for Islamabad was drawn up in 1960, Islamabad and Rawalpindi, along with the adjoining areas, was to be integrated to form a large metropolitan area called Islamabad/Rawalpindi Metropolitan Area. The area would consist of the developing Islamabad, the old colonial cantonment city of Rawalpindi, and Margalla Hills National Park, including surrounding rural areas. However, Islamabad city is part of the Islamabad Capital Territory, while Rawalpindi is part of Rawalpindi District, which is part of province of Punjab.\n\nInitially, it was proposed that the three areas would be connected by four major highways: Murree Highway, Islamabad Highway, Soan Highway, and Capital Highway. However, to date only two highways have been constructed: Kashmir Highway (the former Murree Highway) and Islamabad Highway. Plans of constructing Margalla Avenue are also underway. Islamabad is the hub all the governmental activities while Rawalpindi is the centre of all industrial, commercial, and military activities. The two cities are considered sister cities and are highly interdependent.\nIslamabad is a net contributor to the Pakistani economy, as whilst having only 0.8% of the country's population, it contributes 1% to the country's GDP. Islamabad Stock Exchange, founded in 1989, is Pakistan's third largest stock exchange after Karachi Stock Exchange and Lahore Stock Exchange. The exchange has 118 members with 104 corporate bodies and 18 individual members. The average daily turnover of the stock exchange is over 1 million shares. As of 2012, Islamabad LTU (Large Tax Unit) was responsible for Rs 371 billion in tax revenue, which amounts to 20% of all the revenue collected by Federal Board of Revenue.\n\nIslamabad has seen an expansion in information and communications technology with the addition two Software Technology Parks, which house numerous national and foreign technological and information technology companies. The tech parks are located in Evacuee Trust Complex and Awami Markaz. Awami Markaz houses 36 IT companies while Evacuee Trust house 29 companies. Call centres for foreign companies have been targeted as another significant area of growth, with the government making efforts to reduce taxes by as much as 10% to encourage foreign investments in the information technology sector. Most of Pakistan's state-owned companies like PIA, PTV, PTCL, OGDCL, and Zarai Taraqiati Bank Ltd. are based in Islamabad. Headquarters of all major telecommunication operators such as PTCL, Mobilink, Telenor, Ufone, and China Mobile are located in Islamabad.\n\n\nThe Rawalpindi-Islamabad Metrobus is a bus rapid transit system that serves the twin cities of Rawalpindi and Islamabad in Pakistan. It uses dedicated bus lanes for all of its route covering 24 bus stations. Islamabad is well connected with other parts of the country through car rental services such as Alvi Transport Network and Pakistan Car Rentals.\n\nAll major cities and towns are accessible through regular trains and bus services running mostly from the neighbouring city of Rawalpindi. Lahore and Peshawar are linked to Islamabad through a network of motorways, which has significantly reduced travelling times between these cities. M-2 Motorway is long and connect Islamabad and Lahore. M-1 Motorway connects Islamabad with Peshawar and is long. Islamabad is linked to Rawalpindi through the Faizabad Interchange, which has a daily traffic volume of about 48,000 vehicles.\n\nIslamabad has the highest literacy rate of Pakistan at 95%. Islamabad also has some of Pakistan's major universities, including Quaid-i-Azam University, the International Islamic University, and the National University of Sciences and Technology and Pakistan Institute of Engineering and Applied Sciences\n\nPrivate School Network Islamabad is working for private educational institutions. The president of PSN is Dr. Muhammad Afzal Babur from Bhara Kahu. PSN is divided into eight zones in Islamabad. In Tarlai Zone Chaudhary Faisal Ali from Faisal Academy Tarlai Kalan is Zonal General Sectary of PSN.\n\nQuaid-e-Azam University has several faculties. The institute is located in a semi-hilly area, east of the Secretariat buildings and near the base of Margalla Hills. This Post-Graduate institute is spread over . The nucleus of the campus has been designed as an axial spine with a library as its center.\nOther universities include the following:\n\n\nIslamabad United became the first ever team to win Pakistan Super League in 2016. And now the federal team Is participating in the Pakistan Cup. The team is under the captaincy of Misbah-ul-Haq, former captain of Pakistan, The Islamabad United was also under Misbah.\n\n\n\n"}
{"id": "15295", "url": "https://en.wikipedia.org/wiki?curid=15295", "title": "Intelligent design", "text": "Intelligent design\n\nIntelligent design (ID) is a pseudoscientific argument for the existence of God, presented by its proponents as \"an evidence-based scientific theory about life's origins\". Proponents claim that \"certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection.\" ID is a form of creationism that lacks empirical support and offers no testable or tenable hypotheses, so it is not science. The leading proponents of ID are associated with the Discovery Institute, a fundamentalist Christian and politically conservative think tank based in the United States.\n\nThough the phrase \"intelligent design\" had featured previously in theological discussions of the design argument, the first publication of the term \"intelligent design\" in its present use as an alternative term for creationism was in \"Of Pandas and People\", a 1989 creationist textbook intended for high school biology classes. The term was substituted into drafts of the book, directly replacing references to \"creation science\" and \"creationism\", after the 1987 United States Supreme Court's \"Edwards v. Aguillard\" decision, which barred the teaching of \"creation science\" in public schools on constitutional grounds. From the mid-1990s, the intelligent design movement (IDM), supported by the Discovery Institute, advocated inclusion of intelligent design in public school biology curricula. This led to the 2005 \"Kitzmiller v. Dover Area School District\" trial in which U.S. District Judge John E. Jones III found that intelligent design was not science, that it \"cannot uncouple itself from its creationist, and thus religious, antecedents,\" and that the school district's promotion of it therefore violated the Establishment Clause of the First Amendment to the United States Constitution.\n\nID presents two main arguments against evolutionary explanations: irreducible complexity and specified complexity. These arguments assert that certain features (biological and informational, respectively) are too complex to be the result of natural processes. As a positive argument against evolution, ID proposes an analogy between natural systems and human artifacts, a version of the theological argument from design for the existence of God. ID proponents then conclude by analogy that the complex features, as defined by ID, are evidence of design.\n\nDetailed scientific examination has rebutted the claims that evolutionary explanations are inadequate, and this premise of intelligent design—that evidence against evolution constitutes evidence for design—is a false dichotomy. It is asserted that ID challenges the methodological naturalism inherent in modern science though proponents concede that they have yet to produce a scientific theory.\n\nBy 1910 evolution was not a topic of major religious controversy in America, but in the 1920s the Fundamentalist–Modernist Controversy in theology resulted in Fundamentalist Christian opposition to teaching evolution, and the origins of modern creationism. Teaching of evolution was effectively suspended in U.S. public schools until the 1960s, and when evolution was then reintroduced into the curriculum, there was a series of court cases in which attempts were made to get creationism taught alongside evolution in science classes. Young Earth creationists (YEC) promoted creation science as \"an alternative scientific explanation of the world in which we live\". This frequently invoked the argument from design to explain complexity in nature as demonstrating the existence of God.\n\nThe argument from design, the teleological argument or \"argument from intelligent design\", has been advanced in theology for centuries. It can be summarised briefly as \"Wherever complex design exists, there must have been a designer; nature is complex; therefore nature must have had an intelligent designer.\" Thomas Aquinas presented it in his fifth proof of God's existence as a syllogism. In 1802, William Paley's \"Natural Theology\" presented examples of intricate purpose in organisms. His version of the watchmaker analogy argued that, in the same way that a watch has evidently been designed by a craftsman, complexity and adaptation seen in nature must have been designed, and the perfection and diversity of these designs shows the designer to be omnipotent, the Christian God. Like creation science, intelligent design centers on Paley's religious argument from design, but while Paley's natural theology was open to deistic design through God-given laws, intelligent design seeks scientific confirmation of repeated miraculous interventions in the history of life. Creation science prefigured the intelligent design arguments of irreducible complexity, even featuring the bacterial flagellum. In the United States, attempts to introduce creation science in schools led to court rulings that it is religious in nature, and thus cannot be taught in public school science classrooms. Intelligent design is also presented as science, and shares other arguments with creation science but avoids literal Biblical references to such things as the Flood story from the Book of Genesis or using Bible verses to age the Earth.\n\nBarbara Forrest writes that the intelligent design movement began in 1984 with the book \"The Mystery of Life's Origin: Reassessing Current Theories\", co-written by creationist Charles B. Thaxton, a chemist, with two other authors, and published by Jon A. Buell's Foundation for Thought and Ethics. Thaxton held a conference in 1988, \"Sources of Information Content in DNA\", which attracted creationists such as Stephen C. Meyer.\n\nIn March 1986, a review by Meyer used information theory to suggest that messages transmitted by DNA in the cell show \"specified complexity\" specified by intelligence, and must have originated with an intelligent agent. In November of that year, Thaxton described his reasoning as a more sophisticated form of Paley's argument from design. At the \"Sources of Information Content in DNA\" conference in 1988, he said that his intelligent cause view was compatible with both metaphysical naturalism and supernaturalism.\n\nIntelligent design avoids identifying or naming the intelligent designer—it merely states that one (or more) must exist—but leaders of the movement have said the designer is the Christian God. Whether this lack of specificity about the designer's identity in public discussions is a genuine feature of the concept, or just a posture taken to avoid alienating those who would separate religion from the teaching of science, has been a matter of great debate between supporters and critics of intelligent design. The \"Kitzmiller v. Dover Area School District\" court ruling held the latter to be the case.\n\nSince the Middle Ages, discussion of the religious \"argument from design\" or \"teleological argument\" in theology, with its concept of \"intelligent design\", has persistently referred to the theistic Creator God. Although ID proponents chose this provocative label for their proposed alternative to evolutionary explanations, they have de-emphasized their religious antecedents and denied that ID is natural theology, while still presenting ID as supporting the argument for the existence of God.\n\nWhile intelligent design proponents have pointed out past examples of the phrase \"intelligent design\" that they said were not creationist and faith-based, they have failed to show that these usages had any influence on those who introduced the label in the intelligent design movement.\n\nVariations on the phrase appeared in Young Earth creationist publications: a 1967 book co-written by Percival Davis referred to \"design according to which basic organisms were created\". In 1970, A. E. Wilder-Smith published \"The Creation of Life: A Cybernetic Approach to Evolution\" which defended Paley's design argument with computer calculations of the improbability of genetic sequences, which he said could not be explained by evolution but required \"the abhorred necessity of divine intelligent activity behind nature\", and that \"the same problem would be expected to beset the relationship between the designer behind nature and the intelligently designed part of nature known as man.\" In a 1984 article as well as in his affidavit to \"Edwards v. Aguillard\", Dean H. Kenyon defended creation science by stating that \"biomolecular systems require intelligent design and engineering know-how\", citing Wilder-Smith. Creationist Richard B. Bliss used the phrase \"creative design\" in \"Origins: Two Models: Evolution, Creation\" (1976), and in \"Origins: Creation or Evolution\" (1988) wrote that \"while evolutionists are trying to find non-intelligent ways for life to occur, the creationist insists that an intelligent design must have been there in the first place.\" The first systematic use of the term, defined in a glossary and claimed to be other than creationism, was in \"Of Pandas and People\", co-authored by Davis and Kenyon.\n\nThe most common modern use of the words \"intelligent design\" as a term intended to describe a field of inquiry began after the United States Supreme Court ruled in 1987 in the case of \"Edwards v. Aguillard\" that it is unconstitutional for a state to require the teaching of creationism in public school science curricula.\n\nA Discovery Institute report says that Charles B. Thaxton, editor of \"Pandas\", had picked the phrase up from a NASA scientist, and thought, \"That's just what I need, it's a good engineering term.\" In drafts of the book, over one hundred uses of the root word \"creation\", such as \"creationism\" and \"Creation Science\", were changed, almost without exception, to \"intelligent design\", while \"creationists\" was changed to \"design proponents\" or, in one instance, \"cdesign proponentsists\". In June 1988, Thaxton held a conference titled \"Sources of Information Content in DNA\" in Tacoma, Washington, and in December decided to use the label \"intelligent design\" for his new creationist movement. Stephen C. Meyer was at the conference, and later recalled that \"The term \"intelligent design\" came up...\"\n\n\"Of Pandas and People\" was published in 1989, and in addition to including all the current arguments for ID, was the first book to make systematic use of the terms \"intelligent design\" and \"design proponents\" as well as the phrase \"design theory\", defining the term \"intelligent design\" in a glossary and representing it as not being creationism. It thus represents the start of the modern intelligent design movement. \"Intelligent design\" was the most prominent of around fifteen new terms it introduced as a new lexicon of creationist terminology to oppose evolution without using religious language. It was the first place where the phrase \"intelligent design\" appeared in its primary present use, as stated both by its publisher Jon A. Buell, and by William A. Dembski in his expert witness report for \"Kitzmiller v. Dover Area School District\".\n\nThe National Center for Science Education (NCSE) has criticized the book for presenting all of the basic arguments of intelligent design proponents and being actively promoted for use in public schools before any research had been done to support these arguments. Although presented as a scientific textbook, philosopher of science Michael Ruse considers the contents \"worthless and dishonest\". An American Civil Liberties Union lawyer described it as a political tool aimed at students who did not \"know science or understand the controversy over evolution and creationism\". One of the authors of the science framework used by California schools, Kevin Padian, condemned it for its \"sub-text\", \"intolerance for honest science\" and \"incompetence\".\n\nThe term \"irreducible complexity\" was introduced by biochemist Michael Behe in his 1996 book \"Darwin's Black Box\", though he had already described the concept in his contributions to the 1993 revised edition of \"Of Pandas and People\". Behe defines it as \"a single system which is composed of several well-matched interacting parts that contribute to the basic function, wherein the removal of any one of the parts causes the system to effectively cease functioning\".\n\nBehe uses the analogy of a mousetrap to illustrate this concept. A mousetrap consists of several interacting pieces—the base, the catch, the spring and the hammer—all of which must be in place for the mousetrap to work. Removal of any one piece destroys the function of the mousetrap. Intelligent design advocates assert that natural selection could not create irreducibly complex systems, because the selectable function is present only when all parts are assembled. Behe argued that irreducibly complex biological mechanisms include the bacterial flagellum of \"E. coli\", the blood clotting cascade, cilia, and the adaptive immune system.\n\nCritics point out that the irreducible complexity argument assumes that the necessary parts of a system have always been necessary and therefore could not have been added sequentially. They argue that something that is at first merely advantageous can later become necessary as other components change. Furthermore, they argue, evolution often proceeds by altering preexisting parts or by removing them from a system, rather than by adding them. This is sometimes called the \"scaffolding objection\" by an analogy with scaffolding, which can support an \"irreducibly complex\" building until it is complete and able to stand on its own.\nBehe has acknowledged using \"sloppy prose\", and that his \"argument against Darwinism does not add up to a logical proof.\" Irreducible complexity has remained a popular argument among advocates of intelligent design; in the Dover trial, the court held that \"Professor Behe's claim for irreducible complexity has been refuted in peer-reviewed research papers and has been rejected by the scientific community at large.\"\n\nIn 1986, Charles B. Thaxton, a physical chemist and creationist, used the term \"specified complexity\" from information theory when claiming that messages transmitted by DNA in the cell were specified by intelligence, and must have originated with an intelligent agent.\nThe intelligent design concept of \"specified complexity\" was developed in the 1990s by mathematician, philosopher, and theologian William A. Dembski. Dembski states that when something exhibits specified complexity (i.e., is both complex and \"specified\", simultaneously), one can infer that it was produced by an intelligent cause (i.e., that it was designed) rather than being the result of natural processes. He provides the following examples: \"A single letter of the alphabet is specified without being complex. A long sentence of random letters is complex without being specified. A Shakespearean sonnet is both complex and specified.\" He states that details of living things can be similarly characterized, especially the \"patterns\" of molecular sequences in functional biological molecules such as DNA.\nDembski defines complex specified information (CSI) as anything with a less than 1 in 10 chance of occurring by (natural) chance. Critics say that this renders the argument a tautology: complex specified information cannot occur naturally because Dembski has defined it thus, so the real question becomes whether or not CSI actually exists in nature.\n\nThe conceptual soundness of Dembski's specified complexity/CSI argument has been discredited in the scientific and mathematical communities. Specified complexity has yet to be shown to have wide applications in other fields, as Dembski asserts. John Wilkins and Wesley R. Elsberry characterize Dembski's \"explanatory filter\" as \"eliminative\" because it eliminates explanations sequentially: first regularity, then chance, finally defaulting to design. They argue that this procedure is flawed as a model for scientific inference because the asymmetric way it treats the different possible explanations renders it prone to making false conclusions.\n\nRichard Dawkins, another critic of intelligent design, argues in \"The God Delusion\" (2006) that allowing for an intelligent designer to account for unlikely complexity only postpones the problem, as such a designer would need to be at least as complex. Other scientists have argued that evolution through selection is better able to explain the observed complexity, as is evident from the use of selective evolution to design certain electronic, aeronautic and automotive systems that are considered problems too complex for human \"intelligent designers\".\n\nIntelligent design proponents have also occasionally appealed to broader teleological arguments outside of biology, most notably an argument based on the fine-tuning of universal constants that make matter and life possible and which are argued not to be solely attributable to chance. These include the values of fundamental physical constants, the relative strength of nuclear forces, electromagnetism, and gravity between fundamental particles, as well as the ratios of masses of such particles. Intelligent design proponent and Center for Science and Culture fellow Guillermo Gonzalez argues that if any of these values were even slightly different, the universe would be dramatically different, making it impossible for many chemical elements and features of the Universe, such as galaxies, to form. Thus, proponents argue, an intelligent designer of life was needed to ensure that the requisite features were present to achieve that particular outcome.\n\nScientists have generally responded that these arguments are poorly supported by existing evidence. Victor J. Stenger and other critics say both intelligent design and the weak form of the anthropic principle are essentially a tautology; in his view, these arguments amount to the claim that life is able to exist because the Universe is able to support life. The claim of the improbability of a life-supporting universe has also been criticized as an argument by lack of imagination for assuming no other forms of life are possible. Life as we know it might not exist if things were different, but a different sort of life might exist in its place. A number of critics also suggest that many of the stated variables appear to be interconnected and that calculations made by mathematicians and physicists suggest that the emergence of a universe similar to ours is quite probable.\n\nThe contemporary intelligent design movement formulates its arguments in secular terms and intentionally avoids identifying the intelligent agent (or agents) they posit. Although they do not state that God is the designer, the designer is often implicitly hypothesized to have intervened in a way that only a god could intervene. Dembski, in \"The Design Inference\" (1998), speculates that an alien culture could fulfill these requirements. \"Of Pandas and People\" proposes that SETI illustrates an appeal to intelligent design in science. In 2000, philosopher of science Robert T. Pennock suggested the Raëlian UFO religion as a real-life example of an extraterrestrial intelligent designer view that \"make[s] many of the same bad arguments against evolutionary theory as creationists\". The authoritative description of intelligent design, however, explicitly states that the \"Universe\" displays features of having been designed. Acknowledging the paradox, Dembski concludes that \"no intelligent agent who is strictly physical could have presided over the origin of the universe or the origin of life.\" The leading proponents have made statements to their supporters that they believe the designer to be the Christian God, to the exclusion of all other religions.\n\nBeyond the debate over whether intelligent design is scientific, a number of critics argue that existing evidence makes the design hypothesis appear unlikely, irrespective of its status in the world of science. For example, Jerry Coyne asks why a designer would \"give us a pathway for making vitamin C, but then destroy it by disabling one of its enzymes\" (see pseudogene) and why a designer would not \"stock oceanic islands with reptiles, mammals, amphibians, and freshwater fish, despite the suitability of such islands for these species\". Coyne also points to the fact that \"the flora and fauna on those islands resemble that of the nearest mainland, even when the environments are very different\" as evidence that species were not placed there by a designer. Previously, in \"Darwin's Black Box\", Behe had argued that we are simply incapable of understanding the designer's motives, so such questions cannot be answered definitively. Odd designs could, for example, \"...have been placed there by the designer for a reason—for artistic reasons, for variety, to show off, for some as-yet-undetected practical purpose, or for some unguessable reason—or they might not.\" Coyne responds that in light of the evidence, \"either life resulted not from intelligent design, but from evolution; or the intelligent designer is a cosmic prankster who designed everything to make it look as though it had evolved.\"\n\nIntelligent design proponents such as Paul Nelson avoid the problem of poor design in nature by insisting that we have simply failed to understand the perfection of the design. Behe cites Paley as his inspiration, but he differs from Paley's expectation of a perfect Creation and proposes that designers do not necessarily produce the best design they can. Behe suggests that, like a parent not wanting to spoil a child with extravagant toys, the designer can have multiple motives for not giving priority to excellence in engineering. He says that \"Another problem with the argument from imperfection is that it critically depends on a psychoanalysis of the unidentified designer. Yet the reasons that a designer would or would not do anything are virtually impossible to know unless the designer tells you specifically what those reasons are.\" This reliance on inexplicable motives of the designer makes intelligent design scientifically untestable. Retired UC Berkeley law professor, author and intelligent design advocate Phillip E. Johnson puts forward a core definition that the designer creates for a purpose, giving the example that in his view AIDS was created to punish immorality and is not caused by HIV, but such motives cannot be tested by scientific methods.\n\nAsserting the need for a designer of complexity also raises the question \"What designed the designer?\" Intelligent design proponents say that the question is irrelevant to or outside the scope of intelligent design. Richard Wein counters that \"...scientific explanations often create new unanswered questions. But, in assessing the value of an explanation, these questions are not irrelevant. They must be balanced against the improvements in our understanding which the explanation provides. Invoking an unexplained being to explain the origin of other beings (ourselves) is little more than question-begging. The new question raised by the explanation is as problematic as the question which the explanation purports to answer.\" Richard Dawkins sees the assertion that the designer does not need to be explained as a thought-terminating cliché. In the absence of observable, measurable evidence, the very question \"What designed the designer?\" leads to an infinite regression from which intelligent design proponents can only escape by resorting to religious creationism or logical contradiction.\n\nThe intelligent design movement is a direct outgrowth of the creationism of the 1980s. The scientific and academic communities, along with a U.S. federal court, view intelligent design as either a form of creationism or as a direct descendant that is closely intertwined with traditional creationism; and several authors explicitly refer to it as \"intelligent design creationism\".\n\nThe movement is headquartered in the Center for Science and Culture, established in 1996 as the creationist wing of the Discovery Institute to promote a religious agenda calling for broad social, academic and political changes. The Discovery Institute's intelligent design campaigns have been staged primarily in the United States, although efforts have been made in other countries to promote intelligent design. Leaders of the movement say intelligent design exposes the limitations of scientific orthodoxy and of the secular philosophy of naturalism. Intelligent design proponents allege that science should not be limited to naturalism and should not demand the adoption of a naturalistic philosophy that dismisses out-of-hand any explanation that includes a supernatural cause. The overall goal of the movement is to \"reverse the stifling dominance of the materialist worldview\" represented by the theory of evolution in favor of \"a science consonant with Christian and theistic convictions\".\n\nPhillip E. Johnson stated that the goal of intelligent design is to cast creationism as a scientific concept. All leading intelligent design proponents are fellows or staff of the Discovery Institute and its Center for Science and Culture. Nearly all intelligent design concepts and the associated movement are the products of the Discovery Institute, which guides the movement and follows its wedge strategy while conducting its \"Teach the Controversy\" campaign and their other related programs.\n\nLeading intelligent design proponents have made conflicting statements regarding intelligent design. In statements directed at the general public, they say intelligent design is not religious; when addressing conservative Christian supporters, they state that intelligent design has its foundation in the Bible. Recognizing the need for support, the Institute affirms its Christian, evangelistic orientation:\n\nBarbara Forrest, an expert who has written extensively on the movement, describes this as being due to the Discovery Institute's obfuscating its agenda as a matter of policy. She has written that the movement's \"activities betray an aggressive, systematic agenda for promoting not only intelligent design creationism, but the religious worldview that undergirds it.\"\n\nAlthough arguments for intelligent design by the intelligent design movement are formulated in secular terms and intentionally avoid positing the identity of the designer, the majority of principal intelligent design advocates are publicly religious Christians who have stated that, in their view, the designer proposed in intelligent design is the Christian conception of God. Stuart Burgess, Phillip E. Johnson, William A. Dembski, and Stephen C. Meyer are evangelical Protestants; Michael Behe is a Roman Catholic; and Jonathan Wells is a member of the Unification Church. Non-Christian proponents include David Klinghoffer, who is Jewish, Michael Denton and David Berlinski, who are agnostic, and Muzaffar Iqbal, a Pakistani-Canadian Muslim. Phillip E. Johnson has stated that cultivating ambiguity by employing secular language in arguments that are carefully crafted to avoid overtones of theistic creationism is a necessary first step for ultimately reintroducing the Christian concept of God as the designer. Johnson explicitly calls for intelligent design proponents to obfuscate their religious motivations so as to avoid having intelligent design identified \"as just another way of packaging the Christian evangelical message.\" Johnson emphasizes that \"...the first thing that has to be done is to get the Bible out of the discussion. ...This is not to say that the biblical issues are unimportant; the point is rather that the time to address them will be after we have separated materialist prejudice from scientific fact.\"\n\nThe strategy of deliberately disguising the religious intent of intelligent design has been described by William A. Dembski in \"The Design Inference\". In this work, Dembski lists a god or an \"alien life force\" as two possible options for the identity of the designer; however, in his book \"Intelligent Design: The Bridge Between Science and Theology\" (1999), Dembski states:\n\nDembski also stated, \"ID is part of God's general revelation [...] Not only does intelligent design rid us of this ideology materialism , which suffocates the human spirit, but, in my personal experience, I've found that it opens the path for people to come to Christ.\" Both Johnson and Dembski cite the Bible's Gospel of John as the foundation of intelligent design.\n\nBarbara Forrest contends such statements reveal that leading proponents see intelligent design as essentially religious in nature, not merely a scientific concept that has implications with which their personal religious beliefs happen to coincide. She writes that the leading proponents of intelligent design are closely allied with the ultra-conservative Christian Reconstructionism movement. She lists connections of (current and former) Discovery Institute Fellows Phillip E. Johnson, Charles B. Thaxton, Michael Behe, Richard Weikart, Jonathan Wells and Francis J. Beckwith to leading Christian Reconstructionist organizations, and the extent of the funding provided the Institute by Howard Ahmanson, Jr., a leading figure in the Reconstructionist movement.\n\nNot all creationist organizations have embraced the intelligent design movement. According to Thomas Dixon, \"Religious leaders have come out against ID too. An open letter affirming the compatibility of Christian faith and the teaching of evolution, first produced in response to controversies in Wisconsin in 2004, has now been signed by over ten thousand clergy from different Christian denominations across America. In 2006, the director of the Vatican Observatory, the Jesuit astronomer George Coyne, condemned ID as a kind of 'crude creationism' which reduced God to a mere engineer.\" Hugh Ross of Reasons to Believe, a proponent of Old Earth creationism, believes that the efforts of intelligent design proponents to divorce the concept from Biblical Christianity make its hypothesis too vague. In 2002, he wrote: \"Winning the argument for design without identifying the designer yields, at best, a sketchy origins model. Such a model makes little if any positive impact on the community of scientists and other scholars. [...] ...the time is right for a direct approach, a single leap into the origins fray. Introducing a biblically based, scientifically verifiable creation model represents such a leap.\"\n\nLikewise, two of the most prominent YEC organizations in the world have attempted to distinguish their views from those of the intelligent design movement. Henry M. Morris of the Institute for Creation Research (ICR) wrote, in 1999, that ID, \"even if well-meaning and effectively articulated, will not work! It has often been tried in the past and has failed, and it will fail today. The reason it won't work is because it is not the Biblical method.\" According to Morris: \"The evidence of intelligent design ... must be either followed by or accompanied by a sound presentation of true Biblical creationism if it is to be meaningful and lasting.\" In 2002, Carl Wieland, then of Answers in Genesis (AiG), criticized design advocates who, though well-intentioned, \"'left the Bible out of it'\" and thereby unwittingly aided and abetted the modern rejection of the Bible. Wieland explained that \"AiG's major 'strategy' is to boldly, but humbly, call the church back to its Biblical foundations ... [so] we neither count ourselves a part of this movement nor campaign against it.\"\n\nThe unequivocal consensus in the scientific community is that intelligent design is not science and has no place in a science curriculum. The U.S. National Academy of Sciences has stated that \"creationism, intelligent design, and other claims of supernatural intervention in the origin of life or of species are not science because they are not testable by the methods of science.\" The U.S. National Science Teachers Association and the American Association for the Advancement of Science have termed it pseudoscience. Others in the scientific community have denounced its tactics, accusing the ID movement of manufacturing false attacks against evolution, of engaging in misinformation and misrepresentation about science, and marginalizing those who teach it. More recently, in September 2012, Bill Nye warned that creationist views threaten science education and innovations in the United States.\n\nIn 2001, the Discovery Institute published advertisements under the heading \"A Scientific Dissent From Darwinism\", with the claim that listed scientists had signed this statement expressing skepticism:\n\nThe ambiguous statement did not exclude other known evolutionary mechanisms, and most signatories were not scientists in relevant fields, but starting in 2004 the Institute claimed the increasing number of signatures indicated mounting doubts about evolution among scientists. The statement formed a key component of Discovery Institute campaigns to present intelligent design as scientifically valid by claiming that evolution lacks broad scientific support, with Institute members continued to cite the list through at least 2011. As part of a strategy to counter these claims, scientists organised Project Steve, which gained more signatories named Steve (or variants) than the Institute's petition, and a counter-petition, \"A Scientific Support for Darwinism\", which quickly gained similar numbers of signatories.\n\nSeveral surveys were conducted prior to the December 2005 decision in \"Kitzmiller v. Dover School District\", which sought to determine the level of support for intelligent design among certain groups. According to a 2005 Harris poll, 10% of adults in the United States viewed human beings as \"so complex that they required a powerful force or intelligent being to help create them.\" Although Zogby polls commissioned by the Discovery Institute show more support, these polls suffer from considerable flaws, such as having a very low response rate (248 out of 16,000), being conducted on behalf of an organization with an expressed interest in the outcome of the poll, and containing leading questions.\n\nThe 2017 Gallup creationism survey found that 38% of adults in the United States hold the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which was noted as being at the lowest level in 35 years. Previously, a series of Gallup polls in the United States from 1982 through 2014 on \"Evolution, Creationism, Intelligent Design\" found support for \"human beings have developed over millions of years from less advanced formed of life, but God guided the process\" of between 31% and 40%, support for \"God created human beings in pretty much their present form at one time within the last 10,000 years or so\" varied from 40% to 47%, and support for \"human beings have developed over millions of years from less advanced forms of life, but God had no part in the process\" varied from 9% to 19%. The polls also noted answers to a series of more detailed questions.\n\nThere have been allegations that ID proponents have met discrimination, such as being refused tenure or being harshly criticized on the Internet. In the documentary film \"\", released in 2008, host Ben Stein presents five such cases. The film contends that the mainstream science establishment, in a \"scientific conspiracy to keep God out of the nation's laboratories and classrooms\", suppresses academics who believe they see evidence of intelligent design in nature or criticize evidence of evolution. Investigation into these allegations turned up alternative explanations for perceived persecution.\n\nThe film portrays intelligent design as motivated by science, rather than religion, though it does not give a detailed definition of the phrase or attempt to explain it on a scientific level. Other than briefly addressing issues of irreducible complexity, \"Expelled\" examines it as a political issue. The scientific theory of evolution is portrayed by the film as contributing to fascism, the Holocaust, communism, atheism, and eugenics.\n\n\"Expelled\" has been used in private screenings to legislators as part of the Discovery Institute intelligent design campaign for Academic Freedom bills. Review screenings were restricted to churches and Christian groups, and at a special pre-release showing, one of the interviewees, PZ Myers, was refused admission. The American Association for the Advancement of Science describes the film as dishonest and divisive propaganda aimed at introducing religious ideas into public school science classrooms, and the Anti-Defamation League has denounced the film's allegation that evolutionary theory influenced the Holocaust. The film includes interviews with scientists and academics who were misled into taking part by misrepresentation of the topic and title of the film. Skeptic Michael Shermer describes his experience of being repeatedly asked the same question without context as \"surreal\".\n\nAdvocates of intelligent design seek to keep God and the Bible out of the discussion, and present intelligent design in the language of science as though it were a scientific hypothesis. For a theory to qualify as scientific, it is expected to be:\n\n\nFor any theory, hypothesis or conjecture to be considered scientific, it must meet most, and ideally all, of these criteria. The fewer criteria are met, the less scientific it is; and if it meets only a few or none at all, then it cannot be treated as scientific in any meaningful sense of the word. Typical objections to defining intelligent design as science are that it lacks consistency, violates the principle of parsimony, is not scientifically useful, is not falsifiable, is not empirically testable, and is not correctable, dynamic, progressive or provisional.\n\nIntelligent design proponents seek to change this fundamental basis of science by eliminating \"methodological naturalism\" from science and replacing it with what the leader of the intelligent design movement, Phillip E. Johnson, calls \"theistic realism\". Intelligent design proponents argue that naturalistic explanations fail to explain certain phenomena and that supernatural explanations provide a very simple and intuitive explanation for the origins of life and the universe. Many intelligent design followers believe that \"scientism\" is itself a religion that promotes secularism and materialism in an attempt to erase theism from public life, and they view their work in the promotion of intelligent design as a way to return religion to a central role in education and other public spheres.\n\nIt has been argued that methodological naturalism is not an \"assumption\" of science, but a \"result\" of science well done: the God explanation is the least parsimonious, so according to Occam's razor, it cannot be a scientific explanation.\n\nThe failure to follow the procedures of scientific discourse and the failure to submit work to the scientific community that withstands scrutiny have weighed against intelligent design being accepted as valid science. The intelligent design movement has not published a properly peer-reviewed article supporting ID in a scientific journal, and has failed to publish supporting peer-reviewed research or data. The only article published in a peer-reviewed scientific journal that made a case for intelligent design was quickly withdrawn by the publisher for having circumvented the journal's peer-review standards. The Discovery Institute says that a number of intelligent design articles have been published in peer-reviewed journals, but critics, largely members of the scientific community, reject this claim and state intelligent design proponents have set up their own journals with peer review that lack impartiality and rigor, consisting entirely of intelligent design supporters.\n\nFurther criticism stems from the fact that the phrase \"intelligent\" design makes use of an assumption of the quality of an observable intelligence, a concept that has no scientific consensus definition. The characteristics of intelligence are assumed by intelligent design proponents to be observable without specifying what the criteria for the measurement of intelligence should be. Critics say that the design detection methods proposed by intelligent design proponents are radically different from conventional design detection, undermining the key elements that make it possible as legitimate science. Intelligent design proponents, they say, are proposing both searching for a designer without knowing anything about that designer's abilities, parameters, or intentions (which scientists do know when searching for the results of human intelligence), as well as denying the very distinction between natural/artificial design that allows scientists to compare complex designed artifacts against the background of the sorts of complexity found in nature.\n\nAmong a significant proportion of the general public in the United States, the major concern is whether conventional evolutionary biology is compatible with belief in God and in the Bible, and how this issue is taught in schools. The Discovery Institute's \"Teach the Controversy\" campaign promotes intelligent design while attempting to discredit evolution in United States public high school science courses. The scientific community and science education organizations have replied that there is no scientific controversy regarding the validity of evolution and that the controversy exists solely in terms of religion and politics.\n\nEugenie C. Scott, along with Glenn Branch and other critics, has argued that many points raised by intelligent design proponents are arguments from ignorance.\nIn the argument from ignorance, a lack of evidence for one view is erroneously argued to constitute proof of the correctness of another view. Scott and Branch say that intelligent design is an argument from ignorance because it relies on a lack of knowledge for its conclusion: lacking a natural explanation for certain specific aspects of evolution, we assume intelligent cause. They contend most scientists would reply that the unexplained is not unexplainable, and that \"we don't know yet\" is a more appropriate response than invoking a cause outside science. Particularly, Michael Behe's demands for ever more detailed explanations of the historical evolution of molecular systems seem to assume a false dichotomy, where either evolution or design is the proper explanation, and any perceived failure of evolution becomes a victory for design. Scott and Branch also contend that the supposedly novel contributions proposed by intelligent design proponents have not served as the basis for any productive scientific research.\n\nIn his conclusion to the Kitzmiller trial, Judge John E. Jones III wrote that \"ID is at bottom premised upon a false dichotomy, namely, that to the extent evolutionary theory is discredited, ID is confirmed.\" This same argument had been put forward to support creation science at the \"McLean v. Arkansas\" (1982) trial, which found it was \"contrived dualism\", the false premise of a \"two model approach\". Behe's argument of irreducible complexity puts forward negative arguments against evolution but does not make any positive scientific case for intelligent design. It fails to allow for scientific explanations continuing to be found, as has been the case with several examples previously put forward as supposed cases of irreducible complexity.\n\nIntelligent design proponents often insist that their claims do not require a religious component. However, various philosophical and theological issues are naturally raised by the claims of intelligent design.\n\nIntelligent design proponents attempt to demonstrate scientifically that features such as irreducible complexity and specified complexity could not arise through natural processes, and therefore required repeated direct miraculous interventions by a Designer (often a Christian concept of God). They reject the possibility of a Designer who works merely through setting natural laws in motion at the outset, in contrast to theistic evolution (to which even Charles Darwin was open). Intelligent design is distinct because it asserts repeated miraculous interventions in addition to designed laws. This contrasts with other major religious traditions of a created world in which God's interactions and influences do not work in the same way as physical causes. The Roman Catholic tradition makes a careful distinction between ultimate metaphysical explanations and secondary, natural causes.\n\nThe concept of direct miraculous intervention raises other potential theological implications. If such a Designer does not intervene to alleviate suffering even though capable of intervening for other reasons, some imply the designer is not omnibenevolent (see problem of evil and related theodicy).\n\nFurther, repeated interventions imply that the original design was not perfect and final, and thus pose a problem for any who believe that the Creator's work had been both perfect and final. Intelligent design proponents seek to explain the problem of poor design in nature by insisting that we have simply failed to understand the perfection of the design (for example, proposing that vestigial organs have unknown purposes), or by proposing that designers do not necessarily produce the best design they can, and may have unknowable motives for their actions.\n\nIntelligent design has also been characterized as a God-of-the-gaps argument, which has the following form:\n\nA God-of-the-gaps argument is the theological version of an argument from ignorance. A key feature of this type of argument is that it merely answers outstanding questions with explanations (often supernatural) that are unverifiable and ultimately themselves subject to unanswerable questions. Historians of science observe that the astronomy of the earliest civilizations, although astonishing and incorporating mathematical constructions far in excess of any practical value, proved to be misdirected and of little importance to the development of science because they failed to inquire more carefully into the mechanisms that drove the heavenly bodies across the sky. It was the Greek civilization that first practiced science, although not yet as a formally defined experimental science, but nevertheless an attempt to rationalize the world of natural experience without recourse to divine intervention. In this historically motivated definition of science any appeal to an intelligent creator is explicitly excluded for the paralysing effect it may have on scientific progress.\n\n\"Kitzmiller v. Dover Area School District\" was the first direct challenge brought in the United States federal courts against a public school district that required the presentation of intelligent design as an alternative to evolution. The plaintiffs successfully argued that intelligent design is a form of creationism, and that the school board policy thus violated the Establishment Clause of the First Amendment to the United States Constitution.\n\nEleven parents of students in Dover, Pennsylvania, sued the Dover Area School District over a statement that the school board required be read aloud in ninth-grade science classes when evolution was taught. The plaintiffs were represented by the American Civil Liberties Union (ACLU), Americans United for Separation of Church and State (AU) and Pepper Hamilton LLP. The National Center for Science Education acted as consultants for the plaintiffs. The defendants were represented by the Thomas More Law Center. The suit was tried in a bench trial from September 26 to November 4, 2005, before Judge John E. Jones III. Kenneth R. Miller, Kevin Padian, Brian Alters, Robert T. Pennock, Barbara Forrest and John F. Haught served as expert witnesses for the plaintiffs. Michael Behe, Steve Fuller and Scott Minnich served as expert witnesses for the defense.\n\nOn December 20, 2005, Judge Jones issued his 139-page findings of fact and decision, ruling that the Dover mandate was unconstitutional, and barring intelligent design from being taught in Pennsylvania's Middle District public school science classrooms. On November 8, 2005, there had been an election in which the eight Dover school board members who voted for the intelligent design requirement were all defeated by challengers who opposed the teaching of intelligent design in a science class, and the current school board president stated that the board did not intend to appeal the ruling.\n\nIn his finding of facts, Judge Jones made the following condemnation of the \"Teach the Controversy\" strategy:\n\nJudge Jones himself anticipated that his ruling would be criticized, saying in his decision that:\n\nAs Jones had predicted, John G. West, Associate Director of the Center for Science and Culture, said:\n\nNewspapers have noted with interest that the judge is \"a Republican and a churchgoer\".\n\nSubsequently, the decision has been examined in a search for flaws and conclusions, partly by intelligent design supporters aiming to avoid future defeats in court. In its Winter issue of 2007, the \"Montana Law Review\" published three articles.\nIn the first, David K. DeWolf, John G. West and Casey Luskin, all of the Discovery Institute, argued that intelligent design is a valid scientific theory, the Jones court should not have addressed the question of whether it was a scientific theory, and that the Kitzmiller decision will have no effect at all on the development and adoption of intelligent design as an alternative to standard evolutionary theory. In the second Peter H. Irons responded, arguing that the decision was extremely well reasoned and spells the death knell for the intelligent design efforts to introduce creationism in public schools, while in the third, DeWolf, \"et al.\", answer the points made by Irons. However, fear of a similar lawsuit has resulted in other school boards abandoning intelligent design \"teach the controversy\" proposals.\n\nA number of anti-evolution bills have been introduced in the United States Congress and State legislatures since 2001, based largely upon language drafted by the Discovery Institute for the Santorum Amendment. Their aim has been to expose more students to articles and videos produced by advocates of intelligent design that criticise evolution. They have been presented as supporting \"academic freedom\", on the supposition that teachers, students, and college professors face intimidation and retaliation when discussing scientific criticisms of evolution, and therefore require protection. Critics of the legislation have pointed out that there are no credible scientific critiques of evolution, and an investigation in Florida of allegations of intimidation and retaliation found no evidence that it had occurred. The vast majority of the bills have been unsuccessful, with the one exception being Louisiana's Louisiana Science Education Act, which was enacted in 2008.\n\nIn April 2010, the American Academy of Religion issued \"Guidelines for Teaching About Religion in K‐12 Public Schools in the United States\", which included guidance that creation science or intelligent design should not be taught in science classes, as \"Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning.\" However, these worldviews as well as others \"that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others.\"\n\nIn June 2007, the Council of Europe's Committee on Culture, Science and Education issued a report, \"The dangers of creationism in education\", which states \"Creationism in any of its forms, such as 'intelligent design', is not based on facts, does not use any scientific reasoning and its contents are pathetically inadequate for science classes.\" In describing the dangers posed to education by teaching creationism, it described intelligent design as \"anti-science\" and involving \"blatant scientific fraud\" and \"intellectual deception\" that \"blurs the nature, objectives and limits of science\" and links it and other forms of creationism to denialism. On October 4, 2007, the Council of Europe's Parliamentary Assembly approved a resolution stating that schools should \"resist presentation of creationist ideas in any discipline other than religion\", including \"intelligent design\", which it described as \"the latest, more refined version of creationism\", \"presented in a more subtle way\". The resolution emphasises that the aim of the report is not to question or to fight a belief, but to \"warn against certain tendencies to pass off a belief as science\".\n\nIn the United Kingdom, public education includes religious education as a compulsory subject, and there are many faith schools that teach the ethos of particular denominations. When it was revealed that a group called Truth in Science had distributed DVDs produced by Illustra Media featuring Discovery Institute fellows making the case for design in nature, and claimed they were being used by 59 schools, the Department for Education and Skills (DfES) stated that \"Neither creationism nor intelligent design are taught as a subject in schools, and are not specified in the science curriculum\" (part of the National Curriculum, which does not apply to independent schools or to education in Scotland). The DfES subsequently stated that \"Intelligent design is not a recognised scientific theory; therefore, it is not included in the science curriculum\", but left the way open for it to be explored in religious education in relation to different beliefs, as part of a syllabus set by a local Standing Advisory Council on Religious Education. In 2006, the Qualifications and Curriculum Authority produced a \"Religious Education\" model unit in which pupils can learn about religious and nonreligious\nviews about creationism, intelligent design and evolution by natural selection.\n\nOn June 25, 2007, the UK Government responded to an e-petition by saying that creationism and intelligent design should not be taught as science, though teachers would be expected to answer pupils' questions within the standard framework of established scientific theories. Detailed government \"Creationism teaching guidance\" for schools in England was published on September 18, 2007. It states that \"Intelligent design lies wholly outside of science\", has no underpinning scientific principles, or explanations, and is not accepted by the science community as a whole. Though it should not be taught as science, \"Any questions about creationism and intelligent design which arise in science lessons, for example as a result of media coverage, could provide the opportunity to explain or explore why they are not considered to be scientific theories and, in the right context, why evolution is considered to be a scientific theory.\" However, \"Teachers of subjects such as RE, history or citizenship may deal with creationism and intelligent design in their lessons.\"\n\nThe British Centre for Science Education lobbying group has the goal of \"countering creationism within the UK\" and has been involved in government lobbying in the UK in this regard. Northern Ireland's Department for Education says that the curriculum provides an opportunity for alternative theories to be taught. The Democratic Unionist Party (DUP)—which has links to fundamentalist Christianity—has been campaigning to have intelligent design taught in science classes. A DUP former Member of Parliament, David Simpson, has sought assurances from the education minister that pupils will not lose marks if they give creationist or intelligent design answers to science questions. In 2007, Lisburn city council voted in favor of a DUP recommendation to write to post-primary schools asking what their plans are to develop teaching material in relation to \"creation, intelligent design and other theories of origin\".\n\nPlans by Dutch Education Minister Maria van der Hoeven to \"stimulate an academic debate\" on the subject in 2005 caused a severe public backlash. After the 2006 elections, she was succeeded by Ronald Plasterk, described as a \"molecular geneticist, staunch atheist and opponent of intelligent design\". As a reaction on this situation in the Netherlands, the Director General of the Flemish Secretariat of Catholic Education () in Belgium, , declared that: \"Catholic scientists already accepted the theory of evolution for a long time and that intelligent design and creationism doesn't belong in Flemish Catholic schools. It's not the tasks of the politics to introduce new ideas, that's task and goal of science.\"\nThe status of intelligent design in Australia is somewhat similar to that in the UK (see Education in Australia). In 2005, the Australian Minister for Education, Science and Training, Brendan Nelson, raised the notion of intelligent design being taught in science classes. The public outcry caused the minister to quickly concede that the correct forum for intelligent design, if it were to be taught, is in religion or philosophy classes. The Australian chapter of Campus Crusade for Christ distributed a DVD of the Discovery Institute's documentary \"Unlocking the Mystery of Life\" (2002) to Australian secondary schools. Tim Hawkes, the head of The King's School, one of Australia's leading private schools, supported use of the DVD in the classroom at the discretion of teachers and principals.\nMuzaffar Iqbal, a notable Pakistani-Canadian Muslim, signed \"A Scientific Dissent From Darwinism\", a petition from the Discovery Institute. Ideas similar to intelligent design have been considered respected intellectual options among Muslims, and in Turkey many intelligent design books have been translated. In Istanbul in 2007, public meetings promoting intelligent design were sponsored by the local government, and David Berlinski of the Discovery Institute was the keynote speaker at a meeting in May 2007.\n\nIn 2011, the International Society for Krishna Consciousness (ISKCON) Bhaktivedanta Book Trust published an intelligent design book titled \"Rethinking Darwin: A Vedic Study of Darwinism and Intelligent Design\". The book included contributions from intelligent design advocates William A. Dembski, Jonathan Wells and Michael Behe as well as from Hindu creationists Leif A. Jensen and Michael Cremo.\n\n\n\n"}
{"id": "15302", "url": "https://en.wikipedia.org/wiki?curid=15302", "title": "Integrin", "text": "Integrin\n\nIntegrins are transmembrane receptors that facilitate cell-extracellular matrix (ECM) adhesion. Upon ligand binding, integrins activate signal transduction pathways that mediate cellular signals such as regulation of the cell cycle, organization of the intracellular cytoskeleton, and movement of new receptors to the cell membrane. The presence of integrins allows rapid and flexible responses to events at the cell surface (\"e.g\". signal platelets to initiate an interaction with coagulation factors).\n\nSeveral types of integrins exist, and one cell may have multiple different types on its surface. Integrins are found in all animals while integrin-like receptors are found in plant cells.\n\nIntegrins work alongside other receptors such as cadherins, the immunoglobulin superfamily cell adhesion molecules, selectins and syndecans, to mediate cell–cell and cell–matrix interaction. Ligands for integrins include fibronectin, vitronectin, collagen and laminin.\n\nIntegrins are obligate heterodimers, meaning that they have two subunits: α (alpha) and β (beta). Integrins in mammals have twenty-four α and nine β subunits, in \"Drosophila\" five α and two β subunits, and in \"Caenorhabditis\" nematodes two α subunits and one β subunit. The α and β subunits each penetrate the plasma membrane and possess several cytoplasmic domains.\n\nVariants of some subunits are formed by differential RNA splicing; for example, four variants of the beta-1 subunit exist. Through different combinations of the α and β subunits, around 24 unique integrins are generated.\n\nIntegrin subunits span the cell membrane and have short cytoplasmic domains of 40–70 amino acids. The exception is the beta-4 subunit, which has a cytoplasmic domain of 1,088 amino acids, one of the largest of any membrane protein. Outside the cell membrane, the α and β chains lie close together along a length of about 23 nm; the final 5 nm N-termini of each chain forms a ligand-binding region for the ECM. They have been compared to lobster claws, although they don't actually \"pinch\" their ligand, they chemically interact with it at the insides of the \"tips\" of their \"pinchers\".\n\nThe molecular mass of the integrin subunits can vary from 90 kDa to 160 kDa. Beta subunits have four cysteine-rich repeated sequences. Both α and β subunits bind several divalent cations. The role of divalent cations in the α subunit is unknown, but may stabilize the folds of the protein. The cations in the β subunits are more interesting: they are directly involved in coordinating at least some of the ligands that integrins bind.\n\nIntegrins can be categorized in multiple ways. For example, some α chains have an additional structural element (or \"domain\") inserted toward the N-terminal, the alpha-A domain (so called because it has a similar structure to the A-domains found in the protein von Willebrand factor; it is also termed the α-I domain). Integrins carrying this domain either bind to collagens (e.g. integrins α1 β1, and α2 β1), or act as cell-cell adhesion molecules (integrins of the β2 family). This α-I domain is the binding site for ligands of such integrins. Those integrins that don't carry this inserted domain also have an A-domain in their ligand binding site, but \"this\" A-domain is found on the β subunit.\n\nIn both cases, the A-domains carry up to three divalent cation binding sites. One is permanently occupied in physiological concentrations of divalent cations, and carries either a calcium or magnesium ion, the principal divalent cations in blood at median concentrations of 1.4 mM (calcium) and 0.8 mM (magnesium). The other two sites become occupied by cations when ligands bind—at least for those ligands involving an acidic amino acid in their interaction sites. An acidic amino acid features in the integrin-interaction site of many ECM proteins, for example as part of the amino acid sequence Arginine-Glycine-Aspartic acid (\"RGD\" in the one-letter amino acid code).\n\nDespite many years of effort, discovering the high-resolution structure of integrins proved to be challenging, as membrane proteins are classically difficult to purify, and as integrins are large, complex and linked to many sugar trees (\"highly glycosylated\"). Low-resolution images of detergent extracts of intact integrin GPIIbIIIa, obtained using electron microscopy, and even data from indirect techniques that investigate the solution properties of integrins using ultracentrifugation and light scattering, were combined with fragmentary high-resolution crystallographic or NMR data from single or paired domains of single integrin chains, and molecular models postulated for the rest of the chains.\n\nThe X-ray crystal structure obtained for the complete extracellular region of one integrin, αvβ3, shows the molecule to be folded into an inverted V-shape that potentially brings the ligand-binding sites close to the cell membrane. Perhaps more importantly, the crystal structure was also obtained for the same integrin bound to a small ligand containing the RGD-sequence, the drug cilengitide. As detailed above, this finally revealed why divalent cations (in the A-domains) are critical for RGD-ligand binding to integrins. The interaction of such sequences with integrins is believed to be a primary switch by which ECM exerts its effects on cell behaviour.\n\nThe structure poses many questions, especially regarding ligand binding and signal transduction. The ligand binding site is directed towards the C-terminal of the integrin, the region where the molecule emerges from the cell membrane. If it emerges orthogonally from the membrane, the ligand binding site would apparently be obstructed, especially as integrin ligands are typically massive and well cross-linked components of the ECM. In fact, little is known about the angle that membrane proteins subtend to the plane of the membrane; this is a problem difficult to address with available technologies. The default assumption is that they emerge rather like little lollipops, but the evidence for this sweet supposition is noticeable by its absence. The integrin structure has drawn attention to this problem, which may have general implications for how membrane proteins work. It appears that the integrin transmembrane helices are tilted (see \"Activation\" below), which hints that the extracellular chains may also not be orthogonal with respect to the membrane surface.\n\nAlthough the crystal structure changed surprisingly little after binding to cilengitide, the current hypothesis is that integrin function involves changes in shape to move the ligand-binding site into a more accessible position, away from the cell surface, and this shape change also triggers intracellular signaling. There is a wide body of cell-biological and biochemical literature that supports this view. Perhaps the most convincing evidence involves the use of antibodies that only recognize integrins when they have bound to their ligands, or are activated. As the \"footprint\" that an antibody makes on its binding target is roughly a circle about 3 nm in diameter, the resolution of this technique is low. Nevertheless, these so-called LIBS (Ligand-Induced-Binding-Sites) antibodies unequivocally show that dramatic changes in integrin shape routinely occur. However, how the changes detected with antibodies look on the structure is still unknown.\n\nWhen released into the cell membrane, newly synthesized integrin dimers are speculated to be found in the same \"bent\" conformation revealed by the structural studies described above. One school of thought claims that this bent form prevents them from interacting with their ligands, although bent forms can predominate in high-resolution EM structures of integrin bound to an ECM ligand. Therefore, at least in biochemical experiments, integrin dimers must apparently not be 'unbent' in order to prime them and allow their binding to the ECM. In cells, the priming is accomplished by a protein talin, which binds to the β tail of the integrin dimer and changes its conformation. The α and β integrin chains are both class-I transmembrane proteins: they pass the plasma membrane as single transmembrane alpha-helices. Unfortunately, the helices are too long, and recent studies suggest that, for integrin gpIIbIIIa, they are tilted with respect both to one another and to the plane of the membrane. Talin binding alters the angle of tilt of the β3 chain transmembrane helix in model systems and this may reflect a stage in the process of inside-out signalling which primes integrins. Moreover, talin proteins are able to dimerize and thus are thought to intervene in the clustering of integrin dimers which leads to the formation of a focal adhesion. Recently, the Kindlin-1 and Kindlin-2 proteins have also been found to interact with integrin and activate it.\n\nIntegrins have two main functions:-\n\nHowever, they are also involved in a wide range of other biological activities, including immune patrolling, cell migration, and binding to cells by certain viruses, such as adenovirus, echovirus, hantavirus, and foot-and-mouth disease viruses.\n\nA prominent function of the integrins is seen in the molecule GPIIbIIIa, an integrin on the surface of blood platelets (thrombocytes) responsible for attachment to fibrin within a developing blood clot. This molecule dramatically increases its binding affinity for fibrin/fibrinogen through association of platelets with exposed collagens in the wound site. Upon association of platelets with collagen, GPIIbIIIa changes shape, allowing it to bind to fibrin and other blood components to form the clot matrix and stop blood loss.\n\nIntegrins couple the ECM outside a cell to the cytoskeleton (in particular, the microfilaments) inside the cell. Which ligand in the ECM the integrin can bind to is defined by which α and β subunits the integrin is made of. Among the ligands of integrins are fibronectin, vitronectin, collagen, and laminin. The connection between the cell and the ECM may help the cell to endure pulling forces without being ripped out of the ECM. The ability of a cell to create this kind of bond is also of vital importance in ontogeny.\n\nCell attachment to the ECM is a basic requirement to build a multicellular organism. Integrins are not simply hooks, but give the cell critical signals about the nature of its surroundings. Together with signals arising from receptors for soluble growth factors like VEGF, EGF, and many others, they enforce a cellular decision on what biological action to take, be it attachment, movement, death, or differentiation. Thus integrins lie at the heart of many cellular biological processes. The attachment of the cell takes place through formation of cell adhesion complexes, which consist of integrins and many cytoplasmic proteins, such as talin, vinculin, paxillin, and alpha-actinin. These act by regulating kinases such as FAK (focal adhesion kinase) and Src kinase family members to phosphorylate substrates such as p130CAS thereby recruiting signaling adaptors such as CRK. These adhesion complexes attach to the actin cytoskeleton. The integrins thus serve to link two networks across the plasma membrane: the extracellular ECM and the intracellular actin filamentous system. Integrin α6β4 is an exception: it links to the keratin intermediate filament system in epithelial cells.\n\nFocal adhesions are large molecular complexes, which are generated following interaction of integrins with ECM, then their clustering. The clusters likely provide sufficient intracellular binding sites to permit the formation of stable signaling complexes on the cytoplasmic side of the cell membrane. So the focal adhesions contain integrin ligand, integrin molecule, and associate plaque proteins. Binding is propelled by changes in free energy. As previously stated, these complexes connect the extracellular matrix to actin bundles. Cryo-electron tomography reveals that the adhesion contains particles on the cell membrane with diameter of 25 +/- 5 nm and spaced at approximately 45 nm. Treatment with Rho-kinase inhibitor Y-27632 reduces the size of the particle, and it is extremely mechanosensitive.\n\nOne important function of integrins on cells in tissue culture is their role in cell migration. Cells adhere to a substrate through their integrins. During movement, the cell makes new attachments to the substrate at its front and concurrently releases those at its rear. When released from the substrate, integrin molecules are taken back into the cell by endocytosis; they are transported through the cell to its front by the endocytic cycle, where they are added back to the surface. In this way they are cycled for reuse, enabling the cell to make fresh attachments at its leading front. It is not yet clear whether cell migration in tissue culture is an artefact of integrin processing, or whether such integrin-dependent cell migration also occurs in living organisms.\n\nIntegrins play an important role in cell signaling by modulating the cell signaling pathways of transmembrane protein kinases such as receptor tyrosine kinases (RTK). While the interaction between integrin and receptor tyrosine kinases originally was thought of as uni-directional and supportive, recent studies indicate that integrins have additional, multi-faceted roles in cell signaling. \nIntegrins can regulate the receptor tyrosine kinase signaling by recruiting specific adaptors to the plasma membrane. For example, β1c integrin recruits Gab1/Shp2 and presents Shp2 to IGF1R, resulting in dephosphorylation of the receptor. In a reverse direction, when a receptor tyrosine kinase is activated, integrins co-localise at focal adhesion with the receptor tyrosine kinases and their associated signaling molecules.\n\nThe repertoire of integrins expressed on a particular cell can specify the signaling pathway due to the differential binding affinity of ECM ligands for the integrins. The tissue stiffness and matrix composition can initiate specific signaling pathways regulating cell behavior. Clustering and activation of the integrins/actin complexes strengthen the focal adhesion interaction and initiate the framework for cell signaling through assembly of adhesomes.\n\nDepending on the integrin's regulatory impact on specific receptor tyrosine kinases, the cell can experience:\n\nKnowledge of the relationship between integrins and receptor tyrosine kinase has laid a foundation for new approaches to cancer therapy. Specifically, targeting integrins associated with RTKs is an emerging approach for inhibiting angiogenesis.\n\nIntegrins have an important function in neuroregeneration after injury of the peripheral nervous system (PNS). Integrins are present at the growth cone of damaged PNS neurons and attach to ligands in the ECM to promote axon regeneration. It is unclear whether integrins can promote axon regeneration in the adult central nervous system (CNS). There are two obstacles that prevent integrin-mediated regeneration in the CNS: 1) integrins are not localised in the axon of most adult CNS neurons and 2) integrins become inactivated by molecules in the scar tissue after injury.\n\nThe following are 16 of the ~24 integrins found in vertebrates:\nBeta-1 integrins interact with many alpha integrin chains. Gene knockouts of integrins in mice are not always lethal, which suggests that during embryonal development, one integrin may substitute its function for another in order to allow survival. Some integrins are on the cell surface in an inactive state, and can be rapidly primed, or put into a state capable of binding their ligands, by cytokines. Integrins can assume several different well-defined shapes or \"conformational states\". Once primed, the conformational state changes to stimulate ligand binding, which then activates the receptors — also by inducing a shape change — to trigger outside-in signal transduction.\n\n"}
{"id": "15303", "url": "https://en.wikipedia.org/wiki?curid=15303", "title": "Ion channel", "text": "Ion channel\n\nIon channels are pore-forming membrane proteins that allow ions to pass through the channel pore. Their functions include establishing a resting membrane potential, shaping action potentials and other electrical signals by gating the flow of ions across the cell membrane, controlling the flow of ions across secretory and epithelial cells, and regulating cell volume. Ion channels are present in the membranes of all excitable cells. Ion channels are one of the two classes of ionophoric proteins, along with ion transporters (including the sodium-potassium pump, sodium-calcium exchanger, and sodium-glucose transport proteins).\n\nThe study of ion channels often involves biophysics, electrophysiology, and pharmacology, while using techniques including voltage clamp, patch clamp, immunohistochemistry, X-ray crystallography, fluoroscopy, and RT-PCR. Their classification as molecules is referred to as channelomics.\n\nThere are two distinctive features of ion channels that differentiate them from other types of ion transporter proteins:\nIon channels are located within the membrane of all excitable cells, and of many intracellular organelles. They are often described as narrow, water-filled tunnels that allow only ions of a certain size or charge to pass through. This characteristic is called selective permeability. The archetypal channel pore is just one or two atoms wide at its narrowest point and is selective for specific species of ion, such as sodium or potassium. However, some channels may be permeable to the passage of more than one type of ion, typically sharing a common charge: positive (cations) or negative (anions). Ions often move through the segments of the channel pore in single file nearly as quickly as the ions move through free solution. In many ion channels, passage through the pore is governed by a \"gate\", which may be opened or closed in response to chemical or electrical signals, temperature, or mechanical force.\n\nIon channels are integral membrane proteins, typically formed as assemblies of several individual proteins. Such \"multi-subunit\" assemblies usually involve a circular arrangement of identical or homologous proteins closely packed around a water-filled pore through the plane of the membrane or lipid bilayer. For most voltage-gated ion channels, the pore-forming subunit(s) are called the α subunit, while the auxiliary subunits are denoted β, γ, and so on.\n\nBecause channels underlie the nerve impulse and because \"transmitter-activated\" channels mediate conduction across the synapses, channels are especially prominent components of the nervous system. Indeed, numerous toxins that organisms have evolved for shutting down the nervous systems of predators and prey (e.g., the venoms produced by spiders, scorpions, snakes, fish, bees, sea snails, and others) work by modulating ion channel conductance and/or kinetics. In addition, ion channels are key components in a wide variety of biological processes that involve rapid changes in cells, such as cardiac, skeletal, and smooth muscle contraction, epithelial transport of nutrients and ions, T-cell activation and pancreatic beta-cell insulin release. In the search for new drugs, ion channels are a frequent target.\n\nThere are over 300 types of ion channels just in the cells of the inner ear. Ion channels may be classified by the nature of their gating, the species of ions passing through those gates, the number of gates (pores) and localization of proteins.\n\nFurther heterogeneity of ion channels arises when channels with different constitutive subunits give rise to a specific kind of current. Absence or mutation of one or more of the contributing types of channel subunits can result in loss of function and, potentially, underlie neurologic diseases.\n\nIon channels may be classified by gating, i.e. what opens and closes the channels. For example, voltage-gated ion channels open or close depending on the voltage gradient across the plasma membrane, while ligand-gated ion channels open or close depending on binding of ligands to the channel.\n\nVoltage-gated ion channels open and close in response to membrane potential.\n\n\nAlso known as ionotropic receptors, this group of channels open in response to specific ligand molecules binding to the extracellular domain of the receptor protein. Ligand binding causes a conformational change in the structure of the channel protein that ultimately leads to the opening of the channel gate and subsequent ion flux across the plasma membrane. Examples of such channels include the cation-permeable \"nicotinic\" Acetylcholine receptor, ionotropic glutamate-gated receptors, acid sensing ion channels (ASICs), ATP-gated P2X receptors, and the anion-permeable γ-aminobutyric acid-gated GABA receptor.\n\nIon channels activated by second messengers may also be categorized in this group, although ligands and second messengers are otherwise distinguished from each other.\n\nGating also includes activation and inactivation by second messengers from the inside of the cell membrane – rather than from outside the cell, as in the case for ligands.\n\n\n\nIon channels are also classified according to their subcellular localization. The plasma membrane accounts for around 2% of the total membrane in the cell, whereas intracellular organelles contain 98% of the cell's membrane. The major intracellular compartments are endoplasmic reticulum, Golgi apparatus, and mitochondria. On the basis of localization, ion channels are classified as:\n\n\nSome ion channels are classified by the duration of their response to stimuli:\n\n\nChannels differ with respect to the ion they let pass (for example, Na, K, Cl), the ways in which they may be regulated, the number of subunits of which they are composed and other aspects of structure. Channels belonging to the largest class, which includes the voltage-gated channels that underlie the nerve impulse, consists of four subunits with six transmembrane helices each. On activation, these helices move about and open the pore. Two of these six helices are separated by a loop that lines the pore and is the primary determinant of ion selectivity and conductance in this channel class and some others. The existence and mechanism for ion selectivity was first postulated in the late 1960s by Bertil Hille and Clay Armstrong. The idea of the ionic selectivity for potassium channels was that the carbonyl oxygens of the protein backbones of the \"selectivity filter\" (named by Bertil Hille) could efficiently replace the water molecules that normally shield potassium ions, but that sodium ions were smaller and cannot be completely dehydrated to allow such shielding, and therefore could not pass through. This mechanism was finally confirmed when the first structure of an ion channel was elucidated. A bacterial potassium channel KcsA, consisting of just the selectivity filter, \"P\" loop and two transmembrane helices was used as a model to study the permeability and the selectivity of ion channels in the Mackinnon lab. The determination of the molecular structure of KcsA by Roderick MacKinnon using X-ray crystallography won a share of the 2003 Nobel Prize in Chemistry.\n\nBecause of their small size and the difficulty of crystallizing integral membrane proteins for X-ray analysis, it is only very recently that scientists have been able to directly examine what channels \"look like.\" Particularly in cases where the crystallography required removing channels from their membranes with detergent, many researchers regard images that have been obtained as tentative. An example is the long-awaited crystal structure of a voltage-gated potassium channel, which was reported in May 2003. One inevitable ambiguity about these structures relates to the strong evidence that channels change conformation as they operate (they open and close, for example), such that the structure in the crystal could represent any one of these operational states. Most of what researchers have deduced about channel operation so far they have established through electrophysiology, biochemistry, gene sequence comparison and mutagenesis.\n\nChannels can have single (CLICs) to multiple transmembrane (K channels, P2X receptors, Na channels) domains which span plasma membrane to form pores. Pore can determine the selectivity of the channel. Gate can be formed either inside or outside the pore region.\n\nA variety of inorganic and organic molecules can modulate ion channel activity and conductance.\nSome commonly used blockers include:\n\n\nThere are a number of disorders which disrupt normal functioning of ion channels and have disastrous consequences for the organism. Genetic and autoimmune disorders of ion channels and their modifiers are known as channelopathies. See for a full list.\n\n\nThe fundamental properties of currents mediated by ion channels were analyzed by the British biophysicists Alan Hodgkin and Andrew Huxley as part of their Nobel Prize-winning research on the action potential, published in 1952. They built on the work of other physiologists, such as Cole and Baker's research into voltage-gated membrane pores from 1941. The existence of ion channels was confirmed in the 1970s by Bernard Katz and Ricardo Miledi using noise analysis. It was then shown more directly with an electrical recording technique known as the \"patch clamp\", which led to a Nobel Prize to Erwin Neher and Bert Sakmann, the technique's inventors. Hundreds if not thousands of researchers continue to pursue a more detailed understanding of how these proteins work. In recent years the development of automated patch clamp devices helped to increase significantly the throughput in ion channel screening.\n\nThe Nobel Prize in Chemistry for 2003 was awarded to Roderick MacKinnon for his studies on the physico-chemical properties of ion channel structure and function, including x-ray crystallographic structure studies.\n\nRoderick MacKinnon commissioned \"Birth of an Idea\", a tall sculpture based on the KcsA potassium channel. The artwork contains a wire object representing the channel's interior with a blown glass object representing the main cavity of the channel structure.\n\n\n"}
{"id": "15304", "url": "https://en.wikipedia.org/wiki?curid=15304", "title": "IDE", "text": "IDE\n\nIDE, iDE, or Ide may refer to:\n\n\n\n\n\n\n\n\n\n"}
{"id": "15305", "url": "https://en.wikipedia.org/wiki?curid=15305", "title": "Integrated development environment", "text": "Integrated development environment\n\nAn integrated development environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of a source code editor, build automation tools, and a debugger. Most of the modern IDEs have intelligent code completion. Some IDEs, such as NetBeans and Eclipse, contain a compiler, interpreter, or both; others, such as SharpDevelop and Lazarus, do not. The boundary between an integrated development environment and other parts of the broader software development environment is not well-defined. Sometimes a version control system, or various tools to simplify the construction of a graphical user interface (GUI), are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram, for use in object-oriented software development.\n\nIntegrated development environments are designed to maximize programmer productivity by providing tight-knit components with similar user interfaces. IDEs present a single program in which all development is done. This program typically provides many features for authoring, modifying, compiling, deploying and debugging software. This contrasts with software development using unrelated tools, such as vi, GCC or make.\n\nOne aim of the IDE is to reduce the configuration necessary to piece together multiple development utilities, instead it provides the same set of capabilities as one cohesive unit. Reducing setup time can increase developer productivity, especially in cases where learning to use the IDE is faster than manually integrating and learning all of the individual tools. Tighter integration of all development tasks has the potential to improve overall productivity beyond just helping with setup tasks. For example, code can be continuously parsed while it is being edited, providing instant feedback when syntax errors are introduced. Allowing developers to debug code much faster and easier with an IDE. \n\nSome IDEs are dedicated to a specific programming language, allowing a feature set that most closely matches the programming paradigms of the language. However, there are many multiple-language IDEs.\n\nWhile most modern IDEs are graphical, text-based IDEs such as Turbo Pascal were in popular use before the widespread availability of windowing systems like Microsoft Windows and the X Window System (X11). They commonly use function keys or hotkeys to execute frequently used commands or macros.\n\nIDEs initially became possible when developing via a console or terminal. Early systems could not support one, since programs were prepared using flowcharts, entering programs with punched cards (or paper tape, etc.) before submitting them to a compiler. Dartmouth BASIC was the first language to be created with an IDE (and was also the first to be designed for use while sitting in front of a console or terminal). Its IDE (part of the Dartmouth Time Sharing System) was command-based, and therefore did not look much like the menu-driven, graphical IDEs popular after the advent of the Graphical User Interface. However it integrated editing, file management, compilation, debugging and execution in a manner consistent with a modern IDE.\n\nMaestro I is a product from Softlab Munich and was the world's first integrated development environment for software. Maestro I was installed for 22,000 programmers worldwide. Until 1989, 6,000 installations existed in the Federal Republic of Germany. Maestro was arguably the world leader in this field during the 1970s and 1980s. Today one of the last Maestro I can be found in the Museum of Information Technology at Arlington.\n\nOne of the first IDEs with a plug-in concept was Softbench. In 1995 \"Computerwoche\" commented that the use of an IDE was not well received by developers since it would fence in their creativity.\n\nAs of March 2015, the most popular IDEs are Eclipse and Visual Studio.\n\nThe IDE editor usually provides syntax highlighting, it can show both the structures, the language keywords and the syntax errors with visually distinct colors and font effects.\n\nAdvanced IDEs provide support for automated refactoring.\n\nAn IDE is expected to provide integrated version control, in order to interact with source repositories.\n\nIDEs are also used for debugging, using an integrated debugger, with support for setting breakpoints in the editor, visual rendering of steps, etc.\n\nIDEs may provide advanced support for code search: in order to find class and function declarations, usages, variable and field read/write, etc. IDEs can use different kinds of user interface for code search, for example form-based widgets and natural-language based interfaces.\n\nVisual programming is a usage scenario in which an IDE is generally required. Visual Basic allows users to create new applications by moving programming, building blocks, or code nodes to create flowcharts or structure diagrams that are then compiled or interpreted. These flowcharts often are based on the Unified Modeling Language.\n\nThis interface has been popularized with the Lego Mindstorms system, and is being actively pursued by a number of companies wishing to capitalize on the power of custom browsers like those found at Mozilla. KTechlab supports flowcode and is a popular opensource IDE and Simulator for developing software for microcontrollers. Visual programming is also responsible for the power of distributed programming (cf. LabVIEW and EICASLAB software). An early visual programming system, Max, was modeled after analog synthesizer design and has been used to develop real-time music performance software since the 1980s. Another early example was Prograph, a dataflow-based system originally developed for the Macintosh. The graphical programming environment \"Grape\" is used to program qfix robot kits.\n\nThis approach is also used in specialist software such as Openlab, where the end users want the flexibility of a full programming language, without the traditional learning curve associated with one.\n\nSome IDEs support multiple languages, such as GNU Emacs based on C and Emacs Lisp, and IntelliJ IDEA, Eclipse, MyEclipse or NetBeans, all based on Java, or MonoDevelop, based on C#, or PlayCode.\n\nSupport for alternative languages is often provided by plugins, allowing them to be installed on the same IDE at the same time. For example, Flycheck is a modern on-the-fly syntax checking extension for GNU Emacs 24 with support for 39 languages. Eclipse, and Netbeans have plugins for C/C++, Ada, GNAT (for example AdaGIDE), Perl, Python, Ruby, and PHP, which are selected between automatically based on file extension, environment or project settings.\n\nUnix programmers can combine command-line POSIX tools into a complete development environment, capable of developing large programs such as the Linux kernel and its environment. In this sense, the entire Unix system functions as an IDE. The free software GNU tools (GNU Compiler Collection (GCC), GNU Debugger (gdb), and GNU make) are available on many platforms, including Windows. The pervasive Unix philosophy of \"everything is a text stream\" enables developers who favor command-line oriented tools to use editors with support for many of the standard Unix and GNU build tools, building an IDE with programs like\nEmacs\nor Vim. Data Display Debugger is intended to be an advanced graphical front-end for many text-based debugger standard tools. Some programmers prefer managing makefiles and their derivatives to the similar code building tools included in a full IDE. For example, most contributors to the PostgreSQL database use make and gdb directly to develop new features. Even when building PostgreSQL for Microsoft Windows using Visual C++, Perl scripts are used as a replacement for make rather than relying on any IDE features. Some Linux IDEs such as Geany attempt to provide a graphical front end to traditional build operations.\n\nOn the various Microsoft Windows platforms, command-line tools for development are seldom used. Accordingly, there are many commercial and non-commercial products. However, each has a different design commonly creating incompatibilities. Most major compiler vendors for Windows still provide free copies of their command-line tools, including Microsoft (Visual C++, Platform SDK, .NET Framework SDK, nmake utility).\n\nIDEs have always been popular on the Apple Macintosh's classic Mac OS and macOS, dating back to Macintosh Programmer's Workshop, Turbo Pascal, THINK Pascal and THINK C environments of the mid-1980s. Currently macOS programmers can choose between native IDEs like Xcode and open-source tools such as Eclipse and Netbeans. ActiveState Komodo is a proprietary multilanguage IDE supported on macOS.\n\nSome features of IDEs can benefit from advances in AI. In particular, one can collect information from IDE actions across developers in order to augment IDE features. For instance, a data-driven approach to code completion results in intelligent code completion.\n\nA web integrated development environment (Web IDE or WIDE), also known as cloud IDE, is a browser based IDE that allows for software development or web development. A web IDE can be accessed from a web browser, such as Google Chrome or Internet Explorer, allowing for a portable work environment. A web IDE does not usually contain all of the same features as a traditional, or desktop, IDE, although all of the basic IDE features, such as syntax highlighting, are typically present.\n\nA web IDE, like most websites, is usually composed of two pieces: a frontend and a backend. The frontend is usually written in Javascript, using AJAX methods to communicate with the backend using a HTTP API, although in some cases, a browser extension or desktop application serves as the frontend and communicates with the backend without the need for a browser. The backend takes care of creating, saving, and opening files, as well as running any terminal commands if the IDE supports it. This setup allows for portability and continuity. The state of the IDE can be saved and reopened on another machine. This also allows for compiling or running programs to continue while the user is away.\n\nMany web IDEs support several programming languages, while others only support a specific language. Most web IDEs allow access to a Command-line interface (CLI) that allows the user to install or run any software that is needed for development, allowing \"full\" control over the development environment. Open source web IDEs allow for installation on local servers or machines and can be used to give the developer more control over the development environment.\n\nMost Web IDEs also include real time collaboration features, allowing multiple users to simultaneously work with other developers around the world (or locally) in real time.\n\nPros:\n\nCons:\n\nNotable Web IDEs:\n"}
{"id": "15308", "url": "https://en.wikipedia.org/wiki?curid=15308", "title": "Ian McKellen", "text": "Ian McKellen\n\nSir Ian Murray McKellen (born 25 May 1939) is an English actor. He is the recipient of six Laurence Olivier Awards, a Tony Award, a Golden Globe Award, a Screen Actors Guild Award, a BIF Award, two Saturn Awards, four Drama Desk Awards, and two Critics' Choice Awards. He has also received two Oscar nominations, four BAFTA nominations and five Emmy Award nominations.\n\nMcKellen's career spans genres ranging from Shakespearean and modern theatre to popular fantasy and science fiction. The BBC states that his \"performances have guaranteed him a place in the canon of English stage and film actors\". A recipient of every major theatrical award in the UK, McKellen is regarded as a British cultural icon. He started his professional career in 1961 at the Belgrade Theatre as a member of their highly regarded repertory company. In 1965, McKellen made his first West End appearance. In 1969, he was invited to join the Prospect Theatre Company to play the lead parts in Shakespeare's \"Richard II\" and Marlowe's \"Edward II\", and he firmly established himself as one of the country's foremost classical actors. In the 1970s, McKellen became a stalwart of the Royal Shakespeare Company and the National Theatre of Great Britain. He achieved worldwide fame for his film roles, including the titular King in \"Richard III\" (1995), James Whale in \"Gods and Monsters\" (1998), Magneto in the \"X-Men\" films, and Gandalf in \"The Lord of the Rings\" and \"The Hobbit\" trilogies.\n\nMcKellen was appointed Commander of the Order of the British Empire in the 1979 Birthday Honours, was knighted in the 1991 New Year Honours for services to the performing arts, and made a Companion of Honour for services to drama and to equality in the 2008 New Year Honours. He has been openly gay since 1988, and continues to be a champion for LGBT social movements worldwide. He was awarded Freedom of the City of London in October 2014.\n\nMcKellen was born on 25 May 1939 in Burnley, Lancashire, the son of Margery Lois (née Sutcliffe) and Denis Murray McKellen, a civil engineer. He was their second child, with a sister, Jean, five years his senior. Shortly before the outbreak of the Second World War in September 1939, his family moved to Wigan. They lived there until Ian was twelve years old, before relocating to Bolton in 1951, after his father had been promoted. The experience of living through the war as a young child had a lasting impact on him, and he later said that \"only after peace resumed ... did I realise that war wasn't normal.\" When an interviewer remarked that he seemed quite calm in the aftermath of 11 September attacks, McKellen said: \"Well, darling, you forget—I slept under a steel plate until I was four years old.\".\n\nMcKellen's father was a civil engineer and lay preacher, and was of Protestant Irish and Scottish descent. Both of McKellen's grandfathers were preachers, and his great-great-grandfather, James McKellen, was a \"strict, evangelical Protestant minister\" in Ballymena, County Antrim. His home environment was strongly Christian, but non-orthodox. \"My upbringing was of low nonconformist Christians who felt that you led the Christian life in part by behaving in a Christian manner to everybody you met.\" When he was 12, his mother died of breast cancer; his father died when he was 24. After his coming out as gay to his stepmother, Gladys McKellen, who was a member of the Religious Society of Friends, he said, \"Not only was she not fazed, but as a member of a society which declared its indifference to people's sexuality years back, I think she was just glad for my sake that I wasn't lying anymore.\" His great-great-grandfather Robert J. Lowes was an activist and campaigner in the ultimately successful campaign for a Saturday half-holiday in Manchester, the forerunner to the modern five-day work week, thus making Lowes a \"grandfather of the modern weekend\".\n\nMcKellen attended Bolton School (Boys' Division), of which he is still a supporter, attending regularly to talk to pupils. McKellen's acting career started at Bolton Little Theatre, of which he is now the patron. An early fascination with the theatre was encouraged by his parents, who took him on a family outing to \"Peter Pan\" at the Opera House in Manchester when he was three. When he was nine, his main Christmas present was a fold-away wood and bakelite Victorian theatre from Pollocks Toy Theatres, with cardboard scenery and wires to push on the cut-outs of Cinderella and of Laurence Olivier's Hamlet.\n\nHis sister took him to his first Shakespeare play, \"Twelfth Night\", by the amateurs of Wigan's Little Theatre, shortly followed by their \"Macbeth\" and Wigan High School for Girls' production of \"A Midsummer Night's Dream\", with music by Mendelssohn, with the role of Bottom played by Jean McKellen, who continued to act, direct, and produce amateur theatre until her death.\n\nIn 1958, McKellen, at the age of 18, won a scholarship to St Catharine's College, Cambridge, where he read English literature. He has since been made an Honorary Fellow of the College. While at Cambridge, McKellen was a member of the Marlowe Society, where he appeared in 23 plays over the course of 3 years. At that young age he was already giving performances that have since become legendary such as his Justice Shallow in \"Henry IV\" alongside Trevor Nunn and Derek Jacobi (March 1959), \"Cymbeline\" (as Posthumus, opposite Margaret Drabble as Imogen) and \"Doctor Faustus\". During this period McKellen had already been directed by Peter Hall, John Barton and Dadie Rylands, all of whom would have a huge impact on McKellen's future career.\n\nMcKellen made his first professional appearance in 1961 at the Belgrade Theatre, as Roper in \"A Man for All Seasons\", although an audio recording of the Marlowe Society's \"Cymbeline\" had gone on commercial sale as part of the Argo Shakespeare series.\n\nAfter four years in regional repertory theatres he made his first West End appearance, in \"A Scent of Flowers\", regarded as a \"notable success\". In 1965 he was a member of Laurence Olivier's National Theatre Company at the Old Vic, which led to roles at the Chichester Festival. With the Prospect Theatre Company, McKellen made his breakthrough performances of Richard II (directed by Richard Cottrell) and Marlowe's Edward II (directed by Toby Robertson) at the Edinburgh festival in 1969, the latter causing a storm of protest over the enactment of the homosexual Edward's lurid death.\n\nIn the 1970s and 1980s McKellen became a well-known figure in British theatre, performing frequently at the Royal Shakespeare Company and the Royal National Theatre, where he played several leading Shakespearean roles, including the title role in \"Macbeth\" (which he had first played for Trevor Nunn in a \"gripping...out of the ordinary\" production, with Judi Dench, at Stratford in 1976), and Iago in \"Othello\", in award-winning productions directed by Nunn. Both of these productions were adapted into television films, also directed by Nunn.\n\nIn 2007 he returned to the Royal Shakespeare Company, in productions of \"King Lear\" and \"The Seagull\", both directed by Trevor Nunn. In 2009 he appeared in a very popular revival of \"Waiting for Godot\" at London's Haymarket Theatre, directed by Sean Mathias, and playing opposite Patrick Stewart. He is Patron of English Touring Theatre and also President and Patron of the Little Theatre Guild of Great Britain, an association of amateur theatre organisations throughout the UK. In late August 2012, he took part in the opening ceremony of the London Paralympics, portraying Prospero from \"The Tempest\".\n\nIn October 2017, McKellen played King Lear at Chichester Festival Theatre, a role which he said was likely to be his \"last big Shakespearean part\". He will perform the play at the Duke of York's Theatre in London's West End during the summer of 2018.\n\nMcKellen had taken film roles throughout his career—beginning in 1969 with his role of George Matthews in \"A Touch of Love\", and his first leading role was in 1980 as D. H. Lawrence in \"Priest of Love\", but it was not until the 1990s that he became more widely recognised in this medium after several roles in blockbuster Hollywood films. In 1993 he had a supporting role as a South African tycoon in the critically acclaimed \"Six Degrees of Separation\", in which he starred with Stockard Channing, Donald Sutherland, and Will Smith. In the same year, he appeared in minor roles in the television miniseries \"Tales of the City\", based on the novel by his friend Armistead Maupin, and the film \"Last Action Hero\", in which he briefly played Death opposite Arnold Schwarzenegger and Charles Dance.\n\nLater in 1993, McKellen appeared in the television film \"And the Band Played On\" about the discovery of the AIDS virus for which McKellen won a CableACE Award for Supporting Actor in a Movie or Miniseries and was nominated for the Emmy Award for Outstanding Supporting Actor in a Miniseries or a Movie. In 1995, he played the title role in \"Richard III\", which transported the setting into an alternative 1930s in which England is ruled by fascists. The film was a critical success. McKellen co-produced and co-wrote the film, adapting the play for the screen based on a stage production of Shakespeare's play directed by Richard Eyre for the Royal National Theatre in which McKellen had appeared. As executive producer he returned his £50,000 fee to complete the filming of the final battle. In his review of the film, \"The Washington Post\" film critic Hal Hinson called McKellen's performance a \"lethally flamboyant incarnation\" and said his \"florid mastery ... dominates everything\". His performance in the title role garnered BAFTA and Golden Globe nominations for Best Actor and won the European Film Award for Best Actor. His screenplay was nominated for the BAFTA Award for Best Adapted Screenplay.\nHe appeared in the modestly acclaimed film \"Apt Pupil\", which was directed by Bryan Singer and based on a story by Stephen King. McKellen portrayed a fugitive Nazi officer living under a false name in the US who is befriended by a curious teenager (Brad Renfro) who threatens to expose him unless he tells his story in detail. He was subsequently nominated for the Academy Award for Best Actor for his role in the 1998 film \"Gods and Monsters\", wherein he played James Whale, the director of \"Show Boat\" (1936) and \"Frankenstein\".\n\nIn 1999 McKellen was cast, again under the direction of Bryan Singer, to play the comic book supervillain Magneto in the 2000 film \"X-Men\" and its sequels \"X2: X-Men United\" (2003) and \"\" (2006). He later made a short appearance as an older Magneto in 2014's \"\", sharing the role with Michael Fassbender, who played a younger version of the character in 2011's \"\".\n\nWhile filming the first \"X-Men\" film in 1999, McKellen was cast as the wizard Gandalf in Peter Jackson's three-film adaptation of \"The Lord of the Rings\" (consisting of \"\" (2001), \"\" (2002), and \"\" (2003) ). He received honors from the Screen Actors Guild for Best Supporting Actor in a Motion Picture for his work in \"The Fellowship of the Ring\" and was nominated for the Academy Award for Best Supporting Actor for the same role. He provided the voice of Gandalf for several video game adaptations of the \"Lord of the Rings\" films, then reprised the role on screen in Jackson's film adaptation of \"The Hobbit\", which was released in three parts from 2012 to 2014.\n\nOn 16 March 2002, he hosted \"Saturday Night Live\". In 2003, McKellen made a guest appearance as himself on the American cartoon show \"The Simpsons\" in a special British-themed episode entitled \"The Regina Monologues\", along with the then UK Prime Minister Tony Blair and author J. K. Rowling. In April and May 2005, he played the role of Mel Hutchwright in Granada Television's long running British soap opera, \"Coronation Street\", fulfilling a lifelong ambition. He narrated Richard Bell's film \"Eighteen\" as a grandfather who leaves his World War II memoirs on audio-cassette for his teenage grandson.\n\nMcKellen has appeared in limited release films, such as \"Emile\" (which was shot in three weeks following the \"X2\" shoot), \"Neverwas\" and \"Asylum\". He appeared as Sir Leigh Teabing in \"The Da Vinci Code\". During a 17 May 2006 interview on \"The Today Show\" with the \"Da Vinci Code\" cast and director, Matt Lauer posed a question to the group about how they would have felt if the film had borne a prominent disclaimer that it is a work of fiction, as some religious groups wanted. McKellen responded, \"I've often thought the Bible should have a disclaimer in the front saying 'This is fiction.' I mean, walking on water? It takes... an act of faith. And I have faith in this movie – not that it's true, not that it's factual, but that it's a jolly good story.\" He continued, \"And I think audiences are clever enough and bright enough to separate out fact and fiction, and discuss the thing when they've seen it\". McKellen appeared in the 2006 BBC series of Ricky Gervais' comedy series \"Extras\", where he played himself directing Gervais' character Andy Millman in a play about gay lovers. McKellen received a 2007 Primetime Emmy Award for Outstanding Guest Actor - Comedy Series nomination for his performance. In 2009 he portrayed Number Two in \"The Prisoner\", a remake of the 1967 cult series \"The Prisoner\". In 2013, McKellen co-starred in the ITV sitcom \"Vicious\" as Freddie Thornhill, alongside Derek Jacobi. The series revolves around an elderly gay couple who have been together for 50 years. On 23 August 2013 the show was renewed for a six-episode second series which began airing in June 2015.\n\nIn November 2013, McKellen appeared in the \"Doctor Who\" 50th anniversary comedy homage \"The Five(ish) Doctors Reboot\". In October 2015, McKellen appeared as Norman to Anthony Hopkins' Sir in a BBC Two production of Ronald Harwood's \"The Dresser\", alongside Edward Fox and Emily Watson. In 2017, McKellen voiced Cogsworth, the Beast's loyal majordomo, who was turned into a pendulum clock, in a live-action adaptation of Disney's \"Beauty and the Beast\".\n\nIn 2017, McKellen appeared in the documentary McKellen: Playing the Part, directed by director Joe Stephenson. The documentary explores Sir Ian McKellen's life and career as and actor.\n\nMcKellen and his first partner, Brian Taylor, a history teacher from Bolton, began their relationship in 1964. Their relationship lasted for eight years, ending in 1972. They lived in London, where McKellen continued to pursue his career as an actor. For over a decade, he has lived in a five-storey Victorian conversion in Narrow Street, Limehouse. In 1978 he met his second partner, Sean Mathias, at the Edinburgh Festival. This relationship lasted until 1988, and according to Mathias, was tempestuous, with conflicts over McKellen's success in acting versus Mathias's somewhat less-successful career. Mathias later directed McKellen in \"Waiting for Godot\" at the Theatre Royal Haymarket in 2009. The pair entered into a business partnership with Evgeny Lebedev, purchasing the lease of The Grapes public house in Narrow Street.\n\nMcKellen is an atheist. In the late 1980s, McKellen lost his appetite for meat except for fish, and has since followed a mainly pescetarian diet. In 2001, Ian McKellen received the Artist Citizen of the World Award (France).\n\nHe has a tattoo of the Elvish number nine, written using J. R. R Tolkien's Constructed script of Tengwar, on his shoulder in reference to his involvement in the \"Lord of the Rings\" and the fact that his character was one of the original nine companions of the Fellowship of the Ring. The other actors of \"The Fellowship\" (Elijah Wood, Sean Astin, Orlando Bloom, Billy Boyd, Sean Bean, Dominic Monaghan and Viggo Mortensen) have the same tattoo. John Rhys-Davies, whose character was also one of the original nine companions, arranged for his stunt double to get the tattoo instead.\n\nHe was diagnosed with prostate cancer in 2006. In 2012, McKellen stated on his blog that \"There is no cause for alarm. I am examined regularly and the cancer is contained. I've not needed any treatment.\"\n\nHe became an ordained minister of the Universal Life Church in early 2013 in order to preside over the marriage of his friend and \"X-Men\" co-star Patrick Stewart to the singer Sunny Ozell.\n\nMcKellen was awarded an honorary Doctorate of Letters by Cambridge University on 18 June 2014. He was made a Freeman of the city of London on Thursday 30 October 2014. The ceremony took place at Guildhall in London. McKellen was nominated by London's Lord Mayor Fiona Woolf, who said he was chosen as he was an \"exceptional actor\" and \"tireless campaigner for equality\". He is also a Fellow of St Catherine's College, Oxford.\n\nWhile McKellen had made his sexual orientation known to fellow actors early on in his stage career, it was not until 1988 that he came out to the general public, in a programme on BBC Radio. The context that prompted McKellen's decision – overriding any concerns about a possible negative effect on his career – was that the controversial Section 28 of the Local Government Bill, known simply as Section 28, was then under consideration in the British Parliament. Section 28 proposed prohibiting local authorities from promoting homosexuality \"... as a kind of pretended family relationship\". McKellen became active in fighting the proposed law, and, during a BBC Radio 3 programme where he debated Section 28 with the conservative journalist Peregrine Worsthorne, declared himself gay. McKellen has stated that he was influenced in his decision by the advice and support of his friends, among them noted gay author Armistead Maupin. In a 1998 interview that discusses the 29th anniversary of the Stonewall riots, McKellen commented, I have many regrets about not having come out earlier, but one of them might be that I didn't engage myself in the politicking. He has said of this period: My own participating in that campaign was a focus for people [to] take comfort that if Ian McKellen was on board for this, perhaps it would be all right for other people to be as well, gay and straight. Section 28 was, however, enacted and remained on the statute books until 2000 in Scotland and 2003 in England and Wales. Section 28 never applied in Northern Ireland.\n\nIn 2003, during an appearance on \"Have I Got News For You\", McKellen claimed when he visited Michael Howard, then Environment Secretary (responsible for local government), in 1988 to lobby against Section 28, Howard refused to change his position but did ask him to leave an autograph for his children. McKellen agreed, but wrote, \"Fuck off, I'm gay.\" McKellen described Howard's junior ministers, Conservatives David Wilshire and Dame Jill Knight, who were the architects of Section 28, as the 'ugly sisters' of a political pantomime.\n\nMcKellen has continued to be very active in LGBT rights efforts. In a statement on his website regarding his activism, the actor commented that:\nMcKellen is a co-founder of Stonewall, an LGBT rights lobby group in the United Kingdom, named after the Stonewall riots. McKellen is also patron of LGBT History Month, Pride London, Oxford Pride, GAY-GLOS, The Lesbian & Gay Foundation, and FFLAG where he appears in their video \"Parents Talking\".\n\nIn 1994, at the closing ceremony of the Gay Games, he briefly took the stage to address the crowd, saying, \"I'm Sir Ian McKellen, but you can call me Serena\": This nickname, given to him by Stephen Fry, had been circulating within the gay community since McKellen's knighthood was conferred. In 2002, he was the Celebrity Grand Marshal of the San Francisco Pride Parade and he attended the Academy Awards with his then-boyfriend, New Zealander Nick Cuthell. In 2006, McKellen spoke at the pre-launch of the 2007 LGBT History Month in the UK, lending his support to the organisation and its founder, Sue Sanders. In 2007, he became a patron of The Albert Kennedy Trust, an organisation that provides support to young, homeless and troubled LGBT people.\n\nIn 2006, he became a patron of Oxford Pride, stating:I send my love to all members of Oxford Pride, their sponsors and supporters, of which I am proud to be one... Onlookers can be impressed by our confidence and determination to be ourselves and gay people, of whatever age, can be comforted by the occasion to take the first steps towards coming out and leaving the closet forever behind.\n\nMcKellen has taken his activism internationally, and caused a major stir in Singapore, where he was invited to do an interview on a morning show and shocked the interviewer by asking if they could recommend him a gay bar; the programme immediately ended. In December 2008, he was named in \"Out\" annual Out 100 list.\n\nIn 2010, McKellen extended his support for Liverpool's Homotopia festival in which a group of gay and lesbian Merseyside teenagers helped to produce an anti-homophobia campaign pack for schools and youth centres across the city. In May 2011, he called Sergey Sobyanin, Moscow's mayor, a \"coward\" for refusing to allow gay parades in the city.\n\nIn 2014, he was named in the top 10 on the World Pride Power list.\n\nIn April 2010, along with actors Brian Cox and Eleanor Bron, McKellen appeared in a series of TV advertisements to support Age UK, the charity recently formed from the merger of Age Concern and Help the Aged. All three actors gave their time free of charge.\n\nA cricket fan since childhood, McKellen umpired in March 2011 for a charity cricket match in New Zealand to support earthquake victims of the February 2011 Christchurch earthquake.\n\nMcKellen is an honorary board member for the New York and Washington, DC based organization Only Make Believe. Only Make Believe creates and performs interactive plays in children's hospitals and care facilities. He was honoured by the organisation in 2012 and hosted their annual Make Believe on Broadway Gala in November 2013. He garnered publicity for the organisation by stripping down to his Lord of the Rings underwear on stage.\n\nMcKellen also has a history of supporting individual theatres. While in New Zealand filming \"The Hobbit\" in 2012, he announced a special New Zealand tour \"Shakespeare, Tolkien, and You!\", with proceeds going to help save the Isaac Theatre Royal, which suffered extensive damage during the 2011 Christchurch earthquake. McKellen said he opted to help save the building as it was the last theatre he played in New Zealand (\"Waiting for Godot\" in 2010) and the locals' love for it made it a place worth supporting. In July 2017, he performed a new one-man show for a week at Park Theatre (London), donating the proceeds to the theatre.\n\nA friend of Ian Charleson and an admirer of his work, McKellen contributed an entire chapter to \"For Ian Charleson: A Tribute\". A recording of McKellen's voice is heard before performances at the Royal Festival Hall, reminding patrons to ensure their mobile phones and watch alarms are switched off and to keep coughing to a minimum. He also took part in the 2012 Summer Paralympics opening ceremony in London as Prospero from Shakespeare's \"The Tempest\".\n\n\n\nNotes\n\n"}
{"id": "15309", "url": "https://en.wikipedia.org/wiki?curid=15309", "title": "Intellivision", "text": "Intellivision\n\nThe Intellivision is a home video game console released by Mattel Electronics in 1979. The name \"Intellivision\" is a portmanteau of \"intelligent television\". Development of the console began in 1977, the same year as the introduction of its main competitor, the Atari 2600. In 1984 Mattel sold their video game assets to a former Mattel Electronics executive and investors that would become INTV Corporation. Games development started in 1978 and continued until 1990 when the Intellivision was discontinued. From 1980 to 1983 over 3 million Intellivision units were sold.\n\nIn 2009, video game website IGN named the Intellivision the No. 14 greatest video game console of all time. It remained Mattel's only video game console until the release of the HyperScan in 2006.\n\nThe Intellivision was developed at Mattel in Hawthorne, California along with their Mattel Electronics line of handheld electronic games. Mattel's Design and Development group began investigating a home video game system in 1977. It was to have rich graphics and long lasting gameplay to distinguish itself from its competitors. Mattel identified a new but expensive chipset from National Semiconductor and negotiated better pricing for a simpler design. Their consultant, APh Technological Consulting, suggested a General Instrument chipset, listed as the Gimini programmable set in the GI 1977 catalog. The GI chipset lacked reprogrammable graphics and Mattel worked with GI to implement changes. GI published an updated chipset in its 1978 catalog. After initially choosing National in August 1977, Mattel waited for two months before ultimately going with the proposed GI chipset in the fall of 1977. A team at Mattel, headed by David Chandler began engineering the hardware, including the famous hand controllers. In 1978, David Rolfe of APh developed the executive control software (Exec) and with a group of Caltech summer student hires, programmed the first games. Graphics were designed by a group of artists at Mattel led by Dave James.\n\nThe Intellivision was test marketed in Fresno, California in 1979 with a total of four games available. It was released nationwide in 1980 with a price tag of US$299, a pack-in game: \"Las Vegas Poker & Blackjack\" and a library of ten cartridges. Mattel Electronics would become a subsidiary in 1981.\n\nThough not the first system to challenge Warner Communications's Atari, it was the first to pose a serious threat to the market leader. A series of advertisements featuring George Plimpton were produced that demonstrated the superiority of the Intellivision's graphics and sound to those of the Atari 2600, using side-by-side game comparisons. One of the slogans of the television advertisements stated that Intellivision was \"the closest thing to the real thing\"; one example in an advertisement compared golf games. The other console's games had a blip sound and cruder graphics, while the Intellivision featured a realistic swing sound and striking of the ball, and graphics that suggested a more 3D look. There was also an advertisement comparing the Atari 2600 to it, featuring the slogan \"I didn't know\". In its first year, Mattel sold out its initial 175,000 production run of Intellivision \"Master Components\". In 1981, over 1 million Intellivision consoles were sold.\nThe Intellivision Master Component was branded and distributed by various companies. Before Mattel shifted manufacturing to Hong Kong, Mattel Intellivisions were manufactured by GTE Sylvania. \"GTE Sylvania\" Intellivisions were produced along with Mattel's, with the brand name the only differentiation. The Sears \"Super Video Arcade\", manufactured by Mattel in Hong Kong, has a restyled beige top cover and detachable controllers. The Sears Intellivision modified the default titlescreen by removing the \"Mattel Electronics\" captioning. In 1982 Radio Shack marketed the \"Tandyvision One\", similar to the original Intellivision but with the gold plates replaced with more wood trim. In Japan Intellivisions were branded by Bandai in 1982, and in Brazil there were Digimed and Digiplay Intellivisions manufactured by Sharp in 1983.\n\nInside every Intellivision is 4K of ROM containing the Exec software. It provides two benefits: reusable code that can effectively make a 4K cartridge an 8K game, and a software framework for new programmers to develop games more easily and quickly. It also allows other programmers to more easily review and continue another's project. Under the supervision of David Rolfe (APh) and graphics supplied by Mattel artist Dave James, APh was able to quickly create the Intellivision launch title library using mostly summer students. The drawback is that to be flexible and handle many different types of games the Exec runs less efficiently than a dedicated program. Intellivision games that leverage the Exec run at a 20 Hz frame rate instead of the 60 Hz frame rate for which the Intellivision was designed. Using the Exec framework is optional, but almost all Intellivision games released by Mattel Electronics are 20 Hz. The limited ROM space also meant there was no room for computer artificial intelligence and many early games required two players.\n\nInitially, all Intellivision games were programmed by the outside firm, APh Technological Consulting, with 19 cartridges produced before Christmas 1980. Once the Intellivision project became successful, software development would be brought in-house. Mattel formed its own software development group and began hiring programmers. The original five members of that Intellivision team were Mike Minkoff, Rick Levine, John Sohl, Don Daglow, and manager Gabriel Baum. Levine and Minkoff, a long-time Mattel Toys veteran, both came over from the hand-held Mattel games engineering team. During 1981 Mattel hired programmers as fast it could. Early in 1982 Mattel Electronics relocated from Mattel headquarters to an unused industrial building. Office renovation work happened as new staff moved in. To keep these programmers from being hired away by rival Atari, their identity and work location was kept a closely guarded secret. In public, the programmers were referred to collectively as the Blue Sky Rangers.\n\nMost of the early games were based on traditional real-world concepts such as sports, with an emphasis on realism and depth of play within the technology of the day. The Intellivision was not marketed as a toy; as such, games such as \"Sea Battle\" and \"B-17 Bomber\" are not made in a pick-up-and-play format which arcade style games are. Reading the instructions is often a prerequisite to play. Every cartridge produced by Mattel Electronics included two plastic controller \"overlays\" to help navigate the 12 keypad buttons, although not every game made use of the keypad. Mattel organized its games into networks: \"Major League Sports, Action, Strategy, Gaming, Children's Learning\" and later \"Space Action\", and \"Arcade\". The network concept was dropped in 1983, as were the convenient gate-fold style boxes for storing the cartridge, instructions, and overlays.\n\nStarting in 1981 programmers looking for credit and royalties on sales began leaving both APh and Mattel Electronics to create Intellivision cartridges for third party publishers. They helped form Imagic in 1981 and in 1982 others joined Activision and Atari. Cheshire Engineering was formed by a few senior APh programmers including David Rolfe, author of the Exec, and Tom Loughry who created one of the most popular Intellivision games \"\". Cheshire would create Intellivision games for Activision. Third party developers Activision, Imagic, and Coleco started producing Intellivision cartridges in 1982. And then Atari, Parker Brothers, Sega, and Interphase followed in 1983. The third party developers, not having legal access to Exec knowledge, often bypassed the Exec framework to create smooth 30 Hz and 60 Hz Intellivision games such as \"The Dreadnaught Factor\". Cheaper ROM prices also allowed for larger games as 8K, 12K, and then 16K cartridges became common. The first Mattel Electronics Intellivision game to run at 60 Hz was \"Masters of the Universe\" in 1983. Marketing dubbed the term \"Super Graphics\" on the game's packaging and marketing.\n\nMattel Electronics' team of programmers was diverse in experience and talent, proving to be an advantage. As competitors were often depending on licensing well known trademarks to sell video games, Mattel would have to focus on original ideas. Don Daglow was a key early programmer at Mattel and became director of Intellivision game development. Daglow created \"Utopia\", a precursor to the sim genre and, with Eddie Dombrower, the ground breaking sports simulation \"World Series Major League Baseball\". Daglow was also involved with the popular Intellivision games \"Tron Deadly Discs\" and \"Shark! Shark!\". After Mattel Electronics closed in 1984, their programmers would go on to make significant contributions to the video game industry. Don Daglow and Eddie Dombrower went on to Electronic Arts to create Earl Weaver Baseball, Don Daglow founded Stormfront Studios. Bill Fisher, Steve Roney and Mike Breen founded Quicksilver Software and David Warhol founded Realtime Associates.\n\nFrom the beginning, Intellivision's packaging and promotional materials, as well as television commercials, promised the addition of a soon-to-be-available accessory called the \"Keyboard Component\". The Intellivision was designed as a modular home computer. The \"Master Component\" could be purchased as a stand-alone video game system and the \"Keyboard Component\" could be added, providing the computer keyboard and tape drive. Not meant to be a hobbyist or business computer, the Intellivision home computer was meant to run pre-programmed software and bring \"data flow\" (Videotex) into the home.\n\nThe Keyboard Component added an 8-bit 6502 processor making the Intellivision a dual processor computer. It had 16K 10-bit shared RAM that could load and execute both Intellivision CP1610 and 6502 program code from tape; a large amount as typical cartridges of the day were 4K. The cassettes have two tracks of digital data and two tracks of analog audio completely controlled by the computer. Two tracks are read only for the software, and two tracks for user data. The tape-drive was block addressed with high speed indexing. A high resolution 40x24 monochrome text display could overlay regular Intellivision graphics. There was an input for a microphone and two additional expansion ports for peripherals and RAM expansion. The Microsoft BASIC programming cartridge used one of these ports. Expanded memory cartridges could support 1000 8KB pages. A third pass-through cartridge port was for regular Intellivision cartridges. It uses the Intellivision's power supply. A 40-column thermal printer was available, and a telephone modem was planned along with voice synthesis and voice recognition.\n\nDavid Rolfe of APh wrote a control program for the Keyboard Component called \"PicSe\" (Picture Sequencer) specifically for the development of multimedia applications. PicSe synchronized the graphics and analog audio while concurrently saving or loading data to tape. Productivity software for home finances, personal improvement, and self education were planned. Subject experts were consulted and their voices recorded and used in the software.\n\nThree applications using the PicSe system were released on cassette tape: \n\nFive BASIC applications were released on tape: \"Programs written in BASIC did not have access to Intellivision graphics and would be sold at a lower price.\"\n\nWhile the Keyboard Component was an ambitious piece of engineering for its time it was repeatedly delayed as the engineers tried to reduce manufacturing costs. In August 1979 the Intellivision Keyboard Component, in breadboard form, was successfully entered into the Sears Market Research Program. In December 1979 Mattel had production design working units but decided on a significant internal design change to consolidate circuit boards. In September 1980 it was test marketed in Fresno, California but without software, except for the BASIC programming cartridge. In the fall of 1981 design changes were finally implemented and the \"Keyboard Component\" was released at $600 in Seattle and New Orleans only. Those that complained in writing could buy a Keyboard Component directly from Mattel. The printer, a rebadged Alphacom Sprinter 40, was only available by mail order.\n\nThe keyboard component's repeated delays became so notorious around Mattel headquarters that comedian Jay Leno, when performing at Mattel's 1981 Christmas party, got his biggest titter of the evening with the line: \"You know what the three big lies are, don't you? 'The check is in the mail,' 'I'll still respect you in the morning,' and 'The keyboard will be out in spring.'\"\n\nComplaints from consumers who had chosen to buy the Intellivision specifically on the promise of a \"coming soon\" personal-computer upgrade, eventually caught the attention of the Federal Trade Commission (FTC), who started investigating Mattel Electronics for fraud and false advertising. In mid-1982 the FTC ordered Mattel to pay a monthly fine (said to be $10,000) until the promised computer upgrade was in full retail distribution. To protect themselves from the ongoing fines, the Keyboard Component was officially canceled in August 1982 and the Entertainment Computer System (ECS) module offered up in its place. Part of Mattel's settlement with the FTC involved offering to buy back all of the existing Keyboard Components from customers. Mattel provided a full refund but without a receipt paid $550 for the Keyboard Component, $60 for the BASIC cartridge, and $30 for each cassette software. Any customer who opted to keep theirs was required to sign a waiver with the understanding that no more software would be written for the system and absolved Mattel of any future responsibility for technical support. They were also compensated with $1000 worth of Mattel Electronics products.\n\nWhile approximately 4,000 Keyboard Components were manufactured, it is not clear how many of them actually found their way into the hands of Intellivision customers. Today, very few of them still exist. Many of the units were dismantled for parts. Others were used by Mattel Electronics programmers as part of their development system. A Keyboard Component could be interfaced with an Intellivision development system in place of the hand-built Magus board RAM cartridge. Data transfer to the Keyboard Component RAM had to be done serially and was slower compared with the Magus board parallel interface.\n\nThe keyboard component debacle was ranked as No. 11 on \"GameSpy\"s \"25 dumbest moments in gaming\".\n\nIn mid-1981, Mattel's upper management was becoming concerned that the keyboard component division would never be able to produce a sellable product. As a result, Mattel Electronics set up a competing internal engineering team whose stated mission was to produce an inexpensive add-on called the \"Basic Development System\", or BDS, to be sold as an educational device to introduce kids to the concepts of computer programming.\n\nThe rival BDS engineering group, who had to keep the project's real purpose a secret among themselves, fearing that if David Chandler, the head of the keyboard component team, found out about it he would use his influence to end the project, eventually came up with a much less expensive alternative. Originally dubbed the \"Lucky\", from LUCKI: Low User-Cost Keyboard Interface, it lacked many of the sophisticated features envisioned for the original keyboard component. Gone, for example, was the 16K (8MB max) of RAM, the secondary CPU, and high resolution text; instead, the ECS offered a mere 2KB RAM expansion, a built-in BASIC that was marginally functional, plus a much-simplified cassette and printer interface.\n\nUltimately, this fulfilled the original promises of turning the Intellivision into a computer, making it possible to write programs and store them to tape, and interfacing with a printer well enough to allow Mattel to claim that they had delivered the promised computer upgrade and stop the FTC's mounting fines. It even offered, via an additional sound chip (AY-3-8917) inside the ECS module and an optional 49-key music synthesizer keyboard, the possibility of turning the Intellivision into a multi-voice synthesizer which could be used to play or learn music.\n\nIn the fall of 1982, the LUCKI, now renamed the Entertainment Computer System (ECS), was presented at the annual sales meeting, officially ending the ill-fated keyboard component project. A new advertising campaign was aired in time for the 1982 Christmas season, and the ECS itself was shown to the public at the January 1983 Consumer Electronic Show (CES) in Las Vegas. A few months later, the ECS hit the market, and the FTC agreed to drop the $10K per day fines.\n\nBy the time the ECS made its retail debut as the Intellivision Computer Module, an internal shake-up at the top levels of Mattel Electronics' management had caused the company's focus to shift away from hardware add-ons in favor of software, and the ECS received very little in terms of furthering the marketing push. Further hardware developments, including a planned Program Expander that would have added another 16K of RAM and a more intricate, fully featured Extended-BASIC to the system, were halted. In the end a half-dozen software titles were released for the ECS; a few more were completed but not released.\n\nThe ECS also offered four player game-play with the optional addition of two extra hand controllers. Four player games were in development when Mattel Electronics closed in 1984. \"World Cup Soccer\" was later completed and released in 1985 by Dextel in Europe and then INTV Corporation in North America. The documentation does not mention it but when the ECS Computer Adapter is used, \"World Cup Soccer\" can be played with one to four players, or two players cooperatively against the computer.\n\nIn 1982 Mattel introduced a new peripheral for the Intellivision: the \"Intellivoice Voice Synthesis Module\". A speech synthesizer which produces speech with compatible cartridges. The Intellivoice was original in two respects: human sounding male and female voices with distinct accents, and the speech-supporting games were designed with speech being an integral part of the game-play.\n\nLike the Intellivision chip-set, the Intellivoice chip-set was developed by General Instrument. The SP0256-012 orator chip has 2KB ROM inside, and is used to store the speech for numerical digits, some common words, and the phrase \"Mattel Electronics presents\". Speech can also be processed from the Intellivoice's SP650 buffer chip, stored and loaded from cartridge memory. That buffer chip has its own I/O and the Intellivoice has a 30-pin expansion port under a removable top plate. Mattel Electronics planned to use that connector for wireless hand controllers.\n\nMattel Electronics built a state of the art voice processing lab to produce the phrases used in Intellivoice games. However, the amount of speech that could be compressed into an 8K or 12K cartridge and still leave room for a game was limited. Intellivoice cartridges \"Space Spartans\" and \"B-17 Bomber\" did sell about 300,000 copies each, priced a few dollars more than regular Intellivision cartridges. However, at $79 the Intellivoice did not sell as well as Mattel expected, and Intellivoices were later offered free with the purchase of a Master Component. In August 1983 the Intellivoice system was quietly phased out. A children's title called \"Magic Carousel\", and foreign language versions of \"Space Spartans\" were completed but shelved. Additional games \"Woody Woodpecker\" and \"Space Shuttle\" went unfinished with the voice recordings unused.\n\nThe four titles available for the Intellivoice system, in order of their release, were:\n\n\nA fifth title, \"Intellivision World Series Major League Baseball\", developed as part of the Entertainment Computer System series, also supports the Intellivoice if both the ECS and Intellivoice are connected concurrently. Unlike the Intellivoice-specific games, however, \"World Series Major League Baseball\" is also playable without the Intellivoice module (but not without the ECS).\n\nIn the spring of 1983, Mattel introduced the \"Intellivision II\". Not the technology upgrade that was expected but a cheaper, compact replacement to the original. The Intellivision II was designed to be inexpensive to manufacture and service, with updated styling. It also had longer controller cords. The Intellivision II was initially released without a pack-in game but was later packaged with BurgerTime in the United States and Lock'N'Chase in Canada. In 1984 the Digiplay Intellivision II was introduced in Brazil. Brasil was the only country outside North America to have the redesigned Intellivision II.\n\nUsing an external AC Adapter (16.2VAC), consolidating some ICs, and taking advantage of relaxed FCC emission standards, the Intellivision II has a significantly smaller footprint than the original. The controllers, now detachable, have a different feel with plastic rather than rubber side buttons, and a flat membrane keypad. Users of the original Intellivision would miss the ability to find keypad buttons by the tactile feel of the original controller bubble keypad.\n\nOne functional difference was the addition of a video input to the cartridge port; added specifically to support the System Changer. The System Changer, also released in 1983 by Mattel, is an Intellivision peripheral that plays Atari 2600 cartridges through the Intellivision. The Intellivision hand controllers can be used to play Atari 2600 games. The System Changer also has two controller ports compatible with Atari joysticks. The original Intellivision requires a hardware modification in order to work with the \"System Changer\"; a service provided by Mattel. Otherwise the Intellivision II was promoted to be compatible with the original.\n\nIt was discovered that a few Coleco Intellivision games did not work on the Intellivision II. Mattel secretly changed the Intellivision's internal ROM program (Exec) in an attempt to lock out 3rd party titles. A few of Coleco's early games were affected but the 3rd party developers quickly figured out how to get around it. Mattel's own \"Electric Company Word Fun\", however, will not run on the Intellivision II due to this change. In an unrelated issue but also due to Exec changes, Super Pro Football experiences a minor glitch where the quarterback does not appear until after the ball is hiked. There were also some minor changes to the sound chip (AY-3-8914A/AY-3-8916) affecting sound effects in some games. Programmers at Mattel discovered the audio differences and avoided the problem in future games.\n\nDave Chandler's group began designing Mattel's next generation console in 1981 codenamed \"Decade\", now referred to as the \"Intellivision IV\". It was based on the 32-bit MC68000 processor and a custom designed advanced graphic interface chip. Specifications called for dual display support, 240x192 resolution bitmap and 40x24 tiled graphics modes, four colors per tile, 16 multicolored sprites per scan-line with 3D rotation and shading, 16 programmable 12-bit colors and antialiasing. A machine that could lead Mattel Electronics into the 1990s, however on August 4, 1983 most hardware people at Mattel Electronics had been laid off.\n\nIn 1982, with new machines introduced by competitors, Mattel marketing wanted to bring an upgraded system to market sooner. The \"Intellivision III\" was to be an upgraded but backward compatible system; based on a similar CP1610 processor and with an improved graphics STIC chip producing double the resolution with more sprites and colors. The Intellivision III existed in the lab and a new EXEC was written for it but little else. It was cancelled in mid-1983. A Mattel document called \"Target Specification Intellivision III\" has the following.\n\n\nAccording to the company's 1982 10-K report to the Securities and Exchange Commission, Mattel had almost 20% of the domestic video-game market. Although the Atari 2600 had more third-party development, \"Creative Computing Video & Arcade Games\" reported after visiting the summer 1982 Consumer Electronics Show that \"The momentum is tremendous\". Activision and Imagic began releasing games for the Intellivision, as did hardware rival Coleco. Mattel created \"M Network\" branded games for Atari's system. The company's advertisement budget increased to over $20 million for the year. In its October 1982 stockholders' report Mattel announced that \"Electronics\" had, so far that year, posted a nearly $100 million profit on nearly $500 million sales; a threefold increase over October 1981.\n\nHowever, the same report predicted a loss for the upcoming quarter. Still hiring continued, and optimism that the investment in software and hardware development will payoff. The \"M Network\" brand expanded to personal computers. An office in Taiwan was opened to handle Apple II programming. The original five-person Mattel game development team had grown to 110 people under new vice president Baum, while Daglow led Intellivision development and top engineer Minkoff directed all work on all other platforms. In February 1983, Mattel Electronics opened an office in the south of France to provide European input to Intellivision games and develop games for the ColecoVision. At its peak Mattel Electronics employed 1800 people.\n\nAmid the flurry of new hardware and software development, there was trouble for the Intellivision. New game systems (ColecoVision and Atari 5200) introduced in 1982 took advantage of falling RAM prices to offer graphics closer to arcade quality. In 1983 the price of home computers, particularly the Commodore 64 came down drastically to compete with video game system sales. The market became flooded with hardware and software, and retailers were ill-equipped to cope. In spring 1983 hiring at Mattel Electronics came to a halt.\n\nAt the June 1983 Consumer Electronics Show in Chicago, Mattel Electronics had the opportunity to show off all their new products. The response was underwhelming. Amidst massive losses top management was replaced. On July 12, 1983, Mattel Electronics President Josh Denham was replaced with outsider Mack Morris. Morris brought in former Mattel Electronics president and marketing director Jeff Rochlis as a consultant and all projects were under review. The Intellivision III was cancelled and then all new hardware development was stopped when 660 jobs were cut on August 4. The price of the Intellivision II (which launched at $150 earlier that year) was lowered to $69, Mattel Electronics was to be a software company. However, by October 1983 \"Electronics\"' losses were over $280 million and one third of the programming staff were laid off. In November another third were gone, and on January 20, 1984 the remaining programming staff were laid-off. The Taiwan and French offices continued a little while longer due to contract and legal obligations. On February 4, 1984 Mattel sold the Intellivision business for $20 million. In 1983. 750,000 Intellivision Master Components were sold, more than three million units from 1980 to 1983.\n\nFormer Mattel Electronics Senior Vice President of Marketing, Terrence Valeski, understood that although losses were huge, the demand for video games increased in 1983. Valeski found investors and purchased the rights to Intellivision, the games, and inventory from Mattel. A new company, Intellivision Inc, was formed and by the end of 1984 Valeski bought out the other investors and changed the name to INTV Corporation. They continued to supply the large toy stores and sold games through direct mail order. At first they sold the existing inventory of games and Intellivision II systems. When the inventory of games sold out they produced more, but without the Mattel name or unnecessary licenses on the printed materials. To lower costs, the boxes, instructions, and overlays were produced at lower quality compared to Mattel.\n\nIn France, the Mattel Electronics office found investors and became Nice Ideas in April 1984. They continued to work on Intellivision, Colecovision, and other computer games. They produced Intellivision \"World Cup Soccer\" and \"Championship Tennis\", both released in 1985 by European publisher Dextel.\n\nIn 1985 INTV Corporation introduced the \"INTV System III\", also branded as the \"Intellivision Super Pro System\", using the same design as the original Intellivision model but in black and silver. That same year INTV Corp introduced two new games that were completed at Mattel but not released: \"Thunder Castle\" and \"World Championship Baseball\". With their early success INTV Corp decided to produce new games and in 1986 introduced \"Super Pro Football\", an update of Mattel \"NFL Football\". INTV Corp continued a relationship that Mattel had with Data East and produced all new titles such as \"Commando\" in 1987 and \"Body Slam Wrestling\" in 1988. Also in 1987 INTV Corp released \"Dig Dug\", purchased from Atari where the game was completed but not released in 1984. They also got into producing next generation games with the production of \"Monster Truck Rally\" for Nintendo NES in 1991, also released as \"Stadium Mud Buggies\" for Intellivision in 1989.\n\nLicensing agreements with Nintendo and Sega required INTV Corporation to discontinue the Intellivision in 1990. INTV Corporation did publish 21 new Intellivision cartridges bringing the Intellivision library to a total of 124 cartridges plus one compilation cartridge.\n\nIn 1989 INTV Corp and World Book Encyclopedia entered into an agreement to manufacture an educational video game system called Tutorvision. It is a modified Intellivision, the case molded in light beige with gold and blue trim. The Exec ROM expanded, system RAM increased to 1.75K, and graphics RAM increased to 2KB. That is enough graphics RAM to define unique graphic tiles for the entire screen.\n\nGames were designed by World Book, \"J. Hakansson Associates\", and programmed by Realtime Associates. Sixteen titles were in production, plus one Canadian variation. However, the cartridges and the Tutorvision were never released; instead World Book and INTV Corporation sued each other. In 1990 INTV Corporation filed for bankruptcy protection and closed in 1991.\n\nAn unknown number of later Intellivision SuperPro systems have Tutorvision hardware inside. A subset of these units contain the full Tutorvision EXEC and can play Tutorvision games.\n\nIntellivision games became readily available again when Keith Robinson and Stephen Roney, both former Intellivision programmers at Mattel Electronics, obtained exclusive rights to the Intellivision and games in 1997. That year they formed \"Intellivision Productions\" and made \"Intellivision for PC Volume 1\" available as a free download. Intellivision games could be played on a modern computer for the first time. That download includes three Intellivision games and an MS-DOS Intellivision emulator that plays original game code. It was followed by \"Volume 2\" and another three games including \"Deep Pockets Super Pro Pool & Billiards\"; a game completed in 1990 but never released until this download in 1997. In 2000 the \"Intellipack 3\" download was available with another four Intellivision games and emulators for Windows or Macintosh.\n\nIntellivision Productions released Intellivision Lives! and Intellivision Rocks on compact disc in 1998 and 2001. These compilation CDs play the original game code through emulators for MS-DOS, Windows, and Macintosh computers. Together they have over 100 Intellivision games including never before released \"King of the Mountain, Takeover, Robot Rubble\", \"League of Light\", and others. Intellivision Rocks includes Intellivison Activision and Imagic games licensed from Activision. Some games could not be included due to licensing, others simply used different titles to avoid trademarked names. The CDs are also a resource for development history, box art, hidden features, programmer biographies, video interviews, and original commercials.\n\nAlso in 1997 Intellivision Productions announced they would sell development tools allowing customers to program their own Intellivision games. They were to provide documentation, PC compatible cross-assemblers, and the \"Magus II\" PC Intellivision cartridge interface. Unfortunately, the project was cancelled but they did provide copies of \"Your Friend the EXEC\", the programmers guide to the Intellivision Executive control software. By 2000 Intellivision hobbyists ultimately created their own development tools, including Intellivision memory cartridges.\n\nIn 2005 Intellivision Productions announced that new Intellivision cartridges were to be produced. \"Deep Pockets and Illusions will be the first two releases in a series of new cartridges for the Intellivision. The printed circuit boards, the cartridge casings, the boxes are all being custom manufactured for this special series.\" \"Illusions\" was completed at Mattel Electronics' French office in 1983 but never released. \"Deep Pockets Super Pro Pool & Billiards\" was programmed for INTV Corporation in 1990 and only released as a ROM file in 1998. However, no cartridges were produced. Previously, in 2000, Intellivision Productions did release new cartridges for the Atari 2600 and Colecovision. \"Sea Battle\" and \"Swordfight\" were Atari 2600 games created by Mattel Electronics in the early 1980s but not previously released. \"Steamroller\" (Colecovision) was developed for Activision in 1984 and not previously released.\n\nAlso in 1999, Activision released \"A Collection of Intellivision Classic Games\" for PlayStation. Also known as \"Intellivision Classics\", it has 30 emulated Intellivision games as well as video interviews of some of the original programmers. All of the games were licensed from Intellivision Productions and none of the Activision or Imagic Intellivision games were included. In 2003, Crave Entertainment released a PlayStation 2 version of Intellivision Lives! and then Xbox and GameCube version in 2004. In 2010 Virtual Play Games released Intellivision Lives! for the Nintendo DS including one never before released game, \"Blow Out\". In 2008 Microsoft made Intellivision Lives! an available download on the Xbox Live Marketplace as an Xbox Original and playable on the Xbox 360.\n\nIn 2003, the Intellivision 25 and Intellivision 15 direct-to-TV systems were released by Techno Source Ltd. These are an all-in-one single controller design that plugs directly into a television. One includes 25 games the other ten. These Intellivision games were not emulated but rewritten for the native processor (Nintendo NES compatible) and adapted to a contemporary controller. As such they look and play differently than Intellivision. In 2005 they were updated for two-player play as the Intellivision X2 with 15 games. They were commercially very successful altogether selling about 4 million units by end of 2006.\n\nSeveral licensed Intellivision games became available to Windows computers through the GameTap subscription gaming service in 2005 including \"Astrosmash, Buzz Bombers, Hover Force, Night Stalker, Pinball, Shark! Shark!, Skiing and Snafu\". Installation of the GameTap Player software was required to access the emulator and games. The VH1 Online Arcade made nine Intellivision games available in 2007. Using a Shockwave emulator these Intellivision games could be played directly through a web browser with Shockwave Player. In 2010 VH1 Classic and MTV Networks released 6 Intellivision games to iOS. Intellivision games were first adapted to mobile phones and published by THQ Wireless in 2001. On March 24, 2010, Microsoft launched the Game Room service for Xbox Live and Games for Windows Live. This service includes support for Intellivision titles and allows players to compete against one another for high scores via online leaderboards. At the 2011 Consumer Electronics Show, Microsoft announced a version of Game Room for Windows Phone, promising a catalog of 44 Intellivision titles. Atgames and their Direct2Drive digital store has Windows compatible Intellivision compilations available for download purchase.\n\nThe number of Intellivision games that can be played effectively with contemporary game controllers is limited. On October 1, 2014, AtGames Digital Media, Inc., under license from Intellivision Productions, Inc., released the Intellivision Flashback classic game console. It is a miniature sized Intellivision console with two original sized Intellivision controllers. While adapters have been available to interface original Intellivision controllers to personal computers, the Intellivision Flashback includes two new Intellivision controllers identical in layout and function to the originals. It comes with 60 emulated Intellivision games built into ROM and a sample set of plastic overlays for 10 games. The Advanced Dungeons & Dragons games were included as \"Crown of Kings\" and \"Minotaur\". As with many of the other Intellivision compilations, no games requiring third party licensing were included.\n\nIn May 2018, Tommy Tallarico announced that he plans to launch a new Intellivision console system, and formed a new company \"Intellivision Entertainment\", serving as president. \"Intellivision Productions\" has been renamed \"Blue Sky Rangers Inc.\" and their video game intellectual property has been transferred to \"Intellivision Entertainment\". \n\nAt the Portland Retro Gaming Expo, in October 2018, the Intellivision Amico was officially revealed, and is scheduled to launch on October 10, 2020. The Amico features a \"21st century 2D chip and architecture\", making the console strictly for games with two-dimensional graphics. Intellivision Entertainment are also planning for the Amico to have an all-exclusive library of games, with Intellivision Entertainment funding some developers to make games for the platform. All games will be rated E-everyone or E10+. Updated versions of many of the popular classic Intellivision games will be available with improved graphics and sound, and unlike previous Intellivision compilations, will include some previous third-party licenses, such as Tron Deadly Discs. New versions of several classic Atari games, as well as games from Imagic, along with arcade classics such as Moon Patrol and BurgerTime are planned.\n\nKen Uston published \"Ken Uston's Guide to Buying and Beating the Home Video Games\" in 1982 as a guide to potential buyers of console systems/cartridges, as well as a brief strategy guide to numerous cartridge games then in existence. He described Intellivision as \"the most mechanically reliable of the systems… The controller (used during \"many hours of experimentation\") worked with perfect consistency. The unit never had overheating problems, nor were loose wires or other connections encountered.\" However, Uston rated the controls and control system as \"below average\" and the worst of the consoles he tested (including Atari 2600, Magnavox Odyssey², Astrovision, and Fairchild Channel F).\n\nJeff Rovin lists \"Intellivision\" as one of the seven major suppliers of videogames in 1982, and mentions it as \"the unchallenged king of graphics\", however stating that the controllers can be \"difficult to operate\", the fact that if a controller breaks the entire unit must be shipped off for repairs (since they did not detach at first), and that the overlays \"are sometimes so stubborn as to tempt one's patience\" .\n\nA 1996 article in \"Next Generation\" said the Intellivision \"had greater graphics power than the dominant Atari 2600. It was slower than the 2600 and had less software available, but it was known for its superior sports titles.\" A year later \"Electronic Gaming Monthly\" assessed the Intellivision in an overview of older gaming consoles, remarking that the controllers \"were as comfortable as they were practical. The unique disk-shaped directional pad provided unprecedented control for the time, and the numeric keypad opened up new options previously unavailable in console gaming.\" They praised the breadth of the software library but said there was a lack of genuinely stand-out games.\n\n\n\"Intellivision, Super Video Arcade, Tandyvision One, Intellivision II, INTV System III, Super Pro System\"\n\nThe Intellivision controller features:\n\n\nThe directional pad was called a \"control disc\" and marketed as having the \"functionality of both a joystick and a paddle\". The controller was ranked the fourth worst video game controller by IGN editor Craig Harris.\n\n\n\n"}
{"id": "15316", "url": "https://en.wikipedia.org/wiki?curid=15316", "title": "Imperialism", "text": "Imperialism\n\nImperialism is a state government, practice, or advocacy of extending power and dominion, especially by direct territorial acquisition or by gaining political and economic control of other areas. Because it always involves the use of power, whether military force or some subtler form, imperialism has often been considered morally reprehensible, and the term is frequently employed in international propaganda to denounce and discredit an opponent's foreign policy.\n\nIt is different from new imperialism, as the term \"imperialism\" is usually applied to the colonization of the Americas between the 15th and 19th centuries, as opposed to the expansion of Western Powers and Japan during the late 19th and early 20th centuries. However, both are examples of imperialism.\n\nThe word imperialism originated from the Latin word \"imperium\", which means supreme power. It first became common with its current sense in Great Britain, during the 1870s and was used with a negative connotation. Previously the word imperialism had been used to describe to what was perceived as Napoleon III's attempts of obtaining political support through foreign military interventions. The term was and is mainly applied to Western (and Japanese) political and economic dominance, especially in Asia and Africa, in the 19th and 20th centuries. Its precise meaning continues to be debated by scholars. Some writers, such as Edward Said, use the term more broadly to describe any system of domination and subordination organised with an imperial center and a periphery. This definition encompasses both nominal empires and neocolonialism.\n\n\"The word 'empire' comes from the Latin word imperium; for which the closest modern English equivalent would perhaps be 'sovereignty', or simply 'rule'\". The greatest distinction of an empire is through the amount of land that a nation has conquered and expanded. Political power grows from conquering land; however, cultural and economic aspects flourish through sea and trade routes. A distinction about empires is \"that although political empires were built mostly by expansion overland, economic and cultural influences spread at least as much by sea\". Some of the main aspects of trade that went overseas consisted of animals and plant products. European empires in Asia and Africa \"have come to be seen as the classic forms of imperialism: and indeed most books on the subject confine themselves to the European seaborne empires\". European expansion caused the world to be divided by how developed and developing nation are portrayed through the world systems theory. The two main regions are the core and the periphery. The core consists of areas of high income and profit; the periphery is on the opposing side of the spectrum consisting of areas of low income and profit. These critical theories of Geo-politics have led to increased discussion of the meaning and impact of imperialism on the modern post-colonial world. The Russian leader Lenin suggested that \"imperialism was the highest form of capitalism, claiming that imperialism developed after colonialism, and was distinguished from colonialism by monopoly capitalism\". This idea from Lenin stresses how important new political world order has become in our modern era. Geopolitics now focuses on states becoming major economic players in the market; some states today are viewed as empires due to their political and economic authority over other nations.\nThe term \"imperialism\" is often conflated with \"colonialism\"; however, many scholars have argued that each have their own distinct definition. Imperialism and colonialism have been used in order to describe one's perceived superiority, domination and influence upon a person or group of people. Robert Young writes that while imperialism operates from the center, is a state policy and is developed for ideological as well as financial reasons, colonialism is simply the development for settlement or commercial intentions. However, colonialism still includes invasion. Colonialism in modern usage also tends to imply a degree of geographic separation between the colony and the imperial power. Particularly, Edward Said distinguishes the difference between imperialism and colonialism by stating; \"imperialism involved 'the practice, the theory and the attitudes of a dominating metropolitan center ruling a distant territory', while colonialism refers to the 'implanting of settlements on a distant territory.' Contiguous land empires such as the Russian or Ottoman have traditionally been excluded from discussions of colonialism, though this is beginning to change, since it is accepted that they also sent populations into the territories they ruled. Thus it can be said that imperialism includes some form of colonialism, but colonialism itself does not automatically imply imperialism, as it lacks a political focus.\n\nImperialism and colonialism both dictate the political and economic advantage over a land and the indigenous populations they control, yet scholars sometimes find it difficult to illustrate the difference between the two. Although imperialism and colonialism focus on the suppression of \"an other\", if colonialism refers to the process of a country taking physical control of another, imperialism refers to the political and monetary dominance, either formally or informally. Colonialism is seen to be the architect deciding how to start dominating areas and then imperialism can be seen as creating the idea behind conquest cooperating with colonialism. Colonialism is when the imperial nation begins a conquest over an area and then eventually is able to rule over the areas the previous nation had controlled. Colonialism's core meaning is the exploitation of the valuable assets and supplies of the nation that was conquered and the conquering nation then gaining the benefits from the spoils of the war. The meaning of imperialism is to create an empire, by conquering the other state's lands and therefore increasing its own dominance. Colonialism is the builder and preserver of the colonial possessions in an area by a population coming from a foreign region. Colonialism can completely change the existing social structure, physical structure and economics of an area; it is not unusual that the characteristics of the conquering peoples are inherited by the conquered indigenous populations. Few colonies remain remote from their mother country. Thus, most will eventually establish a separate nationality or remain under complete control of their mother colony.\n\nStephen Howe, while generally hostile to empires, has summarized the beneficial effects of the main empires:\nA controversial aspect of imperialism is the defense and justification of empire-building based on seemingly rational grounds. In ancient China, tianxia denoted the lands, space, and area divinely appointed to the Emperor by universal and well-defined principles of order. The center of this land was directly apportioned to the Imperial court, forming the center of a world view that centered on the Imperial court and went concentrically outward to major and minor officials and then the common citizens, tributary states, and finally ending with the fringe \"barbarians\". Tianxia's idea of hierarchy gave Chinese a privileged position and was justified through the promise of order and peace. J. A. Hobson identifies this justification on general grounds as: \"It is desirable that the earth should be peopled, governed, and developed, as far as possible, by the races which can do this work best, i.e. by the races of highest 'social efficiency'\". Many others argued that imperialism is justified for several different reasons. Friedrich Ratzel believed that in order for a state to survive, imperialism was needed. Halford Mackinder felt that Great Britain needed to be one of the greatest imperialists and therefore justified imperialism. The purportedly scientific nature of \"Social Darwinism\" and a theory of races formed a supposedly rational justification for imperialism. Under this doctrine, the French politician Jules Ferry could declare in 1883 that \"Superior races have a right, because they have a duty. They have the duty to civilize the inferior races.\" The rhetoric of colonizers being racially superior appears to have achieved its purpose, for example throughout Latin America \"whiteness\" is still prized today and various forms of blanqueamiento (whitening) are common.\n\nThe Royal Geographical Society of London and other geographical societies in Europe had great influence and were able to fund travelers who would come back with tales of their discoveries. These societies also served as a space for travellers to share these stories. Political geographers such as Friedrich Ratzel of Germany and Halford Mackinder of Britain also supported imperialism. Ratzel believed expansion was necessary for a state's survival while Mackinder supported Britain's imperial expansion; these two arguments dominated the discipline for decades.\n\nGeographical theories such as environmental determinism also suggested that tropical environments created uncivilized people in need of European guidance. For instance, American geographer Ellen Churchill Semple argued that even though human beings originated in the tropics they were only able to become fully human in the temperate zone. Tropicality can be paralleled with Edward Said's Orientalism as the west's construction of the east as the \"other\". According to Said, orientalism allowed Europe to establish itself as the superior and the norm, which justified its dominance over the essentialized Orient.\n\nTechnology and economic efficiency were often improved in territories subjected to imperialism through the building of roads, other infrastructure and introduction of new technologies.\n\nThe principles of imperialism are often generalizable to the policies and practices of the British Empire \"during the last generation, and proceeds rather by diagnosis than by historical description\". British imperialism in some sparsely-inhabited regions appears to have applied a principle now termed Terra nullius (Latin expression which stems from Roman law meaning 'empty land'). The country of Australia serves as a case study in relation to British settlement and colonial rule of the continent in the eighteenth century, that was arguably premised on \"terra nullius\", as its settlers considered it unused by its original inhabitants.\n\nImperial control, territorial and cultural, is justified through discourses about the imperialists' understanding of different spaces. Conceptually, imagined geographies explain the limitations of the imperialist understanding of the societies (human reality) of the different spaces inhabited by the non–European Other.\n\nIn \"Orientalism\" (1978), Edward Said said that the West developed the concept of The Orient—an imagined geography of the Eastern world—which functions as an essentializing discourse that represents neither the ethnic diversity nor the social reality of the Eastern world. That by reducing the East into cultural essences, the imperial discourse uses place-based identities to create cultural difference and psychologic distance between \"We, the West\" and \"They, the East\" and between \"Here, in the West\" and \"There, in the East\".\n\nThat cultural differentiation was especially noticeable in the books and paintings of early Oriental studies, the European examinations of the Orient, which misrepresented the East as irrational and backward, the opposite of the rational and progressive West. Defining the East as a negative vision of the Western world, as its inferior, not only increased the sense-of-self of the West, but also was a way of ordering the East, and making it known to the West, so that it could be dominated and controlled. Therefore, Orientalism was the ideological justification of early Western imperialism—a body of knowledge and ideas that rationalized social, cultural, political, and economic control of other, non-white peoples.\n\nOne of the main tools used by imperialists was cartography. Cartography is \"the art, science and technology of making maps\" but this definition is problematic. It implies that maps are objective representations of the world when in reality they serve very political means. For Harley, maps serve as an example of Foucault's power and knowledge concept.\n\nTo better illustrate this idea, Bassett focuses his analysis of the role of nineteenth-century maps during the \"scramble for Africa\". He states that maps \"contributed to empire by promoting, assisting, and legitimizing the extension of French and British power into West Africa\". During his analysis of nineteenth-century cartographic techniques, he highlights the use of blank space to denote unknown or unexplored territory. This provided incentives for imperial and colonial powers to obtain \"information to fill in blank spaces on contemporary maps\".\n\nAlthough cartographic processes advanced through imperialism, further analysis of their progress reveals many biases linked to eurocentrism. According to Bassett, \"[n]ineteenth-century explorers commonly requested Africans to sketch maps of unknown areas on the ground. Many of those maps were highly regarded for their accuracy\" but were not printed in Europe unless Europeans verified them.\nImperialism in ancient times is clear in the history of China and in the history of western Asia and the Mediterranean—an unending succession of empires. The tyrannical empire of the Assyrians was replaced (6th–4th century BCE) by that of the Persians, in strong contrast to the Assyrian in its liberal treatment of subjected peoples, assuring it long duration. It eventually gave way to the imperialism of Greece. When Greek imperialism reached an apex under Alexander the Great (356–323 BCE), a union of the eastern Mediterranean with western Asia was achieved. But the cosmopolis, in which all citizens of the world would live harmoniously together in equality, remained a dream of Alexander. It was partially realized when the Romans built their empire from Britain to Egypt.\n\nCultural imperialism is an extremely fuzzy concept, pointing to the supposed influence of one dominant culture over others, i.e. a form of soft power, which changes the moral, cultural, and societal worldview of the subordinate country. In some ways, this is such an expansion of the concept of imperialism as to be meaningless. This is more than just \"foreign\" music, television or film becoming popular with young people, but that popular culture changing their own expectations of life and their desire for their own country to become more like the foreign country depicted. For example, depictions of opulent American lifestyles in the soap opera Dallas during the Cold War changed the expectations of Romanians; a more recent example is the influence of smuggled South Korean drama series in North Korea. The importance of soft power is not lost on authoritarian regimes, fighting such influence with bans on foreign popular culture, control of the internet and unauthorised satellite dishes etc. Nor is such a usage of culture recent, as part of Roman imperialism local elites would be exposed to the benefits and luxuries of Roman culture and lifestyle, with the aim that they would then become willing participants.\n\nImperialism has been subject to moral or immoral censure by its critics, and thus the term is frequently used in international propaganda as a pejorative for expansionist and aggressive foreign policy.\n\nThe Age of Imperialism, a time period beginning around 1760, saw European industrializing nations, engaging in the process of colonizing, influencing, and annexing other parts of the world. 19th century episodes included the \"Scramble for Africa.\"\n\nIn the 1970s British historians John Gallagher (1919–1980) and Ronald Robinson (1920–1999) argued that European leaders rejected the notion that \"imperialism\" required formal, legal control by one government over a colonial region. Much more important was informal control of independent areas. According to Wm. Roger Louis, \"In their view, historians have been mesmerized by formal empire and maps of the world with regions colored red. The bulk of British emigration, trade, and capital went to areas outside the formal British Empire. Key to their thinking is the idea of empire 'informally if possible and formally if necessary.'\" Oron Hale says that Gallagher and Robinson looked at the British involvement in Africa where they \"found few capitalists, less capital, and not much pressure from the alleged traditional promoters of colonial expansion. Cabinet decisions to annex or not to annex were made, usually on the basis of political or geopolitical considerations.\"\n\nLooking at the main empires from 1875–1914, historians estimate a mixed record in terms of profitability. At first planners expected that colonies would provide an excellent captive market for manufactured items. Apart from India, this was seldom true. By the 1890s, imperialists saw the economic benefit primarily in the production of inexpensive raw materials to feed the domestic manufacturing sector. Overall, Great Britain did very well in terms of profits from India, but not from most of the rest of its empire. The Netherlands did very well in the East Indies. Germany and Italy got very little trade or raw materials from their empires. France did slightly better. The Belgian Congo was notoriously profitable when it was a capitalistic rubber plantation owned and operated by King Leopold II as a private enterprise. However, scandal after scandal regarding very badly mistreated labour led the international community to force the government of Belgium to take it over in 1908, and it became much less profitable. The Philippines cost the United States much more than expected because of military action against rebels.\n\nBecause of the resources made available by imperialism, the world's economy grew significantly and became much more interconnected in the decades before World War I, making the many imperial powers rich and prosperous.\n\nEurope's expansion into territorial imperialism was largely focused on economic growth by collecting resources from colonies, in combination with assuming political control by military and political means. The colonization of India in the mid-18th century offers an example of this focus: there, the \"British exploited the political weakness of the Mughal state, and, while military activity was important at various times, the economic and administrative incorporation of local elites was also of crucial significance\" for the establishment of control over the subcontinent's resources, markets, and manpower. Although a substantial number of colonies had been designed to provide economic profit and to ship resources to home ports in the seventeenth and eighteenth centuries, Fieldhousesuggests that in the nineteenth and twentieth centuries in places such as Africa and Asia, this idea is not necessarily valid:\n\nDuring this time, European merchants had the ability to \"roam the high seas and appropriate surpluses from around the world (sometimes peaceably, sometimes violently) and to concentrate them in Europe\".\n\nEuropean expansion greatly accelerated in the 19th century. To obtain raw materials, Europe expanded imports from other countries and from the colonies. European industrialists sought raw materials such as dyes, cotton, vegetable oils, and metal ores from overseas. Concurrently, industrialization was quickly making Europe the center of manufacturing and economic growth, driving resource needs.\n\nCommunication became much more advanced during European expansion. With the invention of railroads and telegraphs, it became easier to communicate with other countries and to extend the administrative control of a home nation over its colonies. Steam railroads and steam-driven ocean shipping made possible the fast, cheap transport of massive amounts of goods to and from colonies.\n\nAlong with advancements in communication, Europe also continued to advance in military technology. European chemists made new explosives that made artillery much more deadly. By the 1880s, the machine gun had become a reliable battlefield weapon. This technology gave European armies an advantage over their opponents, as armies in less-developed countries were still fighting with arrows, swords, and leather shields (e.g. the Zulus in Southern Africa during the Anglo-Zulu War of 1879).\n\nAnglophone academic studies often base their theories regarding imperialism on the British experience of Empire. The term \"imperialism\" was originally introduced into English in its present sense in the late 1870s by opponents of the allegedly aggressive and ostentatious imperial policies of British Prime Minister Benjamin Disraeli. Supporters of \"imperialism\" such as Joseph Chamberlain quickly appropriated the concept. For some, imperialism designated a policy of idealism and philanthropy; others alleged that it was characterized by political self-interest, and a growing number associated it with capitalist greed.\n\nJohn A. Hobson, A leading English Liberal, developed a highly influential interpretation of \"Imperialism: A Study\" (1902) that expanded on his belief that free enterprise capitalism had a negative impact on the majority of the population. In \"Imperialism\" he argued that the financing of overseas empires drained money that was needed at home. It was invested abroad because lower wages paid the workers overseas made for higher profits and higher rates of return, compared to domestic wages. So although domestic wages remained higher, they did not grow nearly as fast as they might have otherwise. Exporting capital, he concluded, put a lid on the growth of domestic wages in the domestic standard of living. . By the 1970s, historians such as David K. Fieldhouse and Oron Hale could argue that \"the Hobsonian foundation has been almost completely demolished.\" The British experience failed to support it. However, European Socialists picked up Hobson's ideas and made it into their own theory of imperialism, most notably in Lenin's \"Imperialism, the Highest Stage of Capitalism\" (1916). Lenin portrayed Imperialism as the closure of the world market and the end of capitalist free-competition that arose from the need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion. Later Marxist theoreticians echo this conception of imperialism as a structural feature of capitalism. which explained the World War as the battle between imperialists for control of external markets. Lenin's treatise became a standard textbook that flourished until the Collapse of communism in 1989–91.\n\nSome theoreticians on the non-Communist left have emphasized the structural or systemic character of \"imperialism\". Such writers have expanded the period associated with the term so that it now designates neither a policy, nor a short space of decades in the late 19th century, but a world system extending over a period of centuries, often going back to Christopher Columbus and, in some accounts, to the Crusades. As the application of the term has expanded, its meaning has shifted along five distinct but often parallel axes: the moral, the economic, the systemic, the cultural, and the temporal. Those changes reflect—among other shifts in sensibility—a growing unease, even great distaste, with the pervasiveness of such power, specifically, Western power.\n\nHistorians and political theorists have long debated the correlation between capitalism, class and imperialism. Much of the debate was pioneered by such theorists as J. A. Hobson (1858–1940), Joseph Schumpeter (1883–1950), Thorstein Veblen (1857–1929), and Norman Angell (1872–1967). While these non-Marxist writers were at their most prolific before World War I, they remained active in the interwar years. Their combined work informed the study of imperialism and its impact on Europe, as well as contributing to reflections on the rise of the military-political complex in the United States from the 1950s. Hobson argued that domestic social reforms could cure the international disease of imperialism by removing its economic foundation. Hobson theorized that state intervention through taxation could boost broader consumption, create wealth, and encourage a peaceful, tolerant, multipolar world order.\n\nWalter Rodney, in his 1972 classic How Europe Underdeveloped Africa, proposes the idea that imperialism is a phase of capitalism \"in which Western European capitalist countries, the US, and Japan established political, economic, military and cultural hegemony over other parts of the world which were initially at a lower level and therefore could not resist domination.\" As a result, Imperialism \"for many years embraced the whole world – one part being the exploiters and the other the exploited, one part being dominated and the other acting as overlords, one part making policy and the other being dependent.\"\n\nThe concept of environmental determinism served as a moral justification for the domination of certain territories and peoples. The environmental determinist school of thought held that the environment in which certain people lived determined those persons' behaviours; and this validated their domination. For example, the Western world saw people living in tropical environments as \"less civilized\", therefore justifying colonial control as a civilizing mission. Across the three major waves of European colonialism (the first in the Americas, the second in Asia and the last in Africa), environmental determinism served to place categorically indigenous people in a racial hierarchy. This takes two forms, orientalism and tropicality.\n\nSome geographic scholars under colonizing empires divided the world into climatic zones. These scholars believed that Northern Europe and the Mid-Atlantic temperate climate produced a hard-working, moral, and upstanding human being. In contrast, tropical climates allegedly yielded lazy attitudes, sexual promiscuity, exotic culture, and moral degeneracy. The people of these climates were believed to be in need of guidance and intervention from a European empire to aid in the governing of a more evolved social structure; they were seen as incapable of such a feat. Similarly, orientalism could promote a view of a people based on their geographical location.\n\nBritain's imperialist ambitions can be seen as early as the sixteenth century. In 1599 the British East India Company was established and was chartered by Queen Elizabeth in the following year. With the establishment of trading posts in India, the British were able to maintain strength relative to other empires such as the Portuguese who already had set up trading posts in India. In 1767 political activity caused exploitation of the East India Company causing the plundering of the local economy, almost bringing the company into bankruptcy. By the year 1670 Britain's imperialist ambitions were well off as she had colonies in Virginia, Massachusetts, Bermuda, Honduras, Antigua, Barbados, Jamaica and Nova Scotia.\n\nDue to the vast imperialist ambitions of European countries, Britain had several clashes with France. This competition was evident in the colonization of what is now known as Canada. John Cabot claimed Newfoundland for the British while the French established colonies along the St. Lawrence River and claiming it as \"New France\". Britain continued to expand by colonizing countries such as New Zealand and Australia, both of which were not empty land as they had their own locals and cultures. Britain's nationalistic movements were evident with the creation of the commonwealth countries where there was a shared nature of national identity.\n\nThe \"First\" British Empire was based on mercantilism, and involved colonies and holdings primarily in North America, the Caribbean, and India. Its growth was reversed by the loss of the American colonies in 1776. Britain made compensating gains in India, Australia, and in constructing an informal economic empire through control of trade and finance in Latin America after the independence of Spanish and Portuguese colonies in about 1820. By the 1840s, Britain had adopted a highly successful policy of free trade that gave it dominance in the trade of much of the world. After losing its first Empire to the Americans, Britain then turned its attention towards Asia, Africa, and the Pacific. Following the defeat of Napoleonic France in 1815, Britain enjoyed a century of almost unchallenged dominance and expanded its imperial holdings around the globe. Unchallenged at sea, British dominance was later described as \"Pax Britannica\" (\"British Peace\"), a period of relative peace in Europe and the world (1815–1914) during which the British Empire became the global hegemon and adopted the role of global policeman.\n\nIn the early 19th century, the Industrial Revolution began to transform Britain; by the time of the Great Exhibition in 1851 the country was described as the \"workshop of the world\". The British Empire expanded to include India, large parts of Africa and many other territories throughout the world. Alongside the formal control it exerted over its own colonies, British dominance of much of world trade meant that it effectively controlled the economies of many regions, such as Asia and Latin America. Domestically, political attitudes favoured free trade and laissez-faire policies and a gradual widening of the voting franchise. During this century, the population increased at a dramatic rate, accompanied by rapid urbanisation, causing significant social and economic stresses. To seek new markets and sources of raw materials, the Conservative Party under Disraeli launched a period of imperialist expansion in Egypt, South Africa, and elsewhere. Canada, Australia, and New Zealand became self-governing dominions.\n\nA resurgence came in the late 19th century with the Scramble for Africa and major additions in Asia and the Middle East. The British spirit of imperialism was expressed by Joseph Chamberlain and Lord Rosebury, and implemented in Africa by Cecil Rhodes. The pseudo-sciences of Social Darwinism and theories of race formed an ideological underpinning during this time. Other influential spokesmen included Lord Cromer, Lord Curzon, General Kitchener, Lord Milner, and the writer Rudyard Kipling. The British Empire was the largest Empire that the world has ever seen both in terms of landmass and population. Its power, both military and economic, remained unmatched. After the First Boer War, the South African Republic and Orange Free State were recognized by Britain but eventually re-annexed after the Second Boer War.\n\nWorld War II had weakened Britain's position in the world, especially financially. Decolonization movements proliferated throughout the Cold War, resulting in Indian independence and the establishment of independent states throughout Africa. British imperialism continued for a few years, notably with its involvement in the Iranian coup d'état of 1953 and in Egypt during the Suez Crisis in 1956. However, with the United States and Soviet Union emerging from World War II as the sole superpowers, Britain's role as a worldwide power declined significantly.\n\nAncient China has been one of the world's oldest empires that still exist. Due to its long history of being imperialist expansion, China has been seen by its neighboring countries as a threat due to large population, giant economy, large military force as well as its territorial evolution in most of history of China.\n\nStarting with the unification of China under the Qin Dynasty, later Chinese dynasties continued to follow its form of expansions. The most successful Chinese imperial dynasties are Tang and Qing Dynasty, due to its expansions.\n\nDuring the 16th century, the French colonization of the Americas began with the creation of New France. It was followed by French East India Company's trading posts in Africa and Asia in the 17th century. France had its \"First colonial empire\" from 1534 until 1814, including New France (Canada, Acadia, Newfoundland and Louisiana), French West Indies (Saint-Domingue, Guadeloupe, Martinique), French Guiana, Senegal (Gorée), Mascarene Islands (Mauritius Island, Réunion) and French India.\n\nIts \"Second colonial empire\" began with the conquest of Algiers in 1830 and came for the most part to an end with the granting of independence to Algeria in 1962. The French imperial history was marked by numerous wars, large and small, and also by significant help to France itself from the colonials in the world wars. France took control of Algeria in 1830 but began in earnest to rebuild its worldwide empire after 1850, concentrating chiefly in North and West Africa (French North Africa, French West Africa, French Equatorial Africa), as well as South-East Asia (French Indochina), with other conquests in the South Pacific (New Caledonia, French Polynesia).\n\nFrench Republicans, at first hostile to empire, only became supportive when Germany started to build her own colonial empire. As it developed, the new empire took on roles of trade with France, supplying raw materials and purchasing manufactured items, as well as lending prestige to the motherland and spreading French civilization and language as well as Catholicism. It also provided crucial manpower in both World Wars. It became a moral justification to lift the world up to French standards by bringing Christianity and French culture. In 1884 the leading exponent of colonialism, Jules Ferry declared France had a civilising mission: \"The higher races have a right over the lower races, they have a duty to civilize the inferior\". Full citizenship rights – \"assimilation\" – were offered, although in reality assimilation was always on the distant horizon. Contrasting from Britain, France sent small numbers of settlers to its colonies, with the only notable exception of Algeria, where French settlers nevertheless always remained a small minority.\n\nIn the 19th and 20th centuries, the French colonial empire was the second-largest colonial empire in the world behind the British Empire, extending over 12,347,000 km (4,767,000 sq. miles) at its height in the 1920s and 1930s. France controlled nearly 1/10th of the Earth's land area, with a population of 110 million people on the eve of World War II (5% of the world's population at the time).\n\nIn World War II, Charles de Gaulle and the Free French used the overseas colonies as bases from which they fought to liberate France. However, after 1945 anti-colonial movements began to challenge the Empire. France fought and lost a bitter war in Vietnam in the 1950s. Whereas they won the war in Algeria, de Gaulle decided to grant Algeria independence anyway in 1962. French settlers and many local supporters relocated to France. Nearly all of France's colonies gained independence by 1960, but France retained great financial and diplomatic influence. It has repeatedly sent troops to assist its former colonies in Africa in suppressing insurrections and coups d'état.\n\nGerman participation in imperialism was negligible until the late 19th century. Prussia unified the other states into the second German Empire in 1871. Its Chancellor, Otto von Bismarck (1862–90), long opposed colonial acquisitions, arguing that the burden of obtaining, maintaining, and defending such possessions would outweigh any potential benefits. He felt that colonies did not pay for themselves, that the German bureaucratic system would not work well in the tropics and the diplomatic disputes over colonies would distract Germany from its central interest, Europe itself.\n\nHowever, public opinion and elite opinion in Germany demanded colonies for reasons of international prestige, so Bismarck was forced to oblige. In 1883–84 Germany began to build a colonial empire in Africa and the South Pacific. The establishment of the German colonial empire started with German New Guinea in 1884.\n\nGerman colonies included the present territories of in Africa: Tanzania, Rwanda, Burundi, Namibia, Cameroon, Ghana and Togo; in Oceania: New Guinea, Solomon islands, Nauru, Marshall Islands, Mariana Islands, Caroline Islands and Samoa; and in Asia: Tsingtao, Chefoo and the Jiaozhou Bay.\n\nBy the Treaty of Versailles, all German colonies were lost after World War I.\n\nFor over 200 years, Japan remained isolated from the rest of the world, staying in a feudal system. However, in the 1850s military pressure by the United States and other world powers forced Japan to open itself to the world markets, ending the period of isolation. A period of conflicts and revolutions unleashed in the uncertainty of the new period, ending in 1867 with the reunification of the political power in only one leader: the Japanese Emperor. Everything was ready for Japan to embrace the Industrial Revolution. However, from the start the Japanese didn't look too enthusiastic on relying on other countries to obtain industrial manufactures and remaining forever an underdeveloped nation, key to this understanding is the traditional Japanese psyche of self-sustainability that made them survive isolated for centuries. Thus, Japan initiated a process of modernization through central planning and a firm direction of the government. This became one of the fastest modernization processes in world history: in a matter of few decades it went through the full technological evolution that took centuries in Europe. Japan's late industrialization example became a leading case for underdeveloped countries that suffered from European rule. By the early 1900s, Japan was a naval power that could hold its own against an established European power like Russia.\n\nWithout much natural resources and territory to sustain the increasing Japanese population that industrialization brought, Japan turned to imperialism and expansionism as a way to compensate for its lackings and also to strengthen itself, the national motto \"\"Fukoku kyōhei\"\" (富国強兵, \"Enrich the state, strengthen the military\") as a sign of this attitude. Also the mentioned typical self-sustaining mentality of the Japanese was a cause of this change in foreign policy.\n\nAnd Japan was eager to take every opportunity. In 1869 they took advantage of the defeat of the rebels of the Republic of Ezo to incorporate definitely the island of Hokkaido to Japan. For centuries, Japan viewed the Ryukyu Islands as one of its provinces. In 1871 the Mudan incident happened: cannibal Taiwanese aborigines murdered 54 Ryūkyūan sailors that had their ship shipwrecked. At that time the Ryukyu Islands were claimed by both Qing China and Japan, and the Japanese interpreted the incident as an attack on their citizens. They took steps to bring the islands in their jurisdiction: in 1872 the Japanese Ryukyu Domain was declared, and in 1874 a retaliatory incursion to Taiwan was sent, which was a success. The success of this expedition emboldened the Japanese: not even the Americans could defeat the Taiwanese cannibals in the Formosa Expedition of 1867. Very few gave it much thought at the time, but this was the first move in the Japanese expansionism series. Japan occupied Taiwan for the rest of 1874 and then left owing to Chinese pressures, but in 1879 it finally annexed the Ryukyu Islands. In 1875 Qing China sent a 300-men force to subdue the Taiwanese cannibals, but unlike the Japanese the Chinese were routed, ambushed and 250 of their men were killed; the failure of this expedition exposed once more the failure of Qing China to exert effective control in Taiwan, and acted as another incentive for the Japanese to annex Taiwan. Eventually, the spoils for winning the First Sino-Japanese War in 1894 included Taiwan.\n\nIn 1875 Japan took its first operation against Joseon Korea, another territory that for centuries it coveted; the Ganghwa Island incident made Korea open to international trade. Korea was annexed in 1910. As a result of winning the Russo-Japanese War in 1905, Japan took part of Sakhalin Island from Russia. Precisely, the victory against the Russian Empire shook the world: never before an Asian nation defeated a European power, and in Japan it was seen as a feat. Japan's victory against Russia would act as an antecedent for Asian countries in the fight against the Western powers for Decolonization. During World War I, Japan took German-leased territories in China's Shandong Province, as well as the Mariana, Caroline, and Marshall Islands, and kept the islands as League of nations mandates. At first, Japan was in good standing with the victorious Allied powers of World War I, but different discrepancies and dissatisfaction with the rewards of the treaties cooled the relations with them, for example American pressure forced it to return the Shandong area. By the '30s, economic depression, urgency of resources and a growing distrust in the Allied powers made Japan lean to a hardened militaristic stance. Through the decade, it would grow closer to Germany and Italy, forming together the Axis alliance. In 1931 Japan took Manchuria from China. International reactions condemned this move, but Japan's already strong skepticism against Allied nations meant that it nevertheless carried on.\nDuring the Second Sino-Japanese War in 1937, Japan's military invaded central China. By now, relations with the Allied powers were at the bottom, and an international boycott against Japan to deprive it of natural resources was enforced. Thus a military move to gain access to them was needed, and so Japan attacked Pearl Harbor, bringing the United States to World War II. Using its superior technological advances in naval aviation and its modern doctrines of amphibious and naval warfare, Japan achieved one of the fastest maritime expansions in history, by the end of the Pacific War, Japan had conquered much of East Asia and the Pacific, including the east of China, Hong Kong, Thailand, Vietnam, Cambodia, Myanmar, Malaysia, Singapore, the Philippines, Indonesia, part of New Guinea and many islands of the Pacific Ocean. Just as Japan's late industrialization success and victory against the Russian Empire was seen as an example among underdeveloped Asia-Pacific nations, the Japanese took advantage of this and promoted among its conquered the goal to jointly create an anti-European \"Greater East Asia Co-Prosperity Sphere\". This plan helped the Japanese gain support from native populations during its conquests. However, the United States were benefited by the long-term, war of attrition and over time the massive output of their industrial muscle, together with improvements in their military doctrines, turned the war in their favor. Japan's defeat in 1945 meant that its imperial gains, along with the proposed Pan-Asian sphere, were lost altogether, but this anti-European experience was one of the leading antecedents in the Decolonization movements in East Asia and the Pacific in the second half of the 20th century.\n\nThe Ottoman Empire was an imperial state that lasted from 1299 to 1922. In 1453, Mehmed the Conqueror besieged the capital of the Byzantine Empire, resulting in the Fall of Constantinople after 1,500 years of Roman rule. Thereafter, making it the capital of the empire. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a powerful multinational, multilingual empire, which invaded and colonized much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. Its repeated invasions, and brutal treatment of Slavs led to the great migration of the Serbs to escape persecution. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the empire, while others were granted various types of autonomy during the course of centuries.\n\nWith Constantinople as its capital and control of lands around the Mediterranean basin, the Ottoman Empire was at the center of interactions between the Eastern and Western worlds for six centuries. Following a long period of military setbacks against European powers, the Ottoman Empire gradually declined into the late nineteenth century. The empire allied with Germany in the early 20th century, with the imperial ambition of recovering its lost territories, but it dissolved in the aftermath of its defeat in the First World War. The residue was the new state of Turkey in the Ottoman Anatolian heartland, as well as the creation of modern Balkan and Middle Eastern states, thus ending Turkish colonial ambitions.\n\nBy the 18th century, the Russian Empire extended its control to the Pacific, forming a common border with the Qing Empire. This took place in a large number of military invasions of the lands east, west, and south of it. The Polish–Russian War of 1792 took place after Polish nobility from the Polish–Lithuanian Commonwealth wrote the Constitution of May 3, 1791. The war resulted in eastern Poland being conquered by Imperial Russia as a colony until 1918. The southern campaigns involved a series of Russo-Persian Wars, which began with the Persian Expedition of 1796, resulting in the acquisition of Georgia (country) as a protectorate. Between 1800 and 1864, Imperial armies invaded south in the Russian conquest of the Caucasus, the Murid War, and the Russo-Circassian War. This last conflict led to the ethnic cleansing of Circassians from their lands. The Russian conquest of Siberia over the Khanate of Sibir took place in the 16th and 17th centuries, and resulted in the slaughter of various indigenous tribes by Russians, including the Daur, the Koryaks, the Itelmens, Mansi people and the Chukchi. The Russian colonization of Central and Eastern Europe and Siberia and treatment of the resident indigenous peoples has been compared to European colonization of the Americas, with similar negative impacts on the indigenous Siberians as upon the indigenous peoples of the Americas. The extermination of indigenous Siberian tribes was so complete that a relatively small population of only 180,000 are said to exist today. The Russian Empire exploited and suppressed Cossacks hosts during this period, before turning them into the special military estate Sosloviye in the late 18th century. Cossacks were then used in Imperial Russian campaigns against other tribes.\n\nBolshevik leaders had effectively reestablished a polity with roughly the same extent as that empire by 1921, however with an internationalist ideology: Lenin in particular asserted the right to limited self-determination for national minorities within the new territory. Beginning in 1923, the policy of \"Indigenization\" [korenizatsiya] was intended to support non-Russians develop their national cultures within a socialist framework. Never formally revoked, it stopped being implemented after 1932. After World War II, the Soviet Union installed socialist regimes modeled on those it had installed in 1919–20 in the old Russian Empire, in areas its forces occupied in Eastern Europe. The Soviet Union and the People's Republic of China supported post–World War II communist movements in foreign nations and colonies to advance their own interests, but were not always successful.\n\nTrotsky, and others, believed that the revolution could only succeed in Russia as part of a world revolution. Lenin wrote extensively on the matter and famously declared that Imperialism was the highest stage of capitalism. However, after Lenin's death, Joseph Stalin established 'socialism in one country' for the Soviet Union, creating the model for subsequent inward looking Stalinist states and purging the early Internationalist elements. The internationalist tendencies of the early revolution would be abandoned until they returned in the framework of a client state in competition with the Americans during the Cold War. In the after Stalin period in the late 1950s, the new political leader Nikita Khrushchev put pressure on the Soviet-American relations starting a new wave of anti-imperialist propaganda. In his speech on the UN conference in 1960, he announced the continuation of the war on imperialism, stating that soon the people of different countries will come together and overthrow their imperialist leaders. Although the Soviet Union declared itself anti-imperialist, critics argue that it exhibited traits common to historic empires. Some scholars hold that the Soviet Union was a hybrid entity containing elements common to both multinational empires and nation states. Some also argued that the USSR practiced colonialism as did other imperial powers and was carrying on the old Russian tradition of expansion and control. Mao Zedong once argued that the Soviet Union had itself become an imperialist power while maintaining a socialist façade. Moreover, the ideas of imperialism were widely spread in action on the higher levels of government. Some Marxists within the Russian Federation and later the USSR, like Sultan Galiev and Vasyl Shakhrai, considered the Soviet regime a renewed version of the Russian imperialism and colonialism.\n\nA former colony itself, the early United States expressed its opposition to Imperialism, at least in a form distinct from its own Manifest Destiny, through policies such as the Monroe Doctrine. However, beginning in the late 19th and early 20th century, policies such as Theodore Roosevelt’s interventionism in Central America and Woodrow Wilson’s mission to \"make the world safe for democracy\" changed all this. They were often backed by military force, but were more often effected from behind the scenes. This is consistent with the general notion of hegemony and imperium of historical empires. In 1898, Americans who opposed imperialism created the Anti-Imperialist League to oppose the US annexation of the Philippines and Cuba. One year later, a war erupted in the Philippines causing business, labor and government leaders in the US to condemn America's occupation in the Philippines as they also denounced them for causing the deaths of many Filipinos. American foreign policy was denounced as a \"racket\" by Smedley Butler, a former American general who had become a spokesman for the far left.\n\nAt the start of World War II, President Franklin D. Roosevelt was opposed to European colonialism, especially in India. He pulled back when Britain's Winston Churchill demanded that victory in the war be the first priority. Roosevelt expected that the United Nations would take up the problem of decolonization.\n\nSome have described the internal strife between various people groups as a form of imperialism or colonialism. This internal form is distinct from informal U.S. imperialism in the form of political and financial hegemony. This internal form of imperialism is also distinct from the United States' formation of \"colonies\" abroad. Through the treatment of its indigenous peoples during westward expansion, the United States took on the form of an imperial power prior to any attempts at external imperialism. This internal form of empire has been referred to as \"internal colonialism\". Participation in the African slave trade and the subsequent treatment of its 12 to 15 million Africans is viewed by some to be a more modern extension of America's \"internal colonialism\". However, this internal colonialism faced resistance, as external colonialism did, but the anti-colonial presence was far less prominent due to the nearly complete dominance that the United States was able to assert over both indigenous peoples and African-Americans. In his lecture on April 16, 2003, Edward Said made a bold statement on modern imperialism in the United States, whom he described as using aggressive means of attack towards the contemporary Orient, \"due to their backward living, lack of democracy and the violation of women’s rights. The western world forgets during this process of converting the other that enlightenment and democracy are concepts that not all will agree upon\".\n\nSpanish imperialism in the colonial era corresponds with the rise and decline of the Spanish Empire, conventionally recognized as emerging in 1402 with the conquest of the Canary Islands and fully dissolving by 1975 with the loss of Spanish Sahara. Following the successes of exploratory maritime voyages conducted during the Age of Discovery, such as those undertaken by Christopher Columbus, Spain committed considerable financial and military resources towards developing a robust navy capable of conducting large-scale, transatlantic expeditionary operations in order to establish and solidify a firm imperial presence across portions of North America, South America, and the geographic regions comprising the Caribbean basin. Concomitant with Spanish endorsement and sponsorship of transatlantic expeditionary voyages was the deployment of \"Conquistadors\", which further expanded Spanish imperial boundaries through the acquisition and development of territories and colonies.\n\nIn congruence with the colonialist activities of competing European imperial powers throughout the 15th – 19th centuries, the Spanish were equally engrossed in extending geopolitical power. The Caribbean basin functioned as a key geographic focal point for advancing Spanish imperialism. Similar to the strategic prioritization Spain placed towards achieving victory in the conquests of the Aztec Empire and Inca Empire, Spain placed equal strategic emphasis on expanding the nation's imperial footprint within the Caribbean basin.\n\nEchoing the prevailing ideological perspectives regarding colonialism and imperialism embraced by Spain's European rivals during the colonial era, including the English, French, and the Dutch, the Spanish utilized colonialism as a means of expanding imperial geopolitical borders and securing the defense of maritime trade routes in the Caribbean basin.\n\nWhile leveraging colonialism in the same geographic operating theater as its imperial rivals, Spain maintained distinct imperial objectives and instituted a unique form of colonialism in support of its imperial agenda. Spain placed significant strategic emphasis on the acquisition, extraction, and exportation of precious metals (primarily gold and silver). A second objective was the evangelization of subjugated indigenous populations residing in mineral-rich and strategically favorable locations. Notable examples of these indigenous groups include the Taίno populations inhabiting Puerto Rico and segments of Cuba. Compulsory labor and slavery were widely institutionalized across Spanish-occupied territories and colonies, with an initial emphasis on directing labor towards mining activity and related methods of procuring semi-precious metals. The emergence of the \"Encomienda\" system during the 16th–17th centuries in occupied colonies within the Caribbean basin reflects a gradual shift in imperial prioritization, increasingly focusing on large-scale production and exportation of agricultural commodities.\n\nThe scope and scale of Spanish participation in imperialism within the Caribbean basin remains a subject of scholarly debate among historians. A fundamental source of contention stems from the inadvertent conflation of theoretical conceptions of imperialism and colonialism. Furthermore, significant variation exists in the definition and interpretation of these terms as expounded by historians, anthropologists, philosophers, and political scientists.\n\nAmong historians, there is substantial support in favor of approaching imperialism as a conceptual theory emerging during the 18th – 19th centuries, particularly within Britain, propagated by key exponents such as Joseph Chamberlain and Benjamin Disraeli. In accordance with this theoretical perspective, the activities of the Spanish in the Caribbean are not components of a preeminent, ideologically-driven form of imperialism. Rather, these activities are more accurately classified as representing a form of colonialism.\n\nFurther divergence among historians can be attributed to varying theoretical perspectives regarding imperialism that are proposed by emerging academic schools of thought. Noteworthy examples include cultural imperialism, whereby proponents such as John Downing and Annabelle Sreberny-Modammadi define imperialism as \"\"...the conquest and control of one country by a more powerful one.\"\" Cultural imperialism signifies the dimensions of the process that go beyond economic exploitation or military force.\" Moreover, colonialism is understood as \"\"...the form of imperialism in which the government of the colony is run directly by foreigners.\"\"\n\nIn spite of diverging perspectives and the absence of a unilateral scholarly consensus regarding imperialism among historians, within the context of Spanish expansion in the Caribbean basin during the colonial era, imperialism can be interpreted as an overarching ideological agenda that is perpetuated through the institution of colonialism. In this context, colonialism functions as an instrument designed to achieve specific imperialist objectives.\n\n\n\n"}
{"id": "15317", "url": "https://en.wikipedia.org/wiki?curid=15317", "title": "IPv4", "text": "IPv4\n\nInternet Protocol version 4 (IPv4) is the fourth version of the Internet Protocol (IP). It is one of the core protocols of standards-based internetworking methods in the Internet, and was the first version deployed for production in the ARPANET in 1983. It still routes most Internet traffic today, despite the ongoing deployment of a successor protocol, IPv6. IPv4 is described in IETF publication RFC 791 (September 1981), replacing an earlier definition (RFC 760, January 1980).\n\nIPv4 is a connectionless protocol for use on packet-switched networks. It operates on a best effort delivery model, in that it does not guarantee delivery, nor does it assure proper sequencing or avoidance of duplicate delivery. These aspects, including data integrity, are addressed by an upper layer transport protocol, such as the Transmission Control Protocol (TCP).\n\nIPv4 uses 32-bit addresses which limits the address space to (2) addresses. \n\nIPv4 reserves special address blocks for private networks (~18 million addresses) and multicast addresses (~270 million addresses).\n\nIPv4 addresses may be represented in any notation expressing a 32-bit integer value. They are most often written in the dot-decimal notation, which consists of four octets of the address expressed individually in decimal numbers and separated by periods. \n\nFor example, the quad-dotted IP address 192.0.2.235 represents the 32-bit decimal number 3221226219, which in hexadecimal format is 0xC00002EB. This may also be expressed in dotted hex format as 0xC0.0x00.0x02.0xEB, or with octal byte values as 0300.0000.0002.0353.\n\nCIDR notation combines the address with its routing prefix in a compact format, in which the address is followed by a slash character (/) and the count of consecutive \"1\" bits in the routing prefix (subnet mask).\n\nIn the original design of IPv4, an IP address was divided into two parts: the network identifier was the most significant octet of the address, and the host identifier was the rest of the address. The latter was also called the \"rest field\". This structure permitted a maximum of 256 network identifiers, which was quickly found to be inadequate.\n\nTo overcome this limit, the most-significant address octet was redefined in 1981 to create \"network classes\", in a system which later became known as classful networking. The revised system defined five classes. Classes A, B, and C had different bit lengths for network identification. The rest of the address was used as previously to identify a host within a network. Because of the different sizes of fields in different classes, each network class had a different capacity for addressing hosts. In addition to the three classes for addressing hosts, Class D was defined for multicast addressing and Class E was reserved for future applications.\n\nDividing existing classful networks into subnets began in 1985 with the publication of . This division was made more flexible with the introduction of variable-length subnet masks (VLSM) in in 1987. In 1993, based on this work, introduced Classless Inter-Domain Routing (CIDR), which expressed the number of bits (from the most significant) as, for instance, /24, and the class-based scheme was dubbed \"classful\", by contrast. CIDR was designed to permit repartitioning of any address space so that smaller or larger blocks of addresses could be allocated to users. The hierarchical structure created by CIDR is managed by the Internet Assigned Numbers Authority (IANA) and the regional Internet registries (RIRs). Each RIR maintains a publicly searchable WHOIS database that provides information about IP address assignments.\n\nThe Internet Engineering Task Force (IETF) and the Internet Assigned Numbers Authority (IANA) have restricted from general use various reserved IP addresses for special purposes. Some are used for maintenance of routing tables, for multicast traffic, operation under failure modes, or to provide addressing space for public, unrestricted uses on private networks.\n<section begin=IPv4-special-address-blocks/>\nOf the approximately four billion addresses defined in IPv4, three ranges are reserved for use in private networks. Packets addresses in these ranges are not routable in the public Internet, because they are ignored by all public routers. Therefore, private hosts cannot directly communicate with public networks, but require network address translation at a routing gateway for this purpose.\n<section begin=IPv4-private-networks/>\n\nSince two private networks, e.g., two branch offices, cannot directly interoperate via the public Internet, the two networks must be bridged across the Internet via a virtual private network (VPN) or an IP tunnel, which encapsulate the packet in a protocol layer during transmission across the public network. Additionally, encapsulated packets may be encrypted for the transmission across public networks to secure the data.\n\nRFC 3927 defines the special address block 169.254.0.0/16 for link-local addressing. These addresses are only valid on links (such as a local network segment or point-to-point connection) connected to a host. These addresses are not routable. Like private addresses, these addresses cannot be the source or destination of packets traversing the internet. These addresses are primarily used for address autoconfiguration (Zeroconf) when a host cannot obtain an IP address from a DHCP server or other internal configuration methods.\n\nWhen the address block was reserved, no standards existed for address autoconfiguration. Microsoft created an implementation called Automatic Private IP Addressing (APIPA), which was deployed on millions of machines and became a de facto standard. Many years later, in May 2005, the IETF defined a formal standard in RFC 3927, entitled \"Dynamic Configuration of IPv4 Link-Local Addresses\".\n\nThe class A network 127.0.0.0 (classless network 127.0.0.0/8) is reserved for loopback. IP packets whose source addresses belong to this network should never appear outside a host. The modus operandi of this network expands upon that of a loopback interface:\n\nNetworks with subnet masks of at least 24 bits, i.e. Class C networks in classful networking, and networks with CIDR suffixes /24 to /30 (255.255.255.0–255.255.255.252) may not have an address ending in 0 or 255.\n\nClassful addressing prescribed only three possible subnet masks: Class A, 255.0.0.0 or /8; Class B, 255.255.0.0 or /16; and Class C, 255.255.255.0 or /24. For example, in the subnet 192.168.5.0/255.255.255.0 (192.168.5.0/24) the identifier 192.168.5.0 commonly is used to refer to the entire subnet. To avoid ambiguity in representation, the address ending in the octet \"0\" is reserved.\n\nA broadcast address is an address that allows information to be sent to all interfaces in a given subnet, rather than a specific machine. Generally, the broadcast address is found by obtaining the bit complement of the subnet mask and performing a bitwise OR operation with the network identifier. In other words, the broadcast address is the last address in the address range of the subnet. For example, the broadcast address for the network 192.168.5.0 is 192.168.5.255. For networks of size /24 or larger, the broadcast address always ends in 255.\n\nHowever, this does not mean that every address ending in 0 or 255 cannot be used as a host address. For example, in the /16 subnet 192.168.0.0/255.255.0.0, which is equivalent to the address range 192.168.0.0–192.168.255.255, the broadcast address is 192.168.255.255. One can use the following addresses for hosts, even though they end with 255: 192.168.1.255, 192.168.2.255, etc. Also, 192.168.0.0 is the network identifier and must not be assigned to an interface. The addresses 192.168.1.0, 192.168.2.0, etc., may be assigned, despite ending with 0.\n\nIn the past, conflict between network addresses and broadcast addresses arose because some software used non-standard broadcast addresses with zeros instead of ones.\n\nIn networks smaller than /24, broadcast addresses do not necessarily end with 255. For example, a CIDR subnet 203.0.113.16/28 has the broadcast address 203.0.113.31.\n\nHosts on the Internet are usually known by names, e.g., www.example.com, not primarily by their IP address, which is used for routing and network interface identification. The use of domain names requires translating, called \"resolving\", them to addresses and vice versa. This is analogous to looking up a phone number in a phone book using the recipient's name.\n\nThe translation between addresses and domain names is performed by the Domain Name System (DNS), a hierarchical, distributed naming system which allows for subdelegation of name spaces to other DNS servers.\n\nSince the 1980s, it was apparent that the pool of available IPv4 addresses was being depleted at a rate that was not initially anticipated in the original design of the network address system. The main market forces which accelerated IPv4 address depletion included:\n\nThe threat of exhaustion motivated the introduction of a number of remedial technologies, such as classful networks, Classless Inter-Domain Routing (CIDR) methods, network address translation (NAT) and strict usage-based allocation policies. To provide a long-term solution to the pending address exhaustion, IPv6 was created in the 1990s, which made many more addresses available by increasing the address size to 128 bits. IPv6 has been in commercial deployment since 2006. \n\nThe primary address pool of the Internet, maintained by IANA, was exhausted on 3 February 2011, when the last 5 blocks were allocated to the 5 RIRs. APNIC was the first RIR to exhaust its regional pool on 15 April 2011, except for a small amount of address space reserved for the transition to IPv6, which will be allocated under a much more restricted policy.\n\nThe accepted and standard long term solution is to use IPv6 which increased the address size to 128 bits, providing a vastly increased address space that also allows improved route aggregation across the Internet and offers large subnetwork allocations of a minimum of 2 host addresses to end-users. However IPv4-only hosts cannot directly communicate with IPv6-only hosts so IPv6 alone does not provide an immediate solution to the IPv4 exhaustion problem. Migration to IPv6 is in progress but completion is expected to take considerable time. \n\nAn IP packet consists of a header section and a data section.\n\nAn IP packet has no data checksum or any other footer after the data section.\nTypically the link layer encapsulates IP packets in frames with a CRC footer that detects most errors,\nand typically the end-to-end TCP layer checksum detects most other errors.\n\nThe IPv4 packet header consists of 14 fields, of which 13 are required. The 14th field is optional and aptly named: options. The fields in the header are packed with the most significant byte first (big endian), and for the diagram and discussion, the most significant bits are considered to come first (MSB 0 bit numbering). The most significant bit is numbered 0, so the version field is actually found in the four most significant bits of the first byte, for example.\n\nThe packet payload is not included in the checksum. Its contents are interpreted based on the value of the Protocol header field.\n\nSome of the common payload protocols are:\n\nSee List of IP protocol numbers for a complete list.\n\nThe Internet Protocol enables traffic between networks. The design accommodates networks of diverse physical nature; it is independent of the underlying transmission technology used in the Link Layer. Networks with different hardware usually vary not only in transmission speed, but also in the maximum transmission unit (MTU). When one network wants to transmit datagrams to a network with a smaller MTU, it may fragment its datagrams. In IPv4, this function was placed at the Internet Layer, and is performed in IPv4 routers, which thus require no implementation of any higher layers for the function of routing IP packets.\n\nIn contrast, IPv6, the next generation of the Internet Protocol, does not allow routers to perform fragmentation; hosts must determine the path MTU before sending datagrams.\n\nWhen a router receives a packet, it examines the destination address and determines the outgoing interface to use and that interface's MTU. If the packet size is bigger than the MTU, and the Do not Fragment (DF) bit in the packet's header is set to 0, then the router may fragment the packet.\n\nThe router divides the packet into fragments. The max size of each fragment is the MTU minus the IP header size (20 bytes minimum; 60 bytes maximum). The router puts each fragment into its own packet, each fragment packet having following changes:\n\nFor example, for an MTU of 1,500 bytes and a header size of 20 bytes, the fragment offsets would be multiples of\nformula_1.\nThese multiples are 0, 185, 370, 555, 740, ...\n\nIt is possible that a packet is fragmented at one router, and that the fragments are further fragmented at another router. For example, a packet of 4,520 bytes, including the 20 bytes of the IP header (without options) is fragmented to two packets on a link with an MTU of 2,500 bytes:\n\nThe total data size is preserved: 2480 bytes + 2020 bytes = 4500 bytes.\nThe offsets are\nformula_2\nand\nformula_3.\n\nOn a link with an MTU of 1,500 bytes, each fragment results in two fragments:\n\nAgain, the data size is preserved: 1480 + 1000 = 2480, and 1480 + 540 = 2020.\n\nAlso in this case, the \"More Fragments\" bit remains 1 for all the fragments that came with 1 in them and for the last fragment that arrives, it works as usual, that is the MF bit is set to 0 only in the last one. And of course, the Identification field continues to have the same value in all re-fragmented fragments. This way, even if fragments are re-fragmented, the receiver knows they have initially all started from the same packet.\n\nThe last offset and last data size are used to calculate the total data size:\nformula_4.\n\nA receiver knows that a packet is a fragment if at least one of the following conditions is true:\n\n\nThe receiver identifies matching fragments using the foreign and local address, the protocol ID, and the identification field. The receiver reassembles the data from fragments with the same ID using both the fragment offset and the more fragments flag. When the receiver receives the last fragment (which has the \"more fragments\" flag set to 0), it can calculate the length of the original data payload, by multiplying the last fragment's offset by eight, and adding the last fragment's data size. In the example above, this calculation was 495*8 + 540 = 4500 bytes.\n\nWhen the receiver has all fragments, they can be correctly ordered by using the offsets, and reassembled to yield the original data segment.\n\nThe Internet Protocol is the protocol that defines and enables internetworking at the Internet Layer and thus forms the Internet. It uses a logical addressing system. IP addresses are not tied in any permanent manner to hardware identifications and, indeed, a network interface can have multiple IP addresses. Hosts and routers need additional mechanisms to identify the relationship between device interfaces and IP addresses, in order to properly deliver an IP packet to the destination host on a link. The Address Resolution Protocol (ARP) performs this IP-address-to-hardware-address translation for IPv4. (A hardware address is also called a MAC address.) In addition, the reverse correlation is often necessary. For example, when an IP host is booted or connected to a network it needs to determine its IP address, unless an address is preconfigured by an administrator. Protocols for such inverse correlations exist in the Internet Protocol Suite. Currently used methods are Dynamic Host Configuration Protocol (DHCP), Bootstrap Protocol (BOOTP) and, infrequently, reverse ARP.\n\n\n"}
{"id": "15318", "url": "https://en.wikipedia.org/wiki?curid=15318", "title": "IPv6", "text": "IPv6\n\nInternet Protocol version 6 (IPv6) is the most recent version of the Internet Protocol (IP), the communications protocol that provides an identification and location system for computers on networks and routes traffic across the Internet. IPv6 was developed by the Internet Engineering Task Force (IETF) to deal with the long-anticipated problem of IPv4 address exhaustion. IPv6 is intended to replace IPv4. IPv6 became a Draft Standard in December 1998, and became an Internet Standard on 14 July 2017.\n\nDevices on the Internet are assigned a unique IP address for identification and location definition. With the rapid growth of the Internet after commercialization in the 1990s, it became evident that far more addresses would be needed to connect devices than the IPv4 address space had available. By 1998, the Internet Engineering Task Force (IETF) had formalized the successor protocol. IPv6 uses a 128-bit address, theoretically allowing 2, or approximately addresses. The actual number is slightly smaller, as multiple ranges are reserved for special use or completely excluded from use. The total number of possible IPv6 addresses is more than times as many as IPv4, which uses 32-bit addresses and provides approximately 4.3 billion addresses. The two protocols are not designed to be interoperable, complicating the transition to IPv6. However, several IPv6 transition mechanisms have been devised to permit communication between IPv4 and IPv6 hosts.\n\nIPv6 provides other technical benefits in addition to a larger addressing space. In particular, it permits hierarchical address allocation methods that facilitate route aggregation across the Internet, and thus limit the expansion of routing tables. The use of multicast addressing is expanded and simplified, and provides additional optimization for the delivery of services. Device mobility, security, and configuration aspects have been considered in the design of the protocol.\n\nIPv6 addresses are represented as eight groups of four hexadecimal digits with the groups being separated by colons, for example 2001:0db8:0000:0042:0000:8a2e:0370:7334, but methods to abbreviate this full notation exist.\n\nIPv6 is an Internet Layer protocol for packet-switched internetworking and provides end-to-end datagram transmission across multiple IP networks, closely adhering to the design principles developed in the previous version of the protocol, Internet Protocol Version 4 (IPv4). \n\nIn addition to offering more addresses, IPv6 also implements features not present in IPv4. It simplifies aspects of address configuration, network renumbering, and router announcements when changing network connectivity providers. It simplifies processing of packets in routers by placing the responsibility for packet fragmentation into the end points. The IPv6 subnet size is standardized by fixing the size of the host identifier portion of an address to 64 bits. Network security was a design requirement of the IPv6 architecture, and included the original specification of IPsec.\n\nThe addressing architecture of IPv6 is defined in and allows three different types of transmission: unicast, anycast and multicast.\n\nInternet Protocol Version 4 (IPv4) was the first publicly used version of the Internet Protocol. IPv4 was developed as a research project by the Defense Advanced Research Projects Agency (DARPA), a United States Department of Defense agency, before becoming the foundation for the Internet and the World Wide Web. IPv4 includes an addressing system that uses numerical identifiers consisting of 32 bits. These addresses are typically displayed in quad-dotted notation as decimal values of four octets, each in the range 0 to 255, or 8 bits per number. Thus, IPv4 provides an addressing capability of 2 or approximately 4.3 billion addresses. Address exhaustion was not initially a concern in IPv4 as this version was originally presumed to be a test of DARPA's networking concepts. During the first decade of operation of the Internet, it became apparent that methods had to be developed to conserve address space. In the early 1990s, even after the redesign of the addressing system using a classless network model, it became clear that this would not suffice to prevent IPv4 address exhaustion, and that further changes to the Internet infrastructure were needed.\n\nThe last unassigned top-level address blocks of 16 million IPv4 addresses were allocated in February 2011 by the Internet Assigned Numbers Authority (IANA) to the five regional Internet registries (RIRs). However, each RIR still has available address pools and is expected to continue with standard address allocation policies until one /8 Classless Inter-Domain Routing (CIDR) block remains. After that, only blocks of 1024 addresses (/22) will be provided from the RIRs to a local Internet registry (LIR). As of September 2015, all of Asia-Pacific Network Information Centre (APNIC), the Réseaux IP Européens Network Coordination Centre (RIPE_NCC), Latin America and Caribbean Network Information Centre (LACNIC), and American Registry for Internet Numbers (ARIN) have reached this stage. This leaves African Network Information Center (AFRINIC) as the sole regional internet registry that is still using the normal protocol for distributing IPv4 addresses. As of November 2018, AFRINIC's minimum allocation is /22 or 1024 IPv4 addresses. A LIR may receive additional allocation when about 80% of all the address space has been utilized. \n\nIt is widely expected that the Internet will use IPv4 alongside IPv6 for the foreseeable future. Direct communication between the IPv4 and IPv6 network protocols is not possible.\n\nOn the Internet, data is transmitted in the form of network packets. IPv6 specifies a new packet format, designed to minimize packet header processing by routers. Because the headers of IPv4 packets and IPv6 packets are significantly different, the two protocols are not interoperable. However, most transport and application-layer protocols need little or no change to operate over IPv6; exceptions are application protocols that embed Internet-layer addresses, such as File Transfer Protocol (FTP) and Network Time Protocol (NTP), where the new address format may cause conflicts with existing protocol syntax.\n\nThe main advantage of IPv6 over IPv4 is its larger address space. The length of an IPv6 address is 128 bits, compared with 32 bits in IPv4. The address space therefore has 2 or approximately addresses (340,282,366,920,938,463,463,374,607,431,768,211,456, which is approximately 340 undecillion, or 340 billion billion billion billion, addresses).\n\nIn addition, the IPv4 address space is poorly allocated; in 2011, approximately 14% of all available addresses were utilized. While these numbers are large, it was not the intent of the designers of the IPv6 address space to assure geographical saturation with usable addresses. Rather, the longer addresses simplify allocation of addresses, enable efficient route aggregation, and allow implementation of special addressing features. In IPv4, complex Classless Inter-Domain Routing (CIDR) methods were developed to make the best use of the small address space. The standard size of a subnet in IPv6 is 2 addresses, the square of the size of the entire IPv4 address space. Thus, actual address space utilization rates will be small in IPv6, but network management and routing efficiency are improved by the large subnet space and hierarchical route aggregation.\n\nMulticasting, the transmission of a packet to multiple destinations in a single send operation, is part of the base specification in IPv6. In IPv4 this is an optional (although commonly) implemented feature. IPv6 multicast addressing has features and protocols in common with IPv4 multicast, but also provides changes and improvements by eliminating the need for certain protocols. IPv6 does not implement traditional IP broadcast, i.e. the transmission of a packet to all hosts on the attached link using a special \"broadcast address\", and therefore does not define broadcast addresses. In IPv6, the same result is achieved by sending a packet to the link-local \"all nodes\" multicast group at address ff02::1, which is analogous to IPv4 multicasting to address 224.0.0.1. IPv6 also provides for new multicast implementations, including embedding rendezvous point addresses in an IPv6 multicast group address, which simplifies the deployment of inter-domain solutions.\n\nIn IPv4 it is very difficult for an organization to get even one globally routable multicast group assignment, and the implementation of inter-domain solutions is arcane. Unicast address assignments by a local Internet registry for IPv6 have at least a 64-bit routing prefix, yielding the smallest subnet size available in IPv6 (also 64 bits). With such an assignment it is possible to embed the unicast address prefix into the IPv6 multicast address format, while still providing a 32-bit block, the least significant bits of the address, or approximately 4.2 billion multicast group identifiers. Thus each user of an IPv6 subnet automatically has available a set of globally routable source-specific multicast groups for multicast applications.\n\nIPv6 hosts can configure themselves automatically when connected to an IPv6 network using the Neighbor Discovery Protocol via Internet Control Message Protocol version 6 (ICMPv6) router discovery messages. When first connected to a network, a host sends a link-local router solicitation multicast request for its configuration parameters; routers respond to such a request with a router advertisement packet that contains Internet Layer configuration parameters. Routers present a special case of requirements for address configuration, as they often are sources of autoconfiguration information, such as router and prefix advertisements. Stateless configuration of routers can be achieved with a special router renumbering protocol.\n\nRenumbering an existing network for a new connectivity provider with different routing prefixes is a major effort with IPv4. With IPv6, however, changing the prefix announced by a few routers can in principle renumber an entire network, since the host identifiers (the least-significant 64 bits of an address) can be independently self-configured by a host.\n\nIf IPv6 stateless address auto-configuration is unsuitable, IPv6 just like IPv4 allows for stateful configuration with the Dynamic Host Configuration Protocol version 6 (DHCPv6) or manual static configuration of hosts.\n\nLike IPv4, IPv6 supports globally unique IP addresses. The design of IPv6 intended to re-emphasize the end-to-end principle of network design that was originally conceived during the establishment of the early Internet. In this approach each device on the network has a unique address globally reachable directly from any other location on the Internet.\n\nA unique IP address can potentially be used to track the network activity of a device. Moreover, when using IPv6 address auto-configuration, the Interface Identifier (MAC address) of a network card is used to make its public IPv6 interface identifier unique, exposing the type of hardware used and providing a unique handle for a user's online activity. Autoconfiguration on the basis of the network card MAC address is therefore a particular privacy concern for mobile devices, such as laptops, because when they access the Internet from different local area networks, their MAC based interface identifier would always stay the same. Thus the MAC address based interface identifier can be used to track the movement and usage of a particular mobile device.\n\nWhen IPv6 was developed in the mid-90s, the Internet was not accessed by a large number of mobile devices and privacy was not the priority it has become today. To address these privacy concerns, the SLAAC protocol was updated with mechanisms that were termed “Privacy Extensions for Stateless Address Autoconfiguration in IPv6”, codified in RFC 4941. This allows for the IPv6 address interface identifier to be generated randomly. If the same interface identifier is generated for two devices in the same local area network, the Duplicate Address Detection (DAD) function of the IPv6 Neighbor Discovery Protocol (NDP) will resolve the situation. The SLAAC privacy extension also implements a time out, which is configurable, so that the IPv6 interface addresses will be discarded and a new interface identifier is generated. Typically the time out is configured to 24 hours. So IPv6 autoconfiguration will generate and set a new IPv6 host address every day. As of late 2014 the SLAAC privacy extensions functionality was implemented by the following operating systems: all Microsoft Windows after Windows XP, all versions of Mac OS X from 10.7 onward, all versions of iOS since 4.3, all versions of Android since 4.0 (Ice Cream Sandwich). The privacy extension is now enabled by default in Windows (since XP SP1), OS X (since 10.7), and iOS (since version 4.3). Some Linux distributions have enabled privacy extensions as well.\n\nInternet Protocol Security (IPsec) was originally developed for IPv6, but found widespread deployment first in IPv4, for which it was re-engineered. IPsec was a mandatory part of all IPv6 protocol implementations, and Internet Key Exchange (IKE) was recommended, but with RFC 6434 the inclusion of IPsec in IPv6 implementations was downgraded to a recommendation because it was considered impractical to require full IPsec implementation for all types of devices that may use IPv6. However, as of RFC 4301 IPv6 protocol implementations that do implement IPsec need to implement IKEv2 and need to support a minimum set of cryptographic algorithms. This requirement will help to make IPsec implementations more interoperable between devices from different vendors. The IPsec Authentication Header (AH) and the Encapsulating Security Payload header (ESP) are implemented as IPv6 extension headers.\n\nThe packet header in IPv6 is simpler than the IPv4 header. Many rarely used fields have been moved to optional header extensions. With the simplified IPv6 packet header the process of packet forwarding by routers has been simplified. Although IPv6 packet headers are at least twice the size of IPv4 packet headers, packet processing by routers is generally more efficient, because less processing is required in routers due to the headers being aligned to match common word sizes.\n\nMoreover, an IPv6 header does not include a checksum. The IPv4 header checksum is calculated for the IPv4 header, and has to be recalculated by routers every time the time to live (called hop limit in the IPv6 protocol) is reduced by one. The absence of a checksum in the IPv6 header furthers the end-to-end principle of Internet design, which envisioned that most processing in the network occurs in the leaf nodes. Integrity protection for the data that is encapsulated in the IPv6 packet is assumed to be assured by both the link layer or error detection in higher-layer protocols, namely the Transmission Control Protocol (TCP) and the User Datagram Protocol (UDP) on the transport layer. Thus, while IPv4 allowed UDP datagram headers to have no checksum (indicated by 0 in the header field), IPv6 requires a checksum in UDP headers.\n\nIPv6 routers do not perform IP fragmentation. IPv6 hosts are required to either perform path MTU discovery, perform end-to-end fragmentation, or to send packets no larger than the default Maximum transmission unit (MTU), which is 1280 octets.\n\nUnlike mobile IPv4, mobile IPv6 avoids triangular routing and is therefore as efficient as native IPv6. IPv6 routers may also allow entire subnets to move to a new router connection point without renumbering.\n\nThe IPv6 packet header has a minimum size of 40 octets (320 bits). Options are implemented as extensions. This provides the opportunity to extend the protocol in the future without affecting the core packet structure. However, a study in 2015 indicated that some network operators dropped IPv6 packets with extension headers when they traversed transit autonomous systems.\n\nIPv4 limits packets to 65,535 (2−1) octets of payload. An IPv6 node can optionally handle packets over this limit, referred to as jumbograms, which can be as large as 4,294,967,295 (2−1) octets. The use of jumbograms may improve performance over high-MTU links. The use of jumbograms is indicated by the Jumbo Payload Option extension header.\n\nAn IPv6 packet has two parts: a header and payload.\n\nThe header consists of a fixed portion with minimal functionality required for all packets and may be followed by optional extensions to implement special features.\n\nThe fixed header occupies the first 40 octets (320 bits) of the IPv6 packet. It contains the source and destination addresses, traffic classification options, a hop counter, and the type of the optional extension or payload which follows the header. This \"Next Header\" field tells the receiver how to interpret the data which follows the header. If the packet contains options, this field contains the option type of the next option. The \"Next Header\" field of the last option, points to the upper-layer protocol that is carried in the packet's payload.\n\nExtension headers carry options that are used for special treatment of a packet in the network, e.g., for routing, fragmentation, and for security using the IPsec framework.\n\nWithout special options, a payload must be less than . With a Jumbo Payload option (in a \"Hop-By-Hop Options\" extension header), the payload must be less than 4 GB.\n\nUnlike with IPv4, routers never fragment a packet. Hosts are expected to use Path MTU Discovery to make their packets small enough to reach the destination without needing to be fragmented. See IPv6 packet fragmentation.\n\nIPv6 addresses have 128 bits. The design of the IPv6 address space implements a different design philosophy than in IPv4, in which subnetting was used to improve the efficiency of utilization of the small address space. In IPv6, the address space is deemed large enough for the foreseeable future, and a local area subnet always uses 64 bits for the host portion of the address, designated as the interface identifier, while the most-significant 64 bits are used as the routing prefix.\n\nThe identifier is only unique within the subnet to which a host is connected. IPv6 has a mechanism for automatic address detection, so that address autoconfiguration always produces unique assignments.\n\nThe 128 bits of an IPv6 address are represented in 8 groups of 16 bits each. Each group is written as four hexadecimal digits (sometimes called hextets or more formally a hexadectets and informally a quibble or quad-nibble ) and the groups are separated by colons (:). An example of this representation is .\n\nFor convenience, an IPv6 address may be abbreviated to shorter notations by application of the following rules.\n\nAn example of application of these rules:\n\nThe loopback address is defined in and may be abbreviated to by using both rules.\n\nAs an IPv6 address may have more than one representation, the IETF has issued a proposed standard for representing them in text.\n\nAll interfaces of IPv6 hosts require a link-local address. A IPv6 link-local address is derived from the MAC address of the interface and the prefix . The process involves filling the address space with prefix bits left-justified to the most-significant bit, and filling the MAC address in EUI-64 format into the least-significant bits. If any bits remain to be filled between the two parts, those are set to zero.\n\nBecause IPv6 does not implement broadcast, on which the functionality of the Address Resolution Protocol (ARP) is based, the Neighbor Discovery Protocol (NDP, ND) is used at the link layer to map Layer 3 IPv6 addresses to Layer 2 addresses, such as the MAC address of Ethernet network cards. The NFP relies on ICMPv6 and multicast transmission. IPv6 hosts verify the uniqueness of their IPv6 addresses in a local area network (LAN) by sending a neighbor solicitation message asking for the link layer address of the IPv6 address. If any other host in the LAN is using that address, it responds. In a LAN, MAC addresses are designed to be unique on each network card, which minimizes chances of duplication.\n\nAfter having generated a link-local address, the IPv6 host determines if the LAN is connected to any router network card with IPv6 implementation by sending out a ICMPv6 router solicitation message to the all-routers multicast group with its link-local address as source. If there is no answer after a predetermined number of attempts, the host concludes that no routers are connected. If it does get a response from a router, there will be network information inside that is needed to create a globally unique address. There are also two flag bits that tell the host whether it should use DHCP to get further information and addresses:\n\nThe assignment procedure for global addresses is similar to local address construction. The prefix is supplied from router advertisements on the network. Multiple prefix announcements cause multiple addresses to be configured.\n\nStateless address autoconfiguration (SLAAC) requires a address block, as defined in . Local Internet registries are assigned at least blocks, which they divide among subordinate networks. The initial recommendation stated assignment of a subnet to end-consumer sites (). This was replaced by , which \"recommends giving home sites significantly more than a single , but does not recommend that every home site be given a either\". s are specifically considered. It remains to be seen if ISPs will honor this recommendation. For example, during initial trials, Comcast customers were given a single network.\n\nIn the Domain Name System (DNS), hostnames are mapped to IPv6 addresses by AAAA resource records, so-called \"quad-A\" records. For reverse resolution, the IETF reserved the domain ip6.arpa, where the name space is hierarchically divided by the 1-digit hexadecimal representation of nibble units (4 bits) of the IPv6 address. This scheme is defined in .\nAt the design stage of the IPv6 DNS architecture, the AAAA scheme faced a rival proposal. This alternate approach, designed to facilitate network renumbering, uses \"A6\" records for the forward lookup and a number of other innovations such as \"bit-string labels\" and \"DNAME\" records. It is defined in and its references (with further discussion of the pros and cons of both schemes in ), but has been deprecated to experimental status ().\n\nIPv6 is not foreseen to supplant IPv4 instantaneously. Both protocols will continue to operate simultaneously for some time. Therefore, IPv6 transition mechanisms are needed to enable IPv6 hosts to reach IPv4 services and to allow isolated IPv6 hosts and networks to reach each other over IPv4 infrastructure.\n\nAccording to Silvia Hagen, a dual-stack implementation of the IPv4 and IPv6 on devices is the easiest way to migrate to IPv6. Many other transition mechanisms use tunneling to encapsulate IPv6 traffic within IPv4 networks and vice versa. This is an imperfect solution, which reduces the maximum transmission unit (MTU) of a link and therefore complicates Path MTU Discovery, and may increase latency.\n\nDual-stack IP implementations provide complete IPv4 and IPv6 protocol stacks in the same network node on top of the common physical layer implementation, such as Ethernet. This permits dual-stack hosts to participate in IPv6 and IPv4 networks simultaneously. The method is defined in .\n\nA device with dual-stack implementation has an IPv4 and IPv6 address, and can communicate with other nodes in the LAN or the Internet using either IPv4 or IPv6. The Domain Name System (DNS) protocol is used by both IP implementations to resolve fully qualified domain names (FQDN) and IP addresses, but dual stack requires that the resolving DNS server can resolve both types of addresses. Such a dual stack DNS server would hold IPv4 addresses in the A records, and IPv6 addresses in the AAAA records. Depending on the destination that is to be resolved, a DNS name server may return an IPv4 or IPv6 IP address, or both. A default address selection mechanism, or preferred protocol, needs to be configured either on hosts or the DNS server. The IETF has published Happy Eyeballs to assist dual stack applications, so that they can connect using both IPv4 and IPv6, but prefer an IPv6 connection if it is available. Dual-stack also needs to be implemented on routers, so that they can forward IPv6 packets using the IPv6 versions of routing protocols. When dual stack networks protocols are in place the application layer can be migrated to IPv6.\n\nHowever, outdated DNS server implementations don’t support IPv6. While dual-stack is supported by major operating system and network device vendors, legacy networking hardware and servers don't support IPv6.\n\nInternet service providers (ISPs) are increasingly providing their business and private customers with public-facing IPv6 global unicast addresses. However, if in the local area network (LAN) IPv4 is still used, and the ISP can only provide a public facing IPv6, the IPv4 LAN addresses are translated into the public facing IPv6 address using NAT64, a network address translation (NAT) mechanism. Some ISPs cannot provide their customers with public-facing IPv4 and IPv6 addresses, thus supporting dual stack networking, because some ISPs have exhausted their globally routable IPv4 address pool. Meanwhile, ISP customers are still trying to reach IPv4 web servers and other destinations.\n\nA significant percentage of ISPs in all Regional Internet Registry (RIR) zones have obtained IPv6 address space. This includes many of the world’s major ISPs and mobile network operators, such as Verizon Wireless, StarHub Cable, Chubu Telecommunications, Kabel Deutschland, Swisscom, T-Mobile, Internode and Telefonica.\n\nWhile some ISPs still allocate customers only IPv4 addresses, many ISPs allocate their customers only an IPv6 or dual stack IPv4 and IPv6. ISPs report the share of IPv6 traffic from customers over their network to be anything between 20% and 40%, but by mid-2017 IPv6 traffic still only accounted for a fraction of total traffic at several large Internet exchange points (IXPs). AMS-IX reported it to be 2% and SeattleIX reported 7%. A 2017 survey found that many DSL customers that were served by a dual stack ISP did not request DNS servers to resolve fully qualified domain names into IPv6 addresses. The survey also found that the majority of traffic from IPv6-ready webserver resources were still requested and served over IPv4, mostly due to ISP customers that did not use the dual stack facility provided by their ISP and to a lesser extent due to customers of IPv4-only ISPs.\n\nThe technical basis for tunneling, or encapsulating IPv6 packets in IPv4 packets, is outlined in RFC 4213. When the Internet backbone was IPv4 only one of the frequently used tunneling protocols was 6to4. Teredo tunneling was also frequently used for integrating IPv6 LANs with the IPv4 Internet backbone. Teredo is outlined in RFC 4380 and allows IPv6 local area networks to tunnel over IPv4 networks, by encapsulating IPv6 packets within UDP. The Teredo relay is an IPv6 router that mediates between a Teredo server and the native IPv6 network. It was expected that 6to4 and Teredo would be widely deployed until ISP networks would switch to native IPv6, but by 2014 Google Statistics showed that the use of both mechanisms had dropped to almost 0.\n\nHybrid dual-stack IPv6/IPv4 implementations recognize a special class of addresses, the IPv4-mapped IPv6 addresses. These addresses are typically written with a 96-bit prefix in the standard IPv6 format, and the remaining 32 bits written in the customary dot-decimal notation of IPv4.\n\nAddresses in this group consist of an 80-bit prefix of zeros, the next 16 bits are ones, and the remaining, least-significant 32 bits contain the IPv4 address. For example, ::ffff:192.0.2.128 represents the IPv4 address 192.0.2.128. Another deprecated format for IPv4-compatible IPv6 addresses is ::192.0.2.128.\n\nBecause of the significant internal differences between IPv4 and IPv6 protocol stacks, some of the lower-level functionality available to programmers in the IPv6 stack does not work the same when used with IPv4-mapped addresses. Some common IPv6 stacks do not implement the IPv4-mapped address feature, either because the IPv6 and IPv4 stacks are separate implementations (e.g., Microsoft Windows 2000, XP, and Server 2003), or because of security concerns (OpenBSD). On these operating systems, a program must open a separate socket for each IP protocol it uses. On some systems, e.g., the Linux kernel, NetBSD, and FreeBSD, this feature is controlled by the socket option IPV6_V6ONLY, as specified in .\n\nCompatibility with IPv6 networking is mainly a software or firmware issue. However, much of the older hardware that could in principle be upgraded is likely to be replaced instead. In 2010, the American Registry for Internet Numbers (ARIN) suggested that all Internet servers be prepared to serve IPv6-only clients by January 2012.\n\nHost software may have only IPv4 or only IPv6 networking software, or it may support dual-stack, or hybrid dual-stack operation. All personal computers and smartphones running recent major operating system versions support IPv6. Many popular applications with networking capabilities are compliant. Some software transitioning mechanisms are outlined in , , and .\n\nThe CableLabs consortium published the 160 Mbit/s DOCSIS 3.0 IPv6-ready specification for cable modems in August 2006. DOCSIS 2.0 was updated as \"DOCSIS 2.0 + IPv6\" to provide IPv6 support, which may be available with a firmware upgrade.\n\nThe addition of nodes having IPv6 enabled by default by the software manufacturer, may result in the inadvertent creation of \"shadow networks\", causing IPv6 traffic flowing into networks having only IPv4 security management in place. This may also occur with operating system upgrades, when the newer operating system enables IPv6 by default, while the older one did not. Failing to update the security infrastructure to accommodate IPv6 can lead to IPv6 traffic bypassing it. Shadow networks have occurred on business networks in which enterprises are replacing Windows XP systems that do not have an IPv6 stack enabled by default, with Windows 7 systems, that do. Some IPv6 stack implementors have therefore recommended disabling IPv4 mapped addresses and instead using a dual-stack network where supporting both IPv4 and IPv6 is necessary.\n\nResearch has shown that the use of fragmentation can be leveraged to evade network security controls, similar to IPv4. As a result, requires that the first fragment of an IPv6 packet contains the entire IPv6 header chain, such that some very pathological fragmentation cases are forbidden. Additionally, as a result of research on the evasion of RA-Guard in , has deprecated the use of fragmentation with Neighbor Discovery, and discouraged the use of fragmentation with Secure Neighbor Discovery (SEND).\n\nDue to the anticipated global growth of the Internet, the Internet Engineering Task Force (IETF) in the early 1990s started an effort to develop a next generation IP protocol. By the beginning of 1992, several proposals appeared for an expanded Internet addressing system and by the end of 1992 the IETF announced a call for white papers. In September 1993, the IETF created a temporary, ad-hoc \"IP Next Generation\" (IPng) area to deal specifically with such issues. The new area was led by Allison Mankin and Scott Bradner, and had a directorate with 15 engineers from diverse backgrounds for direction-setting and preliminary document review: The working-group members were J. Allard (Microsoft), Steve Bellovin (AT&T), Jim Bound (Digital Equipment Corporation), Ross Callon (Wellfleet), Brian Carpenter (CERN), Dave Clark (MIT), John Curran (NEARNET), Steve Deering (Xerox), Dino Farinacci (Cisco), Paul Francis (NTT), Eric Fleischmann (Boeing), Mark Knopper (Ameritech), Greg Minshall (Novell), Rob Ullmann (Lotus), and Lixia Zhang (Xerox).\n\nThe Internet Engineering Task Force adopted the IPng model on 25 July 1994, with the formation of several IPng working groups. By 1996, a series of RFCs was released defining Internet Protocol version 6 (IPv6), starting with . (Version 5 was used by the experimental Internet Stream Protocol.)\n\nThe first RFC to standardize IPv6 was the in 1995, which became obsoleted by the in 1998. In July 2017 this RFC was obsoleted and replaced by RFC 8200.\n\nThe 1993 introduction of Classless Inter-Domain Routing (CIDR) in the routing and IP address allocation for the Internet, and the extensive use of network address translation (NAT), delayed IPv4 address exhaustion. The final phase of exhaustion started on 3 February 2011. However, despite a decade long development and implementation history as a Standards Track protocol, general worldwide deployment of IPv6 is increasing slowly. , about 4% of domain names and 16.2% of the networks on the Internet had IPv6 protocol support.\n\nIPv6 has been implemented on all major operating systems in use in commercial, business, and home consumer environments. Since 2008, the domain name system can be used in IPv6. IPv6 was first used in a major world event during the 2008 Summer Olympic Games, the largest showcase of IPv6 technology since the inception of IPv6. Some governments including the Federal government of the United States and China have issued guidelines and requirements for IPv6 capability.\n\nIn 2009, Verizon mandated IPv6 operation, and reduced IPv4 to an optional capability, for LTE cellular hardware. , T-Mobile USA also supports external IPv6 access.\n\nAs of 2014, IPv4 still carried more than 99% of worldwide Internet traffic. The Internet exchanges in Amsterdam and Seattle are the only large exchanges that publicly show IPv6 traffic statistics, which as of October 2018 are tracking at about 2.9% and 7.7%, growing at about 1.9% and -2.6% per year, respectively. , the percentage of users reaching Google services with IPv6 reached 26.0% for the first time, growing at about 4.7% per year. This growth is down from 7.2% per year between July 2016 and July 2017. about 26% of Alexa Top 1000 web servers support IPv6.\n\n\n"}
{"id": "15319", "url": "https://en.wikipedia.org/wiki?curid=15319", "title": "Inca Empire", "text": "Inca Empire\n\nThe Inca Empire (,  \"The Four Regions\"), also known as the Incan Empire and the Inka Empire, was the largest empire in pre-Columbian America. Its political and administrative structure is considered by most scholars to have been the most developed in the Americas before Columbus' arrival. The administrative, political and military center of the empire was located in Cusco, Peru. The Inca civilization arose from the Peruvian highlands sometime in the early 13th century. Its last stronghold was conquered by the Spanish in 1572.\n\nFrom 1438 to 1533, the Incas incorporated a large portion of western South America, centered on the Andean Mountains, using conquest and peaceful assimilation, among other methods. At its largest, the empire joined Peru, southwest Ecuador, western and south central Bolivia, northwest Argentina, northern Chile and a small part of southwest Colombia into a state comparable to the historical empires of Eurasia. Its official language was Quechua. Many local forms of worship persisted in the empire, most of them concerning local sacred \"Huacas\", but the Inca leadership encouraged the sun worship of Inti – their sun god – and imposed its sovereignty above other cults such as that of Pachamama. The Incas considered their king, the Sapa Inca, to be the \"son of the sun.\"\n\nThe Inca Empire was unique in that it lacked many features associated with civilization in the Old World. In the words of one scholar,\n\nThe Incan economy has been described in contradictory ways by scholars:\n\nThe Inca empire functioned largely without money and without markets. Instead, exchange of goods and services was based on reciprocity between individuals and among individuals, groups, and Inca rulers. \"Taxes\" consisted of a labour obligation of a person to the Empire. The Inca rulers (who theoretically owned all the means of production) reciprocated by granting access to land and goods and providing food and drink in celebratory feasts for their subjects.\n\nThe Inca referred to their empire as \"Tawantinsuyu\", \"the four \"suyu\"\". In Quechua, \"tawa\" is four and \"-ntin\" is a suffix naming a group, so that a \"tawantin\" is a quartet, a group of four things taken together, in this case representing the four \"suyu\" (\"regions\" or \"provinces\") whose corners met at the capital. The four \"suyu\" were: Chinchaysuyu (north), Antisuyu (east; the Amazon jungle), Qullasuyu (south) and Kuntisuyu (west). The name \"Tawantinsuyu\" was, therefore, a descriptive term indicating a union of provinces. The Spanish transliterated the name as \"Tahuatinsuyo\" or \"Tahuatinsuyu\".\n\nThe term \"Inka\" means \"ruler\" or \"lord\" in Quechua and was used to refer to the ruling class or the ruling family. The Incas were a very small percentage of the total population of the empire, probably numbering only 15,000 to 40,000, but ruling a population of around 10 million people. The Spanish adopted the term (transliterated as \"Inca\" in Spanish) as an ethnic term referring to all subjects of the empire rather than simply the ruling class. As such, the name \"Imperio inca\" (\"Inca Empire\") referred to the nation that they encountered and subsequently conquered.\n\nThe Inca Empire was the last chapter of thousands of years of Andean civilizations. The Andean civilization was one of five civilizations in the world deemed by scholars to be \"pristine\", that is indigenous and not derivative from other civilizations.\n\nThe Inca Empire was preceded by two large-scale empires in the Andes: the Tiwanaku (c. 300–1100 AD), based around Lake Titicaca and the Wari or Huari (c. 600–1100 AD) centered near the city of Ayacucho. The Wari occupied the Cuzco area for about 400 years. Thus, many of the characteristics of the Inca Empire derived from earlier multi-ethnic and expansive Andean cultures.\n\nCarl Troll has argued that the development of the Inca state in the central Andes was aided by conditions that allows for the elaboration of the staple food chuño. Chuño, which can be stored for long periods, is made of potato dried at the freezing temperatures that are common at nighttime in the southern Peruvian highlands. Such link between the Inca state and chuño may be questioned as potatoes and other crops such as maize can also be dried with only sunlight. Troll did also argue that llamas, the Inca's pack animal, can be found in its largest numbers in this very same region. It is worth considering the maximum extent of the Inca Empire roughly coincided with the greatest distribution of llamas and alpacas in Pre-Hispanic America. The link between the Andean biomes of puna and páramo, pastoralism and the Inca state is a matter of research. As a third point Troll pointed out irrigation technology as advantageous to the Inca state-building. While Troll theorized environmental influences on the Inca Empire he opposed environmental determinism arguing that culture lay at the core of the Inca civilization.\n\nThe Inca people were a pastoral tribe in the Cusco area around the 12th century. Incan oral history tells an origin story of three caves. The center cave at Tampu T'uqu \"(Tambo Tocco)\" was named Qhapaq T'uqu (\"principal niche\", also spelled \"Capac Tocco\"). The other caves were Maras T'uqu \"(Maras Tocco)\" and Sutiq T'uqu \"(Sutic Tocco)\". Four brothers and four sisters stepped out of the middle cave. They were: Ayar Manco, Ayar Cachi, Ayar Awqa \"(Ayar Auca)\" and Ayar Uchu; and Mama Ocllo, Mama Raua, Mama Huaco and Mama Qura \"(Mama Cora)\". Out of the side caves came the people who were to be the ancestors of all the Inca clans.\n\nAyar Manco carried a magic staff made of the finest gold. Where this staff landed, the people would live. They traveled for a long time. On the way, Ayar Cachi boasted about his strength and power. His siblings tricked him into returning to the cave to get a sacred llama. When he went into the cave, they trapped him inside to get rid of him.\n\nAyar Uchu decided to stay on the top of the cave to look over the Inca people. The minute he proclaimed that, he turned to stone. They built a shrine around the stone and it became a sacred object. Ayar Auca grew tired of all this and decided to travel alone. Only Ayar Manco and his four sisters remained.\n\nFinally, they reached Cusco. The staff sank into the ground. Before they arrived, Mama Ocllo had already borne Ayar Manco a child, Sinchi Roca. The people who were already living in Cusco fought hard to keep their land, but Mama Huaca was a good fighter. When the enemy attacked, she threw her bolas (several stones tied together that spun through the air when thrown) at a soldier (gualla) and killed him instantly. The other people became afraid and ran away.\n\nAfter that, Ayar Manco became known as Manco Cápac, the founder of the Inca. It is said that he and his sisters built the first Inca homes in the valley with their own hands. When the time came, Manco Cápac turned to stone like his brothers before him. His son, Sinchi Roca, became the second emperor of the Inca.\n\nUnder the leadership of Manco Cápac, the Inca formed the small city-state Kingdom of Cusco (Quechua \"Qusqu', Qosqo\"). In 1438, they began a far-reaching expansion under the command of Sapa Inca (paramount leader) Pachacuti-Cusi Yupanqui, whose name literally meant \"earth-shaker\". The name of Pachacuti was given to him after he conquered the Tribe of Chancas (modern Apurímac). During his reign, he and his son Tupac Yupanqui brought much of the Andes mountains (roughly modern Peru and Ecuador) under Inca control.\n\nPachacuti reorganized the kingdom of Cusco into the Tahuantinsuyu, which consisted of a central government with the Inca at its head and four provincial governments with strong leaders: Chinchasuyu (NW), Antisuyu (NE), Kuntisuyu (SW) and Qullasuyu (SE). Pachacuti is thought to have built Machu Picchu, either as a family home or summer retreat, although it may have been an agricultural station.\n\nPachacuti sent spies to regions he wanted in his empire and they brought to him reports on political organization, military strength and wealth. He then sent messages to their leaders extolling the benefits of joining his empire, offering them presents of luxury goods such as high quality textiles and promising that they would be materially richer as his subjects.\n\nMost accepted the rule of the Inca as a \"fait accompli\" and acquiesced peacefully. Refusal to accept Inca rule resulted in military conquest. Following conquest the local rulers were executed. The ruler's children were brought to Cusco to learn about Inca administration systems, then return to rule their native lands. This allowed the Inca to indoctrinate them into the Inca nobility and, with luck, marry their daughters into families at various corners of the empire.\n\nTraditionally the son of the Inca ruler led the army. Pachacuti's son Túpac Inca Yupanqui began conquests to the north in 1463 and continued them as Inca ruler after Pachacuti's death in 1471. Túpac Inca's most important conquest was the Kingdom of Chimor, the Inca's only serious rival for the Peruvian coast. Túpac Inca's empire stretched north into modern-day Ecuador and Colombia.\n\nTúpac Inca's son Huayna Cápac added a small portion of land to the north in modern-day Ecuador and in parts of Peru. At its height, the Inca Empire included Peru and Bolivia, most of what is now Ecuador and a large portion of what is today Chile, north of the Maule River. Traditional historiography claims the advance south halted after the Battle of the Maule where they met determined resistance from the Mapuche. This view is challanged by historian Osvaldo Silva who argues instead that it was the social and political framework of the Mapuche that posed the main difficulty in imposing imperial rule. Silva does also add that accepting the battle of the Maule as a statemale, the Incas lacked incentives for conquest they had had when fighting more complex societies such as the Chimú Empire. Silva also disputes the date given by traditional historiography for the battle: the late 15th century during the reign of Topa Inca Yupanqui (1471–93). Instead, he places it in 1532 during the Inca Civil War. Nevertheless, Silva agress on the the claim that the bulk of the Incan conquestest were made during the late 15th century. At the time of the Incan Civil War an Inca army was, according to Diego de Rosales, subduing a revolt among the Diaguitas of Copiapó and Coquimbo. \n\nThe empire's push into the Amazon Basin near the Chinchipe River was stopped by the Shuar in 1527. The empire extended into corners of Argentina and Colombia. However, most of the southern portion of the Inca empire, the portion denominated as Qullasuyu, was located in the Altiplano.\n\nThe Inca Empire was an amalgamation of languages, cultures and peoples. The components of the empire were not all uniformly loyal, nor were the local cultures all fully integrated. The Inca empire as a whole had an economy based on exchange and taxation of luxury goods and labour. The following quote describes a method of taxation:\n\nFor as is well known to all, not a single village of the highlands or the plains failed to pay the tribute levied on it by those who were in charge of these matters. There were even provinces where, when the natives alleged that they were unable to pay their tribute, the Inca ordered that each inhabitant should be obliged to turn in every four months a large quill full of live lice, which was the Inca's way of teaching and accustoming them to pay tribute.\nSpanish conquistadors led by Francisco Pizarro and his brothers explored south from what is today Panama, reaching Inca territory by 1526. It was clear that they had reached a wealthy land with prospects of great treasure, and after another expedition in 1529 Pizarro traveled to Spain and received royal approval to conquer the region and be its viceroy. This approval was received as detailed in the following quote: \"In July 1529 the queen of Spain signed a charter allowing Pizarro to conquer the Incas. Pizarro was named governor and captain of all conquests in Peru, or New Castile, as the Spanish now called the land.\"\n\nWhen they returned to Peru in 1532, a war of succession between the sons of Sapa Inca Huayna Capac, Huáscar and Atahualpa, and unrest among newly conquered territories weakened the empire. Perhaps more importantly, smallpox, influenza, typhus and measles had spread from Central America.\n\nThe forces led by Pizarro consisted of 168 men, one cannon, and 27 horses. Conquistadors ported lances, arquebuses, steel armor and long swords. In contrast, the Inca used weapons made out of wood, stone, copper and bronze, putting them at significant technological disadvantage. In addition, due to the absence of horses in the Americas, the Inca did not develop tactics to fight cavalry. However, the Inca were still effective warriors, being able to successfully fight the Mapuche, which later would strategically defeat the Spanish as they expanded further south.\n\nThe first engagement between the Inca and the Spanish was the Battle of Puná, near present-day Guayaquil, Ecuador, on the Pacific Coast; Pizarro then founded the city of Piura in July 1532. Hernando de Soto was sent inland to explore the interior and returned with an invitation to meet the Inca, Atahualpa, who had defeated his brother in the civil war and was resting at Cajamarca with his army of 80,000 troops, that were at the moment armed only with hunting tools (knives and lassos for hunting llamas).\n\nPizarro and some of his men, most notably a friar named Vincente de Valverde, met with the Inca, who had brought only a small retinue. The Inca offered them ceremonial chicha in a golden cup, which the Spanish rejected. The Spanish interpreter, Friar Vincente, read the \"Requerimiento\" that demanded that he and his empire accept the rule of King Charles I of Spain and convert to Christianity. Atahualpa dismissed the message and asked them to leave. After this, the Spanish began their attack against the mostly unarmed Inca, captured Atahualpa as hostage, and forced the Inca to collaborate.\n\nAtahualpa offered the Spaniards enough gold to fill the room he was imprisoned in and twice that amount of silver. The Inca fulfilled this ransom, but Pizarro deceived them, refusing to release the Inca afterwards. During Atahualpa's imprisonment Huáscar was assassinated elsewhere. The Spaniards maintained that this was at Atahualpa's orders; this was used as one of the charges against Atahualpa when the Spaniards finally executed him, in August 1533.\n\nAlthough \"defeat\" often implies an unwanted loss in battle, much of the Inca elite \"actually welcomed the Spanish invaders as liberators and willingly settled down with them to share rule of Andean farmers and miners.\"\n\nThe Spanish installed Atahualpa's brother Manco Inca Yupanqui in power; for some time Manco cooperated with the Spanish while they fought to put down resistance in the north. Meanwhile, an associate of Pizarro, Diego de Almagro, attempted to claim Cusco. Manco tried to use this intra-Spanish feud to his advantage, recapturing Cusco in 1536, but the Spanish retook the city afterwards. Manco Inca then retreated to the mountains of Vilcabamba and established the small Neo-Inca State, where he and his successors ruled for another 36 years, sometimes raiding the Spanish or inciting revolts against them. In 1572 the last Inca stronghold was conquered and the last ruler, Túpac Amaru, Manco's son, was captured and executed. This ended resistance to the Spanish conquest under the political authority of the Inca state.\n\nAfter the fall of the Inca Empire many aspects of Inca culture were systematically destroyed, including their sophisticated farming system, known as the vertical archipelago model of agriculture. Spanish colonial officials used the Inca mita corvée labor system for colonial aims, sometimes brutally. One member of each family was forced to work in the gold and silver mines, the foremost of which was the titanic silver mine at Potosí. When a family member died, which would usually happen within a year or two, the family was required to send a replacement.\n\nThe effects of smallpox on the Inca empire were even more devastating. Beginning in Colombia, smallpox spread rapidly before the Spanish invaders first arrived in the empire. The spread was probably aided by the efficient Inca road system. Smallpox was only the first epidemic. Other diseases, including a probable Typhus outbreak in 1546, influenza and smallpox together in 1558, smallpox again in 1589, diphtheria in 1614, and measles in 1618, all ravaged the Inca people.\n\nThe number of people inhabiting Tawantinsuyu at its peak is uncertain, with estimates ranging from 4–37 million. Most population estimates are in the range of 6 to 14 million. In spite of the fact that the Inca kept excellent census records using their quipus, knowledge of how to read them was lost as almost all fell into disuse and disintegrated over time or were destroyed by the Spaniards.\n\nThe main form of communication and record-keeping in the empire were quipus, ceramics, textiles and various dialects of Quechua, the language the Incas imposed upon the peoples within the empire. While Quechua had been spoken in the Andean region, including central Peru, for several centuries prior to the expansion of the Inca civilization, the dialect of Quechua the Incas imposed was an adaptation from the Kingdom of Cusco (an early form of \"Southern Quechua\" originally named Qhapaq Runasimi, or 'the great language of the people'), or what some historians define as the Cusco dialect.\n\nThe language imposed by the Incas diverted from its original phonetics as some societies formed their own regional varieties. The diversity of Quechua at that point and even today does not come directly from the Incas, who were just a part of the reason for Quechua's diversity. The civilizations within the empire that had previously spoken Quechua kept their own variety distinct from the Quechua the Incas spread. Although these dialects of Quechua had a similar linguistic structure, they differed according to the region in which they were spoken.\n\nAlthough many of the societies within the empire spoke or learned to speak Quechua, others continued to speak their original languages, such as Aymara, which remains in use in contemporary Bolivia, where it is the primary indigenous language and in various regions surrounding Bolivia. The linguistic body of the Inca Empire was thus varied. The Inca's impact outlasted their empire, as the Spanish continued the use of Quechua.\n\nThe Incas were not known to develop a written form of communication; however, they visually recorded narratives through paintings on vases and cups (qirus). These paintings are usually accompanied by geometric patterns known as toqapu, which are also found in textiles. Researchers have speculated that toqapu patterns could have served as a form of written communication (e.g.: heraldry, or glyphs), however this remains unclear.\n\nThe high infant mortality rates that plagued the Inca Empire caused newborn infants to solely be given the term ‘wawa’ when they were born. Most families did not invest very much into their child until they reached the age of two, or most of the times three years old. Once the child reached the age of three a ‘coming of age’ ceremony occurred. This ceremony was called the \"rutuchikuy\" ritual. For the Incas this ceremony indicated that the child had entered the stage of ‘ignorance’. During this ceremony the family would invite all relatives to their house for food and dance and then each member of the family would receive a lock of hair from the young child. After each family member had received a piece of the hair, the father would then shave the child’s head. According to Covey, this stage of life was categorized by a stage of “ignorance, inexperience, and lack of reason, a condition that the child would overcome with time.” For the Incan society in order to advance from the stage of ignorance to development the child must learn the roles around the family associated with their gender.\n\nThe next important ritual was to celebrate the maturity of child. Unlike the previous ritual ceremony, the celebration of maturity was to signify the child's sexual potency. In the western world this ceremony would be known as a celebration of puberty, however for the Incas it was called \"warachikuy\" for boys and \"qikuchikuy\" for girls. The \"warachikuy\" ceremony included dancing, fasting, tasks to show their strength, and family ceremonies. The boy would also be given new clothes and taught how to act as an unmarried man. On the other hand, the girls ceremony \"qikuchikuy\" signified the onset of menstruation, therefore the girl would go into the forest alone and not return until the bleeding had ended. In the forest she would fast, and upon arrival the girl would be given a new name, adult clothing, and advice. This 'folly' stage of life was important because this was the time the young adults could have sex without having to be a parent.\n\nBetween the ages 20–30 the young adults were \"ripe for serious thought and labor\". The young adults were able to hold on to their 'youthful status' during this age by living at home and helping out their home community. The young adults only reached their 'full potential' once they were married.\n\nIn the end, the male and female terms describe the individuals loss of sexual vitality and humanity. Specifically, the decrepitude stage signifies the loss of their mental well being and further physical dystrophy.\nIn the Incan Empire, the age of marriage differed for men and women; men typically married at the age of 20, while women usually got married around 4 years earlier at the age of 16. Men who were highly ranked in society could have multiple wives, but those lower in the ranks could only take a single wife. Marriages were typically within classes and resembled a more business-like agreement. Once married, the women were expected to cook, collect food and watch over the children and livestock. Girls and mothers would also work around the house to keep it orderly to please the public inspectors. These duties remained the same even after wives became pregnant and with the added responsibility of praying and making offerings to Kanopa, who was the god of pregnancy. It was typical for marriages to begin on a trial basis with both men and women having a say in the longevity of the marriage. If the man felt that it wouldn’t work out or if the woman wanted to return to her parent’s home the marriage would end. Once the marriage was final, the only way the two could be divorced was if they did not have a child together. Marriage within the Empire was crucial for survival. A family was considered disadvantaged if there was not a married couple at the center because everyday life centered around the balance of male and female tasks.\n\nIn the eyes of the Inca, male and female roles were considered equal. The \"indigenous cultures saw the two genders as complementary parts of a whole.\" In other words, there was not a hierarchical structure in the domestic sphere for the Incas. Within the domestic sphere, women were known as the weavers. Women's everyday tasks included: spinning, watching the children, weaving cloth, cooking, brewing chichi, preparing fields for cultivation, planting seeds, bearing children, harvesting, weeding, hoeing, herding, and carrying water. Men on the other hand, \"weeded, plowed, participated in combat, helped in the harvest, carried firewood, built houses, herded llama and alpaca, and spun and wove when necessary\". On looking Spaniards did not understand the complementary nature of men and women roles within the Inca culture and believed women were treated like slaves. However, Inca women did not view themselves as slaves, nor did they do their job for the man. The women completed their daily tasks for the improvement of her household and community, to ensure her family would survive. Furthermore, women were allowed to own land and herds because inheritance was passed down from both the mother's and father's side of the family . Kinship within the Inca society followed a parallel line of descent. In other words, women ascended from women and men ascended from men. Due to the parallel descent, women had access to land and other necessities through her mother, and communities flourished because of the environmental social ties among women .\n\nInca myths were transmitted orally until early Spanish colonists recorded them; however, some scholars claim that they were recorded on quipus, Andean knotted string records.\n\nThe Inca believed in reincarnation. After death, the passage to the next world was fraught with difficulties. The spirit of the dead, \"camaquen,\" would need to follow a long road and during the trip the assistance of a black dog that could see in the dark was required. Most Incas imagined the after world to be like an earthly paradise with flower-covered fields and snow-capped mountains.\n\nIt was important to the Inca that they not die as a result of burning or that the body of the deceased not be incinerated. Burning would cause their vital force to disappear and threaten their passage to the after world. Those who obeyed the Inca moral code – \"ama suwa, ama llulla, ama quella\" (do not steal, do not lie, do not be lazy) – \"went to live in the Sun's warmth while others spent their eternal days in the cold earth\". The Inca nobility practiced cranial deformation. They wrapped tight cloth straps around the heads of newborns to shape their soft skulls into a more conical form, thus distinguishing the nobility from other social classes.\n\nThe Incas made human sacrifices. As many as 4,000 servants, court officials, favorites and concubines were killed upon the death of the Inca Huayna Capac in 1527. The Incas performed child sacrifices around important events, such as the death of the Sapa Inca or during a famine. These sacrifices were known as \"qhapaq hucha\".\n\nThe Incas were polytheists who worshipped many gods. These included:\n\nThe Inca Empire employed central planning. The Inca Empire traded with outside regions, although they did not operate a substantial internal market economy. While axe-monies were used along the northern coast, presumably by the provincial \"mindaláe\" trading class, most households in the empire lived in a traditional economy in which households were required to pay taxes, usually in the form of the \"mit'a\" corvée labor, and military obligations, though barter (or \"trueque\") was present in some areas. In return, the state provided security, food in times of hardship through the supply of emergency resources, agricultural projects (e.g. aqueducts and terraces) to increase productivity and occasional feasts. The economy rested on the material foundations of the vertical archipelago, a system of ecological complementarity in accessing resources and the cultural foundation of \"ayni\", or reciprocal exchange.\n\nThe Sapa Inca was conceptualized as divine and was effectively head of the state religion. The \"Willaq Umu\" (or Chief Priest) was second to the emperor. Local religious traditions continued and in some cases such as the Oracle at Pachacamac on the Peruvian coast, were officially venerated. Following Pachacuti, the Sapa Inca claimed descent from Inti, who placed a high value on imperial blood; by the end of the empire, it was common to incestuously wed brother and sister. He was \"son of the sun,\" and his people the \"intip churin\", or \"children of the sun,\" and both his right to rule and mission to conquer derived from his holy ancestor. The Sapa Inca also presided over ideologically important festivals, notably during the \"Inti Raymi\", or \"Sunfest\" attended by soldiers, mummified rulers, nobles, clerics and the general population of Cusco beginning on the June solstice and culminating nine days later with the ritual breaking of the earth using a foot plow by the Inca. Moreover, Cusco was considered cosmologically central, loaded as it was with \"huacas\" and radiating \"ceque\" lines and geographic center of the Four Quarters; Inca Garcilaso de la Vega called it \"the navel of the universe\".\n\nThe Inca Empire was a federalist system consisting of a central government with the Inca at its head and four quarters, or \"suyu\": Chinchay Suyu (NW), Anti Suyu (NE), Kunti Suyu (SW) and Qulla Suyu (SE). The four corners of these quarters met at the center, Cusco. These \"suyu\" were likely created around 1460 during the reign of Pachacuti before the empire reached its largest territorial extent. At the time the \"suyu\" were established they were roughly of equal size and only later changed their proportions as the empire expanded north and south along the Andes.\n\nCusco was likely not organized as a \"wamani\", or province. Rather, it was probably somewhat akin to a modern federal district, like Washington, DC or Mexico City. The city sat at the center of the four \"suyu\" and served as the preeminent center of politics and religion. While Cusco was essentially governed by the Sapa Inca, his relatives and the royal \"panaqa\" lineages, each \"suyu\" was governed by an \"Apu\", a term of esteem used for men of high status and for venerated mountains. Both Cusco as a district and the four \"suyu\" as administrative regions were grouped into upper \"hanan\" and lower \"hurin\" divisions. As the Inca did not have written records, it is impossible to exhaustively list the constituent \"wamani\". However, colonial records allow us to reconstruct a partial list. There were likely more than 86 \"wamani\", with more than 48 in the highlands and more than 38 on the coast.\n\nThe most populous \"suyu\" was Chinchaysuyu, which encompassed the former Chimu empire and much of the northern Andes. At its largest extent, it extended through much of modern Ecuador and into modern Colombia.\n\nThe largest \"suyu\" by area was Qullasuyu, named after the Aymara-speaking Qulla people. It encompassed the Bolivian Altiplano and much of the southern Andes, reaching Argentina and as far south as the Maipo or Maule river in Central Chile. Historian José Bengoa singled out Quillota as likely being the foremost Inca settlement in Chile.\n\nThe second smallest \"suyu\", Antisuyu, was northwest of Cusco in the high Andes. Its name is the root of the word \"Andes.\"\n\nKuntisuyu was the smallest \"suyu\", located along the southern coast of modern Peru, extending into the highlands towards Cusco.\n\nThe Inca state had no separate judiciary or codified laws. Customs, expectations and traditional local power holders governed behavior. The state had legal force, such as through \"tokoyrikoq\" (lit. \"he who sees all\"), or inspectors. The highest such inspector, typically a blood relative to the Sapa Inca, acted independently of the conventional hierarchy, providing a point of view for the Sapa Inca free of bureaucratic influence.\n\nThe Inca had three moral precepts that governed their behavior:\n\nColonial sources are not entirely clear or in agreement about Inca government structure, such as exact duties and functions of government positions. But the basic structure can be broadly described. The top was the \"Sapa Inca\". Below that may have been the \"Willaq Umu\", literally the \"priest who recounts\", the High Priest of the Sun. However, beneath the \"Sapa Inca\" also sat the \"Inkap rantin\", who was a confidant and assistant to the \"Sapa Inca\", perhaps similar to a Prime Minister. Starting with Topa Inca Yupanqui, a \"Council of the Realm\" was composed of 16 nobles: 2 from \"hanan\" Cusco; 2 from \"hurin\" Cusco; 4 from Chinchaysuyu; 2 from Cuntisuyu; 4 from Collasuyu; and 2 from Antisuyu. This weighting of representation balanced the \"hanan\" and \"hurin\" divisions of the empire, both within Cusco and within the Quarters (\"hanan suyukuna\" and \"hurin suyukuna\").\n\nWhile provincial bureaucracy and government varied greatly, the basic organization was decimal. Taxpayers – male heads of household of a certain age range – were organized into corvée labor units (often doubling as military units) that formed the state's muscle as part of mit'a service. Each unit of more than 100 tax-payers were headed by a \"kuraka\", while smaller units were headed by a \"kamayuq\", a lower, non-hereditary status. However, while \"kuraka\" status was hereditary and typically served for life, the position of a \"kuraka\" in the hierarchy was subject to change based on the privileges of superiors in the hierarchy; a \"pachaka kuraka\" could be appointed to the position by a \"waranqa kuraka\". Furthermore, one \"kuraka\" in each decimal level could serve as the head of one of the nine groups at a lower level, so that a \"pachaka kuraka\" might also be a \"waranqa kuraka\", in effect directly responsible for one unit of 100 tax-payers and less directly responsible for nine other such units.\n\nArchitecture was the most important of the Incan arts, with textiles reflecting architectural motifs. The most notable example is Machu Picchu, which was constructed by Inca engineers. The prime Inca structures were made of stone blocks that fit together so well that a knife could not be fitted through the stonework. These constructs have survived for centuries, with no use of mortar to sustain them.\n\nThis process was first used on a large scale by the Pucara (c. 300 BC–AD 300) peoples to the south in Lake Titicaca and later in the city of Tiwanaku (c. AD 400–1100) in present-day Bolivia. The rocks were sculpted to fit together exactly by repeatedly lowering a rock onto another and carving away any sections on the lower rock where the dust was compressed. The tight fit and the concavity on the lower rocks made them extraordinarily stable, despite the ongoing challenge of earthquakes and volcanic activity.\n\nPhysical measures used by the Inca were based on human body parts. Units included fingers, the distance from thumb to forefinger, palms, cubits and wingspans. The most basic distance unit was \"thatkiy\" or \"thatki\", or one pace. The next largest unit was reported by Cobo to be the \"topo\" or \"tupu\", measuring 6,000 \"thatkiy\"s, or about ; careful study has shown that a range of is likely. Next was the \"wamani\", composed of 30 \"topo\"s (roughly ). To measure area, 25 by 50 wingspans were used, reckoned in \"topo\"s (roughly ). It seems likely that distance was often interpreted as one day's walk; the distance between \"tambo\" way-stations varies widely in terms of distance, but far less in terms of time to walk that distance.\n\nInca calendars were strongly tied to astronomy. Inca astronomers understood equinoxes, solstices and zenith passages, along with the Venus cycle. They could not, however, predict eclipses. The Inca calendar was essentially lunisolar, as two calendars were maintained in parallel, one solar and one lunar. As 12 lunar months fall 11 days short of a full 365-day solar year, those in charge of the calendar had to adjust every winter solstice. Each lunar month was marked with festivals and rituals. Apparently, the days of the week were not named and days were not grouped into weeks. Similarly, months were not grouped into seasons. Time during a day was not measured in hours or minutes, but in terms of how far the sun had travelled or in how long it had taken to perform a task.\n\nThe sophistication of Inca administration, calendrics and engineering required facility with numbers. Numerical information was stored in the knots of \"quipu\" strings, allowing for compact storage of large numbers. These numbers were stored in base-10 digits, the same base used by the Quechua language and in administrative and military units. These numbers, stored in \"quipu\", could be calculated on \"yupanas\", grids with squares of positionally varying mathematical values, perhaps functioning as an abacus. Calculation was facilitated by moving piles of tokens, seeds or pebbles between compartments of the \"yupana\". It is likely that Inca mathematics at least allowed division of integers into integers or fractions and multiplication of integers and fractions.\n\nAccording to mid-17th-century Jesuit chronicler Bernabé Cobo, the Inca designated officials to perform accounting-related tasks. These officials were called quipo camayos. Study of khipu sample VA 42527 (Museum für Völkerkunde, Berlin) revealed that the numbers arranged in calendrically significant patterns were used for agricultural purposes in the \"farm account books\" kept by the khipukamayuq (accountant or warehouse keeper) to facilitate the closing of accounting books.\n\nCeramics were painted using the polychrome technique portraying numerous motifs including animals, birds, waves, felines (popular in the Chavin culture) and geometric patterns found in the Nazca style of ceramics. In a culture without a written language, ceramics portrayed the basic scenes of everyday life, including the smelting of metals, relationships and scenes of tribal warfare. The most distinctive Inca ceramic objects are the Cusco bottles or \"aryballos\". Many of these pieces are on display in Lima in the Larco Archaeological Museum and the National Museum of Archaeology, Anthropology and History.\n\nAlmost all of the gold and silver work of the Incan empire was melted down by the conquistadors.\n\nThe Inca recorded information on assemblages of knotted strings, known as Quipu, although they can no longer be decoded. Originally it was thought that Quipu were used only as mnemonic devices or to record numerical data. Quipus are also believed to record history and literature.\n\nThe Inca made many discoveries in medicine. They performed successful skull surgery, by cutting holes in the skull to alleviate fluid buildup and inflammation caused by head wounds. Many skull surgeries performed by Inca surgeons were successful. Survival rates were 80–90%, compared to about 30% before Inca times.\n\nThe Incas revered the coca plant as sacred/magical. Its leaves were used in moderate amounts to lessen hunger and pain during work, but were mostly used for religious and health purposes. The Spaniards took advantage of the effects of chewing coca leaves. The Chasqui, messengers who ran throughout the empire to deliver messages, chewed coca leaves for extra energy. Coca leaves were also used as an anaesthetic during surgeries.\n\nThe Inca army was the most powerful at that time, because they could turn an ordinary villager or farmer into a soldier. Every able bodied male Inca of fighting age had to take part in war in some capacity at least once and to prepare for warfare again when needed. By the time the empire reached its largest size, every section of the empire contributed in setting up an army for war.\n\nThe Incas had no iron or steel and their weapons were not much more effective than those of their opponents. They went into battle with drums beating and trumpets blowing. Their armor included:\n\nThe Inca weaponry included:\n\nRoads allowed quick movement (on foot) for the Inca army and shelters called \"tambo\" and storage silos called qullqas were built one day's travelling distance from each other, so that an army on campaign could always be fed and rested. This can be seen in names of ruins such as \"Ollantay Tambo\", or My Lord's Storehouse. These were set up so the Inca and his entourage would always have supplies (and possibly shelter) ready as they traveled.\n\nChronicles and references from the 16th and 17th centuries support the idea of a banner. However, it represented the Inca (emperor), not the empire.\n\nFrancisco López de Jerez wrote in 1534:\n... todos venían repartidos en sus escuadras con sus banderas y capitanes que los mandan, con tanto concierto como turcos.(... all of them came distributed into squads, with their flags and captains commanding them, as well-ordered as Turks.) \n\nChronicler Bernabé Cobo wrote:\n\nThe royal standard or banner was a small square flag, ten or twelve spans around, made of cotton or wool cloth, placed on the end of a long staff, stretched and stiff such that it did not wave in the air and on it each king painted his arms and emblems, for each one chose different ones, though the sign of the Incas was the rainbow and two parallel snakes along the width with the tassel as a crown, which each king used to add for a badge or blazon those preferred, like a lion, an eagle and other figures.\n\n<br>(... el guión o estandarte real era una banderilla cuadrada y pequeña, de diez o doce palmos de ruedo, hecha de lienzo de algodón o de lana, iba puesta en el remate de una asta larga, tendida y tiesa, sin que ondease al aire, y en ella pintaba cada rey sus armas y divisas, porque cada uno las escogía diferentes, aunque las generales de los Incas eran el arco celeste y dos culebras tendidas a lo largo paralelas con la borda que le servía de corona, a las cuales solía añadir por divisa y blasón cada rey las que le parecía, como un león, un águila y otras figuras.)<br>-\n\nGuaman Poma's 1615 book, \"El primer nueva corónica y buen gobierno\", shows numerous line drawings of Inca flags. In his 1847 book \"A History of the Conquest of Peru\", \"William H. Prescott ... says that in the Inca army each company had its particular banner and that the imperial standard, high above all, displayed the glittering device of the rainbow, the armorial ensign of the Incas.\" A 1917 world flags book says the Inca \"heir-apparent ... was entitled to display the royal standard of the rainbow in his military campaigns.\"\n\nIn modern times the rainbow flag has been wrongly associated with the Tawantinsuyu and displayed as a symbol of Inca heritage by some groups in Peru and Bolivia. The city of Cusco also flies the Rainbow Flag, but as an official flag of the city. The Peruvian president Alejandro Toledo (2001–2006) flew the Rainbow Flag in Lima's presidential palace. However, according to Peruvian historiography, the Inca Empire never had a flag. Peruvian historian María Rostworowski said, \"I bet my life, the Inca never had that flag, it never existed, no chronicler mentioned it\". Also, to the Peruvian newspaper \"El Comercio\", the flag dates to the first decades of the 20th century, and even the Congress of the Republic of Peru has determined that flag is a fake by citing the conclusion of National Academy of Peruvian History:\n\n\"The official use of the wrongly called 'Tawantinsuyu flag' is a mistake. In the Pre-Hispanic Andean World there did not exist the concept of a flag, it did not belong to their historic context\".\n<br>National Academy of Peruvian History\n\nIncas were able to adapt to their high-altitude living through successful acclimatization, which is characterized by increasing oxygen supply to the blood tissues. For the native Inca living in the Andean highlands, this was achieved through the development of a larger lung capacity, and an increase in red blood cell counts, hemoglobin concentration, and capillary beds.\n\nCompared to other humans, the Incas had slower heart rates, almost one-third larger lung capacity, about 2 L (4 pints) more blood volume and double the amount of hemoglobin, which transfers oxygen from the lungs to the rest of the body. While the Conquistadors may have been slightly taller, the Inca had the advantage of coping with the extraordinary altitude.\n\n\n"}
{"id": "15321", "url": "https://en.wikipedia.org/wiki?curid=15321", "title": "Inca (disambiguation)", "text": "Inca (disambiguation)\n\nThe Inca Empire was the largest empire in pre-Columbian America.\n\nInca, Inka, or İncə may also refer to:\n\n\n\n\n\n\n\n\n\n"}
{"id": "15323", "url": "https://en.wikipedia.org/wiki?curid=15323", "title": "Internet Protocol", "text": "Internet Protocol\n\nThe Internet Protocol (IP) is the principal communications protocol in the Internet protocol suite for relaying datagrams across network boundaries. Its routing function enables internetworking, and essentially establishes the Internet.\n\nIP has the task of delivering packets from the source host to the destination host solely based on the IP addresses in the packet headers. For this purpose, IP defines packet structures that encapsulate the data to be delivered. It also defines addressing methods that are used to label the datagram with source and destination information.\n\nHistorically, IP was the connectionless datagram service in the original Transmission Control Program introduced by Vint Cerf and Bob Kahn in 1974; the other being the connection-oriented Transmission Control Protocol (TCP). The Internet protocol suite is therefore often referred to as TCP/IP.\n\nThe first major version of IP, Internet Protocol Version 4 (IPv4), is the dominant protocol of the Internet. Its successor, Internet Protocol Version 6 (IPv6), has been growing in adoption for the last years, reaching almost 25% of the Internet traffic as of October, 2018.\n\nThe Internet Protocol is responsible for addressing host interfaces, encapsulating data into datagrams (including fragmentation and reassembly) and routing datagrams from a source host interface to a destination host interface across one or more IP networks. For these purposes, the Internet Protocol defines the format of packets and provides an addressing system.\n\nEach datagram has two components: a header and a payload. The IP header includes source IP address, destination IP address, and other metadata needed to route and deliver the datagram. The payload is the data that is transported. This method of nesting the data payload in a packet with a header is called encapsulation.\n\nIP addressing entails the assignment of IP addresses and associated parameters to host interfaces. The address space is divided into subnetworks, involving the designation of network prefixes. IP routing is performed by all hosts, as well as routers, whose main function is to transport packets across network boundaries. Routers communicate with one another via specially designed routing protocols, either interior gateway protocols or exterior gateway protocols, as needed for the topology of the network.\n\nIn May 1974, the Institute of Electrical and Electronic Engineers (IEEE) published a paper entitled \"A Protocol for Packet Network Intercommunication\". The paper's authors, Vint Cerf and Bob Kahn, described an internetworking protocol for sharing resources using packet switching among network nodes. A central control component of this model was the \"Transmission Control Program\" that incorporated both connection-oriented links and datagram services between hosts. The monolithic Transmission Control Program was later divided into a modular architecture consisting of the Transmission Control Protocol and User Datagram Protocol at the transport layer and the Internet Protocol at the network layer. The model became known as the \"Department of Defense (DoD) Internet Model\" and \"Internet protocol suite\", and informally as \"TCP/IP\".\n\nIP versions 0 to 3 were experimental versions, used between 1977 and 1979. The following Internet Experiment Note (IEN) documents describe versions of the Internet Protocol prior to the modern version of IPv4:\n\nThe dominant internetworking protocol in the Internet Layer in use today is IPv4; the number 4 is the protocol version number carried in every IP datagram. IPv4 is described in (1981).\n\nVersion 5 was used by the Internet Stream Protocol, an experimental streaming protocol.\n\nThe successor to IPv4 is IPv6. IPv6 was a result of several years of experimentation and dialog during which various protocol models were proposed, such as TP/IX (), PIP () and TUBA (TCP and UDP with Bigger Addresses, ). Its most prominent difference from version 4 is the size of the addresses. While IPv4 uses 32 bits for addressing, yielding c. 4.3 billion () addresses, IPv6 uses 128-bit addresses providing ca. 340 undecillion, or addresses. Although adoption of IPv6 has been slow, , all United States government systems have demonstrated basic infrastructure support for IPv6.\n\nThe assignment of the new protocol as IPv6 was uncertain until due diligence revealed that IPv6 had not yet been used previously. Other protocol proposals named \"IPv9\" and \"IPv8\" briefly surfaced, but had no affiliation with any international standards body, and have had no support. However, on April 1, 1994, the IETF published an April Fools' Day joke about IPv9.\n\nThe design of the Internet protocol suite adheres to the end-to-end principle, a concept adapted from the CYCLADES project. Under the end-to-end principle, the network infrastructure is considered inherently unreliable at any single network element or transmission medium and is dynamic in terms of availability of links and nodes. No central monitoring or performance measurement facility exists that tracks or maintains the state of the network. For the benefit of reducing network complexity, the intelligence in the network is purposely located in the end nodes.\n\nAs a consequence of this design, the Internet Protocol only provides best-effort delivery and its service is characterized as unreliable. In network architectural language, it is a connectionless protocol, in contrast to connection-oriented communication. Various error conditions may occur, such as data corruption, packet loss and duplication. Because routing is dynamic, meaning every packet is treated independently, and because the network maintains no state based on the path of prior packets, different packets may be routed to the same destination via different paths, resulting in out-of-order delivery to the receiver.\n\nAll error conditions in the network must be detected and compensated by the participating end nodes. The upper layer protocols of the Internet protocol suite are responsible for resolving reliability issues. For example, a host may buffer network data to ensure correct ordering before the data is delivered to an application.\n\nIPv4 provides safeguards to ensure that the IP packet header is error-free. A routing node calculates a checksum for a packet. If the checksum is bad, the routing node discards the packet. Although the Internet Control Message Protocol (ICMP) allows such notification, the routing node is not required to notify either end node of these errors. By contrast, in order to increase performance, and since current link layer technology is assumed to provide sufficient error detection, the IPv6 header has no checksum to protect it.\n\nThe dynamic nature of the Internet and the diversity of its components provide no guarantee that any particular path is actually capable of, or suitable for, performing the data transmission requested. One of the technical constraints is the size of data packets allowed on a given link. Facilities exist to examine the maximum transmission unit (MTU) size of the local link and Path MTU Discovery can be used for the entire intended path to the destination.\n\nThe IPv4 internetworking layer has the ability to automatically fragment the original datagram into smaller units for transmission. In this case, IP provides re-ordering of fragments delivered out of order. An IPv6 network does not perform fragmentation or reassembly, and as per the end-to-end principle, requires end stations and higher-layer protocols to avoid exceeding the network's MTU.\n\nThe Transmission Control Protocol (TCP) is an example of a protocol that adjusts its segment size to be smaller than the MTU. The User Datagram Protocol (UDP) and ICMP disregard MTU size, thereby forcing IP to fragment oversized datagrams.\n\nDuring the design phase of the ARPANET and the early Internet, the security aspects and needs of a public, international network could not be adequately anticipated. Consequently, many Internet protocols exhibited vulnerabilities highlighted by network attacks and later security assessments. In 2008, a thorough security assessment and proposed mitigation of problems was published. The IETF has been pursuing further studies.\n\n\n"}
{"id": "15328", "url": "https://en.wikipedia.org/wiki?curid=15328", "title": "Impeachment", "text": "Impeachment\n\nImpeachment is the process by which a legislative body levels charges against a government official. It does not mean removal from office; it is only a statement of charges, akin to an indictment in criminal law. Once an individual is impeached, he or she must then face the possibility of conviction by a legislative vote, which judgment entails removal from office.\n\nBecause impeachment and conviction of officials involve an overturning of the normal constitutional procedures by which individuals achieve high office (election, ratification, or appointment) and because it generally requires a supermajority, they are usually reserved for those deemed to have committed serious abuses of their office. In the United States, for example, impeachment at the federal level is limited to those who may have committed \"high crimes and misdemeanors\".\n\nImpeachment exists under constitutional law in many countries around the world, including Brazil, the Republic of Ireland, India, the Philippines, Russia, South Korea, and the United States.\n\nThe word \"impeachment\" derives from Old French \"empeechier\" from Latin \"impedicare\" expressing the idea of becoming caught or entrapped, and has analogues in the modern French verb \"empêcher\" (to prevent) and the modern English \"impede\". Medieval popular etymology also associated it (wrongly) with derivations from the Latin \"impetere\" (to attack). (In its more frequent and more technical usage, impeachment of a witness means challenging the honesty or credibility of that person.)\n\nImpeachment was first used in the British political system. Specifically, the process was first used by the English \"Good Parliament\" against Baron Latimer in the second half of the 14th century. Following the British example, the constitutions of Virginia (1776), Massachusetts (1780) and other states thereafter adopted the impeachment mechanism, but they restricted the punishment to removal of the official from office. As well, in private organizations, a motion to impeach can be used to prefer charges.\n\nThe Austrian Federal President can be impeached by the Federal Assembly (\"Bundesversammlung\") before the Constitutional Court. The constitution also provides for the recall of the president by a referendum. Neither of these courses has ever been taken. This is likely because while the President is vested with considerable powers on paper, they act as a largely ceremonial figurehead in practice, and are thus hardly in a position to abuse their powers.\n\nThe President of the Federative Republic of Brazil, state governors and municipal mayors may be impeached by the Chamber of Deputies and tried and removed by the Federal Senate. Upon conviction, the officeholder has his political rights revoked for eight years—which has the effect of barring him from running for any office.\n\nFernando Collor de Mello, the 32nd President of Brazil, resigned in 1992 amidst impeachment proceedings. Despite his resignation, the Senate nonetheless voted to convict him and bar him from holding any office for eight years, due to evidence of bribery and misappropriation.\n\nIn 2016, the Chamber of Deputies initiated an impeachment case against President Dilma Rousseff on allegations of budgetary mismanagement. Following her conviction, she was replaced by Vice President Michel Temer, who had served as acting president while Rousseff's case was pending.\n\nThe President of Bulgaria can be removed only for high treason or violation of the constitution. The process is started by a two-thirds majority vote of the Parliament to impeach the President, whereupon the Constitutional Court decides whether the President is guilty of the crime of which he is charged. If he is found guilty, he is removed from power. No Bulgarian President has ever been impeached. The same procedure can be used to remove the Vice President of Bulgaria, which has also never happened.\n\nThe process of impeaching the President of Croatia can be initiated by a two-thirds majority vote in favor in the Sabor and is thereafter referred to the Constitutional Court, which must accept such a proposal with a two-thirds majority vote in favor in order for the president to be removed from office. This has, however, never occurred in the history of the Republic of Croatia. However, in case of a successful impeachment motion a president's constitutional term of five years would be terminated and an election called within 60 days of the vacancy occurring. During the period of vacancy the presidential powers and duties would be carried out by the Speaker of the Croatian Parliament in his/her capacity as Acting President of the Republic.\n\nPrior to 2013 the President of the Czech Republic could be impeached only for an act of high treason (which is not defined in the Constitution of the Czech Republic itself). The process has to start in the Senate of the Czech Republic which only has the right to impeach the president, this passes the case to the Constitutional Court of the Czech Republic which has to decide whether the President is guilty or not. If the Court decides that the President is guilty then the President loses his office and the ability to be elected President of the Czech Republic ever again. No Czech president has ever been impeached, though, members of the Senate sought to impeach President Vaclav Klaus in 2013. This case was dismissed by the court reasoning that his mandate has expired.\n\nIn 2013 the constitution changed; now the process can be started by at least three-fifths of present senators and must be approved by at least \nthree-fifths of all members of Parliament. Also, the President can be impeached not only for high treason (newly defined in the Constitution) but also for a serious infringement of the Constitution.\n\nThe Federal President of Germany can be impeached both by the Bundestag and by the Bundesrat for willfully violating Federal law. Once the Bundestag or the Bundesrat impeaches the president, the Federal Constitutional Court decides whether the President is guilty as charged and, if this is the case, whether to remove him or her from office. The Federal Constitutional Court also has the power to remove federal judges from office for willfully violating core principles of the federal constitution or a state constitution. The impeachment procedure is regulated in Article 61 of the Basic Law for the Federal Republic of Germany.\n\nThe Chief Executive of Hong Kong can be impeached by the Legislative Council. A motion for investigation, initiated jointly by at least one-fourth of all the legislators charging the Chief Executive with \"serious breach of law or dereliction of duty\" and refusing to resign, shall first be passed by the Council. An independent investigation committee, chaired by the Chief Justice of the Court of Final Appeal, will then carry out the investigation and report back to the Council. If the Council find the evidence sufficient to substantiate the charges, it may pass a motion of impeachment by a two-thirds majority.\n\nHowever, the Legislative Council does not have the power actually to remove the Chief Executive from office, as the Chief Executive is appointed by the Central People's Government. The Council can only report the result to the Central People's Government for its decision.\n\nThe president, judges including chief justice of supreme court and high courts can be impeached by the parliament before the expiry of the term for violation of the Constitution. Other than impeachment, no other penalty can be given to a president in position for the violation of the Constitution under of the constitution. However a president after his term/removal can be punished for his already proven unlawful activity under disrespecting constitution, etc. No president has faced impeachment proceedings. Hence, the provisions for impeachment have never been tested. The president in position cannot be charged and needs to step down in order for that to happen.\n\nThe Assembly of Experts can impeach the Supreme Leader of Iran and appoint a new one, although the Supreme Leader elects half the Assembly.\n\nThe President of Iran can be impeached jointly by the members of the Assembly (Majlis) and the Supreme Leader. A new presidential election is then triggered. Abolhassan Banisadr, Iran's first president, was impeached in June 1981 and removed from the office. Mohammad-Ali Rajai was elected as the new president.\n\nCabinet ministers can be impeached by the members of the Assembly. Presidential appointment of a new minister is subject to a parliamentary vote of confidence. Impeachment of ministers has been a fairly commonly used tactic in the power struggle between the president and the assembly during the last several governments.\n\nIn the Republic of Ireland formal impeachment only applies to the Irish president. Article 12 of the Irish Constitution provides that, unless judged to be \"permanently incapacitated\" by the Supreme Court, the president can only be removed from office by the houses of the Oireachtas (parliament) and only for the commission of \"stated misbehaviour\". Either house of the Oireachtas may impeach the president, but only by a resolution approved by a majority of at least two-thirds of its total number of members; and a house may not consider a proposal for impeachment unless requested to do so by at least thirty of its number.\n\nWhere one house impeaches the president, the remaining house either investigates the charge or commissions another body or committee to do so. The investigating house can remove the president if it decides, by at least a two-thirds majority of its members, both that the president is guilty of the charge, and that the charge is sufficiently serious as to warrant the president's removal. To date no impeachment of an Irish president has ever taken place. The president holds a largely ceremonial office, the dignity of which is considered important, so it is likely that a president would resign from office long before undergoing formal conviction or impeachment.\n\nThe Republic's Constitution and law also provide that only a joint resolution of both houses of the Oireachtas may remove a judge. Although often referred to as the \"impeachment\" of a judge, this procedure does not technically involve impeachment.\n\nIn Italy, according to Article 90 of the Constitution, the President of the Republic can be impeached through a majority vote of the Parliament in joint session for high treason and for attempting to overthrow the Constitution. If impeached, the President of the Republic is then tried by the Constitutional Court integrated with sixteen citizens older than forty chosen by lot from a list compiled by the Parliament every nine years.\n\nItalian press and political forces made use of the term \"impeachment\" for the attempt by some members of parliamentary opposition to initiate the procedure provided for in Article 90 against Presidents Francesco Cossiga (1991), Giorgio Napolitano (2014) and Sergio Mattarella (2018)\n\nMembers of the Liechtenstein Government can be impeached before the State Court for breaches of the Constitution or of other laws. As a hereditary monarchy the Sovereign Prince can not be impeached as he \"is not subject to the jurisdiction of the courts and does not have legal responsibility\". The same is true of any member of the Princely House who exercises the function of head of state should the Prince be temporarily prevented or in preparation for the Succession.\n\nIn the Republic of Lithuania, the President may be impeached by a three-fifths majority in the Seimas. President Rolandas Paksas was removed from office by impeachment on April 6, 2004 after the Constitutional Court of Lithuania found him guilty of having violated his oath and the constitution. He was the first European head of state to have been impeached.\n\nMembers of government, representatives of the national assembly (Stortinget) and Supreme Court judges can be impeached for criminal offenses tied to their duties and committed in office, according to the Constitution of 1814, §§ 86 and 87. The procedural rules were modeled after the US rules and are quite similar to them. Impeachment has been used eight times since 1814, last in 1927. Many argue that impeachment has fallen into desuetude. In cases of impeachment, an appointed court (Riksrett) takes effect.\n\nThe country's ruling coalition said on August 7, 2008, that it would seek the impeachment of President Pervez Musharraf, alleging the U.S.-backed former general had \"eroded the trust of the nation\" and increasing pressure on him to resign. He resigned on August 18, 2008. Another kind of impeachment in Pakistan is known as the vote of less-confidence or vote of mis-understanding and has been practiced by provincial assemblies to weaken the national assembly.\n\nImpeaching a president requires a two-thirds majority support of lawmakers in a joint session of both houses of Parliament.\n\nImpeachment in the Philippines follows procedures similar to the United States. Under Sections 2 and 3, Article XI, Constitution of the Philippines, the House of Representatives of the Philippines has the exclusive power to initiate all cases of impeachment against the President, Vice President, members of the Supreme Court, members of the Constitutional Commissions (Commission on Elections, Civil Service Commission and the Commission on Audit), and the Ombudsman. When a third of its membership has endorsed the impeachment articles, it is then transmitted to the Senate of the Philippines which tries and decide, as impeachment tribunal, the impeachment case.\n\nA main difference from US proceedings however is that only one third of House members are required to approve the motion to impeach the President (as opposed to a simple majority of those present and voting in their US counterpart). In the Senate, selected members of the House of Representatives act as the prosecutors and the Senators act as judges with the Senate President presiding over the proceedings (the Chief Justice jointly presides with the Senate President if the President is on trial). Like the United States, to convict the official in question requires that a minimum of two thirds (i.e. 16 of 24 members) of all the Members of the Senate vote in favor of conviction. If an impeachment attempt is unsuccessful or the official is acquitted, no new cases can be filed against that impeachable official for at least one full year.\n\nThe 1987 Philippine Constitution says the grounds for impeachment include culpable violation of the Constitution, bribery, graft and corruption, and betrayal of public trust. These offenses are considered \"high crimes and misdemeanors\" under the Philippine Constitution.\n\nThe President, Vice President, Supreme Court justices, and members of the Constitutional Commission and Ombudsman are all considered impeachable officials under the Constitution.\n\nPresident Joseph Estrada was the first official impeached by the House in 2000, but the trial ended prematurely due to outrage over a vote to open an envelope where that motion was narrowly defeated by his allies. Estrada was deposed days later during the 2001 EDSA Revolution.\n\nIn 2005, 2006, 2007 and 2008, impeachment complaints were filed against President Gloria Macapagal-Arroyo, but none of the cases reached the required endorsement of 1/3 of the members for transmittal to, and trial by, the Senate.\n\nIn March 2011, the House of Representatives impeached Ombudsman Merceditas Gutierrez, becoming the second person to be impeached. In April, Gutierrez resigned prior to the Senate's convening as an impeachment court.\n\nIn December 2011, in what was described as \"blitzkrieg fashion\", 188 of the 285 members of the House of Representatives voted to transmit the 56-page Articles of Impeachment against Supreme Court Chief Justice Renato Corona.\n\nTo date, three officials had been successfully impeached by the House of Representatives, and two were not convicted. The latter, Chief Justice Renato C. Corona, on May 29, 2012 has been convicted by the Senate guilty under Article II of the Articles of Impeachment (of betraying public trust), with 20-3 votes from the Senator Judges.\n\nIn Polish law there is no impeachment procedure defined, as it is present in the other countries. Infringements of the law can be investigated only by special Parliament's Committee or (if accusations involve people holding the highest offices of state) by the State Tribunal. The State Tribunal is empowered to rule for the removal of individuals from public office but it is not a common practice.\n\nThe President can be impeached by Parliament and is then suspended. A referendum then follows to determine whether the suspended President should be removed from office. President Traian Băsescu was impeached twice by the Parliament: in 2007 and more recently in July 2012. A referendum was held on May 19, 2007 and a large majority of the electorate voted against removing the president from office. For the most recent suspension a referendum was held on July 29, 2012, but was invalidated due to low turnout, however the results were heavily against the president ()\n\nThe President of Russia can be impeached if both the State Duma (which initiates the impeachment process through the formation of a special investigation committee) and the Federation Council of Russia vote by a two-thirds majority in favor of impeachment and, additionally, the Supreme Court finds the President guilty of treason or a similarly heavy crime against the nation and the Constitutional Court confirms that the constitutional procedure of the impeachment process was correctly observed. In 1995–1999, the Duma made several attempts to impeach then-President Boris Yeltsin, but they never had a sufficient number of votes for the process to reach the Federation Council.\n\nAccording to the Article 65 Clause 1 of Constitution of South Korea, if President, Prime Minister, or other state council members including Supreme Court and Constitutional court members, violate the Constitution or other laws of official duty, the National Assembly can impeach them. Clause 2 states the impeachment bill may be proposed by one third or more of the total members of the National Assembly, and shall require majority voting and approved by two thirds or more of the total members of the National Assembly. This article also states that any person against whom a motion for impeachment has been passed shall be suspended from exercising his power until the impeachment has been adjudicated and shall not extend further than removal from public office. Provided, That it shall not exempt the person impeached from civil or criminal liability.\n\nTwo presidents have been impeached since the foundation of the Sixth Republic of Korea and adoption of the new Constitution of South Korea in 1987. Roh Moo-hyun in 2004 was impeached by the National Assembly but was overturned by the Constitutional Court. Park Geun-hye in 2016 was impeached by the National Assembly, and the impeachment was confirmed by the Constitutional Court on March 10, 2017.\n\nIn Taiwan, according to the Additional Articles of the Constitution of the Republic of China, impeachment of the president or the vice president by the Legislative Yuan shall be initiated upon the proposal of more than one-half of the total members of the Legislative Yuan and passed by more than two-thirds of the total members of the Legislative Yuan, whereupon it shall be presented to the grand justices of the Judicial Yuan for adjudication.\n\nDuring the crisis which started in November 2013, the increasing political stress of the face-down between the protestors occupying Independence Square in Kiev and the State Security forces under the control of President Yanukovych led to deadly armed force being used on the protestors. Following the negotiated return of Kiev's City Hall on February 16, 2014, occupied by the protesters since November 2013, the security forces thought they could also retake \"Maidan\", Independence Square. The ensuing fighting from 17 through 21 February 2014 resulted in a considerable number of deaths and a more generalised alienation of the population, and the withdrawal of President Yanukovych to his support area in the East of Ukraine.\nIn the wake of the President's departure, Parliament convened on February 22; it reinstated the 2004 Constitution, which reduced Presidential authority, and voted impeachment of President Yanukovych as \"de facto\" recognition of his departure from office as President of an integrated Ukraine. The President riposted that Parliament's acts were illegal as they could pass into law only by Presidential signature.\n\nIn the United Kingdom anybody may be prosecuted and tried by the two Houses of Parliament for any crime. The first recorded impeachment is that of William Latimer, 4th Baron Latimer during the Good Parliament of 1376. The last was that of Henry Dundas, 1st Viscount Melville in 1806.\n\nThe House of Commons holds the power to impeach. Any member may make an accusation of any crime. The member must support the charge with evidence and move for impeachment. If the Commons carries the motion, the mover receives orders to go to the bar at the House of Lords and to impeach the accused \"in the name of the House of Commons, and all the commons of the United Kingdom.\"\n\nThe mover must tell the Lords that the House of Commons will, in due time, exhibit particular articles against the accused, and make good the same. The Commons then usually selects a committee to draw up the charges and create an \"Article of Impeachment\" for each. (In the case of Warren Hastings, however, the drawing up of the articles preceded the formal impeachment.) Once the committee has delivered the articles to the Lords, replies go between the accused and the Commons via the Lords. If the Commons have impeached a peer, the Lords take custody of the accused; otherwise, custody goes to Black Rod. The accused remains in custody unless the Lords allow bail. The Lords set a date for the trial while the Commons appoints managers, who act as prosecutors in the trial. The accused may defend by counsel.\n\nThe House of Lords hears the case. The procedure used to be that the Lord Chancellor presided (or the Lord High Steward if the defendant was a peer); but this was when the Lord Chancellor was both the Lords' presiding officer and head of the judiciary of England and Wales. Since both these roles were removed from that office by the Constitutional Reform Act 2005, which created the Lord Speaker to preside over the Lords and made the Lord Chief Justice head of the judiciary, it is not certain who would preside over an impeachment trial today. If Parliament is not in session, then the trial is conducted by a \"Court of the Lord High Steward\" instead of the House of Lords (even if the defendant is not a peer). The differences between this court and the House of Lords are that in the House all of the peers are judges of both law and fact, whereas in the Court the Lord High Steward is the sole judge of law and the peers decide the facts only; and the bishops are not entitled to sit and vote in the Court. Traditionally, peers would wear their parliamentary robes during the hearings.\n\nThe hearing resembles an ordinary trial: both sides may call witnesses and present evidence. At the end of the hearing the lords vote on the verdict, which is decided by a simple majority, one charge at a time. Upon being called, a peer must rise and declare \"guilty, upon my honour\" or \"not guilty, upon my honour\". After voting on all of the articles has taken place, and if the Lords find the defendant guilty, the Commons may move for judgment; the Lords may not declare the punishment until the Commons have so moved. The Lords may then decide whatever punishment they find fit, within the law. A royal pardon cannot excuse the defendant from trial, but a pardon may reprieve a convicted defendant. However, a pardon cannot override a decision to remove the defendant from the public office they hold.\n\nParliament has held the power of impeachment since medieval times. Originally, the House of Lords held that impeachment could apply only to members of the peerage, as the nobility (the Lords) would try their own peers, while commoners ought to try their peers (other commoners) in a jury. However, in 1681, the Commons declared that they had the right to impeach anyone, and the Lords have respected this resolution. Offices held \"during good behaviour\" are terminable by the writ of either \"quo warranto\" or \"scire facias\", which has even been employed by and against well-placed judges.\n\nAfter the reign of Edward IV, impeachment fell into disuse, the bill of attainder becoming the preferred form of dealing with undesirable subjects of the Crown. However, during the reign of James I and thereafter, impeachments became more popular, as they did not require the assent of the Crown, while bills of attainder did, thus allowing Parliament to resist royal attempts to dominate Parliament. The most recent cases of impeachment dealt with Warren Hastings, Governor-General of India between 1773 and 1786 (impeached in 1788; the Lords found him not guilty in 1795), and Henry Dundas, 1st Viscount Melville, First Lord of the Admiralty, in 1806 (acquitted). The last attempted impeachment occurred in 1848, when David Urquhart accused Lord Palmerston of having signed a secret treaty with Imperial Russia and of receiving monies from the Tsar. Palmerston survived the vote in the Commons; the Lords did not hear the case.\n\nQueen Caroline, consort of King George IV, was tried by the House of Commons and acquitted. The process began as impeachment proceedings, but then became a different procedure as a bill of pains and penalties.\n\nThe procedure has, over time, become rarely used and some legal authorities (such as Halsbury's Laws of England) consider it to be probably obsolete. The principles of \"responsible government\" require that the Prime Minister and other executive officers answer to Parliament, rather than to the Sovereign. Thus the Commons can remove such an officer through a motion of no confidence without a long, drawn-out impeachment. However, it is argued by some that the remedy of impeachment remains as part of British constitutional law, and that legislation would be required to abolish it. Furthermore, impeachment as a means of punishment for wrongdoing, as distinct from being a means of removing a minister, remains a valid reason for accepting that it continues to be available, at least in theory.\n\nThe Select Committee on Parliamentary Privilege in 1967 recommended \"that the right to impeach, which has long been in disuse, be now formally abandoned\". Their recommendation not having been implemented in the meantime, the Select Committee on Privileges in 1977 declared it \"to be of continuing validity\" and again urged that it be adopted. Shortly before this report was issued, in April 1977 the Young Liberals' annual conference unanimously passed a motion calling on Liberal Party leader David Steel to move for the impeachment of Ronald King Murray QC, the Lord Advocate, over his handling of the Patrick Meehan miscarriage of justice affair. Steel did not move any such motion but Murray (who later became Lord Murray, a Senator of the College of Justice of Scotland) agreed that the power still existed.\n\nThe Joint Committee on Parliamentary Privilege in 1999 noted the previous recommendations to formally abandon the power impeachment, and stated that \"The circumstances in which impeachment has taken place are now so remote from the present that the procedure may be considered obsolete\". Notwithstanding, on August 25, 2004, Plaid Cymru MP Adam Price announced his intention to move for the impeachment of Tony Blair for his role in involving Britain in the 2003 invasion of Iraq. He asked the Leader of the House of Commons Peter Hain whether he would confirm that the power to impeach was still available, reminding Hain that as President of the Young Liberals he had supported the attempted impeachment of Murray. Hain responded by quoting the 1999 Joint Committee's report, and the advice of the Clerk of the House of Commons that impeachment \"effectively died with the advent of full responsible Parliamentary government\".\n\nThe election court has some of the powers associated with impeachment cases in other countries, and can remove elected officials from office in the case of electoral fraud. Lutfur Rahman was the directly elected mayor of Tower Hamlets, in London until he was removed from office for breaching electoral rules.\n\nSimilar to the British system, Article One of the United States Constitution gives the House of Representatives the sole power of impeachment and the Senate the sole power to try impeachments of officers of the U.S. federal government. (Various state constitutions include similar measures, allowing the state legislature to impeach the governor or other officials of the state government.) In contrast to the British system, in the United States impeachment is only the first of two stages, and conviction during the second stage requires \"the concurrence of two thirds of the members present.\" Impeachment does not necessarily result in removal from office; it is only a legal statement of charges, parallel to an indictment in criminal law. An official who is impeached faces a second legislative vote (whether by the same body or another), which determines conviction, or failure to convict, on the charges embodied by the impeachment. Most constitutions require a supermajority to convict. Although the subject of the charge is criminal action, it does not constitute a criminal trial; the only question under consideration is the removal of the individual from office, and the possibility of a subsequent vote preventing the removed official from ever again holding political office in the jurisdiction where he or she was removed. Impeachment with respect to political office should not be confused with witness impeachment.\n\nThe Constitution defines impeachment at the federal level and limits impeachment to \"The President, Vice President, and all civil officers of the United States\" who may be impeached and removed only for \"treason, bribery, or other high crimes and misdemeanors\". Several commentators have suggested that Congress alone may decide for itself what constitutes a \"high crime or misdemeanor\", especially since \"Nixon v. United States\" stated that the Supreme Court did not have the authority to determine whether the Senate properly \"tried\" a defendant.\nIn 1970, then-House Minority Leader Gerald R. Ford defined the criterion as he saw it: \"An impeachable offense is whatever a majority of the House of Representatives considers it to be at a given moment in history.\"\n\nThe central question regarding the Constitutional dispute about the impeachment of members of the legislature is whether members of Congress are officers of the United States. The Constitution grants the House the power to impeach \"The President, the Vice President, and all civil Officers of the United States.\" It has been suggested that members of Congress are not officers of the United States. Others, however, believe that members are civil officers and are subject to impeachment.\n\nThe House of Representatives impeached Senator William Blount in 1798, resulting in his expulsion. However, after initially hearing his impeachment, charges were dismissed for lack of jurisdiction. Left unsettled was the question whether members of Congress were civil officers of the United States. The House has not impeached a Member of Congress since Blount. As each House has the authority to expel its own members without involving the other chamber, expulsion has been the method used for removing Members of Congress.\n\nJefferson's Manual, which is integral to the Rules of the House of Representatives, states that impeachment is set in motion by charges made on the floor, charges proferred by a memorial, a member's resolution referred to a committee, a message from the president, or from facts developed and reported by an investigating committee of the House. It further states that a proposition to impeach is a question of high privilege in the House and at once supersedes business otherwise in order under the rules governing the order of business.\n\nAt the federal level, the impeachment process is a two-step procedure. The House of Representatives must first pass, by a simple majority of those present and voting, articles of impeachment, which constitute the formal allegation or allegations. Upon passage, the defendant has been \"impeached\". Next, the Senate tries the accused. In the case of the impeachment of a president, the Chief Justice of the United States presides over the proceedings. For the impeachment of any other official, the Constitution is silent on who shall preside, suggesting that this role falls to the Senate's usual presiding officer, the President of the Senate who is also the Vice President of the United States.\n\nIn theory at least, as President of the Senate, the Vice President of the United States could preside over their own impeachment, although legal theories suggest that allowing a defendant to be the judge in their own case would be a blatant conflict of interest. If the Vice President did not preside over an impeachment (of anyone besides the President), the duties would fall to the President pro tempore of the Senate.\n\nTo convict an accused, \"the concurrence of two thirds of the members present\" is required. Conviction removes the defendant from office. Following conviction, the Senate may vote to further punish the individual by barring him or her from holding future federal office, elected or appointed. Conviction by the Senate does not bar criminal prosecution. Even after an accused has left office, it is possible to disqualify the person from future office or from certain emoluments of his prior office (such as a pension). If there is no charge for which a two-thirds majority of the senators present vote \"guilty\", the defendant is acquitted and no punishment is imposed.\n\nCongress regards impeachment as a power to be used only in extreme cases; the House of Representatives has initiated impeachment proceedings only 64 times since 1789 (most recently the 2010 impeachment, then removal from office, of Judge Thomas Porteous of the United States District Court for the Eastern District of Louisiana) with only the following 19 of these proceedings actually resulting in the House's passing Articles of Impeachment:\n\nThere have been unsuccessful attempts to initiate impeachment proceedings against Richard Nixon, George W. Bush, Barack Obama, and Donald Trump.\n\nThree state governors have been impeached and removed from office:\n\n\n"}
{"id": "15334", "url": "https://en.wikipedia.org/wiki?curid=15334", "title": "Ibizan Hound", "text": "Ibizan Hound\n\nThe Ibizan Hound (, ) is a lean, agile dog of the hound family. There are two hair types of the breed: smooth and wire. The more commonly seen type is the smooth. Some consider there to be a third type, long, but the longhair is most likely a variation of the wire.\n\nThe Ibizan Hound is an elegant and agile breed, with an athletic and attractive outline and a ground-covering springy trot. Though graceful in appearance, it has good bone girth and is a rugged/hardy breed. Its large upright ears - a hallmark of the breed - are broad at the base and frame a long and elegant headpiece. The neck is long and lean. It has a unique front assembly with well laid-back shoulders and relatively straight upper arm. Coming in both smooth and wire-coated varieties, their coat is a combination of red and white with the nose, ears, eye rims, and pads of feet being a light tan color. Its eyes are a striking amber color and have an alert and intelligent expression. The Ibizan may range in height, depending on which Standard you follow, from and weigh from , males being larger than females.\n\nIbizan Hounds are intelligent, active, and engaging by nature. They rank 53rd in Stanley Coren's The Intelligence of Dogs, being of average working/obedience intelligence, but many Ibizan owners will enjoy recounting a multitude of examples of their problem-solving abilities. They are true \"clowns\" of the dog world, delighting in entertaining their people with their antics. Though somewhat independent and stubborn at times, they do take well to training if positive methods are used, but will balk at punitive training methods. They are generally quiet, but will alarm bark if necessary, so they make good watch dogs. They are sensitive hounds, and very good around children and other dogs alike. They generally make good house dogs, but are active and athletic, therefore need a lot of daily exercise. They do not make good kennel dogs. Ibizan hounds are sweet, but they are very stubborn and independent.\n\nIbizan Hounds are \"escapologists\": they are able to jump incredible heights from a standstill, so they need very tall fences. They also have been known to climb, and many can escape from crates, open baby gates and even locks. They have a strong prey drive, therefore they cannot be trusted off leash unless in a safely enclosed area. Once off the leash, they might not come back for a long time. A hound that knows where its home is and the surrounding area will usually return unscathed.\n\nThe Ibizan Hound is typical of the Hound Group in that it rarely suffers from hereditary illness. Minor health concerns for the breed include seizures and allergies; very rarely, one will see axonal dystrophy, cataract, retinal dysplasia and deafness in the breed. Ibizan Hound owners should have their dogs' eyes tested by a veterinarian before breeding. CERF and BAER testing is recommended for the breed. Ibizan Hounds are sensitive to barbiturate anesthesia, and typically live between 12 and 14 years.\n\nThis breed originates in the island of Eivissa and has been traditionally used in the Catalan-speaking areas of Spain, and France where it was known under the name of \"le charnigue\", to hunt rabbits and other small game. The Ibizan Hound is a fast dog that can hunt on all types of terrain, working by scent, sound and sight. Hunters run these dogs in mostly female packs, with perhaps a male or two, as the female is considered the better hunter.\n\nTraditionally a farmer may have one dog and a very well off farmer two dogs to catch rabbits for food. However in the last twenty years it is seen as a sport where between five and fifteen dogs can be seen in the chase of one rabbit.\n\nThe Ibizan Hound authority Miquel Rosselló has provided a detailed description of a working trial which characterises their typical hunting technique and action, strikingly illustrated with action photos by Charles Camberoque which demonstrate hunt behaviour and typical hunt terrain. \nWhile local hunters will at times use one dog or a brace, and frequently packs of six to eight or as many as fifteen, the working trial requires an evaluation of one or two braces. A brace is called a \"colla\". The couples should be tested on at least two to five rabbits (not hares), without the use of any other hunting aid. An inspection and evaluation of the exterior, fitness, character and obedience of the dogs is recommended prior to the hunt. \nThe trial is qualified as having 5 parts. The dogs should show: (1) careful tracking and scenting of the rabbit, without being distracted in the least, 0-30 points; (2) correct signalling of the game, patient stand, strong jump into the air, obedience 0-10 points; (3) chase, giving tongue, speed, sureness, anticipation 0-30 points; (4) putting the game to cover at close quarters, listening, waiting, obedience, correct attack 0-10 point; and (5) good catch, or correct indication of the game’s location, retrieval, obedience 0-20 points.\n\nIndividual dogs are expected to show a great degree of discipline, obedience and co-operation. They should be extremely agile, have good speed and a powerful vertical jump from a stationary position in rough and often heavily covered ground. They should have excellent scent-tracking abilities, give tongue at the right time when approaching the game closely, and otherwise be silent so that they can locate the game by sound.\n\nThe Ibizan Hound is similar in function and type to several breeds, such as the Pharaoh Hound, the Cirneco dell'Etna, the Portuguese Podengo, and the Podenco Canario. The Ibizan Hound is the largest of these breeds, classified by the Fédération Cynologique Internationale as primitive types.\n\nIt is believed the Ibizan Hound evolves from the \"tesem\", the ancient Egyptian hunting dog. Representations of this dog on the walls of ancient tombs show a striking similarity to the modern Ibizan Hound. These dogs would have been brought to the island of Eivissa by the Phoenicians, who founded settlements there as early as the 8th century BC. A recent DNA analysis found that the breed was formed recently from other breeds. \nA more recent article \nargues that continued trait selective breeding may be behind this lack of support.\n\nIn the United States, the Ibizan Hound is frequently competed in lure coursing through the AKC and ASFA, and also competes in LGRA straight racing and NOTRA oval track racing. Some parts of the country also use them for coursing live prey, generally jackrabbits.\n\nThe Ibizan Hound breed is recognized by the Fédération Cynologique Internationale, Continental Kennel Club, American Kennel Club, United Kennel Club, Kennel Club of Great Britain, Canadian Kennel Club, National Kennel Club, New Zealand Kennel Club, Australian National Kennel Council, America's Pet Registry, and American Canine Registry. It was fully recognized by the American Kennel Club in 1979.\n\nAccording to journalist Norman Lewis, when an owner no longer wants to own one of these dogs (having too much of an appetite, for instance), it is considered very bad luck to kill the dog. Instead, they release the dog on the other side of the island, so that someone else might 'adopt' the animal.\n\n"}
{"id": "15335", "url": "https://en.wikipedia.org/wiki?curid=15335", "title": "Irish wolfhound", "text": "Irish wolfhound\n\nThe Irish Wolfhound (, ) is a breed of domestic dog (\"Canis lupus familiaris\"), specifically a very large sighthound from Ireland. The name originates from its purposewolf hunting with dogsrather than from its appearance. Originally developed from war hounds with the purpose of being a hunting or guard dog. The Irish Wolfhound can be an imposing sight due to their formidable size.\n\nThe breed is very old; there are suggestions it may have been brought to Ireland as early as 7000 BC. These dogs are mentioned, as cú (variously translated as hound, Irish hound, war dog, wolf dog, etc.) in Irish laws and in Irish literature which dates from the 5th century or, in the case of the Sagas, from the old Irish period - AD 600-900. The word \"Cú\" often became an added prefix of respect on the names of warriors as well as kings denoting that they were worthy of the respect and loyalty of a Cú.\n\nAncient artworks and writings have encouraged modern authors to imagine their existence as a breed by 273 BC. However, there is indication that large dogs may have existed even as early as 279 BC when the Tectosages and Tolistobogii Celts sacked Delphi. Survivors left accounts of the fierce Celts and the huge dogs who fought with them and at their side. They were mentioned by Julius Caesar in his treatise, The Gallic Wars, and by 391 AD, they were written about by Roman Consul, Quintus Aurelius Symmachus, who received seven of them, \"canes Scotici\", as a gift to be used for fighting lions and bears, in his words, \"all Rome viewed (them) with wonder\".\n\nWolfhounds were bred as hunting dogs by the ancients, who called them \"Cú Faoil\". The Irish continued to breed them for this purpose, as well as to guard their homes and protect their stock. Cúchulain, a name which translates literally as \"hound of Culain\", gained his name when as a child, known then as Sétanta, when he slew the ferocious guard dog of Culain, forcing him to offer himself as a replacement.\n\nDuring the English Conquest of Ireland, only the nobility were allowed to own Irish Wolfhounds, the numbers permitted depending on position. They were much coveted and were frequently given as gifts to important personages and foreign nobles. Wolfhounds were the companions of the regal, and were housed themselves alongside them. King John of England, in about 1210 presented an Irish hound, Gelert, to Llewellyn, a prince of Wales. The poet The Hon William Robert Spencer Immortalized this hound in a poem.\n\nIn his \"History of Ireland\" completed 1571, Edmund Champion gives a description of the hounds used for hunting the wolves on the Dublin and Wicklow mountains. He says: They (the Irish) are not without wolves and greyhounds to hunt them, bigger of bone and limb than a colt. Due to their popularity overseas many were exported to European royal houses leaving numbers in Ireland depleted. This led to a declaration by Oliver Cromwell himself being published in Kilkenny on 27 April 1652 to ensure that sufficient numbers remained to control the wolf population.\n\nReferences to the Irish Wolfhound in the 18th century tell of its great size, strength and greyhound shape as well as its scarcity. Writing in 1790, Bewick described it as the largest and most beautiful of the dog kind; about 36 inches high, generally of a white or cinnamon colour, somewhat like the Greyhound but more robust. He said that their aspect was mild, disposition peaceful, and strength so great that in combat the Mastiff or Bulldog was far from being an equal to them. The last wolf in Ireland is thought to have been killed at Myshall, on the slopes of Mount Leinster in Co. Carlow in 1786 by a pack of wolfdogs kept by a Mr Watson of Ballydarton. The remaining hounds in the hands of a few families who were mainly descendants of the old Irish chieftains, were now symbols of status rather than hunters, they were said to be the last of their race.\n\nScotsman Captain George Augustus Graham is responsible with a few other breeders for attempting to reaffirm the breed's existence. In 1879 he wrote: \"It has been ascertained beyond all question that there are few specimens of the breed still left in Ireland and England to be considered Irish Wolfhounds, though falling short of the requisite dimensions. This blood is now in my possession.\" Captain Graham devoted his life to ensuring the survival of the Irish Wolfhound. Owing to the small numbers of surviving specimens outcrossing was used in the breeding programme. It is believed that Borzoi, Great Dane, Scottish Deerhound and English Mastiff dogs all played their part in Graham's creation of the dog we currently know. The famous English Mastiff Garnier's Lion was bred to the Deerhound Lufra, and their offspring Marquis enters Wolfhound pedigrees through his granddaughter Young Donagh. Graham included \"a single outcross of Tibetan Wolf Dog\". This was long assumed to have been a Tibetan Mastiff. However, a photograph of \"Wolf\" shows a bearded, long-coated dog—what would now be called a \"Tibetan Kyi Apso\" or \"dokhyi apso\". In 1885 Captain Graham with other breeders founded the Irish Wolfhound Club, and the Breed Standard of Points to establish and agree the ideal to which breeders should aspire.\n\nThe Wolfhound was historically a dog that only nobles could own and was taken up by the British during their rule in Ireland. This made it unpopular as a national symbol and the Kerry Blue Terrier was adopted by Republicans such as Michael Collins.The Wolfhound has been adopted as a symbol by both rugby codes. The national rugby league team is nicknamed the Wolfhounds, and the Irish Rugby Football Union, which governs rugby union, changed the name of the country's A (second-level) national team in that code to the Ireland Wolfhounds in 2010.\n\nConsidered by the American Kennel Club to be the tallest of all dog breeds, describing the breed as, \"Of great size and commanding appearance, the Irish Wolfhound is remarkable in combining power and swiftness with keen sight. The largest and tallest of the galloping hounds, in general type he is a rough-coated, Greyhound-like breed; very muscular, strong though gracefully built; movements easy and active; head and neck carried high, the tail carried with an upward sweep with a slight curve towards the extremity\". The average height of an Irish Wolfhound should be taller than that of a Great Dane. However, the wolfhound is not to be confused with being the heaviest, as its structure should be similar to that of a Greyhound, with a very broad and deep chest that tucks up.\n\nIts colour may be grey, brindle, red, black, white, fawn, and wheaten.\n\nThe Irish Wolfhound was bred for long solitary hunts based solely on the dog's ability to visualize its landscape and perceive, unlike scent hounds (such as Bloodhounds and Beagles) who rely on scent rather than sight. For this reason, the neck of an Irish Wolfhound should be long with the head held high the majority of the time. The Irish Wolfhound should also appear to be longer than it is tall.\nOnce used to hunt wolves, an Irish Wolfhound’s structure should appear as if it is “fast enough to catch a wolf, and strong enough to kill it”.\n\nThe AKC specifies the minimum height as for mature males, for females; the minimum weight: for males, for females. It is not rare to see modern day female hounds reaching the minimal height requirements of those of male hounds; most females are well over and in most AKC conformation shows a wolfhound’s height is looked at with as much importance as the hound’s head and face structure. Per the AKC, great size, including height of shoulder and proportionate length of body is to be aimed at, to firmly establish a breed averaging in males. The height/weight standards in Ireland and England are slightly different: males /, females /.\n\nIrish Wolfhounds have a varied range of personalities and are most often noted for their personal quirks and individualism. An Irish Wolfhound, however, is rarely mindless, and despite its large size is rarely found to be destructive in the house or boisterous. This is because the breed is generally introverted, intelligent, and reserved in character. An easygoing animal, the Irish Wolfhound is quiet by nature. Wolfhounds often create a strong bond with their family and can become quite destructive or morose if left alone for long periods of time. An Irish Wolfhound is not a guard dog and will protect individuals rather than the house or the owner’s possessions. However independent the wolfhound is, the breed becomes attached to both owners and other dogs they are raised with and is therefore not the most adaptable of breeds. Bred for independence, an Irish Wolfhound is not necessarily keen on defending spaces. A wolfhound is most easily described by its historical motto, “gentle when stroked, fierce when provoked”. They should not be territorially aggressive to other domestic dogs but are born with specialized skills and it is common for hounds at play to course another dog. This is a specific hunting behavior, not a fighting or territorial domination behavior. Most Wolfhounds are very gentle with children. The Irish Wolfhound is relatively easy to train. They respond well to firm, but gentle, consistent leadership. However, historically these dogs were required to work at great distances from their masters and think independently when hunting rather than waiting for detailed commands and this can still be seen in the breed.\n\nThe Wolfhound of today is far from the one that struck fear into the hearts of the Ancient Romans. Irish Wolfhounds are often favored for their loyalty, affection, patience and devotion. Although at some points in history they have been used as watchdogs, unlike some breeds, the Irish Wolfhound is usually unreliable in this role as they are often friendly toward strangers, although their size can be a natural deterrent. However, when protection is required this dog is never found wanting. When they or their family are in any perceived danger they display a fearless nature. Author and Irish Wolfhound breeder Linda Glover believes the dogs' close affinity with humans makes them acutely aware and sensitive to ill will or malicious intentions leading to their excelling as a guardian rather than guard dog.\n\nLike many large dog breeds, Irish Wolfhounds have a relatively short lifespan. Published lifespan estimations vary between 6 and 10 years with 7 years being the average. Dilated cardiomyopathy and bone cancer are the leading cause of death and like all deep-chested dogs, gastric torsion (bloat) is common; the breed is affected by hereditary intrahepatic portosystemic shunt.\n\nIn a privately funded study conducted under the auspices of the Irish Wolfhound Club of America and based on an owner survey, Irish Wolfhounds in the United States from 1966 to 1986 lived to a mean age of 6.47 and died most frequently of bone cancer. A more recent study by the UK Kennel Club puts the average age of death at 7 years.\n\nStudies have shown that neutering is associated with a higher risk of bone cancer in various breeds, with one study suggesting that castration of male Irish Wolfhounds should be avoided at least until the dog is fully grown.\n\nIrish Wolfhounds should not receive additional supplements when a good dog food is used. It is generally accepted that they should be fed a low protein adult dog food (19 to 21% protein) from puppyhood onward. Most breeders today recommend that they not be supplemented to slow their rapid growth.\n\nIrish Wolfhounds are the tallest of all dog breeds, sometimes reaching 7 feet tall on their hind legs. They are well suited to rural life, but their medium energy profile allows them to adjust fairly well to suburban and urban life as well, provided they receive appropriate exercise.\n\nGenetically, the Irish Wolfhound as a breed is threatened by a bottleneck related to the over-use of a popular sire.\n\n\n\n\n"}
{"id": "15336", "url": "https://en.wikipedia.org/wiki?curid=15336", "title": "Italian Greyhound", "text": "Italian Greyhound\n\nThe Italian Greyhound (in Italian: \"piccolo levriero italiano\") is a small breed of dog; of the sighthound type, sometimes called the Italian for short, and nicknamed the \"IG\" or \"Iggy\".\n\nThe Italian Greyhound is the smallest of the sighthounds, typically weighing about and standing about tall at the shoulder. They are in the toy group in the UK and US but in the sighthound group in the Fédération Cynologique Internationale (FCI).\n\nThe Italian Greyhound's chest is deep, with a tucked up abdomen, long slender legs and a long neck that tapers down to a small head. The head is long and pointed, like a full sized Greyhound. Overall, they look like \"miniature\" Greyhounds. Though many Italian Greyhound owners dispute the use of the term \"miniature greyhound\" in reference to the breed itself, by definition of the American Kennel Club they are true genetic Greyhounds, with a bloodline extending back over 2,000 years. Their current small stature is a function of selective breeding. Their gait is distinctive and should be high stepping and free, rather like that of a horse. They are able to run at top speed with a double suspension gallop, and can achieve a top speed of up to .\n\nRecognised coat colours in the UK are Black, Blue (grey), Red and Fawn. For The Kennel Club (UK), all colours except brindle are accepted. For the American Kennel Club, and the Australian National Kennel Council, parti colored Italian Greyhounds are accepted, while the FCI standard for international shows allows white only on the chest and feet. Coat colours of Black, Blue and Isabelle (Fawn) exist in all nuances.\n\nThe modern Italian Greyhound's appearance is a result of breeders throughout Europe, particularly Austrian, German, Italian, French and British, making great contributions to the forming of this breed. The Italian Greyhound should resemble a small Greyhound, or rather a Sloughi.\n\nThe Italian Greyhound makes a good companion dog and enjoys the company of people. However, the breed's slim build and short coat make them somewhat fragile, and injury can result from rough or careless play with children. They are fast, agile and athletic, and love to run. Due to their size, and in some lineages poor bone density, they are prone to broken legs. Italian Greyhounds make reasonably good watchdogs, as they bark at unfamiliar sounds. They may also bark at passers-by and other animals. However, they should not be considered \"true\" guard dogs as they are often aloof with strangers and easily spooked to run.\n\nAs sighthounds, Italian Greyhounds instinctively hunt by sight and have an extremely high predator drive. As with most sighthounds, martingale collars are often used with Italian greyhounds since their necks are nearly the same width as their skulls. Some Italian Greyhounds take part in dog agility. The breed's lithe body and its love of action provide potential to do well at this sport, although their natural inclination is for straight-out racing rather than for working tightly as a team with a handler on a technical course. Lure coursing is well-suited to the Italian Greyhound. They are fast dogs, and while they are not as well suited to racing as their larger cousins, many Italian Greyhounds participate in amateur straight-track and oval-track racing.\n\nDogs of this breed have an extremely short and almost odorless coat that requires little more than an occasional bath about once a month (though many veterinarians suggest that even bathing once per month is too frequent for this breed), but a wipe-down with a damp cloth is recommended after walks as seeds, burrs and floating dust in the air can get into the coat and irritate the skin. This breed sheds medium to little hair.\n\nThe Italian Greyhound has a median lifespan of 13.5 in a 2004 UK Kennel Club survey. A 1993 US breed club survey gives an average lifespan of 9 years but more than a quarter of the dogs had \"accidents\" recorded as cause of death. \n\nHealth problems that can be found in the breed:\nTheir scissor-bite and thin jaw bones make them susceptible to periodontal disease, which can be avoided with good dental care. Daily brushing has been shown to be very beneficial as well as regular dental cleanings from the vet.\n\nResponsible breeders will routinely check their dogs for the onset of various inherited disorders, these commonly include (but are not limited to): CERF examinations on eyes, OFA patellar examinations, OFA thyroid function panels, von Willebrand's factor, OFA hip and Legg-Perthes disease x-rays, and others. In research by the Ortheopedic Foundation for Animals, the Italian Greyhound was found to be the least affected by hip dysplasia out of 157 breeds. Tests were conducted on 169 individual Italian Greyhounds, of which none were found to have hip dysplasia and 59.2% scored excellent on their hip evaluations.\n\nThe name of the breed is a reference to the breed's popularity in Renaissance Italy. Mummified dogs very similar to the Italian Greyhound (or small Greyhounds) have been found in Egypt, and pictorials of small Greyhounds have been found in Pompeii, and they were probably the only accepted companion-dog there. Dogs similar to Italian Greyhounds are recorded as having been seen around Emperor Nero's court in Rome in the first century AD.\n\nThe breed is believed to have originated more than 2,000 years ago in the countries now known as Greece and Turkey. This belief is based on the depiction of miniature greyhounds in the early decorative arts of these countries and on the archaeological discovery of small greyhound skeletons. By the Middle Ages, the breed had become distributed throughout Southern Europe and was later a favorite of the Italians of the sixteenth century, among whom miniature dogs were in great demand. Sadly, though, 'designer' breeders tried, and failed, to make the breed even smaller by crossbreeding it with other breeds of dogs. This only led to mutations with deformed skulls, bulging eyes and dental problems. The original Italian Greyhound had almost disappeared when groups of breeders got together and managed to return the breed to normal. From this period onward the history of the breed can be fairly well traced as it spread through Europe, arriving in England in the seventeenth century.\n\nThe grace of the breed has prompted several artists to include the dogs in paintings, among others Velázquez, Pisanello, and Giotto.\n\nThe breed has been popular with royalty. Among the royal aficionados are Mary, Queen of Scots, Queen Anne, Queen Victoria, Catherine the Great, Frederick the Great and Maud, Queen of Norway.\n\n"}
{"id": "15341", "url": "https://en.wikipedia.org/wiki?curid=15341", "title": "Into the Woods", "text": "Into the Woods\n\nInto the Woods is a musical with music and lyrics by Stephen Sondheim and book by James Lapine. The musical intertwines the plots of several Brothers Grimm and Charles Perrault fairy tales, exploring the consequences of the characters' wishes and quests. The main characters are taken from \"Little Red Riding Hood\", \"Jack and the Beanstalk\", \"Rapunzel\", and \"Cinderella\", as well as several others. The musical is tied together by a story involving a childless baker and his wife and their quest to begin a family (the original beginning of The Grimm Brothers' \"Rapunzel\"), their interaction with a witch who has placed a curse on them, and their interaction with other storybook characters during their journey.\n\nThe musical debuted in San Diego at the Old Globe Theatre in 1986 and premiered on Broadway on November 5, 1987, where it won several Tony Awards, including Best Score, Best Book, and Best Actress in a Musical (Joanna Gleason), in a year dominated by \"The Phantom of the Opera\" (1988). The musical has since been produced many times, with a 1988 US national tour, a 1990 West End production, a 1997 tenth anniversary concert, a 2002 Broadway revival, a 2010 London revival, and in 2012 as part of New York City's outdoor Shakespeare in the Park series.\n\nA Disney film adaptation directed by Rob Marshall and starring Meryl Streep, Emily Blunt, James Corden, Anna Kendrick, Chris Pine, Tracey Ullman, Christine Baranski and Johnny Depp was released in 2014. The film grossed over $213 million worldwide, and received three Academy Award nominations and three Golden Globe Award nominations.\n\nThe Narrator introduces four characters who each have a wish: Cinderella wishes to attend the King's festival; Jack, a simple poor boy, wishes that his cow, Milky White, would give milk; a Baker and his Wife wish they could have a child; Little Red Ridinghood wishes for bread from the Baker to take to her grandmother's house.\n\nJack's weary mother nags him into selling the cow, and Cinderella's stepmother and stepsisters, Florinda and Lucinda, tease Cinderella about wanting to attend the King's festival.\n\nThe Baker's neighbor, an ugly old witch, reveals that the source of the couple's infertility is a curse she placed on the Baker's line after catching his father stealing her vegetables, including six magic beans. The Witch also took the Baker's father's newborn child Rapunzel. She explains the curse will be lifted if the Baker and his Wife can find the four ingredients that the Witch needs for a certain potion; \"the cow as white as milk, the cape as red as blood, the hair as yellow as corn, and the slipper as pure as gold,\" all before the chime of midnight in three days' time. All begin their journeys into the woods—Jack goes to market to sell his beloved Milky White, Cinderella's family rides to the Festival while Cinderella goes to her mother's grave to ask for guidance, Little Red goes to her grandmother's house, and the Baker, refusing his wife's help, goes to find the ingredients (\"Prologue\").\n\nCinderella visits her mother's grave and receives a beautiful gown and golden slippers from her mother's spirit (\"Cinderella at the Grave\"). Jack encounters a Mysterious Man who mocks him for trying to sell his cow for more than a \"sack of beans\" and then vanishes. Little Red Ridinghood meets a hungry Wolf who convinces her to take a detour on her way to Granny's (\"Hello, Little Girl\"). The Baker and his Wife squabble over her presence in the woods, but come across Jack with Milky White. Not having the money necessary to buy the cow, they convince Jack that the beans the Baker has found in his father's old hunting jacket are magic beans and buy the cow for five of them. Jack bids a tearful goodbye to his cow (\"I Guess This Is Goodbye\"), and the Baker orders his wife to return to the village with the cow. He has qualms about being so dishonest, but his wife reasons that the chance to have a child justifies their trickery (\"Maybe They're Magic\").\n\nThe Witch has raised Rapunzel as her own daughter, keeping her locked away from the world in a tall tower accessible only by climbing Rapunzel's long, golden hair (\"Our Little World\"). But this day a handsome prince spies the beautiful Rapunzel and resolves to climb the tower himself. In another part of the wood, the Baker has tracked down Little Red Ridinghood. Following the Witch's advice, he attempts to simply steal the red cape, but her ensuing temper tantrum guilts him into returning it. When Little Red Ridinghood arrives at her grandmother's house, she is swallowed by the Wolf. The Baker, in pursuit of the cape, slays the Wolf, pulling Little Red Ridinghood and her grandmother from the beast's innards. Little Red Ridinghood rewards him with the red cape, reflecting on her new experiences (\"I Know Things Now\"). Meanwhile, Jack's mother angrily tosses the beans aside, which grow into an enormous stalk overnight, and sends her son to bed without supper. As Cinderella flees the Festival, pursued by another handsome prince and his steward, the Wife helps her hide and quizzes Cinderella about the ball. Cinderella explains that it was a nice ball (\"A Very Nice Prince\") but seems nonplussed by the experience. As a giant beanstalk begins to sprout from the ground next to Jack's cottage, the Baker's Wife spots Cinderella's pure gold slippers. She tries to chase after Cinderella but inadvertently allows Milky White to run off, leaving the Baker's Wife without slippers or the cow. The characters each state morals and credos as the first midnight chimes (\"First Midnight\") and they continue their journeys through the woods.\n\nThe next morning, Jack describes his adventure climbing the beanstalk (\"Giants in the Sky\"). He gives the Baker five gold pieces he stole from the giants to buy back his cow. When the Baker hesitates, Jack climbs back up the beanstalk to find more. The Mysterious Man emerges and taunts the Baker, stealing the money. The Baker's Wife confesses she has lost the cow, and she and the Baker split up to look for it. Cinderella's Prince and Rapunzel's Prince, who are brothers, meet and compare the their newfound and unobtainable amours (\"Agony\"). The Baker's Wife, who is eavesdropping, takes note when Rapunzel's prince mentions that he is in love with a girl in a tower with hair \"as yellow as corn.\" The Baker's Wife fools Rapunzel into letting down her hair by telling her that she is her prince and pulls out a piece of it. Meanwhile, The Mysterious Man returns Milky White to the Baker.\n\nThe Baker's Wife and Cinderella meet again, and the Baker's Wife makes a desperate grab for her shoes, almost succeeding before Cinderella flees. The Baker and his wife reunite, now with three of the four items. The Baker admits that they will have to work together to fulfill the quest (\"It Takes Two\"). Jack arrives with a hen that lays golden eggs and attempts to buy Milky White back, but the cow suddenly keels over dead as midnight chimes. Again, the characters recite morals (\"Second Midnight\"). The Witch discovers that the Prince has been visiting Rapunzel and, in fury and anguish, demands that Rapunzel stay with her so she can protect her from the outside world (\"Stay with Me\"). When Rapunzel refuses, the Witch cuts off Rapunzel's hair and banishes her to a desert. The Mysterious Man gives the Baker the money to buy another cow. Jack encounters Little Red Ridinghood, who is now sporting a wolf skin cape and a large knife for protection. She goads him into returning once again to the Giant's home to steal a magic harp.\n\nCinderella, returning from the last night of the festival, describes how the Prince spread pitch on the stairs to prevent her from escaping. Caught between wanting to escape and wanting to stay, she eventually resolves to let the Prince decide, leaving him one of her slippers as a clue to her identity (\"On the Steps of the Palace\"). The Baker's Wife frantically tries to convince her to give up her other shoe, offering her the sixth magic bean in exchange for it. Cinderella throws the bean aside, but trades shoes with the Baker's Wife and flees. The Baker arrives with another cow; they now have all four items. The Prince's Steward grabs the slipper from the Baker's Wife, and they are fighting over it when a great crash is heard and Jack's mother runs in to report that there is a dead Giant in her backyard. The Prince, more concerned with finding Cinderella, waves her off and departs with one of the slippers, giving the other to the Baker and his wife. Jack, to his mother's relief, returns with the magic harp. The Witch discovers that the new cow is unsatisfactory (as it's a regular cow which has been is covered with flour). However, the Witch is able to resurrect Milky White and instructs the Baker and his Wife to feed the other ingredients to her. Jack tries to milk her, but no milk comes. The Baker's Wife reveals where she got the yellow hair, and the Witch furiously explains that she cannot have touched any of the ingredients. The Mysterious Man tells the Baker to feed the hair-like corn silk to the cow. Now Milky White gives milk which is the potion. The Witch reveals that the Mysterious Man is the Baker's father. The Witch drinks the potion. At the chime of the third midnight the Mysterious Man falls dead, his reparation complete, the curse is broken, and the Witch regains the youth and beauty she had before the curse.\n\nCinderella's Prince searches for the girl whose foot fits the slipper; the stepsisters try but can only get it on by cutting off parts of their feet (\"Careful My Toe\"). Cinderella appears, her foot fits the slipper, and she becomes the Prince's bride. Rapunzel bears twins in the desert where her Prince finds her. The Witch attempts to curse the couple, but with the curse broken, her powers are gone. At Cinderella's wedding to the Prince, Florinda and Lucinda are blinded by birds as they try to win Cinderella's favor. The Baker's Wife, very pregnant, thanks Cinderella for the slipper. Everyone is at the wedding. Everyone but the Witch and the stepsisters congratulate themselves on being able to live happily \"Ever After,\" though they fail to notice another beanstalk growing sky-high...\n\nThe Narrator introduces the action again: \"Once Upon a Time...Later.\" All the characters seem happy but are still wishing: The Baker and his Wife have their precious baby boy, but wish for more room and bicker over the Baker's unwillingness to hold his child; Jack and his mother are rich and well-fed, but Jack misses his kingdom in the sky; Cinderella is living with her Prince Charming in the Palace, but is getting bored. (\"So Happy\").\n\nEveryone is suddenly knocked over by a loud crash. The enormous foot of a Giant has destroyed the Witch's garden, sparing only a few beans. The Baker and his Wife decide that they must tell the Royal Family, and the Baker travels to the palace. His news is ignored by the Prince's Steward, and also by Jack's Mother when he stops at her house to ask for Jack's aid. When he returns home, Little Red Ridinghood arrives on her way to Granny's: her house has been destroyed and her mother is missing. The Baker and his Wife decide to escort her. Meanwhile, Jack decides that he must slay the Giant and Cinderella learns from her bird friends that her mother's grave was disturbed and decides to investigate, dressed in her old rags. Once again, everyone heads into the woods, but this time the mood is somber, for \"the skies are strange, the winds are strong\" (\"Into the Woods\" Reprise).\n\nRapunzel has also fled to the woods in a hysterical fit, driven mad by her treatment at the Witch's hands. Her Prince has followed her, but when he encounters his brother they each confess they have another reason for their presence in the woods. They have grown bored and frustrated with their marriages and now lust after two beautiful women asleep in the woods - Snow White and Sleeping Beauty (\"Agony\" Reprise).\n\nThe Baker, his Wife, and Little Red Ridinghood get lost in the woods and find Cinderella's family and the Steward, who reveal that the castle was set upon by the Giant. The Witch arrives as well, bringing news that the Giant has destroyed the village and the Baker's house. Suddenly, thunderous footsteps are heard and the Giant appears. To the shock of all, this Giant is a woman–widow of the Giant that Jack killed. Her booming voice proclaims that she wants Jack's blood in revenge. To satisfy the Giantess, the group realizes they must give her someone, but are unable to decide on whom until they realize that the Narrator is still commenting on the actions from the sidelines. Everyone offers her the narrator as a sacrifice, but he convinces them how lost they would be without him. Nevertheless, the Witch throws him into the Giantess's arms and he is killed by being dropped. Jack's mother finds the group and aggressively defends her son, angering the Giantess, and the Steward clubs Jack's mother to quiet her, inadvertently killing her. As the Giantess leaves to search for Jack, Rapunzel runs into her path and is trampled, to the horror of the Witch (\"Witch's Lament\").\n\nThe Royal Family continue on their way, fleeing despite the Baker's pleas for them to stay and fight the Giant. The Witch declares she will find Jack and sacrifice him to the Giant, and the Baker and his Wife decide they must find him first and split up to search. The Baker's Wife meets Cinderella's Prince, and he rapidly seduces her (\"Any Moment\"). Meanwhile, the Baker discovers Cinderella at her mother's ruined grave and convinces her to join their group for safety. The Prince, satisfied, leaves the Baker's Wife with a few platitudes, and she reflects on her adventure of the woods and her return to domestic life with family (\"Moments in the Woods\"). However, she has lost her way, stumbles into the path of the Giant, and is consequently killed by a falling tree.\n\nThe Baker, Little Red, and Cinderella await the return of the Baker's Wife when the Witch drags in Jack, whom she found weeping over the Baker's Wife's body. The grief-stricken Baker unwittingly agrees to give Jack to the Giantess, causing an argument. The characters first blame each other for their predicament, until finally they all decide to blame the Witch for growing the beans in the first place (\"Your Fault\"). Disgusted and fed up, the Witch curses and scolds them for their inability to accept responsibility, throwing away the rest of her magic beans. This brings her mother's curse on her again, and she vanishes. (\"Last Midnight\").\n\nThe Baker flees, but is visited by his father's spirit who convinces him to face his responsibilities (\"No More\"). The Baker returns and helps plan killing the Giantess, using Cinderella's bird friends to peck out the Giant's eyes at an area smeared with pitch, where Jack and the Baker can finally deliver a fatal blow. Cinderella stays behind to protect the Baker's child and when her Prince passes by, he nearly fails to recognize her. She confronts him, having learned of his infidelity from her birds and he explains his feelings of unfulfillment and his reasons for seducing another woman. She asks him to go, and he sorrowfully leaves.\n\nLittle Red returns with the news that her grandmother has been killed by the Giantess. Meanwhile, the Baker tells Jack that his mother is dead. Jack vows to kill the steward until the Baker convinces him that will not benefit anyone. Cinderella comforts Little Red and tries to address her qualms–does killing the Giant make them no better than she is? The Baker and Cinderella explain to Jack and Little Red that everyone is connected, and choices have consequences (\"No One Is Alone\").\n\nThe four remaining characters slay the Giant and the deceased characters now including the Royal Family (who have lost their way and starved to death in the woods) and the Princes (who have their new paramours, Snow White and Sleeping Beauty, on their arms) return to share one last set of morals with the audience. The survivors resolve to band together and rebuild. The spirit of the Baker's Wife appears to comfort her mourning husband, advising him to tell their child their story. The Baker begins to tell the child the story of the play, while the Witch appears with the final moral: \"Careful the things you say, Children Will Listen.\" All join in on a last reprise of the title song, surmising that we all must venture into the woods while remembering the choices we've made and learning from each endeavor we come across (\"Finale\"). As the characters conclude the song singing, \"Into the woods, and out of the woods and happily ever after\", Cinderella closes the show with one last \"I wish...\"\n\n\"Into the Woods\" premiered at the Old Globe Theatre in San Diego, California, on December 4, 1986 and ran for 50 performances under the direction of James Lapine. Many of the performers from that production appeared in the Broadway cast but John Cunningham, who played the Narrator, Wolf and Steward, and George Coe, as the Mysterious Man and Cinderella's Father, were replaced by Tom Aldredge among others. Kenneth Marshall as Cinderella's Prince was replaced by Robert Westenberg (who also played the Wolf), LuAnne Ponce, who played Little Red Ridinghood, was replaced by Danielle Ferland, Ellen Foley, the Witch, was replaced by Bernadette Peters. Kay McClelland, who played both Rapunzel and the Stepsister Florinda, stayed with the cast but only played Florinda, Rapunzel being played by Pamela Winslow.\n\nThe show underwent much evolution, but the most notable change was the addition of the song \"No One Is Alone\" in the middle of the run.\n\n\"Into The Woods\" opened on Broadway at the Martin Beck Theatre on November 5, 1987, and closed on September 3, 1989 after 765 performances. It starred Bernadette Peters, Joanna Gleason, Chip Zien, Kim Crosby, Ben Wright, Danielle Ferland, Chuck Wagner, Merle Louise, Tom Aldredge, and Robert Westenberg. The musical was directed by James Lapine, with musical staging by Lar Lubovitch, settings by Tony Straiges, lighting by Richard Nelson, costumes by Ann Hould-Ward (based on original concepts by Patricia Zipprodt and Ann Hould-Ward), and makeup by Jeff Raum. The original production won the 1988 New York Drama Critics' Circle Award and the Drama Desk Award for Best Musical, and the original cast recording won a Grammy Award. The show was nominated for ten Tony Awards, and won three: Best Score (Stephen Sondheim), Best Book (James Lapine) and Best Actress in a Musical (Joanna Gleason).\n\nPeters left the show after almost five months due to a prior commitment to film the movie \"Slaves of New York\". The Witch was then played by: Betsy Joslyn (from March 30, 1988); Phylicia Rashad (from April 14, 1988); Betsy Joslyn (from July 5, 1988); Nancy Dussault (from December 13, 1988); and Ellen Foley (from August 1, 1989 until the closing).\n\nOther cast replacements included Dick Cavett as the Narrator (as of July 19, 1988) (for a temporary engagement after which Tom Aldredge returned), Edmund Lyndeck as the Mysterious Man, Patricia Ben Peterson as Cinderella, LuAnne Ponce returning to the role of Little Red Ridinghood, Jeff Blumenkrantz as Jack, Marin Mazzie as Rapunzel (as of March 7, 1989) and Kay McClelland, Lauren Mitchell, Cynthia Sikes and Mary Gordon Murray as the Baker's Wife.\n\nIn 1989, from May 23 to May 25 the full original cast (with the exception of Cindy Robinson as Snow White instead of Jean Kelly) reunited for three performances to tape the musical in its entirety for the Season 10 premiere episode of PBS’s \"American Playhouse\", which first aired on March 15, 1991. The show was filmed professionally with seven cameras on the set of the Martin Beck Theater in front of an audience with certain elements changed from its standard production only slightly for the recording in order to better fit the screen rather than the stage such as the lighting, minor costume differences, and others. There were also pick up shots not filmed in front of an audience for various purposes. This video has since been released on Tape and DVD and on occasion, remastered and re-released.\n\nTenth Anniversary benefit performances were held on November 9, 1997 at The Broadway Theatre (New York), with most of the original cast. Original cast understudies Chuck Wagner and Jeff Blumenkrantz played Cinderella's Prince/Wolf and The Steward in place of Robert Westenberg and Philip Hoffmann, while Jonathan Dokuchitz (who joined the Broadway production as an understudy in 1989) played Rapunzel's Prince in place of Wagner. This concert featured the duet \"Our Little World,\" written for the first London production of the show.\n\nOn November 9, 2014, most of the original cast reunited for two reunion concerts and discussion in Costa Mesa, California. Mo Rocca hosted the reunion and interviewed Stephen Sondheim and James Lapine as well as each cast member. Appearing were Bernadette Peters, Joanna Gleason, Chip Zien, Danielle Ferland, Ben Wright and real life husband and wife, Robert Westenberg and Kim Crosby. The same group presented this discussion/concert on June 21, 2015 at the Brooklyn Academy of Music, New York City.\n\nA United States tour began on November 22, 1988 with Cleo Laine playing the Witch, replaced by Betsy Joslyn in May 1989. Rex Robbins played the Narrator and Mysterious Man, Charlotte Rae played Jack's Mother, and the Princes were played by Chuck Wagner and Douglas Sills. The set was almost completely reconstructed, and there were certain changes to the script, changing certain story elements. The 10-month tour played cities around the country, such as Fort Lauderdale, Florida, Los Angeles, and Atlanta. The tour ran at the John F. Kennedy Center for the Performing Arts from June 1989 to July 16, 1989, with the reviewer for \"The Washington Post\" writing: \"his lovely score -- poised between melody and dissonance -- is the perfect measure of our tenuous condition. The songs invariably follow the characters' thinking patterns, as they weigh their options and digest their experience. Needless to say, that doesn't make for traditional show-stoppers. But it does make for vivacity of another kind. And Sondheim's lyrics...are brilliant... I think you'll find these cast members alert and engaging.\"\n\nThe original West End production opened on September 25, 1990 at the Phoenix Theatre and closed on February 23, 1991 after 197 performances. It was directed by Richard Jones, and produced by David Mirvish, with choreography by Anthony Van Laast, costumes by Sue Blane and orchestrations by Jonathan Tunick. The cast featured Julia McKenzie as the Witch, Ian Bartholomew as the Baker, Imelda Staunton as the Baker's Wife and Clive Carter as the Wolf/Cinderella's Prince. The show received seven Olivier Award nominations in 1991, winning for Best Actress in a Musical (Staunton) and Best Director of a Musical (Jones).\n\nThe song \"Our Little World\" was added. This song was a duet sung between the Witch and Rapunzel giving further insight into the care the Witch has for her self-proclaimed daughter and the desire Rapunzel has to see the world outside of her tower. The overall feel of the show was a lot darker than that of the original Broadway production. Critic Michael Billington wrote, \"But the evening's triumph belongs also to director Richard Jones, set designer Richard Hudson and costume designer Sue Blane who evoke exactly the right mood of haunted theatricality. Old-fashioned footlights give the faces a sinister glow. The woods themselves are a semi-circular, black-and-silver screen punctuated with nine doors and a crazy clock: they achieve exactly the 'agreeable terror' of Gustave Dore's children's illustrations. And the effects are terrific: doors open to reveal the rotating magnified eyeball or the admonitory finger of the predatory giant.\"\n\nA new intimate production of the show opened (billed as the first London revival) at the Donmar Warehouse on 16 November 1998, closing on 13 February 1999. This revival was directed by John Crowley and designed by his brother, Bob Crowley. The cast included Clare Burt as the Witch, Nick Holder as the Baker, Sophie Thompson as the Baker's Wife, Jenna Russell as Cinderella, Sheridan Smith as Little Red Ridinghood and Frank Middlemass as the Narrator/Mysterious Man. Russell later appeared as the Baker's Wife in the 2010 Regent's Park production. Thompson won the 1999 Olivier Award for Best Actress in a Musical for her performance, while the production itself was nominated for Outstanding Musical Production.\n\nA revival opened at the Ahmanson Theatre in Los Angeles, running from February 1, 2002 to March 24, 2002. This production was directed and choreographed with the same principal cast that later ran on Broadway.\n\nThe 2002 Broadway revival, directed by James Lapine and choreographed by John Carrafa, began previews on April 13, 2002 and opened April 30, 2002 at the Broadhurst Theatre, closing on December 29 after a run of 18 previews and 279 regular performances. It starred Vanessa L. Williams as the Witch, John McMartin as the Narrator, Stephen DeRosa as the Baker, Kerry O'Malley as the Baker's Wife, Gregg Edelman as Cinderella's Prince/Wolf, Christopher Sieber as Rapunzel's Prince/Wolf, Molly Ephraim as Little Red Ridinghood, Adam Wylie as Jack, and Laura Benanti as Cinderella. Judi Dench provided the pre-recorded voice of the Giant.\n\nLapine revised the script slightly for this production, with a cameo appearance of the \"Three Little Pigs\" restored from the earlier San Diego production. Other changes, apart from numerous small dialogue changes, included the addition of the song \"Our Little World,\" a duet for the Witch and Rapunzel written for the first London production, the addition of a second wolf in the song \"Hello Little Girl\" who competes for Little Red's attention with the first Wolf, the portrayal of Jack's cow by a live performer (Chad Kimball) in an intricate costume and new lyrics were written for \"The Last Midnight,\" now sung by the Witch as a menacing lullaby to the Baker's baby.\n\nThis production featured scenic design by Douglas W. Schmidt, costume design by Susan Hilferty, lighting design by Brian MacDevitt, sound design by Dan Moses Schreier and projection design by Elaine J. McCarthy. The revival won the Tony Awards for the Best Revival of a Musical and Best Lighting Design. This Broadway revival wardrobe is on display at the Costume World in South Florida.\n\nA revival at the Royal Opera House's Linbury Studio in Covent Garden had a limited run from June 14 through June 30, 2007 followed by a short stint at The Lowry theatre, Salford Quays, Manchester between 4–7 July. The production mixed Opera singers, Musical Theatre actors as well as Film and television actors; including Anne Reid as Jack's Mother and Gary Waldhorn as the Narrator. The production itself, directed by Will Tuckett, was met with mixed reviews; although there were clear stand out performances.\n\nThe production completely sold out three weeks before opening. As this was an 'opera' production, the show and its performers were overlooked for the 'musical' nominations in the 2008 Olivier Awards. This production featured Suzie Toase (Little Red), Peter Caulfield (Jack), Beverley Klein (Witch), Anna Francolini (Baker's Wife), Clive Rowe (Baker), Nicholas Garrett (Wolf), and Lara Pulver (Lucinda). This was the second Sondheim musical to be staged by the Opera House, following 2003's \"Sweeney Todd\".\n\nThe Olivier Award winning Regent's Park Open Air Theatre production, directed by Timothy Sheader and choreographed by Liam Steel, ran for a six-week limited season from 6 August to 11 September 2010. The cast included Hannah Waddingham as the Witch, Mark Hadfield as the Baker, Jenna Russell as the Baker’s wife, Helen Dallimore as Cinderella, and Judi Dench as the recorded voice of the Giant. Gareth Valentine was the Musical Director. The musical was performed outdoors in a wooded area. Whilst the book remained mostly unchanged, the subtext of the plot was dramatically altered by casting the role of the Narrator as a young school boy lost in the woods following a family argument – a device used to further illustrate the musical’s themes of parenting and adolescence.\n\nThe production opened to wide critical acclaim, much of the press commenting on the effectiveness of the open air setting. The \"Telegraph\" reviewer, for example, wrote: \"It is an inspired idea to stage this show in the magical, sylvan surroundings of Regent's Park, and designer Soutra Gilmour has come up with a marvellously rickety, adventure playground of a set, all ladders, stairs and elevated walkways, with Rapunzel discovered high up in a tree.\" \"The New York Times\" reviewer commented: \"The natural environment makes for something genuinely haunting and mysterious as night falls on the audience...\" Stephen Sondheim attended twice, reportedly extremely pleased with the production. The production also won the Laurence Olivier Award for Best Musical Revival and Michael Xavier, who played Cinderella's Prince and the Wolf, was nominated for the Laurence Olivier Award for Best Performance in a Supporting Role in a Musical.\n\nThe production was recorded in its entirety.\n\nThe Regent's Park Open Air Theatre production transferred to the Public Theater's 2012 summer series of free performances Shakespeare in the Park at the Delacorte Theater in Central Park, New York, with an American cast as well as new designers. Sheader again was the director and Steel served as co-director and choreographer. Performances were originally to run from July 24 (delayed from July 23 due to the weather) to August 25, 2012, but the show was extended till September 1, 2012. The cast included Amy Adams as The Baker's Wife, Donna Murphy as The Witch, Denis O'Hare as The Baker, Chip Zien as the Mysterious Man/Cinderella's Father, Jack Broderick as the young Narrator, Gideon Glick as Jack, Cooper Grodin as Rapunzel’s Prince, Ivan Hernandez as Cinderella’s Prince/Wolf, Tina Johnson as Granny, Josh Lamon as the Steward, Jessie Mueller as Cinderella, Laura Shoop as Cinderella’s Mother, Tess Soltau as Rapunzel, and Glenn Close as the Voice of the Giant. The set was a \"collaboration between original Open Air Theatre designer Soutra Gilmour and...John Lee Beatty, [and] rises over 50 feet in the air, with a series of tree-covered catwalks and pathways.\" The production was dedicated to Nora Ephron, who died earlier in 2012. In February 2012 and in May 2012, reports of a possible Broadway transfer surfaced with the production's principal actors in negotiations to reprise their roles. In January 2013, it was announced that the production will not transfer to Broadway due to scheduling conflicts.\n\nA production played in Sydney from 19 March 1993 to 5 June 1993 at the Drama Theatre, Sydney Opera House. It starred Judi Connelli, Geraldine Turner, Tony Sheldon, Philip Quast, Pippa Grandison, and DJ Foster. A Melbourne Theatre Company played from 17 January 1998 to 21 February 1998 at the Playhouse, Victorian Arts Centre. It starred Rhonda Burchmore, John McTernan, Gina Riley, Lisa McCune, Peter Carroll, Tamsin Carroll and Robert Grubb.\n\nThe first professional Spanish language production, \"Dentro del Bosque\", was produced by University of Puerto Rico Repertory Theatre and premiered in San Juan at Teatro de la Universidad (University Theatre) on March 14, 2013. The cast included Víctor Santiago as Baker, Ana Isabelle as Baker's Wife and Lourdes Robles as the Witch\n\nThe Roundabout Theatre production, directed by Noah Brody and Ben Steinfeld, began performances Off-Broadway on December 19, 2014 and officially opened on January 22, 2015, at the Laura Pels Theatre. Like the original Broadway production 28 years prior, this production had a try-out run at the Old Globe Theatre in San Diego, California from July 12, 2014 – August 17, 2014 with the opening night taking place on July 17. This new version is completely minimalistically reimagined by the Fiasco Theater Company, featuring only ten actors playing multiple parts, and one piano accompanist.\n\nThe DreamCatcher Theatre production opened in January 2015 and played a sold out run at the Adrienne Arsht Center in Miami, Florida. Tituss Burgess starred as The Witch, the first male actor to do so. The cast also included Arielle Jacobs as The Bakers Wife. The musical had a production at The Muny in Forest Park, St. Louis, Missouri running from July 21 through 28 2015. The cast included Heather Headley (Witch), Erin Dilly (Baker's Wife), Rob McClure (Baker), Ken Page (Narrator), Elena Shaddow (Cinderella). The Hart House Theatre production in Toronto, Ontario from January 15, 2016 to January 30, 2016. A production ran at the West Yorkshire Playhouse in Leeds in a collaboration with Opera North from 2 June 2016 to 25 June 2016.\n\nThe Israeli premiere, אל תוך היער (El Toch Ha-ya-ar), opened in Tel Aviv on August 2016 for a limited run produced by The Tramp Productions and Stuff Like That, starring Roi Dolev as The Witch, the second male actor to do so.\n\nThe principal original casts of notable stage productions of \"Into the Woods.\"\n<nowiki>*</nowiki>In the 2016 national tour, the show was minimalistic with only 11 cast members (Including one musician), therefore, some actors were cast in multiple roles.\n\nThe musical has been adapted into a child-friendly version for use by schools and young companies, with the second act completely removed, as well as almost half the material from the first. The show is shortened from the original 2 and a half hours to fit in a 50-minute range, and the music transposed into keys that more easily fit young voices.\n\nA theatrical film adaptation of the musical was produced by Walt Disney Pictures, directed by Rob Marshall, and starring Meryl Streep, Emily Blunt, James Corden, Anna Kendrick, Chris Pine, Tracey Ullman, Christine Baranski, Lilla Crawford, Daniel Huttlestone, MacKenzie Mauzy, Billy Magnussen, and Johnny Depp. The film was released on December 25, 2014. It was a critical and commercial hit, grossing over $213 million worldwide. For her performance as the Witch, Streep was nominated for the Academy Award for Best Supporting Actress. The film also received Academy Award nominations for Best Production Design and Best Costume Design.\n\n\nIn most productions of \"Into the Woods\", including the original Broadway production, several parts are doubled. Cinderella's Prince and the Wolf, who share the characteristic of being unable to control their appetites, are usually played by the same actor. Similarly, the Narrator and the Mysterious Man, who share the characteristic of commenting on the story while avoiding any personal involvement or responsibility. Granny and Cinderella's Mother, who are both matriarchal characters in the story, are also typically played by the same person, who also gives voice to the nurturing but later murderous Giant's Wife.\n\nThe show covers multiple themes: growing up, parents and children, accepting responsibility, morality, and finally, wish fulfillment and its consequences. The \"Time Magazine\" reviewers wrote that the play's \"basic insight... is at heart, most fairy tales are about the loving yet embattled relationship between parents and children. Almost everything that goes wrong—which is to say, almost everything that can—arises from a failure of parental or filial duty, despite the best intentions.\" Stephen Holden wrote that the themes of the show include parent-child relationships and the individual's responsibility to the community. The witch isn't just a scowling old hag, but a key symbol of moral ambivalence. James Lapine said that the most unpleasant person (the Witch) would have the truest things to say and the \"nicer\" people would be less honest. In the Witch's words: \"I'm not good; I'm not nice; I'm just right.\"\n\nGiven the show's debut during the 1980s, the height of the US AIDS crisis, the work has been interpreted to be a parable about AIDS. In this interpretation, the Giant's Wife serves as a metaphor for HIV/AIDS, killing good and bad characters indiscriminately and forcing the survivors to band together to stop the threat and move on from the devastation, reflecting the devastation to many communities during the AIDS crisis. When asked about the thematic connection, Sondheim acknowledged that initial audiences interpreted it as an AIDS metaphor, but stated that the work was not intended to be specific.\n\nThe score is also notable in Sondheim's output, because of its intricate reworking and development of small musical motifs. In particular, the opening words, \"I wish\", are set to the interval of a rising major second and this small unit is both repeated and developed throughout the show, just as Lapine's book explores the consequences of self-interest and \"wishing.\" The dialogue in the show is characterized by the heavy use of syncopated speech. In many instances, the characters' lines are delivered with a fixed beat that follows natural speech rhythms, but is also purposely composed in eighth, sixteenth, and quarter note rhythms as part of a spoken song. Like many Sondheim/Lapine productions, the songs contain thought-process narrative, where characters converse or think aloud.\n\nSondheim drew on parts of his troubled childhood when writing the show. In 1987, he told \"Time Magazine\" that the \"father uncomfortable with babies [was] his father, and [the] mother who regrets having had children [was] his mother.\" \n\n"}
{"id": "15342", "url": "https://en.wikipedia.org/wiki?curid=15342", "title": "Isaac Klein", "text": "Isaac Klein\n\nIsaac Klein (September 5, 1905 – January 23, 1979) was a prominent rabbi and halakhic authority within Conservative Judaism.\n\nKlein was born in the small village of Várpalánka, today part of Mukachevo, in what was then Hungary. He emigrated with his family to the United States in 1921. He earned a BA from City College of New York in 1931. Although nearing ordination at the Yeshiva University's Rabbi Isaac Elchanan Theological Seminary, he transferred to the Jewish Theological Seminary of America (JTSA), where he was ordained in 1934 and received the advanced Jewish legal degree of \"Hattarat Hora’ah\" under the great talmudic scholar Rabbi Professor Louis Ginzberg. He was one of only three people, along with Boaz Cohen and Louis Finkelstein, to ever to receive this degree from JTSA. Klein subsequently earned a PhD from Harvard under the pioneering academic of Judaic studies Harry Wolfson. \n\nHe married the former Henriette Levine in 1932 and had three daughters, Hannah, Miriam, and Rivke. Devoted to his family, he dedicated his major work, \"A Guide to Jewish Religious Practice\" to his children, sons-in-law and 13 grandchildren listing each by name. \n\nKlein served as rabbi at Kadimoh Congregation in Springfield, Massachusetts from 1934–1953; Temple Emanu-El, Buffalo, New York, 1953–1968; Temple Shaarey Zedek, Buffalo, (which was created from the merger of Emanu-El with Temple Beth David in 1968), 1968-1972. A beloved Rabbi, he influenced generations of congregants and visiting students and, together with his wife who was an educator, founded Jewish day schools in both Springfield and Buffalo.\n\nDespite the difficulties facing a congregational Rabbi raising a family, Klein volunteered for the U.S. Army during World War II as a chaplain, motivated by a cause he saw as clearly right with important implications for the Jewish People. He served over 4 years, rising to the rank of Major and was an advisor to the high commissioner of the Occupation government. He also served on special assignments for Jewish soldiers in the U.S. Army in the 1950s, receiving the simulated rank of Brigadier General for these missions. His experiences in the war are described in his book \"The Anguish and the Ecstasy of a Jewish Chaplain\".\n\nKlein was a leader of the right-wing of the Conservative movement. He was president of the Rabbinical Assembly, 1958–1960, and a member of its Committee on Jewish Law and Standards, 1948-1979. He was the author of several books, notably, \"A Guide to Jewish Religious Practice\". One of the outstanding halakhists of the movement, he served as a leading member of the Committee on Jewish Law and Standards from 1948 until his death in 1979.\n\nAs a leading authority on halakha he authored many important teshuvot (responsa), many of which were published in his influential \"Responsa and Halakhic Studies\". From the 1950s to 1970s, he wrote a comprehensive guide to Jewish law that was used to teach halakha at the JTSA. In 1979 he assembled this into \"A Guide to Jewish Religious Practice\", which is used widely by laypeople and rabbis within Conservative Judaism.\n\nThe philosophy upon which \"A Guide to Jewish Religious Practice\" is written is stated in the foreword: \"The premise on which Torah is based is that all aspects of life - leisure no less than business, worship or rites of passage (birth, bar mitzvah, marriage, divorce, death) - are part of the covenant and mandate under which every Jew is to serve God in everything he does. In the eyes of Torah there is, strictly speaking, no such thing as the purely private domain, for even in solitude - be it the privacy of the bath or the unconsciousness of sleep - one has the capacity and the duty to serve God.\" This message, of life seen in consonance with the dictates of Judaism, permeates many pages of the book. Rabbi Louis Finkelstein, scholar of the JTSA, wrote: \"There are those who would think that we have but two alternatives, to reject or to accept the law, but in either case to treat it as a dead letter. Both of these alternatives are repugnant to the whole tradition of Judaism. Jewish law must be preserved but it is subject to interpretation by those who have mastered it, and the interpretation placed upon it by duly authorized masters in every generation must be accepted with as much reverence as those which were given in previous generations.\" \n\nThis understanding of traditional preservation of the law through its continuous interpretation lies at the heart of Klein's extensive study of Jewish law. \n\nKlein's papers are located at the University Archives, State University of New York at Buffalo (see finding aid). The archives include fifteen reels of microfilm. The collection consists of extensive writings by Klein on traditional Jewish practice and law. This includes manuscript material for his books \"Guide to Jewish Religious Practice\" (1979), \"The Ten Commandments in a Changing World \" (1963), \"The Anguish and the Ecstasy of a Jewish Chaplain\" (1974), and his translation of \"The Code of Maimonides (Mishneh Torah): Book 7, The Book of Agriculture\" (1979). The collection also contains speeches, sermons, articles, and remarks from the Conservative Jewish viewpoint on subjects such as Jewish medical ethics, dietary laws, adoption, and marriage and divorce. Meeting minutes, annual reports, bulletins, and sermons relating to Klein's rabbinical vocations in Springfield, Massachusetts and Buffalo, New York are also included. The papers contain photographs, wartime letters, and military records of Klein documenting his service in World War II as a director of Jewish religious affairs in Germany.\n\n"}
{"id": "15343", "url": "https://en.wikipedia.org/wiki?curid=15343", "title": "Intron", "text": "Intron\n\nAn intron is any nucleotide sequence within a gene that is removed by RNA splicing during maturation of the final RNA product. The word \"intron\" is derived from the term \"intragenic region\", i.e. a region inside a gene. The term \"intron\" refers to both the DNA sequence within a gene and the corresponding sequence in RNA transcripts. Sequences that are joined together in the final mature RNA after RNA splicing are exons. Introns are found in the genes of most organisms and many viruses, and can be located in a wide range of genes, including those that generate proteins, ribosomal RNA (rRNA), and transfer RNA (tRNA). When proteins are generated from intron-containing genes, RNA splicing takes place as part of the RNA processing pathway that follows transcription and precedes translation.\n\nIntrons were first discovered in protein-coding genes of adenovirus, and were subsequently identified in genes encoding transfer RNA and ribosomal RNA genes. Introns are now known to occur within a wide variety of genes throughout organisms and viruses within all of the biological kingdoms.\n\nThe fact that genes were split or interrupted by introns was discovered independently in 1977 by Phillip Allen Sharp and Richard J. Roberts, for which they shared the Nobel Prize in Physiology or Medicine in 1993. The term \"intron\" was introduced by American biochemist Walter Gilbert:\n\n\"The notion of the cistron [i.e., gene] ... must be replaced by that of a transcription unit containing regions which will be lost from the mature messenger – which I suggest we call introns (for intragenic regions) – alternating with regions which will be expressed – exons.\" (Gilbert 1978)\n\nThe term \"intron\" also refers to \"intracistron\", i.e., an additional piece of DNA that arises within a cistron.\n\nAlthough introns are sometimes called \"intervening sequences\", the term \"intervening sequence\" can refer to any of several families of internal nucleic acid sequences that are not present in the final gene product, including inteins, untranslated sequences (UTR), and nucleotides removed by RNA editing, in addition to introns.\n\nThe frequency of introns within different genomes is observed to vary widely across the spectrum of biological organisms. For example, introns are extremely common within the nuclear genome of jawed vertebrates (e.g. humans and mice), where protein-coding genes almost always contain multiple introns, while introns are rare within the nuclear genes of some eukaryotic microorganisms, for example baker's/brewer's yeast (\"Saccharomyces cerevisiae\"). In contrast, the mitochondrial genomes of vertebrates are entirely devoid of introns, while those of eukaryotic microorganisms may contain many introns.\n\nA particularly extreme case is the \"Drosophila dhc7\" gene containing a ≥3.6 megabase (Mb) intron, which takes roughly three days to transcribe. On the other extreme, a recent study suggests that the shortest known eukaryotic intron length is 30 base pairs (bp) belonging to the human \"MST1L\" gene.\n\nSplicing of all intron-containing RNA molecules is superficially similar, as described above. However, different types of introns were identified through the examination of intron structure by DNA sequence analysis, together with genetic and biochemical analysis of RNA splicing reactions.\n\nAt least four distinct classes of introns have been identified:\nGroup III introns are proposed to be a fifth family, but little is known about the biochemical apparatus that mediates their splicing. They appear to be related to group II introns, and possibly to spliceosomal introns.\n\nNuclear pre-mRNA introns (spliceosomal introns) are characterized by specific intron sequences located at the boundaries between introns and exons. These sequences are recognized by spliceosomal RNA molecules when the splicing reactions are initiated. In addition, they contain a branch point, a particular nucleotide sequence near the 3' end of the intron that becomes covalently linked to the 5' end of the intron during the splicing process, generating a branched (\"lariat\") intron. Apart from these three short conserved elements, nuclear pre-mRNA intron sequences are highly variable. Nuclear pre-mRNA introns are often much longer than their surrounding exons.\n\nTransfer RNA introns that depend upon proteins for removal occur at a specific location within the anticodon loop of unspliced tRNA precursors, and are removed by a tRNA splicing endonuclease. The exons are then linked together by a second protein, the tRNA splicing ligase. Note that self-splicing introns are also sometimes found within tRNA genes.\n\nGroup I and group II introns are found in genes encoding proteins (messenger RNA), transfer RNA and ribosomal RNA in a very wide range of living organisms., Following transcription into RNA, group I and group II introns also make extensive internal interactions that allow them to fold into a specific, complex three-dimensional architecture. These complex architectures allow some group I and group II introns to be \"self-splicing\", that is, the intron-containing RNA molecule can rearrange its own covalent structure so as to precisely remove the intron and link the exons together in the correct order. In some cases, particular intron-binding proteins are involved in splicing, acting in such a way that they assist the intron in folding into the three-dimensional structure that is necessary for self-splicing activity. Group I and group II introns are distinguished by different sets of internal conserved sequences and folded structures, and by the fact that splicing of RNA molecules containing group II introns generates branched introns (like those of spliceosomal RNAs), while group I introns use a non-encoded guanosine nucleotide (typically GTP) to initiate splicing, adding it on to the 5'-end of the excised intron.\n\nWhile introns do not encode protein products, they are integral to gene expression regulation. Some introns themselves encode functional RNAs through further processing after splicing to generate noncoding RNA molecules. Alternative splicing is widely used to generate multiple proteins from a single gene. Furthermore, some introns play essential roles in a wide range of gene expression regulatory functions such as Nonsense-mediated decay and mRNA export.\n\nThe biological origins of introns are obscure. After the initial discovery of introns in protein-coding genes of the eukaryotic nucleus, there was significant debate as to whether introns in modern-day organisms were inherited from a common ancient ancestor (termed the introns-early hypothesis), or whether they appeared in genes rather recently in the evolutionary process (termed the introns-late hypothesis). Another theory is that the spliceosome and the intron-exon structure of genes is a relic of the RNA world (the introns-first hypothesis). There is still considerable debate about the extent to which of these hypotheses is most correct. The popular consensus at the moment is that introns arose within the eukaryote lineage as selfish elements.\n\nEarly studies of genomic DNA sequences from a wide range of organisms show that the intron-exon structure of homologous genes in different organisms can vary widely. More recent studies of entire eukaryotic genomes have now shown that the lengths and density (introns/gene) of introns varies considerably between related species. For example, while the human genome contains an average of 8.4 introns/gene (139,418 in the genome), the unicellular fungus \"Encephalitozoon cuniculi\" contains only 0.0075 introns/gene (15 introns in the genome). Since eukaryotes arose from a common ancestor (common descent), there must have been extensive gain or loss of introns during evolutionary time. This process is thought to be subject to selection, with a tendency towards intron gain in larger species due to their smaller population sizes, and the converse in smaller (particularly unicellular) species. Biological factors also influence which genes in a genome lose or accumulate introns.\n\nAlternative splicing of introns within a gene acts to introduce greater variability of protein sequences translated from a single gene, allowing multiple related proteins to be generated from a single gene and a single precursor mRNA transcript. The control of alternative RNA splicing is performed by a complex network of signaling molecules that respond to a wide range of intracellular and extracellular signals.\n\nIntrons contain several short sequences that are important for efficient splicing, such as acceptor and donor sites at either end of the intron as well as a branch point site, which are required for proper splicing by the spliceosome. Some introns are known to enhance the expression of the gene that they are contained in by a process known as intron-mediated enhancement (IME).\n\nActively transcribed regions of DNA frequently form R-loops that are vulnerable to DNA damage. In highly expressed yeast genes, introns inhibit R-loop formation and the occurrence of DNA damage. Genome-wide analysis in both yeast and humans revealed that intron-containing genes have decreased R-loop levels and decreased DNA damage compared to intronless genes of similar expression. Insertion of an intron within an R-loop prone gene can also suppress R-loop formation and recombination. Bonnet et al. (2017) speculated that the function of introns in maintaining genetic stability may explain their evolutionary maintenance at certain locations, particularly in highly expressed genes.\n\nThe physical presence of Introns promotes cellular resistance to starvation via intron enhanced repression of ribosomal protein genes of nutrient-sensing pathways.\n\nIntrons may be lost or gained over evolutionary time, as shown by many comparative studies of orthologous genes. Subsequent analyses have identified thousands of examples of intron loss and gain events, and it has been proposed that the emergence of eukaryotes, or the initial stages of eukaryotic evolution, involved an intron invasion. Two definitive mechanisms of intron loss, Reverse Transcriptase-Mediated Intron Loss (RTMIL) and genomic deletions, have been identified, and are known to occur. The definitive mechanisms of intron gain, however, remain elusive and controversial. At least seven mechanisms of intron gain have been reported thus far: Intron Transposition, Transposon Insertion, Tandem Genomic Duplication, Intron Transfer, Intron Gain during Double-Strand Break Repair (DSBR), Insertion of a Group II Intron, and Intronization. In theory it should be easiest to deduce the origin of recently gained introns due to the lack of host-induced mutations, yet even introns gained recently did not arise from any of the aforementioned mechanisms. These findings thus raise the question of whether or not the proposed mechanisms of intron gain fail to describe the mechanistic origin of many novel introns because they are not accurate mechanisms of intron gain, or if there are other, yet to be discovered, processes generating novel introns.\n\nIn intron transposition, the most commonly purported intron gain mechanism, a spliced intron is thought to reverse splice into either its own mRNA or another mRNA at a previously intron-less position. This intron-containing mRNA is then reverse transcribed and the resulting intron-containing cDNA may then cause intron gain via complete or partial recombination with its original genomic locus. Transposon insertions can also result in intron creation. Such an insertion could intronize the transposon without disrupting the coding sequence when a transposon inserts into the sequence AGGT, resulting in the duplication of this sequence on each side of the transposon. It is not yet understood why these elements are spliced, whether by chance, or by some preferential action by the transposon. In tandem genomic duplication, due to the similarity between consensus donor and acceptor splice sites, which both closely resemble AGGT, the tandem genomic duplication of an exonic segment harboring an AGGT sequence generates two potential splice sites. When recognized by the spliceosome, the sequence between the original and duplicated AGGT will be spliced, resulting in the creation of an intron without alteration of the coding sequence of the gene. Double-stranded break repair via non-homologous end joining was recently identified as a source of intron gain when researchers identified short direct repeats flanking 43% of gained introns in Daphnia. These numbers must be compared to the number of conserved introns flanked by repeats in other organisms, though, for statistical relevance. For group II intron insertion, the retrohoming of a group II intron into a nuclear gene was proposed to cause recent spliceosomal intron gain.\n\nIntron transfer has been hypothesized to result in intron gain when a paralog or pseudogene gains an intron and then transfers this intron via recombination to an intron-absent location in its sister paralog. Intronization is the process by which mutations create novel introns from formerly exonic sequence. Thus, unlike other proposed mechanisms of intron gain, this mechanism does not require the insertion or generation of DNA to create a novel intron.\n\nThe only hypothesized mechanism of recent intron gain lacking any direct evidence is that of group II intron insertion, which when demonstrated in vivo, abolishes gene expression. Group II introns are therefore likely the presumed ancestors of spliceosomal introns, acting as site-specific retroelements, and are no longer responsible for intron gain. Tandem genomic duplication is the only proposed mechanism with supporting in vivo experimental evidence: a short intragenic tandem duplication can insert a novel intron into a protein-coding gene, leaving the corresponding peptide sequence unchanged. This mechanism also has extensive indirect evidence lending support to the idea that tandem genomic duplication is a prevalent mechanism for intron gain. The testing of other proposed mechanisms in vivo, particularly intron gain during DSBR, intron transfer, and intronization, is possible, although these mechanisms must be demonstrated in vivo to solidify them as actual mechanisms of intron gain. Further genomic analyses, especially when executed at the population level, may then quantify the relative contribution of each mechanism, possibly identifying species-specific biases that may shed light on varied rates of intron gain amongst different species.\n\nStructure:\nSplicing:\nFunction\nOthers:\n\n"}
{"id": "15345", "url": "https://en.wikipedia.org/wiki?curid=15345", "title": "IEE", "text": "IEE\n\nIEE may stand for:\n\n\n"}
{"id": "15346", "url": "https://en.wikipedia.org/wiki?curid=15346", "title": "Institute of National Remembrance", "text": "Institute of National Remembrance\n\nThe Institute of National Remembrance – Commission for the Prosecution of Crimes against the Polish Nation (; IPN) is a Polish government-affiliated research institute with lustration prerogatives, as well as prosecution powers. It was created by legislation enacted by the Parliament of Poland. The Institute specialises in the legal and historical examination of the 20th century history of Poland in particular. IPN investigates both Nazi and Communist crimes committed in Poland between 1939 and the Revolutions of 1989, documents its findings and disseminates the results of its investigations to the public.\n\nThe Institute was established in law by the Polish Parliament on 18 December 1998, and incorporated the earlier 1991-passed Main Commission for the Prosecution of Crimes against the Polish Nation (which itself had replaced a 1945-passed body on Nazi crimes). It began its activities on 1 July 2000. During the first fifteen years following its inception the IPN collected over of archives, released 1,794 publications, organized 453 exhibits, held 817 conferences, and launched 30 educational internet portals. In the same period, the Institute researchers held interviews with over 103,000 witnesses and interrogated 508 individuals charged with criminal offences, leading to 137 sentences by the courts of justice.\n\nAccording to a new law which went into effect on 15 March 2007, IPN was to be mandated to carry out lustration procedures prescribed by Polish law. However, key articles of that law were judged unconstitutional by Poland's constitutional court on 11 May 2007, so the role of IPN in the lustration process is at present unclear. The IPN is a founding member of the Platform of European Memory and Conscience organisation.\n\nIPN's main areas of activity, in line with its original mission statement, include researching and documenting the losses which were suffered by the Polish Nation as a result of World War II and during the post-war totalitarian period. The Institute informs about the patriotic traditions of resistance against the occupational forces, and the Polish citizens' fight for sovereignty of the nation, including their efforts in defence of freedom and human dignity in general. IPN investigates crimes committed on Polish soil against Polish citizens as well as people of other citizenships wronged in the country. War crimes which are not affected by statute of limitations according to Polish law include:\nIt is the IPN's duty to prosecute crimes against peace and humanity, as much as war crimes. Its mission includes the need to compensate for damages which were suffered by the repressed and harmed people at a time when human rights were disobeyed by the state,\nand educate the public about recent history of Poland. IPN collects, organises and archives all documents about the Polish communist security apparatus active from 22 July 1944 to 31 December 1989.\n\nIPN was created by special legislation on 18 December 1998. IPN is governed by the chairman. This chairman is chosen by a supermajority (60%) of the Polish Parliament (Sejm) with the approval of the Senate of Poland on a request by a Collegium of IPN. The chairman has a 5-year term of office. The first chairman of the IPN was Leon Kieres, elected by the Sejm for five years on 8 June 2000 (term 30 June 2000 – 29 December 2005). The second chairman was Janusz Kurtyka, elected on 9 December 2005 with a term that started 29 December 2005 until his death in the Smolensk airplane crash on 10 April 2010. Franciszek Gryciuk was acting chairman from 2010 to 2011, when the current chairman, Łukasz Kamiński, was elected by the Sejm.\n\nThe IPN is divided into:\n\nOn 29 April 2010, acting president Bronislaw Komorowski signed into law a parliamentary act that reformed the Institute of National Remembrance.\n\nThe research conducted by IPN from December 2000 falls into four main topical areas:\n\nAmong the most widely reported cases investigated by the IPN thus far is the Jedwabne Pogrom, a pogrom of Polish Jews \"committed directly by Poles, but inspired by the Germans\" in 1941. A selection of other cases include:\n\nIPN is involved in dissemination of its research results in the form of publications, particularly the IPN Bulletin (Biuletyn IPN, \"Pamięć.pl\") and the \"Remembrance and Justice\" periodicals, exhibitions, seminars, panel discussions, film reviews, workshops and school curricula. Since December 2000 IPN has organized over 30 academic conferences (particularly the Warsaw Congress of Science organized every year in September); 22 exhibitions in various museums and educational competitions involving thousands of students. \"IPN Bulletin\" is of an informative and popular-scientific character and contains articles pertaining to the history of Poland in the years 1939–1990 as well as describes the current IPN activities. \"Remembrance and Justice\" appears every half a year and is a scientific historical magazine. IPN also publishes books which are usually edited as collections of documents, reports and memories, but also scientific elaborations (78 of such publications have appeared till April 2007).\n\nThe Public Education Office co-operates on a permanent basis with the Ministry of National Education and Sport, having signed a Co-operation Agreement in 2001. IPN gives opinions of curricula and textbooks on history that are used in Polish schools and is involved in teacher training activities. The IPN also co-organizes postgraduate diploma studies on history at the Jagiellonian University and the University of Maria Curie-Skłodowska.\n\nThe Institution of National Remembrance has created several board games to help educate people about recent Polish history\n\nOn 18 December 2006 Polish law regulating IPN was changed and came into effect on 15 March 2007. This change gave IPN new lustration powers. However, key articles of that law were judged unconstitutional by Poland's Constitutional Court on 11 May 2007, making the role of IPN in lustration unclear and putting the whole process into question.\n\nIn 2008, Adam Michnik said that the IPN is \"engaging in activities that destroy this memory. Today's memory police resort to the hateful methods of the communist secret services and direct them at a victim of this very secret service. These policemen violate the truth and fundamental ethical principles.\"\n\nConcerns have been raised of politicization of the IPN, starting with its legal mandate (no comparable institution in any other European country holds prosecutorial power) and continuing to its choice of staff, which at times tended towards particular political views.\n\nOne of the most controversial aspects of IPN is a by-product of its role in collecting and publishing previously secret archives from the Polish communist security apparatus, the Służba Bezpieczeństwa: revealing secret agents and collaborators (a process called \"lustration\"). One incident which drew criticism involved the so-called Wildstein list; a partial list of names of people who allegedly worked for the communist era Polish intelligence service, which was copied from IPN archives (without IPN permission) in 2004 by journalist Bronisław Wildstein and published in the Internet in 2005. The list gained much attention in Polish media and politics, and during that time IPN security procedures and handling of the matter came under criticism.\n\nThe election of a new IPN president in December 2005 was controversial. Janusz Kurtyka, the incumbent IPN president, was contested by Andrzej Przewoźnik. Przewoźnik's candidature received a severe setback after documents were found which suggested his possible co-operation with Służba Bezpieczeństwa, Communist Poland's internal intelligence agency and secret police. Przewoźnik was cleared of the accusations only after he had lost the election.\n\nPrzewoźnik and Kurtyka both died in the 2010 Polish Air Force Tu-154 crash.\n\nIn September 2017, a historian in charge of education in Lublin for the IPN, wrote in a column in \"Gazeta Polska\" that \"after the aggression of Germany into Poland, the situation of the Jews did not look very bad\" and \"although the [Nazi] occupation authorities took over, they ordered the wearing of armbands with the star of David, charged them heavy taxes, began to designate Jews-only zones only for the Jews, but at the same time permitted the creation of Judenrat, that is, organs of self-government.\" In 2014, the same historian said in an expert opinion to a Polish court that the Nazi party was a leftist party and that the swastika is an ambiguous symbol. These statements were widely criticized by other historians including Dariusz Libionka, and the IPN issued a statement saying that the \"In connection with the thesis in the article by Tomasz Panfil in the Gazeta Polska, the Institute of National Remembrance declares that position presented there is in no way compatible with the historical knowledge about the situation of the Jewish population in Poland after September 1, 1939.\" and that it expects the historian \"will, in his scientific and journalistic activities, show diligence and respect to the principles of historical and research reliability.\" In October 2017, education minister Anna Zalewska presented the historian with a medal for \"for special merits for education\".\n\nIn October 2017, the Simon Wiesenthal Center urged the IPN to fire the deputy director of its publishing office because he had published several books by Holocaust denier David Irving. The IPN responded that the official \"is not a Holocaust denier himself so there is no reason to dismiss him\".\n\nIPN actions have also attracted support. In 2006 an open letter was published, declaring that:\nHistory of Solidarity and anti-communist resistance in Poland cannot be damaged by scientific studies and resulting increase in our knowledge of the past. History of opposition to totalitarianism belongs to millions of Poles and not to one social or political group which usurps the right to decide which parts of national history should be discussed and which forgotten.\nThis letter was signed by a former Prime Minister of Poland, Jan Olszewski; the Mayor of Zakopane, Piotr Bąk; Polish-American Professor and member of the United States Holocaust Memorial Council Marek Jan Chodakiewicz; Professors Maria Dzielska, Piotr Franaszek and Tomasz Gąsowski of the Jagiellonian University; Professor Marek Czachor of Gdańsk University of Technology, journalist and writer Marcin Wolski; Solidarity co-founder Anna Walentynowicz and dozens of others.\n\n\n"}
{"id": "15347", "url": "https://en.wikipedia.org/wiki?curid=15347", "title": "Intelligence (disambiguation)", "text": "Intelligence (disambiguation)\n\nIntelligence is one or more capacities of the mind.\n\nIntelligence may also refer to:\n\n\n\n\n\n\n"}
{"id": "15352", "url": "https://en.wikipedia.org/wiki?curid=15352", "title": "Identical particles", "text": "Identical particles\n\nIdentical particles, also called indistinguishable or indiscernible particles, are particles that cannot be distinguished from one another, even in principle. Species of identical particles include, but are not limited to elementary particles such as electrons, composite subatomic particles such as atomic nuclei, as well as atoms and molecules. Quasiparticles also behave in this way. Although all known indistinguishable particles are \"tiny\", there is no exhaustive list of all possible sorts of particles nor a clear-cut limit of applicability, as explored in quantum statistics.\n\nThere are two main categories of identical particles: bosons, which can share quantum states, and fermions, which do not share quantum states as described by the Pauli exclusion principle. Examples of bosons are photons, gluons, phonons, helium-4 nuclei and all mesons. Examples of fermions are electrons, neutrinos, quarks, protons, neutrons, and helium-3 nuclei.\n\nThe fact that particles can be identical has important consequences in statistical mechanics. Calculations in statistical mechanics rely on probabilistic arguments, which are sensitive to whether or not the objects being studied are identical. As a result, identical particles exhibit markedly different statistical behaviour from distinguishable particles. For example, the indistinguishability of particles has been proposed as a solution to Gibbs' mixing paradox.\n\nThere are two methods for distinguishing between particles. The first method relies on differences in the intrinsic physical properties of the particles, such as mass, electric charge, and spin. If differences exist, it is possible to distinguish between the particles by measuring the relevant properties. However, it is an empirical fact that microscopic particles of the same species have completely equivalent physical properties. For instance, every electron in the universe has exactly the same electric charge; this is why it is possible to speak of such a thing as \"the charge of the electron\".\n\nEven if the particles have equivalent physical properties, there remains a second method for distinguishing between particles, which is to track the trajectory of each particle. As long as the position of each particle can be measured with infinite precision (even when the particles collide), then there would be no ambiguity about which particle is which.\n\nThe problem with the second approach is that it contradicts the principles of quantum mechanics. According to quantum theory, the particles do not possess definite positions during the periods between measurements. Instead, they are governed by wavefunctions that give the probability of finding a particle at each position. As time passes, the wavefunctions tend to spread out and overlap. Once this happens, it becomes impossible to determine, in a subsequent measurement, which of the particle positions correspond to those measured earlier. The particles are then said to be indistinguishable.\n\nWhat follows is an example to make the above discussion concrete, using the formalism developed in the article on the mathematical formulation of quantum mechanics.\n\nLet \"n\" denote a complete set of (discrete) quantum numbers for specifying single-particle states (for example, for the particle in a box problem, take \"n\" to be the quantized wave vector of the wavefunction.) For simplicity, consider a system composed of two identical particles. Suppose that one particle is in the state \"n\", and another is in the state \"n\". Intuitively, the quantum state of the system should be\n\nThis is simply the canonical way of constructing a basis for a tensor product space formula_2 of the combined system from the individual spaces. However, this expression implies the ability to identify the particle with \"n\" as \"particle 1\" and the particle with \"n\" as \"particle 2\". If the particles are indistinguishable, this is impossible by definition, because formula_3 and formula_4 are two different states. Two states are physically only equivalent if they differ by a complex phase factor. Requiring this condition leads to the conclusion that the states are given by the following two possibilities:\n\nTo see this, imagine a two identical particle system. Suppose it is known that one of the particles is in state formula_6 and the other is in state formula_7. Prior to the measurement, there is no way to know if particle 1 is in state formula_6 and particle 2 is in state formula_7, or the other way around because the particles are indistinguishable. So, there are equal probabilities for each of the states to occur - meaning that the system is in superposition of both states prior to the measurement. \nStates where this is a sum are known as symmetric; states involving the difference are called antisymmetric. More completely, symmetric states have the form\n\nwhile antisymmetric states have the form\n\nNote that if \"n\" and \"n\" are the same, the antisymmetric expression gives zero, which cannot be a state vector as it cannot be normalized. In other words, in an antisymmetric state two identical particles cannot occupy the same single-particle states. This is known as the Pauli exclusion principle, and it is the fundamental reason behind the chemical properties of atoms and the stability of matter.\n\nThe importance of symmetric and antisymmetric states is ultimately based on empirical evidence. It appears to be a fact of nature that identical particles do not occupy states of a mixed symmetry, such as\n\nThere is actually an exception to this rule, which will be discussed later. On the other hand, it can be shown that the symmetric and antisymmetric states are in a sense special, by examining a particular symmetry of the multiple-particle states known as exchange symmetry.\n\nDefine a linear operator \"P\", called the exchange operator. When it acts on a tensor product of two state vectors, it exchanges the values of the state vectors:\n\n\"P\" is both Hermitian and unitary. Because it is unitary, it can be regarded as a symmetry operator. This symmetry may be described as the symmetry under the exchange of labels attached to the particles (i.e., to the single-particle Hilbert spaces).\n\nClearly, formula_14 (the identity operator), so the eigenvalues of \"P\" are +1 and −1. The corresponding eigenvectors are the symmetric and antisymmetric states:\n\nIn other words, symmetric and antisymmetric states are essentially unchanged under the exchange of particle labels: they are only multiplied by a factor of +1 or −1, rather than being \"rotated\" somewhere else in the Hilbert space. This indicates that the particle labels have no physical meaning, in agreement with the earlier discussion on indistinguishability.\n\nIt will be recalled that \"P\" is Hermitian. As a result, it can be regarded as an observable of the system, which means that, in principle, a measurement can be performed to find out if a state is symmetric or antisymmetric. Furthermore, the equivalence of the particles indicates that the Hamiltonian can be written in a symmetrical form, such as\n\nIt is possible to show that such Hamiltonians satisfy the commutation relation\n\nAccording to the Heisenberg equation, this means that the value of \"P\" is a constant of motion. If the quantum state is initially symmetric (antisymmetric), it will remain symmetric (antisymmetric) as the system evolves. Mathematically, this says that the state vector is confined to one of the two eigenspaces of \"P\", and is not allowed to range over the entire Hilbert space. Thus, that eigenspace might as well be treated as the actual Hilbert space of the system. This is the idea behind the definition of Fock space.\n\nThe choice of symmetry or antisymmetry is determined by the species of particle. For example, symmetric states must always be used when describing photons or helium-4 atoms, and antisymmetric states when describing electrons or protons.\n\nParticles which exhibit symmetric states are called bosons. The nature of symmetric states has important consequences for the statistical properties of systems composed of many identical bosons. These statistical properties are described as Bose–Einstein statistics.\n\nParticles which exhibit antisymmetric states are called fermions. Antisymmetry gives rise to the Pauli exclusion principle, which forbids identical fermions from sharing the same quantum state. Systems of many identical fermions are described by Fermi–Dirac statistics.\n\nParastatistics are also possible.\n\nIn certain two-dimensional systems, mixed symmetry can occur. These exotic particles are known as anyons, and they obey fractional statistics. Experimental evidence for the existence of anyons exists in the fractional quantum Hall effect, a phenomenon observed in the two-dimensional electron gases that form the inversion layer of MOSFETs. There is another type of statistic, known as braid statistics, which are associated with particles known as plektons.\n\nThe spin-statistics theorem relates the exchange symmetry of identical particles to their spin. It states that bosons have integer spin, and fermions have half-integer spin. Anyons possess fractional spin.\n\nThe above discussion generalizes readily to the case of \"N\" particles. Suppose there are \"N\" particles with quantum numbers \"n\", \"n\", ..., n. If the particles are bosons, they occupy a totally symmetric state, which is symmetric under the exchange of \"any two\" particle labels:\n\nHere, the sum is taken over all different states under permutations \"p\" acting on \"N\" elements. The square root left to the sum is a normalizing constant. The quantity \"m\" stands for the number of times each of the single-particle states \"n\" appears in the \"N\"-particle state. Note that \"∑ m = N\".\n\nIn the same vein, fermions occupy totally antisymmetric states:\n\nHere, is the sign of each permutation (i.e.formula_21 if formula_22 is composed of an even number of transpositions, and formula_23 if odd). Note that there is no formula_24 term, because each single-particle state can appear only once in a fermionic state. Otherwise the sum would again be zero due to the antisymmetry, thus representing a physically impossible state. This is the Pauli exclusion principle for many particles.\n\nThese states have been normalized so that\n\nSuppose there is a system of \"N\" bosons (fermions) in the symmetric (antisymmetric) state\n\nand a measurement is performed on some other set of discrete observables, \"m\". In general, this yields some result \"m\" for one particle, \"m\" for another particle, and so forth. If the particles are bosons (fermions), the state after the measurement must remain symmetric (antisymmetric), i.e.\n\nThe probability of obtaining a particular result for the \"m\" measurement is\n\nIt can be shown that \n\nwhich verifies that the total probability is 1. The sum has to be restricted to \"ordered\" values of \"m\", ..., \"m\" to ensure that each multi-particle state is not counted more than once.\n\nSo far, the discussion has included only discrete observables. It can be extended to continuous observables, such as the position \"x\".\n\nRecall that an eigenstate of a continuous observable represents an infinitesimal \"range\" of values of the observable, not a single value as with discrete observables. For instance, if a particle is in a state |\"ψ\"⟩, the probability of finding it in a region of volume \"d\"\"x\" surrounding some position \"x\" is\n\nAs a result, the continuous eigenstates |\"x\"⟩ are normalized to the delta function instead of unity:\n\nSymmetric and antisymmetric multi-particle states can be constructed from continuous eigenstates in the same way as before. However, it is customary to use a different normalizing constant:\n\nA many-body wavefunction can be written,\n\nwhere the single-particle wavefunctions are defined, as usual, by\n\nThe most important property of these wavefunctions is that exchanging any two of the coordinate variables changes the wavefunction by only a plus or minus sign. This is the manifestation of symmetry and antisymmetry in the wavefunction representation:\n\nThe many-body wavefunction has the following significance: if the system is initially in a state with quantum numbers \"n\", ..., n, and a position measurement is performed, the probability of finding particles in infinitesimal volumes near \"x\", \"x\", ..., \"x\" is\n\nThe factor of \"N\"! comes from our normalizing constant, which has been chosen so that, by analogy with single-particle wavefunctions,\n\nBecause each integral runs over all possible values of \"x\", each multi-particle state appears \"N\"! times in the integral. In other words, the probability associated with each event is evenly distributed across \"N\"! equivalent points in the integral space. Because it is usually more convenient to work with unrestricted integrals than restricted ones, the normalizing constant has been chosen to reflect this.\n\nFinally, antisymmetric wavefunction can be written as the determinant of a matrix, known as a Slater determinant:\n\nThe Hilbert space for formula_42 particles is given by the tensor product formula_43. The permutation group of formula_44 acts on this space by permuting the entries. By definition the expectation values for an observable formula_45 of formula_42 indistinguishable particles should be invariant under these permutation. This means that for all formula_47 and formula_48\nor equivalently for each formula_48\nTwo states are equivalent whenever their expectation values coincide for all observables. If we restrict to observables of formula_52 identical particles, and hence observables satisfying the equation above, we find that the following states (after normalization) are equivalent\nThe equivalence classes are in bijective relation with irreducible subspaces of formula_43 under formula_44.\n\nTwo obvious irreducible subspaces are the one dimensional symmetric/bosonic subspace and anti-symmetric/fermionic subspace. There are however more types of irreducible subspaces. States associated with these other irreducible subspaces are called parastatistic states. Young tableaux provide a way to classify all of these irreducible subspaces.\n\nThe indistinguishability of particles has a profound effect on their statistical properties. To illustrate this, consider a system of \"N\" distinguishable, non-interacting particles. Once again, let \"n\" denote the state (i.e. quantum numbers) of particle \"j\". If the particles have the same physical properties, the \"n\"'s run over the same range of values. Let \"ε\"(\"n\") denote the energy of a particle in state \"n\". As the particles do not interact, the total energy of the system is the sum of the single-particle energies. The partition function of the system is\n\nwhere \"k\" is Boltzmann's constant and \"T\" is the temperature. This expression can be factored to obtain\n\nwhere\n\nIf the particles are identical, this equation is incorrect. Consider a state of the system, described by the single particle states [\"n\", ..., \"n\"]. In the equation for \"Z\", every possible permutation of the \"n\"'s occurs once in the sum, even though each of these permutations is describing the same multi-particle state. Thus, the number of states has been over-counted.\n\nIf the possibility of overlapping states is neglected, which is valid if the temperature is high, then the number of times each state is counted is approximately \"N\"<nowiki>!</nowiki>. The correct partition function is\n\nNote that this \"high temperature\" approximation does not distinguish between fermions and bosons.\n\nThe discrepancy in the partition functions of distinguishable and indistinguishable particles was known as far back as the 19th century, before the advent of quantum mechanics. It leads to a difficulty known as the Gibbs paradox. Gibbs showed that in the equation \"Z\" = \"ξ\", the entropy of a classical ideal gas is\n\nwhere \"V\" is the volume of the gas and \"f\" is some function of \"T\" alone. The problem with this result is that \"S\" is not extensive – if \"N\" and \"V\" are doubled, \"S\" does not double accordingly. Such a system does not obey the postulates of thermodynamics.\n\nGibbs also showed that using \"Z\" = \"ξ\"/\"N\"! alters the result to\n\nwhich is perfectly extensive. However, the reason for this correction to the partition function remained obscure until the discovery of quantum mechanics.\n\nThere are important differences between the statistical behavior of bosons and fermions, which are described by Bose–Einstein statistics and Fermi–Dirac statistics respectively. Roughly speaking, bosons have a tendency to clump into the same quantum state, which underlies phenomena such as the laser, Bose–Einstein condensation, and superfluidity. Fermions, on the other hand, are forbidden from sharing quantum states, giving rise to systems such as the Fermi gas. This is known as the Pauli Exclusion Principle, and is responsible for much of chemistry, since the electrons in an atom (fermions) successively fill the many states within shells rather than all lying in the same lowest energy state.\n\nThe differences between the statistical behavior of fermions, bosons, and distinguishable particles can be illustrated using a system of two particles. The particles are designated A and B. Each particle can exist in two possible states, labelled formula_62 and formula_63, which have the same energy.\n\nThe composite system can evolve in time, interacting with a noisy environment. Because the formula_62 and formula_63 states are energetically equivalent, neither state is favored, so this process has the effect of randomizing the states. (This is discussed in the article on quantum entanglement.) After some time, the composite system will have an equal probability of occupying each of the states available to it. The particle states are then measured.\n\nIf A and B are distinguishable particles, then the composite system has four distinct states: formula_66, formula_67, formula_68, and formula_69. The probability of obtaining two particles in the formula_62 state is 0.25; the probability of obtaining two particles in the formula_63 state is 0.25; and the probability of obtaining one particle in the formula_62 state and the other in the formula_63 state is 0.5.\n\nIf A and B are identical bosons, then the composite system has only three distinct states: formula_66, formula_67, and formula_76. When the experiment is performed, the probability of obtaining two particles in the formula_62 state is now 0.33; the probability of obtaining two particles in the formula_63 state is 0.33; and the probability of obtaining one particle in the formula_62 state and the other in the formula_63 state is 0.33. Note that the probability of finding particles in the same state is relatively larger than in the distinguishable case. This demonstrates the tendency of bosons to \"clump.\"\n\nIf A and B are identical fermions, there is only one state available to the composite system: the totally antisymmetric state formula_81. When the experiment is performed, one particle is always in the formula_62 state and the other is in the formula_63 state.\n\nThe results are summarized in Table 1:\n\nAs can be seen, even a system of two particles exhibits different statistical behaviors between distinguishable particles, bosons, and fermions. In the articles on Fermi–Dirac statistics and Bose–Einstein statistics, these principles are extended to large number of particles, with qualitatively similar results.\n\nTo understand why particle statistics work the way that they do, note first that particles are point-localized excitations and that particles that are spacelike separated do not interact. In a flat \"d\"-dimensional space \"M\", at any given time, the configuration of two identical particles can be specified as an element of \"M\" × \"M\". If there is no overlap between the particles, so that they do not interact directly, then their locations must belong to the space the subspace with coincident points removed. The element describes the configuration with particle I at x and particle II at y, while describes the interchanged configuration. With identical particles, the state described by ought to be indistinguishable from the state described by . Now consider the homotopy class of continuous paths from to , within the space . If \"M\" is R where , then this homotopy class only has one element. If \"M\" is R, then this homotopy class has countably many elements (i.e. a counterclockwise interchange by half a turn, a counterclockwise interchange by one and a half turns, two and a half turns, etc., a clockwise interchange by half a turn, etc.). In particular, a counterclockwise interchange by half a turn is \"not\" homotopic to a clockwise interchange by half a turn. Lastly, if \"M\" is R, then this homotopy class is empty.\n\nSuppose first that . The universal covering space of which is none other than itself, only has two points which are physically indistinguishable from , namely itself and . So, the only permissible interchange is to swap both particles. This interchange is an involution, so its only effect is to multiply the phase by a square root of 1. If the root is +1, then the points have Bose statistics, and if the root is −1, the points have Fermi statistics.\n\nIn the case \"M\" = R, the universal covering space of has infinitely many points that are physically indistinguishable from . This is described by the infinite cyclic group generated by making a counterclockwise half-turn interchange. Unlike the previous case, performing this interchange twice in a row does not recover the original state; so such an interchange can generically result in a multiplication by exp(\"iθ\") for any real \"θ\" (by unitarity, the absolute value of the multiplication must be 1). This is called anyonic statistics. In fact, even with two \"distinguishable\" particles, even though is now physically distinguishable from , the universal covering space still contains infinitely many points which are physically indistinguishable from the original point, now generated by a counterclockwise rotation by one full turn. This generator, then, results in a multiplication by exp(\"iφ\"). This phase factor here is called the mutual statistics.\n\nFinally, in the case \"M\" = R, the space is not connected, so even if particle I and particle II are identical, they can still be distinguished via labels such as \"the particle on the left\" and \"the particle on the right\". There is no interchange symmetry here.\n\n\n"}
