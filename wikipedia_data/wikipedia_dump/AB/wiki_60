{"id": "19499", "url": "https://en.wikipedia.org/wiki?curid=19499", "title": "Mariah Carey", "text": "Mariah Carey\n\nMariah Carey (born March 27, 1969 or 1970) is an American singer and songwriter. Referred to as the \"Songbird Supreme\" by the \"Guinness World Records\", she is noted for her five-octave vocal range, power, melismatic style, and signature use of the whistle register. She rose to fame in 1990 after signing to Columbia Records and releasing her eponymous debut album, which topped the US \"Billboard\" 200 for 11 consecutive weeks. Soon after, Carey became the first and only artist to have their first five singles reach number one on the US \"Billboard\" Hot 100 chart, from \"Vision of Love\" to \"Emotions\".\n\nFollowing her marriage to Sony Music head Tommy Mottola, Carey became the label's highest-selling act with the follow-up albums \"Music Box\" (1993), \"Merry Christmas\" (1994), and \"Daydream\" (1995). These albums spawned some of Carey's most successful singles, including \"Hero\", \"Without You\", \"All I Want for Christmas Is You\", and \"One Sweet Day\"; the latter became the longest-running U.S. number-one single in history, with a total of 16 weeks. After separating from Mottola, Carey adopted a new image and incorporated more elements of hip hop into her music with the release of \"Butterfly\" (1997). \"Billboard\" named her the most successful artist of the 1990s in the United States, while the World Music Awards honored her as the world's best-selling recording artist of the 1990s.\n\nAfter 11 consecutive years charting a \"Billboard\" Hot 100 number-one single, Carey parted with Columbia in 2000 and signed a record-breaking $100 million recording contract with Virgin Records. However, following her highly publicized physical and emotional breakdown, as well as the failure of her film \"Glitter\" (2001) and its accompanying soundtrack, her contract was bought out for $50 million by Virgin and she signed with Island Records the next year. After a relatively unsuccessful period, she returned to the top of music charts with \"The Emancipation of Mimi\" (2005). It became the world's second best-selling album of 2005 and produced \"We Belong Together\", which made her the only artist to top the \"Billboard\" Hot 100 Decade-End chart twice. With the release of \"Touch My Body\" (2008), Carey gained her 18th number-one single in the United States, more than any other solo artist.\n\nThroughout her career, Carey has sold more than 200 million records worldwide, making her one of the best-selling music artists of all time. According to the Recording Industry Association of America (RIAA), she is the third-best-selling female artist in the United States, with 63.5 million certified albums. In 2012, she was ranked second on VH1's list of the 100 Greatest Women in Music. Aside from her commercial accomplishments, Carey has won 5 Grammy Awards, 19 World Music Awards, 10 American Music Awards, and 14 Billboard Music Awards, and has been consistently credited with inspiring generations of singers.\n\nMariah Carey was born in Huntington, New York. Her father, Alfred Roy Carey, was of African American and Afro-Venezuelan descent, while her mother, Patricia (née Hickey), is of Irish descent. The last name Carey was adopted by her Venezuelan grandfather, Francisco Núñez, after he came to New York. Patricia was an occasional opera singer and vocal coach before she met Alfred in 1960. As he began earning a living as an aeronautical engineer, the couple wed later that year, and moved into a small suburb in New York. After their elopement, Patricia's family disowned her for marrying a black man. Carey later explained that growing up, she felt neglected by her maternal family, which greatly affected her. During the years between the births of Carey's older sister Alison and herself, the Carey family struggled within the community due to their ethnicity. Carey's name was derived from the song \"They Call the Wind Maria\", originally from the 1951 Broadway musical \"Paint Your Wagon.\" When Carey was three, her parents divorced.\nAfter their separation, Alison moved in with her father, while the other two children, Mariah and brother Morgan, remained with their mother. Carey would grow apart from her father, and would later stop seeing him altogether. By the age of four, Carey recalled that she had begun to sneak the radio under her covers at night, and just sing and try to find peace within the music. During elementary school, she excelled in subjects that she enjoyed, such as music, art, and literature, but did not find interest in others. After several years of financial struggles, Patricia earned enough money to move her family into a stable and more affluent sector in New York. Carey had begun writing poems and adding melodies to them, thus starting as a singer-songwriter while attending Harborfields High School in Greenlawn, New York, where she graduated in 1987. Carey excelled in her music, and demonstrated usage of the whistle register, though only beginning to master and control it through her training with her mother. Though introducing her daughter to classical opera, Patricia never pressured her to pursue a career in it, as she never seemed interested. Carey recalled that she kept her singer-songwriter works a secret and noted that Patricia had \"never been a pushy mom. She never said, 'Give it more of an operatic feel.' I respect opera like crazy, but it didn't influence me.\"\n\nWhile in high school, Carey began writing songs with Gavin Christopher. They needed an assistant who could play the keyboard: \"We called someone and he couldn't come, so by accident we stumbled upon Ben [Margulies]. Ben came to the studio, and he really couldn't play the keyboards very well – he was really more of a drummer – but after that day, we kept in touch, and we sort of clicked as writers.\" Carey and Christopher began writing and composing songs in the basement of his father's store during Carey's senior year. After composing their first song together, \"Here We Go 'Round Again\", which Carey described as having a Motown vibe, they continued writing material for a full-length demo. She began living in a one-bedroom apartment in Manhattan, which she shared with four other female students. Carey worked as a waitress for various restaurants, usually getting fired after two weeks. While requiring work to pay for her rent, Carey still had musical ambitions, as she continued working late into the night with Margulies in hopes of completing a demo. After completing her four song demo tape, Carey attempted to pass it to music labels, but failed each time. Shortly thereafter, she was introduced to rising pop singer Brenda K. Starr.\n\nAs Starr's friendship with Carey grew, so did her interest in helping Carey succeed in the industry. In December 1988, Carey accompanied Starr to a record executives' gala, where she handed her demo tape to the head of Columbia Records, Tommy Mottola, who listened to it on his way back home. After the first two songs, he was interested in her; later, after searching for Carey for two weeks, he immediately signed her and began mapping out her commercial debut. While she maintained that she wanted to continue working with Margulies, Mottola enlisted top producers of the time, including Ric Wake, Narada Michael Walden and Rhett Lawrence. Mottola and the staff at Columbia had planned to market Carey as their main female pop artist, competing with Whitney Houston and Madonna (signed to Arista and Sire Records respectively). After the completion of her debut album, \"Mariah Carey\", Columbia spent more than $1 million promoting it. Despite a weak start, the album eventually reached the top of the \"Billboard\" 200, after Carey's exposure at the 33rd Annual Grammy Awards. \"Mariah Carey\" stayed atop the charts for eleven consecutive weeks, and she won the Best New Artist, and Best Female Pop Vocal Performance awards for her single \"Vision of Love.\" In addition to \"Vision of Love\", the album yielded the \"Billboard\" Hot 100 number one singles \"Love Takes Time\", \"Someday\", and \"I Don't Wanna Cry\". Carey became the first musical act since the Jackson 5 to have their first four singles reach number one. \"Mariah Carey\" finished as the best-selling album in the United States in 1991, while totaling sales of over 15 million copies.\n\nCarey began recording her second studio album, \"Emotions\", in 1991. She described it as an homage to Motown soul music, as she felt the need to pay tribute to the type of music that had influenced her as a child. For the project, Carey worked with Walter Afanasieff, who only had a small role on her debut, as well as Robert Clivillés and David Cole, from the dance group C+C Music Factory. Carey's relationship with Margulies deteriorated over a personal contract Carey had signed with him before signing the record deal with Columbia, agreeing to split not only the songwriting royalties from the songs, but half of her earnings as well. However, when the time came to write music for \"Emotions,\" Sony officials made it clear he would only be paid the fair amount given to co-writers on an album. Margulies later filed a lawsuit against Sony which ultimately led to their parting of ways. \"Emotions\" was released on September 17, 1991, and was accepted by critics as a more mature album than its predecessor. While praised for Carey's improved songwriting, production, and new sound, the album was criticized for its material, thought weaker than that of her debut. Though the album managed sales of over eight million copies globally, \"Emotions\" failed to reach the commercial and critical heights of its predecessor.\n\nAs after the release of her debut, critics again questioned whether Carey would embark on a world tour to promote her material. Although Carey explained that stage fright and the style of her songs made a tour very daunting, speculation grew that Carey was a \"studio worm,\" and that she was incapable of producing the perfect pitch and 5-octave vocal range for which she was known. In hopes of putting to rest any claims of her being a manufactured artist, Carey and Walter Afanasieff decided to book an appearance on MTV Unplugged, a television program aired by MTV. The show presented name artists \"unplugged\" or stripped of studio equipment. While Carey favored her more soulful and powerful songs, it was decided that her most popular content would be included. Days before the show's taping, Carey and Afanasieff thought of adding a cover version of an older song, in order to provide something different and unexpected. They chose \"I'll Be There\", a song made popular by The Jackson 5 in 1970. On March 16, 1992, Carey recorded a seven-piece set-list at Kaufman Astoria Studios in Queens, New York. The revue was met with critical acclaim, leading to it being aired more than three times as often as an average episode would. The success tempted Sony officials to market it. Sony decided to release it as an EP, priced low because it was short. The EP proved to be a success, contrary to critics and speculations that Carey was just a studio artist, and was given a triple-Platinum certification by the Recording Industry Association of America (RIAA), and managed Gold and Platinum certifications in several European markets.\n\nDuring early 1993, Carey began working on her third studio album, \"Music Box\". After \"Emotions\" failed to achieve the commercial heights of her debut album, Carey and Columbia came to the agreement that the next album would contain a more pop-influenced sound in order to appeal to a wider audience. During Carey's writing sessions, she began working mostly with Afanasieff, with whom she co-wrote and produced most of \"Music Box\". On August 31, \"Music Box\" was released around the world, debuting at number-one on the \"Billboard\" 200. The album was met with mixed reception from music critics; while many praised the album's pop influence and strong content, others felt that Carey made less usage of her acclaimed vocal range. Ron Wynn from AllMusic described Carey's different form of singing on the album: \"It was wise for Carey to display other elements of her approach, but sometimes excessive spirit is preferable to an absence of passion.\"\n\nAfter declining to tour for her past two albums, Carey agreed to embark on a short string of concerts in late 1993, titled the Music Box Tour. Spanning only six dates across the United States, the short but successful tour was a large step for Carey, who dreaded the hassle of touring. With the release of the album's second and third singles, Carey achieved several career milestones and expanded her popularity throughout Europe. \"Hero\" became Carey's eighth chart topper in the United States and would eventually come to be one of the most popular and inspirational songs of her career, while her cover of Badfinger's \"Without You\" became her first number one single in Germany, Sweden, and the United Kingdom. \"Music Box\" spent prolonged periods at number one on international album charts and eventually became one of the best-selling albums of all time, with worldwide sales of over 28 million copies.\n\nReleased on October 3, 1995, \"Daydream\" combined the pop sensibilities of \"Music Box\" with downbeat R&B and hip hop influences. Critically, the album was heralded as Carey's best to date; \"The New York Times\" named it one of 1995's best albums, and wrote, \"best cuts bring R&B candy-making to a new peak of textural refinement [...] Carey's songwriting has taken a leap forward and become more relaxed, sexier and less reliant on thudding clichés.\" The album's second single, \"One Sweet Day\", a collaboration with R&B group Boyz II Men, remained atop the \"Billboard\" Hot 100 for a record-breaking 16 consecutive weeks, becoming the longest-running number-one song in history. \"Daydream\" became her biggest-selling album in the United States, and became her second album to be certified Diamond by the RIAA, following \"Music Box\". The album sold 2.2 million copies in Japan alone and eventually reached global sales of over 25 million copies.\n\nDue to the album's success, \"Daydream\" and its singles were respectively nominated in six categories at the 38th Grammy Awards. Carey, along with Boyz II Men, opened the event with a performance of \"One Sweet Day\". However, Carey did not receive any award, prompting her to comment \"What can you do? I will never be disappointed again. After I sat through the whole show and didn't win once, I can handle anything.\" Following her awards ceremony disappointments, Carey opted to embark on the Daydream World Tour. It had seven dates, three in Japan and four throughout Europe. When tickets went on sale, Carey set records when all 150,000 tickets for her three shows at Japan's largest stadium, Tokyo Dome, sold out in under three hours, breaking the previous record held by The Rolling Stones.\n\nWith her following albums, Carey began to take more initiative and control with her music, and started infusing more genres into her work. For \"Butterfly\", she sought to work with other producers and writers other than Afanasieff, such as Sean Combs, Kamaal Fareed, Missy Elliott and Jean Claude Oliver and Samuel Barnes from Trackmasters. During the album's recording, Carey and Mottola separated, with Carey citing it as her way of achieving freedom, and a new lease on life. Aside from the album's different approach, critics took notice of Carey's altered style of singing, which she described as breathy vocals. Her new-found style of singing was met with mixed reception; some critics felt this was a sign of maturity, that she did not feel the need to always show off her upper range, while others felt it was a sign of her weakening and waning voice. The album's lead single, \"Honey\", and its accompanying music video, introduced a more overtly sexual image than Carey had ever demonstrated, and furthered reports of her freedom from Mottola. Carey believed that her image was not \"that much of a departure from what I've done in the past [...] It's not like I went psycho and thought I would be a rapper. Personally, this album is about doing whatever the hell I wanted to do.\" Reviews for \"Butterfly\" were generally positive: \"Rolling Stone\" wrote, \"It's not as if Carey has totally dispensed with her old saccharine, Houston-style balladry [...] but the predominant mood of 'Butterfly' is one of coolly erotic reverie.\" AllMusic editor Stephen Thomas Erlewine described Carey's vocals as \"sultrier and more controlled than ever,\" and heralded \"Butterfly\" as one of her \"best records and illustrates that Carey continues to improve and refine her music, which makes her a rarity among her '90s peers.'\" The album was a commercial success, although not to the degree of her previous albums \"Mariah Carey\", \"Music Box\" and \"Daydream\".\nCarey began developing other projects during the late 1990s. On April 14, 1998, Carey partook in the VH1 Divas benefit concert, where she sang alongside Aretha Franklin, Celine Dion, Shania Twain, Gloria Estefan, and Carole King. Carey had begun developing a film project \"All That Glitters\", later re-titled to simply \"Glitter\", and wrote songs for other projects, such as \"Men in Black\" (1997) and \"How the Grinch Stole Christmas\" (2000). After \"Glitter\" fell into developmental hell, Carey postponed the project, and began writing material for a new album. Sony Music executives wanted her to prepare a greatest hits collection in time for the holiday season. They wanted to release an album that featured her number one singles in the United States, and her international chart toppers on the European versions, without any new material, while Carey felt that a compilation album should reflect on her most personal songs, not just her most commercial. The album, titled \"#1's\" (1998), featured a duet with Whitney Houston, \"When You Believe\", which was included on the soundtrack for \"The Prince of Egypt\" (1998). \"#1's\" became a phenomenon in Japan, selling over one million copies in its opening week, making Carey as the only international artist to accomplish this feat. It sold over 3.25 million copies in Japan after only the first three months, and holds the record as the best-selling album by a non-Asian artist.\n\nDuring the spring of 1999, Carey began working on the final album per her record contract with Sony. However, due to the pressure and the awkward relationship Carey had developed with Sony, she completed the album in a period of three months in the summer of 1999, quicker than any of her other albums. Titled \"Rainbow\" (1999), the album found Carey once again working with a new array of music producers and songwriters, such as Jay-Z and DJ Clue?. Carey also wrote two ballads with David Foster and Diane Warren, whom she used to replace Afanasieff. \"Rainbow\" was released on November 2, 1999, to the highest first week sales of her career at the time, however debuting at number two on the \"Billboard\" 200. In the meantime Carey's troubled relationship with Columbia grew, as they halted promotion after the album's first two singles. They felt \"Rainbow\" did not have any strong single to be released, whereas Carey wanted to release a ballad. This led to a very public feud, as Carey began posting messages on her website, telling fans inside information on the dispute, as well as instructing them to request \"Can't Take That Away (Mariah's Theme)\" on radio stations. Ultimately, the song was only given a very limited and low-promotion release. Critical reception of \"Rainbow\" was generally enthusiastic, with the \"Sunday Herald\" saying that the album \"sees her impressively tottering between soul ballads and collaborations with R&B heavyweights like Snoop Doggy Dogg and Usher [...] It's a polished collection of pop-soul.\" Though a commercial success, \"Rainbow\" became Carey's lowest selling album to that point in her career.\n\nAfter she received \"Billboard\"s Artist of the Decade Award and the World Music Award for Best-Selling Female Artist of the Millennium, Carey parted from Columbia and signed an estimated $100 million, five-album recording contract with Virgin Records America (EMI Records) in April 2001. Carey was given full conceptual and creative control over the project. She opted to record an album partly mixed with 1980s influenced disco and other similar genres, in order to go hand-in-hand with the film's setting. She often stated that Columbia had regarded her as a commodity, with her separation from Mottola exacerbating her relations with label executives. Just a few months later, in July 2001, it was widely reported that Carey had suffered a physical and emotional breakdown. She had left messages on her website that complained of being overworked, and her three-year relationship with the singer Luis Miguel ended. In an interview the following year, she said, \"I was with people who didn't really know me and I had no personal assistant. I'd do interviews all day long and get two hours of sleep a night, if that.\" Due to the pressure from the media, her heavy work schedule and the split from Miguel, Carey began posting a series of disturbing messages on her official website, and displayed erratic behavior on several live promotional outings. On July 19, 2001, Carey made a surprise appearance on the MTV program \"Total Request Live\" (TRL). As the show's host Carson Daly began taping following a commercial break, Carey came out pushing an ice cream cart while wearing a large men's shirt, and began a striptease, in which she shed her shirt to reveal a tight yellow and green ensemble. While she later revealed that Daly was aware of her presence in the building prior to her appearance, Carey's appearance on TRL garnered strong media attention. Only days later, Carey began posting irregular voice notes and messages on her official website: \"I'm trying to understand things in life right now and so I really don't feel that I should be doing music right now. What I'd like to do is just a take a little break or at least get one night of sleep without someone popping up about a video. All I really want is [to] just be me and that's what I should have done in the first place ... I don't say this much but guess what, I don't take care of myself.\" Following the quick removal of the messages, Berger commented that Carey had been \"obviously exhausted and not thinking clearly\" when she posted the letters.\nOn July 26, she was suddenly hospitalized, citing \"extreme exhaustion\" and a \"physical and emotional breakdown.\" Carey was admitted to an undisclosed hospital in Connecticut, and remained hospitalized and under doctor's care for two weeks, followed by an extended absence from the public. Following the heavy media coverage surrounding Carey's publicized breakdown and hospitalization, Virgin Records America and 20th Century Fox delayed the release of both \"Glitter\", as well as its soundtrack of the same name. When discussing the project's weak commercial reaction, Carey blamed both her frame of mind during the time of its release, its postponement, as well as the soundtrack having been released on September 11. Critics panned \"Glitter\", as well as its accompanying soundtrack; both were unsuccessful commercially. The accompanying soundtrack album, \"Glitter\", became Carey's lowest-selling album to that point. The \"St. Louis Post-Dispatch\" dismissed it as \"an absolute mess that'll go down as an annoying blemish on a career that, while not always critically heralded, was at least nearly consistently successful.\" Following the negative cloud that was enveloping Carey's personal life at the time, as well as the project's poor reception, her $100 million five-album record deal with Virgin Records America (EMI Records) was bought out for $50 million. Soon after, Carey flew to Capri, Italy for a period of five months, in which she began writing material for her new album, stemming from all the personal experiences she had endured throughout the past year. Carey later said that her time at Virgin was \"a complete and total stress-fest [...] I made a total snap decision which was based on money and I never make decisions based on money. I learned a big lesson from that.\" Later that year, she signed a contract with Island Records, valued at more than $24 million, and launched the record label MonarC. To add further to Carey's emotional burdens, her father, with whom she had little contact since childhood, died of cancer that year.\n\nIn 2002, Carey was cast in the independent film, \"WiseGirls\", alongside Mira Sorvino and Melora Walters, who co-starred as waitresses at a mobster-operated restaurant. It premiered at the Sundance Film Festival, and received generally negative critical response, though Carey's portrayal of the character was praised; Roger Friedman of Fox News referred to her as \"a Thelma Ritter for the new millennium,\" and wrote, \"Her line delivery is sharp and she manages to get the right laughs.\" Later that year, Carey performed the American national anthem to rave reviews at the Super Bowl XXXVI at the Louisiana Superdome in New Orleans, Louisiana. Towards the end of 2002, Carey released her next studio album \"Charmbracelet\", which she said marked \"a new lease on life\" for her. Though released in the wake of \"Glitter\" and Carey's return to the music scene, sales of \"Charmbracelet\" were moderate and the quality of Carey's vocals came under criticism. Joan Anderson from \"The Boston Globe\" declared the album \"the worst of her career, and revealed a voice [that is] no longer capable of either gravity-defying gymnastics or soft coos,\" while AllMusic editor Stephen Thomas Erlewine expressed similar sentiments and wrote, \"What is a greater problem is that Mariah's voice is shot, sounding in tatters throughout the record. She can no longer coo or softly croon nor can she perform her trademark gravity-defying vocal runs.\"\n\nIn April 2003, Carey announced she would be touring later in the year. The Charmbracelet World Tour: An Intimate Evening with Mariah Carey, spanned North America and East Asia over three months, generally playing in smaller venues rather than arenas. Throughout the United States, the shows were done in theaters, and something more Broadway-influenced, \"It's much more intimate so you'll feel like you had an experience. You experience a night with me.\" However, while smaller productions were booked throughout the tour's stateside leg, Carey performed at stadiums in Asia and Europe, performing for a crowd of over 35,000 in Manila, 50,000 in Malaysia, and to over 70,000 people in China. In the United Kingdom, it became Carey's first tour to feature shows outside London, booking arena stops in Glasgow, Birmingham and Manchester. Charmbracelet World Tour: An Intimate Evening with Mariah Carey garnered generally positive reviews from music critics and concert goers, with many complimenting the quality of Carey's live vocals, as well as the production as a whole.\n\nThroughout 2004, Carey focused on composing material for her tenth studio album, \"The Emancipation of Mimi\" (2005). The album found Carey working predominantly with Jermaine Dupri, as well as Bryan-Michael Cox, Manuel Seal, The Neptunes and Kanye West. The album debuted atop the charts in several countries, and was warmly accepted by critics. Caroline Sullivan of \"The Guardian\" defined it as \"cool, focused and urban [... some of] the first Mariah Carey tunes in years which I wouldn't have to be paid to listen to again,\" while \"USA Today\"s Elysa Gardner wrote, \"The ballads and midtempo numbers that truly reflect the renewed confidence of a songbird who has taken her shots and kept on flying.\" The album's second single, \"We Belong Together\", became a \"career re-defining\" song for Carey, at a point when many critics had considered her career over. Music critics heralded the song as her \"return to form,\" as well as the \"return of The Voice,\" while many felt it would revive \"faith\" in Carey's potential as a balladeer. \"We Belong Together\" broke several records in the United States and became Carey's sixteenth chart topper on the \"Billboard\" Hot 100. After staying at number one for fourteen non-consecutive weeks, the song became the second longest running number one song in US chart history, behind Carey's 1996 collaboration with Boyz II Men, \"One Sweet Day\". \"Billboard\" listed it as the \"song of the decade\" and the ninth most popular song of all time. Besides its chart success, the song broke several airplay records, and according to Nielsen BDS, gathered both the largest one-day and one-week audiences in history.\n\nDuring the week of September 25, 2005, Carey set another record, becoming the first female to occupy the first two spots atop the Hot 100, as \"We Belong Together\" remained at number one, and her next single, \"Shake It Off\" moved into the number two spot (Ashanti had topped the chart in 2002 while being a \"featured\" singer on the number two single). On the \"Billboard\" Hot 100 Year-end Chart of 2005, the song was declared the number one song, a career first for Carey. \"Billboard\" listed \"We Belong Together\" ninth on The \"Billboard\" Hot 100 All-Time Top Songs and was declared the most popular song of the 2000s decade by \"Billboard\".\n\n\"The Emancipation of Mimi\" earned ten Grammy Award nominations: eight in 2006 for the original release (the most received by Carey in a single year), and two in 2007 for the \"Ultra Platinum Edition\" (from which \"Don't Forget About Us\" became her seventeenth number-one hit). Carey won Best Contemporary R&B Album and Best Female R&B Vocal Performance and Best R&B Song for \"We Belong Together\". \"The Emancipation of Mimi\" was the best-selling album in the United States in 2005, with nearly five million units sold. It was the first album by a solo female artist to become the year's best-selling album since Alanis Morissette's \"Jagged Little Pill\" in 1996. At the end of 2005, the IFPI reported that \"The Emancipation of Mimi\" had sold more than 7.7 million copies globally, and was the second-best-selling album of the year after Coldplay's \"X&Y\". To date, \"The Emancipation of Mimi\" has sold over 12 million copies worldwide.\n\nIn support of the album, Carey embarked on her first headlining tour in three years, named The Adventures of Mimi after a \"Carey-centric fan's\" music diary. The tour spanned 40 dates, with 32 in the United States and Canada, two in Africa, and six in Japan. It received warm reception from music critics and concert goers, many of which celebrated the quality of Carey's live vocals, as well as the show as a whole. Carey played to about 60,000 fans in the two shows in Tunis. A live recording titled \"The Adventures of Mimi\" DVD was released in November 2007 internationally and December 2007 in the U.S.\n\nBy spring 2007, Carey had begun to work on her eleventh studio album, \"E=MC²\", in a private villa in Anguilla. Although \"E=MC²\" was well received by most critics, some of them criticized it for being very similar to the formula used on \"The Emancipation of Mimi\". Two weeks before the album's release, \"Touch My Body\", the record's lead single, reached the top position on the \"Billboard\" Hot 100, becoming Carey's eighteenth number one and making her the solo artist with the most number one singles in United States history, pushing her past Elvis Presley into second place according to the magazine's revised methodology. Carey is second only to The Beatles, who have twenty number-one singles. Additionally, it gave Carey her 79th week atop the Hot 100, tying her with Presley as the artist with the most weeks at number one in the \"Billboard\" chart history.\"\n\"E=MC²\" debuted at number one on the \"Billboard\" 200 with 463,000 copies sold, the biggest opening week sales of her career. In 2008, Carey also played an aspiring singer named Krystal in \"Tennessee\" and had a cameo appearance in Adam Sandler's film \"You Don't Mess with the Zohan\", playing herself. Since the album's release, Carey had planned to embark on an extensive tour in support of \"E=MC²\". However the tour was suddenly cancelled in early December 2008. Carey later stated that she had been pregnant during that time period, and suffered a miscarriage, hence she cancelled the tour. On January 20, 2009, Carey performed \"Hero\" at the Neighborhood Inaugural Ball after Barack Obama was sworn as the first African-American president of the United States. On July 7, 2009, Carey – alongside Trey Lorenz – performed her version of The Jackson 5 song \"I'll Be There\" at the memorial service for Michael Jackson.\n\nIn 2009, she appeared as a social worker in \"Precious\", the movie adaptation of the 1996 novel \"Push\" by Sapphire. The film garnered mostly positive reviews from critics, also for Carey's performance. \"Variety\" described her acting as \"pitch-perfect.\" In January 2010, Carey won the Breakthrough Actress Performance Award for her role in \"Precious\" at the Palm Springs International Film Festival. On September 25, 2009, Carey's twelfth studio album, \"Memoirs of an Imperfect Angel\", was released. Reception for the album was mostly mixed; Stephen Thomas Erlewine of AllMusic called it \"her most interesting album in a decade,\" while Jon Caramanica from \"The New York Times\" criticized Carey's vocal performances, decrying her overuse of her softer vocal registers at the expense of her more powerful lower and upper registers. Commercially, the album debuted at number three on the \"Billboard\" 200, and became the lowest-selling studio album of her career. The album's lead single, \"Obsessed\", debuted at number eleven and peaked at number seven on the chart, and became Carey's 27th US top-ten hit, tying her with Elton John and Janet Jackson as the fifth most top-ten hits. The album's follow-up single, a cover of Foreigner's \"I Want to Know What Love Is\", managed to break airplay records in Brazil. The song spent 27 weeks atop the Brasil Hot 100 Airplay, making it the longest running song in the chart's history.\n\nOn December 31, 2009, Carey embarked her seventh concert tour, Angels Advocate Tour, which visited the United States and Canada and ended on September 26, 2010. A planned remix album of \"Memoirs of an Imperfect Angel\"; titled \"Angels Advocate\" was slated for a March 30, 2010 release, but was eventually cancelled.\n\nFollowing the cancellation of \"Angels Advocate\", it was announced that Carey would return to the studio to start work on her thirteenth studio album. It was later revealed that it would be her second Christmas album, and follow-up to \"Merry Christmas\". Longtime collaborators for the project included Jermaine Dupri, Johntá Austin, Bryan-Michael Cox, and Randy Jackson, as well as new collaborators such as Marc Shaiman. The release date for the album, titled \"Merry Christmas II You\", was November 2, 2010; the track list included six new songs as well as a remix of \"All I Want for Christmas Is You\". \"Merry Christmas II You\" debuted at number four on the \"Billboard\" 200 with sales of 56,000 copies, becoming Carey's 16th top ten album in the United States. The album debuted at number one on the R&B/Hip-Hop Albums chart, making it only the second Christmas album to top this chart.\nIn May 2010, Carey dropped out of her planned appearance in \"For Colored Girls\", the film adaptation of the play \"For Colored Girls Who Have Considered Suicide When the Rainbow Is Enuf\", citing medical reasons. In February 2011, Carey announced that she had officially began writing new material for her upcoming fourteenth studio album. Carey recorded a duet with Tony Bennett for his \"Duets II\" album, titled \"When Do The Bells Ring For Me?\" In October 2011, Carey announced that she re-recorded \"All I Want for Christmas Is You\" with Justin Bieber as a duet for his Christmas album, \"Under the Mistletoe\". In November 2011, Carey was included in the remix to the mixtape single \"Warning\" by Uncle Murda; the remix also features 50 Cent and Young Jeezy. That same month, Carey released a duet with John Legend titled \"When Christmas Comes\", originally part of \"Merry Christmas II You\".\n\nOn March 1, 2012, Carey performed at New York City's Gotham Hall; her first time performing since pregnancy. She also performed a three song set at a special fundraiser for US President Barack Obama held in New York's Plaza Hotel. A new song titled \"Bring It On Home\", which Carey wrote specifically for the event to show her support behind Obama's re-election campaign, was also performed. In August 2012, she released a stand alone single, \"Triumphant (Get 'Em)\", featuring American rappers Rick Ross and Meek Mill and co-written and co-produced by Carey, Jermaine Dupri, and Bryan-Michael Cox. Carey joined the judging panel of \"American Idol\" season twelve as Jennifer Lopez's replacement, joining Randy Jackson, Nicki Minaj and Keith Urban. In November 2013, she explained about hating to work at \"American Idol\" adding, \"It was like going to work every day in hell with Satan,\" referring to her on-set squabbles with Minaj. Carey appeared in Lee Daniels' 2013 film \"The Butler\", about a White House butler who served eight American presidents over the course of three decades. Carey made guest voice-star as a redneck character on the adult animated series \"American Dad!\" on November 24, 2013.\n\nIn February 2013 Carey recorded and released a song called \"Almost Home\", for the soundtrack of the Walt Disney Studios film \"Oz the Great and Powerful\". The video was directed by photographer David LaChapelle. News started coming around about the singer's fourteenth studio album. Some of the people that Carey worked with on the album included: DJ Clue?, Randy Jackson, Q-Tip, R. Kelly, David Morales, Loris Holland, Stevie J, James Fauntleroy II, Ray Angry, Afanasieff, Dupri, Bryan-Michael Cox, James \"Big Jim\" Wright, Hit-Boy, The-Dream, Da Brat, and Rodney Jerkins. Carey told \"Billboard\": \"It's about making sure I have tons of good music, because at the end of the day that's the most important thing... There are a lot more raw ballads than people might expect...there are also uptempo and signature-type songs that represent [my] different facets as an artist.\"\n\nThe lead single, \"Beautiful\" featuring singer Miguel, was released on May 6, 2013, and peaked at number 15 on the Hot 100. Carey taped a performance of \"Beautiful\" along with a medley of her greatest hits on May 15, 2013; the taping aired on the \"American Idol\" finale the following day. On October 14, 2013, Carey announced that the album's former title track has been chosen as the second single; it premiered via Facebook on November 11, 2013. During a Q&A session following the song's release, Carey gave an update about the album, stating: \"Now I've been inspired to add two more songs, so we're almost there. I can't even express this properly but I feel like this is gonna be my favorite album.\" Following another song release, \"You're Mine (Eternal)\", it was announced that \"The Art of Letting Go\" would no longer be the title of the album. After the final name was announced, \"Me. I Am Mariah... The Elusive Chanteuse\" was released on May 27, 2014.\n\nIn October 2014, Carey announced All I Want For Christmas Is You, A Night of Joy & Festivity, an annual residency show at the Beacon Theatre in New York City. The first leg included six shows, running from December 15–22, 2014. Carey announced the second leg in October 2015. The second leg ran for 8 shows, from December 8–18, 2015.\n\nOn January 30, 2015, it was announced that Carey had left Universal Music Group's Def Jam Recordings to reunite with L.A. Reid and Sony Music via Epic Records. Carey also announced her new #1 to Infinity residency at The Colosseum at Caesars Palace in Las Vegas the same month. To coincide with the residency, Carey released \"#1 to Infinity\", a greatest hits compilation album containing all of her eighteen \"Billboard\" Hot 100 number one singles, along with a new recording, \"Infinity\", which was released as a single on April 27. In 2015 Carey had her directorial debut for the Hallmark Channel Christmas movie \"A Christmas Melody\", in which she also performed as one of the main characters. Filming for the project took place during October 2015. In December 2015, Carey announced The Sweet Sweet Fantasy Tour which spanned a total of 27-dates beginning in March 2016, marking the first time the singer had done a significant tour of mainland Europe in 13 years. Four stops included shows in South Africa. The tour grossed 30.3 million dollars.\nOn March 15, 2016, Carey announced that she was filming \"Mariah's World\", a docu-series for the E! network documenting her Sweet Sweet Fantasy tour and her wedding planning process. Carey told \"The New York Times\", \"I thought it would be a good opportunity to kind of, like, show my personality and who I am, even though I feel like my real fans have an idea of who I am... A lot of people have misperceptions about this and that.\" The series premiered on December 4, 2016. Carey guest starred on the musical drama \"Empire\", as a superstar singer named Kitty and sung the song \"Infamous\" featuring Jussie Smollett. On December 5, 2016, Carey participated in the \"\" benefit concert, alongside Vanessa Williams, Chaka Khan, Patti Labelle, and Teyana Taylor. On December 31, 2016, on \"Dick Clark's New Year's Rockin' Eve\" in Times Square received worldwide attention after technical difficulties caused Carey's in-ear monitors to malfunction, resulting in what \"The New York Times\" referred to as a \"performance train wreck.\" The singer cited her inability to hear the music without in-ear auditory feedback as the cause for the mishap. Carey's representatives and Dick Clark Productions placed blame on each other.\n\nOn February 3, 2017, Carey released the single \"I Don't\" featuring YG. Later that month, she voiced the Mayor of Gotham City in the animated film \"The Lego Batman Movie\". In April 2017, it was announced that Carey is launching her own record label, Butterfly MC Records, a joint partnership with Epic Records. In July 2017, Carey made a cameo in the comedy film \"Girls Trip\", starring Queen Latifah, Jada Pinkett Smith, and Regina Hall. The same month, Carey embarked on a tour with Lionel Richie, titled, All the Hits Tour. Carey was also featured in the official remix for French Montana's single \"Unforgettable\", alongside Swae Lee. In October 2017, Carey released a new soundtrack single, \"The Star\", for the movie of the same name. Carey also developed an animated Christmas film, titled \"Mariah Carey's All I Want For Christmas Is You\", for which soundtrack she recorded an original song called \"Lil' Snowman.\" The film was released direct-to-video on November 14, 2017. In the same month, the singer resumed her All I Want for Christmas Is You, a Night of Joy and Festivity concert series, which for the first time visited other countries including England and France. On December 31, 2017, the singer returned to perform on \"Dick Clark's New Year's Rockin' Eve\" after the technical difficulties that hindered her previous performance, in what \"The New York Times\" described as a \"made-for-television act of pop culture redemption\".\n\nIn 2018, Carey signed a worldwide deal with Live Nation Entertainment. The first commitment out of the deal was her new Las Vegas residency, The Butterfly Returns, which was launched in July 2018 to critical acclaim. The 17-show residency has grossed $3.6 million so far, with shows continuing in 2019. Following the residency, Carey embarked on her tour in Asia and returned to Europe with her concert series. While on tour, a representative from Sony Music Asia Pacific presented Carey with a certificate for achieving 1.6 billion sales units in Asia Pacific. She also released professional live footage of her performance of \"All I Want For Christmas Is You\" during one of her European Christmas tour shows on YouTube.\n\nIn September 2018, Carey announced plans to release her fifteenth studio album later in the year. The project was announced alongside the release of a new song titled \"GTFO\", which she performed on September 21, 2018, when she headlined the 2018 iHeartRadio Music Festival. The album's lead single, \"With You\", was released in October and performed for the first time at the American Music Awards of 2018. The single became Carey's highest-charting non-holiday song on the US Adult Contemporary chart in over nine years and her highest-charting song on the Adult R&B Songs chart in over ten years. Later in October, Carey announced the name of her new album and released promotional singles \"The Distance\" featuring rapper Ty Dolla $ign and \"A No No\" in advance of its release. The album, titled \"Caution\", was released on November 16, 2018, and received universal acclaim from critics. \"Caution\" was described as a \"fine-tuning\" of Carey's previous work and was praised for its freshness which made it \"pleasingly defiant.\" By December 2018, the album had been featured on numerous year-end lists by music critics and publications.\n\nCarey is a philanthropist who has been involved with several charitable organizations. She became associated with the Fresh Air Fund in the early 1990s, and is the co-founder of a camp located in Fishkill, New York, that enables inner-city youth to embrace the arts and introduces them to career opportunities. The camp was called Camp Mariah \"for her generous support and dedication to Fresh Air children,\" and she received a Congressional Horizon Award for her youth-related charity work. Carey also donated royalties from her hits \"Hero\" and \"One Sweet Day\" to charities. She is well-known nationally for her work with the Make-A-Wish Foundation in granting the wishes of children with life-threatening illnesses, and in November 2006 she was awarded the Foundation's Wish Idol for her \"extraordinary generosity and her many wish granting achievements.\" Carey has volunteered for the Police Athletic League of New York City and contributed to the obstetrics department of New York Presbyterian Hospital Cornell Medical Center. A percentage of the sales of \"MTV Unplugged\" was donated to various other charities. In 2008, Carey was named Hunger Ambassador of the World Hunger Relief Movement. In February 2010, the song, \"100%\", which was originally written and recorded for the film, \"Precious\", was used as one of the theme songs for the 2010 Winter Olympics, with all money proceeds going to Team USA.\n\nOne of Carey's most high-profile benefit concert appearances was on VH1's 1998 \"Divas Live\" special, during which she performed alongside other female singers in support of the Save the Music Foundation. The concert was a ratings success, and Carey participated in the Divas 2000 special. In 2007, the Save the Music Foundation honored Carey at their tenth gala event for her support towards the foundation since its inception. She appeared at the \"\" nationally televised fundraiser in the aftermath of the September 11 attacks, and in December 2001, she performed before peacekeeping troops in Kosovo. Carey hosted the CBS television special \"At Home for the Holidays\", which documented real-life stories of adopted children and foster families. In 2005, Carey performed for Live 8 in London and at the Hurricane Katrina relief telethon \"Shelter from the Storm.\" In August 2008, Carey and other singers recorded the charity single, \"Just Stand Up\" produced by Babyface and L. A. Reid, to support Stand Up to Cancer. In 2008, Carey performed in a New Year's Eve concert for the family of Libyan dictator Muammar Gaddafi, something she later claimed to \"feel horrible and embarrassed to have participated in.\" In March 2011, Carey's representative Cindi Berger stated that royalties for the song \"Save The Day\", which was written for her fourteenth studio album, would be donated to charities that create awareness to human rights issues to make amends for the Gadaffi error. Berger also said that \"Mariah has and continues to donate her time, money and countless hours of personal service to many organizations both here and abroad.\" \"Save The Day\" was never released.\n\nDeclining offers to appear in commercials in the United States during her early career, Carey was not involved in brand marketing initiatives until 2006, when she participated in endorsements for Intel Centrino personal computers and launched a jewelry and accessories line for teenagers, Glamorized, in American Claire's and Icing stores. During this period, as part of a partnership with Pepsi and Motorola, Carey recorded and promoted a series of exclusive ringtones, including \"Time of Your Life\". She signed a licensing deal with the cosmetics company Elizabeth Arden, and in 2007, she released her own fragrance, \"M.\" The Elizabeth Arden deal has netted her $150 million. In 2007, \"Forbes\" named her as the fifth richest woman in entertainment, with an estimated net worth of US$270 million. In November 2011, it was reported that Carey's net worth was valued at more than $500 million. On November 29, 2010, she debuted a collection on HSN, which included jewelry, shoes and fragrances. In 2013, human rights activists criticized Carey for performing in a concert for Angola's \"father-daughter kleptocracy\" and accused her of accepting \"dictator cash.\"\n\nCarey has said that from childhood she has been influenced by Billie Holiday, Stevie Ray Vaughan, and R&B and soul musicians such as Al Green, Stevie Wonder, Gladys Knight and Aretha Franklin. Her music contains strong influences of gospel music, and she credits the Clark Sisters, Shirley Caesar and Edwin Hawkins as the most influential in her early years. When Carey incorporated hip hop into her sound, speculation arose that she was making an attempt to take advantage of the genre's popularity, but she told \"Newsweek\", \"People just don't understand. I grew up with this music.\" She has expressed appreciation for rappers such as the Sugarhill Gang, Eric B. & Rakim, the Wu-Tang Clan, The Notorious B.I.G. and Mobb Deep, with whom she collaborated on the single \"The Roof (Back in Time)\" (1998). Carey was heavily influenced by Minnie Riperton, and began experimenting with the whistle register due to her original practice of the range.\n\nDuring Carey's career, her vocal and musical style, along with her level of success, has been compared to Whitney Houston, who she has also cited as an influence, and Celine Dion. Carey and her peers, according to Garry Mulholland, are \"the princesses of wails [...] virtuoso vocalists who blend chart-oriented pop with mature MOR torch song.\" Author and writer Lucy O'Brien attributed the comeback of Barbra Streisand's \"old-fashioned showgirl\" to Carey and Dion, and described them and Houston as \"groomed, airbrushed and overblown to perfection.\" Carey's musical transition and use of more revealing clothing during the late 1990s were, in part, initiated to distance herself from this image, and she subsequently said that most of her early work was \"schmaltzy MOR.\" Some have noted that unlike Houston and Dion, Carey co-writes and produces her own songs.\n\nLove is the subject of the majority of Carey's lyrics, although she has written about themes such as racism, social alienation, death, world hunger, and spirituality. She has said that much of her work is partly autobiographical, but \"Time\" magazine wrote: \"If only Mariah Carey's music had the drama of her life. Her songs are often sugary and artificial—NutraSweet soul. But her life has passion and conflict,\" applying it to the first stages of her career. He commented that as her albums progressed, so too her songwriting and music blossomed into more mature and meaningful material. Jim Faber of the \"New York Daily News\", made similar comments, \"For Carey, vocalizing is all about the performance, not the emotions that inspired it. Singing, to her, represents a physical challenge, not an emotional unburdening.\" While reviewing \"Music Box\", Stephen Holden from \"Rolling Stone\" commented that Carey sang with \"sustained passion,\" while Arion Berger of \"Entertainment Weekly\" wrote that during some vocal moments, Carey becomes \"too overwhelmed to put her passion into words.\" In 2001, \"The Village Voice\" wrote in regards to what they considered Carey's \"centerless ballads,\" writing, \"Carey's Strawberry Shortcake soul still provides the template with which teen-pop cuties draw curlicues around those centerless [Diane] Warren ballads [...] it's largely because of [Blige] that the new R&B demands a greater range of emotional expression, smarter poetry, more from-the-gut testifying, and less [sic] unnecessary notes than the squeaky-clean and just plain squeaky Mariah era. Nowadays it's the Christina Aguileras and Jessica Simpsons who awkwardly oversing, while the women with roof-raising lung power keep it in check when tune or lyric demands.\"\n\nCarey's output makes use of electronic instruments such as drum machines, keyboards and synthesizers. Many of her songs contain piano-driven melodies, as she was given piano lessons when she was six years old. Carey said that she cannot read sheet music and prefers to collaborate with a pianist when composing her material, but feels that it is easier to experiment with faster and less-conventional melodies and chord progressions using this technique. While Carey learned to play the piano at a young age, and incorporates several ranges of production and instrumentation into her music, she has maintained that her voice has always been her most important asset: \"My voice is my instrument; it always has been.\" Carey began commissioning remixes of her material early in her career and helped to spearhead the practice of recording entirely new vocals for remixes. Disc jockey David Morales has collaborated with Carey on several occasions, starting with \"Dreamlover\" (1993), which popularized the tradition of remixing R&B songs into house records, and which \"Slant\" magazine named one of the greatest dance songs of all time. From \"Fantasy\" (1995) onward, Carey enlisted both hip-hop and house producers to re-structure her album compositions. \"Entertainment Weekly\" included two remixes of \"Fantasy\" on a list of Carey's greatest recordings compiled in 2005: a National Dance Music Award-winning remix produced by Morales, and a Sean Combs production featuring rapper Ol' Dirty Bastard. The latter has been credited with popularizing the R&B/hip-hop collaboration trend that has continued into the 2000s, through artists such as Ashanti and Beyoncé. Combs said that Carey \"knows the importance of mixes, so you feel like you're with an artist who appreciates your work—an artist who wants to come up with something with you.\"\n\nCarey possesses a five-octave vocal range, and has the ability to reach notes beyond the 7th octave. Referred to as the \"songbird supreme\" by the \"Guinness World Records\", she was ranked first in a 2003 MTV and \"Blender\" magazine countdown of the 22 Greatest Voices in Music, as voted by fans and readers in an online poll. Carey said of the poll: \"What it really means is voice of the MTV generation. Of course, it's an enormous compliment, but I don't feel that way about myself.\" She also placed second in \"Cove\" magazine's list of \"The 100 Outstanding Pop Vocalists.\"\n\nRegarding her voice type, Carey said that she is an alto, though several critics have described her as a Coloratura soprano. The singer claims that she has nodules in her vocal cords since childhood, due to which she can sing in a higher register than others. However, tiredness and sleep deprivation can affect her vocals due to the nodules, and Carey explained that she went through a lot of practice to maintain a balance during singing.\n\nJon Pareles of \"The New York Times\" described Carey's lower register as a \"rich, husky alto\" that extends to \"dog-whistle high notes.\" Additionally, towards the late 1990s, Carey began incorporating breathy vocals into her material. Tim Levell from the BBC News described her vocals as \"sultry close-to-the-mic breathiness,\" while \"USA Today\"s Elysa Gardner wrote \"it's impossible to deny the impact her vocal style, a florid blend of breathy riffing and resonant belting, has had on today's young pop and R&B stars.\"\n\nSasha Frere-Jones of \"The New Yorker\" adds her timbre possesses various colors, saying, \"Carey's sound changes with nearly every line, mutating from a steely tone to a vibrating growl and then to a humid, breathy coo. Her wide vocal range allows Carey to take melodies from alto bottom notes to coloratura soprano upper register.\" Carey also possesses a \"whisper register.\" In an interview with the singer, Ron Givens of \"Entertainment Weekly\" described it this way, \"first, a rippling, soulful ooh comes rolling effortlessly from her throat: alto. Then, after a quick breath, she goes for the stratosphere, with a sound that nearly changes the barometric pressure in the room. In one brief swoop, she seems to squeal and roar at the same time.\"\n\nHer sense of pitch is admired and Jon Pareles adds \"she can linger over sensual turns, growl with playful confidence, syncopate like a scat singer... with startlingly exact pitch.\"\n\nCarey's vocal style and singing ability have significantly impacted popular and contemporary music. As music critic G. Brown from \"The Denver Post\" wrote, \"For better or worse, Mariah Carey's five-octave range and melismatic style have influenced a generation of pop singers.\" According to \"Rolling Stone\", \"Her mastery of melisma, the fluttering strings of notes that decorate songs like \"Vision of Love\", inspired the entire \"American Idol\" vocal school, for better or worse, and virtually every other female R&B singer since the Nineties.\" Jody Rosen of \"Slate\" wrote of Carey's influence in modern music, calling her the most influential vocal stylist of the last two decades, the person who made rococo melismatic singing. Rosen further exemplified Carey's influence by drawing parallel with American Idol, which to her, \"often played out as a clash of melisma-mad Mariah wannabes. And, today, nearly 20 years after Carey's debut, major labels continue to bet the farm on young stars such as the winner of Britain's X Factor show, Leona Lewis, with her Generation Next gloss on Mariah's big voice and big hair.\" Sean Daly of \"St. Petersburg Times\" wrote, \"Depending on how you feel about public humiliation, the best/worst parts of American Idol are the audition shows, which normally break down into three distinct parts:(1) The Talented Kids.(2) The Weird Kids.(3) The Mariahs.\" Daly further commented, \"The Mariahs are the hardest ones to watch, mainly because most of them think they're reeeaaally good. The poor, disillusioned hopefuls plant themselves in front of judges Simon Cowell, Paula Abdul and Randy Jackson – then proceed to stretch, break and mutilate every note of a song, often Mariah's Hero, a tune that has ruined more throats than smoker's cough.\" \"New York Magazine's\" editor Roger Deckker said that in regarding Carey as an influential artist in music, he commented that \"Whitney Houston may have introduced melisma (the vocally acrobatic style of lending a word an extra syllable or twenty) to the charts, but it was Mariah—with her jaw-dropping range—who made it into America's default sound.\" Deckker also added that \"Every time you turn on American Idol, you are watching her children.\" Despite her vocal prowess, Carey's vocal technique particularly with the use of melisma and belting, has been subject to public scrutiny mainly because of young singers such as from talent shows have been overly imitating her singing technique in which critics commented \"Mariah Carey is, without a doubt, the worst thing to happen to amateur singing since the karaoke machine.\" As Professor Katherine L. Meizel noted in her book, \"The Mediation of Identity Politics in American Idol\", \"Carey's influence not just stops in the emulation of melisma or her singing amongst the wannabe's, it's also her persona, her diva, her stardom which inspires them... a pre-fame conic look.\"\n\nAmong the hip hop, pop, and R&B artists who have cited Carey as an influence are Aneeka, Ariana Grande, Britney Spears, Beyoncé, Katy Perry, Lady Gaga, Bridgit Mendler, Christina Aguilera, Jessica Simpson, Rihanna, Grimes, Kelly Clarkson, Nelly Furtado, Bonnie McKee, Leona Lewis, Brandy Norwood, Pink, Mary J. Blige, Melanie Fiona, Missy Elliott, Sam Smith, Hikaru Utada, Regine Velasquez, Sarah Geronimo, Charice Jordin Sparks, Justin Bieber, Jessica Sanchez, and Sandy. According to Stevie Wonder: \"When people talk about the great influential singers, they talk about Aretha, Whitney and Mariah. That's a testament to her talent. Her range is that amazing.\" Beyoncé credits Carey's singing and her song \"Vision of Love\" as influencing her to begin practicing vocal \"runs\" as a child, as well as helping her pursue a career as a musician. Rihanna has stated that Carey is one of her major influences and idol. Aguilera said in the early stages of her career that Carey was a big influence in her singing career and one of her idols. According to Pier Dominguez, author of \"Christina Aguilera: A star is made\", Aguilera has stated how she loved listening to Whitney Houston, but it was Carey who had the biggest influence on her vocal styling. Carey's carefully choreographed image of a grown woman struck a chord with Aguilera. Her influence on Aguilera also grew from the fact that both are of mixed heritage. Philip Brasor, editor of \"The Japan Times\", expressed how Carey's vocal and melismatic style even influenced Asian singers. He wrote that Japanese singer Hikaru Utada \"sang what she heard, from the diaphragm and with her own take on the kind of melisma that became de rigueur in American pop after the ascendance of Mariah Carey.\" In an article titled \"Out With Mariah's Melisma, In With Kesha's Kick\", writer David Browne of \"The New York Times\" discusses how the once ubiquitous melisma pop style suddenly lost in favor of the now ubiquitous autotune in which the former was heavily popularized by the likes of Mariah Carey and Whitney Houston. Browne had commented \"But beginning two decades ago, melisma overtook pop in a way it hadn't before. Mariah Carey's debut hit from 1990, \"Vision of Love\", followed two years later by Whitney Houston's version of \"I Will Always Love You\", set the bar insanely high for notes stretched louder, longer and knottier than most pop fans had ever heard.\" Browne further added \"A subsequent generation of singers, including Ms. Aguilera, Jennifer Hudson and Beyoncé, built their careers around melisma. (Men like Brian McKnight and Tyrese also indulged in it, but women tended to dominate the form.)\"\n\nCarey is also credited for introducing R&B and hip hop into mainstream pop culture, and for popularizing rap as a featuring act through her post-1995 songs. Sasha Frere-Jones, editor of \"The New Yorker\" commented, \"It became standard for R&B/hip-hop stars like Missy Elliott and Beyoncé, to combine melodies with rapped verses. And young white pop stars—including Britney Spears, Jessica Simpson, Christina Aguilera, and 'N Sync—have spent much of the past ten years making pop music that is unmistakably R&B.\" Moreover Jones concludes that \"[Carey's] idea of pairing a female songbird with the leading male MCs of hip-hop changed R&B and, eventually, all of pop. Although now anyone is free to use this idea, the success of \"The Emancipation of Mimi\" suggests that it still belongs to Carey.\" Judnick Mayard, writer of \"The Fader\", wrote that in regarding of R&B and hip hop collaboration, \"The champion of this movement is Mariah Carey.\" Mayard also expressed that \"To this day ODB and Mariah may still be the best and most random hip hop collaboration of all time,\" citing that due to the record \"Fantasy\", \"R&B and Hip-Hop were the best of step siblings.\" Kelefa Sanneh of \"The New York Times\" wrote, \"In the mid-1990s Ms. Carey pioneered a subgenre that some people call the thug-love duet. Nowadays clean-cut pop stars are expected to collaborate with roughneck rappers, but when Ms. Carey teamed up with Ol' Dirty Bastard, of the Wu-Tang Clan, for the 1995 hit \"Fantasy (Remix)\", it was a surprise, and a smash.\" Aside from her pop culture and musical influence, Carey is credited for releasing a classic Christmas song called \"All I Want for Christmas Is You.\" In a retrospective look at Carey's career, Sasha Frere-Jones of \"The New Yorker\" said, the \"charming\" song was one of Carey's biggest accomplishments, calling it \"one of the few worthy modern additions to the holiday canon.\" \"Rolling Stone\" ranked \"All I Want for Christmas Is You\" fourth on its Greatest Rock and Roll Christmas Songs list, calling it a \"holiday standard.\" In a review of her \"Greatest Hits\" album, Devon Powers of PopMatters writes that \"She has influenced countless female vocalists after her. At 32, she is already a living legend—even if she never sings another note.\" While reviewing a concert of Carey in Sydney, Elise Vout of \"MTV Australia\" wrote that \"it's not amazing choreography or high production value you're going to see, it's the larger than life personality, unique voice, and legend that is Mariah Carey.\"\n\nCarey began dating Tommy Mottola while recording \"Music Box\", and married him on June 5, 1993. After the release of \"Daydream\" and the success that followed, Carey began focusing on her personal life, which was a constant struggle at the time. Carey's relationship with Mottola began to deteriorate, due to their growing creative differences in terms of her albums, as well as his controlling nature. On May 30, 1997, the couple announced their separation, with their divorce finalized by the time Mottola remarried on December 2, 2000. Carey was in a three-year relationship with singer Luis Miguel from 1998 to 2001.\n\nCarey met actor and comedian Nick Cannon while they shot her music video for her song \"Bye Bye\" on an island off the coast of Antigua. On April 30, 2008, Carey married Cannon in The Bahamas. At 35 weeks into her pregnancy, she gave birth to their fraternal twins, Moroccan and Monroe, on April 30, 2011 via Cesarean section. Monroe is named after Marilyn Monroe; Moroccan is named after the Moroccan-decor room in Carey's apartment where Cannon proposed to her.\n\nIn August 2014, Cannon confirmed he and Carey had separated a few months earlier. He filed for divorce on December 12, 2014. It was finalized in 2016.\n\nOn January 21, 2016, Carey and Australian billionaire James Packer announced that they were engaged. By October, however, they had ended their engagement.\n\nCarey is an active Episcopalian. She stated in 2006: \"I do believe that I have been born again in a lot of ways. I think what I've changed are my priorities and my relationships with God. I feel the difference when I don't have my private moments to pray. ... I'm a fighter, but I learned that I'm not in charge. Whatever God wants to happen is what's going to happen. I feel like I've had endless second, third, fourth, fifth and sixth chances. It's by the grace of God I'm still here.\"\n\nIn April 2018, Carey opened up about her struggle with bipolar II disorder. She, self-reportedly, was diagnosed in 2001, but kept the diagnosis private. Recently, she has sought out treatment in the form of medication and therapy.\n\nThroughout Carey's career, she has collected many honors and awards, including the World Music Awards' Best Selling Female Artist of the Millennium, the Grammy's Best New Artist in 1991, and \"Billboard\"s Special Achievement Award for the Artist of the Decade during the 1990s. In a career spanning over 20 years, Carey has sold over 200 million records worldwide, making her one of the biggest-selling artists in music history. Carey is ranked as the best-selling female artist of the Nielsen SoundScan era, with over 52 million copies sold. Carey was ranked first in MTV and \"Blender\" magazine's 2003 countdown of the 22 Greatest Voices in Music, and was placed second in \"Cove\" magazine's list of \"The 100 Outstanding Pop Vocalists.\" Aside from her voice, she has become known for her songwriting. Yahoo Music editor Jason Ankeny wrote, \"She earned frequent comparison to rivals Whitney Houston and Celine Dion, but did them both one better by composing all of her own material.\" According to \"Billboard\" magazine, she was the most successful artist of the 1990s in the United States. At the 2000 World Music Awards, Carey was given a Legend Award for being the \"best-selling female pop artist of the millennium,\" as well as the \"Best-selling artist of the 90s\" in the United States, after releasing a series of albums of multiplatinum status in Asia and Europe, such as \"Music Box\" and \"Number 1's\". She is also a recipient of the Chopard Diamond award in 2003, recognizing sales of over 100 million albums worldwide. Additionally, the Recording Industry Association of America (RIAA) lists Carey as the third-best-selling female artist, with shipments of over 63 million units in the US. In Japan, Carey has the top four highest-selling albums of all time by a non-Asian artist.\n\nCarey has spent 79 weeks at the number-one position on \"Billboard\" Hot 100, the greatest number for any artist in US chart history. On that same chart, she has accumulated 18 number-one singles, the most for any solo artist (and second after The Beatles). Carey has also had three songs debut at the top of the Hot 100 chart. In 1994, Carey released her holiday album \"Merry Christmas\" has sold over 15 million copies worldwide, and is the best-selling Christmas album of all time. It also produced the successful single \"All I Want for Christmas Is You\", which became the only holiday song and ringtone to reach multi-platinum status in the US. In Japan, \"Number 1's\" has sold over 3,250,000 copies and is the best-selling album of all time in Japan by a non-Asian artist. Her hit single \"One Sweet Day\", which featured Boyz II Men, spent sixteen consecutive weeks at the top of \"Billboard\"s Hot 100 chart in 1996, setting the record for the most weeks atop the Hot 100 chart in history. After Carey's success in Asia with \"Merry Christmas\", \"Billboard\" estimated Carey as the all-time best-selling international artist in Japan. In 2008, \"Billboard\" listed \"We Belong Together\" ninth on The \"Billboard\" Hot 100 All-Time Top Songs and second on Top Billboard Hot 100 R&B/Hip-Hop Songs. The song was also declared the most popular song of the 2000s decade by \"Billboard\". In 2009, Carey's cover of Foreigner's song \"I Want to Know What Love Is\" became the longest-running number-one song in Brazilian singles chart history, spending 27 consecutive weeks at number-one. Additionally, Carey has had three songs debut at number-one on the \"Billboard\" Hot 100: \"Fantasy\", \"One Sweet Day\" and \"Honey\", making her the artist with the most number-one debuts in the chart's 52-year history. Also, she is the first female artist to debut at number 1 in the U.S. with \"Fantasy\". In 2010, Carey's 13th album and second Christmas album, \"Merry Christmas II You\", debuted at No. 1 on the R&B/Hip-Hop Albums chart, making it only the second Christmas album to top that chart. On November 19, 2010, \"Billboard\" magazine named Carey in their \"Top 50 R&B/Hip-Hop Artists of the Past 25 Years\" chart at number four. In 2012, Carey was ranked second on VH1's list of the \"100 Greatest Women in Music.\" \"Billboard\" magazine ranks her at number five on the \"Billboard\" Hot 100 All-Time Top Artists, making Carey the second most successful female artist in the history of the \"Billboard\" Hot 100 chart. In August 2015, Carey was honored with a star on the Hollywood Walk of Fame. In 2017, PETA gave her their \"Angel for Animals Award,\" in honor of her work on the animated film \"All I Want for Christmas Is You\", in which a young girl adopts a homeless dog.\n\n\n\n\n\n\n\n"}
{"id": "19500", "url": "https://en.wikipedia.org/wiki?curid=19500", "title": "Mervyn Peake", "text": "Mervyn Peake\n\nMervyn Laurence Peake (9 July 1911 – 17 November 1968) was an English writer, artist, poet, and illustrator. He is best known for what are usually referred to as the \"Gormenghast\" books. The three works were part of what Peake conceived as a lengthy cycle, the completion of which was prevented by his death. They are sometimes compared to the work of his older contemporary J. R. R. Tolkien, but his surreal fiction was influenced by his early love for Charles Dickens and Robert Louis Stevenson rather than Tolkien's studies of mythology and philology.\n\nPeake also wrote poetry and literary nonsense in verse form, short stories for adults and children (\"Letters from a Lost Uncle\"), stage and radio plays, and \"Mr Pye\", a relatively tightly-structured novel in which God implicitly mocks the evangelical pretensions and cosy world-view of the eponymous hero.\n\nPeake first made his reputation as a painter and illustrator during the 1930s and 1940s, when he lived in London, and he was commissioned to produce portraits of well-known people. For a short time at the end of World War II he was commissioned by various newspapers to depict war scenes. A collection of his drawings is still in the possession of his family. Although he gained little popular success in his lifetime, his work was highly respected by his peers, and his friends included Dylan Thomas and Graham Greene. His works are now included in the collections of the National Portrait Gallery, the Imperial War Museum and The National Archives.\n\nIn 2008, \"The Times\" named Peake among their list of \"The 50 greatest British writers since 1945\".\n\nMervyn Peake was born of British parents in Kuling (Lushan) in Jiangxi Province of central China in 1911, only three months before the revolution and the founding of the Republic of China. His father Ernest Cromwell Peake was a medical missionary doctor with the London Missionary Society of the Congregationalist tradition and his mother, Amanda Elizabeth Powell, had come to China as a missionary assistant.\n\nThe Peakes were given leave to visit England just before World War I in 1914 and returned to China in 1916. Mervyn Peake attended Tientsin Grammar School until the family left for England in December 1922 via the Trans-Siberian Railway. About this time he wrote a novella, \"The White Chief of the Umzimbooboo Kaffirs\". Peake never returned to China but it has been noted that Chinese influences can be detected in his works, not least in the castle of Gormenghast itself, which in some respects echoes the ancient walled city of Beijing, as well as the enclosed compound where he grew up in Tianjin. It is also likely that his early exposure to the contrasts between the lives of the Europeans and of the Chinese, and between the poor and the wealthy in China, also exerted an influence on the Gormenghast books.\n\nHis education continued at Eltham College, Mottingham (1923–29), where his talents were encouraged by his English teacher, Eric Drake. Peake completed his formal education at Croydon School of Art in the autumn of 1929 and then from December 1929 to 1933 at the Royal Academy Schools, where he first painted in oils. By this time he had written his first long poem, \"A Touch o' the Ash\". In 1931 he had a painting accepted for display by the Royal Academy and exhibited his work with the so-called \"Soho Group\".\n\nHis early career in the 1930s was as a painter in London, although he lived on the Channel Island of Sark for a time. He first moved to Sark in 1932 where his former teacher Eric Drake was setting up an artists' colony. In 1934 he exhibited with the Sark artists both in the Sark Gallery built by Drake and at the Cooling Galleries in London. In 1935 he exhibited at the Royal Academy and at the Leger Galleries in London.\n\nIn 1936 he returned to London and was commissioned to design the sets and costumes for \"Insect Play\" and his work was acclaimed in \"The Sunday Times\". He also began teaching life drawing at Westminster School of Art where he met the painter Maeve Gilmore, whom he married in 1937. They had three children, Sebastian (1940–2012), Fabian (b. 1942), and Clare (b. 1949).\n\nHe had a very successful exhibition of paintings at the Calmann Gallery in London in 1938 and his first book, the self-illustrated children's pirate romance \"Captain Slaughterboard Drops Anchor\" (based on a story he had written around 1936) was first published in 1939 by \"Country Life\". In December 1939 he was commissioned by Chatto & Windus to illustrate a children's book, \"Ride a Cock Horse and Other Nursery Rhymes\", published for the Christmas market in 1940.\n\nAt the outbreak of World War II he applied to become a war artist for he was keen to put his skills at the service of his country. He imagined \"An Exhibition by the Artist, Adolf Hitler\", in which horrific images of war with ironic titles were offered as 'artworks' by the Nazi leader. Although the drawings were bought by the British Ministry of Information, his application was turned down and he was conscripted into the Army, where he served first with the Royal Artillery, then with the Royal Engineers. He began writing \"Titus Groan\" at this time.\n\nIn April 1942, after his requests for commissions as a war artist – or even leave to depict war damage in London – had been consistently refused, he suffered a nervous breakdown and was sent to Southport Hospital. That autumn he was taken on as a graphic artist by the Ministry of Information for a period of six months to work on propaganda illustrations. The next spring he was invalided out of the Army. In 1943 he was commissioned by the War Artists' Advisory Committee, WAAC, to paint glassblowers at the Chance Brothers factory in Birmingham where cathode ray tubes for early radar sets were being produced. Peake was next given a full-time, three-month WAAC contract to depict various factory subjects and was also asked to submit a large painting showing RAF pilots being debriefed. Some of these paintings are on permanent display in Manchester Art Gallery whilst other examples are in the Imperial War Museum collection.\n\nThe five years between 1943 and 1948 were some of the most productive of his career. He finished \"Titus Groan\" and \"Gormenghast\" and completed some of his most acclaimed illustrations for books by other authors, including Lewis Carroll's \"Hunting of the Snark\" (for which he was reportedly paid only £5) and \"Alice in Wonderland\", Samuel Taylor Coleridge's \"The Rime of the Ancient Mariner\", the Brothers Grimm's \"Household Tales\", \"All This and Bevin Too\" by Quentin Crisp and Robert Louis Stevenson's \"Strange Case of Dr Jekyll and Mr Hyde\", as well as producing many original poems, drawings, and paintings.\n\nPeake designed the logo for Pan Books. The publishers offered him either a flat fee of £10 or a royalty of one farthing per book. On the advice of Graham Greene, who told him that paperback books were a passing fad that wouldn't last, Peake opted for the £10.\n\nA book of nonsense poems, \"Rhymes Without Reason\", was published in 1944 and was described by John Betjeman as \"outstanding\". Shortly after the war ended in 1945, Edgar Ainsworth, the art editor of \"Picture Post\" commissioned Peake to visit France and Germany for the magazine. With writer Tom Pocock he was among the first British civilians to witness the horrors of the Nazi concentration camp at Belsen, where the remaining prisoners, too sick to be moved, were dying before his very eyes. He made several drawings, but not surprisingly he found the experience profoundly harrowing, and expressed in deeply felt poems the ambiguity of turning their suffering into art.\n\nIn 1946 the family moved to Sark, where Peake continued to write and illustrate, and Maeve painted. \"Gormenghast\" was published in 1950, and the family moved back to England, settling in Smarden, Kent. Peake taught part-time at the Central School of Art, began his comic novel \"Mr Pye\", and renewed his interest in theatre. His father died that year and left his house in Hillside Gardens in Wallington, Surrey to Mervyn. \"Mr Pye\" was published in 1953, and he later adapted it as a radio play. The BBC broadcast other plays of his in 1954 and 1956.\n\nIn 1956 Mervyn and Maeve visited Spain, financed by a friend who hoped that Peake's health, which was already declining, would be improved by the holiday. That year his novella \"Boy in Darkness\" was published beside stories by William Golding and John Wyndham in a volume called \"Sometime, Never\". On 18 December the BBC broadcast his radio play \"The Eye of the Beholder\" (later revised as \"The Voice of One\") in which an avant-garde artist is commissioned to paint a church mural. Peake placed much hope in his play \"The Wit To Woo\" which was finally staged in London's West End in 1957, but it was a critical and commercial failure. This affected him greatly – his health degenerated rapidly and he was again admitted to hospital with a nervous breakdown.\n\nHe was showing unmistakable early symptoms of dementia, for which he was given electroconvulsive therapy, to little avail. Over the next few years he gradually lost the ability to draw steadily and quickly, although he still managed to produce some drawings with the help of his wife. Among his last completed works were the illustrations for Balzac's \"Droll Stories\" (1961) and for his own poem \"The Rhyme of the Flying Bomb\" (1962), which he had written some 15 years earlier.\n\n\"Titus Alone\" was published in 1959 and was revised in 1970 by Langdon Jones, editor of \"New Worlds\", to remove apparent inconsistencies introduced by the publisher's careless editing. A 1995 edition of all three completed Gormenghast novels includes a very short fragment of the beginning of what would have been the fourth Gormenghast novel, \"Titus Awakes\", as well as a listing of events and themes he wanted to address in that and later Gormenghast novels.\n\nThroughout the 1960s, Peake's health declined into physical and mental incapacitation, and he died on 17 November 1968 at a care home run by his brother-in-law, at Burcot, near Oxford. He was buried in the churchyard of St Mary's in the village of Burpham, Sussex.\n\nA 2003 study published in JAMA Neurology assessed that Peake's death was the result of dementia with Lewy bodies (DLB).\n\nHis work, especially the \"Gormenghast\" series, became much better known and more widely appreciated after his death. They have since been translated into more than two dozen languages.\n\nSix volumes of Peake's verse were published during his lifetime; \"Shapes & Sounds\" (1941), \"Rhymes without Reason\" 1944, \"The Glassblowers\" (1950), \"The Rhyme of the Flying Bomb\" (1962), \"Poems & Drawings\" (1965), and \"A Reverie of Bone\" (1967). After his death came \"Selected Poems\" (1972), followed by \"Peake's Progress\" in (1979 – though the Penguin edition of 1982, with many corrections, including a whole stanza inadvertently omitted from the hardback edition, is to be preferred). \"The Collected Poems of Mervyn Peake\" was published by Carcanet Press in June 2008. Other collections include \"The Drawings of Mervyn Peake\" (1974), \"Writings and Drawings\" (1974), and \"Mervyn Peake: the man and his art\" (2006). An extremely expensive limited edition of the collected works, issued to celebrate Peake's centenary year, was published by Queen Anne Press, but the editing and reproduction of drawings did not match the price asked.\n\nIn 2010 an archive consisting of 28 containers of material, which included correspondence between Peake and Laurie Lee, Walter de la Mare and C. S. Lewis, plus 39 Gormenghast notebooks and original drawings for both \"Alice Through the Looking Glass\" and \"Alice's Adventures in Wonderland\", was acquired by the British Library. Peake's three children presented on BBC Radio Four in 2018 a half-hour memoir of their father's life, emphasizing the importance of the island of Sark. \n\nIn 1983, the Australian Broadcasting Corporation broadcast eight hour-long episodes for radio dramatising the complete Gormenghast Trilogy. This was the first to include the third book \"Titus Alone\".\n\nIn 1984, BBC Radio 4 broadcast two 90-minute plays based on \"Titus Groan\" and \"Gormenghast\", adapted by Brian Sibley and starring Sting as Steerpike and Freddie Jones as the Artist (narrator). A slightly abridged compilation of the two, running to 160 minutes, and entitled \"Titus Groan of Gormenghast\", was broadcast on Christmas Day, 1992. BBC 7 repeated the original versions on 21 and 28 September 2003.\n\nIn 1986, \"Mr Pye\" was adapted as a four-part Channel 4 miniseries starring Derek Jacobi.\n\nIn 2000, the BBC and WGBH Boston co-produced a lavish miniseries, titled \"Gormenghast\", based on the first two books of the series. It starred Jonathan Rhys-Meyers as Steerpike, Neve McIntosh as Fuchsia, June Brown as Nannie Slagg, Ian Richardson as Lord Groan, Christopher Lee as Flay, Richard Griffiths as Swelter, Warren Mitchell as Barquentine, Celia Imrie as Countess Gertrude, Lynsey Baxter and Zoë Wanamaker as the twins, Cora and Clarice, and John Sessions as Dr Prunesquallor. The supporting cast included Olga Sosnovska, Stephen Fry and Eric Sykes and the series is also notable as the last screen performance by comedy legend Spike Milligan (as the Headmaster).\n\nA 30-minute TV short film \"A Boy in Darkness\" (also made in 2000 and adapted from Peake's novella) was the first production from the BBC Drama Lab. It was set in a 'virtual' computer-generated world created by young computer game designers, and starred Jack Ryder (from \"EastEnders\") as Titus, with Terry Jones (\"Monty Python's Flying Circus\") narrating.\n\nIrmin Schmidt, founder of seminal German 'Krautrock' group Can wrote an opera called \"Gormenghast\", based on the novels; it was first performed in Wuppertal, Germany, in November 1998. A number of early songs by New Zealand rock group Split Enz were inspired by Peake's work. The song \"The Drowning Man\", by British band The Cure, is inspired by events in \"Gormenghast,\" and the song \"Lady Fuchsia\" by another British band, Strawbs, is also based on events in the novels.\n\nPeake's play \"The Cave\", which dates from the mid-1950s, was given a first public reading at the Blue Elephant Theatre in Camberwell (London) in 2009, and had its world premiere in the same theatre, directed by Aaron Paterson, on 19 October 2010.\n\nIn 2011 Brian Sibley adapted the story again, this time as six one-hour episodes broadcast on BBC Radio 4 as the Classic Serial starting on 10 July 2011. The serial was titled \"The History of Titus Groan\" and adapted all three novels written by Mervyn Peake and the recently discovered concluding volume, \"Titus Awakes\" completed by his widow, Maeve Gilmore. It starred Luke Treadaway as Titus, David Warner as the Artist and Carl Prekopp as Steerpike. It also starred Paul Rhys, Miranda Richardson, James Fleet, Tamsin Greig, Fenella Woolgar, Adrian Scarborough and Mark Benton among others.\n\nSting owned the film rights to the \"Gormenghast\" novels for a brief period in the 1980s, during which he discussed the possibility of adapting the novels into a series of concept albums, but he abandoned the idea after declaring the Radio 4 audio drama as ideal. As of 2015, author Neil Gaiman was in talks to adapt the novels for the big screen.\n\nGormenghast\n\"Boy in Darkness and other stories\" (2007, the correct text and five other pieces)\n\nOther Works\n\n\n\n"}
{"id": "19501", "url": "https://en.wikipedia.org/wiki?curid=19501", "title": "Martial arts", "text": "Martial arts\n\nMartial arts are codified systems and traditions of combat practiced for a number of reasons such as self-defense, military and law enforcement applications, physical, mental and spiritual development; as well as entertainment and the preservation of a nation's intangible cultural heritage.\n\nAlthough the term \"martial art\" has become associated with the fighting arts of East Asia, it originally referred to the combat systems of Europe as early as the 1550s. The term is derived from Latin and means \"arts of Mars\", the Roman god of war. Some authors have argued that fighting arts or fighting systems would be more appropriate on the basis that many martial arts were never \"martial\" in the sense of being used or created by professional warriors.\nMartial arts may be categorized along a variety of criteria, including:\n\nUnarmed martial arts can be broadly grouped into focusing on strikes, those focusing on grappling and those that cover both fields, often described as hybrid martial arts.\n\nStrikes\n\nGrappling\n\n\nThe traditional martial arts, which train in armed combat, often encompass a wide spectrum of melee weapons, including bladed weapons and polearms. Such traditions include eskrima, silat, kalaripayat, kobudo, and historical European martial arts, especially those of the German Renaissance. Many Chinese martial arts also feature weapons as part of their curriculum.\n\nSometimes, training with one specific weapon will be considered a style of martial arts in its own right, which is especially the case in Japanese martial arts with disciplines such as kenjutsu and kendo (sword), bojutsu (staff), and kyudo (archery). Similarly, modern martial arts and sports include modern fencing, stick-fighting systems like canne de combat and jogo do pau, and modern competitive archery.\n\n\nMany martial arts, especially those from Asia, also teach side disciplines which pertain to medicinal practices. This is particularly prevalent in traditional Asian martial arts which may teach bone-setting, herbalism, and other aspects of traditional medicine.\n\nMartial arts can also be linked with religion and spirituality. Numerous systems are reputed to have been founded, disseminated, or practiced by monks or nuns.\n\nThroughout Asia, meditation may be incorporated as part of training. In those countries influenced by Hindu-Buddhist philosophy, the art itself may be used as an aid to attaining enlightenment.\n\nJapanese styles, when concerning non-physical qualities of the combat, are often strongly influenced by Mahayana Buddhist philosophy. Concepts like \"empty mind\" and \"beginner's mind\" are recurrent. Aikido, for instance, can have a strong philosophical belief of the flow of energy and peace fostering, as idealised by its founder Morihei Ueshiba.\n\nTraditional Korean martial arts place emphasis on the development of the practitioner's spiritual and philosophical development. A common theme in most Korean styles, such as taekkyeon and taekwondo, is the value of \"inner peace\" in a practitioner, which is stressed to be only achieved through individual meditation and training. The Koreans believe that the use of physical force is only justified through defense.\n\nSystema draws upon breathing and relaxation techniques, as well as elements of Russian Orthodox thought, to foster self-conscience and calmness, and to benefit the practitioner in different levels: the physical, the psychological and the spiritual.\n\nSome martial arts in various cultures can be performed in dance-like settings for various reasons, such as for evoking ferocity in preparation for battle or showing off skill in a more stylized manner. Many such martial arts incorporate music, especially strong percussive rhythms. (See also war dance.)\n\nThe oldest works of art depicting scenes of battle are cave paintings from eastern Spain (Spanish Levant) dated between 10,000 and 6,000 BCE that show organized groups fighting with bows and arrows.\n\nChinese martial arts originated during the legendary, possibly apocryphal, Xia Dynasty more than 4000 years ago. It is said the Yellow Emperor Huangdi (legendary date of ascension 2698 BC) introduced the earliest fighting systems to China. The Yellow Emperor is described as a famous general who before becoming China's leader, wrote lengthy treatises on medicine, astrology and martial arts. One of his main opponents was Chi You who was credited as the creator of jiao di, a forerunner to the modern art of Chinese wrestling.\n\nThe foundation of modern Asian martial arts is likely a blend of early Chinese and Indian martial arts. During the Warring States period of Chinese history (480-221 BC) extensive development in martial philosophy and strategy emerged, as described by Sun Tzu in \"The Art of War\" (c. 350 BC). Legendary accounts link the origin of Shaolinquan to the spread of Buddhism from ancient India during the early 5th century AD, with the figure of Bodhidharma, to China. Written evidence of martial arts in Southern India dates back to the Sangam literature of about the 2nd century BC to the 2nd century AD. The combat techniques of the Sangam period were the earliest precursors to Kalaripayattu.\nIn Europe, the earliest sources of martial arts traditions date to Ancient Greece. Boxing (\"pygme\", \"pyx\"), wrestling (\"pale\") and pankration were represented in the Ancient Olympic Games. The Romans produced gladiatorial combat as a public spectacle.\n\nA number of historical combat manuals have survived from the European Middle Ages. This includes such styles as sword and shield, two-handed swordfighting and other types of melee weapons besides unarmed combat. Amongst these are transcriptions of Johannes Liechtenauer's mnemonic poem on the longsword dating back to the late fourteenth century. Likewise, Asian martial arts became well-documented during the medieval period, Japanese martial arts beginning with the establishment of the samurai nobility in the 12th century, Chinese martial arts with Ming era treatises such as Ji Xiao Xin Shu, Indian martial arts in medieval texts such as the Agni Purana and the Malla Purana, and Korean martial arts from the Joseon era and texts such as Muyejebo (1598).\n\nEuropean swordsmanship always had a sportive component, but the duel was always a possibility until World War I. Modern sport fencing began developing during the 19th century as the French and Italian military academies began codifying instruction. The Olympic games led to standard international rules, with the Féderation Internationale d'Escrime founded in 1913. Modern boxing originates with Jack Broughton's rules in the 18th century, and reaches its present form with the Marquess of Queensberry Rules of 1867.\n\nCertain traditional combat sports and fighting styles exist all over the world, rooted in local culture and folklore. The most common of these are styles of folk wrestling, some of which have been practiced since antiquity and are found in the most remote areas. Other examples include forms of stick fighting and boxing. While these arts are based on historical traditions of folklore, they are not \"historical\" in the sense that they reconstruct or preserve a historical system from a specific era. They are rather contemporary regional sports that coexist with the modern forms of martial arts sports as they have developed since the 19th century, often including cross-fertilization between sports and folk styles; thus, the traditional Thai art of muay boran developed into the modern national sport of muay Thai, which in turn came to be practiced worldwide and contributed significantly to modern hybrid styles like kickboxing and mixed martial arts. Singlestick, an English martial art can be seen often utilized in morris dancing. Many European dances share elements of martial arts with examples including Ukrainian Hopak, Polish Zbójnicki (use of ciupaga), the Czech dance odzemek, and the Norwegian Halling.\n\nThe mid to late 19th century marks the beginning of the history of martial arts as modern sports developed out of earlier traditional fighting systems. In Europe, this concerns the developments of boxing and fencing as sports. In Japan, the same period marks the formation of the modern forms of judo, jujutsu, karate, and kendo (among others) based on revivals of old schools of Edo period martial arts which had been suppressed during the Meiji Restoration. Modern muay Thai rules date to the 1920s. In China, the modern history of martial arts begins in the Nanjing decade (1930s) following the foundation of the Central Guoshu Institute in 1928 under the Kuomintang government.\n\nWestern interest in Asian martial arts arises towards the end of the 19th century, due to the increase in trade between the United States with China and Japan. Relatively few Westerners actually practiced the arts, considering it to be mere performance. Edward William Barton-Wright, a railway engineer who had studied jujutsu while working in Japan between 1894 and 1897, was the first man known to have taught Asian martial arts in Europe. He also founded an eclectic style named Bartitsu which combined jujutsu, judo, wrestling, boxing, savate and stick fighting.\n\nFencing and Greco-Roman wrestling was included in the 1896 Summer Olympics.\nFILA Wrestling World Championships and Boxing at the Summer Olympics were introduced in 1904.\nThe tradition of awarding championship belts in wrestling and boxing can be traced to the Lonsdale Belt, introduced in 1909.\n\nThe International Boxing Association was established in 1920. World Fencing Championships have been held since 1921.\n\nAs Western influence grew in Asia a greater number of military personnel spent time in China, Japan and South Korea during World War II and the Korean War and were exposed to local fighting styles. Jujutsu, judo and karate first became popular among the mainstream from the 1950s-60s. Due in part to Asian and Hollywood martial arts movies, most modern American martial arts are either Asian-derived or Asian influenced.\nThe term kickboxing (キックボクシング) was created by the Japanese boxing promoter Osamu Noguchi for a variant of muay Thai and karate that he created in the 1950s. American kickboxing was developed in the 1970s, as a combination of boxing and karate. Taekwondo was developed in the context of the Korean War in the 1950s.\n\nThe later 1960s and 1970s witnessed an increased media interest in Chinese martial arts, influenced by martial artist Bruce Lee. Bruce Lee is credited as one of the first instructors to openly teach Chinese martial arts to Westerners. World Judo Championships have been held since 1956, Judo at the Summer Olympics was introduced in 1964. Karate World Championships were introduced in 1970.\n\nFollowing the \"kung fu wave\" in Hong Kong action cinema in the 1970s, a number of mainstream films produced during the 1980s contributed significantly to the perception of martial arts in western popular culture. These include \"The Karate Kid\" (1984) and \"Bloodsport\" (1988). This era produced some Hollywood action stars with martial arts background, such as Jean-Claude Van Damme and Chuck Norris.\n\nAlso during the 20th century, a number of martial arts were adapted for self-defense purposes for military hand-to-hand combat. World War II combatives, KAPAP (1930s) and Krav Maga (1950s) in Israel, Systema in Soviet-era Russia, and Sanshou in the People's Republic of China are examples of such systems. The US military de-emphasized hand-to-hand combat training during the Cold War period, but revived it with the introduction of LINE in 1989.\n\nDuring the 1990s Brazilian jiu-jitsu became popular and proved to be effective in mixed martial arts competitions such as the UFC and PRIDE.\n\nIn 1993 the first Pancrase event was held. The K-1 rules of kickboxing were introduced based on 1980s Seidokaikan karate.\n\nJackie Chan and Jet Li are prominent movie figures who have been responsible for promoting Chinese martial arts in recent years.\n\nWith the continual discovery of more medieval and Renaissance fighting manuals, the practice of Historical European Martial Arts and other Western Martial Arts are growing in popularity across the United States and Europe.\n\nNovember 29, 2011, UNESCO inscribed taekkyeon onto its Intangible Cultural Heritage of Humanity List.\n\nMany martial arts which originated in Southern India were banned by the government of the British Raj, few of them which barely survived are Kalaripayattu and Silambam. These and other martial arts survived by telling the British government it was a form of dance. Varma kalai, a martial arts concentrating on vital points, was almost dead but is gradually being revived.\n\nTesting or evaluation is important to martial artists of many disciplines who wish to determine their progression or own level of skill in specific contexts. Students often undergo periodic testing and grading by their own teacher in order to advance to a higher level of recognized achievement, such as a different belt color or title. The type of testing used varies from system to system but may include forms or sparring.\n\nVarious forms and sparring are commonly used in martial art exhibitions and tournaments. Some competitions pit practitioners of different disciplines against each other using a common set of rules, these are referred to as mixed martial arts competitions. Rules for sparring vary between art and organization but can generally be divided into \"light-contact\", \"medium-contact\", and \"full-contact\" variants, reflecting the amount of force that should be used on an opponent.\n\nThese types of sparring restrict the amount of force that may be used to hit an opponent, in the case of light sparring this is usually to 'touch' contact, e.g. a punch should be 'pulled' as soon as or before contact is made. In medium-contact (sometimes referred to as semi-contact) the punch would not be 'pulled' but not hit with full force. As the amount of force used is restricted, the aim of these types of sparring is not to knock out an opponent; a point system is used in competitions.\n\nA referee acts to monitor for fouls and to control the match, while judges mark down scores, as in boxing. Particular targets may be prohibited, certain techniques may be forbidden (such as headbutting or groin hits), and fighters may be required to wear protective equipment on their head, hands, chest, groin, shins or feet. Some grappling arts, such as aikido, use a similar method of compliant training that is equivalent to light or medium contact.\n\nIn some styles (such as fencing and some styles of taekwondo sparring), competitors score points based on the landing of a single technique or strike as judged by the referee, whereupon the referee will briefly stop the match, award a point, then restart the match. Alternatively, sparring may continue with the point noted by the judges. Some critics of point sparring feel that this method of training teaches habits that result in lower combat effectiveness. Lighter-contact sparring may be used exclusively, for children or in other situations when heavy contact would be inappropriate (such as beginners), medium-contact sparring is often used as training for full contact.\n\nFull-contact sparring or competition, where strikes or techniques are not pulled but used with full force as the name implies, has a number of tactical differences from light and medium-contact sparring. It is considered by some to be requisite in learning realistic unarmed combat.\n\nIn full-contact sparring, the aim of a competitive match is to knock out the opponent or to force the opponent to submit.\nWhere scoring takes place it may be a subsidiary measure, only used if no clear winner has been established by other means; in some competitions, such as the UFC 1, there was no scoring, though most now use some form of judging as a backup. Due to these factors, full-contact matches tend to be more aggressive in character, but rule sets may still mandate the use of protective equipment, or limit the techniques allowed.\n\nNearly all mixed martial arts organizations such as UFC, Pancrase, Shooto use a form of full-contact rules as do professional boxing organizations and K-1. Kyokushin karate requires advanced practitioners to engage in bare-knuckled, full-contact sparring allowing kicks, knees and punching although punching to the head is disallowed while wearing only a karate \"gi\" and groin protector. Brazilian jiu-jitsu and judo matches do not allow striking, but are full-contact in the sense that full force is applied in the permitted grappling and submission techniques. Competitions held by the World Taekwondo Federation requires the use of Headgear and padded vest, but are full contact in the sense that full force is applied to strikes to the head and body, and win by knockout is possible.\n\nMartial arts have crossed over into sports when forms of sparring become competitive, becoming a sport in its own right that is dissociated from the original combative origin, such as with western fencing. The Summer Olympic Games includes judo, taekwondo, western archery, boxing, javelin, wrestling and fencing as events, while Chinese wushu recently failed in its bid to be included, but is still actively performed in tournaments across the world. Practitioners in some arts such as kickboxing and Brazilian jiu-jitsu often train for sport matches, whereas those in other arts such as aikido generally spurn such competitions. Some schools believe that competition breeds better and more efficient practitioners, and gives a sense of good sportsmanship. Others believe that the rules under which competition takes place have diminished the combat effectiveness of martial arts or encourage a kind of practice which focuses on winning trophies rather than a focus such as cultivating a particular moral character.\n\nThe question of \"which is the best martial art\" has led to inter style competitions fought with very few rules allowing a variety of fighting styles to enter with few limitations. This was the origin of the first Ultimate Fighting Championship tournament (later renamed UFC 1: The Beginning) in the U.S. inspired by the Brazilian Vale tudo tradition and along with other minimal rule competitions, most notably those from Japan such as Shooto and Pancrase, have evolved into the combat sport of Mixed Martial Arts (MMA).\n\nSome martial artists compete in non-sparring competitions such as breaking or choreographed routines of techniques such as poomse, kata and aka, or modern variations of the martial arts which include dance-influenced competitions such as tricking. Martial traditions have been influenced by governments to become more sport-like for political purposes; the central impetus for the attempt by the People's Republic of China in transforming Chinese martial arts into the committee-regulated sport of wushu was suppressing what they saw as the potentially subversive aspects of martial training, especially under the traditional system of family lineages.\n\nMartial arts training aims to result in several benefits to trainees, such as their physical, mental, emotional and spiritual health.\n\nThrough systematic practice in the martial arts a person's physical fitness may be boosted (strength, stamina, speed, flexibility, movement coordination, etc.) as the whole body is exercised and the entire muscular system is activated. \nBeyond contributing to physical fitness, martial arts training also has benefits for mental health, contributing to self-esteem, self-control, emotional and spiritual well-being. For this reason, a number of martial arts schools have focused purely on therapeutic aspects, de-emphasizing the historical aspect of self-defense or combat completely.\n\nAccording to Bruce Lee, martial arts also have the nature of an art, since there is emotional communication and complete emotional expression.\n\nSome traditional martial concepts have seen new use within modern military training. Perhaps the most recent example of this is point shooting which relies on muscle memory to more effectively utilize a firearm in a variety of awkward situations, much the way an iaidoka would master movements with their sword.\n\nDuring the World War II era William E. Fairbairn and Eric A. Sykes were recruited by the Special Operations Executive (SOE) to teach their martial art of defendu (itself drawing on Western boxing and jujutsu) and pistol shooting to UK, US, and Canadian special forces. The book \"Kill or Get Killed\", written by Colonel Rex Applegate, was based on the defendu taught by Sykes and Fairbairn. Both Fairbairn's \"Get Tough\" and Appelgate's \"Kill or Get Killed\" became classic works on hand-to-hand combat.\n\nTraditional hand-to-hand, knife, and spear techniques continue to see use in the composite systems developed for today's wars. Examples of this include European Unifight, the US Army's Combatives system developed by Matt Larsen, the Israeli army's KAPAP and Krav Maga, and the US Marine Corps's \"Marine Corps Martial Arts Program\" (MCMAP). Unarmed dagger defenses identical to those found in the manual of Fiore dei Liberi and the Codex Wallerstein were integrated into the U.S. Army's training manuals in 1942\nand continue to influence today's systems along with other traditional systems such as eskrima and silat.\n\nThe rifle-mounted bayonet which has its origin in the spear, has seen use by the United States Army, the United States Marine Corps, and the British Army as recently as the Iraq War.\n\nMany martial arts are also seen and used in Law Enforcement hand to hand training. For example, the Tokyo Riot Police's use of aikido.\n\nMartial arts since the 1970s has become a significant industry, a subset of the wider sport industry (including cinema and sports television).\n\nHundreds of millions of people worldwide practice some form of martial art.\nWeb Japan (sponsored by the Japanese Ministry of Foreign Affairs) claims there are 50 million karate practitioners worldwide.\nThe South Korean government in 2009 published an estimate that taekwondo is practiced by 70 million people in 190 countries.\n\nThe wholesale value of martial arts related sporting equipment shipped in the United States was estimated at 314 million USD in 2007; participation in the same year was estimated at 6.9 million (ages 6 or older, 2% of US population).\nR. A. Court, CEO of Martial Arts Channel, stated the total revenue of the US martial arts industry at USD 40 billion and the number of US practitioners at 30 million in 2003.\n\nMartial arts equipment can include that which is used for conditioning, protection and weapons. Specialized conditioning equipment can include breaking boards, dummy partners such as the wooden dummy, and targets such as punching bags and the makiwara. Protective equipment for sparring and competition includes boxing gloves and headgear.\n\nAsian martial arts experienced a surge of popularity in the west during the 1970s, and the rising demand resulted in numerous low quality or fraudulent schools. Fueled by fictional depictions in martial arts movies, this led to the ninja craze of the 1980s in the United States. There were also numerous fraudulent ads for martial arts training programs, inserted into comic books circa the 1960s and 1970s, which were read primarily by adolescent boys.\n\nWhen the martial arts came to the United States in the seventies, lower ranks (kyu) began to be given colorful belts to show progress. This proved to be commercially viable and colored-belt systems were adopted in many martial arts degree mills (also known as \"McDojos\" and \"Belt Factories\") as a means to generate additional cash. This was covered in \"\" (June 2010).\n\n"}
{"id": "19509", "url": "https://en.wikipedia.org/wiki?curid=19509", "title": "Finitary relation", "text": "Finitary relation\n\nIn mathematics, a finitary relation has a finite number of \"places\". In set theory and logic, a \"relation\" is a property that assigns truth values to formula_1-tuples of individuals. Typically, the property describes a possible connection between the components of a formula_1-tuple. For a given set of formula_1-tuples, a truth value is assigned to each formula_1-tuple according to whether the property does or does not hold.\n\nAn example of a \"ternary relation\" (i.e., between three individuals) is: \"formula_5 was introduced to formula_6 by formula_7\", where formula_8 is a 3-tuple of persons; for example, \"Beatrice Wood was introduced to Henri-Pierre Roché by Marcel Duchamp\" is true, while \"Karl Marx was introduced to Friedrich Engels by Queen Victoria\" is false.\n\n\"Relation\" is formally defined in the next section. In this section we introduce the concept of a relation with a familiar everyday example. Consider the relation involving three roles that people might play, expressed in a statement of the form \"\"X\" thinks that \"Y\" likes \"Z\" \". The facts of a concrete situation could be organized in a table like the following:\n\nEach row of the table records a fact or makes an assertion of the form \"\"X\" thinks that \"Y\" likes \"Z\" \". For instance, the first row says, in effect, \"Alice thinks that Bob likes Denise\". The table represents a relation \"S\" over the set \"P\" of people under discussion:\n\nThe data of the table are equivalent to the following set of ordered triples:\n\nIt is usual to write \"S\"(Alice, Bob, Denise) to say the same thing as the first row of the table. The relation \"S\" is a \"ternary\" relation, since there are \"three\" items involved in each row. The relation itself is a mathematical object defined in terms of concepts from set theory (i.e., the relation is a subset of the Cartesian product on {Person X, Person Y, Person Z}), that carries all of the information from the table in one neat package. Mathematically, then, a relation is simply an \"ordered set\".\n\nThe table for relation \"S\" is an extremely simple example of a relational database. The theoretical aspects of databases are the specialty of one branch of computer science, while their practical impacts have become all too familiar in our everyday lives. Computer scientists, logicians, and mathematicians, however, tend to see different things when they look at these concrete examples and samples of the more general concept of a relation.\n\nFor one thing, databases are designed to deal with empirical data, and experience is always finite, whereas mathematics at the very least concerns itself with potential infinity. This difference in perspective brings up a number of ideas that may be usefully introduced at this point, if by no means covered in depth.\n\nThe variable formula_1 giving the number of \"\"places\"\" in the relation, 3 for the above example, is a non-negative integer, called the relation's \"arity\", \"adicity\", or \"dimension\". A relation with formula_1 places is variously called a formula_1\"-ary\", a formula_1\"-adic\", or a formula_1\"-dimensional\" relation. Relations with a finite number of places are called \"finite-place\" or \"finitary\" relations. It is possible to generalize the concept to include \"infinitary\" relations between infinitudes of individuals, for example infinite sequences; however, in this article only finitary relations are discussed, which will from now on simply be called relations.\n\nSince there is only one 0-tuple, the so-called empty tuple ( ), there are only two zero-place relations: the one that always holds, and the one that never holds. They are sometimes useful for constructing the base case of an induction argument. One-place relations are called unary relations. For instance, any set (such as the collection of Nobel laureates) can be viewed as a collection of individuals having some property (such as that of having been awarded the Nobel prize). Two-place relations are called binary relations or, in the past, \"dyadic relations\". Binary relations are very common, given the ubiquity of relations such as:\n\nA formula_1\"-ary\" relation is a straightforward generalization of a binary relation.\n\nThe simpler of the two definitions of \"k\"-place relations encountered in mathematics is:\n\nDefinition 1. A relation \"L\" over the sets \"X\", …, \"X\" is a subset of their Cartesian product, written \"L\" ⊆ \"X\" × … × \"X\".\n\nRelations are classified according to the number of sets in the defining Cartesian product, in other words, according to the number of terms following \"L\". Hence:\nRelations with more than four terms are usually referred to as \"k\"-ary or \"n\"-ary, for example, \"a 5-ary relation\". A \"k\"-ary relation is simply a set of \"k\"-tuples.\n\nThe second definition makes use of an idiom that is common in mathematics, stipulating that \"such and such is an \"n\"-tuple\" in order to ensure that such and such a mathematical object is determined by the specification of \"n\" component mathematical objects. In the case of a relation \"L\" over \"k\" sets, there are \"k\" + 1 things to specify, namely, the \"k\" sets plus a subset of their Cartesian product. In the idiom, this is expressed by saying that \"L\" is a (\"k\" + 1)-tuple.\n\nDefinition 2. A relation \"L\" over the sets \"X\", …, \"X\" is a (\"k\" + 1)-tuple \"L\" = (\"X\", …, \"X\", \"G\"(\"L\")), where \"G\"(\"L\") is a subset of the Cartesian product \"X\" × … × \"X\". \"G\"(\"L\") is called the \"graph\" of \"L\".\n\nElements of a relation are more briefly denoted by using boldface characters, for example, the constant element a = (a, …, a) or the variable element x = (\"x\", …, \"x\").\n\nA statement of the form \"a is in the relation \"L\" \" or \"a satisfies \"L\" \" is taken to mean that a is in \"L\" under the first definition and that a is in \"G\"(\"L\") under the second definition.\n\nThe following considerations apply under either definition:\n\nAs a rule, whatever definition best fits the application at hand will be chosen for that purpose, and anything that falls under it will be called a relation for the duration of that discussion. If it becomes necessary to distinguish the two definitions, an entity satisfying the second definition may be called an \"embedded\" or \"included\" relation.\n\nIf \"L\" is a relation over the domains \"X\", …, \"X\", it is conventional to consider a sequence of terms called \"variables\", \"x\", …, \"x\", that are said to \"range over\" the respective domains.\n\nLet a Boolean domain B be a two-element set, say, B = {0, 1}, whose elements can be interpreted as logical values, typically 0 = false and 1 = true. The characteristic function of the relation \"L\", written \"ƒ\" or χ(\"L\"), is the Boolean-valued function \"ƒ\" : \"X\" × … × \"X\" → B, defined in such a way that \"ƒ\"(formula_23) = 1 just in case the \"k\"-tuple formula_23 is in the relation \"L\". Such a function can also be called an indicator function, particularly in probability and statistics, to avoid confusion with the notion of a characteristic function in probability theory.\n\nIt is conventional in applied mathematics, computer science, and statistics to refer to a Boolean-valued function like \"ƒ\" as a \"k\"-place predicate. From the more abstract viewpoint of formal logic and model theory, the relation \"L\" constitutes a \"logical model\" or a \"relational structure\" that serves as one of many possible interpretations of some \"k\"-place predicate symbol.\n\nBecause relations arise in many scientific disciplines as well as in many branches of mathematics and logic, there is considerable variation in terminology. This article treats a relation as the set-theoretic extension of a relational concept or term. A variant usage reserves the term \"relation\" to the corresponding logical entity, either the logical comprehension, which is the totality of intensions or abstract properties that all of the elements of the relation in extension have in common, or else the symbols that are taken to denote these elements and intensions. Further, some writers of the latter persuasion introduce terms with more concrete connotations, like \"relational structure\", for the set-theoretic extension of a given relational concept.\n\nThe logician Augustus De Morgan, in work published around 1860, was the first to articulate the notion of relation in anything like its present sense. He also stated the first formal results in the theory of relations (on De Morgan and relations, see Merrill 1990). Charles Sanders Peirce restated and extended De Morgan's results.\n\nIn the 19th century Peirce, Gottlob Frege, Georg Cantor, Richard Dedekind, and others advanced the theory of relations. Many of their ideas, especially on relations called orders, were summarized in Principles of Mathematics (1903) by Bertrand Russell. Russell and A. N. Whitehead made free use of these results in their \"Principia Mathematica\".\n\n\n"}
{"id": "19510", "url": "https://en.wikipedia.org/wiki?curid=19510", "title": "Mokele-mbembe", "text": "Mokele-mbembe\n\nIn Congo River Basin mythology, Mokele-mbembe (Lingala: , \"one who stops the flow of rivers\") is a water-dwelling entity, sometimes described as a living creature, sometimes as a spirit.\n\nDuring the early 20th century, descriptions of the entity increasingly reflected public fascination with dinosaurs, including aspects of particular dinosaur species now known among scientists to be incorrect, and the entity became increasingly described alongside a number of purported living dinosaurs in Africa.\n\nOver time, the entity became a point of focus in particular among adherents of the pseudosciences of cryptozoology and young Earth creationism, resulting in numerous expeditions led by cryptozoologists and funded by young Earth creationists and groups with the aim of finding evidence that invalidates scientific consensus regarding evolution. Paleontologist Donald Prothero remarks that \"the quest for Mokele Mbembe ... is part of the effort by creationists to overthrow the theory of evolution and teaching of science by any means possible\". Additionally, Prothero observes that \"the only people looking for Mokele-mbembe are creationist ministers, not wildlife biologists.\"\n\n1909 saw the first mention of a brontosaurus-like creature in \"Beasts and Men\", the autobiography of famed big-game hunter Carl Hagenbeck. He claimed to have heard from two independent sources about a creature living in Rhodesia which was described to them by natives as \"half elephant, half dragon.\" Naturalist Joseph Menges had also told Hagenbeck about similar stories. Hagenbeck speculated that \"it can only be some kind of dinosaur, seemingly akin to the brontosaurus.\" Another of Hagenbeck's sources, Hans Schomburgk, asserted that while at Lake Bangweulu, he noted a lack of hippopotami; his native guides informed him of a large hippo-killing creature that lived in Lake Bangweulu; however, as noted below, Schomburgk thought that native testimony was sometimes unreliable.\n\nReports of entities described to be dinosaur-like in Africa caused a minor sensation in the mass media, and newspapers in Europe and North America carried many articles on the subject in 1910–1911; some took the reports at face value, others were more skeptical.\n\nAccording to German adventurer Lt. Paul Gratz's account from 1911:The crocodile is found only in very isolated specimens in Lake Bangweulu, except in the mouths of the large rivers at the north. In the swamp lives the \"nsanga\", much feared by the natives, a degenerate saurian which one might well confuse with the crocodile were it not that its skin has no scales and its toes are armed with claws. I did not succeed in shooting a \"nsanga\", but on the island of Mbawala I came by some strips of its skin.\n\nAnother report comes from German Captain , as described by Willy Ley in \"Exotic Zoology\" (1959). Von Stein was ordered to conduct a survey of German colonies in what is now Cameroon in 1913. He heard stories of an enormous reptile called \"Mokéle-mbêmbe\" alleged to live in the jungles, and included a description in his official report. According to Ley, \"von Stein worded his report with utmost caution,\" knowing it might be seen as unbelievable. Nonetheless, von Stein thought the tales were credible: trusted native guides had related the tales to him, and the stories were related to him by independent sources, yet featured many of the same details. Though von Stein's report was never formally published, Ley quoted von Stein as writing:\n\nThe animal is said to be of a brownish-gray color with a smooth skin, its size is approximately that of an elephant; at least that of a hippopotamus. It is said to have a long and very flexible neck and only one tooth but a very long one; \"some say it is a horn\". A few spoke about a long, muscular tail like that of an alligator. Canoes coming near it are said to be doomed; the animal is said to attack the vessels at once and to kill the crews but without eating the bodies. The creature is said to live in the caves that have been washed out by the river in the clay of its shores at sharp bends. It is said to climb the shores even at daytime in search of food; its diet is said to be entirely vegetable. This feature disagrees with a possible explanation as a myth. The preferred plant was shown to me, it is a kind of liana with large white blossoms, with a milky sap and applelike fruits. At the Ssombo River I was shown a path said to have been made by this animal in order to get at its food. The path was fresh and there were plants of the described type nearby. But since there were too many tracks of elephants, hippos, and other large mammals it was impossible to make out a particular spoor with any amount of certainty.\n\nAlfred Aloysius Smith, who had worked for a British trading company in what is now Gabon in the late 1800s, briefly mentions in his 1927 memoir the \"jago-nini\" and \"amali\":Aye, and behind the Cameroon there's things living we know nothing about. I could 'a' made books about many things. The \"Jago-Nini\" they say is still in the swamps and rivers. Giant diver it means. Comes out of the water and devours people. Old men'll tell you what their grandfathers saw but they still believe its there. Same as the Amali I've always taken it to be. I've seen the Amali's footprint. About the size of a good frying pan in circumference and three claws instead of five.He also speculates that \"some great creature like the Amali\" could be responsible for finding broken and splintered ivory in (now known to be mythical) elephants' graveyards, as well as claiming to have given a chiseled out cave painting of the \"amali\" to Ulysses S. Grant.\n\nIn August and September of 2018, Lensgreve of Knuthenborg, Adam Christoffer Knuth, along with a film crew from DR and a DNA scientist, traveled to Lake Tele in Congo, in search of the Mokele-mbembe. They did not find the dinosaur. However they found a new green algae, which has not been discovered before. \n"}
{"id": "19513", "url": "https://en.wikipedia.org/wiki?curid=19513", "title": "Intuitionism", "text": "Intuitionism\n\nIn the philosophy of mathematics, intuitionism, or neointuitionism (opposed to preintuitionism), is an approach where mathematics is considered to be purely the result of the constructive mental activity of humans rather than the discovery of fundamental principles claimed to exist in an objective reality. That is, logic and mathematics are not considered analytic activities wherein deep properties of objective reality are revealed and applied but are instead considered the application of internally consistent methods used to realize more complex mental constructs, regardless of their possible independent existence in an objective reality.\n\nThe fundamental distinguishing characteristic of intuitionism is its interpretation of what it means for a mathematical statement to be true. In Brouwer's original intuitionism, the truth of a mathematical statement is a subjective claim: a mathematical statement corresponds to a mental construction, and a mathematician can assert the truth of a statement only by verifying the validity of that construction by intuition. The vagueness of the intuitionistic notion of truth often leads to misinterpretations about its meaning. Kleene formally defined intuitionistic truth from a realist position, yet Brouwer would likely reject this formalization as meaningless, given his rejection of the realist/Platonist position. Intuitionistic truth therefore remains somewhat ill-defined. However, because the intuitionistic notion of truth is more restrictive than that of classical mathematics, the intuitionist must reject some assumptions of classical logic to ensure that everything they prove is in fact intuitionistically true. This gives rise to intuitionistic logic.\n\nTo an intuitionist, the claim that an object with certain properties exists is a claim that an object with those properties can be constructed. Any mathematical object is considered to be a product of a construction of a mind, and therefore, the existence of an object is equivalent to the possibility of its construction. This contrasts with the classical approach, which states that the existence of an entity can be proved by refuting its non-existence. For the intuitionist, this is not valid; the refutation of the non-existence does not mean that it is possible to find a construction for the putative object, as is required in order to assert its existence. As such, intuitionism is a variety of mathematical constructivism; but it is not the only kind.\n\nThe interpretation of negation is different in intuitionist logic than in classical logic. In classical logic, the negation of a statement asserts that the statement is \"false\"; to an intuitionist, it means the statement is \"refutable\"\n(e.g., that there is a counterexample). There is thus an asymmetry between a positive and negative statement in intuitionism. If a statement \"P\" is provable, then it is certainly impossible to prove that there is no proof of \"P\". But even if it can be shown that no disproof of \"P\" is possible, we cannot conclude from this absence that there \"is\" a proof of \"P\". Thus \"P\" is a stronger statement than \"not-not-P\".\n\nSimilarly, to assert that \"A\" or \"B\" holds, to an intuitionist, is to claim that either \"A\" or \"B\" can be \"proved\". In particular, the law of excluded middle, \"\"A\" or not \"A\"\", is not accepted as a valid principle. For example, if \"A\" is some mathematical statement that an intuitionist has not yet proved or disproved, then that intuitionist will not assert the truth of \"\"A\" or not \"A\"\". However, the intuitionist will accept that \"\"A\" and not \"A\"\" cannot be true. Thus the connectives \"and\" and \"or\" of intuitionistic logic do not satisfy de Morgan's laws as they do in classical logic.\n\nIntuitionistic logic substitutes constructability for abstract truth and is associated with a transition from the proof of model theory to abstract truth in modern mathematics. The logical calculus preserves justification, rather than truth, across transformations yielding derived propositions. It has been taken as giving philosophical support to several schools of philosophy, most notably the Anti-realism of Michael Dummett. Thus, contrary to the first impression its name might convey, and as realized in specific approaches and disciplines (e.g. Fuzzy Sets and Systems), intuitionist mathematics is more rigorous than conventionally founded mathematics, where, ironically, the foundational elements which Intuitionism attempts to construct/refute/refound are taken as intuitively given.\n\nAmong the different formulations of intuitionism, there are several different positions on the meaning and reality of infinity.\n\nThe term potential infinity refers to a mathematical procedure in which there is an unending series of steps. After each step has been completed, there is always another step to be performed. For example, consider the process of counting: \n\nThe term actual infinity refers to a completed mathematical object which contains an infinite number of elements. An example is the set of natural numbers, \n\nIn Cantor's formulation of set theory, there are many different infinite sets, some of which are larger than others. For example, the set of all real numbers is larger than , because any procedure that you attempt to use to put the natural numbers into one-to-one correspondence with the real numbers will always fail: there will always be an infinite number of real numbers \"left over\". Any infinite set that can be placed in one-to-one correspondence with the natural numbers is said to be \"countable\" or \"denumerable\". Infinite sets larger than this are said to be \"uncountable\".\n\nCantor's set theory led to the axiomatic system of Zermelo–Fraenkel set theory (ZFC), now the most common foundation of modern mathematics. Intuitionism was created, in part, as a reaction to Cantor's set theory.\n\nModern constructive set theory includes the axiom of infinity from ZFC (or a revised version of this axiom) and the set of natural numbers. Most modern constructive mathematicians accept the reality of countably infinite sets (however, see Alexander Esenin-Volpin for a counter-example).\n\nBrouwer rejected the concept of actual infinity, but admitted the idea of potential infinity.\n\nIntuitionism's history can be traced to two controversies in nineteenth century mathematics.\n\nThe first of these was the invention of transfinite arithmetic by Georg Cantor and its subsequent rejection by a number of prominent mathematicians including most famously his teacher Leopold Kronecker—a confirmed finitist.\n\nThe second of these was Gottlob Frege's effort to reduce all of mathematics to a logical formulation via set theory and its derailing by a youthful Bertrand Russell, the discoverer of Russell's paradox. Frege had planned a three volume definitive work, but just as the second volume was going to press, Russell sent Frege a letter outlining his paradox, which demonstrated that one of Frege's rules of self-reference was self-contradictory. In an appendix to the second volume, Frege magnanimously acknowledged that one of the axioms of his system did in fact lead to Russell's paradox.\n\nFrege, the story goes, plunged into depression and did not publish the third volume of his work as he had planned. For more see Davis (2000) Chapters 3 and 4: Frege: \"From Breakthrough to Despair\" and Cantor: \"Detour through Infinity.\" See van Heijenoort for the original works and van Heijenoort's commentary.\n\nThese controversies are strongly linked as the logical methods used by Cantor in proving his results in transfinite arithmetic are essentially the same as those used by Russell in constructing his paradox. Hence how one chooses to resolve Russell's paradox has direct implications on the status accorded to Cantor's transfinite arithmetic.\n\nIn the early twentieth century L. E. J. Brouwer represented the \"intuitionist\" position and David Hilbert the formalist position—see van Heijenoort. Kurt Gödel offered opinions referred to as \"Platonist\" (see various sources re Gödel). Alan Turing considers:\n\"non-constructive systems of logic with which not all the steps in a proof are mechanical, some being intuitive\". (Turing 1939, reprinted in Davis 2004, p. 210) Later, Stephen Cole Kleene brought forth a more rational consideration of intuitionism in his Introduction to Meta-mathematics (1952).\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19514", "url": "https://en.wikipedia.org/wiki?curid=19514", "title": "May 6", "text": "May 6\n\n\n\n\n"}
{"id": "19516", "url": "https://en.wikipedia.org/wiki?curid=19516", "title": "March 2", "text": "March 2\n\n\n\n"}
{"id": "19518", "url": "https://en.wikipedia.org/wiki?curid=19518", "title": "Mishnah", "text": "Mishnah\n\nThe Mishnah or Mishna (; , \"study by repetition\", from the verb \"shanah\" , or \"to study and review\", also \"secondary\") is the first major written collection of the Jewish oral traditions known as the \"Oral Torah\". It is also the first major work of Rabbinic literature. The Mishnah was redacted by Judah the Prince at the beginning of the third century CE in a time when, according to the Talmud, the persecution of the Jews and the passage of time raised the possibility that the details of the oral traditions of the Pharisees from the Second Temple period (536 BCE – 70 CE) would be forgotten. Most of the Mishnah is written in Mishnaic Hebrew, while some parts are Aramaic.\n\nThe Mishnah consists of six orders (', singular ' ), each containing 7–12 tractates (', singular ' ; lit. \"web\"), 63 in total, and further subdivided into chapters and paragraphs. The word \"Mishnah\" can also indicate a single paragraph of the work, i.e. the smallest unit of structure in the Mishnah. For this reason the whole work is sometimes called in the plural, \"\".\n\nThe term \"\"Mishnah\"\" originally referred to a method of teaching by presenting topics in a systematic order, as contrasted with \"\", which followed the order of the Bible. As a written compilation, the order of the \"Mishnah\" is by subject matter and includes a much broader selection of halakhic subjects, and discusses individual subjects more thoroughly, than the \"Midrash\".\n\nThe \"Mishnah\" consists of six orders (', singular ' ), each containing 7–12 tractates (', singular ' ; lit. \"web\"), 63 in total. Each ' is divided into chapters (', singular ') and then paragraphs (', singular \"\"). In this last context, the word \"mishnah\" means a single paragraph of the work, i.e. the smallest unit of structure, leading to the use of the plural, \"\"Mishnayot\"\", for the whole work.\n\nBecause of the division into six orders, the \"Mishnah\" is sometimes called 'Shas' (an acronym for \"Shisha Sedarim\" – the \"six orders\"), though that term is more often used for the Talmud as a whole.\n\nThe six orders are:\n\nIn each order (with the exception of Zeraim), tractates are arranged from biggest (in number of chapters) to smallest. A popular mnemonic consists of the acronym \"Z'MaN NaKaT.\"\n\nThe Babylonian Talmud (Hagiga 14a) states that there were either six hundred or seven hundred orders of the Mishnah. Hillel the Elder organized them into six orders to make it easier to remember. The historical accuracy of this tradition is disputed. There is also a tradition that Ezra the scribe dictated from memory not only the 24 books of the Tanakh but 60 esoteric books. It is not known whether this is a reference to the \"Mishnah\", but there is a case for saying that the \"Mishnah\" does consist of 60 tractates. (The current total is 63, but Makkot was originally part of Sanhedrin, and Bava Kamma, Bava Metzia and Bava Batra may be regarded as subdivisions of a single tractate Nezikin.)\n\nReuvein Margolies (1889–1971) posited that there were originally seven orders of Mishnah, citing a Gaonic tradition on the existence of a seventh order containing the laws of \"Sta\"m\" (scribal practice) and Berachot (blessings).\n\nA number of important laws are not elaborated upon in the \"Mishnah\". These include the laws of tzitzit, tefillin (phylacteries), mezuzot, the holiday of Hanukkah, and the laws of conversion to Judaism. These were later discussed in the minor tractates.\n\nNissim ben Jacob's \"Hakdamah Le'mafteach Hatalmud\" argued that it was unnecessary for Judah the Prince to discuss them as many of these laws were so well known. Margolies suggests that as the \"Mishnah\" was redacted after the Bar Kokhba revolt, Judah could not have included discussion of Hanukkah, which commemorates the Jewish revolt against the Seleucid Empire (the Romans would not have tolerated this overt nationalism). Similarly, there were then several decrees in place aimed at suppressing outward signs of national identity, including decrees against wearing tefillin and tzitzit; as conversion to Judaism was against Roman law, Judah would not have discussed this.\n\nDavid Zvi Hoffmann suggests that there existed ancient texts analogous to the present-day \"Shulchan Aruch\" that discussed the basic laws of day to day living and it was therefore not necessary to focus on these laws in the \"Mishnah\".\n\nRabbinic commentaries on the \"Mishnah\" from the next four centuries, done in the Land of Israel and in Babylonia, were eventually redacted and compiled as well. In themselves they are known as \"Gemara\". The books which set out the \"Mishnah\" in its original structure, together with the associated \"Gemara\", are known as Talmuds. Two Talmuds were compiled, the Babylonian Talmud (to which the term \"Talmud\" normally refers) and the Jerusalem Talmud. Unlike the Hebrew \"Mishnah\", the \"Gemara\" is written primarily in Aramaic.\n\nThe \"Mishnah\" teaches the oral traditions by example, presenting actual cases being brought to judgment, usually along with the debate on the matter and the judgment that was given by a notable rabbi based on halakha, mitzvot, and spirit of the teaching (\"Torah\") that guided his decision. In this way, it brings to everyday reality the practice of the \"mitzvot\" as presented in the Torah, and aims to cover all aspects of human living, serve as an example for future judgments, and, most important, demonstrate pragmatic exercise of the Biblical laws, which was much needed since the time when the Second Temple was destroyed (70 CE). The \"Mishnah\" is not the development of new laws, but rather the collection of existing traditions.\n\nThe term \"Mishnah\" is related to the verb \"shanah\", to teach or repeat, and to the adjectives \"\"sheni\"\" and \"\"mishneh\"\", meaning \"second\". It is thus named for being both the one written authority (codex) secondary (only) to the Tanakh as a basis for the passing of judgment, a source and a tool for creating laws, and the first of many books to complement the Tanakh in certain aspects.\n\nBefore the publication of the \"Mishnah\", Jewish scholarship and judgement were predominantly oral, as according to the Talmud, it was not permitted to write them down. The earliest recorded oral law may have been of the midrashic form, in which halakhic discussion is structured as exegetical commentary on the Torah. Rabbis expounded on and debated the Tanakh, the Hebrew Bible, without the benefit of written works (other than the Biblical books themselves), though some may have made private notes () for example of court decisions. The oral traditions were far from monolithic, and varied among various schools, the most famous of which were the House of Shammai and the House of Hillel.\n\nAfter First Jewish–Roman War in 70 CE, with the end of the Second Temple Jewish center in Jerusalem, Jewish social and legal norms were in upheaval. The Rabbis were faced with the new reality of Judaism without a Temple (to serve as the center of teaching and study) and Judea without autonomy. It is during this period that Rabbinic discourse began to be recorded in writing. The possibility was felt that the details of the oral traditions of the Pharisees from the Second Temple period (530s BCE – 70 CE) would be forgotten, so the justification was found to have these oral laws transcribed.\n\nOver time, different traditions of the Oral Law came into being, raising problems of interpretation. According to the \"Mevo Hatalmud\" many rulings were given in a specific context, but would be taken out of it; or a ruling was revisited but the second ruling would not become popularly known. To correct this, Judah the Prince took up the redaction of the \"Mishnah\". If a point was of no conflict, he kept its language; where there was conflict, he reordered the opinions and ruled; and he clarified where context was not given. The idea was not to use his own discretion, but rather to examine the tradition as far back as he could, and only supplement as required.\n\nAccording to Rabbinic Judaism, the Oral Torah () was given to Moses with the Torah at Mount Sinai or Mount Horeb as an exposition to the latter. The accumulated traditions of the Oral Law, expounded by scholars in each generation from Moses onward, is considered as the necessary basis for the interpretation, and often for the reading, of the Written Law. Jews sometimes refer to this as the Masorah (Hebrew: ), roughly translated as tradition, though that word is often used in a narrower sense to mean traditions concerning the editing and reading of the Biblical text (see Masoretic Text). The resulting Jewish law and custom is called halakha.\n\nWhile most discussions in the Mishnah concern the correct way to carry out laws recorded in the Torah, it usually presents its conclusions without explicitly linking them to any scriptural passage, though scriptural quotations do occur. For this reason it is arranged in order of topics rather than in the form of a Biblical commentary. (In a very few cases, there is no scriptural source at all and the law is described as \"Halakha leMoshe miSinai\", \"law to Moses from Sinai\".) The \"Midrash halakha\", by contrast, while presenting similar laws, does so in the form of a Biblical commentary and explicitly links its conclusions to details in the Biblical text. These Midrashim often predate the Mishnah.\n\nThe Mishnah also quotes the Torah for principles not associated with law, but just as practical advice, even at times for humor or as guidance for understanding historical debates.\n\nSome Jews did not accept the codification of the oral law at all. Karaite Judaism, for example, recognised only the Tanakh as authoritative in \"Halakha\" (Jewish religious law) and theology. It vehemently rejected the codification of the Oral Torah in the Mishnah and Talmud and subsequent works of mainstream Rabbinic Judaism which maintained that the Talmud was an authoritative interpretations of the Torah. Karaites maintained that all of the divine commandments handed down to Moses by God were recorded in the written Torah without additional Oral Law or explanation. As a result, Karaite Jews did not accept as binding the written collections of the oral tradition in the Midrash or Talmud. The Karaites comprised a significant portion of the world Jewish population in the 10th and 11th centuries CE, and remain extant, although they currently number in the thousands.\n\nThe rabbis who contributed to the \"Mishnah\" are known as the \"Tannaim\", of whom approximately 120 are known. The period during which the \"Mishnah\" was assembled spanned about 130 years, or five generations, in the first and second centuries CE. Judah the Prince is credited with the final redaction and publication of the \"Mishnah\", although there have been a few additions since his time: those passages that cite him or his grandson, Judah II, and the end of tractate Sotah, which refers to the period after Judah the Prince's death. One must also note that in addition to redacting the \"Mishnah\", Judah the Prince and his court also ruled on which opinions should be followed, though the rulings do not always appear in the text.\n\nMost of the Mishnah is related without attribution (\"\"). This usually indicates that many sages taught so, or that Judah the Prince ruled so. The halakhic ruling usually follows that view. Sometimes, however, it appears to be the opinion of a single sage, and the view of the sages collectively (, \"hachamim\") is given separately.\n\nAs Judah the Prince went through the tractates, the \"Mishnah\" was set forth, but throughout his life some parts were updated as new information came to light. Because of the proliferation of earlier versions, it was deemed too hard to retract anything already released, and therefore a second version of certain laws were released. The Talmud refers to these differing versions as ' (\"First Mishnah\") and ' (\"Last Mishnah\"). David Zvi Hoffmann suggests that \"Mishnah Rishonah\" actually refers to texts from earlier Sages upon which Rabbi based his Mishnah.\n\nThe Talmud records a tradition that unattributed statements of the law represent the views of Rabbi Meir (Sanhedrin 86a), which supports the theory (recorded by Sherira Gaon in his famous \"Iggeret\") that he was the author of an earlier collection. For this reason, the few passages that actually say \"this is the view of Rabbi Meir\" represent cases where the author intended to present Rabbi Meir's view as a \"minority opinion\" not representing the accepted law.\n\nThere are also references to the \"Mishnah of Rabbi Akiva\", suggesting a still earlier collection; on the other hand, these references may simply mean his teachings in general. Another possibility is that Rabbi Akiva and Rabbi Meir established the divisions and order of subjects in the Mishnah, making them the authors of a school curriculum rather than of a book.\n\nAuthorities are divided on whether Rabbi Judah the Prince recorded the Mishnah in writing or established it as an oral text for memorisation. The most important early account of its composition, the \"Iggeret Rav Sherira Gaon\" (Epistle of Rabbi Sherira Gaon) is ambiguous on the point, although the Spanish recension leans to the theory that the Mishnah was written. However, the Talmud records that, in every study session, there was a person called the \"tanna\" appointed to recite the Mishnah passage under discussion. This may indicate that, even if the Mishnah was reduced to writing, it was not available on general distribution.\n\nVery roughly, there are two traditions of Mishnah text. One is found in manuscripts and printed editions of the Mishnah on its own, or as part of the Jerusalem Talmud. The other is found in manuscripts and editions of the Babylonian Talmud; though there is sometimes a difference between the text of a whole paragraph printed at the beginning of a discussion (which may be edited to conform with the text of the Mishnah-only editions) and the line-by-line citations in the course of the discussion. \n\nRobert Brody, in his \"Mishna and Tosefta Studies\" (Jerusalem 2014), warns against over-simplifying the picture by assuming that the Mishnah-only tradition is always the more authentic, or that it represents a \"Palestinian\" as against a \"Babylonian\" tradition. Manuscripts from the Cairo Geniza, or citations in other works, may support either type of reading or other readings altogether.\n\nThe first printed edition of the Mishnah was published in Naples. There have been many subsequent editions, including the late 19th century Vilna edition, which is the basis of the editions now used by the religious public.\n\nVocalized editions were published in Italy, culminating in the edition of David ben Solomon Altaras, publ. Venice 1737. The Altaras edition was republished in Mantua in 1777, in Pisa in 1797 and 1810 and in Livorno in many editions from 1823 until 1936: reprints of the vocalized Livorno editions were published in Israel in 1913, 1962, 1968 and 1976. These editions show some textual variants by bracketing doubtful words and passages, though they do not attempt detailed textual criticism. The Livorno editions are the basis of the Sephardic tradition for recitation.\n\nAs well as being printed on its own, the Mishnah is included in all editions of the Babylonian and Jerusalem Talmuds. Each paragraph is printed on its own, and followed by the relevant Gemara discussion. However, that discussion itself often cites the Mishnah line by line. While the text printed in paragraph form has generally been standardized to follow the Vilna edition, the text cited line by line in the Gemara often preserves important variants, which sometimes reflect the readings of older manuscripts.\n\nThe nearest approach to a critical edition is that of Hanoch Albeck. There is also an edition by Yosef Qafiḥ of the Mishnah together with the commentary of Maimonides, which compares the base text used by Maimonides with the Napoli and Vilna editions and other sources.\n\nThe Mishnah was and still is traditionally studied through recitation (out loud). Jewish communities around the world preserved local melodies for chanting the Mishnah, and distinctive ways of pronouncing its words.\n\nMany medieval manuscripts of the Mishnah are vowelized, and some of these, especially some fragments found in the Genizah, are partially annotated with Tiberian cantillation marks.\n\nToday, many communities have a special tune for the Mishnaic passage \"Bammeh madliqin\" in the Friday night service; there may also be tunes for Mishnaic passages in other parts of the liturgy, such as the passages in the daily prayers relating to sacrifices and incense and the paragraphs recited at the end of the Musaf service on Shabbat. Otherwise, there is often a customary intonation used in the study of Mishnah or Talmud, somewhat similar to an Arabic mawwal, but this is not reduced to a precise system like that for the Biblical books. (In some traditions this intonation is the same as or similar to that used for the Passover Haggadah.) Recordings have been made for Israeli national archives, and Frank Alvarez-Pereyre has published a book-length study of the Syrian tradition of Mishnah reading on the basis of these recordings.\n\nMost vowelized editions of the Mishnah today reflect standard Ashkenazic vowelization, and often contain mistakes. The Albeck edition of the Mishnah was vowelized by Hanokh Yalon, who made careful eclectic use of both medieval manuscripts and current oral traditions of pronunciation from Jewish communities all over the world. The Albeck edition includes an introduction by Yalon detailing his eclectic method.\n\nTwo institutes at the Hebrew University in Jerusalem have collected major oral archives which hold (among other things) extensive recordings of Jews chanting the Mishnah using a variety of melodies and many different kinds of pronunciation. These institutes are the Jewish Oral Traditions Research Center and the National Voice Archives (the \"Phonoteca\" at the Jewish National and University Library). See below for external links.\n\n\nBoth the Mishnah and Talmud contain little serious biographical studies of the people discussed therein, and the same tractate will conflate the points of view of many different people. Yet, sketchy biographies of the Mishnaic sages can often be constructed with historical detail from Talmudic and Midrashic sources.\n\nAccording to the Encyclopaedia Judaica (Second Edition), it is accepted that Judah the Prince added, deleted, and rewrote his source material during the process of redacting the Mishnah. Modern authors who have provided examples of these changes include J.N. Epstein and S. Friedman.\n\nFollowing Judah the Prince's redaction there remained a number of different versions of the Mishnah in circulation. The Mishnah used in the Babylonian rabbinic community differing markedly from that used in the Palestinian one. Indeed within these rabbinic communities themselves there are indications of different versions being used for study. These differences are shown in divergent citations of individual Mishnah passages in the Talmud Yerushalmi and the Talmud Bavli, and in variances of medieval manuscripts and early editions of the Mishnah. The best known examples of these differences is found in J.N.Epstein’s Introduction to the Text of the Mishnah (1948).\n\nEpstein has also concluded that the period of the Amoraim was one of further deliberate changes to the text of the Mishnah, which he views as attempts to return the text to what was regarded as its original form. These lessened over time, as the text of the Mishnah became more and more regarded as authoritative.\n\nMany modern historical scholars have focused on the timing and the formation of the Mishnah. A vital question is whether it is composed of sources which date from its editor's lifetime, and to what extent is it composed of earlier, or later sources. Are Mishnaic disputes distinguishable along theological or communal lines, and in what ways do different sections derive from different schools of thought within early Judaism? Can these early sources be identified, and if so, how? In response to these questions, modern scholars have adopted a number of different approaches.\n\nA notable literary work on the composition of the Mishnah is Milton Steinberg's novel \"As a Driven Leaf\".\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19521", "url": "https://en.wikipedia.org/wiki?curid=19521", "title": "Marathon (disambiguation)", "text": "Marathon (disambiguation)\n\nA marathon is a foot race for humans for a distance of .\n\nMarathon also may refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19522", "url": "https://en.wikipedia.org/wiki?curid=19522", "title": "Monotheism", "text": "Monotheism\n\nMonotheism is defined as the belief in the existence of only one god that created the world, is all-powerful and intervenes in the world. A broader definition of monotheism is the belief in one god. A distinction may be made between exclusive monotheism, and both inclusive monotheism and pluriform (panentheistic) monotheism which, while recognising various distinct gods, postulate some underlying unity.\n\nMonotheism is distinguished from henotheism, a religious system in which the believer worships one god without denying that others may worship different gods with equal validity, and monolatrism, the recognition of the existence of many gods but with the consistent worship of only one deity. The term \"monolatry\" was perhaps first used by Julius Wellhausen.\n\nThe broader definition of monotheism characterizes the traditions of Bábism, the Bahá'í Faith, Balinese Hinduism, Cao Dai (Caodaiism), Cheondoism (Cheondogyo), Christianity, Deism, Eckankar, Hindu sects such as Shaivism and Vaishnavism, Islam, Judaism, Mandaeism, Rastafari, Seicho no Ie, Sikhism, Tengrism (Tangrism), Tenrikyo (Tenriism), Yazidism, and Zoroastrianism, and elements of pre-monotheistic thought are found in early religions such as Atenism, ancient Chinese religion, and .\n\nThe word \"monotheism\" comes from the Greek (\"monos\") meaning \"single\" and (\"theos\") meaning \"god\". The English term was first used by Henry More (1614–1687).\n\nQuasi-monotheistic claims of the existence of a universal deity date to the Late Bronze Age, with Akhenaten's \"Great Hymn to the Aten\". A possible inclination towards monotheism emerged during the Vedic period in Iron-Age South Asia. The Rigveda exhibits notions of monism of the Brahman, particularly in the comparatively late tenth book, which is dated to the early Iron Age, e.g. in the Nasadiya sukta. \n\nSince the sixth century BCE, Zoroastrians have believed in the supremacy of one God above all: Ahura Mazda as the \"Maker of All\" and the first being before all others. Nonetheless, Zoroastrianism was not strictly monotheistic because it venerated other \"yazatas\" alongside Ahura Mazda. Ancient Hindu theology, meanwhile, was monist, but was not strictly monotheistic in worship because it still maintained the existence of many gods, who were envisioned as aspects of one supreme God, Brahman. Numerous ancient Greek philosophers, including Xenophanes of Colophon and Antisthenes believed in a similar polytheistic monism that came close to monotheism, but fell short. Judaism was the first religion to conceive the notion of a personal monotheistic God within a monist context. The concept of ethical monotheism, which holds that morality stems from God alone and that its laws are unchanging, first occurred in Judaism, but is now a core tenet of most modern monotheistic religions, including Zoroastrianism, Christianity, Islam, Sikhism, and Bahá'í Faith.\n\nAccording to Jewish, Christian and Islamic tradition, monotheism was the original religion of humanity; this original religion is sometimes referred to as \"the Adamic religion\", or, in the terms of Andrew Lang, the \"Urreligion\". Scholars of religion largely abandoned that view in the 19th century in favour of an evolutionary progression from animism via polytheism to monotheism, but by 1974 this theory was less widely held, and a modified view similar to Lang's became more prominent. Austrian anthropologist Wilhelm Schmidt had postulated an \"Urmonotheismus\", \"original\" or \"primitive monotheism\" in the 1910s. It was objected that Judaism, Christianity, and Islam had grown up in opposition to polytheism as had Greek philosophical monotheism. More recently, Karen Armstrong and other authors have returned to the idea of an evolutionary progression beginning with animism, which developed into polytheism, which developed into henotheism, which developed into monolatry, which developed into true monotheism.\n\nWhile all adherents of the Abrahamic religions consider themselves to be monotheists, some in Judaism do not consider Christianity to be a pure form of monotheism (due to The Christian doctrine of the Trinity), classifying it as Shituf. Islam likewise does not recognize modern-day Christianity as monotheistic, primarily due to the Christian doctrine of Trinity, which Islam argues was not a part of the original monotheistic Christianity as preached by Jesus.\nChristians, on the other hand, argue that the doctrine of the Trinity is a valid expression of monotheism, citing that the Trinity does not consist of three separate deities, but rather the three persons, who exist consubstantially (as one substance) within a single Godhead.\n\nJudaism is one of the oldest monotheistic religions in the world, although some scholars have argued that the earliest Israelites (pre-7th century BCE) were monolatristic rather than monotheistic. God in later Judaism was strictly monotheistic, an absolute one, indivisible, and incomparable being who is the ultimate cause of all existence. The Babylonian Talmud references other, \"foreign gods\" as non-existent entities to whom humans mistakenly ascribe reality and power. One of the best-known statements of Rabbinical Judaism on monotheism is the Second of Maimonides' 13 Principles of faith:\nSome in Judaism and Islam reject the Christian idea of monotheism. Judaism uses the term \"shituf\" to refer to the worship of God in a manner which Judaism deems to be neither purely monotheistic (though still permissible for non-Jews) nor polytheistic (which would be prohibited).\n\nDuring the 8th century BCE, the worship of YHWH in Israel was in competition with many other cults, described by the Yahwist faction collectively as Baals. The oldest books of the Hebrew Bible reflect this competition, as in the books of Hosea and Nahum, whose authors lament the \"apostasy\" of the people of Israel, threatening them with the wrath of God if they do not give up their polytheistic cults.\n\nAncient Israelite religion was originally polytheistic; the Israelites worshipped many deities, including El, Baal, Asherah, and Astarte. YHWH was originally the national god of the Kingdom of Israel and the Kingdom of Judah. As time progressed, the henotheistic cult of Yahweh grew increasingly militant in its opposition to the worship of other gods. Later, the reforms of King Josiah imposed a form of strict monolatrism. After the fall of Judah to Babylon, a small circle of priests and scribes gathered around the exiled royal court, where they first developed the concept of YHWH as the sole God of the world.\n\n\"Shema Yisrael\" (\"Hear, [O] Israel\") are the first two words of a section of the Torah, and is the title of a prayer that serves as a centerpiece of the morning and evening Jewish prayer services. The first verse encapsulates the monotheistic essence of Judaism: \"Hear, O Israel: the our God, the is one\" (), found in , sometimes alternatively translated as \"The is our God, the alone\". Observant Jews consider the Shema to be the most important part of the prayer service in Judaism, and its twice-daily recitation as a \"mitzvah\" (religious commandment). It is traditional for Jews to say the Shema as their last words, and for parents to teach their children to say it before they go to sleep at night.\n\nAmong early Christians there was considerable debate over the nature of the Godhead, with some denying the incarnation but not the deity of Jesus (Docetism) and others later calling for an Arian conception of God. Despite at least one earlier local synod rejecting the claim of Arius, this Christological issue was to be one of the items addressed at the First Council of Nicaea.\n\nThe First Council of Nicaea, held in Nicaea (in present-day Turkey), convoked by the Roman Emperor Constantine I in 325, was the first ecumenical council of bishops of the Roman Empire, and most significantly resulted in the first uniform Christian doctrine, called the Nicene Creed. With the creation of the creed, a precedent was established for subsequent general ecumenical councils of bishops (synods) to create statements of belief and canons of doctrinal orthodoxy— the intent being to define a common creed for the Church and address heretical ideas.\n\nOne purpose of the council was to resolve disagreements in Alexandria over the nature of Jesus in relationship to the Father; in particular, whether Jesus was of the same substance as God the Father or merely of similar substance. All but two bishops took the first position; while Arius' argument failed.\n\nChristian orthodox traditions (Eastern Orthodox, Oriental Orthodox, Roman Catholic, and most Protestants) follow this decision, which was reaffirmed in 381 at the First Council of Constantinople and reached its full development through the work of the Cappadocian Fathers. They consider God to be a triune entity, called the Trinity, comprising three \"persons\", God the Father, God the Son, and God the Holy Spirit. These three are described as being \"of the same substance\" ().\n\nChristians overwhelmingly assert that monotheism is central to the Christian faith, as the Nicene Creed (and others), which gives the orthodox Christian definition of the Trinity, begins: \"I believe in one God\". From earlier than the times of the Nicene Creed, 325 CE, various Christian figures advocated the triune mystery-nature of God as a normative profession of faith. According to Roger E. Olson and Christopher Hall, through prayer, meditation, study and practice, the Christian community concluded \"that God must exist as both a unity and trinity\", codifying this in ecumenical council at the end of the 4th century.\n\nMost modern Christians believe the Godhead is triune, meaning that the three persons of the Trinity are in one union in which each person is also wholly God. They also hold to the doctrine of a man-god Christ Jesus as God incarnate. These Christians also do not believe that one of the three divine figures is God alone and the other two are not but that all three are mysteriously God and one. Other Christian religions, including Unitarian Universalism, Jehovah's Witnesses, Mormonism and others, do not share those views on the Trinity.\n\nSome Christian faiths, such as Mormonism, argue that the Godhead is in fact three separate individuals which include God the Father, His Son Jesus Christ, and the Holy Ghost. Each individual having a distinct purpose in the grand existence of human kind. Furthermore, Mormons believe that before the Council of Nicaea, the predominant belief among many early Christians was that the Godhead was three separate individuals. In support of this view, they cite early Christian examples of belief in subordinationism.\n\nUnitarianism is a theological movement, named for its understanding of God as one person, in direct contrast to Trinitarianism.\n\nIn Islam, God (Allāh) is all-powerful and all-knowing, the creator, sustainer, ordainer and judge of the universe. God in Islam is strictly singular (\"tawhid\") unique (\"wahid\") and inherently One (\"ahad\"), all-merciful and omnipotent. Allāh exists without place and the Qur'an states that \"No vision can grasp Him, but His grasp is over all vision. God is above all comprehension, yet is acquainted with all things\" (Qur'an 6:103) Allāh is the only God and the same God worshiped in Christianity and Judaism. ().\n\nIslam emerged in the 7th century CE in the context of both Christianity and Judaism, with some thematic elements similar to Gnosticism. Islamic belief states that Muhammad did not bring a new religion from God, but is rather the same religion as practiced by Abraham, Moses, David, Jesus and all the other prophets of God. The assertion of Islam is that the message of God had been corrupted, distorted or lost over time and the Quran was sent to Muhammad in order to correct the lost message of the Torah, New Testament and prior scriptures from God.\n\nThe Qur'an asserts the existence of a single and absolute truth that transcends the world; a unique and indivisible being who is independent of the creation. The Qur'an rejects binary modes of thinking such as the idea of a duality of God by arguing that both good and evil generate from God's creative act. God is a universal god rather than a local, tribal or parochial one; an absolute who integrates all affirmative values and brooks no evil. Ash'ari theology, which dominated Sunni Islam from the tenth to the nineteenth century, insists on ultimate divine transcendence and holds that divine unity is not accessible to human reason. Ash'arism teaches that human knowledge regarding it is limited to what has been revealed through the prophets, and on such paradoxes as God's creation of evil, revelation had to accept \"bila kayfa\" (without [asking] how).\n\n\"Tawhid\" constitutes the foremost article of the Muslim profession of faith, \"There is no god but God, Muhammad is the messenger of God. To attribute divinity to a created entity is the only unpardonable sin mentioned in the Qur'an. The entirety of the Islamic teaching rests on the principle of \"tawhid\".\n\nAs they traditionally profess a concept of monotheism with a singular person as God, Judaism and Islam reject the Christian idea of monotheism. Judaism uses the term Shituf to refer to non-monotheistic ways of worshiping God. Though Muslims venerate Jesus (Isa in Arabic) as a prophet, they do not accept the doctrine that he was a begotten son of God.\n\nAccording to the Quran, the Sabians were a monotheistic religious group. Some Hadiths account them as converts to Islam. However this interpretation may be related to the fact that Quraysh polytheists used to describe anyone who converted to Islam with the word \"Saba\" () which may either mean that this term was used for anyone who changed his religion or that they identified the message of Muhammed as a \"Sabian belief\". The former linguistic explanation (i.e. \"saba\" = \"changed his religion\") is the one adopted by most Muslim scholars.\n\nSabians are often identified with Mandaeism, a small monotheistic community which lives today in Iraq and call themselves \"Yahyawiya\" (). Muslim scholars traditionally viewed them as followers of the prophets Noah and \"Yahya\" (i.e. John the Baptist).\n\nGod in the Bahá'í Faith is taught to be a personal god, too great for humans to fully comprehend. Human primitive understanding of God is achieved through his revelations via his divine intermediary Manifestations. In the Bahá'í faith, such Christian doctrines as the Trinity are seen as compromising the Bahá'í view that God is single and has no equal.\nAnd the very existence of the Bahá'í Faith is a challenge to the Islamic doctrine of the finality of Muhammad's revelation.\nGod in the Bahá'í Faith communicates to humanity through divine intermediaries, known as Manifestations of God. These Manifestations establish religion in the world. It is through these divine intermediaries that humans can approach God, and through them God brings divine revelation and law.\n\nThe Oneness of God is one of the core teachings of the Bahá'í Faith. The obligatory prayers in the Bahá'í Faith involve explicit monotheistic testimony. God is the imperishable, uncreated being who is the source of all existence. He is described as \"a personal God, unknowable, inaccessible, the source of all Revelation, eternal, omniscient, omnipresent and almighty\". Although transcendent and inaccessible directly, his image is reflected in his creation. The purpose of creation is for the created to have the capacity to know and love its creator. God communicates his will and purpose to humanity through intermediaries, known as Manifestations of God, who are the prophets and messengers that have founded religions from prehistoric times up to the present day.\n\nRastafari, sometimes termed Rastafarianism, is classified as both a new religious movement and social movement. It developed in Jamaica during the 1930s. It lacks any centralised authority and there is much heterogeneity among practitioners, who are known as Rastafari, Rastafarians, or Rastas.\n\nRastafari refer to their beliefs, which are based on a specific interpretation of the Bible, as \"Rastalogy\". Central is a monotheistic belief in a single God—referred to as Jah—who partially resides within each individual. The former emperor of Ethiopia, Haile Selassie, is given central importance. Many Rastas regard him as an incarnation of Jah on Earth and as the Second Coming of Christ. Others regard him as a human prophet who fully recognised the inner divinity within every individual.\n\nAmenhotep IV initially introduced Atenism in Year 5 of his reign (1348/1346 BCE) during the 18th dynasty of the New Kingdom. He raised Aten, once a relatively obscure Egyptian Solar deity representing the disk of the sun, to the status of Supreme God in the Egyptian pantheon. To emphasise the change, Aten's name was written in the cartouche form normally reserved for Pharaohs, an innovation of Atenism. This religious reformation appears to coincide with the proclamation of a Sed festival, a sort of royal jubilee intended to reinforce the Pharaoh's divine powers of kingship. Traditionally held in the thirtieth year of the Pharaoh's reign, this possibly was a festival in honour of Amenhotep III, who some Egyptologists think had a coregency with his son Amenhotep IV of two to twelve years.\n\nYear 5 is believed to mark the beginning of Amenhotep IV's construction of a new capital, Akhetaten (\"Horizon of the Aten\"), at the site known today as Amarna. Evidence of this appears on three of the boundary stelae used to mark the boundaries of this new capital. At this time, Amenhotep IV officially changed his name to Akhenaten (\"Agreeable to Aten\") as evidence of his new worship. The date given for the event has been estimated to fall around January 2 of that year. In Year 7 of his reign (1346/1344 BCE), the capital was moved from Thebes to Akhetaten (near modern Amarna), though construction of the city seems to have continued for two more years. In shifting his court from the traditional ceremonial centres Akhenaten was signalling a dramatic transformation in the focus of religious and political power.\n\nThe move separated the Pharaoh and his court from the influence of the priesthood and from the traditional centres of worship, but his decree had deeper religious significance too—taken in conjunction with his name change, it is possible that the move to Amarna was also meant as a signal of Akhenaten's symbolic death and rebirth. It may also have coincided with the death of his father and the end of the coregency. In addition to constructing a new capital in honor of Aten, Akhenaten also oversaw the construction of some of the most massive temple complexes in ancient Egypt, including one at Karnak and one at Thebes, close to the old temple of Amun.\n\nIn Year 9 (1344/1342 BCE), Akhenaten declared a more radical version of his new religion, declaring Aten not merely the supreme god of the Egyptian pantheon, but the only God of Egypt, with himself as the sole intermediary between the Aten and the Egyptian people. Key features of Atenism included a ban on idols and other images of the Aten, with the exception of a rayed solar disc, in which the rays (commonly depicted ending in hands) appear to represent the unseen spirit of Aten. Aten was addressed by Akhenaten in prayers, such as the \"Great Hymn to the Aten\": \"O Sole God beside whom there is none\".\n\nThe details of Atenist theology are still unclear. The exclusion of all but one god and the prohibition of idols was a radical departure from Egyptian tradition, but most scholars see Akhenaten as a practitioner of monolatry rather than monotheism, as he did not actively deny the existence of other gods; he simply refrained from worshiping any but Aten. It is known that Atenism did not solely attribute divinity to the Aten. Akhenaten continued the cult of the Pharaoh, proclaiming himself the son of Aten and encouraging the Egyptian people to worship him. The Egyptian people were to worship Akhenaten; only Akhenaten and Nefertiti could worship Aten directly.\n\nUnder Akhenaten's successors, Egypt reverted to its traditional religion, and Akhenaten himself came to be reviled as a heretic.\n\nThe orthodox faith system held by most dynasties of China since at least the Shang Dynasty (1766 BCE) until the modern period centered on the worship of \"Shangdi\" (literally \"Above Sovereign\", generally translated as \"God\") or Heaven as an omnipotent force. This faith system pre-dated the development of Confucianism and Taoism and the introduction of Buddhism and Christianity. It has features of monotheism in that Heaven is seen as an omnipotent entity, a noncorporeal force with a personality transcending the world. From the writings of Confucius in the \"Analects\", it is known Confucius believed that Heaven cannot be deceived, Heaven guides people's lives and maintains a personal relationship with them, and that Heaven gives tasks for people to fulfill in order to teach them of virtues and morality. However, this faith system was not truly monotheistic since other lesser gods and spirits, which varied with locality, were also worshiped along with \"Shangdi\". Still, later variants such as Mohism (470 BCE–c.391 BCE) approached true monotheism, teaching that the function of lesser gods and ancestral spirits is merely to carry out the will of \"Shangdi\", akin to the angels in Abrahamic religions which in turn counts as only one god. In Mozi's \"Will of Heaven\" (天志), he writes:\n\nWorship of \"Shangdi\" and Heaven in ancient China includes the erection of shrines, the last and greatest being the Temple of Heaven in Beijing, and the offering of prayers. The ruler of China in every Chinese dynasty would perform annual sacrificial rituals to \"Shangdi\", usually by slaughtering a completely healthy bull as sacrifice. Although its popularity gradually diminished after the advent of Taoism and Buddhism, among other religions, its concepts remained in use throughout the pre-modern period and have been incorporated in later religions in China, including terminology used by early Christians in China. Despite the rising of non-theistic and pantheistic spirituality contributed by Taoism and Buddhism, Shangdi was still praised up until the end of the Qing Dynasty as the last ruler of the Qing declared himself son of heaven.\n\nThe Himba people of Namibia practice a form of monotheistic panentheism, and worship the god Mukuru. The deceased ancestors of the Himba and Herero are subservient to him, acting as intermediaries.\n\nThe Igbo people practice a form of monotheism called Odinani. Odinani has monotheistic and panentheistic attributes, having a single God as the source of all things. Although a pantheon of spirits exists, these are lesser spirits prevalent in Odinani expressly serving as elements of Chineke (or Chukwu), the supreme being or high god.\n\nWaaq is the name of a singular God in the traditional religion of many Cushitic people in the Horn of Africa, denoting an early monotheistic religion. However this religion was mostly replaced with the Abrahamic religions. Some (approximately 3%) of Oromo still follow this traditional monotheistic religion called Waaqeffannaa in Oromo.\n\nThe supreme god of the Proto-Indo-European religion was the god *\"Dyḗus Pḥtḗr \". A number of words derived from the name of this supreme deity are used in various Indo-European languages to denote a monotheistic God. Nonetheless, in spite of this, Proto-Indo-European religion itself was not monotheistic.\n\nIn western Eurasia, the ancient traditions of the Slavic religion contained elements of monotheism. In the sixth century AD, the Byzantine chronicler Procopius recorded that the Slavs \"acknowledge that one god, creator of lightning, is the only lord of all: to him do they sacrifice an ox and all sacrificial animals.\" The deity to whom Procopius is referring is the storm god Perún, whose name is derived from *\"Perkunos\", the Proto-Indo-European god of lightning. The ancient Slavs syncretized him with the Germanic god Thor and the Biblical prophet Elijah.\n\nAs an old religion, Hinduism inherits religious concepts spanning monotheism, polytheism, panentheism, pantheism, monism, and atheism among others; and its concept of God is complex and depends upon each individual and the tradition and philosophy followed.\n\nHindu views are broad and range from monism, through pantheism and panentheism (alternatively called monistic theism by some scholars) to monotheism and even atheism. Hinduism cannot be said to be purely polytheistic. Hindu religious leaders have repeatedly stressed that while God's forms are many and the ways to communicate with him are many, God is one. The \"puja\" of the \"murti\" is a way to communicate with the abstract one god (\"Brahman\") which creates, sustains and dissolves creation.\n\nRig Veda 1.164.46,\n\nTraditions of Gaudiya Vaishnavas, the Nimbarka Sampradaya and followers of Swaminarayan and Vallabha consider Krishna to be the source of all avatars, and the source of Vishnu himself, or to be the same as Narayana. As such, he is therefore regarded as \"Svayam Bhagavan\".\n\nWhen Krishna is recognized to be \"Svayam Bhagavan\", it can be understood that this is the belief of Gaudiya Vaishnavism, the Vallabha Sampradaya, and the Nimbarka Sampradaya, where Krishna is accepted to be the source of all other avatars, and the source of Vishnu himself. This belief is drawn primarily \"from the famous statement of the Bhagavatam\" (1.3.28). A viewpoint differing from this theological concept is the concept of Krishna as an \"avatar\" of Narayana or Vishnu. It should be however noted that although it is usual to speak of Vishnu as the source of the avataras, this is only one of the names of the God of Vaishnavism, who is also known as Narayana, Vasudeva and Krishna and behind each of those names there is a divine figure with attributed supremacy in Vaishnavism.\n\nThe Rig Veda discusses monotheistic thought, as do the Atharva Veda and Yajur Veda:\n\"Devas are always looking to the supreme abode of Vishnu\" (\"tad viṣṇoḥ paramaṁ padaṁ sadā paśyanti sṻrayaḥ\" Rig Veda 1.22.20)\n\n\"The One Truth, sages know by many names\" (Rig Veda 1.164.46)\n\n\"When at first the unborn sprung into being, He won His own dominion beyond which nothing higher has been in existence\" (Atharva Veda 10.7.31)\n\n\"There is none to compare with Him. There is no parallel to Him, whose glory, verily, is great.\" (Yajur Veda 32.3)\n\nThe number of auspicious qualities of God are countless, with the following six qualities (\"bhaga\") being the most important:\n\nIn the Shaivite tradition, the \"Shri Rudram\" (Sanskrit श्रि रुद्रम्), to which the Chamakam (चमकम्) is added by scriptural tradition, is a Hindu \"stotra\" dedicated to Rudra (an epithet of Shiva), taken from the Yajurveda (TS 4.5, 4.7). Shri Rudram is also known as \"Sri Rudraprasna\", \"\", and \"Rudradhyaya\". The text is important in Vedanta where Shiva is equated to the Universal supreme God. The hymn is an early example of enumerating the names of a deity, a tradition developed extensively in the sahasranama literature of Hinduism.\n\nThe Nyaya school of Hinduism has made several arguments regarding a monotheistic view. The Naiyanikas have given an argument that such a god can only be one. In the \"Nyaya Kusumanjali\", this is discussed against the proposition of the \"Mimamsa\" school that let us assume there were many demigods (\"devas\") and sages (\"rishis\") in the beginning, who wrote the Vedas and created the world. Nyaya says that:\n\n[If they assume such] omniscient beings, those endowed with the various superhuman faculties of assuming infinitesimal size, and so on, and capable of creating everything, then we reply that the \"law of parsimony\" bids us assume only one such, namely Him, the adorable Lord. There can be no confidence in a non-eternal and non-omniscient being, and hence it follows that according to the system which rejects God, the tradition of the Veda is simultaneously overthrown; there is no other way open.\n\nIn other words, Nyaya says that the polytheist would have to give elaborate proofs for the existence and origin of his several celestial spirits, none of which would be logical, and that it is more logical to assume one eternal, omniscient god.\n\nSikhi is a monotheistic and a revealed religion.\nGod in Sikhi is called \"Vāhigurū\", and is shapeless, timeless, and sightless: \"niraṅkār\", \"akaal\", and \"alakh\". God is present (\"sarav viāpak\") in all of creation. God must be seen from \"the inward eye\", or the \"heart\". Sikhi devotees must meditate to progress towards enlightenment, as its rigorous application permits the existence of communication between God and human beings.\n\nSikhism is a monotheistic faith that arose in northern India during the 16th and 17th centuries. Sikhs believe in one, timeless, omnipresent, supreme creator. The opening verse of the Guru Granth Sahib, known as the Mul Mantra, signifies this:\n\nThe word \"ੴ\" (\"Ik ōaṅkār\") has two components. The first is ੧, the digit \"1\" in Gurmukhi signifying the singularity of the creator. Together the word means: \"One Universal creator God\".\n\nIt is often said that the 1430 pages of the Guru Granth Sahib are all expansions on the Mul Mantra. Although the Sikhs have many names for God, some derived from Islam and Hinduism, they all refer to the same Supreme Being.\n\nThe Sikh holy scriptures refer to the One God who pervades the whole of space and is the creator of all beings in the universe. The following quotation from the Guru Granth Sahib highlights this point:\nHowever, there is a strong case for arguing that the Guru Granth Sahib teaches monism due to its non-dualistic tendencies:\nSikhs believe that God has been given many names, but they all refer to the One God, VāhiGurū. Sikhs believe that members of other religions such as Islam, Hinduism and Christianity all worship the same God, and the names Allah, Rahim, Karim, Hari, Raam and Paarbrahm are frequently mentioned in the Sikh holy scriptures. Although there is no set reference to God in Sikhism, the most commonly used Sikh reference to God is Akal Purakh (which means \"the true immortal\") or Waheguru, the Primal Being.\n\nZoroastrianism combines cosmogonic dualism and eschatological monotheism which makes it unique among the religions of the world. Zoroastrianism proclaims an evolution through time from dualism to monotheism.\n\nZoroastrianism is a monotheistic religion, although Zoroastrianism is often regarded as dualistic, duotheistic or bitheistic, for its belief in the hypostatis of the ultimately good Ahura Mazda \"(creative spirit)\" and the ultimately evil Angra Mainyu \"(destructive spirit)\". Zoroastrianism was once one of the largest religions on Earth, as the official religion of the Persian Empire. By some scholars, the Zoroastrians (\"Parsis\" or \"Zartoshtis\") are credited with being some of the first monotheists and having had influence on other world religions. Gathered statistics shows the number of adherents at as many as 3.5 million, with adherents living in many regions, including South Asia.\n\nThe surviving fragments of the poems of the classical Greek philosopher Xenophanes of Colophon suggest that he held views very similar to those of modern monotheists. His poems harshly criticize the traditional notion of anthropomorphic gods, commenting that \"...if cattle and horses and lions had hands or could paint with their hands and create works such as men do... [they] also would depict the gods' shapes and make their bodies of such a sort as the form they themselves have.\" Instead, Xenophanes declares that there is \"...one god, greatest among gods and humans, like mortals neither in form nor in thought.\" Xenophanes's theology appears to have been monist, but not truly monotheistic in the strictest sense. Although some later philosophers, such as Antisthenes, believed in doctrines similar to those expounded by Xenophanes, his ideas do not appear to have become widely popular.\n\nAlthough Plato himself was a polytheist, in his writings, he often presents Socrates as speaking of \"the god\" in the singular form. He does, however, often speak of the gods in the plural form as well. The Euthyphro dilemma, for example, is formulated as \"Is that which is holy loved by the gods because it is holy, or is it holy because it is loved by the gods?\"\n\nThe development of pure (philosophical) monotheism is a product of the Late Antiquity. During the 2nd to 3rd centuries, early Christianity was just one of several competing religious movements advocating monotheism.\n\n\"The One\" (Τὸ Ἕν) is a concept that is prominent in the writings of the Neoplatonists, especially those of the philosopher Plotinus. In the writings of Plotinus, \"The One\" is described as an inconceivable, transcendent, all-embodying, permanent, eternal, causative entity that permeates throughout all of existence.\nA number of oracles of Apollo from Didyma and Clarus, the so-called \"theological oracles\", dated to the 2nd and 3rd century CE, proclaim that there is only one highest god, of whom the gods of polytheistic religions are mere manifestations or servants. 4th century CE Cyprus had, besides Christianity, an apparently monotheistic cult of Dionysus.\n\nAristotle's concept of the \"Uncaused Cause\"—never incorporated into the polytheistic ancient Greek religion—has been used by many exponents of Abrahamic religions to justify their arguments for the existence of the Judeo-Christian-Islamic God of the Abrahamic religions.\n\nThe Hypsistarians were a religious group who believed in a most high god, according to Greek documents. Later revisions of this Hellenic religion were adjusted towards Monotheism as it gained consideration among a wider populace. The worship of Zeus as the head-god signaled a trend in the direction of monotheism, with less honour paid to the fragmented powers of the lesser gods.\n\nVarious New religious movements, such as Rastafari, Cao Đài, Tenrikyo, Seicho no Ie and Cheondoism are monotheistic.\n\nTengrism or Tangrism (sometimes stylized as Tengriism), occasionally referred to as Tengrianism , is a modern term for a Central Asian religion characterized by features of shamanism, animism, totemism, both polytheism and monotheism, and ancestor worship. Historically, it was the prevailing religion of the Bulgars, Turks, Mongols, and Hungarians, as well as the Xiongnu and the Huns. It was the state religion of the six ancient Turkic states: Avar Khaganate, Old Great Bulgaria, First Bulgarian Empire, Göktürks Khaganate, Eastern Tourkia and Western Turkic Khaganate. In \"Irk Bitig\", Tengri is mentioned as \"Türük Tängrisi\" (God of Turks). The term is perceived among Turkic peoples as a \"national\" religion.\n\nIn Sino-Tibetan and Turco-Mongol traditions, the Supreme God is commonly referred to as the ruler of Heaven, or the Sky Lord granted with omnipotent powers, but it has largely diminished in those regions due to ancestor worship, Taoism's pantheistic views and Buddhism's rejection of a creator God, although Mahayana Buddhism does seem to keep a sense of divinity. On some occasions in the mythology, the Sky Lord as identified as a male has been associated to mate with an Earth Mother, while some traditions kept the omnipotence of the Sky Lord unshared.\n\nNative American theology may be monotheistic, polytheistic, henotheistic, animistic, or some combination thereof.\n\nCherokee for example are monotheist as well as pantheist.\n\nThe Great Spirit, called \"Wakan Tanka\" among the Sioux, and \"Gitche Manitou\" in Algonquian, is a conception of universal spiritual force, or supreme being prevalent among some Native American and First Nation cultures. According to Lakota activist Russell Means a better translation of \"Wakan Tanka\" is the Great Mystery.\n\nSome researchers have interpreted Aztec philosophy as fundamentally monotheistic or panentheistic. While the populace at large believed in a polytheistic pantheon, Aztec priests and nobles might have come to an interpretation of Teotl as a single universal force with many facets. There has been criticism to this idea, however, most notably that many assertions of this supposed monotheism might actually come from post-Conquistador bias, imposing an Antiquity pagan model unto the Aztec.\n\n\n"}
{"id": "19524", "url": "https://en.wikipedia.org/wiki?curid=19524", "title": "May 9", "text": "May 9\n\n\n\n"}
{"id": "19525", "url": "https://en.wikipedia.org/wiki?curid=19525", "title": "Muay Thai", "text": "Muay Thai\n\nMuay Thai (, , ) or literally Thai boxing is a combat sport of Thailand that uses stand-up striking along with various clinching techniques.\nThis discipline is known as the \"art of eight limbs\" as it is characterized by the combined use of fists, elbows, knees, and shins.\nMuay Thai became widespread internationally in the 20th century, when practitioners from Thailand began competing in kickboxing, mixed rules matches, as well as matches under Muay Thai rules around the world. The professional league is governed by The Professional Boxing Association of Thailand (P.A.T) sanctioned by The Sports Authority of Thailand (SAT), and World Professional Muaythai Federation (WMF) overseas.\n\nIt is similar to related styles in other parts of the Indian cultural sphere, namely Lethwei in Myanmar, Pradal Serey in Cambodia, Muay Lao in Laos, and Tomoi in Malaysia.\n\nThe history of Muay Thai can also be traced to the middle of the 18th century. During the battles between the Burmese of the Konbaung Dynasty and Siam, the famous fighter Nai Khanomtom was captured in the year 1767. The Burmese knew of his expertise in hand-to-hand combat and gave him an opportunity to fight for his freedom. Nai Khanomtom managed to knock out ten consecutive Burmese contenders. Impressed by his boxing skill, he was freed by his captors and allowed to return to Siam. He was acknowledged as a hero, and his fighting style became known as Siamese-Style boxing, later to be known as Muay Thai. This fighting style was soon to be recognized as a national sport.\n\nMuay boran, and therefore Muay Thai, was originally called by more generic names such as \"Toi muay\" or simply \"muay\". As well as being a practical fighting technique for use in actual warfare, muay became a sport in which the opponents fought in front of spectators who went to watch for entertainment. These muay contests gradually became an integral part of local festivals and celebrations, especially those held at temples. Eventually, the previously bare-fisted fighters started wearing lengths of hemp rope around their hands and forearms. This type of match was called muay khat chueak (มวยคาดเชือก). Kickboxing was also a component of military training and gained prominence during the reign of King Naresuan in 1560 CE.\n\nMuay Thai is referred to as the \"Art of Eight Limbs\" or the \"Science of Eight Limbs\", because it makes use of punches, kicks, elbows and knee strikes, thus using eight \"points of contact\", as opposed to \"two points\" (fists) in boxing and \"four points\" (hands and feet) used in other more regulated combat sports, such as kickboxing and savate. A practitioner of Muay Thai is known as a \"nak muay\". Western practitioners are sometimes called \"Nak Muay Farang\", meaning \"foreign boxer\".\n\nThe ascension of King Chulalongkorn (Rama V) to the throne in 1868 ushered in a golden age not only for muay but for the whole country of Thailand. Muay progressed greatly during the reign of Rama V as a direct result of the king's personal interest in the sport. The country was at peace and muay functioned as a means of physical exercise, self-defense, attacking, recreation, and personal advancement.\n\n1909-1910: King Chulalongkorn formalizes Muay (Boran) by awarding (in 1910) 3 muen to victors at the funeral fights for his son (in 1909). The region style: Lopburi, Korat, and Chaiya.\n\n1913: British boxing introduced into the curriculum of the Suan Kulap College. The 1st descriptive use of the term “Muay Thai”\n\n1919: British boxing and Muay taught as one sport in the curriculum of the Suan Kulap College. Judo also offered.\n\n1921: 1st permanent ring in Siam at Suan Kulap College. Used for both Muay and British Boxing.\n\n1923: Suan Sanuk Stadium. First international style 3-rope ring with red and blue padded corners, near Lumpinee Park. Muay and British Boxing.\n\nKing Rama VII (r. 1925–35) pushed for codified rules for muay, and they were put into place. Thailand's first boxing ring was built in 1921 at Suan Kularp. Referees were introduced and rounds were now timed by kick. Fighters at the Lumpinee Boxing Stadium began wearing modern gloves, as well as hard groin protectors, during training and in boxing matches against foreigners. Traditional rope-binding (Kaad Chuek) made the hands a hardened, dangerous striking tool. The use of knots in the rope over the knuckles made the strikes more abrasive and damaging for the opponent while protecting the hands of the fighter. This rope-binding was still used in fights between Thais but after the occurrence of a death in the ring, it was decided that fighters should wear gloves and cotton coverlets over the feet and ankles. It was also around this time that the term \"Muay Thai\" became commonly used, while the older form of the style came to be known as \"Muay Boran\", which is now performed primarily as an exhibition art form.\n\nIn 1993, the International Federation of Muaythai Amateur, or IFMA was inaugurated. It became the governing body of amateur Muay Thai consisting of 128 member countries worldwide and is recognized by Olympic Council of Asia.\n\nIn 1995, World Muaythai Council, the oldest and largest professional sanctioning organizations of Muay Thai was set up by the Royal Thai Government and sanctioned by the Sports Authority of Thailand.\n\nIn 1995, the World Muay Thai Federation was founded via the merger of two existing organizations, and established in Bangkok becoming the federation governing international Muay Thai. As of August 2012, it had over 70 member countries. Its President is elected at the World Muay Thai Congress.\n\nIn 2006, Muay Thai was included in SportAccord with IFMA. One of the requirements of SportAccord was that no sport can have a name of a country in its name. As a result, an amendment was made in the IFMA constitution to change the name of the sport from \"Muay Thai\" to \"Muaythai\" – written as one word in accordance with Olympic requirements.\n\nIn 2014 Muay Thai was included in the International World Games Association (IWGA) and will be represented in the official programme of The World Games 2017 in Wrocław, Poland.\n\nIn January 2015, Muay Thai was granted the Patronage of the International University Sports Federation (FISU) and on March the 16th to the 23rd, 2015 the first University World Muaythai Cup will be held in Bangkok.\n\nToday, there are thousands of gyms spread out across the globe.\n\nAccording to Thai folklore at the time of the fall of the ancient Siamese capital of Ayutthaya Kingdom in 1767, the invading Burmese troops rounded up thousands of Siamese and took them to Burma as prisoners. Among them were a large number of Thai boxers, who were taken to the city of Ava.\n\nIn 1774, in the Burmese city of Rangoon, the Burmese King Hsinbyushin (known in Thai as \"King Mangra\") decided to organize a seven-day, seven-night religious festival in honor of Buddha's relics. The festivities included many forms of entertainment, such as the costume plays called \"likay\", comedies and farces, and sword-fighting matches. At one point, King Hsinbyushin wanted to see how Muay Boran would compare to the Lethwei (Burmese Boxing). \"Nai Khanomtom\" was selected to fight against the Burmese champion. The boxing ring was set up in front of the throne and Nai Khanomtom did a traditional Wai Kru pre-fight dance, to pay his respects to his teachers and ancestors, as well as the spectators, dancing around his opponent. This amazed and perplexed the Burmese people, who thought it was black magic. When the fight began, Nai Khanomtom charged out, using punches, kicks, elbows, and knees to pummel his opponent until he collapsed.\n\nHowever the Burmese referee said the Burmese champion was too distracted by the dance, and declared the knockout invalid. The King then asked if Nai Khanomtom would fight nine other Burmese champions to prove himself. He agreed and fought them all, one after the other with no rest periods in between. His last opponent was a great kickboxing teacher from Rakhine. Nai Khanomtom mangled him by his kicks and no one else dared to challenge him.\n\nKing Mangra was so impressed that he allegedly remarked that \"Every part of the Siamese is blessed with venom. Even with his bare hands, he can fell nine or ten opponents. But his Lord was incompetent and lost the country to the enemy. If he had been any good, there was no way the City of Ayutthaya would ever have fallen.\"\n\nKing Mangra granted Nai Khanomtom freedom along with either riches or two beautiful Burmese wives. Nai Khanomtom chose the wives as he said that money was easier to find. He then departed with his wives for Siam. Other variations of this story had him also winning the release of his fellow Thai prisoners. His feat is celebrated every March 17 as Boxer's Day or National Muay Boran Day in his honor and that of muay boran's.\n\nToday, some have wrongly attributed the legend of Nai Khanomtom to King Naresuan, who spent his youth as a royal hostage in Burma while Ayutthaya was a Burmese vassal. However, Nai Khanomtom and King Naresuan lived almost two centuries apart.\n\nFormal Muay Thai techniques are divided into two groups: \"mae mai\", or major techniques, and \"luk mai\", or minor techniques. Muay Thai is often a fighting art of attrition, where opponents exchange blows with one another. This is certainly the case with traditional stylists in Thailand, but is a less popular form of fighting in the contemporary world fighting circuit where the Thai style of exchanging blow for blow is no longer favorable. Almost all techniques in Muay Thai use the entire body movement, rotating the hip with each kick, punch, elbow and block.\n\nThe punch techniques in Muay Thai were originally quite limited, being crosses and a long (or lazy) circular strike made with a straight (but not locked) arm and landing with the heel of the palm. Cross-fertilization with Western boxing and western martial arts mean the full range of western boxing punches are now used: lead jab, straight/cross, hook, uppercut, shovel and corkscrew punches and overhands as well as hammer fists and back fists.\n\nAs a tactic, body punching is used less in Muay Thai than most other striking combat sports to avoid exposing the attacker's head to counter strikes from knees or elbows. To utilize the range of targeting points, in keeping with the center line theory, the fighter can use either the Western or Thai stance which allows for either long range or short range attacks to be undertaken effectively without compromising guard.\n\nThe elbow can be used in several ways as a striking weapon: horizontal, diagonal-upwards, diagonal-downwards, uppercut, downward, backward-spinning and flying. From the side, it can be used as either a finishing move or as a way to cut the opponent's eyebrow so that blood might block his vision. The diagonal elbows are faster than the other forms but are less powerful. The elbow strike is considered the most dangerous form of attack in the sport.\n\nThere is a distinct difference between a single elbow and a follow-up elbow. The single elbow is a move independent from any other, whereas a follow-up elbow is the second strike from the same arm, being a hook or straight punch first with an elbow follow-up. Such elbows, and most other elbow strikes, are used when the distance between fighters becomes too small and there is too little space to throw a hook at the opponent's head.\n\nElbows can be used to great effect as blocks or defenses against, for example, spring knees, side body knees, body kicks or punches. When well connected, an elbow strike can cause serious damage to the opponent, including cuts or even a knockout.\n\nThe two most common kicks in Muay Thai are known as the \"thip\" (literally \"foot jab\") and the \"te chiang\" (kicking upwards in the shape of a triangle cutting under the arm and ribs), or roundhouse kick. The Thai roundhouse kick uses a rotational movement of the entire body and has been widely adopted by practitioners of other combat sports. It is done from a circular stance with the back leg just a little ways back (roughly shoulder width apart) in comparison to instinctive upper body fighting (boxing) where the legs must create a wider base. The roundhouse kick draws its power almost entirely from the rotational movement of the hips, counter-rotation of the shoulders and arms are also often used to add torque to the lower body and increase the power of the kick as well.\n\nIf a roundhouse kick is attempted by the opponent, the Thai boxer will normally check the kick, that is, he will block the kick with the outside of his lower leg. Thai boxers are trained to always connect with the shin. The foot contains many fine bones and is much weaker. A fighter may end up hurting himself if he tries to strike with his foot or instep. Shins are trained by repeatedly striking firm objects, such as pads or heavy bags.\n\nThe foot-thrust, or literally, \"foot jab\", is one of the techniques in Muay Thai. It is mainly used as a defensive technique to control distance or block attacks. Foot-thrusts should be thrown quickly but with enough force to knock an opponent off balance.\n\nIn Western boxing, the two fighters are separated when they clinch; in Muay Thai, however, they are not. It is often in the clinch where knee and elbow techniques are used. To strike and bind the opponent for both offensive and defensive purposes, small amounts of stand-up grappling are used in the clinch. The front clinch should be performed with the palm of one hand on the back of the other. There are three reasons why the fingers must not be intertwined. 1) In the ring fighters are wearing boxing gloves and cannot intertwine their fingers. 2) The Thai front clinch involves pressing the head of the opponent downwards, which is easier if the hands are locked behind the back of the head instead of behind the neck. Furthermore, the arms should be putting as much pressure on the neck as possible. 3) A fighter may incur an injury to one or more fingers if they are intertwined, and it becomes more difficult to release the grip in order to quickly elbow the opponent's head.\n\nA correct clinch also involves the fighter's forearms pressing against the opponent's collar bone while the hands are around the opponent's head rather than the opponent's neck. The general way to get out of a clinch is to push the opponent's head backward or elbow them, as the clinch requires both participants to be very close to one another. Additionally, the non-dominant clincher can try to \"swim\" their arm underneath and inside the opponent's clinch, establishing the previously non-dominant clincher as the dominant clincher.\n\nMuay Thai has several other variants of the clinch or \"chap kho\" , including:\nDefenses in Muay Thai are categorized in six groups:\n\n\nDefensively, the concept of \"wall of defense\" is used, in which shoulders, arms and legs are used to hinder the attacker from successfully executing techniques. Blocking is a critical element in Muay Thai and compounds the level of conditioning a successful practitioner must possess. Low and mid body roundhouse kicks are normally blocked with the upper portion of a raised shin (this block is known as a 'check'). High body strikes are blocked ideally with the forearms and shoulder together, or if enough time is allowed for a parry, the glove (elusively), elbow, or shin will be used. Midsection roundhouse kicks can also be caught/trapped, allowing for a sweep or counter-attack to the remaining leg of the opponent. Punches are blocked with an ordinary boxing guard and techniques similar, if not identical, to basic boxing technique. A common means of blocking a punch is using the hand on the same side as the oncoming punch. For example, if an orthodox fighter throws a jab (being the left hand), the defender will make a slight tap to redirect the punch's angle with the right hand. The deflection is always as small and precise as possible to avoid unnecessary energy expenditure and return the hand to the guard as quickly as possible. Hooks are often blocked with a motion sometimes described as \"combing the hair\", that is, raising the elbow forward and effectively shielding the head with the forearm, flexed biceps and shoulder. More advanced Muay Thai blocks are usually in the form of counter-strikes, using the opponent's weight (as they strike) to amplify the damage that the countering opponent can deliver. This requires impeccable timing and thus can generally only be learned by many repetitions.\n\nLike most full contact fighting sports, Muay Thai has a heavy focus on body conditioning. Muay Thai is specifically designed to promote the level of fitness and toughness required for ring competition. Training regimens include many staples of combat sport conditioning such as running, shadowboxing, rope jumping, body weight resistance exercises, medicine ball exercises, abdominal exercises, and in some cases weight training. Thai boxers rely heavily on kicks utilizing the shin bone. As such, practitioners of Muay Thai will repeatedly hit a dense heavy bag with their shins, conditioning it, hardening the bone through a process called cortical remodeling. Striking a sand-filled bag will also have the same effect.\n\nTraining specific to a Thai fighter includes training with coaches on Thai pads, focus mitts, heavy bag, and sparring. Daily training includes many rounds (3–5 minute periods broken up by a short rest, often 1–2 minutes) of these various methods of practice. Thai pad training is a cornerstone of Muay Thai conditioning that involves practicing punches, kicks, knees, and elbow strikes with a trainer wearing thick pads covering the forearms and hands. These special pads (often referred to as Thai pads) are used to absorb the impact of the fighter's strikes and allow the fighter to react to the attacks of the pad holder in a live situation. The trainer will often also wear a belly pad around the abdominal area so that the fighter can attack with straight kicks or knees to the body at any time during the round.\n\nFocus mitts are specific to training a fighter's hand speed, punch combinations, timing, punching power, defense, and counter-punching and may also be used to practice elbow strikes. Heavy bag training is a conditioning and power exercise that reinforces the techniques practiced on the pads. Sparring is a means to test technique, skills, range, strategy, and timing against a partner. Sparring is often a light to medium contact exercise because competitive fighters on a full schedule are not advised to risk injury by sparring hard. Specific tactics and strategies can be trained with sparring including in close fighting, clinching and kneeing only, cutting off the ring, or using reach and distance to keep an aggressive fighter away.\n\nDue to the rigorous training regimen (some Thai boxers fight almost every other week) professional boxers in Thailand have relatively short careers in the ring. Many retire from competition to begin instructing the next generation of Thai fighters. Most professional Thai boxers come from lower economic backgrounds, and the purse (after other parties get their cut) is sought as means of support for the fighters and their families. Very few higher economic strata Thais join the professional Muay Thai ranks; they usually either do not practice the sport or practice it only as amateur Muay Thai boxers.\n\nThe mongkhon, or mongkol (headband) and pra jiad (armbands) are often worn into the ring before the match begins. They originate back in times when Thailand was in a constant state of war, where young men would tear off pieces of a loved one's clothing (often their mother's sarong) and wear it in battle for good luck as well as to ward off harmful spirits. In modern times the mongkol (lit. 'holy spirit, luck, and protection') is worn as a tribute to the fighter's gym. The mongkol is traditionally presented by a trainer to the fighter once he feels that the fighter is ready to represent the gym in the ring. Often after the fighter has finished the \"wai kru\", the trainer will take the mongkol off of his head and place it in his corner of the ring for luck. They were also used for protection. Whether the fighter is a Buddhist or not, it is common for them to bring the mongkol to a Buddhist monk who blesses it with good luck prior to stepping into the ring.\n\nIn 2016, 9,998 children under the age of 15 were registered with Board of Boxing under the Sport Authority of Thailand, according to Child Safety Promotion and Injury Prevention Research Centre (CSIP). 420 \"young boxers\" registered with the board annually, between 2007 and 2015. Some estimates put the number of child boxers nationwide at between 200,000–300,000, some as young as four years old.\n\nThe Advanced Diagnostic Imaging Centre (AIMC) at Ramathibodi Hospital studied 300 child boxers aged under 15 with two to more than five years of experience, as well as 200 children who do not box. The findings show that child boxers not only sustain brain injuries, they also have a lower IQ, about 10 points lower than average levels. Moreover, IQ levels correlate with the length of their training. Beyond brain damage, the death of young fighters in the ring sometimes occurs.\n\nAdisak Plitapolkarnpim, director of CSIP, was indirectly quoted (in 2016) as having said that Muay Thai practitioners \"younger than 15 years old are being urged to avoid 'head contact' to reduce the risk of brain injuries, while children aged under nine should be banned from the combat fight\"; furthermore the Boxing Act's minimum age to compete professionally\", was largely being flouted; furthermore, indirectly quoted: \"Boxers aged between 13 and 15\" should still be permitted to compete, but \"with light contact to the head and face\"; He said that \"Spectators and a change in the boxing rules can play a vital role in preventing child boxers from suffering brain injuries, abnormality in brain structure, Parkinson's disease and early-onset Alzheimer's later in life...Children aged between nine and 15 can take part in [Thai] boxing, but direct head contact must not be allowed\". Referring to \"Findings [of 2014] on the Worst Forms of Child Labour\" as published by the US Department of Labor's Bureau of International Labor Affairs, he said that, \"We know Muay Thai paid fighters have been exploited in the past like child labourers and the matter still remains a serious concern\".\n\nAt the 13th World Conference on Injury Prevention and Safety Promotion in 2018, it was revealed that up to three percent of the upcoming generation will grow up with learning disabilities unless an amendment is ratified that bans children under 12 from participating in boxing matches. International pediatricians have called on lawmakers in Thailand to help.\n\nMuay Thai is a combat sport that utilizes eight different parts of the body (fists, elbows, knees, and shins), with that being said injuries are quite common in all levels of Muay Thai. An injury is considered reportable if it requires the athlete to rest for more than one day. Many injuries in the sport of Muay Thai go unreported as the fighters may not notice the injuries at first, refusing to admit they need treatment, a heightened pain threshold, fear that their instructor will perceive the injury negatively, and confusion as to what is an injury. Similar to most sports, injury rates tend to be higher in beginners rather than amateurs and professionals. Soft tissue injuries are the most common form of injury in Muay Thai, contributing between 80-90% of all injuries. These injuries are caused by repeated trauma to soft parts of the body. During matches there is little to no padding and that leaves soft tissue vulnerable to strikes. The second most common injury among beginner and amateur Muay Thai fighters are sprains and strains. It appears that these injuries can be easily avoided or reduced. Many participants of a study admitted to inadequate warm up before the event of the injury.The third most common injury are fractures. Fractures are more commonly seen with amateur and professional fighters, because they are allowed full contact and beginners are allowed no contact. The most common sites for fractures are the nose, carpal bones, metacarpals, digits, and ribs. The distribution of injuries are significantly different between the three groups (beginner, amateur, and professional), this seems to be expected as a person progresses through the different levels the forces involved are a lot higher, less padding and protective equipment is used, and athletes are likely to train harder, resulting in more serious injuries among experienced fighters.\n\n\n"}
{"id": "19527", "url": "https://en.wikipedia.org/wiki?curid=19527", "title": "Mao Zedong", "text": "Mao Zedong\n\nMao Zedong (; December 26, 1893September 9, 1976), also known as Chairman Mao, was a Chinese communist revolutionary who became the founding father of the People's Republic of China, which he ruled as the Chairman of the Communist Party of China from its establishment in 1949 until his death in 1976. His theories, military strategies, and political policies are collectively known as Maoism.\n\nMao was the son of a wealthy farmer in Shaoshan, Hunan. He had a Chinese nationalist and anti-imperialist outlook early in his life, and was particularly influenced by the events of the Xinhai Revolution of 1911 and May Fourth Movement of 1919. He later adopted Marxism–Leninism while working at Peking University, and became a founding member of the Communist Party of China (CPC), leading the Autumn Harvest Uprising in 1927. During the Chinese Civil War between the Kuomintang (KMT) and the CPC, Mao helped to found the Chinese Workers' and Peasants' Red Army, led the Jiangxi Soviet's radical land policies, and ultimately became head of the CPC during the Long March. Although the CPC temporarily allied with the KMT under the United Front during the Second Sino-Japanese War (1937–1945), China's civil war resumed after Japan's surrender and in 1949 Mao's forces defeated the Nationalist government, which withdrew to Taiwan.\n\nOn October 1, 1949, Mao proclaimed the foundation of the People's Republic of China (PRC), a single-party state controlled by the CPC. In the following years he solidified his control through land reforms and through a psychological victory in the Korean War, as well as through campaigns against landlords, people he termed \"counter-revolutionaries\", and other perceived enemies of the state. In 1957 he launched a campaign known as the Great Leap Forward that aimed to rapidly transform China's economy from agrarian to industrial. This campaign led to the deadliest famine in history and the deaths of 20–45 million people between 1958 and 1962. In 1966, Mao initiated the Cultural Revolution, a program to remove \"counter-revolutionary\" elements in Chinese society which lasted 10 years and was marked by violent class struggle, widespread destruction of cultural artifacts, and an unprecedented elevation of Mao's cult of personality. The program is now officially regarded as a \"severe setback\" for the PRC. In 1972, Mao welcomed American President Richard Nixon in Beijing, signalling the start of a policy of opening China to the world. After years of ill health, Mao suffered a series of heart attacks in 1976 and died at the age of 82. He was succeeded as paramount leader by Premier Hua Guofeng, who was quickly sidelined and replaced by Deng Xiaoping.\n\nA controversial figure, Mao is regarded as one of the most important and influential individuals in modern world history. He is also known as a political intellect, theorist, military strategist, poet, and visionary. Supporters credit him with driving imperialism out of China, modernising the nation and building it into a world power, promoting the status of women, improving education and health care, as well as increasing life expectancy as China's population grew from around 550 million to over 900 million under his leadership. Conversely, his regime has been called autocratic and totalitarian, and condemned for bringing about mass repression and destroying religious and cultural artifacts and sites. It was additionally responsible for vast numbers of deaths with estimates ranging from 30 to 70 million victims.\n\nMao Zedong was born on December 26, 1893, in Shaoshan village, Hunan Province, China. His father, Mao Yichang, was a formerly impoverished peasant who had become one of the wealthiest farmers in Shaoshan. Growing up in rural Hunan, Mao described his father as a stern disciplinarian, who would beat him and his three siblings, the boys Zemin and Zetan, as well as an adopted girl, Zejian. Mao's mother, Wen Qimei, was a devout Buddhist who tried to temper her husband's strict attitude. Mao too became a Buddhist, but abandoned this faith in his mid-teenage years. At age 8, Mao was sent to Shaoshan Primary School. Learning the value systems of Confucianism, he later admitted that he didn't enjoy the classical Chinese texts preaching Confucian morals, instead favouring popular novels like \"Romance of the Three Kingdoms\" and \"Water Margin\". At age 13, Mao finished primary education, and his father united him in an arranged marriage to the 17-year-old Luo Yixiu, thereby uniting their land-owning families. Mao refused to recognise her as his wife, becoming a fierce critic of arranged marriage and temporarily moving away. Luo was locally disgraced and died in 1910.\n\nWhile working on his father's farm, Mao read voraciously and developed a \"political consciousness\" from Zheng Guanying's booklet which lamented the deterioration of Chinese power and argued for the adoption of representative democracy. Interested in history, Mao was inspired by the military prowess and nationalistic fervour of George Washington and Napoleon Bonaparte. His political views were shaped by Gelaohui-led protests which erupted following a famine in Changsha, the capital of Hunan; Mao supported the protesters' demands, but the armed forces suppressed the dissenters and executed their leaders. The famine spread to Shaoshan, where starving peasants seized his father's grain. He disapproved of their actions as morally wrong, but claimed sympathy for their situation. At age 16, Mao moved to a higher primary school in nearby Dongshan, where he was bullied for his peasant background.\n\nIn 1911, Mao began middle school in Changsha. Revolutionary sentiment was strong in the city, where there was widespread animosity towards Emperor Puyi's absolute monarchy and many were advocating republicanism. The republicans' figurehead was Sun Yat-sen, an American-educated Christian who led the Tongmenghui society. In Changsha, Mao was influenced by Sun's newspaper, \"The People's Independence\" (\"Minli bao\"), and called for Sun to become president in a school essay. As a symbol of rebellion against the Manchu monarch, Mao and a friend cut off their queue pigtails, a sign of subservience to the emperor.\n\nInspired by Sun's republicanism, the army rose up across southern China, sparking the Xinhai Revolution. Changsha's governor fled, leaving the city in republican control. Supporting the revolution, Mao joined the rebel army as a private soldier, but was not involved in fighting. The northern provinces remained loyal to the emperor, and hoping to avoid a civil war, Sun—proclaimed \"provisional president\" by his supporters—compromised with the monarchist general Yuan Shikai. The monarchy was abolished, creating the Republic of China, but the monarchist Yuan became president. The revolution over, Mao resigned from the army in 1912, after six months as a soldier. Around this time, Mao discovered socialism from a newspaper article; proceeding to read pamphlets by Jiang Kanghu, the student founder of the Chinese Socialist Party, Mao remained interested yet unconvinced by the idea.\n\nOver the next few years, Mao Zedong enrolled and dropped out of a police academy, a soap-production school, a law school, an economics school, and the government-run Changsha Middle School. Studying independently, he spent much time in Changsha's library, reading core works of classical liberalism such as Adam Smith's \"The Wealth of Nations\" and Montesquieu's \"The Spirit of the Laws\", as well as the works of western scientists and philosophers such as Darwin, Mill, Rousseau, and Spencer. Viewing himself as an intellectual, years later he admitted that at this time he thought himself better than working people. He was inspired by Friedrich Paulsen, whose liberal emphasis on individualism led Mao to believe that strong individuals were not bound by moral codes but should strive for the greater good, and that the \"end justifies the means\" conclusion of Consequentialism. His father saw no use in his son's intellectual pursuits, cut off his allowance and forced him to move into a hostel for the destitute.\n\nMao desired to become a teacher and enrolled at the Fourth Normal School of Changsha, which soon merged with the First Normal School of Changsha, widely seen as the best in Hunan. Befriending Mao, professor Yang Changji urged him to read a radical newspaper, \"New Youth\" (\"Xin qingnian\"), the creation of his friend Chen Duxiu, a dean at Peking University. Although a Chinese nationalist, Chen argued that China must look to the west to cleanse itself of superstition and autocracy. Mao published his first article in \"New Youth\" in April 1917, instructing readers to increase their physical strength to serve the revolution. He joined the Society for the Study of Wang Fuzhi (\"Chuan-shan Hsüeh-she\"), a revolutionary group founded by Changsha literati who wished to emulate the philosopher Wang Fuzhi.\n\nIn his first school year, Mao befriended an older student, Xiao Zisheng; together they went on a walking tour of Hunan, begging and writing literary couplets to obtain food. A popular student, in 1915 Mao was elected secretary of the Students Society. He organized the Association for Student Self-Government and led protests against school rules. In spring 1917, he was elected to command the students' volunteer army, set up to defend the school from marauding soldiers. Increasingly interested in the techniques of war, he took a keen interest in World War I, and also began to develop a sense of solidarity with workers. Mao undertook feats of physical endurance with Xiao Zisheng and Cai Hesen, and with other young revolutionaries they formed the Renovation of the People Study Society in April 1918 to debate Chen Duxiu's ideas. Desiring personal and societal transformation, the Society gained 70–80 members, many of whom would later join the Communist Party. Mao graduated in June 1919, ranked third in the year.\n\nMao moved to Beijing, where his mentor Yang Changji had taken a job at Peking University. Yang thought Mao exceptionally \"intelligent and handsome\", securing him a job as assistant to the university librarian Li Dazhao, an early Chinese Communist. Li authored a series of \"New Youth\" articles on the October Revolution in Russia, during which the Communist Bolshevik Party under the leadership of Vladimir Lenin had seized power. Lenin was an advocate of the socio-political theory of Marxism, first developed by the German sociologists Karl Marx and Friedrich Engels, and Li's articles brought an understanding of Marxism to the Chinese revolutionary movement. Becoming \"more and more radical\", Mao was influenced by Peter Kropotkin's anarchism, but joined Li's Study Group and \"developed rapidly toward Marxism\" during the winter of 1919.\n\nPaid a low wage, Mao lived in a cramped room with seven other Hunanese students, but believed that Beijing's beauty offered \"vivid and living compensation\". At the university, Mao was widely snubbed by other students due to his rural Hunanese accent and lowly position. He joined the university's Philosophy and Journalism Societies and attended lectures and seminars by the likes of Chen Duxiu, Hu Shi, and Qian Xuantong. Mao's time in Beijing ended in the spring of 1919, when he travelled to Shanghai with friends who were preparing to leave for France. He did not return to Shaoshan, where his mother was terminally ill. She died in October 1919, with her husband dying in January 1920.\n\nOn May 4, 1919, students in Beijing gathered at the Gate of Heavenly Peace to protest the Chinese government's weak resistance to Japanese expansion in China. Patriots were outraged at the influence given to Japan in the Twenty-One Demands in 1915, the complicity of Duan Qirui's Beiyang Government, and the betrayal of China in the Treaty of Versailles, wherein Japan was allowed to receive territories in Shandong which had been surrendered by Germany. These demonstrations ignited the nationwide May Fourth Movement and fueled the New Culture Movement which blamed China's diplomatic defeats on social and cultural backwardness.\n\nIn Changsha, Mao had begun teaching history at the Xiuye Primary School and organizing protests against the pro-Duan Governor of Hunan Province, Zhang Jingyao, popularly known as \"Zhang the Venomous\" due to his corrupt and violent rule. In late May, Mao co-founded the Hunanese Student Association with He Shuheng and Deng Zhongxia, organizing a student strike for June and in July 1919 began production of a weekly radical magazine, \"Xiang River Review\" (\"Xiangjiang pinglun\"). Using vernacular language that would be understandable to the majority of China's populace, he advocated the need for a \"Great Union of the Popular Masses\", strengthened trade unions able to wage non-violent revolution. His ideas were not Marxist, but heavily influenced by Kropotkin's concept of .\n\nZhang banned the Student Association, but Mao continued publishing after assuming editorship of the liberal magazine \"New Hunan\" (\"Xin Hunan\") and offered articles in popular local newspaper \"Justice\" (\"Ta Kung Po\"). Several of these advocated feminist views, calling for the liberation of women in Chinese society; Mao was influenced by his forced arranged-marriage. In December 1919, Mao helped organise a general strike in Hunan, securing some concessions, but Mao and other student leaders felt threatened by Zhang, and Mao returned to Beijing, visiting the terminally ill Yang Changji. Mao found that his articles had achieved a level of fame among the revolutionary movement, and set about soliciting support in overthrowing Zhang. Coming across newly translated Marxist literature by Thomas Kirkup, Karl Kautsky, and Marx and Engels—notably \"The Communist Manifesto\"—he came under their increasing influence, but was still eclectic in his views.\n\nMao visited Tianjin, Jinan, and Qufu, before moving to Shanghai, where he worked as a laundryman and met Chen Duxiu, noting that Chen's adoption of Marxism \"deeply impressed me at what was probably a critical period in my life\". In Shanghai, Mao met an old teacher of his, Yi Peiji, a revolutionary and member of the Kuomintang (KMT), or Chinese Nationalist Party, which was gaining increasing support and influence. Yi introduced Mao to General Tan Yankai, a senior KMT member who held the loyalty of troops stationed along the Hunanese border with Guangdong. Tan was plotting to overthrow Zhang, and Mao aided him by organizing the Changsha students. In June 1920, Tan led his troops into Changsha, and Zhang fled. In the subsequent reorganization of the provincial administration, Mao was appointed headmaster of the junior section of the First Normal School. Now receiving a large income, he married Yang Kaihui in the winter of 1920.\n\nThe Communist Party of China was founded by Chen Duxiu and Li Dazhao in the French concession of Shanghai in 1921 as a study society and informal network. Mao set up a Changsha branch, also establishing a branch of the Socialist Youth Corps. Opening a bookstore under the control of his new Cultural Book Society, its purpose was to propagate revolutionary literature throughout Hunan. He was involved in the movement for Hunan autonomy, in the hope that a Hunanese constitution would increase civil liberties and make his revolutionary activity easier. When the movement was successful in establishing provincial autonomy under a new warlord, Mao forgot his involvement. By 1921, small Marxist groups existed in Shanghai, Beijing, Changsha, Wuhan, Guangzhou, and Jinan; it was decided to hold a central meeting, which began in Shanghai on July 23, 1921. The first session of the National Congress of the Communist Party of China was attended by 13 delegates, Mao included. After the authorities sent a police spy to the congress, the delegates moved to a boat on South Lake near Jiaxing, in Zhejiang, to escape detection. Although Soviet and Comintern delegates attended, the first congress ignored Lenin's advice to accept a temporary alliance between the Communists and the \"bourgeois democrats\" who also advocated national revolution; instead they stuck to the orthodox Marxist belief that only the urban proletariat could lead a socialist revolution.\n\nMao was now party secretary for Hunan stationed in Changsha, and to build the party there he followed a variety of tactics. In August 1921, he founded the Self-Study University, through which readers could gain access to revolutionary literature, housed in the premises of the Society for the Study of Wang Fuzhi, a Qing dynasty Hunanese philosopher who had resisted the Manchus. He joined the YMCA Mass Education Movement to fight illiteracy, though he edited the textbooks to include radical sentiments. He continued organizing workers to strike against the administration of Hunan Governor Zhao Hengti. Yet labor issues remained central. The successful and famous Anyuan coal mines strikes (contrary to later Party historians) depended on both \"proletarian\" and \"bourgeois\" strategies. Liu Shaoqi and Li Lisan and Mao not only mobilised the miners, but formed schools and cooperatives and engaged local intellectuals, gentry, military officers, merchants, Red Gang dragon heads and even church clergy.\n\nMao claimed that he missed the July 1922 Second Congress of the Communist Party in Shanghai because he lost the address. Adopting Lenin's advice, the delegates agreed to an alliance with the \"bourgeois democrats\" of the KMT for the good of the \"national revolution\". Communist Party members joined the KMT, hoping to push its politics leftward.\nMao enthusiastically agreed with this decision, arguing for an alliance across China's socio-economic classes. Mao was a vocal anti-imperialist and in his writings he lambasted the governments of Japan, UK and US, describing the latter as \"the most murderous of hangmen\".\n\nAt the Third Congress of the Communist Party in Shanghai in June 1923, the delegates reaffirmed their commitment to working with the KMT. Supporting this position, Mao was elected to the Party Committee, taking up residence in Shanghai. At the First KMT Congress, held in Guangzhou in early 1924, Mao was elected an alternate member of the KMT Central Executive Committee, and put forward four resolutions to decentralise power to urban and rural bureaus. His enthusiastic support for the KMT earned him the suspicion of Li Li-san, his Hunan comrade.\n\nIn late 1924, Mao returned to Shaoshan, perhaps to recuperate from an illness. He found that the peasantry were increasingly restless and some had seized land from wealthy landowners to found communes. This convinced him of the revolutionary potential of the peasantry, an idea advocated by the KMT leftists but not the Communists. He returned to Guangzhou to run the 6th term of the KMT's Peasant Movement Training Institute from May to September 1926.\nThe Peasant Movement Training Institute under Mao trained cadre and prepared them for militant activity, taking them through military training exercises and getting them to study basic left-wing texts. In the winter of 1925, Mao fled to Guangzhou after his revolutionary activities attracted the attention of Zhao's regional authorities.\n\nWhen party leader Sun Yat-sen died in May 1925, he was succeeded by Chiang Kai-shek, who moved to marginalise the left-KMT and the Communists. Mao nevertheless supported Chiang's National Revolutionary Army, who embarked on the Northern Expedition attack in 1926 on warlords. In the wake of this expedition, peasants rose up, appropriating the land of the wealthy landowners, who were in many cases killed. Such uprisings angered senior KMT figures, who were themselves landowners, emphasizing the growing class and ideological divide within the revolutionary movement.\n\nIn March 1927, Mao appeared at the Third Plenum of the KMT Central Executive Committee in Wuhan, which sought to strip General Chiang of his power by appointing Wang Jingwei leader. There, Mao played an active role in the discussions regarding the peasant issue, defending a set of \"Regulations for the Repression of Local Bullies and Bad Gentry\", which advocated the death penalty or life imprisonment for anyone found guilty of counter-revolutionary activity, arguing that in a revolutionary situation, \"peaceful methods cannot suffice\". In April 1927, Mao was appointed to the KMT's five-member Central Land Committee, urging peasants to refuse to pay rent. Mao led another group to put together a \"Draft Resolution on the Land Question\", which called for the confiscation of land belonging to \"local bullies and bad gentry, corrupt officials, militarists and all counter-revolutionary elements in the villages\". Proceeding to carry out a \"Land Survey\", he stated that anyone owning over 30 \"mou\" (four and a half acres), constituting 13% of the population, were uniformly counter-revolutionary. He accepted that there was great variation in revolutionary enthusiasm across the country, and that a flexible policy of land redistribution was necessary. Presenting his conclusions at the Enlarged Land Committee meeting, many expressed reservations, some believing that it went too far, and others not far enough. Ultimately, his suggestions were only partially implemented.\n\nFresh from the success of the Northern Expedition against the warlords, Chiang turned on the Communists, who by now numbered in the tens of thousands across China. Chiang ignored the orders of the Wuhan-based left KMT government and marched on Shanghai, a city controlled by Communist militias. As the Communists awaited Chiang's arrival, he loosed the White Terror, massacring 5000 with the aid of the Green Gang. In Beijing, 19 leading Communists were killed by Zhang Zuolin. That May, tens of thousands of Communists and those suspected of being communists were killed, and the CPC lost approximately of its members.\n\nThe CPC continued supporting the Wuhan KMT government, a position Mao initially supported, but by the time of the CPC's Fifth Congress he had changed his mind, deciding to stake all hope on the peasant militia. The question was rendered moot when the Wuhan government expelled all Communists from the KMT on July 15. The CPC founded the Workers' and Peasants' Red Army of China, better known as the \"Red Army\", to battle Chiang. A battalion led by General Zhu De was ordered to take the city of Nanchang on August 1, 1927, in what became known as the Nanchang Uprising. They were initially successful, but were forced into retreat after five days, marching south to Shantou, and from there they were driven into the wilderness of Fujian. Mao was appointed commander-in-chief of the Red Army and led four regiments against Changsha in the Autumn Harvest Uprising, in the hope of sparking peasant uprisings across Hunan. On the eve of the attack, Mao composed a poem—the earliest of his to survive—titled \"Changsha\". His plan was to attack the KMT-held city from three directions on September 9, but the Fourth Regiment deserted to the KMT cause, attacking the Third Regiment. Mao's army made it to Changsha, but could not take it; by September 15, he accepted defeat and with 1000 survivors marched east to the Jinggang Mountains of Jiangxi.\n\nJung Chang and Jon Halliday claim that the uprising was in fact sabotaged by Mao to allow him to prevent a group of KMT soldiers from defecting to any other CPC leader. Chang and Halliday also claim that Mao talked the other leaders (including Russian diplomats at the Soviet consulate in Changsha who, Chang and Halliday claim, had been controlling much of the CPC activity) into striking only at Changsha, then abandoning it. Chang and Halliday report a view sent to Moscow by the secretary of the Soviet Consulate in Changsha that the retreat was \"the most despicable treachery and cowardice.\"\n\nThe CPC Central Committee, hiding in Shanghai, expelled Mao from their ranks and from the Hunan Provincial Committee, as punishment for his \"military opportunism\", for his focus on rural activity, and for being too lenient with \"bad gentry\". They nevertheless adopted three policies he had long championed: the immediate formation of Workers' councils, the confiscation of all land without exemption, and the rejection of the KMT. Mao's response was to ignore them. He established a base in Jinggangshan City, an area of the Jinggang Mountains, where he united five villages as a self-governing state, and supported the confiscation of land from rich landlords, who were \"re-educated\" and sometimes executed. He ensured that no massacres took place in the region, and pursued a more lenient approach than that advocated by the Central Committee. He proclaimed that \"Even the lame, the deaf and the blind could all come in useful for the revolutionary struggle\", he boosted the army's numbers, incorporating two groups of bandits into his army, building a force of around troops. He laid down rules for his soldiers: prompt obedience to orders, all confiscations were to be turned over to the government, and nothing was to be confiscated from poorer peasants. In doing so, he molded his men into a disciplined, efficient fighting force.\n\nIn spring 1928, the Central Committee ordered Mao's troops to southern Hunan, hoping to spark peasant uprisings. Mao was skeptical, but complied. They reached Hunan, where they were attacked by the KMT and fled after heavy losses. Meanwhile, KMT troops had invaded Jinggangshan, leaving them without a base. Wandering the countryside, Mao's forces came across a CPC regiment led by General Zhu De and Lin Biao; they united, and attempted to retake Jinggangshan. They were initially successful, but the KMT counter-attacked, and pushed the CPC back; over the next few weeks, they fought an entrenched guerrilla war in the mountains. The Central Committee again ordered Mao to march to south Hunan, but he refused, and remained at his base. Contrastingly, Zhu complied, and led his armies away. Mao's troops fended the KMT off for 25 days while he left the camp at night to find reinforcements. He reunited with the decimated Zhu's army, and together they returned to Jinggangshan and retook the base. There they were joined by a defecting KMT regiment and Peng Dehuai's Fifth Red Army. In the mountainous area they were unable to grow enough crops to feed everyone, leading to food shortages throughout the winter.\n\nIn January 1929, Mao and Zhu evacuated the base with 2,000 men and a further 800 provided by Peng, and took their armies south, to the area around Tonggu and Xinfeng in Jiangxi. The evacuation led to a drop in morale, and many troops became disobedient and began thieving; this worried Li Lisan and the Central Committee, who saw Mao's army as \"lumpenproletariat\", that were unable to share in proletariat class consciousness. In keeping with orthodox Marxist thought, Li believed that only the urban proletariat could lead a successful revolution, and saw little need for Mao's peasant guerrillas; he ordered Mao to disband his army into units to be sent out to spread the revolutionary message. Mao replied that while he concurred with Li's theoretical position, he would not disband his army nor abandon his base. Both Li and Mao saw the Chinese revolution as the key to world revolution, believing that a CPC victory would spark the overthrow of global imperialism and capitalism. In this, they disagreed with the official line of the Soviet government and Comintern. Officials in Moscow desired greater control over the CPC and removed Li from power by calling him to Russia for an inquest into his errors. They replaced him with Soviet-educated Chinese Communists, known as the \"28 Bolsheviks\", two of whom, Bo Gu and Zhang Wentian, took control of the Central Committee. Mao disagreed with the new leadership, believing they grasped little of the Chinese situation, and he soon emerged as their key rival.\n\nIn February 1930, Mao created the Southwest Jiangxi Provincial Soviet Government in the region under his control. In November, he suffered emotional trauma after his wife and sister were captured and beheaded by KMT general He Jian. Mao then married He Zizhen, an 18-year-old revolutionary who bore him five children over the following nine years. Facing internal problems, members of the Jiangxi Soviet accused him of being too moderate, and hence anti-revolutionary. In December, they tried to overthrow Mao, resulting in the Futian incident, during which Mao's loyalists tortured many and executed between 2000 and 3000 dissenters. The CPC Central Committee moved to Jiangxi which it saw as a secure area. In November it proclaimed Jiangxi to be the Soviet Republic of China, an independent Communist-governed state. Although he was proclaimed Chairman of the Council of People's Commissars, Mao's power was diminished, as his control of the Red Army was allocated to Zhou Enlai. Meanwhile, Mao recovered from tuberculosis.\n\nThe KMT armies adopted a policy of encirclement and annihilation of the Red armies. Outnumbered, Mao responded with guerrilla tactics influenced by the works of ancient military strategists like Sun Tzu, but Zhou and the new leadership followed a policy of open confrontation and conventional warfare. In doing so, the Red Army successfully defeated the first and second encirclements. Angered at his armies' failure, Chiang Kai-shek personally arrived to lead the operation. He too faced setbacks and retreated to deal with the further Japanese incursions into China. As a result of the KMT's change of focus to the defence of China against Japanese expansionism, the Red Army was able to expand its area of control, eventually encompassing a population of 3 million. Mao proceeded with his land reform program. In November 1931 he announced the start of a \"land verification project\" which was expanded in June 1933. He also orchestrated education programs and implemented measures to increase female political participation. Chiang viewed the Communists as a greater threat than the Japanese and returned to Jiangxi, where he initiated the fifth encirclement campaign, which involved the construction of a concrete and barbed wire \"wall of fire\" around the state, which was accompanied by aerial bombardment, to which Zhou's tactics proved ineffective. Trapped inside, morale among the Red Army dropped as food and medicine became scarce. The leadership decided to evacuate.\n\nOn October 14, 1934, the Red Army broke through the KMT line on the Jiangxi Soviet's south-west corner at Xinfeng with soldiers and party cadres and embarked on the \"Long March\". In order to make the escape, many of the wounded and the ill, as well as women and children, were left behind, defended by a group of guerrilla fighters whom the KMT massacred. The who escaped headed to southern Hunan, first crossing the Xiang River after heavy fighting, and then the Wu River, in Guizhou where they took Zunyi in January 1935. Temporarily resting in the city, they held a conference; here, Mao was elected to a position of leadership, becoming Chairman of the Politburo, and \"de facto\" leader of both Party and Red Army, in part because his candidacy was supported by Soviet Premier Joseph Stalin. Insisting that they operate as a guerrilla force, he laid out a destination: the Shenshi Soviet in Shaanxi, Northern China, from where the Communists could focus on fighting the Japanese. Mao believed that in focusing on the anti-imperialist struggle, the Communists would earn the trust of the Chinese people, who in turn would renounce the KMT.\n\nFrom Zunyi, Mao led his troops to Loushan Pass, where they faced armed opposition but successfully crossed the river. Chiang flew into the area to lead his armies against Mao, but the Communists outmanoeuvred him and crossed the Jinsha River. Faced with the more difficult task of crossing the Tatu River, they managed it by fighting a battle over the Luding Bridge in May, taking Luding. Marching through the mountain ranges around Ma'anshan, in Moukung, Western Szechuan, they encountered the -strong CPC Fourth Front Army of Zhang Guotao, and together proceeded to Maoerhkai and then Gansu. Zhang and Mao disagreed over what to do; the latter wished to proceed to Shaanxi, while Zhang wanted to retreat east to Tibet or Sikkim, far from the KMT threat. It was agreed that they would go their separate ways, with Zhu De joining Zhang. Mao's forces proceeded north, through hundreds of kilometres of Grasslands, an area of quagmire where they were attacked by Manchu tribesman and where many soldiers succumbed to famine and disease. Finally reaching Shaanxi, they fought off both the KMT and an Islamic cavalry militia before crossing the Min Mountains and Mount Liupan and reaching the Shenshi Soviet; only 7,000–8000 had survived. The Long March cemented Mao's status as the dominant figure in the party. In November 1935, he was named chairman of the Military Commission. From this point onward, Mao was the Communist Party's undisputed leader, even though he would not become party chairman until 1943.\n\nJung Chang and Jon Halliday offered an alternative account on many events during this period in their book \"\". For example, there was no battle at Luding and the CPC crossed the bridge unopposed, the Long March was not a strategy of the CPC but devised by Chiang Kai-shek, and Mao and other top CPC leaders did not walk the Long March but were carried on litters. However, although well received in the popular press, Chang and Halliday's work has been highly criticized by professional historians.\n\nMao's troops arrived at the Yan'an Soviet during October 1935 and settled in Pao An, until spring 1936. While there, they developed links with local communities, redistributed and farmed the land, offered medical treatment, and began literacy programs. Mao now commanded soldiers, boosted by the arrival of He Long's men from Hunan and the armies of Zhu De and Zhang Guotao returned from Tibet. In February 1936, they established the North West Anti-Japanese Red Army University in Yan'an, through which they trained increasing numbers of new recruits. In January 1937, they began the \"anti-Japanese expedition\", that sent groups of guerrilla fighters into Japanese-controlled territory to undertake sporadic attacks. In May 1937, a Communist Conference was held in Yan'an to discuss the situation. Western reporters also arrived in the \"Border Region\" (as the Soviet had been renamed); most notable were Edgar Snow, who used his experiences as a basis for \"Red Star Over China\", and Agnes Smedley, whose accounts brought international attention to Mao's cause.\n\nOn the Long March, Mao's wife He Zizen had been injured by a shrapnel wound to the head. She traveled to Moscow for medical treatment; Mao proceeded to divorce her and marry an actress, Jiang Qing. Mao moved into a cave-house and spent much of his time reading, tending his garden and theorizing. He came to believe that the Red Army alone was unable to defeat the Japanese, and that a Communist-led \"government of national defence\" should be formed with the KMT and other \"bourgeois nationalist\" elements to achieve this goal. Although despising Chiang Kai-shek as a \"traitor to the nation\", on May 5, he telegrammed the Military Council of the Nanking National Government proposing a military alliance, a course of action advocated by Stalin. Although Chiang intended to ignore Mao's message and continue the civil war, he was arrested by one of his own generals, Zhang Xueliang, in Xi'an, leading to the Xi'an Incident; Zhang forced Chiang to discuss the issue with the Communists, resulting in the formation of a United Front with concessions on both sides on December 25, 1937.\n\nThe Japanese had taken both Shanghai and Nanking (Nanjing)—resulting in the Nanking Massacre, an atrocity Mao never spoke of all his life—and was pushing the Kuomintang government inland to Chungking. The Japanese's brutality led to increasing numbers of Chinese joining the fight, and the Red Army grew from to . In August 1938, the Red Army formed the New Fourth Army and the Eighth Route Army, which were nominally under the command of Chiang's National Revolutionary Army. In August 1940, the Red Army initiated the Hundred Regiments Campaign, in which troops attacked the Japanese simultaneously in five provinces. It was a military success that resulted in the death of Japanese, the disruption of railways and the loss of a coal mine. From his base in Yan'an, Mao authored several texts for his troops, including \"Philosophy of Revolution\", which offered an introduction to the Marxist theory of knowledge; \"Protracted Warfare\", which dealt with guerilla and mobile military tactics; and \"New Democracy\", which laid forward ideas for China's future.\n\nIn 1944, the Americans sent a special diplomatic envoy, called the Dixie Mission, to the Communist Party of China. According to Edwin Moise, in \"Modern China: A History 2nd Edition\":\n\nMost of the Americans were favourably impressed. The CPC seemed less corrupt, more unified, and more vigorous in its resistance to Japan than the KMT. United States fliers shot down over North China ... confirmed to their superiors that the CPC was both strong and popular over a broad area. In the end, the contacts with the USA developed with the CPC led to very little.\n\nAfter the end of World War II, the U.S. continued their military assistance to Chiang Kai-shek and his KMT government forces against the People's Liberation Army (PLA) led by Mao Zedong during the civil war. Likewise, the Soviet Union gave quasi-covert support to Mao by their occupation of north east China, which allowed the PLA to move in en masse and take large supplies of arms left by the Japanese's Kwantung Army.\n\nTo enhance the Red Army's military operations, Mao as the Chairman of the Communist Party of China, named his close associate General Zhu De to be its Commander-in-Chief.\n\nIn 1948, under direct orders from Mao, the People's Liberation Army starved out the Kuomintang forces occupying the city of Changchun. At least civilians are believed to have perished during the siege, which lasted from June until October. PLA lieutenant colonel Zhang Zhenglu, who documented the siege in his book \"White Snow, Red Blood\", compared it to Hiroshima: \"The casualties were about the same. Hiroshima took nine seconds; Changchun took five months.\" On January 21, 1949, Kuomintang forces suffered great losses in decisive battles against Mao's forces. In the early morning of December 10, 1949, PLA troops laid siege to Chongqing and Chengdu on mainland China, and Chiang Kai-shek fled from the mainland to Formosa (Taiwan).\n\nThe People's Republic of China was established on October 1, 1949. It was the culmination of over two decades of civil and international wars. Mao's famous phrase \"The Chinese people have stood up\" () associated with the establishment of the People's Republic of China was not used in the speech he delivered from the Gate of Heavenly Peace (Tian'anmen) on October 1.\n\nMao took up residence in Zhongnanhai, a compound next to the Forbidden City in Beijing, and there he ordered the construction of an indoor swimming pool and other buildings. Mao's physician Li Zhisui described him as conducting business either in bed or by the side of the pool, preferring not to wear formal clothes unless absolutely necessary. Li's book, \"The Private Life of Chairman Mao\", is regarded as controversial, especially by those sympathetic to Mao. Mao often visited his villa in Wuhan between 1960 and 1974; the villa includes a garden, living quarters, conference room, bomb shelter and swimming pool.\n\nIn October 1950, Mao made the decision to send the People's Volunteer Army (PVA), a special unit of the People's Liberation Army, into the war in Korea and fight as well as to reinforce the armed forces of North Korea, the Korean People's Army, which had been in full retreat. Historical records showed that Mao directed the PVA campaigns to the minutest details. As the Chairman of the CPC's Central Military Commission (CMC), he was also the Supreme Commander in Chief of the PLA and the People's Republic and Chairman of the ruling CPC. The PVA was under the overall command of then newly installed Premier Zhou Enlai, with General Peng Dehuai as field commander and political commissar.\n\nDuring the land reform, a significant numbers of landlords and well-to-do peasants were beaten to death at mass meetings organised by the Communist Party as land was taken from them and given to poorer peasants, which significantly reduced economic inequality. The Campaign to Suppress Counter-revolutionaries, involved public executions that targeted mainly former Kuomintang officials, businessmen accused of \"disturbing\" the market, former employees of Western companies and intellectuals whose loyalty was suspect. In 1976, the U.S. State department estimated as many as a million were killed in the land reform, and killed in the counter-revolutionary campaign.\n\nMao himself claimed that a total of people were killed in attacks on \"counter-revolutionaries\" during the years 1950–1952. However, because there was a policy to select \"at least one landlord, and usually several, in virtually every village for public execution\", the number of deaths range between 2 million and 5 million. In addition, at least 1.5 million people, perhaps as many as 4 to 6 million, were sent to \"reform through labour\" camps where many perished. Mao played a personal role in organizing the mass repressions and established a system of execution quotas, which were often exceeded. He defended these killings as necessary for the securing of power.\n\nThe Mao government is generally credited with eradicating both consumption and production of opium during the 1950s using unrestrained repression and social reform. Ten million addicts were forced into compulsory treatment, dealers were executed, and opium-producing regions were planted with new crops. Remaining opium production shifted south of the Chinese border into the Golden Triangle region.\n\nStarting in 1951, Mao initiated two successive movements in an effort to rid urban areas of corruption by targeting wealthy capitalists and political opponents, known as the three-anti/five-anti campaigns. Whereas the three-anti campaign was a focused purge of government, industrial and party officials, the five-anti campaign set its sights slightly broader, targeting capitalist elements in general. Workers denounced their bosses, spouses turned on their spouses, and children informed on their parents; the victims were often humiliated at struggle sessions, a method designed to intimidate and terrify people to the maximum. Mao insisted that minor offenders be criticised and reformed or sent to labour camps, \"while the worst among them should be shot\". These campaigns took several hundred thousand additional lives, the vast majority via suicide.\n\nIn Shanghai, suicide by jumping from tall buildings became so commonplace that residents avoided walking on the pavement near skyscrapers for fear that suicides might land on them. Some biographers have pointed out that driving those perceived as enemies to suicide was a common tactic during the Mao-era. For example, in his biography of Mao, Philip Short notes that in the Yan'an Rectification Movement, Mao gave explicit instructions that \"no cadre is to be killed\", but in practice allowed security chief Kang Sheng to drive opponents to suicide and that \"this pattern was repeated throughout his leadership of the People's Republic\".\n\nFollowing the consolidation of power, Mao launched the First Five-Year Plan (1953–1958), which aimed to end Chinese dependence upon agriculture in order to become a world power. With the Soviet Union's assistance, new industrial plants were built and agricultural production eventually fell to a point where industry was beginning to produce enough capital that China no longer needed the USSR's support. The success of the First-Five Year Plan was to encourage Mao to instigate the Second Five-Year Plan in 1958. Mao also launched a phase of rapid collectivization. The CPC introduced price controls as well as a Chinese character simplification aimed at increasing literacy. Large-scale industrialization projects were also undertaken.\n\nPrograms pursued during this time include the Hundred Flowers Campaign, in which Mao indicated his supposed willingness to consider different opinions about how China should be governed. Given the freedom to express themselves, liberal and intellectual Chinese began opposing the Communist Party and questioning its leadership. This was initially tolerated and encouraged. After a few months, however, Mao's government reversed its policy and persecuted those who had criticised the party, totaling perhaps , as well as those who were merely alleged to have been critical, in what is called the Anti-Rightist Movement. Authors such as Jung Chang have alleged that the Hundred Flowers Campaign was merely a ruse to root out \"dangerous\" thinking.\n\nLi Zhisui, Mao's physician, suggested that Mao had initially seen the policy as a way of weakening opposition to him within the party and that he was surprised by the extent of criticism and the fact that it came to be directed at his own leadership. It was only then that he used it as a method of identifying and subsequently persecuting those critical of his government. The Hundred Flowers movement led to the condemnation, silencing, and death of many citizens, also linked to Mao's Anti-Rightist Movement, resulting in deaths possibly in the millions.\n\nIn January 1958, Mao launched the second Five-Year Plan, known as the Great Leap Forward, a plan intended as an alternative model for economic growth to the Soviet model focusing on heavy industry that was advocated by others in the party. Under this economic program, the relatively small agricultural collectives that had been formed to date were rapidly merged into far larger people's communes, and many of the peasants were ordered to work on massive infrastructure projects and on the production of iron and steel. Some private food production was banned, and livestock and farm implements were brought under collective ownership.\n\nUnder the Great Leap Forward, Mao and other party leaders ordered the implementation of a variety of unproven and unscientific new agricultural techniques by the new communes. The combined effect of the diversion of labour to steel production and infrastructure projects, and cyclical natural disasters led to an approximately 15% drop in grain production in 1959 followed by a further 10% decline in 1960 and no recovery in 1961.\n\nIn an effort to win favour with their superiors and avoid being purged, each layer in the party hierarchy exaggerated the amount of grain produced under them. Based upon the fabricated success, party cadres were ordered to requisition a disproportionately high amount of that fictitious harvest for state use, primarily for use in the cities and urban areas but also for export. The result, compounded in some areas by drought and in others by floods, was that rural peasants were left with little food for themselves and many millions starved to death in the Great Chinese Famine. China's population suffered from the Great Famine during the late 20th century. This came as a result of the lack of food production and distribution to the population of China. The people of urban areas in China were given food stamps each month, but the people of rural areas were expected to grow their own crops and give some of the crops back to the government. The deaths in the rural parts of China out ranked the ones in the Urban cities. Also, the government of China continued to export food to other countries during the Great Famine; this food could have been used to feed the starving citizens. These factors lead to the catastrophic death of about 52 million citizens. The famine was a direct cause of the death of some 30 million Chinese peasants between 1959 and 1962. Further, many children who became emaciated and malnourished during years of hardship and struggle for survival died shortly after the Great Leap Forward came to an end in 1962.\n\nThe extent of Mao's knowledge of the severity of the situation has been disputed. Mao's physician believed that he may have been unaware of the extent of the famine, partly due to a reluctance to criticise his policies, and the willingness of his staff to exaggerate or outright fake reports regarding food production. Upon learning of the extent of the starvation, Mao vowed to stop eating meat, an action followed by his staff.\n\nHong Kong-based historian Frank Dikötter, challenged the notion that Mao did not know about the famine throughout the country until it was too late:\n\nThe idea that the state mistakenly took too much grain from the countryside because it assumed that the harvest was much larger than it was is largely a myth—at most partially true for the autumn of 1958 only. In most cases the party knew very well that it was starving its own people to death. At a secret meeting in the Jinjiang Hotel in Shanghai dated March 25, 1959, Mao specifically ordered the party to procure up to one third of all the grain, much more than had ever been the case. At the meeting he announced that \"To distribute resources evenly will only ruin the Great Leap Forward. When there is not enough to eat, people starve to death. It is better to let half of the people die so that the other half can eat their fill.\"\n\nProfessor Emeritus Thomas P. Bernstein of Columbia University offered his view on Mao's statement on starvation in the March 25, 1959, meeting:\n\nSome scholars believe that this shows Mao's readiness to accept mass death on an immense scale. My own view is that this is an instance of Mao's use of hyperbole, another being his casual acceptance of death of half the population during a nuclear war. In other contexts, Mao did not in fact accept mass death. Zhou's Chronology shows that in October 1958, Mao expressed real concern that 40,000 people in Yunnan had starved to death (p. 173). Shortly after the March 25 meeting, he worried about 25.2 million people who were at risk of starvation. But from late summer on, Mao essentially forgot about this issue, until, as noted, the \"Xinyang Incident\" came to light in October 1960.\n\nIn the article \"Mao Zedong and the Famine of 1959–1960: A Study in Wilfulness\", published in 2006 in \"The China Quarterly\", Professor Thomas P. Bernstein also discussed Mao's change of attitudes during different phases of the Great Leap Forward:\n\nIn late autumn 1958, Mao Zedong strongly condemned widespread practices of the Great Leap Forward (GLF) such as subjecting peasants to exhausting labour without adequate food and rest, which had resulted in epidemics, starvation and deaths. At that time Mao explicitly recognized that anti-rightist pressures on officialdom were a major cause of \"production at the expense of livelihood.\" While he was not willing to acknowledge that only abandonment of the GLF could solve these problems, he did strongly demand that they be addressed. After the July 1959 clash at Lushan with Peng Dehuai, Mao revived the GLF in the context of a new, extremely harsh anti-rightist campaign, which he relentlessly promoted into the spring of 1960 together with the radical policies that he previously condemned. Not until spring 1960 did Mao again express concern about abnormal deaths and other abuses, but he failed to apply the pressure needed to stop them. Given what he had already learned about the costs to the peasants of GLF extremism, the Chairman should have known that the revival of GLF radicalism would exact a similar or even bigger price. Instead, he wilfully ignored the lessons of the first radical phase for the sake of achieving extreme ideological and developmental goals.\n\nIn \"\", Jasper Becker notes that Mao was dismissive of reports he received of food shortages in the countryside and refused to change course, believing that peasants were lying and that rightists and kulaks were hoarding grain. He refused to open state granaries, and instead launched a series of \"anti-grain concealment\" drives that resulted in numerous purges and suicides. Other violent campaigns followed in which party leaders went from village to village in search of hidden food reserves, and not only grain, as Mao issued quotas for pigs, chickens, ducks and eggs. Many peasants accused of hiding food were tortured and beaten to death.\n\nWhatever the cause of the disaster, Mao lost esteem among many of the top party cadres. He was eventually forced to abandon the policy in 1962, and he lost political power to moderate leaders such as Liu Shaoqi and Deng Xiaoping. Mao, however, supported by national propaganda, claimed that he was only partly to blame for the famine. As a result, he was able to remain Chairman of the Communist Party, with the Presidency transferred to Liu Shaoqi.\n\nThe Great Leap Forward was a tragedy for the vast majority of the Chinese. Although the steel quotas were officially reached, almost all of the supposed steel made in the countryside was iron, as it had been made from assorted scrap metal in home-made furnaces with no reliable source of fuel such as coal. This meant that proper smelting conditions could not be achieved. According to Zhang Rongmei, a geometry teacher in rural Shanghai during the Great Leap Forward:\n\nWe took all the furniture, pots, and pans we had in our house, and all our neighbours did likewise. We put everything in a big fire and melted down all the metal.\n\nThe worst of the famine was steered towards enemies of the state. As Jasper Becker explains:\n\nThe most vulnerable section of China's population, around five per cent, were those whom Mao called 'enemies of the people'. Anyone who had in previous campaigns of repression been labeled a 'black element' was given the lowest priority in the allocation of food. Landlords, rich peasants, former members of the nationalist regime, religious leaders, rightists, counter-revolutionaries and the families of such individuals died in the greatest numbers.\nAt a large Communist Party conference in Beijing in January 1962, called the \"Conference of the Seven Thousand\", State Chairman Liu Shaoqi denounced the Great Leap Forward as responsible for widespread famine. The overwhelming majority of delegates expressed agreement, but Defense Minister Lin Biao staunchly defended Mao. A brief period of liberalization followed while Mao and Lin plotted a comeback. Liu Shaoqi and Deng Xiaoping rescued the economy by disbanding the people's communes, introducing elements of private control of peasant smallholdings and importing grain from Canada and Australia to mitigate the worst effects of famine.\n\nAt the Lushan Conference in July/August 1959, several ministers expressed concern that the Great Leap Forward had not proved as successful as planned. The most direct of these was Minister of Defence and Korean War veteran General Peng Dehuai. Following Peng's criticism of the Great Leap Forward, Mao orchestrated a purge of Peng and his supporters, stifling criticism of the Great Leap policies. Senior officials who reported the truth of the famine to Mao were branded as \"right opportunists.\" A campaign against right-wing opportunism was launched and resulted in party members and ordinary peasants being sent to prison labor camps where many would subsequently die in the famine. Years later the CPC would conclude that as many as six million people were wrongly punished in the campaign.\n\nThe number of deaths by starvation during the Great Leap Forward is deeply controversial. Until the mid-1980s, when official census figures were finally published by the Chinese Government, little was known about the scale of the disaster in the Chinese countryside, as the handful of Western observers allowed access during this time had been restricted to model villages where they were deceived into believing that the Great Leap Forward had been a great success. There was also an assumption that the flow of individual reports of starvation that had been reaching the West, primarily through Hong Kong and Taiwan, must have been localised or exaggerated as China was continuing to claim record harvests and was a net exporter of grain through the period. Because Mao wanted to pay back early to the Soviets debts totalling 1.973 billion yuan from 1960 to 1962, exports increased by 50%, and fellow Communist regimes in North Korea, North Vietnam and Albania were provided grain free of charge.\n\nCensuses were carried out in China in 1953, 1964 and 1982. The first attempt to analyse this data to estimate the number of famine deaths was carried out by American demographer Dr. Judith Banister and published in 1984. Given the lengthy gaps between the censuses and doubts over the reliability of the data, an accurate figure is difficult to ascertain. Nevertheless, Banister concluded that the official data implied that around 15 million excess deaths incurred in China during 1958–61, and that based on her modelling of Chinese demographics during the period and taking account of assumed under-reporting during the famine years, the figure was around 30 million. The official statistic is 20 million deaths, as given by Hu Yaobang. Yang Jisheng, a former Xinhua News Agency reporter who had privileged access and connections available to no other scholars, estimates a death toll of 36 million. Frank Dikötter estimates that there were at least 45 million premature deaths attributable to the Great Leap Forward from 1958 to 1962. Various other sources have put the figure at between 20 and 46 million.\n\nOn the international front, the period was dominated by the further isolation of China. The Sino-Soviet split resulted in Nikita Khrushchev's withdrawal of all Soviet technical experts and aid from the country. The split concerned the leadership of world communism. The USSR had a network of Communist parties it supported; China now created its own rival network to battle it out for local control of the left in numerous countries. Lorenz M. Lüthi argues:\n\nThe Sino-Soviet split was one of the key events of the Cold War, equal in importance to the construction of the Berlin Wall, the Cuban Missile Crisis, the Second Vietnam War, and Sino-American rapprochement. The split helped to determine the framework of the Second Cold War in general, and influenced the course of the Second Vietnam War in particular.\n\nThe split resulted from Nikita Khrushchev's more moderate Soviet leadership after the death of Stalin in March 1953. Only Albania openly sided with China, thereby forming an alliance between the two countries which would last until after Mao's death in 1976. Warned that the Soviets had nuclear weapons, Mao minimized the threat. Becker says that \"Mao believed that the bomb was a 'paper tiger', declaring to Khrushchev that it would not matter if China lost 300 million people in a nuclear war: the other half of the population would survive to ensure victory\".\n\nStalin had established himself as the successor of \"correct\" Marxist thought well before Mao controlled the Communist Party of China, and therefore Mao never challenged the suitability of any Stalinist doctrine (at least while Stalin was alive). Upon the death of Stalin, Mao believed (perhaps because of seniority) that the leadership of Marxist doctrine would fall to him. The resulting tension between Khrushchev (at the head of a politically and militarily superior government), and Mao (believing he had a superior understanding of Marxist ideology) eroded the previous patron-client relationship between the Communist Party of the Soviet Union and the CPC. In China, the formerly favoured Soviets were now denounced as \"revisionists\" and listed alongside \"American imperialism\" as movements to oppose.\n\nPartly surrounded by hostile American military bases (in South Korea, Japan, and Taiwan), China was now confronted with a new Soviet threat from the north and west. Both the internal crisis and the external threat called for extraordinary statesmanship from Mao, but as China entered the new decade the statesmen of the People's Republic were in hostile confrontation with each other.\n\nDuring the early 1960s, Mao became concerned with the nature of post-1959 China. He saw that the revolution and Great Leap Forward had replaced the old ruling elite with a new one. He was concerned that those in power were becoming estranged from the people they were to serve. Mao believed that a revolution of culture would unseat and unsettle the \"ruling class\" and keep China in a state of \"perpetual revolution\" that, theoretically, would serve the interests of the majority, rather than a tiny and privileged elite. State Chairman Liu Shaoqi and General Secretary Deng Xiaoping favoured the idea that Mao be removed from actual power as China's head of state and government but maintain his ceremonial and symbolic role as Chairman of the Communist Party of China, with the party upholding all of his positive contributions to the revolution. They attempted to marginalise Mao by taking control of economic policy and asserting themselves politically as well. Many claim that Mao responded to Liu and Deng's movements by launching the Great Proletarian Cultural Revolution in 1966. Some scholars, such as Mobo Gao, claim the case for this is overstated. Others, such as Frank Dikötter, hold that Mao launched the Cultural Revolution to wreak revenge on those who had dared to challenge him over the Great Leap Forward.\n\nBelieving that certain liberal bourgeois elements of society continued to threaten the socialist framework, groups of young people known as the Red Guards struggled against authorities at all levels of society and even set up their own tribunals. Chaos reigned in much of the nation, and millions were persecuted. During the Cultural Revolution, nearly all of the schools and universities in China were closed, and the young intellectuals living in cities were ordered to the countryside to be \"re-educated\" by the peasants, where they performed hard manual labour and other work.\n\nThe Cultural Revolution led to the destruction of much of China's traditional cultural heritage and the imprisonment of a huge number of Chinese citizens, as well as the creation of general economic and social chaos in the country. Millions of lives were ruined during this period, as the Cultural Revolution pierced into every part of Chinese life, depicted by such Chinese films as \"To Live\", \"The Blue Kite\" and \"Farewell My Concubine\". It is estimated that hundreds of thousands of people, perhaps millions, perished in the violence of the Cultural Revolution.\n\nWhen Mao was informed of such losses, particularly that people had been driven to suicide, he is alleged to have commented: \"People who try to commit suicide—don't attempt to save them! . . . China is such a populous nation, it is not as if we cannot do without a few people.\" The authorities allowed the Red Guards to abuse and kill opponents of the regime. Said Xie Fuzhi, national police chief: \"Don't say it is wrong of them to beat up bad persons: if in anger they beat someone to death, then so be it.\" As a result, in August and September 1966, there were a reported 1,772 people murdered by the Red Guards in Beijing alone.\n\nIt was during this period that Mao chose Lin Biao, who seemed to echo all of Mao's ideas, to become his successor. Lin was later officially named as Mao's successor. By 1971, however, a divide between the two men had become apparent. Official history in China states that Lin was planning a military coup or an assassination attempt on Mao. Lin Biao died in a plane crash over the air space of Mongolia, presumably as he fled China, probably anticipating his arrest. The CPC declared that Lin was planning to depose Mao and posthumously expelled Lin from the party. At this time, Mao lost trust in many of the top CPC figures. The highest-ranking Soviet Bloc intelligence defector, Lt. Gen. Ion Mihai Pacepa described his conversation with Nicolae Ceaușescu, who told him about a plot to kill Mao Zedong with the help of Lin Biao organised by the KGB.\n\nDespite being considered a feminist figure by some and a supporter of women's rights, documents released by the US Department of State in 2008 show that Mao declared women to be a \"nonsense\" in 1973, in conversation with Kissinger, joking that \"China is a very poor country. We don't have much. What we have in excess is women... Let them go to your place. They will create disasters. That way you can lessen our burdens.\" When Mao offered 10 million women, Kissinger replied by saying that Mao was \"improving his offer\". Mao and Kissinger then agreed that their comments on women be removed from public records, prompted by a Chinese official who feared that Mao's comments might incur public anger if released.\n\nWhen Mao first tasted mangoes in 1968 he was enthused, describing them as a \"spiritual time bomb\". News of his enthusiasm made it to the Pakistani foreign office, and on August 4, 1968, Mao was presented with about 40 mangoes by the Pakistani foreign minister, Syed Sharifuddin Pirzada, in an apparent diplomatic gesture. Mao had his aide send the box of mangoes to his Mao Zedong Propaganda Team at Tsinghua University on August 5, the team stationed there to quiet strife among Red Guard factions. On August 7, an article was published in the \"People's Daily\" saying:\n\nIn the afternoon of the fifth, when the great happy news of Chairman Mao giving mangoes to the Capital Worker and Peasant Mao Zedong Thought Propaganda Team reached the Tsinghua University campus, people immediately gathered around the gift given by the Great Leader Chairman Mao. They cried out enthusiastically and sang with wild abandonment. Tears swelled up in their eyes, and they again and again sincerely wished that our most beloved Great Leader lived then thousand years without bounds ... They all made phone calls to their own work units to spread this happy news; and they also organised all kinds of celebratory activities all night long, and arrived at [the national leadership compound] Zhongnanhai despite the rain to report the good news, and to express their loyalty to the Great Leader Chairman Mao.\n\nSubsequent articles were also written by government officials propagandizing the reception of the mangoes, and another poem in the \"People's Daily\" said: \"Seeing that golden mango/Was as if seeing the great leader Chairman Mao ... Again and again touching that golden mango/the golden mango was so warm\". Few people at this time in China had ever seen a mango before, and a mango was seen as \"a fruit of extreme rarity, like Mushrooms of Immortality\".\nOne of the mangoes was sent to the Beijing Textile Factory, whose revolutionary committee organised a rally in the mangoes' honour. Workers read out quotations from Mao and celebrated the gift. Altars were erected to prominently display the fruit; when the mango peel began to rot after a few days, the fruit was peeled and boiled in a pot of water. Workers then filed by and each was given a spoonful of mango water. The revolutionary committee also made a wax replica of the mango, and displayed this as a centrepiece in the factory. There followed several months of \"mango fever\", as the fruit became a focus of a \"boundless loyalty\" campaign for Chairman Mao. More replica mangoes were created and the replicas were sent on tour around Beijing and elsewhere in China. Many revolutionary committees visited the mangoes in Beijing from outlying provinces; approximately half a million people greeted the replicas when they arrived in Chengdu. Badges and wall posters featuring the mangoes and Mao were produced in the millions. The fruit was shared among all institutions that had been a part of the propaganda team, and large processions were organised in support of the \"zhengui lipin\" (\"precious gift\"), as the mangoes were known as. One dentist in a small village compared a mango to a sweet potato; he was put on trial for malicious slander and executed.\n\nIt has been claimed that Mao used the mangoes to express support for the workers who would go to whatever lengths necessary to end the factional fighting among students, and a \"prime example of Mao's strategy of symbolic support\". Even up until early 1969, participants of Mao Zedong Thought study classes in Beijing would return with mass-produced mango facsimiles and still gain media attention in the provinces.\n\nIn 1969, Mao declared the Cultural Revolution to be over, although various historians in and outside of China mark the end of the Cultural Revolution—as a whole or in part—in 1976, following Mao's death and the arrest of the Gang of Four. In the last years of his life, Mao was faced with declining health due to either Parkinson's disease or, according to his physician, amyotrophic lateral sclerosis, as well as lung ailments due to smoking and heart trouble. Some also attributed Mao's decline in health to the betrayal of Lin Biao. Mao remained passive as various factions within the Communist Party mobilised for the power struggle anticipated after his death.\n\nThe Cultural Revolution is often looked at in all scholarly circles as a greatly disruptive period for China. While one-tenth of Chinese people—an estimated 100 million—did suffer during the period, some scholars, such as Lee Feigon and Mobo Gao, claim there were many great advances, and in some sectors the Chinese economy continued to outperform the West. They hold that the Cultural Revolution period laid the foundation for the spectacular growth that continues in China. During the Cultural Revolution, China detonated its first H-Bomb (1967), launched the Dong Fang Hong satellite (January 30, 1970), commissioned its first nuclear submarines and made various advances in science and technology. Healthcare was free, and living standards in the countryside continued to improve. In comparison, the Great Leap probably did cause a much larger loss of life with its flawed economic policies which encompassed even the peasants.\n\nEstimates of the death toll during the Cultural Revolution, including civilians and Red Guards, vary greatly. An estimate of around 400,000 deaths is a widely accepted minimum figure, according to Maurice Meisner. MacFarquhar and Schoenhals assert that in rural China alone some 36 million people were persecuted, of whom between 750,000 and 1.5 million were killed, with roughly the same number permanently injured. In \"\", Jung Chang and Jon Halliday claim that as many as 3 million people died in the violence of the Cultural Revolution.\n\nHistorian Daniel Leese notes that in the 1950s Mao's personality was hardening:\n\nDuring his leadership, Mao traveled outside China on only two occasions, both state visits to the Soviet Union. When Mao stepped down as head of state on April 27, 1959, further diplomatic state visits and travels abroad were undertaken by president Liu Shaoqi rather than Mao personally.\n\nSmoking may have played an important role in his declining health, for Mao was a heavy smoker during most of his adult life. It became a state secret that he suffered from multiple lung and heart ailments during his later years. There are unconfirmed reports that he possibly had Parkinson's disease in addition to amyotrophic lateral sclerosis, also known as Lou Gehrig's disease.\n\nMao's last public appearance—and the last known photograph of him alive—was on May 27, 1976, when he met the visiting Pakistani Prime Minister Zulfikar Ali Bhutto during the latter's one-day visit to Beijing. Mao suffered two major heart attacks in 1976, one in March and another in July, before a third struck on September 5, rendering him an invalid. Mao Zedong died nearly four days later just after midnight, at 00:10, on September 9, 1976, at age 82. The Communist Party of China delayed the announcement of his death until 16:00 later that day, when a radio message broadcast across the nation announced the news of Mao's passing while appealing for party unity.\n\nMao's embalmed, CPC-flag-draped body lay in state at the Great Hall of the People for one week. During this period, one million people (none of them foreign diplomats, and the majority crying openly or otherwise displaying some kind of sadness) filed past Mao to pay their final respects. Chairman Mao's official portrait was hung on the wall, with a banner reading: \"Carry on the cause left by Chairman Mao and carry on the cause of proletarian revolution to the end\", until September 17. On September 17, Chairman Mao's body was taken in a minibus from the Great Hall of the people to Maojiawan to the 305 Hospital that Liu Zhisui directed, and Mao's internal organs were preserved in formaldehyde.\n\nOn September 18, a somber cacophony of guns, sirens, whistles and horns all across China was spontaneously blown in observance of a three-minute silence, which everybody except those performing essential tasks was ordered to observe. After that, a band in Tiananmen Square, packed with and surrounded by millions of people, played \"The Internationale\". The final service on that day was concluded by Hua Guofeng's 20-minute-long eulogy atop Tiananmen Gate. Mao's body was later permanently interred in a mausoleum in Beijing.\n\nMao remains a controversial figure and there is little agreement over his legacy both in China and abroad. Supporters generally credit and praise him for having unified China and for ending the previous decades of civil war. He is also credited for having improved the status of women in China and for improving literacy and education. His policies caused the deaths of tens of millions of people in China during his 27-year reign, more than any other 20th century leader; the number of people who died under his regime range from 40 million to as many as 70 million. However, supporters point out that in spite of this, life expectancy improved during his reign. His supporters claim that he rapidly industrialised China; however, others have claimed that his policies such as the \"Great Leap Forward\" and the \"Great Proletarian Cultural Revolution\" were impediments to industrialisation and modernisation. His supporters claim that his policies laid the groundwork for China's later rise to become an economic superpower, while others claim that his policies delayed economic development and that China's economy underwent its rapid growth only after Mao's policies had been widely abandoned. Mao's revolutionary tactics continue to be used by insurgents, and his political ideology continues to be embraced by many Communist organizations around the world.\n\nIn mainland China, Mao is still revered by many members and supporters of the Communist Party and respected by the majority of the general population as the \"Founding Father of modern China\", credited for giving \"the Chinese people dignity and self-respect.\" Mobo Gao, in his 2008 book \"The Battle for China's Past: Mao and the Cultural Revolution\", credits Mao for raising the average life expectancy from 35 in 1949 to 63 by 1975, bringing \"unity and stability to a country that had been plagued by civil wars and foreign invasions\", and laying the foundation for China to \"become the equal of the great global powers\". Gao also lauds Mao for carrying out massive land reform, promoting the status of women, improving popular literacy, and positively \"transform(ing) Chinese society beyond recognition.\" Scholars outside of China also credit Mao for boosting literacy (only 20% of the population could read in 1949, compared to 65.5% thirty years later), doubling life expectancy, a near doubling of the population, and developing China's industry and infrastructure, paving the way for its position as a world power.\n\nHowever, Mao has many Chinese critics, both those who live inside and outside China. Opposition to Mao is subject to restriction and censorship in mainland China, but is especially strong elsewhere, where he is often reviled as a brutish ideologue. In the West, his name is generally associated with tyranny and his economic theories are widely discredited—though to some political activists he remains a symbol against capitalism, imperialism and western influence. Even in China, key pillars of his economic theory have been largely dismantled by market reformers like Deng Xiaoping and Zhao Ziyang, who succeeded him as leaders of the Communist Party.\n\nThough the Chinese Communist Party, which Mao led to power, has rejected in practice the economic fundamentals of much of Mao's ideology, it retains for itself many of the powers established under Mao's reign: it controls the Chinese army, police, courts and media and does not permit multi-party elections at the national or local level, except in Hong Kong. Thus it is difficult to gauge the true extent of support for the Chinese Communist Party and Mao's legacy within mainland China. For its part, the Chinese government continues to officially regard Mao as a national hero. On December 25, 2008, China opened the Mao Zedong Square to visitors in his home town of central Hunan Province to mark the 115th anniversary of his birth.\n\nThere continue to be disagreements on Mao's legacy. Former Party official Su Shachi has opined that \"he was a great historical criminal, but he was also a great force for good.\" In a similar vein, journalist Liu Binyan has described Mao as \"both monster and a genius.\" Some historians argue that Mao Zedong was \"one of the great tyrants of the twentieth century\", and a dictator comparable to Adolf Hitler and Joseph Stalin, with a death toll surpassing both. In \"The Black Book of Communism\", Jean Louis Margolin writes that \"Mao Zedong was so powerful that he was often known as the Red Emperor ... the violence he erected into a whole system far exceeds any national tradition of violence that we might find in China.\" Mao was frequently likened to China's First Emperor Qin Shi Huang, notorious for burying alive hundreds of scholars, and personally enjoyed the comparison. During a speech to party cadre in 1958, Mao said he had far outdone Qin Shi Huang in his policy against intellectuals: \"What did he amount to? He only buried alive 460 scholars, while we buried 46,000. In our suppression of the counter-revolutionaries, did we not kill some counter-revolutionary intellectuals? I once debated with the democratic people: You accuse us of acting like Ch'in-shih-huang, but you are wrong; we surpass him 100 times.\" As a result of such tactics, critics have pointed out that:\n\nOthers, such as Philip Short, reject such comparisons in \"Mao: A Life\", arguing that whereas the deaths caused by Nazi Germany and Soviet Russia were largely systematic and deliberate, the overwhelming majority of the deaths under Mao were unintended consequences of famine. Short noted that landlord class were not exterminated as a people due to Mao's belief in redemption through thought reform. He instead compared Mao with 19th-century Chinese reformers who challenged China's traditional beliefs in the era of China's clashes with Western colonial powers. Short argues, \"Mao's tragedy and his grandeur were that he remained to the end in thrall to his own revolutionary dreams ... He freed China from the straitjacket of its Confucian past, but the bright Red future he promised turned out to be a sterile purgatory.\n\nMao's English interpreter Sidney Rittenberg wrote in his memoir \"The Man Who Stayed Behind\" that whilst Mao \"was a great leader in history\", he was also \"a great criminal because, not that he wanted to, not that he intended to, but in fact, his wild fantasies led to the deaths of tens of millions of people.\" Li Rui, Mao's personal secretary, goes further and claims he was dismissive of the suffering and death caused by his policies: \"Mao's way of thinking and governing was terrifying. He put no value on human life. The deaths of others meant nothing to him.\"\n\nIn their 832-page biography, \"\", Jung Chang and Jon Halliday take a very critical view of Mao's life and influence. For example, they note that Mao was well aware that his policies would be responsible for the deaths of millions. While discussing labour-intensive projects such as waterworks and making steel, Mao said to his inner circle in November 1958: \"Working like this, with all these projects, half of China may well have to die. If not half, one-third, or one-tenth—50 million—die.\"\n\nThomas Bernstein of Columbia University argues that this quotation is taken out of context, claiming:\n\nThe Chinese original, however, is not quite as shocking. In the speech, Mao talks about massive earthmoving irrigation projects and numerous big industrial ones, all requiring huge numbers of people. If the projects, he said, are all undertaken simultaneously \"half of China's population unquestionably will die; and if it's not half, it'll be a third or ten percent, a death toll of 50 million people.\" Mao then pointed to the example of Guangxi provincial Party secretary, () who had been dismissed in 1957 for failing to prevent famine in the previous year, adding: \"If with a death toll of 50 million you didn't lose your jobs, I at least should lose mine; whether I should lose my head would also be in question. Anhui wants to do so much, which is quite all right, but make it a principle to have no deaths.\"\nJasper Becker notes, \"archive material gathered by Dikötter ... confirms that far from being ignorant or misled about the famine, the Chinese leadership were kept informed about it all the time. And he exposes the extent of the violence used against the peasants\":\n\nMass killings are not usually associated with Mao and the Great Leap Forward, and China continues to benefit from a more favourable comparison with Cambodia or the Soviet Union. But as fresh and abundant archival evidence shows, coercion, terror and systematic violence were the foundation of the Great Leap, and between 1958 to 1962, by a rough approximation, some 6 to 8 per cent of those who died were tortured to death or summarily killed—amounting to at least 3 million victims.\n\nDikötter argues that CPC leaders \"glorified violence and were inured to massive loss of life. And all of them shared an ideology in which the end justified the means. In 1962, having lost millions of people in his province, Li Jingquan compared the Great Leap Forward to the Long March in which only one in ten had made it to the end: 'We are not weak, we are stronger, we have kept the backbone.'\"\n\nRegarding the large-scale irrigation projects, Dikötter stresses that, in spite of Mao being in a good position to see the human cost, they continued unabated for several years, and ultimately claimed the lives of hundreds of thousands of exhausted villagers. He also notes that \"In a chilling precursor of Cambodia under the Khmer Rouge, villagers in Qingshui and Gansu called these projects the 'killing fields'.\"\n\nThe United States placed a trade embargo on the People's Republic as a result of its involvement in the Korean War, lasting until Richard Nixon decided that developing relations with the PRC would be useful in dealing with the Soviet Union.\n\nThe television series Biography stated: \"[Mao] turned China from a feudal backwater into one of the most powerful countries in the World ... The Chinese system he overthrew was backward and corrupt; few would argue the fact that he dragged China into the 20th century. But at a cost in human lives that is staggering.\"\n\nIn the book \"China in the 21st Century: What Everyone Needs to Know\" published in 2010, Professor Jeffrey N. Wasserstrom of the University of California, Irvine compares China's relationship to Mao Zedong to Americans' remembrance of Andrew Jackson: both countries regard the leaders in a positive light, despite their respective roles in devastating policies. Jackson forcibly moved Native Americans, resulting in thousands of deaths, while Mao was at the helm during the violent years of the Cultural Revolution and the Great Leap Forward:\n\nThough admittedly far from perfect, the comparison is based on the fact that Jackson is remembered both as someone who played a significant role in the development of a political organization (the Democratic Party) that still has many partisans, and as someone responsible for brutal policies toward Native Americans that are now referred to as genocidal.\n\nBoth men are thought of as having done terrible things yet this does not necessarily prevent them from being used as positive symbols. And Jackson still appears on $20 bills, even though Americans tend to view as heinous the institution of slavery (of which he was a passionate defender) and the early 19th-century military campaigns against Native Americans (in which he took part).\n\nAt times Jackson, for all his flaws, is invoked as representing an egalitarian strain within the American democratic tradition, a self-made man of the people who rose to power via straight talk and was not allied with moneyed interests. Mao stands for something roughly similar.\nMao's military writings continue to have a large amount of influence both among those who seek to create an insurgency and those who seek to crush one, especially in manners of guerrilla warfare, at which Mao is popularly regarded as a genius. As an example, the Communist Party of Nepal (Maoist) followed Mao's examples of guerrilla warfare to considerable political and military success even in the 21st century. Mao's major contribution to the military science is his theory of People's War, with not only guerrilla warfare but more importantly, Mobile Warfare methodologies. Mao had successfully applied Mobile Warfare in the Korean War, and was able to encircle, push back and then halt the UN forces in Korea, despite the clear superiority of UN firepower. Mao also gave the impression that he might even welcome a nuclear war.\n\nLet us imagine how many people would die if war breaks out. There are 2.7 billion people in the world, and a third could be lost. If it is a little higher, it could be half ... I say that if the worst came to the worst and one-half dies, there will still be one-half left, but imperialism would be razed to the ground and the whole world would become socialist. After a few years there would be 2.7 billion people again\"\n\nBut historians dispute the sincerity of Mao's words. Robert Service says that Mao \"was deadly serious,\" while Frank Dikötter claims that \"He was bluffing ... the sabre-rattling was to show that he, not Khrushchev, was the more determined revolutionary.\"\n\nMao's poems and writings are frequently cited by both Chinese and non-Chinese. The official Chinese translation of President Barack Obama's inauguration speech used a famous line from one of Mao's poems.\n\nThe ideology of Maoism has influenced many Communists, mainly in the Third World, including revolutionary movements such as Cambodia's Khmer Rouge, Peru's Shining Path, and the Nepalese revolutionary movement. Under the influence of Mao's agrarian socialism and Cultural Revolution, Cambodia's Pol Pot conceived of his disastrous Year Zero policies which purged the nation of its teachers, artists and intellectuals and emptied its cities, resulting in the Cambodian Genocide.\n\nThe Revolutionary Communist Party, USA also claims Marxism–Leninism-Maoism as its ideology, as do other Communist Parties around the world which are part of the Revolutionary Internationalist Movement. China itself has moved sharply away from Maoism since Mao's death, and most people outside of China who describe themselves as Maoist regard the Deng Xiaoping reforms to be a betrayal of Maoism, in line with Mao's view of \"Capitalist roaders\" within the Communist Party.\n\nAs the Chinese government instituted free market economic reforms starting in the late 1970s and as later Chinese leaders took power, less recognition was given to the status of Mao. This accompanied a decline in state recognition of Mao in later years in contrast to previous years when the state organised numerous events and seminars commemorating Mao's 100th birthday. Nevertheless, the Chinese government has never officially repudiated the tactics of Mao. Deng Xiaoping, who was opposed to the Great Leap Forward and the Cultural Revolution, has to a certain extent rejected Mao's legacy, famously saying that Mao was \"70% right and 30% wrong\".\n\nIn the mid-1990s, Mao Zedong's picture began to appear on all new renminbi currency from the People's Republic of China. This was officially instituted as an anti-counterfeiting measure as Mao's face is widely recognised in contrast to the generic figures that appear in older currency. On March 13, 2006, a story in the \"People's Daily\" reported that a proposal had been made to print the portraits of Sun Yat-sen and Deng Xiaoping.\n\nMao gave contradicting statements on the subject of personality cults. In 1955, as a response to the Khrushchev Report that criticised Joseph Stalin, Mao stated that personality cults are \"poisonous ideological survivals of the old society\", and reaffirmed China's commitment to collective leadership. But at the 1958 Party congress in Chengdu, Mao expressed support for the personality cults of people whom he labelled as genuinely worthy figures, not those that expressed \"blind worship\".\n\nIn 1962, Mao proposed the Socialist Education Movement (SEM) in an attempt to educate the peasants to resist the \"temptations\" of feudalism and the sprouts of capitalism that he saw re-emerging in the countryside from Liu's economic reforms. Large quantities of politicised art were produced and circulated—with Mao at the centre. Numerous posters, badges and musical compositions referenced Mao in the phrase \"Chairman Mao is the red sun in our hearts\" () and a \"Savior of the people\" ().\n\nIn October 1966, Mao's \"Quotations from Chairman Mao Tse-tung\", known as the \"Little Red Book\", was published. Party members were encouraged to carry a copy with them, and possession was almost mandatory as a criterion for membership. Over the years, Mao's image became displayed almost everywhere, present in homes, offices and shops. His quotations were typographically emphasised by putting them in boldface or red type in even the most obscure writings. Music from the period emphasised Mao's stature, as did children's rhymes. The phrase \"Long Live Chairman Mao for ten thousand years\" was commonly heard during the era.\n\nMao also has a presence in China and around the world in popular culture, where his face adorns everything from T-shirts to coffee cups. Mao's granddaughter, Kong Dongmei, defended the phenomenon, stating that \"it shows his influence, that he exists in people's consciousness and has influenced several generations of Chinese people's way of life. Just like Che Guevara's image, his has become a symbol of revolutionary culture.\" Since 1950, over 40 million people have visited Mao's birthplace in Shaoshan, Hunan.\n\nHis ancestors were:\n\nMao Zedong had four wives who gave birth to a total of 10 children. They were:\n\n\nHe had several siblings:\n\nNote that the character \"zé\" () appears in all of the siblings' given names. This is a common Chinese naming convention.\n\nFrom the next generation, Zemin's son, Mao Yuanxin, was raised by Mao Zedong's family. He became Mao Zedong's liaison with the Politburo in 1975. In Li Zhisui's \"The Private Life of Chairman Mao\", Mao Yuanxin played a role in the final power-struggles.\n\nMao Zedong had a total of ten children, including:\n\nMao's first and second daughters were left to local villagers because it was too dangerous to raise them while fighting the Kuomintang and later the Japanese. Their youngest daughter (born in early 1938 in Moscow after Mao separated) and one other child (born 1933) died in infancy. Two English researchers who retraced the entire Long March route in 2002–2003 located a woman whom they believe might well be one of the missing children abandoned by Mao to peasants in 1935. Ed Jocelyn and Andrew McEwen hope a member of the Mao family will respond to requests for a DNA test.\n\nThrough his ten children, Mao became grandfather to twelve grandchildren, many of whom he never knew. He has many great-grandchildren alive today. One of his granddaughters is businesswoman Kong Dongmei, one of the richest people in China. His grandson Mao Xinyu is a general in the Chinese army. Both he and Kong have written books about their grandfather.\n\nMao's private life was very secretive at the time of his rule. However, after Mao's death, Li Zhisui, his personal physician, published \"The Private Life of Chairman Mao\", a memoir which mentions some aspects of Mao's private life, such as chain-smoking cigarettes, addiction to powerful sleeping pills and large number of sexual partners. Some scholars and some other people who also personally knew and worked with Mao, however, have disputed the accuracy of these characterisations.\n\nHaving grown up in Hunan, Mao spoke Mandarin with a marked Hunanese accent.\nRoss Terrill noted Mao was a \"son of the soil ... rural and unsophisticated\" in origins, while Clare Hollingworth asserted he was proud of his \"peasant ways and manners\", having a strong Hunanese accent and providing \"earthy\" comments on sexual matters. Lee Feigon noted that Mao's \"earthiness\" meant that he remained connected to \"everyday Chinese life.\"\n\nSinologist Stuart Schram emphasised Mao's ruthlessness, but also noted that he showed no sign of taking pleasure in torture or killing in the revolutionary cause. Lee Feigon considered Mao \"draconian and authoritarian\" when threatened, but opined that he was not the \"kind of villain that his mentor Stalin was\". Alexander Pantsov and Steven I. Levine wrote that Mao was a \"man of complex moods\", who \"tried his best to bring about prosperity and gain international respect\" for China, being \"neither a saint nor a demon.\" They noted that in early life, he strived to be \"a strong, wilful, and purposeful hero, not bound by any moral chains\", and that he \"passionately desired fame and power\".\n\nMao had learned some English language, particularly through Zhang Hanzhi, who was his English teacher, interpreter and diplomat who later married Qiao Guanhua, Foreign Minister of China and the head of China's UN delegation. However, his spoken English was limited to a few single words, phrases, and some short sentences. He first chose to systematically learn English in the 1950s, which was very unusual as the main foreign language first taught in Chinese schools at that time was Russian.\n\nMao was a prolific writer of political and philosophical literature. He is the attributed author of \"Quotations from Chairman Mao Tse-tung\", known in the West as the \"Little Red Book\" and in Cultural Revolution China as the \"Red Treasure Book\" (): first published in January 1964, this is a collection of short extracts from his many speeches and articles, edited by Lin Biao and ordered topically. Mao wrote several other philosophical treatises, both before and after he assumed power. These include:\n\nMao was also a skilled Chinese calligrapher with a highly personal style. In China, Mao was considered a master calligrapher during his lifetime. His calligraphy can be seen today throughout mainland China. His work gave rise to a new form of Chinese calligraphy called \"Mao-style\" or \"Maoti\", which has gained increasing popularity since his death. There currently exist various competitions specialising in Mao-style calligraphy.\n\nAs did most Chinese intellectuals of his generation, Mao's education began with Chinese classical literature. Mao told Edgar Snow in 1936 that he had started the study of the Confucian Analects and the Four Books at a village school when he was eight, but that the books he most enjoyed reading were \"Water Margin\", \"Journey to the West\", the \"Romance of the Three Kingdoms\" and \"Dream of the Red Chamber\". Mao published poems in classical forms starting in his youth and his abilities as a poet contributed to his image in China after he came to power in 1949. His style was influenced by the great Tang dynasty poets Li Bai and Li He.\n\nSome of his most well-known poems are \"Changsha\" (1925), \"The Double Ninth\" (1929.10), \"Loushan Pass\" (1935), \"The Long March\" (1935), \"Snow\" (1936), \"The PLA Captures Nanjing\" (1949), \"Reply to Li Shuyi\" (1957.05.11) and \"Ode to the Plum Blossom\" (1961.12).\n\nMao has been portrayed in film and television numerous times. Some notable actors include: Han Shi, the first actor ever to have portrayed Mao, in a 1978 drama \"Dielianhua\" and later again in a 1980 film \"Cross the Dadu River\"; Gu Yue, who had portrayed Mao 84 times on screen throughout his 27-year career and had won the Best Actor title at the Hundred Flowers Awards in 1990 and 1993; Liu Ye, who played a young Mao in \"The Founding of a Party\" (2011); Tang Guoqiang, who has frequently portrayed Mao in more recent times, in the films \"The Long March\" (1996) and \"The Founding of a Republic\" (2009), and the television series \"Huang Yanpei\" (2010), among others. Mao is a principal character in American composer John Adams' opera \"Nixon in China\" (1987). The Beatles' song \"Revolution\" refers to Mao: \"...but if you go carrying pictures of Chairman Mao you ain't going to make it with anyone anyhow...\"; John Lennon expressed regret over including these lines in the song in 1972.\n\n\n\n\n"}
{"id": "19528", "url": "https://en.wikipedia.org/wiki?curid=19528", "title": "Mechanical engineering", "text": "Mechanical engineering\n\nMechanical engineering is the discipline that applies engineering, physics, engineering mathematics, and materials science principles to design, analyze, manufacture, and maintain mechanical systems. It is one of the oldest and broadest of the engineering disciplines.\n\nThe mechanical engineering field requires an understanding of core areas including mechanics, dynamics, thermodynamics, materials science, structural analysis, and electricity. In addition to these core principles, mechanical engineers use tools such as computer-aided design (CAD), computer-aided manufacturing (CAM), and product life cycle management to design and analyze manufacturing plants, industrial equipment and machinery, heating and cooling systems, transport systems, aircraft, watercraft, robotics, medical devices, weapons, and others. It is the branch of engineering that involves the design, production, and operation of machinery.\n\nMechanical engineering emerged as a field during the Industrial Revolution in Europe in the 18th century; however, its development can be traced back several thousand years around the world. In the 19th century, developments in physics led to the development of mechanical engineering science. The field has continually evolved to incorporate advancements; today mechanical engineers are pursuing developments in such areas as composites, mechatronics, and nanotechnology. It also overlaps with aerospace engineering, metallurgical engineering, civil engineering, electrical engineering, manufacturing engineering, chemical engineering, industrial engineering, and other engineering disciplines to varying amounts. Mechanical engineers may also work in the field of biomedical engineering, specifically with biomechanics, transport phenomena, biomechatronics, bionanotechnology, and modelling of biological systems.\n\nThe application of mechanical engineering can be seen in the archives of various ancient and medieval societies. In ancient Greece, the works of Archimedes (287–212 BC) influenced mechanics in the Western tradition and Heron of Alexandria (c. 10–70 AD) created the first steam engine (Aeolipile). In China, Zhang Heng (78–139 AD) improved a water clock and invented a seismometer, and Ma Jun (200–265 AD) invented a chariot with differential gears. The medieval Chinese horologist and engineer Su Song (1020–1101 AD) incorporated an escapement mechanism into his astronomical clock tower two centuries before escapement devices were found in medieval European clocks. He also invented the world's first known endless power-transmitting chain drive.\n\nDuring the Islamic Golden Age (7th to 15th century), Muslim inventors made remarkable contributions in the field of mechanical technology. Al-Jazari, who was one of them, wrote his famous \"Book of Knowledge of Ingenious Mechanical Devices\" in 1206 and presented many mechanical designs. Al-Jazari is also the first known person to create devices such as the crankshaft and camshaft, which now form the basics of many mechanisms.\n\nDuring the 17th century, important breakthroughs in the foundations of mechanical engineering occurred in England. Sir Isaac Newton formulated Newton's Laws of Motion and developed Calculus, the mathematical basis of physics. Newton was reluctant to publish his works for years, but he was finally persuaded to do so by his colleagues, such as Sir Edmond Halley, much to the benefit of all mankind. Gottfried Wilhelm Leibniz is also credited with creating Calculus during this time period.\n\nDuring the early 19th century industrial revolution, machine tools were developed in England, Germany, and Scotland. This allowed mechanical engineering to develop as a separate field within engineering. They brought with them manufacturing machines and the engines to power them. The first British professional society of mechanical engineers was formed in 1847 Institution of Mechanical Engineers, thirty years after the civil engineers formed the first such professional society Institution of Civil Engineers. On the European continent, Johann von Zimmermann (1820–1901) founded the first factory for grinding machines in Chemnitz, Germany in 1848.\n\nIn the United States, the American Society of Mechanical Engineers (ASME) was formed in 1880, becoming the third such professional engineering society, after the American Society of Civil Engineers (1852) and the American Institute of Mining Engineers (1871). The first schools in the United States to offer an engineering education were the United States Military Academy in 1817, an institution now known as Norwich University in 1819, and Rensselaer Polytechnic Institute in 1825. Education in mechanical engineering has historically been based on a strong foundation in mathematics and science.\n\nDegrees in mechanical engineering are offered at various universities worldwide. Mechanical engineering programs typically take four to five years of study and result in a Bachelor of Engineering (B.Eng. or B.E.), Bachelor of Science (B.Sc. or B.S.), Bachelor of Science Engineering (B.Sc.Eng.), Bachelor of Technology (B.Tech.), Bachelor of Mechanical Engineering (B.M.E.), or Bachelor of Applied Science (B.A.Sc.) degree, in or with emphasis in mechanical engineering. In Spain, Portugal and most of South America, where neither B.Sc. nor B.Tech. programs have been adopted, the formal name for the degree is \"Mechanical Engineer\", and the course work is based on five or six years of training. In Italy the course work is based on five years of education, and training, but in order to qualify as an Engineer one has to pass a state exam at the end of the course. In Greece, the coursework is based on a five-year curriculum and the requirement of a 'Diploma' Thesis, which upon completion a 'Diploma' is awarded rather than a B.Sc.\n\nIn the United States, most undergraduate mechanical engineering programs are accredited by the Accreditation Board for Engineering and Technology (ABET) to ensure similar course requirements and standards among universities. The ABET web site lists 302 accredited mechanical engineering programs as of 11 March 2014. Mechanical engineering programs in Canada are accredited by the Canadian Engineering Accreditation Board (CEAB), and most other countries offering engineering degrees have similar accreditation societies.\n\nIn Australia, mechanical engineering degrees are awarded as Bachelor of Engineering (Mechanical) or similar nomenclature although there are an increasing number of specialisations. The degree takes four years of full-time study to achieve. To ensure quality in engineering degrees, Engineers Australia accredits engineering degrees awarded by Australian universities in accordance with the global Washington Accord. Before the degree can be awarded, the student must complete at least 3 months of on the job work experience in an engineering firm. Similar systems are also present in South Africa and are overseen by the Engineering Council of South Africa (ECSA).\n\nIn India, to become an engineer, one needs to have an engineering degree like a B.Tech or B.E, have a diploma in engineering, or by completing a course in an engineering trade like fitter from the Industrial Training Institute (ITIs) to receive a \"ITI Trade Certificate\" and also pass the All India Trade Test (AITT) with an engineering trade conducted by the National Council of Vocational Training (NCVT) by which one is awarded a \"National Trade Certificate\". A similar system is used in Nepal.\n\nSome mechanical engineers go on to pursue a postgraduate degree such as a Master of Engineering, Master of Technology, Master of Science, Master of Engineering Management (M.Eng.Mgt. or M.E.M.), a Doctor of Philosophy in engineering (Eng.D. or Ph.D.) or an engineer's degree. The master's and engineer's degrees may or may not include research. The Doctor of Philosophy includes a significant research component and is often viewed as the entry point to academia. The Engineer's degree exists at a few institutions at an intermediate level between the master's degree and the doctorate.\n\nStandards set by each country's accreditation society are intended to provide uniformity in fundamental subject material, promote competence among graduating engineers, and to maintain confidence in the engineering profession as a whole. Engineering programs in the U.S., for example, are required by ABET to show that their students can \"work professionally in both thermal and mechanical systems areas.\" The specific courses required to graduate, however, may differ from program to program. Universities and Institutes of technology will often combine multiple subjects into a single class or split a subject into multiple classes, depending on the faculty available and the university's major area(s) of research.\n\nThe fundamental subjects of mechanical engineering usually include:\n\nMechanical engineers are also expected to understand and be able to apply basic concepts from chemistry, physics, chemical engineering, civil engineering, and electrical engineering. All mechanical engineering programs include multiple semesters of mathematical classes including calculus, and advanced mathematical concepts including differential equations, partial differential equations, linear algebra, abstract algebra, and differential geometry, among others.\n\nIn addition to the core mechanical engineering curriculum, many mechanical engineering programs offer more specialized programs and classes, such as control systems, robotics, transport and logistics, cryogenics, fuel technology, automotive engineering, biomechanics, vibration, optics and others, if a separate department does not exist for these subjects.\n\nMost mechanical engineering programs also require varying amounts of research or community projects to gain practical problem-solving experience. In the United States it is common for mechanical engineering students to complete one or more internships while studying, though this is not typically mandated by the university. Cooperative education is another option. Future work skills research puts demand on study components that feed student's creativity and innovation.\n\nMechanical engineers research, design, develop, build, and test mechanical and thermal devices, including tools, engines, and machines.\n\nMechanical engineers typically do the following:\n\nMechanical engineers design and oversee the manufacturing of many products ranging from medical devices to new batteries. They also design power-producing machines such as electric generators, internal combustion engines, and steam and gas turbines as well as power-using machines, such as refrigeration and air-conditioning systems.\n\nLike other engineers, mechanical engineers use computers to help create and analyze designs, run simulations and test how a machine is likely to work.\n\nEngineers may seek license by a state, provincial, or national government. The purpose of this process is to ensure that engineers possess the necessary technical knowledge, real-world experience, and knowledge of the local legal system to practice engineering at a professional level. Once certified, the engineer is given the title of Professional Engineer (in the United States, Canada, Japan, South Korea, Bangladesh and South Africa), Chartered Engineer (in the United Kingdom, Ireland, India and Zimbabwe), \"Chartered Professional Engineer\" (in Australia and New Zealand) or \"European Engineer\" (much of the European Union).\n\nIn the U.S., to become a licensed Professional Engineer (PE), an engineer must pass the comprehensive FE (Fundamentals of Engineering) exam, work a minimum of 4 years as an \"Engineering Intern (EI)\" or \"Engineer-in-Training (EIT)\", and pass the \"Principles and Practice\" or PE (Practicing Engineer or Professional Engineer) exams. The requirements and steps of this process are set forth by the National Council of Examiners for Engineering and Surveying (NCEES), a composed of engineering and land surveying licensing boards representing all U.S. states and territories.\n\nIn the UK, current graduates require a BEng plus an appropriate master's degree or an integrated MEng degree, a minimum of 4 years post graduate on the job competency development, and a peer reviewed project report in the candidates specialty area in order to become a Chartered Mechanical Engineer (CEng, MIMechE) through the Institution of Mechanical Engineers. CEng MIMechE can also be obtained via an examination route administered by the City and Guilds of London Institute.\n\nIn most developed countries, certain engineering tasks, such as the design of bridges, electric power plants, and chemical plants, must be approved by a professional engineer or a chartered engineer. \"Only a licensed engineer, for instance, may prepare, sign, seal and submit engineering plans and drawings to a public authority for approval, or to seal engineering work for public and private clients.\" This requirement can be written into state and provincial legislation, such as in the Canadian provinces, for example the Ontario or Quebec's Engineer Act.\n\nIn other countries, such as Australia, and the UK, no such legislation exists; however, practically all certifying bodies maintain a code of ethics independent of legislation, that they expect all members to abide by or risk expulsion.\n\nThe total number of engineers employed in the U.S. in 2015 was roughly 1.6 million. Of these, 278,340 were mechanical engineers (17.28%), the largest discipline by size. In 2012, the median annual income of mechanical engineers in the U.S. workforce was $80,580. The median income was highest when working for the government ($92,030), and lowest in education ($57,090). In 2014, the total number of mechanical engineering jobs was projected to grow 5% over the next decade. As of 2009, the average starting salary was $58,800 with a bachelor's degree.\n\nThe field of mechanical engineering can be thought of as a collection of many mechanical engineering science disciplines. Several of these subdisciplines which are typically taught at the undergraduate level are listed below, with a brief explanation and the most common application of each. Some of these subdisciplines are unique to mechanical engineering, while others are a combination of mechanical engineering and one or more other disciplines. Most work that a mechanical engineer does uses skills and techniques from several of these subdisciplines, as well as specialized subdisciplines. Specialized subdisciplines, as used in this article, are more likely to be the subject of graduate studies or on-the-job training than undergraduate research. Several specialized subdisciplines are discussed in this section.\n\nMechanics is, in the most general sense, the study of forces and their effect upon matter. Typically, engineering mechanics is used to analyze and predict the acceleration and deformation (both elastic and plastic) of objects under known forces (also called loads) or stresses. Subdisciplines of mechanics include\n\nMechanical engineers typically use mechanics in the design or analysis phases of engineering. If the engineering project were the design of a vehicle, statics might be employed to design the frame of the vehicle, in order to evaluate where the stresses will be most intense. Dynamics might be used when designing the car's engine, to evaluate the forces in the pistons and cams as the engine cycles. Mechanics of materials might be used to choose appropriate materials for the frame and engine. Fluid mechanics might be used to design a ventilation system for the vehicle (see HVAC), or to design the intake system for the engine.\n\nMechatronics is a combination of mechanics and electronics. It is an interdisciplinary branch of mechanical engineering, electrical engineering and software engineering that is concerned with integrating electrical and mechanical engineering to create hybrid systems. In this way, machines can be automated through the use of electric motors, servo-mechanisms, and other electrical systems in conjunction with special software. A common example of a mechatronics system is a CD-ROM drive. Mechanical systems open and close the drive, spin the CD and move the laser, while an optical system reads the data on the CD and converts it to bits. Integrated software controls the process and communicates the contents of the CD to the computer.\n\nRobotics is the application of mechatronics to create robots, which are often used in industry to perform tasks that are dangerous, unpleasant, or repetitive. These robots may be of any shape and size, but all are preprogrammed and interact physically with the world. To create a robot, an engineer typically employs kinematics (to determine the robot's range of motion) and mechanics (to determine the stresses within the robot).\n\nRobots are used extensively in industrial engineering. They allow businesses to save money on labor, perform tasks that are either too dangerous or too precise for humans to perform them economically, and to ensure better quality. Many companies employ assembly lines of robots, especially in Automotive Industries and some factories are so robotized that they can run by themselves. Outside the factory, robots have been employed in bomb disposal, space exploration, and many other fields. Robots are also sold for various residential applications, from recreation to domestic applications.\n\nStructural analysis is the branch of mechanical engineering (and also civil engineering) devoted to examining why and how objects fail and to fix the objects and their performance. Structural failures occur in two general modes: static failure, and fatigue failure. \"Static structural failure\" occurs when, upon being loaded (having a force applied) the object being analyzed either breaks or is deformed plastically, depending on the criterion for failure. \"Fatigue failure\" occurs when an object fails after a number of repeated loading and unloading cycles. Fatigue failure occurs because of imperfections in the object: a microscopic crack on the surface of the object, for instance, will grow slightly with each cycle (propagation) until the crack is large enough to cause ultimate failure.\n\nFailure is not simply defined as when a part breaks, however; it is defined as when a part does not operate as intended. Some systems, such as the perforated top sections of some plastic bags, are designed to break. If these systems do not break, failure analysis might be employed to determine the cause.\n\nStructural analysis is often used by mechanical engineers after a failure has occurred, or when designing to prevent failure. Engineers often use online documents and books such as those published by ASM to aid them in determining the type of failure and possible causes.\n\nOnce theory is applied to a mechanical design, physical testing is often performed to verify calculated results. Structural analysis may be used in an office when designing parts, in the field to analyze failed parts, or in laboratories where parts might undergo controlled failure tests.\n\nThermodynamics is an applied science used in several branches of engineering, including mechanical and chemical engineering. At its simplest, thermodynamics is the study of energy, its use and transformation through a system. Typically, engineering thermodynamics is concerned with changing energy from one form to another. As an example, automotive engines convert chemical energy (enthalpy) from the fuel into heat, and then into mechanical work that eventually turns the wheels.\n\nThermodynamics principles are used by mechanical engineers in the fields of heat transfer, thermofluids, and energy conversion. Mechanical engineers use thermo-science to design engines and power plants, heating, ventilation, and air-conditioning (HVAC) systems, heat exchangers, heat sinks, radiators, refrigeration, insulation, and others.\n\nDrafting or technical drawing is the means by which mechanical engineers design products and create instructions for manufacturing parts. A technical drawing can be a computer model or hand-drawn schematic showing all the dimensions necessary to manufacture a part, as well as assembly notes, a list of required materials, and other pertinent information. A U.S. mechanical engineer or skilled worker who creates technical drawings may be referred to as a drafter or draftsman. Drafting has historically been a two-dimensional process, but computer-aided design (CAD) programs now allow the designer to create in three dimensions.\n\nInstructions for manufacturing a part must be fed to the necessary machinery, either manually, through programmed instructions, or through the use of a computer-aided manufacturing (CAM) or combined CAD/CAM program. Optionally, an engineer may also manually manufacture a part using the technical drawings. However, with the advent of computer numerically controlled (CNC) manufacturing, parts can now be fabricated without the need for constant technician input. Manually manufactured parts generally consist spray coatings, surface finishes, and other processes that cannot economically or practically be done by a machine.\n\nDrafting is used in nearly every subdiscipline of mechanical engineering, and by many other branches of engineering and architecture. Three-dimensional models created using CAD software are also commonly used in finite element analysis (FEA) and computational fluid dynamics (CFD).\n\nMany mechanical engineering companies, especially those in industrialized nations, have begun to incorporate computer-aided engineering (CAE) programs into their existing design and analysis processes, including 2D and 3D solid modeling computer-aided design (CAD). This method has many benefits, including easier and more exhaustive visualization of products, the ability to create virtual assemblies of parts, and the ease of use in designing mating interfaces and tolerances.\n\nOther CAE programs commonly used by mechanical engineers include product lifecycle management (PLM) tools and analysis tools used to perform complex simulations. Analysis tools may be used to predict product response to expected loads, including fatigue life and manufacturability. These tools include finite element analysis (FEA), computational fluid dynamics (CFD), and computer-aided manufacturing (CAM).\n\nUsing CAE programs, a mechanical design team can quickly and cheaply iterate the design process to develop a product that better meets cost, performance, and other constraints. No physical prototype need be created until the design nears completion, allowing hundreds or thousands of designs to be evaluated, instead of a relative few. In addition, CAE analysis programs can model complicated physical phenomena which cannot be solved by hand, such as viscoelasticity, complex contact between mating parts, or non-Newtonian flows.\n\nAs mechanical engineering begins to merge with other disciplines, as seen in mechatronics, multidisciplinary design optimization (MDO) is being used with other CAE programs to automate and improve the iterative design process. MDO tools wrap around existing CAE processes, allowing product evaluation to continue even after the analyst goes home for the day. They also utilize sophisticated optimization algorithms to more intelligently explore possible designs, often finding better, innovative solutions to difficult multidisciplinary design problems.\n\nMechanical engineers are constantly pushing the boundaries of what is physically possible in order to produce safer, cheaper, and more efficient machines and mechanical systems. Some technologies at the cutting edge of mechanical engineering are listed below (see also exploratory engineering).\n\nMicron-scale mechanical components such as springs, gears, fluidic and heat transfer devices are fabricated from a variety of substrate materials such as silicon, glass and polymers like SU8. Examples of MEMS components are the accelerometers that are used as car airbag sensors, modern cell phones, gyroscopes for precise positioning and microfluidic devices used in biomedical applications.\n\nFriction stir welding, a new type of welding, was discovered in 1991 by The Welding Institute (TWI). The innovative steady state (non-fusion) welding technique joins materials previously un-weldable, including several aluminum alloys. It plays an important role in the future construction of airplanes, potentially replacing rivets. Current uses of this technology to date include welding the seams of the aluminum main Space Shuttle external tank, Orion Crew Vehicle test article, Boeing Delta II and Delta IV Expendable Launch Vehicles and the SpaceX Falcon 1 rocket, armor plating for amphibious assault ships, and welding the wings and fuselage panels of the new Eclipse 500 aircraft from Eclipse Aviation among an increasingly growing pool of uses.\n\nComposites or composite materials are a combination of materials which provide different physical characteristics than either material separately. Composite material research within mechanical engineering typically focuses on designing (and, subsequently, finding applications for) stronger or more rigid materials while attempting to reduce weight, susceptibility to corrosion, and other undesirable factors. Carbon fiber reinforced composites, for instance, have been used in such diverse applications as spacecraft and fishing rods.\n\nMechatronics is the synergistic combination of mechanical engineering, electronic engineering, and software engineering. The discipline of mechatronics began as a way to combine mechanical principles with electrical engineering. Mechatronic concepts are used in the majority of electro-mechanical systems. Typical electro-mechanical sensors used in mechatronics are strain gauges, thermocouples, and pressure transducers.\n\nAt the smallest scales, mechanical engineering becomes nanotechnology—one speculative goal of which is to create a molecular assembler to build molecules and materials via mechanosynthesis. For now that goal remains within exploratory engineering. Areas of current mechanical engineering research in nanotechnology include nanofilters, nanofilms, and nanostructures, among others.\nFinite Element Analysis is a computational tool used to estimate stress, strain, and deflection of solid bodies. It uses a mesh setup with user-defined sizes to measure physical quantities at a node. The more nodes there are, the higher the precision. This field is not new, as the basis of Finite Element Analysis (FEA) or Finite Element Method (FEM) dates back to 1941. But the evolution of computers has made FEA/FEM a viable option for analysis of structural problems. Many commercial codes such as NASTRAN, ANSYS, and ABAQUS are widely used in industry for research and the design of components. Some 3D modeling and CAD software packages have added FEA modules. In the recent times, cloud simulation platforms like SimScale are becoming more common.\n\nOther techniques such as finite difference method (FDM) and finite-volume method (FVM) are employed to solve problems relating heat and mass transfer, fluid flows, fluid surface interaction, etc.\n\nBiomechanics is the application of mechanical principles to biological systems, such as humans, animals, plants, organs, and cells. Biomechanics also aids in creating prosthetic limbs and artificial organs for humans. Biomechanics is closely related to engineering, because it often uses traditional engineering sciences to analyze biological systems. Some simple applications of Newtonian mechanics and/or materials sciences can supply correct approximations to the mechanics of many biological systems.\n\nIn the past decade, reverse engineering of materials found in nature such as bone matter has gained funding in academia. The structure of bone matter is optimized for its purpose of bearing a large amount of compressive stress per unit weight. The goal is to replace crude steel with bio-material for structural design.\n\nOver the past decade the Finite element method (FEM) has also entered the Biomedical sector highlighting further engineering aspects of Biomechanics. FEM has since then established itself as an alternative to in vivo surgical assessment and gained the wide acceptance of academia. The main advantage of Computational Biomechanics lies in its ability to determine the endo-anatomical response of an anatomy, without being subject to ethical restrictions. This has led FE modelling to the point of becoming ubiquitous in several fields of Biomechanics while several projects have even adopted an open source philosophy (e.g. BioSpine).\n\nComputational fluid dynamics, usually abbreviated as CFD, is a branch of fluid mechanics that uses numerical methods and algorithms to solve and analyze problems that involve fluid flows. Computers are used to perform the calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions. With high-speed supercomputers, better solutions can be achieved. Ongoing research yields software that improves the accuracy and speed of complex simulation scenarios such as transonic or turbulent flows. Initial validation of such software is performed using a wind tunnel with the final validation coming in full-scale testing, e.g. flight tests.\n\nAcoustical engineering is one of many other sub-disciplines of mechanical engineering and is the application of acoustics. Acoustical engineering is the study of Sound and Vibration. These engineers work effectively to reduce noise pollution in mechanical devices and in buildings by soundproofing or removing sources of unwanted noise. The study of acoustics can range from designing a more efficient hearing aid, microphone, headphone, or recording studio to enhancing the sound quality of an orchestra hall. Acoustical engineering also deals with the vibration of different mechanical systems.\n\nManufacturing engineering, aerospace engineering and automotive engineering are grouped with mechanical engineering at times. A bachelor's degree in these areas will typically have a difference of a few specialized classes.\n\n\n\n\n\n\n"}
{"id": "19529", "url": "https://en.wikipedia.org/wiki?curid=19529", "title": "Minister", "text": "Minister\n\nMinister may refer to:\n\n\n"}
{"id": "19530", "url": "https://en.wikipedia.org/wiki?curid=19530", "title": "March 11", "text": "March 11\n\n\n\n"}
{"id": "19531", "url": "https://en.wikipedia.org/wiki?curid=19531", "title": "Monkey Island (series)", "text": "Monkey Island (series)\n\nMonkey Island is a series of adventure games. The first four games in the series were produced and published by LucasArts, formerly known as Lucasfilm Games. The fifth installment of the franchise was developed by Telltale Games in collaboration with LucasArts. The games follow the misadventures of the hapless Guybrush Threepwood as he struggles to become the most notorious pirate in the Caribbean, defeat the plans of the evil undead pirate LeChuck and win the heart of governess Elaine Marley. Each game's plot usually involves the mysterious Monkey Island and its impenetrable secrets.\n\nThe first game in the series was created as a collaborative effort among Ron Gilbert, Tim Schafer and Dave Grossman. Gilbert worked on the first two games before leaving LucasArts. Grossman and Schafer, who also worked on the first two games, would enjoy success on other games before they both left LucasArts. The rights to \"Monkey Island\" remained with LucasArts, and the third and fourth games were created without direct involvement from the original writing staff. Dave Grossman was the project leader of the fifth game in the series and Ron Gilbert was involved with the first design of the game.\n\nThe Monkey Island series is known for its humor and \"player-friendly\" qualities. The player cannot permanently place the game in an unwinnable state or cause Guybrush to die without great effort. This \"player-friendly\" approach was unusual at the time of the first game's release in 1990; prominent adventure-game rivals included Sierra On-Line and Infocom, both of which were known for games with sudden and frequent character deaths or \"lock-outs\". LucasArts itself used such closed plot paths for its drama games like \"\" (1989), but preferred the open format for other humor-oriented adventure games such as \"Sam & Max Hit the Road\" (1993) and \"Day of the Tentacle\" (1993). After \"\" in 1991, the series went in hiatus until 1997, when it resumed with \"The Curse of Monkey Island\". After the fourth entry, \"Escape from Monkey Island\", the franchise again went on hiatus, though numerous rumors persisted about a revival until the announcement of \"Tales of Monkey Island\" by Telltale Games in early 2009.\n\nMuch of the music of the games is composed by Michael Land. The score largely consists of reggae, Caribbean and dub-inspired music.\n\nThe series also tends to break the fourth wall, as several of the characters acknowledge that they are in a video game.\n\nEach of the games takes place on in the Caribbean around the Golden Age of Piracy sometime between the 17th and 18th centuries. The islands teem with pirates dressed in outfits that seem to come from films and comic books rather than history, and there are many deliberate anachronisms and references to modern-day popular culture.\n\nThe main setting of the \"Monkey Island\" games is the \"Tri-Island Area\", a fictional archipelago in the Caribbean. Since the first game in the series, \"The Secret of Monkey Island\", three of the games have visited the eponymous island of Monkey Island, while all have introduced their own set of islands to explore. \"Monkey Island 2: LeChuck's Revenge\" features four new islands, but does not return to Monkey Island until the final cutscene. \"The Curse of Monkey Island\" introduces three, and \"Escape from Monkey Island\", which revisits some of the older islands, features three new islands as well. As such, the \"Tri-Island area\" actually comprises a total of 13 visitable islands. \"Tales of Monkey Island\" takes place in a new area of the Caribbean called the \"Gulf of Melange\".\n\nThe main islands of the Tri-Island Area are Mêlée Island, Booty Island, and Plunder Island governed by Elaine Marley in place of her long lost grandfather, Horatio Torquemada Marley. Elaine moves from island to island at her convenience, though she considers her governor's mansion on Mêlée Island, the capital island of the area, as home.\n\nOther islands in the region are considered under the umbrella of Tri-Island Area as well, even though not directly governed by Elaine include: Lucre Island, Jambalaya Island, Scabb Island, Phatt Island, Hook Island, Skull Island, Knuttin Atoll, Blood Island, Spittle Island and Pinchpenny Island.\n\nThe Gulf of Melange has its own set of islands: Flotsam Island, the Jerkbait Islands (Spinner Cay, Spoon Island, Roe Island), Brillig Island, Boulder Beach, Isle of Ewe, and the Rock of Gelato.\n\nMonkey Island and Dinky Island are not officially part of any island area, but nonetheless are central to the series' overall back-story and canon.\n\nThe games have a wide cast of characters, many of which reappear throughout the series. Each entry in the series revolves around three main characters: the hero Guybrush Threepwood; his love interest Elaine Marley; and the villain, the ghost pirate LeChuck. Several other characters such as the Voodoo Lady, Stan the salesman, Murray the Demonic Talking Skull and Herman Toothrot make multiple appearances within the series as well.\n\nRon Gilbert's two main inspirations for the story were Disneyland's Pirates of the Caribbean ride and Tim Powers' book \"On Stranger Tides\". The book was the inspiration for the story and characters, while the ride was the inspiration for the ambiance. \"[The POTC Ride] keeps you moving through the adventure,\" Gilbert said in an interview, \"but I've always wished I could get off and wander around, learn more about the characters, and find a way onto those pirate ships. So with The Secret of Monkey Island(TM) I wanted to create a game that had the same flavor, but where you could step off the boat and enter that whole storybook world.\"\n\nSeveral specific references to the ride are made throughout the series, including a puzzle in the based on the ride's famous Jail Cell/Dog With Keys scene (the dog in the scene is even named Walt). The banjo music in the opening menu of the third game is also very reminiscent of the banjo music at the beginning of the ride. Additional references are made to Disneyland and theme parks in general throughout the series, including Guybrush finding an E ticket.\n\nThe series debuted in 1990 with \"The Secret of Monkey Island\" on the Amiga, MS-DOS, Atari ST and Macintosh platforms; the game was later ported to FM Towns and Mega-CD (1993). A remastered version with updated graphics and new voiceovers was released for PlayStation Network, PC Windows, Xbox Live Arcade and OS X. An iPhone version was also released on July 23, 2009.\n\nThe game starts off with the main character Guybrush Threepwood stating \"I want to be a pirate!\" To do so, he must prove himself to three old pirate captains. During the perilous pirate trials, he meets the beautiful governor Elaine Marley, with whom he falls in love, unaware that the ghost pirate LeChuck also has his eyes on her. When Elaine is kidnapped, Guybrush procures crew and ship to track LeChuck down, defeat him and rescue his love.\n\nThe second game, \"Monkey Island 2: LeChuck's Revenge\" from 1991, was available for fewer platforms; it was only released for PC MS-DOS, Amiga, Macintosh, and later for FM Towns. A Special Edition version, in a similar style as The Secret of Monkey Island: Special Edition, was released in July 2010 for iPhone, iPad, iPod Touch, Mac, PC, PS3 and Xbox 360.\n\nAs Guybrush, with a treasure chest in hand, and Elaine hang onto ropes in a void, he tells her the story of the game. He has decided to find the greatest of all treasures, that of Big Whoop. Unwittingly, he helps revive LeChuck, who is now in zombie form. Guybrush is eventually captured by his nemesis, but escapes with help from Wally and finds the treasure only to find himself dangling from a rope, as depicted at the beginning of the game. As Guybrush concludes his story, his rope breaks and he finds himself facing LeChuck, whom he finally defeats using voodoo. The surrealistic ending is open to a number of interpretations. In the manual of The Curse of Monkey Island, it is stated that Guybrush falls victim to a hex implemented by LeChuck.\n\n\"The Curse of Monkey Island\", the third in the series, was exclusively available for PC Windows in 1997 after a 6-year hiatus. The Curse of Monkey Island was released after what could be said to be the biggest technological change in the gaming industry. This new era saw the advent of digital audio, CD-ROM technology, and advancements in graphics.\n\n\"Monkey Island I\" and \"II\" were originally released on floppy discs with text dialog only.\nThe visuals of the third installment was also an advance over the old game, using a cel animation style.\"The Curse of Monkey Island\" is the only game in the series to feature this style of animation; subsequent games used 3D polygon animation.\n\nGuybrush unwittingly turns Elaine into a gold statue with a cursed ring, and she is soon stolen by pirates. He tracks her down before searching for a ring that can lift the curse. LeChuck appears in a fiery demon form, and is on the heels of Guybrush until a stand-off in LeChuck's amusement park ride, Monkey Mountain.\n\n\"Escape from Monkey Island\", the fourth installment, was released in 2000 for PC Windows, and in 2001 for Macintosh and PlayStation 2.\n\nWhen Guybrush Threepwood and Elaine Marley return from their honeymoon, they find that Elaine has been declared officially dead, her mansion is under destruction order, and her position as governor is up for election. Guybrush investigates and unearths a conspiracy by LeChuck and evil real estate developer Ozzie Mandrill to use a voodoo talisman, \"The Ultimate Insult,\" to make all pirates docile in order to turn the Caribbean into a center of tourism.\n\n\"Tales of Monkey Island\" is the fifth installment within the series, co-developed by Telltale Games and LucasArts, with a simultaneous release both on WiiWare and PC. Unlike other installments, \"Tales\" is an episodic adventure consisting of five different episodes. The first episode was released on July 7, with the last one released on December 8, 2009.\n\nDuring a heated battle with his nemesis, the evil pirate LeChuck, Guybrush unwittingly unleashes an insidious pox that rapidly spreads across the Caribbean, turning pirates into zombie-like monsters. The Voodoo Lady sends Guybrush in search of a legendary sea sponge to stem the epidemic, but this seemingly straightforward quest has surprises around every corner.\n\"Tales of Monkey Island\" also released on PlayStation Network as a bundle for US$20.00.\n\nIn November 2011, when CEO of Telltale games Dan Conners was asked a question about another season of \"Monkey Island\", he replied: \"I wish we had the rights to do more \"Monkey\" but we don't. Right now what I gather is LA is focused on building AAA titles internally but honestly we don't talk much these days.\"\n\nThere has also been some speculation on Telltale Games forums about a possible sequel to \"Tales of Monkey Island\", although this was dismissed by Gilbert, who stated, \"Basically, when we were working on \"Tales\", I understood that ... I'm too old for that job now\" in an interview with \"Edge\" in March 2010. The \"Tales\" team claims that, despite a considerably increasing fanbase since 2009–10, there are not any plans to continue the series within the next five-year interval.\n\nWith the purchase of LucasArts by The Walt Disney Company in 2012, the rights to the franchise are now property of Disney. Ron Gilbert has been quoted in November 2012 as not being optimistic about the franchise's future, believing that Disney might abandon the franchise in favour of \"Pirates of the Caribbean\"; however, in December 2012, he was also quoted as wishing to contact Disney, hoping to \"make the game he wants to make\".\n\nIn May 2016, Disney Interactive announced that they would cease production on gaming and transition to a licensing model. Gilbert then took to Twitter on 23 May 2016 to express a desire to buy back the franchise saying \"Please sell me my \"Monkey Island\" and \"Maniac Mansion\" IP. I'll pay actual money for them.\". In 2017, fans of the series launched an online petition in support of Ron Gilbert, asking Disney to sell the franchise to him; as of June 2018, the petition has gathered about 15000 signatures.\n\nIn 2000, Lucasfilm together with Industrial Light & Magic were working on an animated film to be called \"Curse of Monkey Island\", which would be based on the series. Steve Purcell created the concept paintings and Ted Elliott wrote the story. However, during development the film was cancelled. Concept art of the film was released via Purcell's official blog.\n\nIn 2007, fan site World of Monkey Island was contacted by an anonymous source who told them that Ted Elliot had written the script for the film who before then remained unknown to the project.\nElliot would later go on to write the \"Pirates of the Caribbean\" film series, the scripts for the first two of which appear to have been adapted from the aborted project.\n\nThe games in the series share several minigames, puzzles, in-jokes, and references.\n\nEach game contains a map puzzle, wherein Guybrush must use an unconventional map to find his way through a maze. The first game features a set of dance instructions that point the way through the dense forest of Mêlée Island to find the Island's fabled treasure. In the second game, Guybrush must use a song from a dream sequence to find his way through LeChuck's dungeon. The third game is the reverse of this, as the instructions the player receives are traditional directions and need to be used on a theatre light board. The fourth game has a set of directions based on time, and the fifth based on animal sounds and the direction of the wind and finally a map to get one of the items needed for \"The Feast of the Senses\".\n\nEach game features a sequence of some sort, where players must gather the ingredients to create an item. Then, later in the game, the player has to create the item again, but this time around with improvised materials. In 'Secret', Guybrush must brew a voodoo concoction but, lacking ingredients, must improvise with the contents of his inventory, leading to amusing puns. In Monkey Island 2, at two points of the game, Guybrush has to create a voodoo doll, one of Largo LaGrande with legitimate ingredients, and one of LeChuck with improvised ingredients. The same goes with the hangover medicine in 'Curse' and the Ultimate Insult in 'Escape'. 'Tales' starts with Guybrush having to obtain fizzy root-beer then dropping it and him having to instead put together some fizzy root-grog. Later 'Tales' requires Guybrush to put together a 'feast of the senses' to increase the size of La Esponja Grande, and later track down a reversed recipe for the 'diet of the senses'.\n\nEach game also contains a minigame based on learning and repetition of a sequence in order to become more proficient: Insult Sword fighting in the first and third games, a number-based \"password\" as well as a spitting contest in the second, banjo fighting in the third, insult arm wrestling and Monkey Kombat in the fourth, and Pirate Face-Off in the fifth. The first, second and fourth games also feature a puzzle which involves following another character through several locations, a trick also used in \"Indiana Jones and the Fate of Atlantis\". Some other minigames include naval cannon battles, and platform diving.\n\nThe Monkey Island series is full of spoofs, in-jokes, humorous references, and Easter eggs: so many, in fact, that entire web sites are dedicated to their detection and listing.\n\nRunning gags include lines such as \"Look behind you, a three-headed monkey!\", the introduction \"My name is Guybrush Threepwood and I'm a mighty pirate\", \"How appropriate, you fight like a cow\", \"I'm selling these fine leather jackets\" (a reference to \"\"), and \"That's the second biggest [object] I've ever seen\", a catchphrase from the TV series \"Get Smart\" (and in EMI \"That's the second largest... No, that IS the \"largest\" conch shell I've ever seen!\"), and the astounding fact that Guybrush can hold his breath for ten minutes.\n\n\"The Secret of Monkey Island\" poked fun at rival company Sierra's game-over screens. For example, when Guybrush falls off a cliff, a \"game over\" window appears, but then Guybrush bounces back to the top of the cliff, explaining that he landed in a \"rubber tree\". Also, when Guybrush stays underwater for more than ten minutes, he dies and a \"game over\" dialog box identical to that of Sierra's \"King's Quest\" series appears, giving the player an option to restore a saved game and jokingly stating: \"Hope you saved the game!\"\n\nThe \"stump joke\" made fun about the use of multiple floppy disks for one program, but was not initially recognized by gamers as a joke. In \"The Secret of Monkey Island\", Guybrush comes across a passageway hidden beneath a stump, at which point a screen says to insert Disk No. 144. Later, in \"The Curse of Monkey Island\", Guybrush looks through a crack in the ceiling of an underground crypt to find himself peeking out of the same stump.\n\nRon Gilbert has openly admitted that sections of \"Monkey Island 2\" borrowed extensively from the original Pirates of the Caribbean Disneyland ride, such as the famous \"dog holding the keys to the jail-cell\". He has also said that he thought the second movie (\"\") may have 'borrowed' from the \"Monkey Island\" series. The opening menu banjo music in \"Curse\" is also very reminiscent of the beginning of the Disneyland ride.\n\nEach game in the series features cameo appearances by Steve Purcell's characters Sam & Max, who were featured in their own LucasArts adventure game, \"Sam & Max Hit the Road\". These are replaced by the purple tentacle from yet another LucasArts adventure game Day of the Tentacle in the special edition versions.\n\nThere are many comic references to various Lucas projects, especially Star Wars. For instance, in \"Curse,\" when you click on the fort that has been damaged by cannon fire from LeChuck's ship, Guybrush replies \"That's funny, the damage doesn't look as bad from out here,\" which is C-3PO's line after he and R2-D2 escape from Princess Leia's ship in the escape pod. When trying to gain access to the Brimstone Beach Club on Plunder Island, Guybrush attempts to use a 'Jedi mind trick' on the Cabaña Boy at the entrance. In Part V of \"Curse\", LeChuck says to Guybrush during the opening dialogue \"Search yer feelings, you know it to be true!\", to which Guybrush replies \"OH NO! IT CAN'T BE!\", lines similar to those between Darth Vader and Luke Skywalker in \"Star Wars V: The Empire Strikes Back\"\n\nIn LeChuck's Revenge, the Governor of Phatt Island, Governor Phatt, says in his sleep \"Be careful with those snacks, Eugene.\" in reference to the Pink Floyd song \"Careful With That Axe, Eugene.\"\n\nNone of the games explicitly reveal the \"Secret of Monkey Island\" (although creator Ron Gilbert has stated that the secret was not revealed in any of the games, and that the true secret would be revealed if he got to work on the fifth entry in the series). LeChuck himself, when asked in the second and third games, refuses to answer the question; Guybrush can eventually prod LeChuck to confess that he does not know what the secret is.\n\nGilbert stated that he never told anyone what the true secret of Monkey Island is. Gilbert stated in a 2004 interview that when the game was originally conceived it was considered \"too big\", so they split it into three parts. He added that he \"knows what the third [part] is\" and \"how the story's supposed to end,\" indicating that he had a definite concept of the secret and a conclusive third game.\n\nThe team behind \"Escape from Monkey Island\" attempted to resolve the issue by showing that the Giant Monkey Head was actually the control room of a Giant Monkey Robot. The cut-scene in which the revelation was made is called \"The Real Secret of Monkey Island\".\n\n"}
{"id": "19532", "url": "https://en.wikipedia.org/wiki?curid=19532", "title": "Cardiff Arms Park", "text": "Cardiff Arms Park\n\nCardiff Arms Park (), also known as The Arms Park and the BT Sport Cardiff Arms Park for sponsorship reasons from September 2014, is situated in the centre of Cardiff, Wales. It is primarily known as a rugby union stadium, but it also has a bowling green. The Arms Park was host to the British Empire and Commonwealth Games in 1958, and hosted four games in the 1991 Rugby World Cup, including the third-place play-off. The Arms Park also hosted the inaugural Heineken Cup Final of 1995–96 and the following year in 1996–97.\n\nThe history of the rugby ground begins with the first stands appearing for spectators in the ground in 1881–1882. Originally the Arms Park had a cricket ground to the north and a rugby union stadium to the south. By 1969, the cricket ground had been demolished to make way for the present day rugby ground to the north and a second rugby stadium to the south, called the National Stadium. The National Stadium, which was used by Wales national rugby union team, was officially opened on 7 April 1984, however in 1997 it was demolished to make way for the Millennium Stadium in 1999, which hosted the 1999 Rugby World Cup and became the national stadium of Wales. The rugby ground has remained the home of the semi-professional Cardiff RFC yet the professional Cardiff Blues regional rugby union team moved to the Cardiff City Stadium in 2009, but returned three years later.\n\nThe site is owned by Cardiff Athletic Club and has been host to many sports, apart from rugby union and cricket; they include athletics, association football, greyhound racing, tennis, British baseball and boxing. The site also has a bowling green to the north of the rugby ground, which is used by Cardiff Athletic Bowls Club, which is the bowls section of the Cardiff Athletic Club. The National Stadium also hosted many music concerts including Michael Jackson, David Bowie, Bon Jovi, The Rolling Stones and U2.\n\nThe Cardiff Arms Park site was originally called the Great Park, a swampy meadow behind the Cardiff Arms Hotel. The hotel was built by Sir Thomas Morgan, during the reign of Charles I. Cardiff Arms Park was named after this hotel. From 1803, the Cardiff Arms Hotel and the Park had become the property of the Bute family. The Arms Park soon became a popular place for sporting events, and by 1848, Cardiff Cricket Club was using the site for its cricket matches. However, by 1878, Cardiff Arms Hotel had been demolished.\n\nThe 3rd Marquess of Bute stipulated that the ground could only be used for \"recreational purposes\". At that time Cardiff Arms Park had a cricket ground to the north and a rugby union ground to the south. 1881–2 saw the first stands for spectators; they held 300 spectators and cost £50. The architect was Archibald Leitch, famous for designing Ibrox Stadium and Old Trafford, among others. In 1890, new standing areas were constructed along the entire length of the ground, with additional stands erected in 1896.\n\nBy 1912, the Cardiff Football Ground, as it was then known, had a new south stand and temporary stands on the north, east and west ends of the ground. The south stand was covered, while the north terrace was initially without a roof. The improvements were partly funded by the Welsh Rugby Union (WRU). The opening ceremony took place on 5 October 1912, with a match between Newport RFC and Cardiff RFC. The new ground was opened by Lord Ninian Crichton-Stuart. This new development increased the ground capacity to 43,000 and much improved the facilities at the ground compared to the earlier stands.\n\nIn 1922 John Crichton-Stuart, 4th Marquess of Bute, had sold the entire site and it was bought by the Cardiff Arms Park Company Limited for £30,000, it was then leased to the Cardiff Athletic Club (cricket and rugby sections) for 99 years at a cost of £200 per annum.\n\nDuring 1934 the cricket pavilion had been demolished to make way for the new North Stand, which was built on the rugby union ground, costing around £20,000. However, in 1941 the new North Stand and part of the west terracing had been badly damaged in the Blitz by the Luftwaffe during the Second World War.\n\nAt a general meeting of the WRU in June 1953, they made a decision, \"That until such time as the facilities at Swansea were improved, all international matches be played at Cardiff\". At the same time, plans were made for a new South Stand, which was estimated to cost £60,000, however the tender price came out at £90,000, a compromise was made, and it was decided to build a new upper South Stand costing £64,000 instead, with the Cardiff Athletic Club contributing £15,000 and the remainder coming from the WRU. The new South Stand opened in 1956, in time for the 1958 British Empire and Commonwealth Games. This brought the overall capacity of the Arms Park up to 60,000, of which 12,800 spectators were seated and the remainder standing.\n\nThe Arms Park hosted the 1958 British Empire and Commonwealth Games, which was used for the athletics events, but this event caused damage to the drainage system, so much so, that other rugby unions (England, Scotland and Ireland) complained after the Games about the state of the pitch. On 4 December 1960, due to torrential rain, the River Taff burst its banks with the Arms Park pitch being left under of water. The Development Committee was set up to resolve these issues on a permanent basis. They looked at various sites in Cardiff, but they all proved to be unsatisfactory. They also could not agree a solution with the Cardiff Athletic Club, so they purchased about of land at Island Farm in Bridgend, which was previously used as a prisoner-of-war camp. It is best known for being the camp where the biggest escape attempt was made by German prisoners of war in Great Britain during the Second World War. Due to problems including transport issues Glamorgan County Council never gave outline planning permission for the proposals and by June 1964 the scheme was abandoned. At that stage, the cricket ground to the north was still being used by Glamorgan County Cricket Club, and the rugby union ground to the south was used by the national Wales team and Cardiff RFC.\n\nBy 7 October 1966, the first floodlit game was held at Cardiff Arms Park, a game in which Cardiff RFC beat the Barbarians by 12 points to 8.\n\nThe National Stadium, which was also known as the Welsh National Rugby Ground, was designed by Osborne V Webb & Partners and built by G A Williamson & Associates of Porthcawl and Andrew Scott & Company of Port Talbot.\n\nAfter agreement from the Cardiff Athletic Club, the freehold of the south ground was transferred solely to the WRU in July 1968. Work could then begin on the new National Stadium. Glamorgan County Cricket Club would move to Sophia Gardens and the cricket ground to the north would be demolished and a new rugby union stadium built for Cardiff RFC, who would move out of the south ground, allowing the National Stadium to be built, for the sole use of the national rugby union team.\n\nOn 17 October 1970, the new North Stand and the Cardiff RFC ground was completed, the North Stand cost just over £1 million. The West Stand was opened in 1977 and the new East Terrace was completed by March 1980. By the time the final South Stand had been completed and the stadium officially opened on 7 April 1984, the South Stand had cost £4.5 million. At the start of the project, the total cost was estimated at £2.25 million, although by time it was finished in 1984, it had risen by nearly four times that amount.\n\nBoth stadia had approximately east-west alignment: the rugby ground to the north (Castle Street) end; the National Stadium to the south (Wood Street) end. The original capacity was 65,000 but this had to be reduced in later years to 53,000 for safety reasons. 11,000 of these were on the East Terrace and the conversion to all-seater stadium would have reduced the stadium capacity still further to 47,500. This capacity would have been much less than Twickenham and the other major rugby venues and also less than the demand for tickets to major events.\n\nA world record crowd of 56,000 for a rugby union club match watched Llanelli RFC beat Neath RFC by 28 points to 13 points in the final of the Schweppes Cup (WRU Challenge Cup) on 7 May 1988. The first evening game to be played under floodlights was held on 4 September 1991 at 8.00 pm, between Wales and France. The last international match to be held at the National Stadium was between Wales and England on 15 March 1997, and the last ever match that was held at the National Stadium was on 26 April 1997 between Cardiff and Swansea, Cardiff won the SWALEC Cup (WRU Challenge Cup) by 33 points to 26 points.\n\nIn 1997, just thirteen years after the National Stadium had opened, it was considered too small and did not have the facilities required of the time and it was demolished and a new stadium, the Millennium Stadium, was built in its place (completed to a north-south alignment and opened in June 1999). This would become the fourth redevelopment of the Cardiff Arms Park site. Although the Millennium Stadium is on roughly two thirds of the National Stadium, Cardiff Arms Park site, it is currently no longer using the Arms Park name. The official website confuses the issue as well, one part states that \"The Millennium Stadium is located on Westgate Street in Cardiff; next to the Cardiff Arms Park\" whereas another section specifically refers to the stadium as \"The Millennium Stadium, on the Cardiff Arms Park\"\n\nAn agreement in principle was reached in December 2015 between the landlord of the stadium site (Cardiff Athletic Club) and its tenant (Cardiff Blues) to give the club a 150-year lease on the stadium site. This could see the redevelopment of the Arms Park, including a new 15,000 seater stadium at 90 degrees to the existing stadium costing between £20 million and £30 million and surrounded by new offices and apartments.\n\nMore detailed negotiations will begin with a final approval expected early in 2016. If the final agreement goes ahead, Cardiff Athletic Club would receive an upfront payment of approximately £8 million. As part of the agreement, the bowls section would have to vacate its current site at the Arms Park and move to a new facility. At present Cardiff Blues pay Cardiff Athletic Club rent of around £115,000 per annum, however this would nearly double to around £200,000.\n\nOnly the rugby ground and the Cardiff Athletic Bowls Club now use the name Cardiff Arms Park. The rugby ground has two main stands, the North Stand and the South Stand. Both the Stands have terracing below seating. The other stands in the ground are the Westgate Street end (currently known as the Coastal Cottages Family Stand), which has rows of seating below executive boxes, plus the club shop, and the River Taff end (the Barry Nelmes Suite, named after Barry Nelmes, the former Cardiff RFC captain), which has 26 executive boxes. The rugby ground has two main entrances, the south entrance, and the Gwyn Nicholls Memorial Gates (Angel Hotel entrance), which was unveiled on 26 December 1949 in honour of the Welsh international rugby player Gwyn Nicholls. The Cardiff Athletic Clubhouse is situated in the corner of the ground between the South Stand and the Westgate Street end.\n\nThe South Stand of the rugby ground formed a complete unit with the North Stand of the National Stadium. Now the same structure of the South Stand of the rugby ground is also physically attached to the North Stand of the Millennium Stadium. This section is known colloquially as Glanmor's Gap, after Glanmor Griffiths, former chair and President of the WRU. This came about because the WRU were unable to secure enough funding to include the North Stand in the Millennium Stadium, and the National Lottery Commission would not provide any additional funds to be used for the construction of a new ground for Cardiff RFC. The Millennium Stadium was therefore built with the old reinforced concrete structure of the National Stadium (North Stand) and the new steel Millennium Stadium structure built around it.\n\nThere was doubt about the future of the Arms Park after 2010 following the move of the Cardiff Blues to the Cardiff City Stadium. Cardiff RFC Ltd, the company that runs Cardiff Blues and Cardiff RFC, still has a 15-year lease on the Arms Park, but talks are underway to release the rugby club from the terms of the lease, to enable the Millennium Stadium to be redeveloped with a new North Stand and adjoining convention centre. However, it still has the original requirement on the lease, that the land will only be used for \"recreational purposes\", as stipulated by the Bute family. But the Arms Park site is a prime piece of real estate in the centre of Cardiff, which means that it may be difficult to sell the land to property developers. The estimated value of the whole Arms Park site could be at least £25 million, although with the \"recreational use\" requirement, its actual value could be a lot less than that figure. A decision by Cardiff Athletic Club on the future of the Arms Park has yet to be made. In 2011, the Cardiff Blues regional rugby union team made a £6 million bid for the Arms Park, later the WRU made an increased bid of £10 million for the site. Both bids were rejected by the trustees of the Cardiff Athletic Club. However, in 2012 Cardiff Blues announced that they would be making a permanent return to Cardiff Arms Park following declining attendances at the Cardiff City Stadium. In the 2013 off-season, the pitch at the Arms Park was replaced with an artificial FieldTurf surface, intended to prevent any adverse weather conditions from affecting the rugby.\n\nCardiff Arms Park is best known as a rugby union stadium, but Cardiff Athletic Bowls Club (CABC) was established in 1923, and ever since then, the club has used the Arms Park as its bowling green. The bowls club is a section of the Cardiff Athletic Club and shares many of the facilities of the Cardiff Arms Park athletics centre.\nThe Les Spence Memorial Gates were erected in memory of the former Cardiff RFU player, who captained the team in 1936-37. He was born in 1907 and became chairman of the Cardiff RFU and president of the WRU between 1973 and 1974. He was awarded an MBE and died in 1988.\n\nThe Club has produced two Welsh international bowlers; Mr. C Standfast in 1937 and Mr. B Hawkins who represented Wales in the 1982 World Pairs and captained Wales in 1982 and 1984.\n\nThe Riverside Football Club, founded in 1899, played some matches at the Arms Park until 1910, when they moved to Ninian Park, and later became Cardiff City Football Club.\n\nOn 31 May 1989, Wales played its first international game against West Germany at the National Stadium in a World Cup qualifying match, which ended goalless. It was also the first ever international football match held in Great Britain that was watched by all-seater spectators.\n\nThe adjoining Cardiff Rugby Club ground has also been used for Association Football. In July 1995, Ton Pentre played two Intertoto Cup games there, against Heerenveen (Netherlands) and Uniao Leiria (Portugal) as their own ground was not suitable. The Heerenveen game - the first ever soccer match to be played there - kicked off at 6pm on Saturday 1 July 1995 and resulted in the Dutch side winning 7-0. The Wales U-21 team have also played a home game there in the late 1990s.\n\nIn 1958, the British Empire and Commonwealth Games were held in Cardiff. The event was (to date) the biggest sporting event ever held in Wales; however, it would not have been possible without the financial support given by the WRU and the Cardiff Athletic Club. Both the opening and closing ceremonies took place at Cardiff Arms Park, plus all the track and field events, on what had been the greyhound track. It would turn out to be the last time that South Africa would participate in the Games until 1994. South Africa withdrew from the Commonwealth Games in 1961.\n\nBaseball was established early on in Cardiff, and one of the earliest of games to be held at the Arms Park was on 18 May 1918. It was a charity match in aid of the Prisoner of War Fund between Welsh and American teams of the U.S. Beaufort & U.S. Jupiter. British baseball matches have also regularly taken place at the Arms Park and hosted the annual England versus Wales international game every four years. The games are now usually held at Roath Park.\n\nThe first boxing contest held at the Arms Park was on 24 January 1914, when Bombadier Billy Wells beat Gaston Pigot by a knockout in the first round of a 20-round contest. Boxing contests were held later on 14 June 1943, 12 August 1944, 4 October 1951 and 10 September 1952.\n\nAround 25,000 spectators watched international boxing on 1 October 1993, at the National Stadium with a World Boxing Council (WBC) Heavyweight title bout between Lennox Lewis and Frank Bruno. It was the first time that two British-born boxers had fought for the world heavyweight title. Lewis beat Bruno by a technical knockout in the 7th round, in what was called the \"Battle of Britain\". On 30 September 1995, Steve Robinson the World Boxing Organization (WBO) World Featherweight Champion, lost against Prince Naseem Hamed at the rugby ground in 8 rounds.\n\nIn 1819 Cardiff Cricket Club was formed and by 1848 they had moved to their new home at the Arms Park. Glamorgan County Cricket Club, at the time not a first-class county, played their first match at the ground in June 1869 against Monmouthshire Cricket Club. The county club played their first County Championship match on the ground in 1921, competing there every season (except while first-class cricket was suspended during the Second World War) until their final match on the ground against Somerset in August 1966.\n\nCardiff Cricket Club played their final game at the ground against Lydney Cricket Club on 17 September 1966. Both Cardiff Cricket Club and Glamorgan then moved to a new ground at Sophia Gardens on the opposite bank of the River Taff to the Arms Park following work on the creation of the national rugby stadium.\n\nThe first first-class cricket match to be held on the ground was between West of England and East of England, on 20 June 1910. In all more than 240 first-class matches were played on the ground, all but two involving Glamorgan as the home team. Only one List A cricket match was played on the ground, Glamorgan's Gillette Cup fixture against Somerset on 22 May 1963.\n\nGreyhound racing took place at the Arms Park for fifty years from 1927 until 1977.\n\nIn 1876, the Cardiff RFC was formed and soon after they also used the park. On 12 April 1884, the first international match was played at the ground between Wales and Ireland, when 5,000 people watched Wales beat Ireland by two tries and a drop goal to nil.\n\nThe Arms Park rugby ground became the permanent home of the Wales national rugby union team in 1964. Later, the National Stadium was also home to the WRU Challenge Cup from 1972 until the match held at the Stadium on 26 April 1997, at a much reduced capacity, between Cardiff RFC and Swansea RFC. Cardiff RFC won the match 33–26.\n\nThe National Stadium is best known as the venue for what is considered to be \"the greatest try ever scored\" by Gareth Edwards for the Barbarians against New Zealand in what is also called \"the greatest match ever played\" on 27 January 1973. The final result was a win for the Barbarians. The score, 23–11, which translates to 27–13 in today's scoring system.\n\nThe scorers were:\n\nBarbarians: Tries: Gareth Edwards, Fergus Slattery, John Bevan, J P R Williams; Conversions: Phil Bennett (2); Penalty: Phil Bennett.\n\nAll Blacks: Tries: Grant Batty (2); Penalty: Joseph Karam.\n\nThe National Stadium hosted four games in the 1991 Rugby World Cup, including the third-place play-off. The National Stadium was also host to the inaugural Heineken Cup final of 1995–96 when Toulouse beat Cardiff RFC by 21–18 after extra time, in front of 21,800 spectators. The following final in 1996–97 was also held at the National Stadium, this time it was between Brive and Leicester Tigers. Brive won the match 28–9, in front of a crowd of 41,664.\n\nIn 2008, the rugby ground hosted all the games in Pool A of the 2008 IRB Junior World Championship and also the semi-final on 18 June 2008, in which England beat South Africa 26–18.\nUntil February 2012, it had been assumed that the last professional rugby union game to take place at the Arms Park was on 17 May 2009, when Edinburgh beat the Cardiff Blues 36–14 in a Celtic League match during the 2008–09 season.\n\nHowever, on Tuesday, 7 February 2012, it was confirmed that Cardiff Blues would face Connacht at the Arms Park on Friday, 10 February 2012. The Pro12 League game result was a win for the Cardiff Blues 22–15 and attendance of 8,000. The following Tuesday, it was announced that the match against Ulster on Friday, 17 February, would also be at the Arms Park, resulting in a Blues win, 21–14 and attendance of 8,600. The agreement signed during 2009 tied Cardiff Blues to a 20-year contract to play a maximum of 18 games per season for a set fee, rather than per match at Cardiff City Stadium. But on 23 February, it was announced that the two Welsh 'derbies' against the Scarlets and the Ospreys would be played at Cardiff City Stadium, rather than the Arms Park, because of Cardiff Blues' anticipation that the attendance figures would far exceed the maximum capacity of 9,000. On 8 May 2012, it was announced that Cardiff Blues would be returning to the Arms Park on a permanent basis after just three years at the Cardiff City Stadium.\n\nOn 23 May 2014, the rugby ground hosted the final of the 2013–14 Amlin Challenge Cup in which Northampton Saints beat Bath 30-16.\n\nCardiff Arms Park hosted matches of the 1991 Rugby World Cup.\n\nSouth Wales Scorpions played a Rugby League Championship 1 match against London Skolars at Cardiff Arms Park on Sunday, 27 July 2014 and on Sunday 10 May 2015 at Cardiff Arms Park, South Wales Scorpions took on North Wales Crusaders. The 2015 European Cup match between France and Wales was held at Cardiff Arms Park on Friday, 30 October 2015 at 18:30 GMT.\n\nThe highest attendance for a rugby league game at the Arms Park was recorded on 8 June 1996 during the first Super League season when 6,708 saw St. Helens defeat the Sheffield Eagles 43-32. The St Helens team at the time contained Welsh players Anthony Sullivan, Karle Hammond and Keiron Cunningham.\n\nList of rugby league test matches played at Cardiff Arms Park.\nTennis courts were laid out in the Arms Park for Cardiff Tennis Club until the club moved to Sophia Gardens in 1967. In 2003, the club amalgamated with Lisvane Tennis Club to form Lisvane (CAC) Tennis Club, which is still a section of Cardiff Athletic Club (CAC).\n\nMajor music concerts were also held at the National Stadium from 1987 until 1996, they included Tina Turner, U2, Michael Jackson, The Rolling Stones, Dire Straits, Bon Jovi and R.E.M. The last music concert was held on 14 July 1996. Jehovah's Witnesses held their annual conventions at the National Stadium.\n\nThe National Stadium was known primarily as the venue for massed voices singing such hymns as \"Cwm Rhondda\", \"Calon Lân\", \"Men of Harlech\" and \"Hen Wlad Fy Nhadau\" (\"Land of my Fathers\" – the national anthem of Wales). The legendary atmosphere including singing of the crowd was said to be worth at least a try or a goal to the home nation. This tradition of singing has now passed on to the Millennium Stadium.\n\nThe Arms Park has its own choir, called the Cardiff Arms Park Male Choir. It was formed in 1966 as the Cardiff Athletic Club Male Voice Choir, and today performs internationally with a schedule of concerts and tours. In 2000, the choir changed their name to become the Cardiff Arms Park Male Choir.\n\n\n\n"}
{"id": "19535", "url": "https://en.wikipedia.org/wiki?curid=19535", "title": "Mikhail Kalashnikov", "text": "Mikhail Kalashnikov\n\nLieutenant-General Mikhail Timofeyevich Kalashnikov (; 10 November 1919 – 23 December 2013) was a Russian general, inventor, military engineer, writer and small arms designer. He is most famous for developing the AK-47 assault rifle and its improvements, the AKM and AK-74, as well as the PK machine gun and RPK light machine gun.\n\nKalashnikov was, according to himself, a self-taught tinkerer who combined innate mechanical skills with the study of weaponry to design arms that achieved battlefield ubiquity. Even though Kalashnikov felt sorrow at the weapons' uncontrolled distribution, he took pride in his inventions and in their reputation for reliability, emphasizing that his rifle is \"a weapon of defense\" and \"not a weapon for offense\".\n\nKalashnikov was born in Kurya, Altai Governorate, Russian SFSR, now Altai Krai, Russia, as the seventeenth child of the 19 children of Aleksandra Frolovna Kalashnikova (née Kaverina) and Timofey Aleksandrovich Kalashnikov, who were peasants. In 1930, his father and most of his family were deprived of property and deported to the village of Nizhnyaya Mokhovaya, Tomsk Oblast. In his youth, Mikhail suffered from various illnesses and was on the verge of death at age six. He was attracted to all kinds of machinery, but also wrote poetry, dreaming of becoming a poet. He went on to write six books and continued to write poetry all of his life. Kalashnikov's parents were peasants, but, after deportation to Tomsk Oblast, had to combine farming with hunting, and thus Mikhail frequently used his father's rifle in his teens. Kalashnikov continued hunting into his 90s.\n\nAfter completing seventh grade, Mikhail, with his stepfather's permission, left his family and returned to Kurya, hitchhiking for nearly 1,000 km. In Kurya he found a job in mechanics at a tractor station and developed a passion for weaponry. In 1938, he was conscripted into the Red Army. Because of his small size and engineering skills he was assigned as a tank mechanic, and later became a tank commander. While training, he made his first inventions, which concerned not only tanks, but also small weapons, and was personally awarded a wrist watch by Georgy Zhukov. Kalashnikov served on the T-34s of the 24th Tank Regiment, 108th Tank Division stationed in Stryi before the regiment retreated after the Battle of Brody in June 1941. He was wounded in combat in the Battle of Bryansk in October 1941 and hospitalised until April 1942. In the last few months of being in hospital, he overheard some fellow soldiers complaining about the Soviet rifles at the time and this is when he came up with the idea of making a new rifle which later became the AK47.\nSeeing the drawbacks of the standard infantry weapons at the time, he decided to construct a new rifle for the Soviet military. During this time Kalashnikov began designing a submachine gun. Although his first submachine gun design was not accepted into service, his talent as a designer was noticed. From 1942 onwards Kalashnikov was assigned to the Central Scientific-developmental Firing Range for Rifle Firearms of the Chief Artillery Directorate of the Red Army.\n\nIn 1944, he designed a gas-operated carbine for the new 7.62×39mm cartridge. This weapon, influenced by the M1 Garand rifle, lost out to the new Simonov carbine which would be eventually adopted as the SKS; but it became a basis for his entry in an assault rifle competition in 1946.\nHis winning entry, the \"Mikhtim\" (so named by taking the first letters of his name and patronymic, Mikhail Timofeyevich) became the prototype for the development of a family of prototype rifles.. The submachine bore an uncanny resemblance to the German WWII-era StG 44 This process culminated in 1947, when he designed the AK-47 (standing for \"Avtomat Kalashnikova model 1947\"). In 1949, the AK-47 became the standard issue assault rifle of the Soviet Army and went on to become Kalashnikov's most famous invention. While developing his first assault rifles, Kalashnikov competed with two much more experienced weapon designers, Vasily Degtyaryov and Georgy Shpagin, who both accepted the superiority of the AK-47. Kalashnikov named Alexandr Zaitsev and Vladimir Deikin as his major collaborators during those years.\n\nFrom 1949, Mikhail Kalashnikov lived and worked in Izhevsk, Udmurtia. He held a degree of Doctor of Technical Sciences (1971) and was a member of 16 academies.\n\nOver the course of his career, he evolved the basic design into a weapons family. The AKM () first appeared in 1963, was lighter and cheaper to manufacture owing to the use of a stamped steel receiver (in place of the AK-47's milled steel receiver), and contained detail improvements such as a re-shaped stock and muzzle compensator. From the AKM he developed a squad automatic weapon variant, known as the RPK ().\n\nHe also developed the general-purpose PK machine gun (), which used the more powerful 7.62×54R cartridge of the Mosin–Nagant rifle. It is cartridge belt-fed, not magazine-fed, as it is intended to provide heavy sustained fire from a tripod mount, or be used as a light, bipod-mounted weapon. The common characteristics of all these weapons are simple design, ruggedness and ease of maintenance in all operating conditions.\n\nApproximately 100 million AK-47 assault rifles had been produced by 2009, and about half of them are counterfeit, manufactured at a rate of about a million per year. Izhmash, the official manufacturer of AK-47 in Russia, did not patent the weapon until 1997, and in 2006 accounted for only 10% of the world's production. This arm became famous due to its reliability in the most extreme climatic conditions, functioning perfectly in the desert as in the tundra. It is in official use by the militaries of 55 nations, and has been so influential in military struggle that it has been used on national flags. Prominent examples include the flags of Mozambique and Hezbollah, as well as the East Timorese and Zimbabwean coats of arms.\n\nKalashnikov himself claimed he was always motivated by service to his country rather than money, and made no direct profit from weapon production. He did however own 30% of a German company Marken Marketing International (MMI) run by his grandson Igor. The company revamps trademarks and produces merchandise carrying the Kalashnikov name, such as vodka, umbrellas and knives. One of the items is a knife named for the AK-74.\n\nDuring a visit to the United States in the early 2000s, Kalashnikov was invited to tour a Virginia holding site for the forthcoming American Wartime Museum. The former tanker Kalashnikov became visibly moved at the sight of his old tank in action, painted with his name in Cyrillic.\n\nAfter a prolonged illness Kalashnikov was hospitalized on 17 November 2013, in an Udmurtian medical facility in Izhevsk, the capital of Udmurtia and where he lived. He died 23 December 2013, at age 94 from gastric hemorrhage. In January 2014 a letter that Kalashnikov wrote six months before his death to the leader of the Russian Orthodox Church, Patriarch Kirill, was published by the Russian daily newspaper \"Izvestia\". In the letter he stated that he was suffering \"spiritual pain\" about whether he was responsible for the deaths caused by the weapons he created. Translated from the published letter he states, \"I keep having the same unsolved question: if my rifle claimed people's lives, then can it be that I... a Christian and an Orthodox believer, was to blame for their deaths?\"\n\nThe patriarch wrote back, thanked Kalashnikov, and said that he \"was an example of patriotism and a correct attitude toward the country\". Kirill added about the design responsibility for the deaths by the rifle, \"the church has a well-defined position when the weapon is defense of the Motherland, the Church supports its creators and the military, which use it.\"\n\nHe became one of the first people buried in the Federal Military Memorial Cemetery.\n\nKalashnikov's father, Timofey Aleksandrovich Kalashnikov (1883–1930), was a peasant. He completed two grades of parochial school and could read and write. In 1901 he married Aleksandra Frolovna Kaverina (1884–1957), who was illiterate throughout her life. They had 19 children, but only eight survived to adult age; Kalashnikov was born 17th, and was close to death at age six.\n\nIn 1930, the government labeled Timofey Aleksandrovich a kulak, confiscated his property, and deported him to Siberia, along with most of the family. The eldest three siblings, daughters Agasha (b. 1905) and Anna and son Victor, were already married by 1930, and remained in Kuriya. After her husband's death in 1930, Aleksandra Frolovna married Efrem Kosach, a widower who had three children of his own.\n\nMikhail Kalashnikov married twice, the first time to Ekaterina Danilovna Astakhova of Altai Krai. He married the second time to Ekaterina Viktorovna Moiseyeva (1921–1977). She was an engineer and did much technical drawing work for her husband. They had four children: daughters Nelli (b. 1942), Elena (b. 1948) and Natalya (1953–1983), and a son Victor (b. 1942). Victor also became a prominent small arms designer.\n\nThe title to the AK-47 trademark belonged to Mikhail Kalashnikov's family until 4 April 2016, when the Kalashnikov Concern won a lawsuit to invalidate the registration of the trademark.\n\nDuring his career, Kalashnikov designed about 150 models of small weapons. The most famous of them are:\n\n\"Incorporates information from the corresponding article in the Russian Wikipedia\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19537", "url": "https://en.wikipedia.org/wiki?curid=19537", "title": "MUD", "text": "MUD\n\nA MUD (; originally Multi-User Dungeon, with later variants Multi-User Dimension and Multi-User Domain) is a multiplayer real-time virtual world, usually text-based. MUDs combine elements of role-playing games, hack and slash, player versus player, interactive fiction, and online chat. Players can read or view descriptions of rooms, objects, other players, non-player characters, and actions performed in the virtual world. Players typically interact with each other and the world by typing commands that resemble a natural language.\n\nTraditional MUDs implement a role-playing video game set in a fantasy world populated by fictional races and monsters, with players choosing classes in order to gain specific skills or powers. The objective of this sort of game is to slay monsters, explore a fantasy world, complete quests, go on adventures, create a story by roleplaying, and advance the created character. Many MUDs were fashioned around the dice-rolling rules of the \"Dungeons & Dragons\" series of games.\n\nSuch fantasy settings for MUDs are common, while many others have science fiction settings or are based on popular books, movies, animations, periods of history, worlds populated by anthropomorphic animals, and so on. Not all MUDs are games; some are designed for educational purposes, while others are purely chat environments, and the flexible nature of many MUD servers leads to their occasional use in areas ranging from computer science research to geoinformatics to medical informatics to analytical chemistry. MUDs have attracted the interest of academic scholars from many fields, including communications, sociology, law, and economics. At one time, there was interest from the United States military in using them for teleconferencing.\n\nMost MUDs are run as hobbies and are free to players; some may accept donations or allow players to purchase virtual items, while others charge a monthly subscription fee. MUDs can be accessed via standard telnet clients, or specialized MUD clients which are designed to improve the user experience. Numerous games are listed at various web portals, such as The Mud Connector.\n\nThe history of modern massively multiplayer online role-playing games (MMORPGs) like \"EverQuest\" and \"Ultima Online\", and related virtual world genres such as the social virtual worlds exemplified by \"Second Life\", can be traced directly back to the MUD genre. Indeed, before the invention of the term MMORPG, games of this style were simply called graphical MUDs. A number of influential MMORPG designers began as and/or players (such as Raph Koster, Brad McQuaid, Matt Firor, and Brian Green) or were involved with early MUDs (like Mark Jacobs and J. Todd Coleman).\n\n\"Colossal Cave Adventure\", created in 1975 by Will Crowther on a DEC PDP-10 computer, was the first widely used adventure game. The game was significantly expanded in 1976 by Don Woods. Also called \"Adventure\", it contained many D&D features and references, including a computer controlled dungeon master.\n\nNumerous dungeon crawlers were created on the PLATO system at the University of Illinois and other American universities that used PLATO, beginning in 1975. Among them were \"pedit5\", \"oubliette\", \"moria\", \"avathar\", \"krozair\", \"dungeon\", \"dnd\", \"crypt\", and \"drygulch\". By 1978-79, these games were heavily in use on various PLATO systems, and exhibited a marked increase in sophistication in terms of 3D graphics, storytelling, user involvement, team play, and depth of objects and monsters in the dungeons.\n\nInspired by \"Adventure\", a group of students at MIT in the summer of 1977 wrote a game for the PDP-10 minicomputer; called \"Zork\", it became quite popular on the ARPANET. \"Zork\" was ported, under the filename DUNGEN (\"dungeon\"), to FORTRAN by a programmer working at DEC in 1978.\n\nIn 1978 Roy Trubshaw, a student at the University of Essex in the UK, started working on a multi-user adventure game in the MACRO-10 assembly language for a DEC PDP-10. He named the game \"MUD\" (\"Multi-User Dungeon\"), in tribute to the \"Dungeon\" variant of \"Zork\", which Trubshaw had greatly enjoyed playing. Trubshaw converted MUD to BCPL (the predecessor of C), before handing over development to Richard Bartle, a fellow student at the University of Essex, in 1980. The game revolved around gaining points till one achieved the Wizard rank, giving the character immortality and special powers over mortals.\n\n\"MUD\", better known as \"Essex MUD\" and \"MUD1\" in later years, ran on the University of Essex network, and became more widely accessible when a guest account was set up that allowed users on JANET (a British academic X.25 computer network) to connect on weekends and between the hours of 2 AM and 8 AM on weekdays. It became the first Internet multiplayer online role-playing game in 1980, when the university connected its internal network to ARPANet.\n\nThe original \"MUD\" game was closed down in late 1987, reportedly under pressure from CompuServe, to whom Richard Bartle had licensed the game. This left \"MIST\", a derivative of \"MUD1\" with similar gameplay, as the only remaining MUD running on the University of Essex network, becoming one of the first of its kind to attain broad popularity. \"MIST\" ran until the machine that hosted it, a PDP-10, was superseded in early 1991.\n\n1985 saw the origin of a number of projects inspired by the original \"MUD\". These included \"Gods\" by Ben Laurie, a \"MUD1\" clone that included online creation in its endgame, and which became a commercial MUD in 1988; and \"MirrorWorld\", a tolkienesque MUD started by Pip Cordrey who gathered some people on a BBS he ran to create a \"MUD1\" clone that would run on a home computer.\n\nNeil Newell, an avid \"MUD1\" player, started programming his own MUD called \"SHADES\" during Christmas 1985, because \"MUD1\" was closed down during the holidays. Starting out as a hobby, \"SHADES\" became accessible in the UK as a commercial MUD via British Telecom's Prestel and Micronet networks. A scandal on \"SHADES\" led to the closure of Micronet, as described in Indra Sinha's net-memoir, \"The Cybergypsies\".\n\nAt the same time, Compunet started a project named \"Multi-User Galaxy Game\" as a Science Fiction alternative to \"MUD1\", a copy of which they were running on their system at the time. When one of the two programmers left CompuNet, the remaining programmer, Alan Lenton, decided to rewrite the game from scratch and named it Federation II (at the time no Federation I existed). The MUD was officially launched in 1989. Federation II was later picked up by AOL, where it became known simply as \"Federation: Adult Space Fantasy\". Federation later left AOL to run on its own after AOL began offering unlimited service.\n\nIn 1978, around the same time Roy Trubshaw wrote \"MUD\", Alan E. Klietz wrote a game called \"Milieu\" using Multi-Pascal on a CDC Cyber 6600 series mainframe which was operated by the Minnesota Educational Computing Consortium. Klietz ported \"Milieu\" to an IBM XT in 1983, naming the new port \"Scepter of Goth\". \"Scepter\" supported 10 to 16 simultaneous users, typically connecting in by modem. It was one of the first commercial MUDs; franchises were sold to a number of locations. \"Scepter\" was first owned and run by GamBit (of Minneapolis, Minnesota), founded by Bob Alberti. GamBit's assets were later sold to Interplay Productions. Interplay eventually went bankrupt.\n\nIn 1984, Mark Peterson wrote \"The Realm of Angmar\", beginning as a clone of \"Scepter of Goth\". In 1994, Peterson rewrote \"The Realm of Angmar\", adapting it to MS-DOS (the basis for many dial-in BBS systems), and renamed it \"Swords of Chaos\". For a few years this was a very popular form of MUD, hosted on a number of BBS systems, until widespread Internet access eliminated most BBSes. \n\nIn 1984, Mark Jacobs created and deployed a commercial gaming site, \"Gamers World\". The site featured two games coded and designed by Jacobs, a MUD called \"Aradath\" (which was later renamed, upgraded and ported to \"GEnie\" as \"Dragon's Gate\") and a 4X science-fiction game called \"Galaxy\", which was also ported to \"GEnie\". At its peak, the site had about 100 monthly subscribers to both \"Aradath\" and \"Galaxy\". GEnie was shut down in the late 1990s, although \"Dragon's Gate\" was later brought to \"America Online\" before it was finally released on its own. Dragon's Gate was closed on February 10, 2007.\n\nIn the summer of 1980 University of Virginia classmates John Taylor and Kelton Flinn wrote \"Dungeons of Kesmai\", a six player game inspired by \"Dungeons & Dragons\" which used Roguelike ASCII graphics. They founded the Kesmai company in 1982 and in 1985 an enhanced version of \"Dungeons of Kesmai\", \"Island of Kesmai\", was launched on CompuServe. Later, its 2-D graphical descendant \"Legends of Kesmai\" was launched on AOL in 1996. The games were retired commercially in 2000.\n\nThe popularity of MUDs of the University of Essex tradition escalated in the USA during the late 1980s when affordable personal computers with 300 to 2400 bit/s modems enabled role-players to log into multi-line Bulletin Board Systems and online service providers such as CompuServe. During this time it was sometimes said that MUD stands for \"Multi Undergraduate Destroyer\" due to their popularity among college students and the amount of time devoted to them.\n\n\"\" was published by Yehuda Simmons in 1989. It was the first persistent game world of its kind without the traditional hourly resets and points-based puzzle solving progression systems. Avalon introduced equilibrium and balance (cooldowns), skill-based player vs player combat and concepts such as player-run governments and player housing.\n\nThe first popular MUD codebase was AberMUD, written in 1987 by Alan Cox, named after the University of Wales, Aberystwyth. Alan Cox had played the original University of Essex MUD, and the gameplay was heavily influenced by it. AberMUD was initially written in B for a Honeywell L66 mainframe under GCOS3/TSS. In late 1988 it was ported to C, which enabled it to spread rapidly to many Unix platforms upon its release in 1989. AberMUD's popularity resulted in several inspired works, the most notable of which were TinyMUD, LPMud, and DikuMUD.\n\n\"Monster\" was a multi-user adventure game created by Richard Skrenta for the VAX and written in VMS Pascal. It was publicly released in November 1988. \"Monster\" was disk-based and modifications to the game were immediate. \"Monster\" pioneered the approach of allowing players to build the game world, setting new puzzles or creating dungeons for other players to explore. Monster, which comprised about 60,000 lines of code, had a lot of features which appeared to be designed to allow \"Colossal Cave Adventure\" to work in it. Though there never were many network-accessible Monster servers, it inspired James Aspnes to create a stripped down version of \"Monster\" which he called TinyMUD.\n\nTinyMUD, written in C and released in late 1989, spawned a number of descendants, including TinyMUCK and TinyMUSH. TinyMUCK version 2 contained a full programming language named MUF (Multi-User Forth), while MUSH greatly expanded the command interface. To distance itself from the combat-oriented traditional MUDs it was said that the \"D\" in TinyMUD stood for Multi-User \"Domain\" or \"Dimension\"; this, along with the eventual popularity of acronyms other than MUD (such as MUCK, MUSH, MUSE, and so on) for this kind of server, led to the eventual adoption of the term MU* to refer to the TinyMUD family. UberMUD, UnterMUD, and MOO were inspired by TinyMUD but are not direct descendants.\n\nThe first version of Hourglass was written by Yehuda Simmons and later Daniel James for which debuted in 1989 at the last of the London MUD mega Meets aptly named 'Adventure '89' and initially hosted on the IOWA system. Initially written in ARM Assembler on the Acorn Archimedes 440, in 1994 it made the leap from the venerable Archimedes to Debian Linux on the PC and later Red Hat where other than shifting to Ubuntu (operating system) it has remained ever since. An early version of Hourglass was also ported to the PC named Vortex by Ben Maizels in 1992.\n\nAlthough written specifically for Avalon: The Legend Lives it went on to spawn a number of games including Avalon: The First Age which ran from 1999-2014. The now defunct 1996 Age of Thrones and notably Achaea, Dreams of Divine Lands started life in Vortex prior to moving to its own Rapture engine. Hourglass continues to be developed as of 2016 and Avalon: The Legend Lives currently has 2901325 written words and 2248374 lines of game-code (with 2417900 instructions). The original game coming in at 1k in 1989 compared to 102gb in January 2016.\n\nIn 1989 LPMud was developed by Lars Pensjö (hence the LP in LPMud). Pensjö had been an avid player of TinyMUD and AberMUD and wanted to create a world with the flexibility of TinyMUD and the gameplay of AberMUD. In order to accomplish this he wrote what is nowadays known as a virtual machine, which he called the LPMud driver, that ran the C-like LPC programming language used to create the game world. Pensjö's interest in LPMud eventually waned and development was carried on by others such as Jörn \"Amylaar\" Rennecke, Felix \"Dworkin\" Croes, Tim \"Beek\" Hollebeek and Lars Düning. During the early 1990s, LPMud was one of the most popular MUD codebases. Descendants of the original LPMud include MudOS, DGD, SWLPC, FluffOS, and the Pike programming language, the latter the work of long-time LPMud developer Fredrik \"Profezzorn\" Hübinette.\n\nIn 1990, the release of DikuMUD, which was inspired by AberMUD, led to a virtual explosion of hack and slash MUDs based upon its code. DikuMUD inspired numerous derivative codebases, including CircleMUD, Merc, ROM, SMAUG, and GodWars. The original Diku team comprised Sebastian Hammer, Tom Madsen, Katja Nyboe, Michael Seifert, and Hans Henrik Staerfeldt. DikuMUD had a key influence on the early evolution of the MMORPG genre, with \"EverQuest\" (created by avid DikuMUD player Brad McQuaid) displaying such Diku-like gameplay that Verant developers were made to issue a sworn statement that no actual DikuMUD code was incorporated.\n\nIn 1987 David Whatley, having previously played \"Scepter of Goth\" and \"Island of Kesmai\", founded Simutronics with Tom and Susan Zelinski. In the same year they demonstrated a prototype of \"GemStone\" to GEnie. After a short-lived instance of \"GemStone II\", \"GemStone III\" was officially launched in February 1990. \"GemStone III\" became available on AOL in September 1995, followed by the release of \"DragonRealms\" in February 1996. By the end of 1997 \"GemStone III\" and \"DragonRealms\" had become the first and second most played games on AOL.\n\nThe typical MUD will describe to you the room or area you are standing in, listing the objects, players and non-player characters (NPCs) in the area, as well as all of the exits. To carry out a task the player would enter a text command such as take apple or attack dragon. Movement around the game environment is generally accomplished by entering the direction (or an abbreviation of it) in which the player wishes to move, for example typing north or just n would cause the player to exit the current area via the path to the north.\n\nMUD clients often contain functions which make certain tasks within a MUD easier to carry out, for example commands buttons which you can click in order to move in a particular direction or to pick up an item. There are also tools available which add hotkey-activated macros to telnet and MUD clients giving the player the ability to move around the MUD using the arrow keys on their keyboard for example.\n\nWhile there have been many variations in overall focus, gameplay and features in MUDs, some distinct sub-groups have formed that can be used to help categorize different game mechanics, game genres and non-game uses.\n\nPerhaps the most common approach to game design in MUDs is to loosely emulate the structure of a \"Dungeons & Dragons\" campaign focused more on fighting and advancement than role-playing. When these MUDs restrict player-killing in favor of player versus environment conflict and questing, they are labeled Hack and Slash MUDs. This may be considered particularly appropriate since, due to the room-based nature of traditional MUDs, ranged combat is typically difficult to implement, resulting in most MUDs equipping characters mainly with close-combat weapons. This style of game was also historically referred to within the MUD genre as \"adventure games\", but video gaming as a whole has developed a meaning of \"adventure game\" that is greatly at odds with this usage.\n\nMost MUDs restrict player versus player combat, often abbreviated as PK (Player Killing). This is accomplished through hard coded restrictions and various forms of social intervention. MUDs without these restrictions are commonly known as PK MUDs. Taking this a step further are MUDs devoted \"solely\" to this sort of conflict, called pure PK MUDs, the first of which was \"Genocide\" in 1992. \"Genocide\" ideas were influential in the evolution of player versus player online gaming.\n\nRoleplaying MUDs, generally abbreviated as RP MUDs, encourage or enforce that players act out the role of their playing characters at all times. Some RP MUDs provide an immersive gaming environment, while others only provide a virtual world with no game elements. MUDs where roleplay is enforced and the game world is heavily computer-modeled are sometimes known as Roleplay Intensive MUDs, or RPIMUDs. In many cases, Role-Playing muds attempt to differentiate themselves from hack and slash types, by dropping the \"MUD\" name entirely, and instead using MUX (Multi User Experience) or MUSH (Multi User Shared Hallucination.)\n\nSocial MUDs de-emphasize game elements in favor of an environment designed primarily for socializing. They are differentiated from talkers by retaining elements beyond online chat, typically online creation as a community activity and some element of role-playing. Often such MUDs have broadly defined contingents of socializers and roleplayers. Server software in the TinyMUD family, or MU*, is traditionally used to implement social MUDs.\n\nA less-known MUD variant is the talker, a variety of online chat environment typically based on server software like ew-too or NUTS. Most of the early Internet talkers were LPMuds with the majority of the complex game machinery stripped away, leaving just the communication commands. The first Internet talker was \"Cat Chat\" in 1990.\n\nTaking advantage of the flexibility of MUD server software, some MUDs are designed for educational purposes rather than gaming or chat. \"MicroMUSE\" is considered by some to have been the first educational MUD, but it can be argued that its evolution into this role was not complete until 1994, which would make the first of many educational MOOs, \"Diversity University\" in 1993, also the first educational MUD. The MUD medium lends itself naturally to constructionist learning pedagogical approaches. The Mud Institute (TMI) was an LPMud opened in February 1992 as a gathering place for people interested in developing LPMud and teaching LPC after it became clear that Lars Pensjö had lost interest in the project. TMI focussed on both the LPMud driver and library, the driver evolving into MudOS, the TMI Mudlib was never officially released, but was influential in the development of other libraries.\n\nA graphical MUD is a MUD that uses computer graphics to represent parts of the virtual world and its visitors. A prominent early graphical MUD was \"Habitat\", written by Randy Farmer and Chip Morningstar for Lucasfilm in 1985. Graphical MUDs require players to download a special client and the game's artwork. They range from simply enhancing the user interface to simulating 3D worlds with visual spatial relationships and customized avatar appearances.\n\nGames such as \"Meridian 59\", \"EverQuest\", \"Ultima Online\" and \"Dark Age of Camelot\" were routinely called graphical MUDs in their earlier years. \"RuneScape\" was actually originally intended to be a \"text-based\" MUD, but graphics were added very early in development. However, with the increase in computing power and Internet connectivity during the late nineties, and the shift of online gaming to the mass market, the term \"graphical MUD\" fell out of favor, being replaced by MMORPG, Massively Multiplayer Online Role-Playing Game, a term coined by Richard Garriott in 1997.\n\nWithin a MUD's technical infrastructure, a mudlib (concatenation of \"MUD library\") defines the rules of the in-game world. Examples of mudlibs include Ain Soph Mudlib, CDlib, Discworld Mudlib, Lima Mudlib, LPUniversity Mudlib, MorgenGrauen Mudlib, Nightmare Mudlib, and TMI Mudlib.\n\nMUD history has been preserved primarily through community sites and blogs and not through mainstream sources with journalistic repute. As of the late 1990s, a website called The Mud Connector has served as a central and curated repository for active MUDs. In 1995, \"The Independent\" reported that over 60,000 people regularly played about 600 MUDs, up from 170 MUDs three years prior. \"The Independent\" also noted distinct patterns of socialization within MUD communities. Seraphina Brennan of \"Massively\" wrote that the MUD community was \"in decline\" as of 2009.\n\nSherry Turkle developed a theory that the constant use (and in many cases, overuse) of MUDs allows users to develop different personalities in their environments. She uses examples, dating back to the text-based MUDs of the mid-1990s, showing college students who simultaneously live different lives through characters in separate MUDs, up to three at a time, all while doing schoolwork. The students claimed that it was a way to \"shut off\" their own lives for a while and become part of another reality. Turkle claims that this could present a psychological problem of identity for today's youths.\n\n\"A Story About A Tree\" is a short essay written by Raph Koster regarding the death of a \"LegendMUD\" player named Karyn, raising the subject of inter-human relationships in virtual worlds.\n\nObservations of MUD-play show styles of play that can be roughly categorized. Achievers focus on concrete measurements of success such as experience points, levels, and wealth; Explorers investigate every nook and cranny of the game, and evaluate different game mechanical options; Socializers devote most of their energy to interacting with other players; and then there are Killers who focus on interacting negatively with other players, if permitted, killing the other characters or otherwise thwarting their play. Few players play only one way, or play one way all the time; most exhibit a diverse style. According to Richard Bartle, \"People go there as part of a hero's journey—a means of self-discovery\".\n\nResearch has suggested that various factors combine in MUDs to provide users with a sense of \"presence\" rather than simply communication.\n\nAs a noun, the word MUD is variously written MUD, Mud, and mud, depending on speaker and context. It is also used as a verb, with to mud meaning to play or interact with a MUD and mudding referring to the act of doing so. A mudder is, naturally, one who MUDs. Compound words and portmanteaux such as mudlist, mudsex, and mudflation are also regularly coined. Puns on the \"wet dirt\" meaning of \"mud\" are endemic, as with, for example, the names of the ROM (Rivers of MUD), MUCK, MUSH, and CoffeeMUD codebases and the MUD \"Muddy Waters\".\n\n\n\n\n"}
{"id": "19541", "url": "https://en.wikipedia.org/wiki?curid=19541", "title": "Muslims", "text": "Muslims\n\nMuslims () are people who follow or practice Islam, a monotheistic Abrahamic religion. Muslims consider the Quran, their holy book, to be the verbatim word of God as revealed to the Islamic prophet and messenger Muhammad. The majority of Muslims also follow the teachings and practices of Muhammad (\"sunnah\") as recorded in traditional accounts (\"hadith\"). \"Muslim\" is an Arabic word meaning \"submitter\" (to God).\n\nThe beliefs of Muslims include: that God ( \"\") is eternal, transcendent and absolutely one (\"tawhid\"); that God is incomparable, self-sustaining and neither begets nor was begotten; that Islam is the complete and universal version of a primordial faith that has been revealed before through many prophets including Abraham, Ishmael, Isaac, Moses, and Jesus; that these previous messages and revelations have been partially changed or corrupted over time (\"tahrif\") and that the Qur'an is the final unaltered revelation from God (Final Testament).\n\nThe religious practices of Muslims are enumerated in the Five Pillars of Islam: the declaration of faith (\"shahadah\"), daily prayers (\"salat\"), fasting during the month of Ramadan (\"sawm\"), almsgiving (\"zakat\"), and the pilgrimage to Mecca (\"hajj\") at least once in a lifetime.\n\nTo become a Muslim and to convert to Islam, it is essential to utter the \"Shahada\", one of the Five Pillars of Islam, a declaration of faith and trust that professes that there is only one God \"(Allah)\" and that Muhammad is God's messenger. It is a set statement normally recited in Arabic: \"lā ʾilāha ʾillā-llāhu muḥammadun rasūlu-llāh\" () \"There is no god but Allah, (and) Muhammad is the messenger of God.\"\n\nIn Sunni Islam, the shahada has two parts: \"la ilaha illa'llah\" (there is no god but God), and \"Muhammadun rasul Allah\" (Muhammad is the messenger of God), which are sometimes referred to as the first \"shahada\" and the second \"shahada\". The first statement of the shahada is also known as the \"tahlīl\".\n\nIn Shia Islam, the shahada also has a third part, a phrase concerning Ali, the first Shia Imam and the fourth Rashid caliph of Sunni Islam: (), which translates to \"Ali is the \"wali\" of God.\n\nThe word \"muslim\" (, ; , , or \"moslem\" , ) is the active participle of the same verb of which \"islām\" is a verbal noun, based on the triliteral \"S-L-M\" \"to be whole, intact\". A female adherent is a \"muslima\" () (also transliterated as \"Muslimah\" ). The plural form in Arabic is \"muslimūn\" () or \"muslimīn\" (), and its feminine equivalent is \"muslimāt\" (). The Arabic form \"muslimun\" is the stem IV participle of the triliteral \"S-L-M\".\n\nThe ordinary word in English is \"Muslim\". It is sometimes transliterated as \"Moslem\", which is an older spelling. The word \"Mosalman\" (, alternatively \"Mussalman\") is a common equivalent for \"Muslim\" used in Central and South Asia. Until at least the mid-1960s, many English-language writers used the term \"Mohammedans\" or \"Mahometans\". Although such terms were not necessarily intended to be pejorative, Muslims argue that the terms are offensive because they allegedly imply that Muslims worship Muhammad rather than God. Other obsolete terms include \"Muslimite\" and \"Muslimist\".\n\nMusulmán/Mosalmán () is a synonym for \"Muslim\" and is modified from Arabic. It is the origin of the Spanish word ', the (dated) German ', the French word \"musulman\", the Polish words ' and ', the Portuguese word ', the Italian word ' or ', the Romanian word \"musulman\" and the Greek word (all used for a Muslim). In English it was sometimes spelled Mussulman and has become archaic in usage.\n\nApart from Persian, Spanish, Polish, Portuguese, Italian, and Greek, the term could be found, with obvious local differences, in Armenian, Dari, Pashto, Urdu, Hindi, Bengali, Marathi, Punjabi, Turkish, Kazakh, Uzbek, Kyrgyz, Azeri, Maltese, Hungarian, Czech, Bosnian, Bulgarian, Russian, Serbian, Ukrainian, Romanian, Dutch, and Sanskrit.\n\nThe Muslim philosopher Ibn Arabi said:\n\nThe Qur'an describes many prophets and messengers within Judaism and Christianity, and their respective followers, as Muslim: Adam, Noah, Abraham, Ishmael, Jacob, Moses, and Jesus and his apostles are all considered to be Muslims in the Qur'an. The Qur'an states that these men were Muslims because they submitted to God, preached His message and upheld His values, which included praying, charity, fasting and pilgrimage. Thus, in Surah 3:52 of the Qur'an, Jesus' disciples tell him, \"We believe in God; and you be our witness that we are Muslims (\"wa-shahad be anna muslimūn\").\" In Muslim belief, before the Qur'an, God had given the Tawrat (Torah) to Moses, the Zabur (Psalms) to David and the Injil (Gospel) to Jesus, who are all considered important Muslim prophets.\n\nThe most populous Muslim-majority country is Indonesia, home to 12.7% of the world's Muslims, followed by Pakistan (11.0%), Bangladesh (9.2%), and Egypt (4.9%). About 20% of the world's Muslims lives in the Middle East and North Africa.\n\nSizable minorities are also found in India, China, Russia, Ethiopia, the Americas, Australia and parts of Europe. The country with the highest proportion of self-described Muslims as a proportion of its total population is Morocco. Converts and immigrant communities are found in almost every part of the world.\n\nOver 75–90% of Muslims are Sunni. The second and third largest sects, Shia and Ahmadiyya, make up 10–20%, and 1% respectively.\n\nWith about 1.8 billion followers (2015), almost a quarter of earth's population, Islam is the second-largest and the fastest-growing religion in the world. due primarily to the young age and high fertility rate of Muslims, with Muslim having a rate of (3.1) compared to the world average of (2.5). According to the same study, religious switching has no impact on Muslim population, since the number of people who embrace Islam and those who leave Islam are roughly equal.\n\nA Pew Center study in 2016 found that Muslims have the highest number of adherents under the age of 15 (or 34% of the total Muslim population) of any major religion, while only 7% are aged 60+ (the smallest percentage of any major religion). According to the same study, Muslims have the highest fertility rates (3.1) of any major religious group. The study also found that Muslims have the lowest average levels of education after Hindus, with an average of 5.6 years of schooling. About 36% of all Muslims have no formal schooling, and Muslims have the lowest average levels of higher education of any major religious group, with only 8% having graduate and post-graduate degrees.\n\nMuslims typically do not use a knife and fork as is commonly done in most western countries, instead they use their right hand to scoop the food into their mouth or use a piece of bread with their right hand. Islam holds that the left hand will feed Shaitan (\"\"Satan\"\") if used to consume food, and the religion discourages eating with utensils or the left hand. \n\n\n"}
{"id": "19542", "url": "https://en.wikipedia.org/wiki?curid=19542", "title": "MUSH", "text": "MUSH\n\nIn multiplayer online games, a MUSH (a backronymed pun on MUD most often expanded as Multi-User Shared Hallucination, though Multi-User Shared Hack, Habitat, and Holodeck are also observed) is a text-based online social medium to which multiple users are connected at the same time. MUSHes are often used for online social intercourse and role-playing games, although the first forms of MUSH do not appear to be coded specifically to implement gaming activity. MUSH software was originally derived from MUDs; today's two major MUSH variants are descended from TinyMUD, which was fundamentally a social game.\nMUSH has forked over the years and there are now different varieties with different features, although most have strong similarities and one who is fluent in coding one variety can switch to coding for the other with only a little effort. The source code for most widely used MUSH servers is open source and available from its current maintainers.\n\nA primary feature of MUSH codebases that tends to distinguish it from other multi-user environments is the ability, by default, of any player to extend the world by creating new rooms or objects and specifying their behavior in the MUSH's internal scripting language. Another is the default lack of much player or administrative hierarchy imposed by the server itself. Over the years, both of these traits have become less pronounced, as many server administrators choose to eliminate or heavily restrict player-controlled building, and several games have custom coded systems to restore more of a hierarchal system.\n\nThe programming language for MUSH, usually referred to as \"MUSHcode\" or \"softcode\" (to distinguish it from \"hardcode\"the language in which the MUSH server itself is written) was developed by Larry Foard. TinyMUSH started life as a set of enhancements to the original TinyMUD code. \"MUSHcode\" is similar in syntax to Lisp. Most customization is done in \"softcode\" rather than by directly modifying the hardcode.\n\nTraditionally, roleplay consists of a series of \"poses\". Each character makes a \"pose\"that is, writes a description of speech, actions, etc. which the character performs. Special commands allow players to print OOC (out of character) messages, distinguished by a prefixed tag from IC (in character) action. This medium borrows traits from both improvisational stage acting and writing. Roleplaying is one of the primary activities of MUSHes, along with socializing.\n\nThere is nothing in the code base that restricts a new MUSH from being a traditional hack-and-slash MUD-style game. However, the earliest uses of MUSH servers were for roleplaying and socializing, and these early trends have largely governed their descendants.\n\nA large number of roleplaying MUSHes have custom combat systems and other tools coded by their administrators in order to further encourage roleplay. However, as roleplay is the primary goal, many MUSHes have varying ideas of how these programs are used.\n\nAll MUSH servers provide a flag that, when set on a player, bestows the ability to view and modify nearly everything in the game's database. Such players are usually called Wizards, and typically form the basis for the MUSH administration.\n\nAlthough MUSH servers do not impose strong administrative hierarchies, most MUSH games establish additional levels of management besides Wizards. Some do so on a purely organizational basis, naming some Wizards \"Head Wizards\" or \"Junior Wizards\" or assigning sphere of responsibility to Wizards, despite the technical equality of their abilities in the game world. Others provide finer-grained control over capabilities that can be assigned to players so that some players can be granted the ability to view, but not modify, the entire game world, or to perform limited modifications. Other levels of power can include added control over one's own character, or fewer limits on resources. PennMUSH, TinyMUSH, and TinyMUX include the \"Royalty\" flag, which gives a player the powers to do most anything that doesn't involve modifying the database. RhostMUSH has a wide array of staff flags that differ in many ways from its sister servers.\n\nMaintainers and developers of MUSH servers have traditionally shared ideas with one another, so most MUSH servers include concepts or code developed originally in other servers. There is particular interest in ensuring that common MUSHcode features work similarly across servers.\n\nPennMUSH, TinyMUSH, TinyMUX and RhostMUSH are all open-source MUSH servers. Some enthusiasts may exclude one or more of the above on the basis of distribution method, name, or parentage, but all are free-form MUSH servers. Differences in the software tend to focus more on the administrative or softcode side (slightly different function syntax; or different functions altogether; more, or less, administrative controls). The set of commands that players use to interface to the game are essentially standard amongst servers bearing the appellation \"MUSH\".\n\n\n"}
{"id": "19544", "url": "https://en.wikipedia.org/wiki?curid=19544", "title": "Microevolution", "text": "Microevolution\n\nMicroevolution is the change in allele frequencies that occurs over time within a population. This change is due to four different processes: mutation, selection (natural and artificial), gene flow and genetic drift. This change happens over a relatively short (in evolutionary terms) amount of time compared to the changes termed macroevolution which is where greater differences in the population occur.\n\nPopulation genetics is the branch of biology that provides the mathematical structure for the study of the process of microevolution. Ecological genetics concerns itself with observing microevolution in the wild. Typically, observable instances of evolution are examples of microevolution; for example, bacterial strains that have antibiotic resistance.\n\nMicroevolution over time leads to speciation or the appearance of novel structure, sometimes classified as macroevolution. Macro and microevolution describe fundamentally identical processes on different scales.\n\nMacroevolution and microevolution describe fundamentally identical processes on different time scales. Microevolution refers to small evolutionary changes (typically described as changes in allele frequencies) \"within\" a species or population. while macroevolution is evolution on a scale of separated gene pools. Macroevolutionary studies focus on change that occurs at or above the level of species.\n\nMutations are changes in the DNA sequence of a cell's genome and are caused by radiation, viruses, transposons and mutagenic chemicals, as well as errors that occur during meiosis or DNA replication. Errors are introduced particularly often in the process of DNA replication, in the polymerization of the second strand. These errors can also be induced by the organism itself, by cellular processes such as hypermutation.\n\nDuring the process of DNA replication, errors occasionally occur in the polymerization of the second strand. These errors, called mutations, can affect the phenotype of an organism, especially if they occur within the protein coding sequence of a gene. Error rates are usually very low—1 error in every 10–100 million bases—due to the \"proofreading\" ability of DNA polymerases. (Without proofreading error rates are a thousandfold higher; because many viruses rely on DNA and RNA polymerases that lack proofreading ability, they experience higher mutation rates.) Processes that increase the rate of changes in DNA are called mutagenic: mutagenic chemicals promote errors in DNA replication, often by interfering with the structure of base-pairing, while UV radiation induces mutations by causing damage to the DNA structure. Chemical damage to DNA occurs naturally as well, and cells use DNA repair mechanisms to repair mismatches and breaks in DNA—nevertheless, the repair sometimes fails to return the DNA to its original sequence.\n\nIn organisms that use chromosomal crossover to exchange DNA and recombine genes, errors in alignment during meiosis can also cause mutations. Errors in crossover are especially likely when similar sequences cause partner chromosomes to adopt a mistaken alignment making some regions in genomes more prone to mutating in this way. These errors create large structural changes in DNA sequence—duplications, inversions or deletions of entire regions, or the accidental exchanging of whole parts between different chromosomes (called translocation).\n\nMutation can result in several different types of change in DNA sequences; these can either have no effect, alter the product of a gene, or prevent the gene from functioning. Studies in the fly \"Drosophila melanogaster\" suggest that if a mutation changes a protein produced by a gene, this will probably be harmful, with about 70 percent of these mutations having damaging effects, and the remainder being either neutral or weakly beneficial. Due to the damaging effects that mutations can have on cells, organisms have evolved mechanisms such as DNA repair to remove mutations. Therefore, the optimal mutation rate for a species is a trade-off between costs of a high mutation rate, such as deleterious mutations, and the metabolic costs of maintaining systems to reduce the mutation rate, such as DNA repair enzymes. Viruses that use RNA as their genetic material have rapid mutation rates, which can be an advantage since these viruses will evolve constantly and rapidly, and thus evade the defensive responses of e.g. the human immune system.\n\nMutations can involve large sections of DNA becoming duplicated, usually through genetic recombination. These duplications are a major source of raw material for evolving new genes, with tens to hundreds of genes duplicated in animal genomes every million years. Most genes belong to larger families of genes of shared ancestry. Novel genes are produced by several methods, commonly through the duplication and mutation of an ancestral gene, or by recombining parts of different genes to form new combinations with new functions.\n\nHere, domains act as modules, each with a particular and independent function, that can be mixed together to produce genes encoding new proteins with novel properties. For example, the human eye uses four genes to make structures that sense light: three for color vision and one for night vision; all four arose from a single ancestral gene. Another advantage of duplicating a gene (or even an entire genome) is that this increases redundancy; this allows one gene in the pair to acquire a new function while the other copy performs the original function. Other types of mutation occasionally create new genes from previously noncoding DNA.\n\n\"Selection\" is the process by which heritable traits that make it more likely for an organism to survive and successfully reproduce become more common in a population over successive generations.\n\nIt is sometimes valuable to distinguish between naturally occurring selection, natural selection, and selection that is a manifestation of choices made by humans, artificial selection. This distinction is rather diffuse. Natural selection is nevertheless the dominant part of selection.\n\nThe natural genetic variation within a population of organisms means that some individuals will survive more successfully than others in their current environment. Factors which affect reproductive success are also important, an issue which Charles Darwin developed in his ideas on sexual selection.\n\nNatural selection acts on the phenotype, or the observable characteristics of an organism, but the genetic (heritable) basis of any phenotype which gives a reproductive advantage will become more common in a population (see allele frequency). Over time, this process can result in adaptations that specialize organisms for particular ecological niches and may eventually result in the speciation (the emergence of new species).\n\nNatural selection is one of the cornerstones of modern biology. The term was introduced by Darwin in his groundbreaking 1859 book \"On the Origin of Species\", in which natural selection was described by analogy to artificial selection, a process by which animals and plants with traits considered desirable by human breeders are systematically favored for reproduction. The concept of natural selection was originally developed in the absence of a valid theory of heredity; at the time of Darwin's writing, nothing was known of modern genetics. The union of traditional Darwinian evolution with subsequent discoveries in classical and molecular genetics is termed the \"modern evolutionary synthesis\". Natural selection remains the primary explanation for adaptive evolution.\n\n\"Genetic drift\" is the change in the relative frequency in which a gene variant (allele) occurs in a population due to random sampling. That is, the alleles in the offspring in the population are a random sample of those in the parents. And chance has a role in determining whether a given individual survives and reproduces. A population's allele frequency is the fraction or percentage of its gene copies compared to the total number of gene alleles that share a particular form.\n\nGenetic drift is an evolutionary process which leads to changes in allele frequencies over time. It may cause gene variants to disappear completely, and thereby reduce genetic variability. In contrast to natural selection, which makes gene variants more common or less common depending on their reproductive success, the changes due to genetic drift are not driven by environmental or adaptive pressures, and may be beneficial, neutral, or detrimental to reproductive success.\n\nThe effect of genetic drift is larger in small populations, and smaller in large populations. Vigorous debates wage among scientists over the relative importance of genetic drift compared with natural selection. Ronald Fisher held the view that genetic drift plays at the most a minor role in evolution, and this remained the dominant view for several decades. In 1968 Motoo Kimura rekindled the debate with his neutral theory of molecular evolution which claims that most of the changes in the genetic material are caused by genetic drift. The predictions of neutral theory, based on genetic drift, do not fit recent data on whole genomes well: these data suggest that the frequencies of neutral alleles change primarily due to selection at linked sites, rather than due to genetic drift by means of sampling error.\n\nGene flow is the exchange of genes between populations, which are usually of the same species. Examples of gene flow within a species include the migration and then breeding of organisms, or the exchange of pollen. Gene transfer between species includes the formation of hybrid organisms and horizontal gene transfer.\n\nMigration into or out of a population can change allele frequencies, as well as introducing genetic variation into a population. Immigration may add new genetic material to the established gene pool of a population. Conversely, emigration may remove genetic material. As barriers to reproduction between two diverging populations are required for the populations to become new species, gene flow may slow this process by spreading genetic differences between the populations. Gene flow is hindered by mountain ranges, oceans and deserts or even man-made structures such as the Great Wall of China, which has hindered the flow of plant genes.\n\nDepending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile, due to the two different sets of chromosomes being unable to pair up during meiosis. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridization in creating new species of animals is unclear, although cases have been seen in many types of animals, with the gray tree frog being a particularly well-studied example.\n\nHybridization is, however, an important means of speciation in plants, since polyploidy (having more than two copies of each chromosome) is tolerated in plants more readily than in animals. Polyploidy is important in hybrids as it allows reproduction, with the two different sets of chromosomes each being able to pair with an identical partner during meiosis. Polyploid hybrids also have more genetic diversity, which allows them to avoid inbreeding depression in small populations.\n\nHorizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast \"Saccharomyces cerevisiae\" and the adzuki bean beetle \"Callosobruchus chinensis\" may also have occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which appear to have received a range of genes from bacteria, fungi, and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains. Large-scale gene transfer has also occurred between the ancestors of eukaryotic cells and prokaryotes, during the acquisition of chloroplasts and mitochondria.\n\n\"Gene flow\" is the transfer of alleles from one population to another.\n\nMigration into or out of a population may be responsible for a marked change in allele frequencies. Immigration may also result in the addition of new genetic variants to the established gene pool of a particular species or population.\n\nThere are a number of factors that affect the rate of gene flow between different populations. One of the most significant factors is mobility, as greater mobility of an individual tends to give it greater migratory potential. Animals tend to be more mobile than plants, although pollen and seeds may be carried great distances by animals or wind.\n\nMaintained gene flow between two populations can also lead to a combination of the two gene pools, reducing the genetic variation between the two groups. It is for this reason that gene flow strongly acts against speciation, by recombining the gene pools of the groups, and thus, repairing the developing differences in genetic variation that would have led to full speciation and creation of daughter species.\n\nFor example, if a species of grass grows on both sides of a highway, pollen is likely to be transported from one side to the other and vice versa. If this pollen is able to fertilise the plant where it ends up and produce viable offspring, then the alleles in the pollen have effectively been able to move from the population on one side of the highway to the other.\n\nThe term \"microevolution\" was first used by botanist Robert Greenleaf Leavitt in the journal \"Botanical Gazette\" in 1909, addressing what he called the \"mystery\" of how formlessness gives rise to form.\n\nHowever, Leavitt was using the term to describe what we would now call developmental biology; it was not until Russian Entomologist Yuri Filipchenko used the terms \"macroevolution\" and \"microevolution\" in 1927 in his German language work, \"Variabilität und Variation\", that it attained its modern usage. The term was later brought into the English-speaking world by Theodosius Dobzhansky in his book Genetics and the Origin of Species (1937).\n\nIn young Earth creationism and baraminology a central tenet is that evolution can explain diversity in a limited number of created kinds which can interbreed (which they call \"microevolution\") while the formation of new \"kinds\" (which they call \"macroevolution\") is impossible. This acceptance of \"microevolution\" only within a \"kind\" is also typical of old Earth creationism.\n\nScientific organizations such as the American Association for the Advancement of Science describe microevolution as small scale change within species, and macroevolution as the formation of new species, but otherwise not being different from microevolution. In macroevolution, an accumulation of microevolutionary changes leads to speciation. The main difference between the two processes is that one occurs within a few generations, whilst the other takes place over thousands of years (i.e. a quantitative difference). Essentially they describe the same process; although evolution beyond the species level results in beginning and ending generations which could not interbreed, the intermediate generations could.\n\nOpponents to creationism argue that changes in the number of chromosomes can be accounted for by intermediate stages in which a single chromosome divides in generational stages, or multiple chromosomes fuse, and cite the chromosome difference between humans and the other great apes as an example. Creationists insist that since the actual divergence between the other great apes and humans was not observed, the evidence is circumstantial.\n\nDescribing the fundamental similarity between macro and microevolution in his authoritative textbook \"Evolutionary Biology,\" biologist Douglas Futuyma writes,\n\nContrary to the claims of some antievolution proponents, evolution of life forms beyond the species level (i.e. speciation) has indeed been observed and documented by scientists on numerous occasions. In creation science, creationists accepted speciation as occurring within a \"created kind\" or \"baramin\", but objected to what they called \"third level-macroevolution\" of a new genus or higher rank in taxonomy. There is ambiguity in the ideas as to where to draw a line on \"species\", \"created kinds\", and what events and lineages fall within the rubric of microevolution or macroevolution.\n\n\n"}
{"id": "19545", "url": "https://en.wikipedia.org/wiki?curid=19545", "title": "MySQL", "text": "MySQL\n\nMySQL ( \"My S-Q-L\") is an open source relational database management system (RDBMS). Its name is a combination of \"My\", the name of co-founder Michael Widenius's daughter, and \"SQL\", the abbreviation for Structured Query Language.\n\nMySQL is free and open-source software under the terms of the GNU General Public License, and is also available under a variety of proprietary licenses. MySQL was owned and sponsored by the Swedish company MySQL AB, which was bought by Sun Microsystems (now Oracle Corporation). In 2010, when Oracle acquired Sun, Widenius forked the open-source MySQL project to create MariaDB.\n\nMySQL is a component of the LAMP web application software stack (and others), which is an acronym for \"Linux, Apache, MySQL, Perl/PHP/Python\". MySQL is used by many database-driven web applications, including Drupal, Joomla, phpBB, and WordPress. MySQL is also used by many popular websites, including Google (though not for searches), Facebook, Twitter, Flickr, and YouTube.\n\nMySQL is written in C and C++. Its SQL parser is written in yacc, but it uses a home-brewed lexical analyzer. MySQL works on many system platforms, including AIX, BSDi, FreeBSD, HP-UX, eComStation, i5/OS, IRIX, Linux, macOS, Microsoft Windows, NetBSD, Novell NetWare, OpenBSD, OpenSolaris, OS/2 Warp, QNX, Oracle Solaris, Symbian, SunOS, SCO OpenServer, SCO UnixWare, Sanos and Tru64. A port of MySQL to OpenVMS also exists.\n\nThe MySQL server software itself and the client libraries use dual-licensing distribution. They are offered under GPL version 2, or a proprietary license.\n\nSupport can be obtained from the official manual. Free support additionally is available in different IRC channels and forums. Oracle offers paid support via its MySQL Enterprise products. They differ in the scope of services and in price. Additionally, a number of third party organisations exist to provide support and services, including MariaDB and Percona.\n\nMySQL has received positive reviews, and reviewers noticed it \"performs extremely well in the average case\" and that the \"developer interfaces are there, and the documentation (not to mention feedback in the real world via Web sites and the like) is very, very good\". It has also been tested to be a \"fast, stable and true multi-user, multi-threaded sql database server\".\n\nMySQL was created by a Swedish company, MySQL AB, founded by David Axmark, Allan Larsson and Michael \"Monty\" Widenius.\nOriginal development of MySQL by Widenius and Axmark began in 1994. The first version of MySQL appeared on 23 May 1995. It was initially created for personal usage from mSQL based on the low-level language ISAM, which the creators considered too slow and inflexible. They created a new SQL interface, while keeping the same API as mSQL. By keeping the API consistent with the mSQL system, many developers were able to use MySQL instead of the (proprietarily licensed) mSQL antecedent.\n\nAdditional milestones in MySQL development included:\n\n\n\n\nWork on version 6 stopped after the Sun Microsystems acquisition. The MySQL Cluster product uses version 7. The decision was made to jump to version 8 as the next major version number.\n\nOn 15 June 2001, NuSphere sued MySQL AB, TcX DataKonsult AB and its original authors Michael (\"Monty\") Widenius and David Axmark in U.S District Court in Boston for \"breach of contract, tortious interference with third party contracts and relationships and unfair competition\".\n\nIn 2002, MySQL AB sued Progress NuSphere for copyright and trademark infringement in United States district court. NuSphere had allegedly violated MySQL AB's copyright by linking MySQL's GPL'ed code with NuSphere Gemini table without being in compliance with the license. After a preliminary hearing before Judge Patti Saris on 27 February 2002, the parties entered settlement talks and eventually settled. After the hearing, FSF commented that \"Judge Saris made clear that she sees the GNU GPL to be an enforceable and binding license.\"\n\nIn October 2005, Oracle Corporation acquired Innobase OY, the Finnish company that developed the third-party InnoDB storage engine that allows MySQL to provide such functionality as transactions and foreign keys. After the acquisition, an Oracle press release mentioned that the contracts that make the company's software available to MySQL AB would be due for renewal (and presumably renegotiation) some time in 2006. During the MySQL Users Conference in April 2006, MySQL AB issued a press release that confirmed that MySQL AB and Innobase OY agreed to a \"multi-year\" extension of their licensing agreement.\n\nIn February 2006, Oracle Corporation acquired Sleepycat Software, makers of the Berkeley DB, a database engine providing the basis for another MySQL storage engine. This had little effect, as Berkeley DB was not widely used, and was dropped (due to lack of use) in MySQL 5.1.12, a pre-GA release of MySQL 5.1 released in October 2006.\n\nIn January 2008, Sun Microsystems bought MySQL AB for $1 billion.\n\nIn April 2009, Oracle Corporation entered into an agreement to purchase Sun Microsystems, then owners of MySQL copyright and trademark. Sun's board of directors unanimously approved the deal. It was also approved by Sun's shareholders, and by the U.S. government on 20 August 2009. On 14 December 2009, Oracle pledged to continue to enhance MySQL as it had done for the previous four years.\n\nA movement against Oracle's acquisition of MySQL AB, to \"Save MySQL\" from Oracle was started by one of the MySQL AB founders, Monty Widenius. The petition of 50,000+ developers and users called upon the European Commission to block approval of the acquisition. At the same time, some Free Software opinion leaders (including Pamela Jones of Groklaw, Jan Wildeboer and Carlo Piana, who also acted as co-counsel in the merger regulation procedure) advocated for the unconditional approval of the merger. As part of the negotiations with the European Commission, Oracle committed that MySQL server will continue until at least 2015 to use the dual-licensing strategy long used by MySQL AB, with proprietary and GPL versions available. The antitrust of the EU had been \"pressuring it to divest MySQL as a condition for approval of the merger\". But, as revealed by WikiLeaks, the US Department of Justice, at the request of Oracle, pressured the EU to approve the merger unconditionally. The European Commission eventually unconditionally approved Oracle's acquisition of MySQL AB on 21 January 2010.\n\nIn January 2010, before Oracle's acquisition of MySQL AB, Monty Widenius started a GPL-only fork, MariaDB. MariaDB is based on the same code base as MySQL server 5.5 and aims to maintain compatibility with Oracle-provided versions.\n\nMySQL is offered under two different editions: the open source MySQL Community Server and the proprietary Enterprise Server. MySQL Enterprise Server is differentiated by a series of proprietary extensions which install as server plugins, but otherwise shares the version numbering system and is built from the same code base.\n\nMajor features as available in MySQL 5.6:\n\nThe developers release minor updates of the MySQL Server approximately every two months. The sources can be obtained from MySQL's website or from MySQL's GitHub repository, both under the GPL license.\n\nWhen using some storage engines other than the default of InnoDB, MySQL does not comply with the full SQL standard for some of the implemented functionality, including foreign key references. Check constraints are parsed but ignored by all storage engines.\n\nUp until MySQL 5.7, triggers are limited to one per action / timing, meaning that at most one trigger can be defined to be executed after an INSERT operation, and one before INSERT on the same table.\nNo triggers can be defined on views.\n\nMySQL database's inbuilt functions like UNIX_TIMESTAMP() will return 0 after 03:14:07 UTC on 19 January 2038.\nRecently, there had been an attempt to solve the problem which had been assigned to the internal queue.\n\nMySQL can be built and installed manually from source code, but it is more commonly installed from a binary package unless special customizations are required. On most Linux distributions, the package management system can download and install MySQL with minimal effort, though further configuration is often required to adjust security and optimization settings.\n\nThough MySQL began as a low-end alternative to more powerful proprietary databases, it has gradually evolved to support higher-scale needs as well. It is still most commonly used in small to medium scale single-server deployments, either as a component in a LAMP-based web application or as a standalone database server. Much of MySQL's appeal originates in its relative simplicity and ease of use, which is enabled by an ecosystem of open source tools such as phpMyAdmin.\nIn the medium range, MySQL can be scaled by deploying it on more powerful hardware, such as a multi-processor server with gigabytes of memory.\n\nThere are, however, limits to how far performance can scale on a single server ('scaling up'), so on larger scales, multi-server MySQL ('scaling out') deployments are required to provide improved performance and reliability. A typical high-end configuration can include a powerful master database which handles data write operations and is replicated to multiple slaves that handle all read operations. The master server continually pushes binlog events to connected slaves so in the event of failure a slave can be promoted to become the new master, minimizing downtime. Further improvements in performance can be achieved by caching the results from database queries in memory using memcached, or breaking down a database into smaller chunks called shards which can be spread across a number of distributed server clusters.\n\n is a logical backup tool included with both community and enterprise editions of MySQL. It supports backing up from all storage engines. MySQL Enterprise Backup is a hot backup utility included as part of the MySQL Enterprise subscription from Oracle, offering native InnoDB hot backup, as well as backup for other storage engines.\n\nMySQL Fabric is an integrated system for managing a collection of MySQL servers, and a framework on top of which high availability and database sharding is built. MySQL Fabric is open-source, and supports procedure execution in the presence of failure, providing an execution model usually called \"resilient execution.\" MySQL client libraries are extended so they are hiding the complexities of handling failover in the event of a server failure, as well as correctly dispatching transactions to the shards.\n\nMySQL can also be run on cloud computing platforms such as Microsoft Azure, Amazon EC2, Oracle Cloud Infrastructure. Some common deployment models for MySQL on the cloud are:\n\n\n\nA graphical user interface (GUI) is a type of interface that allows users to interact with electronic devices or programs through graphical icons and visual indicators such as secondary notation, as opposed to text-based interfaces, typed command labels or text navigation. GUIs are easier to learn than command-line interfaces (CLIs), which require commands to be typed on the keyboard.\n\nThird-party proprietary and free graphical administration applications (or \"front ends\") are available that integrate with MySQL and enable users to work with database structure and data visually. Some well-known front ends are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA command-line interface is a means of interacting with a computer program where the user issues commands to the program by typing in successive lines of text (command lines). MySQL ships with many command line tools, from which the main interface is the client.\n\nMySQL Utilities is a set of utilities designed to perform common maintenance and administrative tasks. Originally included as part of the MySQL Workbench, the utilities are a stand-alone download available from Oracle.\n\nPercona Toolkit is a cross-platform toolkit for MySQL, developed in Perl. Percona Toolkit can be used to prove replication is working correctly, fix corrupted data, automate repetitive tasks, and speed up servers. Percona Toolkit is included with several Linux distributions such as CentOS and Debian, and packages are available for Fedora and Ubuntu as well. Percona Toolkit was originally developed as Maatkit, but as of late 2011, Maatkit is no longer developed.\n\nMany programming languages with language-specific APIs include libraries for accessing MySQL databases. These include MySQL Connector/Net for integration with Microsoft's Visual Studio (languages such as C# and VB are most commonly used) and the JDBC driver for Java. In addition, an ODBC interface called MySQL Connector/ODBC allows additional programming languages that support the ODBC interface to communicate with a MySQL database, such as ASP or ColdFusion. The HTSQL URL-based query method also ships with a MySQL adapter, allowing direct interaction between a MySQL database and any web client via structured URLs.\n\nA variety of MySQL forks exist, including the following.\n\n\n\n\n\n\n"}
{"id": "19547", "url": "https://en.wikipedia.org/wiki?curid=19547", "title": "Modernism", "text": "Modernism\n\nModernism is a philosophical movement that, along with cultural trends and changes, arose from wide-scale and far-reaching transformations in Western society during the late 19th and early 20th centuries. Among the factors that shaped modernism were the development of modern industrial societies and the rapid growth of cities, followed then by reactions of horror to World War I. Modernism also rejected the certainty of Enlightenment thinking, and many modernists rejected religious belief.\n\nModernism, in general, includes the activities and creations of those who felt the traditional forms of art, architecture, literature, religious faith, philosophy, social organization, activities of daily life, and sciences, were becoming ill-fitted to their tasks and outdated in the new economic, social, and political environment of an emerging fully industrialized world. The poet Ezra Pound's 1934 injunction to \"Make it new!\" was the touchstone of the movement's approach towards what it saw as the now obsolete culture of the past. In this spirit, its innovations, like the stream-of-consciousness novel, atonal (or pantonal) and twelve-tone music, divisionist painting and abstract art, all had precursors in the 19th century.\n\nA notable characteristic of modernism is self-consciousness and irony concerning literary and social traditions, which often led to experiments with form, along with the use of techniques that drew attention to the processes and materials used in creating a painting, poem, building, etc. Modernism explicitly rejected the ideology of realism and makes use of the works of the past by the employment of reprise, incorporation, rewriting, recapitulation, revision and parody.\n\nSome commentators define modernism as a mode of thinking—one or more philosophically defined characteristics, like self-consciousness or self-reference, that run across all the novelties in the arts and the disciplines. More common, especially in the West, are those who see it as a socially progressive trend of thought that affirms the power of human beings to create, improve and reshape their environment with the aid of practical experimentation, scientific knowledge, or technology. From this perspective, modernism encouraged the re-examination of every aspect of existence, from commerce to philosophy, with the goal of finding that which was 'holding back' progress, and replacing it with new ways of reaching the same end. Others focus on modernism as an aesthetic introspection. This facilitates consideration of specific reactions to the use of technology in the First World War, and anti-technological and nihilistic aspects of the works of diverse thinkers and artists spanning the period from Friedrich Nietzsche (1844–1900) to Samuel Beckett (1906–1989).\n\nWhile some scholars see modernism continuing into the twenty first century, others see it evolving into late modernism or high modernism. Postmodernism is a departure from modernism and refutes its basic assumptions.\n\nAccording to one critic, modernism developed out of Romanticism's revolt against the effects of the Industrial Revolution and bourgeois values: \"The ground motive of modernism, Graff asserts, was criticism of the nineteenth-century bourgeois social order and its world view [...] the modernists, carrying the torch of romanticism.\" While J. M. W. Turner (1775–1851), one of the greatest landscape painters of the 19th century, was a member of the Romantic movement, as \"a pioneer in the study of light, colour, and atmosphere\", he \"anticipated the French Impressionists\" and therefore modernism \"in breaking down conventional formulas of representation; [though] unlike them, he believed that his works should always express significant historical, mythological, literary, or other narrative themes.\"\nThe dominant trends of industrial Victorian England were opposed, from about 1850, by the English poets and painters that constituted the Pre-Raphaelite Brotherhood, because of their \"opposition to technical skill without inspiration.\" They were influenced by the writings of the art critic John Ruskin (1819–1900), who had strong feelings about the role of art in helping to improve the lives of the urban working classes, in the rapidly expanding industrial cities of Britain. Art critic Clement Greenberg describes the Pre-Raphaelite Brotherhood as proto-Modernists: \"There the proto-Modernists were, of all people, the pre-Raphaelites (and even before them, as proto-proto-Modernists, the German Nazarenes). The Pre-Raphaelites actually foreshadowed Manet (1832–83), with whom Modernist painting most definitely begins. They acted on a dissatisfaction with painting as practiced in their time, holding that its realism wasn't truthful enough.\" Rationalism has also had opponents in the philosophers Søren Kierkegaard (1813–55) and later Friedrich Nietzsche (1844–1900), both of whom had significant influence on existentialism.\n\nHowever, the Industrial Revolution continued. Influential innovations included steam-powered industrialization, and especially the development of railways, starting in Britain in the 1830s, and the subsequent advancements in physics, engineering, and architecture associated with this. A major 19th-century engineering achievement was The Crystal Palace, the huge cast-iron and plate glass exhibition hall built for The Great Exhibition of 1851 in London. Glass and iron were used in a similar monumental style in the construction of major railway terminals in London, such as Paddington Station (1854) and King's Cross station (1852). These technological advances led to the building of later structures like the Brooklyn Bridge (1883) and the Eiffel Tower (1889). The latter broke all previous limitations on how tall man-made objects could be. These engineering marvels radically altered the 19th-century urban environment and the daily lives of people. The human experience of time itself was altered, with the development of the electric telegraph from 1837, and the adoption of standard time by British railway companies from 1845, and in the rest of the world over the next fifty years.\n\nBut despite continuing technological advances the idea that history and civilization were inherently progressive, and that progress was always good, came under increasing attack in the nineteenth century. Arguments arose that the values of the artist and those of society were not merely different, but that Society was antithetical to Progress, and could not move forward in its present form. Early in the century, the philosopher Schopenhauer (1788–1860) (\"The World as Will and Representation\", 1819) had called into question the previous optimism, and his ideas had an important influence on later thinkers, including Nietzsche. Two of the most significant thinkers of the mid nineteenth century were biologist Charles Darwin (1809–1882), author of \"On the Origin of Species by Means of Natural Selection\" (1859), and political scientist Karl Marx (1818–1883), author of \"Das Kapital\" (1867). Darwin's theory of evolution by natural selection undermined religious certainty and the idea of human uniqueness. In particular, the notion that human beings were driven by the same impulses as \"lower animals\" proved to be difficult to reconcile with the idea of an ennobling spirituality. Karl Marx argued that there were fundamental contradictions within the capitalist system, and that the workers were anything but free.\nHistorians, and writers in different disciplines, have suggested various dates as starting points for modernism. Historian William Everdell, for example, has argued that modernism began in the 1870s, when metaphorical (or ontological) continuity began to yield to the discrete with mathematician Richard Dedekind's (1831–1916) Dedekind cut, and Ludwig Boltzmann's (1844–1906) statistical thermodynamics. Everdell also thinks modernism in painting began in 1885–1886 with Seurat's Divisionism, the \"dots\" used to paint \"A Sunday Afternoon on the Island of La Grande Jatte\". On the other hand, visual art critic Clement Greenberg called Immanuel Kant (1724–1804) \"the first real Modernist\", though he also wrote, \"What can be safely called Modernism emerged in the middle of the last century—and rather locally, in France, with Baudelaire in literature and Manet in painting, and perhaps with Flaubert, too, in prose fiction. (It was a while later, and not so locally, that Modernism appeared in music and architecture).\" The poet Baudelaire's \"Les Fleurs du mal\" (\"The Flowers of Evil\"), and Flaubert's novel \"Madame Bovary\" were both published in 1857.\n\nIn the arts and letters, two important approaches developed separately in France, beginning in the 1860s. The first was Impressionism, a school of painting that initially focused on work done, not in studios, but outdoors (\"en plein air\"). Impressionist paintings demonstrated that human beings do not see objects, but instead see light itself. The school gathered adherents despite internal divisions among its leading practitioners, and became increasingly influential. Initially rejected from the most important commercial show of the time, the government-sponsored Paris Salon, the Impressionists organized yearly group exhibitions in commercial venues during the 1870s and 1880s, timing them to coincide with the official Salon. A significant event of 1863 was the Salon des Refusés, created by Emperor Napoleon III to display all of the paintings rejected by the Paris Salon. While most were in standard styles, but by inferior artists, the work of Manet attracted tremendous attention, and opened commercial doors to the movement. The second French school was Symbolism, which literary historians see beginning with Charles Baudelaire (1821–1867), and including the later poets, Arthur Rimbaud (1854–1891) \"Une Saison en Enfer\" (\"A Season in Hell\", 1873), Paul Verlaine (1844–1896), Stéphane Mallarmé (1842–1898), and Paul Valéry (1871–1945). The symbolists \"stressed the priority of suggestion and evocation over direct description and explicit analogy,\" and were especially interested in \"the musical properties of language.\"\nCabaret, which gave birth to so many of the arts of modernism, including the immediate precursors of film, may be said to have begun in France in 1881 with the opening of the Black Cat in Montmartre, the beginning of the ironic monologue, and the founding of the Society of Incoherent Arts.\n\nInfluential in the early days of modernism were the theories of Sigmund Freud (1856–1939). Freud's first major work was \"Studies on Hysteria\" (with Josef Breuer, 1895). Central to Freud's thinking is the idea \"of the primacy of the unconscious mind in mental life,\" so that all subjective reality was based on the play of basic drives and instincts, through which the outside world was perceived. Freud's description of subjective states involved an unconscious mind full of primal impulses, and counterbalancing self-imposed restrictions derived from social values.\n\nFriedrich Nietzsche (1844–1900) was another major precursor of modernism, with a philosophy in which psychological drives, specifically the \"will to power\" (\"Wille zur Macht\"), was of central importance: \"Nietzsche often identified life itself with 'will to power', that is, with an instinct for growth and durability.\" Henri Bergson (1859–1941), on the other hand, emphasized the difference between scientific, clock time and the direct, subjective, human experience of time. His work on time and consciousness \"had a great influence on twentieth-century novelists,\" especially those Modernists who used the stream of consciousness technique, such as Dorothy Richardson, James Joyce, and Virginia Woolf (1882–1941). Also important in Bergson's philosophy was the idea of \"élan vital\", the life force, which \"brings about the creative evolution of everything.\" His philosophy also placed a high value on intuition, though without rejecting the importance of the intellect.\n\nImportant literary precursors of modernism were Fyodor Dostoyevsky (1821–1881), who wrote the novels \"Crime and Punishment\" (1866) and \"The Brothers Karamazov\" (1880); Walt Whitman (1819–1892), who published the poetry collection \"Leaves of Grass\" (1855–1891); and August Strindberg (1849–1912), especially his later plays, including the trilogy \"To Damascus\" 1898–1901, \"A Dream Play\" (1902) and \"The Ghost Sonata\" (1907). Henry James has also been suggested as a significant precursor, in a work as early as \"The Portrait of a Lady\" (1881).\n\nOut of the collision of ideals derived from Romanticism, and an attempt to find a way for knowledge to explain that which was as yet unknown, came the first wave of works in the first decade of the 20th century, which, while their authors considered them extensions of existing trends in art, broke the implicit contract with the general public that artists were the interpreters and representatives of bourgeois culture and ideas. These \"Modernist\" landmarks include the atonal ending of Arnold Schoenberg's Second String Quartet in 1908, the expressionist paintings of Wassily Kandinsky starting in 1903, and culminating with his first abstract painting and the founding of the Blue Rider group in Munich in 1911, and the rise of fauvism and the inventions of cubism from the studios of Henri Matisse, Pablo Picasso, Georges Braque, and others, in the years between 1900 and 1910.\n\nAn important aspect of modernism is how it relates to tradition through its adoption of techniques like reprise, incorporation, rewriting, recapitulation, revision and parody in new forms.\nT. S. Eliot made significant comments on the relation of the artist to tradition, including: \"[W]e shall often find that not only the best, but the most individual parts of [a poet's] work, may be those in which the dead poets, his ancestors, assert their immortality most vigorously.\" However, the relationship of Modernism with tradition was complex, as literary scholar Peter Childs indicates: \"There were paradoxical if not opposed trends towards revolutionary and reactionary positions, fear of the new and delight at the disappearance of the old, nihilism and fanatical enthusiasm, creativity and despair.\"\n\nAn example of how Modernist art can be both revolutionary and yet be related to past tradition, is the music of the composer Arnold Schoenberg. On the one hand Schoenberg rejected traditional tonal harmony, the hierarchical system of organizing works of music that had guided music making for at least a century and a half. He believed he had discovered a wholly new way of organizing sound, based in the use of twelve-note rows. Yet while this was indeed wholly new, its origins can be traced back in the work of earlier composers, such as Franz Liszt, Richard Wagner, Gustav Mahler, Richard Strauss and Max Reger. Furthermore, it must be noted that Schoenberg also wrote tonal music throughout his career.\n\nIn the world of art, in the first decade of the 20th century, young painters such as Pablo Picasso and Henri Matisse were causing a shock with their rejection of traditional perspective as the means of structuring paintings, though the impressionist Monet had already been innovative in his use of perspective. In 1907, as Picasso was painting \"Les Demoiselles d'Avignon\", Oskar Kokoschka was writing \"Mörder, Hoffnung der Frauen\" (\"Murderer, Hope of Women\"), the first Expressionist play (produced with scandal in 1909), and Arnold Schoenberg was composing his String Quartet No.2 in F sharp minor (1908), his first composition without a tonal centre.\n\nA primary influence that led to Cubism was the representation of three-dimensional form in the late works of Paul Cézanne, which were displayed in a retrospective at the 1907 Salon d'Automne. In Cubist artwork, objects are analyzed, broken up and reassembled in an abstracted form; instead of depicting objects from one viewpoint, the artist depicts the subject from a multitude of viewpoints to represent the subject in a greater context. Cubism was brought to the attention of the general public for the first time in 1911 at the Salon des Indépendants in Paris (held 21 April – 13 June). Jean Metzinger, Albert Gleizes, Henri Le Fauconnier, Robert Delaunay, Fernand Léger and Roger de La Fresnaye were shown together in Room 41, provoking a 'scandal' out of which Cubism emerged and spread throughout Paris and beyond. Also in 1911, Kandinsky painted \"Bild mit Kreis\" (\"Picture with a Circle\"), which he later called the first abstract painting. In 1912, Metzinger and Gleizes wrote the first (and only) major Cubist manifesto, \"Du \"Cubisme\"\", published in time for the Salon de la Section d'Or, the largest Cubist exhibition to date. In 1912 Metzinger painted and exhibited his enchanting \"La Femme au Cheval (Woman with a Horse)\" and \"Danseuse au Café (Dancer in a Café)\". Albert Gleizes painted and exhibited his \"Les Baigneuses (The Bathers)\" and his monumental \"Le Dépiquage des Moissons (Harvest Threshing)\". This work, along with \"La Ville de Paris\" (\"City of Paris\") by Robert Delaunay, was the largest and most ambitious Cubist painting undertaken during the pre-War Cubist period.\n\nIn 1905, a group of four German artists, led by Ernst Ludwig Kirchner, formed Die Brücke (the Bridge) in the city of Dresden. This was arguably the founding organization for the German Expressionist movement, though they did not use the word itself. A few years later, in 1911, a like-minded group of young artists formed Der Blaue Reiter (The Blue Rider) in Munich. The name came from Wassily Kandinsky's \"Der Blaue Reiter\" painting of 1903. Among their members were Kandinsky, Franz Marc, Paul Klee, and August Macke. However, the term \"Expressionism\" did not firmly establish itself until 1913. Though initially mainly a German artistic movement, most predominant in painting, poetry and the theatre between 1910 and 1930, most precursors of the movement were not German. Furthermore, there have been expressionist writers of prose fiction, as well as non-German speaking expressionist writers, and, while the movement had declined in Germany with the rise of Adolf Hitler in the 1930s, there were subsequent expressionist works.\n\nExpressionism is notoriously difficult to define, in part because it \"overlapped with other major 'isms' of the modernist period: with Futurism, Vorticism, Cubism, Surrealism and Dada.\" Richard Murphy also comments: \"the search for an all-inclusive definition is problematic to the extent that the most challenging expressionists\" such as the novelist Franz Kafka, poet Gottfried Benn, and novelist Alfred Döblin were simultaneously the most vociferous anti-expressionists. What, however, can be said, is that it was a movement that developed in the early 20th century mainly in Germany in reaction to the dehumanizing effect of industrialization and the growth of cities, and that \"one of the central means by which expressionism identifies itself as an avant-garde movement, and by which it marks its distance to traditions and the cultural institution as a whole is through its relationship to realism and the dominant conventions of representation.\" More explicitly: that the expressionists rejected the ideology of realism. \n\nThere was a concentrated Expressionist movement in early 20th century German theatre, of which Georg Kaiser and Ernst Toller were the most famous playwrights. Other notable Expressionist dramatists included Reinhard Sorge, Walter Hasenclever, Hans Henny Jahnn, and Arnolt Bronnen. They looked back to Swedish playwright August Strindberg and German actor and dramatist Frank Wedekind as precursors of their dramaturgical experiments. Oskar Kokoschka's \"Murderer, the Hope of Women\" was the first fully Expressionist work for the theatre, which opened on 4 July 1909 in Vienna. The extreme simplification of characters to mythic types, choral effects, declamatory dialogue and heightened intensity would become characteristic of later Expressionist plays. The first full-length Expressionist play was \"The Son\" by Walter Hasenclever, which was published in 1914 and first performed in 1916.\n\nFuturism is yet another modernist movement. In 1909, the Parisian newspaper \"Le Figaro\" published F. T. Marinetti's first manifesto. Soon afterwards a group of painters (Giacomo Balla, Umberto Boccioni, Carlo Carrà, Luigi Russolo, and Gino Severini) co-signed the Futurist Manifesto. Modeled on Marx and Engels' famous \"Communist Manifesto\" (1848), such manifestoes put forward ideas that were meant to provoke and to gather followers. However, arguments in favor of geometric or purely abstract painting were, at this time, largely confined to \"little magazines\" which had only tiny circulations. Modernist primitivism and pessimism were controversial, and the mainstream in the first decade of the 20th century was still inclined towards a faith in progress and liberal optimism.\nAbstract artists, taking as their examples the impressionists, as well as Paul Cézanne (1839–1906) and Edvard Munch (1863–1944), began with the assumption that color and shape, not the depiction of the natural world, formed the essential characteristics of art. Western art had been, from the Renaissance up to the middle of the 19th century, underpinned by the logic of perspective and an attempt to reproduce an illusion of visible reality. The arts of cultures other than the European had become accessible and showed alternative ways of describing visual experience to the artist. By the end of the 19th century many artists felt a need to create a new kind of art which would encompass the fundamental changes taking place in technology, science and philosophy. The sources from which individual artists drew their theoretical arguments were diverse, and reflected the social and intellectual preoccupations in all areas of Western culture at that time. Wassily Kandinsky, Piet Mondrian, and Kazimir Malevich all believed in redefining art as the arrangement of pure color. The use of photography, which had rendered much of the representational function of visual art obsolete, strongly affected this aspect of modernism.\n\nModernist architects and designers, such as Frank Lloyd Wright and Le Corbusier, believed that new technology rendered old styles of building obsolete. Le Corbusier thought that buildings should function as \"machines for living in\", analogous to cars, which he saw as machines for traveling in. Just as cars had replaced the horse, so modernist design should reject the old styles and structures inherited from Ancient Greece or from the Middle Ages. Following this machine aesthetic, modernist designers typically rejected decorative motifs in design, preferring to emphasize the materials used and pure geometrical forms. The skyscraper is the archetypal modernist building, and the Wainwright Building, a 10-story office building built 1890–91, in St. Louis, Missouri, United States, is among the first skyscrapers in the world. Ludwig Mies van der Rohe's Seagram Building in New York (1956–1958) is often regarded as the pinnacle of this modernist high-rise architecture. Many aspects of modernist design still persist within the mainstream of contemporary architecture, though previous dogmatism has given way to a more playful use of decoration, historical quotation, and spatial drama.\nIn 1913—which was the year of philosopher Edmund Husserl's \"Ideas\", physicist Niels Bohr's quantized atom, Ezra Pound's founding of imagism, the Armory Show in New York, and in Saint Petersburg the \"first futurist opera\", Mikhail Matyushin's \"Victory over the Sun\"—another Russian composer, Igor Stravinsky, composed \"The Rite of Spring\", a ballet that depicts human sacrifice, and has a musical score full of dissonance and primitive rhythm. This caused uproar on its first performance in Paris. At this time though modernism was still \"progressive\", increasingly it saw traditional forms and traditional social arrangements as hindering progress, and was recasting the artist as a revolutionary, engaged in overthrowing rather than enlightening society. Also in 1913 a less violent event occurred in France with the publication of the first volume of Marcel Proust's important novel sequence \"À la recherche du temps perdu\" (1913–1927) (\"In Search of Lost Time\"). This is often presented as an early example of a writer using the stream-of-consciousness technique, but Robert Humphrey comments that Proust \"is concerned only with the reminiscent aspect of consciousness\" and that he \"was deliberately recapturing the past for the purpose of communicating; hence he did not write a stream-of-consciousness novel.\"\n\nStream of consciousness was an important modernist literary innovation, and it has been suggested that Arthur Schnitzler (1862–1931) was the first to make full use of it in his short story \"Leutnant Gustl\" (\"None but the Brave\") (1900). Dorothy Richardson was the first English writer to use it, in the early volumes of her novel sequence \"Pilgrimage\" (1915–67). The other modernist novelists that are associated with the use of this narrative technique include James Joyce in \"Ulysses\" (1922) and Italo Svevo in \"La coscienza di Zeno\" (1923).\n\nHowever, with the coming of the Great War of 1914–18 and the Russian Revolution of 1917, the world was drastically changed and doubt cast on the beliefs and institutions of the past. The failure of the previous status quo seemed self-evident to a generation that had seen millions die fighting over scraps of earth: prior to 1914 it had been argued that no one would fight such a war, since the cost was too high. The birth of a machine age which had made major changes in the conditions of daily life in the 19th century now had radically changed the nature of warfare. The traumatic nature of recent experience altered basic assumptions, and realistic depiction of life in the arts seemed inadequate when faced with the fantastically surreal nature of trench warfare. The view that mankind was making steady moral progress now seemed ridiculous in the face of the senseless slaughter, described in works such as Erich Maria Remarque's novel \"All Quiet on the Western Front\" (1929). Therefore, modernism's view of reality, which had been a minority taste before the war, became more generally accepted in the 1920s.\n\nIn literature and visual art some Modernists sought to defy expectations mainly in order to make their art more vivid, or to force the audience to take the trouble to question their own preconceptions. This aspect of modernism has often seemed a reaction to consumer culture, which developed in Europe and North America in the late 19th century. Whereas most manufacturers try to make products that will be marketable by appealing to preferences and prejudices, high modernists rejected such consumerist attitudes in order to undermine conventional thinking. The art critic Clement Greenberg expounded this theory of modernism in his essay \"Avant-Garde and Kitsch\". Greenberg labeled the products of consumer culture \"kitsch\", because their design aimed simply to have maximum appeal, with any difficult features removed. For Greenberg, modernism thus formed a reaction against the development of such examples of modern consumer culture as commercial popular music, Hollywood, and advertising. Greenberg associated this with the revolutionary rejection of capitalism.\n\nSome Modernists saw themselves as part of a revolutionary culture that included political revolution. In Russia after the 1917 Revolution there was indeed initially a burgeoning of avant-garde cultural activity, which included Russian Futurism. However others rejected conventional politics as well as artistic conventions, believing that a revolution of political consciousness had greater importance than a change in political structures. But many modernists saw themselves as apolitical. Others, such as T. S. Eliot, rejected mass popular culture from a conservative position. Some even argue that modernism in literature and art functioned to sustain an elite culture which excluded the majority of the population.\n\nSurrealism, which originated in the early 1920s, came to be regarded by the public as the most extreme form of modernism, or \"the avant-garde of Modernism\". The word \"surrealist\" was coined by Guillaume Apollinaire and first appeared in the preface to his play \"Les Mamelles de Tirésias\", which was written in 1903 and first performed in 1917. Major surrealists include Paul Éluard, Robert Desnos, Max Ernst, Hans Arp, Antonin Artaud, Raymond Queneau, Joan Miró, and Marcel Duchamp.\n\nBy 1930, Modernism had won a place in the establishment, including the political and artistic establishment, although by this time Modernism itself had changed.\n\nModernism continued to evolve during the 1930s. Between 1930 and 1932 composer Arnold Schoenberg worked on \"Moses und Aron\", one of the first operas to make use of the twelve-tone technique, Pablo Picasso painted in 1937 \"Guernica\", his cubist condemnation of fascism, while in 1939 James Joyce pushed the boundaries of the modern novel further with \"Finnegans Wake\". Also by 1930 Modernism began to influence mainstream culture, so that, for example, \"The New Yorker\" magazine began publishing work, influenced by Modernism, by young writers and humorists like Dorothy Parker, Robert Benchley, E. B. White, S. J. Perelman, and James Thurber, amongst others. Perelman is highly regarded for his humorous short stories that he published in magazines in the 1930s and 1940s, most often in \"The New Yorker\", which are considered to be the first examples of surrealist humor in America. Modern ideas in art also began to appear more frequently in commercials and logos, an early example of which, from 1916, is the famous London Underground logo designed by Edward Johnston.\n\nOne of the most visible changes of this period was the adoption of new technologies into daily life of ordinary people in Western Europe and North America. Electricity, the telephone, the radio, the automobile—and the need to work with them, repair them and live with them—created social change. The kind of disruptive moment that only a few knew in the 1880s became a common occurrence. For example, the speed of communication reserved for the stock brokers of 1890 became part of family life, at least in middle class North America. Associated with urbanization and changing social mores also came smaller families and changed relationships between parents and their children.\nAnother strong influence at this time was Marxism. After the generally primitivistic/irrationalist aspect of pre-World War I Modernism, which for many Modernists precluded any attachment to merely political solutions, and the neoclassicism of the 1920s, as represented most famously by T. S. Eliot and Igor Stravinsky—which rejected popular solutions to modern problems—the rise of Fascism, the Great Depression, and the march to war helped to radicalise a generation. Bertolt Brecht, W. H. Auden, André Breton, Louis Aragon and the philosophers Antonio Gramsci and Walter Benjamin are perhaps the most famous exemplars of this Modernist form of Marxism. There were, however, also Modernists explicitly of 'the right', including Salvador Dalí, Wyndham Lewis, T. S. Eliot, Ezra Pound, the Dutch author Menno ter Braak and others.\n\nSignificant Modernist literary works continued to be created in the 1920s and 1930s, including further novels by Marcel Proust, Virginia Woolf, Robert Musil, and Dorothy Richardson. The American Modernist dramatist Eugene O'Neill's career began in 1914, but his major works appeared in the 1920s, 1930s and early 1940s. Two other significant Modernist dramatists writing in the 1920s and 1930s were Bertolt Brecht and Federico García Lorca. D. H. Lawrence's \"Lady Chatterley's Lover\" was privately published in 1928, while another important landmark for the history of the modern novel came with the publication of William Faulkner's \"The Sound and the Fury\" in 1929. In the 1930s, in addition to further major works by Faulkner, Samuel Beckett published his first major work, the novel \"Murphy\" (1938). Then in 1939 James Joyce's \"Finnegans Wake\" appeared. This is written in a largely idiosyncratic language, consisting of a mixture of standard English lexical items and neologistic multilingual puns and portmanteau words, which attempts to recreate the experience of sleep and dreams. In poetry T. S. Eliot, E. E. Cummings, and Wallace Stevens were writing from the 1920s until the 1950s. While Modernist poetry in English is often viewed as an American phenomenon, with leading exponents including Ezra Pound, T. S. Eliot, Marianne Moore, William Carlos Williams, H.D., and Louis Zukofsky, there were important British Modernist poets, including David Jones, Hugh MacDiarmid, Basil Bunting, and W. H. Auden. European Modernist poets include Federico García Lorca, Anna Akhmatova, Constantine Cavafy, and Paul Valéry.\nThe Modernist movement continued during this period in Soviet Russia. In 1930 composer Dimitri Shostakovich's (1906–1975) opera \"The Nose\" was premiered, in which he uses a montage of different styles, including folk music, popular song and atonality. Amongst his influences was Alban Berg's (1985–1935) opera \"Wozzeck\" (1925), which \"had made a tremendous impression on Shostakovich when it was staged in Leningrad.\" However, from 1932 Socialist realism began to oust Modernism in the Soviet Union, and in 1936 Shostakovich was attacked and forced to withdraw his 4th Symphony. Alban Berg wrote another significant, though incomplete, Modernist opera, \"Lulu\", which premiered in 1937. Berg's Violin Concerto was first performed in 1935. Like Shostakovich, other composers faced difficulties in this period. In Germany Arnold Schoenberg (1874–1951) was forced to flee to the U.S. when Hitler came to power in 1933, because of his Modernist atonal style as well as his Jewish ancestry. His major works from this period are a Violin Concerto, Op. 36 (1934/36), and Piano Concerto, Op. 42 (1942). Schoenberg also wrote tonal music in this period with the Suite for Strings in G major (1935) and the Chamber Symphony No. 2 in E minor, Op. 38 (begun in 1906, completed in 1939). During this time Hungarian Modernist Béla Bartók (1881–1945) produced a number of major works, including \"Music for Strings, Percussion and Celesta\" (1936) and the \"Divertimento for String Orchestra\" (1939), String Quartet No. 5 (1934), and No. 6 (his last, 1939). But he too left for the US in 1940, because of the rise of fascism in Hungary. Igor Stravinsky (1882–1971) continued writing in his neoclassical style during the 1930s and 1940s, writing works like the \"Symphony of Psalms\" (1930), Symphony in C (1940) and \"Symphony in Three Movements\" (1945). He also emigrated to the US because of World War II. Olivier Messiaen (1908–1992), however, served in the French army during the war and was imprisoned at Stalag VIII-A by the Germans, where he composed his famous \"Quatuor pour la fin du temps\" (\"Quartet for the End of Time\"). The quartet was first performed in January 1941 to an audience of prisoners and prison guards.\n\nIn painting, during the 1920s and the 1930s and the Great Depression, modernism is defined by Surrealism, late Cubism, Bauhaus, De Stijl, Dada, German Expressionism, and Modernist and masterful color painters like Henri Matisse and Pierre Bonnard as well as the abstractions of artists like Piet Mondrian and Wassily Kandinsky which characterized the European art scene. In Germany, Max Beckmann, Otto Dix, George Grosz and others politicized their paintings, foreshadowing the coming of World War II, while in America, modernism is seen in the form of American Scene painting and the social realism and regionalism movements that contained both political and social commentary dominated the art world. Artists like Ben Shahn, Thomas Hart Benton, Grant Wood, George Tooker, John Steuart Curry, Reginald Marsh, and others became prominent. Modernism is defined in Latin America by painters Joaquín Torres García from Uruguay and Rufino Tamayo from Mexico, while the muralist movement with Diego Rivera, David Siqueiros, José Clemente Orozco, Pedro Nel Gómez and Santiago Martinez Delgado, and Symbolist paintings by Frida Kahlo, began a renaissance of the arts for the region, characterized by a freer use of color and an emphasis on political messages.\n\nDiego Rivera is perhaps best known by the public world for his 1933 mural, \"Man at the Crossroads\", in the lobby of the RCA Building at Rockefeller Center. When his patron Nelson Rockefeller discovered that the mural included a portrait of Vladimir Lenin and other communist imagery, he fired Rivera, and the unfinished work was eventually destroyed by Rockefeller's staff. Frida Kahlo's (Rivera's wife's) works are often characterized by their stark portrayals of pain. Kahlo was deeply influenced by indigenous Mexican culture, which is apparent in her paintings' bright colors and dramatic symbolism. Christian and Jewish themes are often depicted in her work as well; she combined elements of the classic religious Mexican tradition, which were often bloody and violent. Frida Kahlo's Symbolist works relate strongly to Surrealism and to the Magic Realism movement in literature.\n\nPolitical activism was an important piece of David Siqueiros' life, and frequently inspired him to set aside his artistic career. His art was deeply rooted in the Mexican Revolution. The period from the 1920s to the 1950s is known as the Mexican Renaissance, and Siqueiros was active in the attempt to create an art that was at once Mexican and universal. The young Jackson Pollock attended the workshop and helped build floats for the parade.\n\nDuring the 1930s radical leftist politics characterized many of the artists connected to Surrealism, including Pablo Picasso. On 26 April 1937, during the Spanish Civil War, the Basque town of Gernika was bombed by Nazi Germany's Luftwaffe. The Germans were attacking to support the efforts of Francisco Franco to overthrow the Basque government and the Spanish Republican government. Pablo Picasso painted his mural-sized \"Guernica\" to commemorate the horrors of the bombing.\nDuring the Great Depression of the 1930s and through the years of World War II, American art was characterized by Social Realism and American Scene Painting, in the work of Grant Wood, Edward Hopper, Ben Shahn, Thomas Hart Benton, and several others. \"Nighthawks\" (1942) is a painting by Edward Hopper that portrays people sitting in a downtown diner late at night. It is not only Hopper's most famous painting, but one of the most recognizable in American art. The scene was inspired by a diner in Greenwich Village. Hopper began painting it immediately after the attack on Pearl Harbor. After this event there was a large feeling of gloominess over the country, a feeling that is portrayed in the painting. The urban street is empty outside the diner, and inside none of the three patrons is apparently looking or talking to the others but instead is lost in their own thoughts. This portrayal of modern urban life as empty or lonely is a common theme throughout Hopper's work.\n\n\"American Gothic\" is a painting by Grant Wood from 1930. Portraying a pitchfork-holding farmer and a younger woman in front of a house of Carpenter Gothic style, it is one of the most familiar images in 20th-century American art. Art critics had favorable opinions about the painting; like Gertrude Stein and Christopher Morley, they assumed the painting was meant to be a satire of rural small-town life. It was thus seen as part of the trend towards increasingly critical depictions of rural America, along the lines of Sherwood Anderson's 1919 \"Winesburg, Ohio\", Sinclair Lewis's 1920 \"Main Street\", and Carl Van Vechten's \"The Tattooed Countess\" in literature. However, with the onset of the Great Depression, the painting came to be seen as a depiction of steadfast American pioneer spirit.\n\nThe situation for artists in Europe during the 1930s deteriorated rapidly as the Nazis' power in Germany and across Eastern Europe increased. \"Degenerate art\" was a term adopted by the Nazi regime in Germany for virtually all modern art. Such art was banned on the grounds that it was un-German or Jewish Bolshevist in nature, and those identified as degenerate artists were subjected to sanctions. These included being dismissed from teaching positions, being forbidden to exhibit or to sell their art, and in some cases being forbidden to produce art entirely. Degenerate Art was also the title of an exhibition, mounted by the Nazis in Munich in 1937. The climate became so hostile for artists and art associated with modernism and abstraction that many left for the Americas. German artist Max Beckmann and scores of others fled Europe for New York. In New York City a new generation of young and exciting Modernist painters led by Arshile Gorky, Willem de Kooning, and others were just beginning to come of age.\n\nArshile Gorky's portrait of someone who might be Willem de Kooning is an example of the evolution of abstract expressionism from the context of figure painting, cubism and surrealism. Along with his friends de Kooning and John D. Graham, Gorky created biomorphically shaped and abstracted figurative compositions that by the 1940s evolved into totally abstract paintings. Gorky's work seems to be a careful analysis of memory, emotion and shape, using line and color to express feeling and nature.\n\nWhile \"The Oxford Encyclopedia of British Literature\" states that modernism ended by c. 1939 with regard to British and American literature, \"When (if) Modernism petered out and postmodernism began has been contested almost as hotly as when the transition from Victorianism to Modernism occurred.\" Clement Greenberg sees modernism ending in the 1930s, with the exception of the visual and performing arts, but with regard to music, Paul Griffiths notes that, while Modernism \"seemed to be a spent force\" by the late 1920s, after World War II, \"a new generation of composers—Boulez, Barraqué, Babbitt, Nono, Stockhausen, Xenakis\" revived modernism\". In fact many literary Modernists lived into the 1950s and 1960s, though generally they were no longer producing major works. The term \"late modernism\" is also sometimes applied to Modernist works published after 1930. Among Modernists (or late Modernists) still publishing after 1945 were Wallace Stevens, Gottfried Benn, T. S. Eliot, Anna Akhmatova, William Faulkner, Dorothy Richardson, John Cowper Powys, and Ezra Pound. Basil Bunting, born in 1901, published his most important Modernist poem \"Briggflatts\" in 1965. In addition, Hermann Broch's \"The Death of Virgil\" was published in 1945 and Thomas Mann's \"Doctor Faustus\" in 1947. Samuel Beckett, who died in 1989, has been described as a \"later Modernist\". Beckett is a writer with roots in the expressionist tradition of Modernism, who produced works from the 1930s until the 1980s, including \"Molloy\" (1951), \"Waiting for Godot\" (1953), \"Happy Days\" (1961), and \"Rockaby\" (1981). The terms \"minimalist\" and \"post-Modernist\" have also been applied to his later works. The poets Charles Olson (1910–1970) and J. H. Prynne (born 1936) are among the writers in the second half of the 20th century who have been described as late Modernists.\n\nMore recently the term \"late modernism\" has been redefined by at least one critic and used to refer to works written after 1945, rather than 1930. With this usage goes the idea that the ideology of modernism was significantly re-shaped by the events of World War II, especially the Holocaust and the dropping of the atom bomb.\n\nThe postwar period left the capitals of Europe in upheaval with an urgency to economically and physically rebuild and to politically regroup. In Paris (the former center of European culture and the former capital of the art world) the climate for art was a disaster. Important collectors, dealers, and Modernist artists, writers, and poets had fled Europe for New York and America. The surrealists and modern artists from every cultural center of Europe had fled the onslaught of the Nazis for safe haven in the United States. Many of those who didn't flee perished. A few artists, notably Pablo Picasso, Henri Matisse, and Pierre Bonnard, remained in France and survived.\n\nThe 1940s in New York City heralded the triumph of American abstract expressionism, a Modernist movement that combined lessons learned from Henri Matisse, Pablo Picasso, surrealism, Joan Miró, cubism, Fauvism, and early modernism via great teachers in America like Hans Hofmann and John D. Graham. American artists benefited from the presence of Piet Mondrian, Fernand Léger, Max Ernst and the André Breton group, Pierre Matisse's gallery, and Peggy Guggenheim's gallery \"The Art of This Century\", as well as other factors.\n\nParis, moreover, recaptured much of its luster in the 1950s and 60s as the center of a machine art florescence, with both of the leading machine art sculptors Jean Tinguely and Nicolas Schöffer having moved there to launch their careers—and which florescence, in light of the technocentric character of modern life, may well have a particularly long lasting influence.\n\nThe term \"Theatre of the Absurd\" is applied to plays, written primarily by Europeans, that express the belief that human existence has no meaning or purpose and therefore all communication breaks down. Logical construction and argument gives way to irrational and illogical speech and to its ultimate conclusion, silence. While there are significant precursors, including Alfred Jarry (1873–1907), the Theatre of the Absurd is generally seen as beginning in the 1950s with the plays of Samuel Beckett.\n\nCritic Martin Esslin coined the term in his 1960 essay \"Theatre of the Absurd\". He related these plays based on a broad theme of the Absurd, similar to the way Albert Camus uses the term in his 1942 essay, \"The Myth of Sisyphus\". The Absurd in these plays takes the form of man's reaction to a world apparently without meaning, and/or man as a puppet controlled or menaced by invisible outside forces. Though the term is applied to a wide range of plays, some characteristics coincide in many of the plays: broad comedy, often similar to vaudeville, mixed with horrific or tragic images; characters caught in hopeless situations forced to do repetitive or meaningless actions; dialogue full of clichés, wordplay, and nonsense; plots that are cyclical or absurdly expansive; either a parody or dismissal of realism and the concept of the \"well-made play\".\n\nPlaywrights commonly associated with the Theatre of the Absurd include Samuel Beckett (1906–1989), Eugène Ionesco (1909–1994), Jean Genet (1910–1986), Harold Pinter (1930–2008), Tom Stoppard (born 1937), Alexander Vvedensky (1904–1941), Daniil Kharms (1905–1942), Friedrich Dürrenmatt (1921–1990), Alejandro Jodorowsky (born 1929), Fernando Arrabal (born 1932), Václav Havel (1936–2011) and Edward Albee (1928–2016).\nDuring the late 1940s Jackson Pollock's radical approach to painting revolutionized the potential for all contemporary art that followed him. To some extent Pollock realized that the journey toward making a work of art was as important as the work of art itself. Like Pablo Picasso's innovative reinventions of painting and sculpture in the early 20th century via Cubism and constructed sculpture, Pollock redefined the way art is made. His move away from easel painting and conventionality was a liberating signal to the artists of his era and to all who came after. Artists realized that Jackson Pollock's process—placing unstretched raw canvas on the floor where it could be attacked from all four sides using artistic and industrial materials; dripping and throwing linear skeins of paint; drawing, staining, and brushing; using imagery and nonimagery—essentially blasted artmaking beyond any prior boundary. Abstract expressionism generally expanded and developed the definitions and possibilities available to artists for the creation of new works of art.\nThe other abstract expressionists followed Pollock's breakthrough with new breakthroughs of their own. In a sense the innovations of Jackson Pollock, Willem de Kooning, Franz Kline, Mark Rothko, Philip Guston, Hans Hofmann, Clyfford Still, Barnett Newman, Ad Reinhardt, Robert Motherwell, Peter Voulkos and others opened the floodgates to the diversity and scope of all the art that followed them. Rereadings into abstract art by art historians such as Linda Nochlin, Griselda Pollock and Catherine de Zegher critically show, however, that pioneering women artists who produced major innovations in modern art had been ignored by official accounts of its history.\n\nHenry Moore (1898–1986) emerged after World War II as Britain's leading sculptor. He was best known for his semi-abstract monumental bronze sculptures which are located around the world as public works of art. His forms are usually abstractions of the human figure, typically depicting mother-and-child or reclining figures, usually suggestive of the female body, apart from a phase in the 1950s when he sculpted family groups. His forms are generally pierced or contain hollow spaces.\nIn the 1950s, Moore began to receive increasingly significant commissions, including a reclining figure for the UNESCO building in Paris in 1958. With many more public works of art, the scale of Moore's sculptures grew significantly. The last three decades of Moore's life continued in a similar vein, with several major retrospectives taking place around the world, notably a prominent exhibition in the summer of 1972 in the grounds of the Forte di Belvedere overlooking Florence. By the end of the 1970s, there were some 40 exhibitions a year featuring his work. On the campus of the University of Chicago in December 1967, 25 years to the minute after the team of physicists led by Enrico Fermi achieved the first controlled, self-sustaining nuclear chain reaction, Moore's \"Nuclear Energy\" was unveiled. Also in Chicago, Moore commemorated science with a large bronze sundial, locally named \"Man Enters the Cosmos\" (1980), which was commissioned to recognise the space exploration program.\n\nThe \"London School\" of figurative painters, including Francis Bacon (1909–1992), Lucian Freud (1922–2011), Frank Auerbach (born 1931), Leon Kossoff (born 1926), and Michael Andrews (1928–1995), have received widespread international recognition.\n\nFrancis Bacon was an Irish-born British figurative painter known for his bold, graphic and emotionally raw imagery. His painterly but abstracted figures typically appear isolated in glass or steel geometrical cages set against flat, nondescript backgrounds. Bacon began painting during his early 20s but worked only sporadically until his mid-30s. His breakthrough came with the 1944 triptych \"Three Studies for Figures at the Base of a Crucifixion\" which sealed his reputation as a uniquely bleak chronicler of the human condition. His output can be crudely described as consisting of sequences or variations on a single motif; beginning with the 1940s male heads isolated in rooms, the early 1950s screaming popes, and mid to late 1950s animals and lone figures suspended in geometric structures. These were followed by his early 1960s modern variations of the crucifixion in the triptych format. From the mid-1960s to early 1970s, Bacon mainly produced strikingly compassionate portraits of friends. Following the suicide of his lover George Dyer in 1971, his art became more personal, inward-looking, and preoccupied with themes and motifs of death. During his lifetime, Bacon was equally reviled and acclaimed.\n\nLucian Freud was a German-born British painter, known chiefly for his thickly impastoed portrait and figure paintings, who was widely considered the pre-eminent British artist of his time. His works are noted for their psychological penetration, and for their often discomforting examination of the relationship between artist and model. According to William Grimes of \"The New York Times\", \"Lucien Freud and his contemporaries transformed figure painting in the 20th century. In paintings like \"Girl with a White Dog\" (1951–52), Freud put the pictorial language of traditional European painting in the service of an anti-romantic, confrontational style of portraiture that stripped bare the sitter's social facade. Ordinary people—many of them his friends—stared wide-eyed from the canvas, vulnerable to the artist's ruthless inspection.\"\n\nIn abstract painting during the 1950s and 1960s several new directions like hard-edge painting and other forms of geometric abstraction began to appear in artist studios and in radical avant-garde circles as a reaction against the subjectivism of abstract expressionism. Clement Greenberg became the voice of post-painterly abstraction when he curated an influential exhibition of new painting that toured important art museums throughout the United States in 1964. Color Field painting, hard-edge painting and lyrical abstraction emerged as radical new directions.\n\nBy the late 1960s however, postminimalism, process art and Arte Povera also emerged as revolutionary concepts and movements that encompassed both painting and sculpture, via lyrical abstraction and the postminimalist movement, and in early conceptual art. Process art as inspired by Pollock enabled artists to experiment with and make use of a diverse encyclopedia of style, content, material, placement, sense of time, and plastic and real space. Nancy Graves, Ronald Davis, Howard Hodgkin, Larry Poons, Jannis Kounellis, Brice Marden, Colin McCahon, Bruce Nauman, Richard Tuttle, Alan Saret, Walter Darby Bannard, Lynda Benglis, Dan Christensen, Larry Zox, Ronnie Landfield, Eva Hesse, Keith Sonnier, Richard Serra, Sam Gilliam, Mario Merz and Peter Reginato were some of the younger artists who emerged during the era of late modernism that spawned the heyday of the art of the late 1960s.\n\nIn 1962 the Sidney Janis Gallery mounted \"The New Realists\", the first major pop art group exhibition in an uptown art gallery in New York City. Janis mounted the exhibition in a 57th Street storefront near his gallery. The show sent shockwaves through the New York School and reverberated worldwide. Earlier in England in 1958 the term \"Pop Art\" was used by Lawrence Alloway to describe paintings that celebrated the consumerism of the post World War II era. This movement rejected abstract expressionism and its focus on the hermeneutic and psychological interior in favor of art that depicted and often celebrated material consumer culture, advertising, and iconography of the mass production age. The early works of David Hockney and the works of Richard Hamilton and Eduardo Paolozzi (who created the groundbreaking \"I was a Rich Man's Plaything\", 1947) are considered seminal examples in the movement. Meanwhile, in the downtown scene in New York's East Village 10th Street galleries, artists were formulating an American version of pop art. Claes Oldenburg had his storefront, and the Green Gallery on 57th Street began to show the works of Tom Wesselmann and James Rosenquist. Later Leo Castelli exhibited the works of other American artists, including those of Andy Warhol and Roy Lichtenstein for most of their careers. There is a connection between the radical works of Marcel Duchamp and Man Ray, the rebellious Dadaists with a sense of humor, and pop artists like Claes Oldenburg, Andy Warhol, and Roy Lichtenstein, whose paintings reproduce the look of Ben-Day dots, a technique used in commercial reproduction.\n\nMinimalism describes movements in various forms of art and design, especially visual art and music, wherein artists intend to expose the essence or identity of a subject through eliminating all nonessential forms, features, or concepts. Minimalism is any design or style wherein the simplest and fewest elements are used to create the maximum effect.\n\nAs a specific movement in the arts it is identified with developments in post–World War II Western art, most strongly with American visual arts in the 1960s and early 1970s. Prominent artists associated with this movement include Donald Judd, John McCracken, Agnes Martin, Dan Flavin, Robert Morris, Ronald Bladen, Anne Truitt, and Frank Stella. It derives from the reductive aspects of modernism and is often interpreted as a reaction against Abstract expressionism and a bridge to Postminimal art practices. By the early 1960s minimalism emerged as an abstract movement in art (with roots in the geometric abstraction of Kazimir Malevich, the Bauhaus and Piet Mondrian) that rejected the idea of relational and subjective painting, the complexity of abstract expressionist surfaces, and the emotional zeitgeist and polemics present in the arena of action painting. Minimalism argued that extreme simplicity could capture all of the sublime representation needed in art. Minimalism is variously construed either as a precursor to postmodernism, or as a postmodern movement itself. In the latter perspective, early minimalism yielded advanced Modernist works, but the movement partially abandoned this direction when some artists like Robert Morris changed direction in favor of the anti-form movement.\n\nHal Foster, in his essay \"The Crux of Minimalism\", examines the extent to which Donald Judd and Robert Morris both acknowledge and exceed Greenbergian Modernism in their published definitions of minimalism. He argues that minimalism is not a \"dead end\" of Modernism, but a \"paradigm shift toward postmodern practices that continue to be elaborated today.\"\n\nThe terms have expanded to encompass a movement in music that features such repetition and iteration as those of the compositions of La Monte Young, Terry Riley, Steve Reich, Philip Glass, and John Adams. Minimalist compositions are sometimes known as systems music. The term \"minimalist\" often colloquially refers to anything that is spare or stripped to its essentials. It has also been used to describe the plays and novels of Samuel Beckett, the films of Robert Bresson, the stories of Raymond Carver, and the automobile designs of Colin Chapman.\n\nIn the late 1960s Robert Pincus-Witten coined the term \"postminimalism\" to describe minimalist-derived art which had content and contextual overtones that minimalism rejected. The term was applied by Pincus-Whitten to the work of Eva Hesse, Keith Sonnier, Richard Serra and new work by former minimalists Robert Smithson, Robert Morris, Sol LeWitt, Barry Le Va, and others. Other minimalists including Donald Judd, Dan Flavin, Carl Andre, Agnes Martin, John McCracken and others continued to produce late Modernist paintings and sculpture for the remainders of their careers.\n\nSince then, many artists have embraced minimal or postminimal styles, and the label \"Postmodern\" has been attached to them.\n\nRelated to abstract expressionism was the emergence of combining manufactured items with artist materials, moving away from previous conventions of painting and sculpture. The work of Robert Rauschenberg exemplifies this trend. His \"combines\" of the 1950s were forerunners of pop art and installation art, and used assemblages of large physical objects, including stuffed animals, birds and commercial photographs. Rauschenberg, Jasper Johns, Larry Rivers, John Chamberlain, Claes Oldenburg, George Segal, Jim Dine, and Edward Kienholz were among important pioneers of both abstraction and pop art. Creating new conventions of art-making, they made acceptable in serious contemporary art circles the radical inclusion in their works of unlikely materials. Another pioneer of collage was Joseph Cornell, whose more intimately scaled works were seen as radical because of both his personal iconography and his use of found objects.\n\nIn the early 20th century Marcel Duchamp submitted for exhibition a urinal as a sculpture. He professed his intent that people look at the urinal as if it were a work of art because he said it was a work of art. He referred to his work as \"readymades\". \"Fountain\" was a urinal signed with the pseudonym \"R. Mutt\", the exhibition of which shocked the art world in 1917. This and Duchamp's other works are generally labelled as Dada. Duchamp can be seen as a precursor to conceptual art, other famous examples being John Cage's \"4'33\"\", which is four minutes and thirty three seconds of silence, and Rauschenberg's \"Erased de Kooning Drawing\". Many conceptual works take the position that art is the result of the viewer viewing an object or act as art, not of the intrinsic qualities of the work itself. In choosing \"an ordinary article of life\" and creating \"a new thought for that object\" Duchamp invited onlookers to view \"Fountain\" as a sculpture.\n\nMarcel Duchamp famously gave up \"art\" in favor of chess. Avant-garde composer David Tudor created a piece, \"Reunion\" (1968), written jointly with Lowell Cross, that features a chess game in which each move triggers a lighting effect or projection. Duchamp and Cage played the game at the work's premier.\n\nSteven Best and Douglas Kellner identify Rauschenberg and Jasper Johns as part of the transitional phase, influenced by Duchamp, between Modernism and Postmodernism. Both used images of ordinary objects, or the objects themselves, in their work, while retaining the abstraction and painterly gestures of high Modernism.\n\nDuring the late 1950s and 1960s artists with a wide range of interests began to push the boundaries of contemporary art. Yves Klein in France, Carolee Schneemann, Yayoi Kusama, Charlotte Moorman and Yoko Ono in New York City, and Joseph Beuys, Wolf Vostell and Nam June Paik in Germany were pioneers of performance-based works of art. Groups like The Living Theatre with Julian Beck and Judith Malina collaborated with sculptors and painters creating environments, radically changing the relationship between audience and performer, especially in their piece \"Paradise Now\". The Judson Dance Theater, located at the Judson Memorial Church, New York; and the Judson dancers, notably Yvonne Rainer, Trisha Brown, Elaine Summers, Sally Gross, Simonne Forti, Deborah Hay, Lucinda Childs, Steve Paxton and others; collaborated with artists Robert Morris, Robert Whitman, John Cage, Robert Rauschenberg, and engineers like Billy Klüver. Park Place Gallery was a center for musical performances by electronic composers Steve Reich, Philip Glass, and other notable performance artists including Joan Jonas.\n\nThese performances were intended as works of a new art form combining sculpture, dance, and music or sound, often with audience participation. They were characterized by the reductive philosophies of minimalism and the spontaneous improvisation and expressivity of abstract expressionism. Images of Schneeman's performances of pieces meant to shock are occasionally used to illustrate these kinds of art, and she is often seen photographed while performing her piece \"Interior Scroll\". However, the images of her performing this piece are illustrating precisely what performance art is not. In performance art, the performance itself is the medium. Other media cannot illustrate performance art. Performance art is performed, not captured. By its nature performance is momentary and evanescent, which is part of the point of the medium as art. Representations of performance art in other media, whether by image, video, narrative or otherwise, select certain points of view in space or time or otherwise involve the inherent limitations of each medium, and which therefore cannot truly illustrate the medium of performance as art.\n\nDuring the same period, various avant-garde artists created Happenings, mysterious and often spontaneous and unscripted gatherings of artists and their friends and relatives in various specified locations, often incorporating exercises in absurdity, physicality, costuming, spontaneous nudity, and various random or seemingly disconnected acts. Notable creators of happenings included Allan Kaprow—who first used the term in 1958, Claes Oldenburg, Jim Dine, Red Grooms, and Robert Whitman.\n\nAnother trend in art which has been associated with the term postmodern is the use of a number of different media together. Intermedia is a term coined by Dick Higgins and meant to convey new art forms along the lines of Fluxus, concrete poetry, found objects, performance art, and computer art. Higgins was the publisher of the Something Else Press, a concrete poet married to artist Alison Knowles and an admirer of Marcel Duchamp. Ihab Hassan includes \"Intermedia, the fusion of forms, the confusion of realms,\" in his list of the characteristics of postmodern art. One of the most common forms of \"multi-media art\" is the use of video-tape and CRT monitors, termed video art. While the theory of combining multiple arts into one art is quite old, and has been revived periodically, the postmodern manifestation is often in combination with performance art, where the dramatic subtext is removed, and what is left is the specific statements of the artist in question or the conceptual statement of their action.\n\nFluxus was named and loosely organized in 1962 by George Maciunas (1931–1978), a Lithuanian-born American artist. Fluxus traces its beginnings to John Cage's 1957 to 1959 Experimental Composition classes at the New School for Social Research in New York City. Many of his students were artists working in other media with little or no background in music. Cage's students included Fluxus founding members Jackson Mac Low, Al Hansen, George Brecht and Dick Higgins.\n\nFluxus encouraged a do-it-yourself aesthetic and valued simplicity over complexity. Like Dada before it, Fluxus included a strong current of anti-commercialism and an anti-art sensibility, disparaging the conventional market-driven art world in favor of an artist-centered creative practice. Fluxus artists preferred to work with whatever materials were at hand, and either created their own work or collaborated in the creation process with their colleagues.\n\nAndreas Huyssen criticises attempts to claim Fluxus for Postmodernism as \"either the master-code of postmodernism or the ultimately unrepresentable art movement—as it were, postmodernism's sublime.\" Instead he sees Fluxus as a major Neo-Dadaist phenomena within the avant-garde tradition. It did not represent a major advance in the development of artistic strategies, though it did express a rebellion against \"the administered culture of the 1950s, in which a moderate, domesticated modernism served as ideological prop to the Cold War.\"\n\nThe continuation of abstract expressionism, color field painting, lyrical abstraction, geometric abstraction, minimalism, abstract illusionism, process art, pop art, postminimalism, and other late 20th-century Modernist movements in both painting and sculpture continued through the first decade of the 21st century and constitute radical new directions in those mediums.\n\nAt the turn of the 21st century, well-established artists such as Sir Anthony Caro, Lucian Freud, Cy Twombly, Robert Rauschenberg, Jasper Johns, Agnes Martin, Al Held, Ellsworth Kelly, Helen Frankenthaler, Frank Stella, Kenneth Noland, Jules Olitski, Claes Oldenburg, Jim Dine, James Rosenquist, Alex Katz, Philip Pearlstein, and younger artists including Brice Marden, Chuck Close, Sam Gilliam, Isaac Witkin, Sean Scully, Mahirwan Mamtani, Joseph Nechvatal, Elizabeth Murray, Larry Poons, Richard Serra, Walter Darby Bannard, Larry Zox, Ronnie Landfield, Ronald Davis, Dan Christensen, Joel Shapiro, Tom Otterness, Joan Snyder, Ross Bleckner, Archie Rand, Susan Crile, and others continued to produce vital and influential paintings and sculpture.\n\nSee also and Hanshinkan Modernism\n\nPeter Kalliney suggests that,\"Modernist concepts, especially aesthetic autonomy, were fundamental to the literature of decolonization in anglophone Africa.\" In his opinion, Rajat Neogy, Christopher Okigbo, and Wole Soyinka, were among the writer who \"repurposed modernist versions of aesthetic autonomy to declare their freedom from colonial bondage, from systems of racial discrimination, and even from the new postcolonial state\".\n\nThe terms \"modernism\" and \"modernist\", according to scholar William J. Tyler, \"have only recently become part of the standard discourse in English on modern Japanese literature and doubts concerning their authenticity vis-a-vis Western European modernism remain\". Tyler finds this odd, given \"the decidedly modern prose\" of such \"well-known Japanese writers as Kawabata Yasunari, Nagai Kafu, and Jun'ichirō Tanizaki\". However, \"scholars in the visual and fine arts, architecture, and poetry readily embraced \"modanizumu\" as a key concept for describing and analyzing Japanese culture in the 1920s and 1930s\". In 1924, various young Japanese writers, including Kawabata Yasunari, Riichi Yokomitsu started a literary journal \"Bungei Jidai\" (\"The Artistic Age\"). This journal was \"part of an 'art for art's sake' movement, influenced by European Cubism, Expressionism, Dada, and other modernist styles\".\n\nJapanese modernist architect Kenzō Tange (1913–2005) was one of the most significant architects of the 20th century, combining traditional Japanese styles with modernism, and designed major buildings on five continents. Tange was also an influential patron of the Metabolist movement. He said: \"It was, I believe, around 1959 or at the beginning of the sixties that I began to think about what I was later to call structuralism\", He was influenced from an early age by the Swiss modernist, Le Corbusier, Tange gained international recognition in 1949 when he won the competition for the design of Hiroshima Peace Memorial Park.\n\nIn China the \"New Sensationists\" (新感觉派, Xīn Gǎnjué Pài) were a group of writers based in Shanghai who in the 1930s and 1940s were influenced, to varying degrees, by Western and Japanese modernism. They wrote fiction that was more concerned with the unconscious and with aesthetics than with politics or social problems. Among these writers were Mu Shiying, Liu Na'ou, and Shi Zhecun.\n\nIn India, the Progressive Artists' Group was a group of modern artists, mainly based in Mumbai, India formed in 1947. Though it lacked any particular style, it synthesised Indian art with European and North America influences from the first half of the 20th Century, including Post-Impressionism, Cubism and Expressionism.\n\nBy the early 1980s the Postmodern movement in art and architecture began to establish its position through various conceptual and intermedia formats. Postmodernism in music and literature began to take hold earlier. In music, postmodernism is described in one reference work, as a \"term introduced in the 1970s\", while in British literature, \"The Oxford Encyclopedia of British Literature\" sees modernism \"ceding its predominance to postmodernism\" as early as 1939. However, dates are highly debatable, especially as according to Andreas Huyssen: \"one critic's postmodernism is another critic's modernism.\" This includes those who are critical of the division between the two and see them as two aspects of the same movement, and believe that late Modernism continues.\n\nModernism is an encompassing label for a wide variety of cultural movements. Postmodernism is essentially a centralized movement that named itself, based on sociopolitical theory, although the term is now used in a wider sense to refer to activities from the 20th century onwards which exhibit awareness of and reinterpret the modern.\n\nPostmodern theory asserts that the attempt to canonise Modernism \"after the fact\" is doomed to undisambiguable contradictions.\n\nIn a narrower sense, what was Modernist was not necessarily also postmodern. Those elements of Modernism which accentuated the benefits of rationality and socio-technological progress were only Modernist.\n\nModernism's stress on freedom of expression, experimentation, radicalism, and primitivism disregards conventional expectations. In many art forms this often meant startling and alienating audiences with bizarre and unpredictable effects, as in the strange and disturbing combinations of motifs in Surrealism or the use of extreme dissonance and atonality in Modernist music. In literature this often involved the rejection of intelligible plots or characterization in novels, or the creation of poetry that defied clear interpretation.\n\nFrom 1932 Socialist realism began to oust Modernism in the Soviet Union; it had previously endorsed Futurism and Constructivism. The Nazi government of Germany deemed modernism narcissistic and nonsensical, as well as \"Jewish\" (see Antisemitism) and \"Negro\". The Nazis exhibited Modernist paintings alongside works by the mentally ill in an exhibition entitled \"Degenerate Art\". Accusations of \"formalism\" could lead to the end of a career, or worse. For this reason many Modernists of the postwar generation felt that they were the most important bulwark against totalitarianism, the \"canary in the coal mine\", whose repression by a government or other group with supposed authority represented a warning that individual liberties were being threatened. Louis A. Sass compared madness, specifically schizophrenia, and modernism in a less fascist manner by noting their shared disjunctive narratives, surreal images, and incoherence.\n\nIn fact, modernism flourished mainly in consumer/capitalist societies, despite the fact that its proponents often rejected consumerism itself. However, high modernism began to merge with consumer culture after World War II, especially during the 1960s. In Britain, a youth subculture emerged calling itself \"Modernist\" (usually shortened to Mod), following such representative music groups as the Who and the Kinks. The likes of Bob Dylan, Serge Gainsbourg and the Rolling Stones combined popular musical traditions with Modernist verse, adopting literary devices derived from James Joyce, Samuel Beckett, James Thurber, T. S. Eliot, Guillaume Apollinaire, Allen Ginsberg, and others. The Beatles developed along similar lines, creating various Modernist musical effects on several albums, while musicians such as Frank Zappa, Syd Barrett and Captain Beefheart proved even more experimental. Modernist devices also started to appear in popular cinema, and later on in music videos. Modernist design also began to enter the mainstream of popular culture, as simplified and stylized forms became popular, often associated with dreams of a space age high-tech future.\n\nThis merging of consumer and high versions of Modernist culture led to a radical transformation of the meaning of \"Modernism\". First, it implied that a movement based on the rejection of tradition had become a tradition of its own. Second, it demonstrated that the distinction between elite Modernist and mass consumerist culture had lost its precision. Some writers declared that modernism had become so institutionalized that it was now \"post avant-garde\", indicating that it had lost its power as a revolutionary movement. Many have interpreted this transformation as the beginning of the phase that became known as postmodernism. For others, such as art critic Robert Hughes, postmodernism represents an extension of modernism.\n\n\"Anti-modern\" or \"counter-modern\" movements seek to emphasize holism, connection and spirituality as remedies or antidotes to modernism. Such movements see modernism as reductionist, and therefore subject to an inability to see systemic and emergent effects. Many Modernists came to this viewpoint, for example Paul Hindemith in his late turn towards mysticism. Writers such as Paul H. Ray and Sherry Ruth Anderson, in \"\" (2000), Fredrick Turner in \"A Culture of Hope\" and Lester Brown in \"Plan B\", have articulated a critique of the basic idea of modernism itself—that individual creative expression should conform to the realities of technology. Instead, they argue, individual creativity should make everyday life more emotionally acceptable.\n\nSome traditionalist artists like Alexander Stoddart reject modernism generally as the product of \"an epoch of false money allied with false culture\".\n\nIn some fields, the effects of modernism have remained stronger and more persistent than in others. Visual art has made the most complete break with its past. Most major capital cities have museums devoted to modern art as distinct from post-Renaissance art (c. 1400 to c. 1900). Examples include the Museum of Modern Art in New York, the Tate Modern in London, and the Centre Pompidou in Paris. These galleries make no distinction between modernist and Postmodernist phases, seeing both as developments within Modern Art.\n\n\n"}
{"id": "19548", "url": "https://en.wikipedia.org/wiki?curid=19548", "title": "Marshall McLuhan", "text": "Marshall McLuhan\n\nHerbert Marshall McLuhan (; July 21, 1911December 31, 1980) was a Canadian professor, philosopher, and public intellectual. His work is one of the cornerstones of the study of media theory. Born in Edmonton, Alberta, McLuhan studied at the University of Manitoba and the University of Cambridge. He began his teaching career as a professor of English at several universities in the U.S. and Canada before moving to the University of Toronto in 1946, where he remained for the rest of his life.\n\nMcLuhan is known for coining the expression \"the medium is the message\" and the term global village, and for predicting the World Wide Web almost 30 years before it was invented. He was a fixture in media discourse in the late 1960s, though his influence began to wane in the early 1970s. In the years after his death, he continued to be a controversial figure in academic circles. With the arrival of the Internet and the World Wide Web interest was renewed in his work and perspective.\n\nMcLuhan was born on July 21, 1911, in Edmonton, Alberta, to Elsie Naomi (née Hall) and Herbert Ernest McLuhan, both born in Canada. His brother Maurice was born two years later. \"Marshall\" was his maternal grandmother's surname. His mother was a Baptist school teacher who later became an actress; his father was a Methodist and had a real estate business in Edmonton. That business failed when World War I broke out, and McLuhan's father enlisted in the Canadian army. After a year of service, he contracted influenza and remained in Canada, away from the front lines. After his discharge from the army in 1915, the McLuhan family moved to Winnipeg, Manitoba, where Marshall grew up and went to school, attending Kelvin Technical School before enrolling in the University of Manitoba in 1928.\n\nAt Manitoba, McLuhan explored his conflicted relationship with religion and turned to literature to \"gratify his soul's hunger for truth and beauty,\" later referring to this stage as agnosticism. After studying for one year as an engineering student, he changed majors and earned a BA (1933), winning a University Gold Medal in Arts and Sciences. He took an MA (1934) in English from the University of Manitoba in 1934. He had long desired to pursue graduate studies in England and was accepted to the University of Cambridge, having failed to secure a Rhodes scholarship to Oxford.\n\nHe had already earned a BA and an MA degree at Manitoba, but Cambridge required him to enroll as an undergraduate \"affiliated\" student, with one year's credit towards a three-year bachelor's degree, before entering any doctoral studies. He entered Trinity Hall, Cambridge in the autumn of 1934, where he studied under I. A. Richards and F. R. Leavis and was influenced by New Criticism. Upon reflection years afterward, he credited the faculty there with influencing the direction of his later work because of their emphasis on the \"training of perception\" and such concepts as Richards' notion of \"feedforward\". These studies formed an important precursor to his later ideas on technological forms. He received the required bachelor's degree from Cambridge in 1936 and entered their graduate program. Later, he returned from England to take a job as a teaching assistant at the University of Wisconsin–Madison that he held for the 1936–37 academic year, being unable to find a suitable job in Canada.\n\nWhile studying the trivium at Cambridge, he took the first steps toward his eventual conversion to Roman Catholicism in 1937, founded on his reading of G. K. Chesterton. In 1935, he wrote to his mother: \"[H]ad I not encountered Chesterton, I would have remained agnostic for many years at least.\" At the end of March 1937, McLuhan completed what was a slow but total conversion process, when he was formally received into the Roman Catholic Church. After consulting a minister, his father accepted the decision to convert. His mother, however, felt that his conversion would hurt his career and was inconsolable. McLuhan was devout throughout his life, but his religion remained a private matter. He had a lifelong interest in the number three (e.g., the trivium, the Trinity) and sometimes said that the Virgin Mary provided intellectual guidance for him. For the rest of his career, he taught in Roman Catholic institutions of higher education. From 1937 to 1944, he taught English at Saint Louis University (with an interruption from 1939–40 when he returned to Cambridge). There he taught courses on Shakespeare and tutored and befriended Walter J. Ong, who went on to write his PhD dissertation on a topic that McLuhan had called to his attention, and who also became a well-known authority on communication and technology.\n\nMcLuhan met Corinne Lewis in St. Louis, a teacher and aspiring actress from Fort Worth, Texas, and they were married on August 4, 1939. They spent 1939–40 in Cambridge, where he completed his master's degree (awarded in January 1940) and began to work on his doctoral dissertation on Thomas Nashe and the verbal arts. While the McLuhans were in England, war had broken out in Europe. For this reason, he obtained permission to complete and submit his dissertation from the United States, without having to return to Cambridge for an oral defence. In 1940, the McLuhans returned to Saint Louis University, where he continued teaching and they started a family. He was awarded a Ph.D. in December 1943. He next taught at Assumption College in Windsor, Ontario from 1944 to 1946, then moved to Toronto in 1946 where he joined the faculty of St. Michael's College, a Catholic college of the University of Toronto. Hugh Kenner was one of his students and Canadian economist and communications scholar Harold Innis was a university colleague who had a strong influence on his work. McLuhan wrote in 1964: \"I am pleased to think of my own book \"The Gutenberg Galaxy\" as a footnote to the observations of Innis on the subject of the psychic and social consequences, first of writing then of printing.\"\n\nIn the early 1950s, McLuhan began the Communication and Culture seminars at the University of Toronto, funded by the Ford Foundation. As his reputation grew, he received a growing number of offers from other universities and, to keep him, the university created the Centre for Culture and Technology in 1963. He published his first major work during this period: \"\" (1951). The work was an examination of the effect of advertising on society and culture. He and Edmund Carpenter also produced an important journal called \"Explorations\" throughout the 1950s. McLuhan and Carpenter have been characterized as the Toronto School of communication theory, together with Harold Innis, Eric A. Havelock, and Northrop Frye. During this time, McLuhan supervised the doctoral thesis of modernist writer Sheila Watson on the subject of Wyndham Lewis. He remained at the University of Toronto through 1979, spending much of this time as head of his Centre for Culture and Technology.\n\nMcLuhan was named to the Albert Schweitzer Chair in Humanities at Fordham University in the Bronx for one year (1967–68). While at Fordham, he was diagnosed with a benign brain tumour, and it was treated successfully. He returned to Toronto where he taught at the University of Toronto for the rest of his life and lived in Wychwood Park, a bucolic enclave on a hill overlooking the downtown where Anatol Rapoport was his neighbour. In 1970, he was made a Companion of the Order of Canada. In 1975, the University of Dallas hosted him from April to May, appointing him to the McDermott Chair.\n\nMarshall and Corinne McLuhan had six children: Eric, twins Mary and Teresa, Stephanie, Elizabeth, and Michael. The associated costs of a large family eventually drove him to advertising work and accepting frequent consulting and speaking engagements for large corporations, IBM and AT&T among them. Woody Allen's Oscar-winning motion picture \"Annie Hall\" (1977) featured McLuhan in a cameo as himself; a pompous academic arguing with Allen in a cinema queue is silenced by McLuhan suddenly appearing and saying, \"You know nothing of my work.\" This was one of McLuhan's most frequent statements to and about those who disagreed with him.\n\nIn September 1979, he suffered a stroke which affected his ability to speak. The University of Toronto's School of Graduate Studies tried to close his research centre shortly thereafter, but was deterred by substantial protests, most notably by Woody Allen. He never fully recovered from the stroke and died in his sleep on December 31, 1980.\n\nDuring his years at Saint Louis University (1937–1944), McLuhan worked concurrently on two projects: his doctoral dissertation and the manuscript that was eventually published in 1951 as the book \"The Mechanical Bride: Folklore of Industrial Man\", which included only a representative selection of the materials that McLuhan had prepared for it.\n\nMcLuhan's 1942 Cambridge University doctoral dissertation surveys the history of the verbal arts (grammar, logic, and rhetoric—collectively known as the trivium) from the time of Cicero down to the time of Thomas Nashe. In his later publications, McLuhan at times uses the Latin concept of the \"trivium\" to outline an orderly and systematic picture of certain periods in the history of Western culture. McLuhan suggests that the Late Middle Ages, for instance, were characterized by the heavy emphasis on the formal study of logic. The key development that led to the Renaissance was not the rediscovery of ancient texts but a shift in emphasis from the formal study of logic to rhetoric and grammar. Modern life is characterized by the re-emergence of grammar as its most salient feature—a trend McLuhan felt was exemplified by the New Criticism of Richards and Leavis.\n\nIn \"The Mechanical Bride\", McLuhan turned his attention to analysing and commenting on numerous examples of persuasion in contemporary popular culture. This followed naturally from his earlier work as both dialectic and rhetoric in the classical trivium aimed at persuasion. At this point his focus shifted dramatically, turning inward to study the influence of communication media independent of their content. His famous aphorism \"the medium is the message\" (elaborated in his 1964 book, \"\") calls attention to this intrinsic effect of communications media.\n\nMcLuhan also started the journal \"Explorations\" with anthropologist Edmund \"Ted\" Carpenter. In a letter to Walter Ong dated May 31, 1953, McLuhan reported that he had received a two-year grant of $43,000 from the Ford Foundation to carry out a communication project at the University of Toronto involving faculty from different disciplines, which led to the creation of the journal.\n\nAt a Fordham lecture in 1999, Tom Wolfe suggested that a major under-acknowledged influence on McLuhan's work is the Jesuit philosopher Pierre Teilhard de Chardin whose ideas anticipated those of McLuhan, especially the evolution of the human mind into the \"noosphere\". In fact, McLuhan warns against outright dismissing or whole-heartedly accepting de Chardin's observations early on in his second published book \"The Gutenberg Galaxy\" (p. 32): \"This externalization of our senses creates what de Chardin calls the 'noosphere' or a technological brain for the world. Instead of tending towards a vast Alexandrian library the world has become a computer, an electronic brain, exactly as in an infantile piece of science fiction. And as our senses have gone outside us, Big Brother goes inside. So, unless aware of this dynamic, we shall at once move into a phase of panic terrors, exactly befitting a small world of tribal drums, total interdependence, and super-imposed co-existence.\"\n\nIn his private life, McLuhan wrote to friends saying: \"I am not a fan of Pierre Teilhard de Chardin. The idea that anything is better because it comes later is surely borrowed from pre-electronic technologies.\" Further, McLuhan noted to a Catholic collaborator: \"The idea of a Cosmic thrust in one direction ... is surely one of the lamest semantic fallacies ever bred by the word ‘evolution’ ... That development should have any direction at all is inconceivable except to the highly literate community.\"\n\nMcLuhan's first book, \"\" (1951), is a pioneering study in the field now known as popular culture. His interest in the critical study of popular culture was influenced by the 1933 book \"Culture and Environment\" by F. R. Leavis and Denys Thompson, and the title \"The Mechanical Bride\" is derived from a piece by the Dadaist artist Marcel Duchamp.\n\nLike his 1962 book \"The Gutenberg Galaxy\", \"The Mechanical Bride\" is composed of a number of short essays that can be read in any order—what he styled the \"mosaic approach\" to writing a book. Each essay begins with a newspaper or magazine article or an advertisement, followed by McLuhan's analysis thereof. The analyses bear on aesthetic considerations as well as on the implications behind the imagery and text. McLuhan chose the ads and articles included in his book not only to draw attention to their symbolism and their implications for the corporate entities that created and disseminated them, but also to mull over what such advertising implies about the wider society at which it is aimed.\n\nMcLuhan's \"The Gutenberg Galaxy: The Making of Typographic Man\" (written in 1961, first published in Canada by University of Toronto Press in 1962) is a pioneering study in the fields of oral culture, print culture, cultural studies, and media ecology.\n\nThroughout the book, McLuhan takes pains to reveal how communication technology (alphabetic writing, the printing press, and the electronic media) affects cognitive organization, which in turn has profound ramifications for social organization:\n...[I]f a new technology extends one or more of our senses outside us into the social world, then new ratios among all of our senses will occur in that particular culture. It is comparable to what happens when a new note is added to a melody. And when the sense ratios alter in any culture then what had appeared lucid before may suddenly become opaque, and what had been vague or opaque will become translucent.\n\nHis episodic history takes the reader from pre-alphabetic tribal humankind to the electronic age. According to McLuhan, the invention of movable type greatly accelerated, intensified, and ultimately enabled cultural and cognitive changes that had already been taking place since the invention and implementation of the alphabet, by which McLuhan means phonemic orthography. (McLuhan is careful to distinguish the phonetic alphabet from logographic/logogramic writing systems, like Egyptian hieroglyphs or ideograms.)\n\nPrint culture, ushered in by the Gutenberg press in the middle of the fifteenth century, brought about the cultural predominance of the visual over the aural/oral. Quoting with approval an observation on the nature of the printed word from \"Prints and Visual Communication\" by William Ivins, McLuhan remarks:\nIn this passage [Ivins] not only notes the ingraining of lineal, sequential habits, but, even more important, points out the visual homogenizing of experience of print culture, and the relegation of auditory and other sensuous complexity to the background. [...] The technology and social effects of typography incline us to abstain from noting interplay and, as it were, \"formal\" causality, both in our inner and external lives. Print exists by virtue of the static separation of functions and fosters a mentality that gradually resists any but a separative and compartmentalizing or specialist outlook.\n\nThe main concept of McLuhan's argument (later elaborated upon in \"The Medium Is the Massage\") is that new technologies (like alphabets, printing presses, and even speech itself) exert a gravitational effect on cognition, which in turn affects social organization: print technology changes our perceptual habits (\"visual homogenizing of experience\"), which in turn affects social interactions (\"fosters a mentality that gradually resists all but a... specialist outlook\"). According to McLuhan, the advent of print technology contributed to and made possible most of the salient trends in the Modern period in the Western world: individualism, democracy, Protestantism, capitalism and nationalism. For McLuhan, these trends all reverberate with print technology's principle of \"segmentation of actions and functions and principle of visual quantification.\"\n\nIn the early 1960s, McLuhan wrote that the visual, individualistic print culture would soon be brought to an end by what he called \"electronic interdependence\": when electronic media replaces visual culture with aural/oral culture. In this new age, humankind will move from individualism and fragmentation to a collective identity, with a \"tribal base.\" McLuhan's coinage for this new social organization is the \"global village\".\n\nThe term is sometimes described as having negative connotations in \"The Gutenberg Galaxy\", but McLuhan himself was interested in exploring effects, not making value judgments:\nInstead of tending towards a vast Alexandrian library the world has become a computer, an electronic brain, exactly as an infantile piece of science fiction. And as our senses have gone outside us, Big Brother goes inside. So, unless aware of this dynamic, we shall at once move into a phase of panic terrors, exactly befitting a small world of tribal drums, total interdependence, and superimposed co-existence. [...] Terror is the normal state of any oral society, for in it everything affects everything all the time. [...] In our long striving to recover for the Western world a unity of sensibility and of thought and feeling we have no more been prepared to accept the tribal consequences of such unity than we were ready for the fragmentation of the human psyche by print culture.\n\nKey to McLuhan's argument is the idea that technology has no \"per se\" moral bent—it is a tool that profoundly shapes an individual's and, by extension, a society's self-conception and realization:\n\nIs it not obvious that there are always enough moral problems without also taking a moral stand on technological grounds? [...] Print is the extreme phase of alphabet culture that detribalizes or decollectivizes man in the first instance. Print raises the visual features of alphabet to highest intensity of definition. Thus print carries the individuating power of the phonetic alphabet much further than manuscript culture could ever do. Print is the technology of individualism. If men decided to modify this visual technology by an electric technology, individualism would also be modified. To raise a moral complaint about this is like cussing a buzz-saw for lopping off fingers. \"But\", someone says, \"we didn't know it would happen.\" Yet even witlessness is not a moral issue. It is a problem, but not a moral problem; and it would be nice to clear away some of the moral fogs that surround our technologies. It would be good for morality.\n\nThe moral valence of technology's effects on cognition is, for McLuhan, a matter of perspective. For instance, McLuhan contrasts the considerable alarm and revulsion that the growing quantity of books aroused in the latter seventeenth century with the modern concern for the \"end of the book\". If there can be no universal moral sentence passed on technology, McLuhan believes that \"there can only be disaster arising from unawareness of the causalities and effects inherent in our technologies\".\n\nThough the World Wide Web was invented almost thirty years after \"The Gutenberg Galaxy\", and ten years after his death, McLuhan prophesied the web technology seen today as early as 1962:\n\nThe next medium, whatever it is—it may be the extension of consciousness—will include television as its content, not as its environment, and will transform television into an art form. A computer as a research and communication instrument could enhance retrieval, obsolesce mass library organization, retrieve the individual's encyclopedic function and flip into a private line to speedily tailored data of a saleable kind.\n\nFurthermore, McLuhan coined and certainly popularized the usage of the term \"surfing\" to refer to rapid, irregular and multidirectional movement through a heterogeneous body of documents or knowledge, e.g., statements like \"Heidegger surf-boards along on the electronic wave as triumphantly as Descartes rode the mechanical wave.\" Paul Levinson's 1999 book \"Digital McLuhan\" explores the ways that McLuhan's work can be better understood through the lens of the digital revolution.\n\nMcLuhan frequently quoted Walter Ong's \"Ramus, Method, and the Decay of Dialogue\" (1958), which evidently had prompted McLuhan to write \"The Gutenberg Galaxy\". Ong wrote a highly favorable review of this new book in \"America\". However, Ong later tempered his praise, by describing McLuhan's \"The Gutenberg Galaxy\" as \"a racy survey, indifferent to some scholarly detail, but uniquely valuable in suggesting the sweep and depth of the cultural and psychological changes entailed in the passage from illiteracy to print and beyond.\" McLuhan himself said of the book, \"I'm not concerned to get any kudos out of [\"The Gutenberg Galaxy\"]. It seems to me a book that somebody should have written a century ago. I wish somebody else had written it. It will be a useful prelude to the rewrite of \"Understanding Media\" [the 1960 NAEB report] that I'm doing now.\" \n\nMcLuhan's \"The Gutenberg Galaxy\" won Canada's highest literary award, the Governor-General's Award for Non-Fiction, in 1962. The chairman of the selection committee was McLuhan's colleague at the University of Toronto and oftentime intellectual sparring partner, Northrop Frye.\n\nMcLuhan's most widely known work, \"Understanding Media: The Extensions of Man\" (1964), is a pioneering study in media theory. Dismayed by the way people approached and used new media such as television, McLuhan famously argued that in the modern world \"we live mythically and integrally ... but continue to think in the old, fragmented space and time patterns of the pre-electric age.\"\n\nMcLuhan proposed that media themselves, not the content they carry, should be the focus of study—popularly quoted as \"the medium is the message\". McLuhan's insight was that a medium affects the society in which it plays a role not by the content delivered over the medium, but by the characteristics of the medium itself. McLuhan pointed to the light bulb as a clear demonstration of this concept. A light bulb does not have content in the way that a newspaper has articles or a television has programs, yet it is a medium that has a social effect; that is, a light bulb enables people to create spaces during nighttime that would otherwise be enveloped by darkness. He describes the light bulb as a medium without any content. McLuhan states that \"a light bulb creates an environment by its mere presence.\" More controversially, he postulated that content had little effect on society—in other words, it did not matter if television broadcasts children's shows or violent programming, to illustrate one example—the effect of television on society would be identical. He noted that all media have characteristics that engage the viewer in different ways; for instance, a passage in a book could be reread at will, but a movie had to be screened again in its entirety to study any individual part of it.\n\nIn the first part of \"Understanding Media,\" McLuhan also stated that different media invite different degrees of participation on the part of a person who chooses to consume a medium. Some media, like the movies, were \"hot\"—that is, they enhance one single sense, in this case vision, in such a manner that a person does not need to exert much effort in filling in the details of a movie image. McLuhan contrasted this with \"cool\" TV, which he claimed requires more effort on the part of the viewer to determine meaning, and comics, which due to their minimal presentation of visual detail require a high degree of effort to fill in details that the cartoonist may have intended to portray. A movie is thus said by McLuhan to be \"hot\", intensifying one single sense \"high definition\", demanding a viewer's attention, and a comic book to be \"cool\" and \"low definition\", requiring much more conscious participation by the reader to extract value.\n\n\"Any hot medium allows of less participation than a cool one, as a lecture makes for less participation than a seminar, and a book for less than a dialogue.\"\n\nHot media usually, but not always, provide complete involvement without considerable stimulus. For example, print occupies visual space, uses visual senses, but can immerse its reader. Hot media favour analytical precision, quantitative analysis and sequential ordering, as they are usually sequential, linear and logical. They emphasize one sense (for example, of sight or sound) over the others. For this reason, hot media also include radio, as well as film, the lecture, and photography.\n\nCool media, on the other hand, are usually, but not always, those that provide little involvement with substantial stimulus. They require more active participation on the part of the user, including the perception of abstract patterning and simultaneous comprehension of all parts. Therefore, according to McLuhan cool media include television, as well as the seminar and cartoons. McLuhan describes the term \"cool media\" as emerging from jazz and popular music and, in this context, is used to mean \"detached.\" Cool medium incorporates increased involvement but decreased description while hot medium is the opposite, decreased involvement and increased description. In other words, a society that appears to be actively participating in the streaming of content but not considering the effects of the tool is not allowing an \"extension of ourselves.\" \n\nThis concept appears to force media into binary categories. However, McLuhan's hot and cool exist on a continuum: they are more correctly measured on a scale than as dichotomous terms.\n\nSome theorists have attacked McLuhan's definition and treatment of the word \"medium\" for being too simplistic. Umberto Eco, for instance, contends that McLuhan's medium conflates channels, codes, and messages under the overarching term of the medium, confusing the vehicle, internal code, and content of a given message in his framework.\n\nIn \"Media Manifestos\", Régis Debray also takes issue with McLuhan's envisioning of the medium. Like Eco, he is ill at ease with this reductionist approach, summarizing its ramifications as follows:\n\nFurthermore, when \"Wired\" interviewed him in 1995, Debray stated that he views McLuhan \"more as a poet than a historian, a master of intellectual collage rather than a systematic analyst ... McLuhan overemphasizes the technology behind cultural change at the expense of the usage that the messages and codes make of that technology.\"\n\nDwight Macdonald, in turn, reproached McLuhan for his focus on television and for his \"aphoristic\" style of prose, which he believes left \"Understanding Media\" filled with \"contradictions, non-sequiturs, facts that are distorted and facts that are not facts, exaggerations, and chronic rhetorical vagueness.\" \n\nAdditionally, Brian Winston’s \"Misunderstanding Media\", published in 1986, chides McLuhan for what he sees as his technologically deterministic stances. Raymond Williams and James W. Carey further this point of contention, claiming:\n\nThe work of McLuhan was a particular culmination of an aesthetic theory which became, negatively, a social theory [...] It is an apparently sophisticated technological determinism which has the significant effect of indicating a social and cultural determinism [...] If the medium – whether print or television – is the cause, of all other causes, all that men ordinarily see as history is at once reduced to effects. (Williams 1990, 126/7)\n\nDavid Carr states that there has been a long line of \"academics who have made a career out of deconstructing McLuhan’s effort to define the modern media ecosystem,\" whether it be due to what they see as McLuhan's ignorance toward sociohistorical context or the style of his argument.\n\nWhile some critics have taken issue with McLuhan's writing style and mode of argument, McLuhan himself urged readers to think of his work as \"probes\" or \"mosaics\" offering a toolkit approach to thinking about the media. His eclectic writing style has also been praised for its postmodern sensibilities and suitability for virtual space.\n\n\"The Medium Is the Massage\", published in 1967, was McLuhan's best seller, \"eventually selling nearly a million copies worldwide.\" Initiated by Quentin Fiore, McLuhan adopted the term \"massage\" to denote the effect each medium has on the human sensorium, taking inventory of the \"effects\" of numerous media in terms of how they \"massage\" the sensorium.\n\nFiore, at the time a prominent graphic designer and communications consultant, set about composing the visual illustration of these effects which were compiled by Jerome Agel. Near the beginning of the book, Fiore adopted a pattern in which an image demonstrating a media effect was presented with a textual synopsis on the facing page. The reader experiences a repeated shifting of analytic registers—from \"reading\" typographic print to \"scanning\" photographic facsimiles—reinforcing McLuhan's overarching argument in this book: namely, that each medium produces a different \"massage\" or \"effect\" on the human sensorium.\n\nIn \"The Medium Is the Massage\", McLuhan also rehashed the argument—which first appeared in the Prologue to 1962's \"The Gutenberg Galaxy\"—that all media are \"extensions\" of our human senses, bodies and minds.\n\nFinally, McLuhan described key points of change in how man has viewed the world and how these views were changed by the adoption of new media. \"The technique of invention was the discovery of the nineteenth [century]\", brought on by the adoption of fixed points of view and perspective by typography, while \"[t]he technique of the suspended judgment is the discovery of the twentieth century\", brought on by the bard abilities of radio, movies and television.\n\nAn audio recording version of McLuhan's famous work was made by Columbia Records. The recording consists of a pastiche of statements made by McLuhan \"interrupted\" by other speakers, including people speaking in various phonations and falsettos, discordant sounds and 1960s incidental music in what could be considered a deliberate attempt to translate the disconnected images seen on TV into an audio format, resulting in the prevention of a connected stream of conscious thought. Various audio recording techniques and statements are used to illustrate the relationship between spoken, literary speech and the characteristics of electronic audio media. McLuhan biographer Philip Marchand called the recording \"the 1967 equivalent of a McLuhan video.\"\n\nMcLuhan used James Joyce's \"Finnegans Wake\", an inspiration for this study of war throughout history, as an indicator as to how war may be conducted in the future.\n\nJoyce's \"Wake\" is claimed to be a gigantic cryptogram which reveals a cyclic pattern for the whole history of man through its Ten Thunders. Each \"thunder\" below is a 100-character portmanteau of other words to create a statement he likens to an effect that each technology has on the society into which it is introduced. In order to glean the most understanding out of each, the reader must break the portmanteau into separate words (and many of these are themselves portmanteaus of words taken from multiple languages other than English) and speak them aloud for the spoken effect of each word. There is much dispute over what each portmanteau truly denotes.\n\nMcLuhan claims that the ten thunders in \"Wake\" represent different stages in the history of man:\n\nIn his 1970 book, \"From Cliché to Archetype\", McLuhan, collaborating with Canadian poet Wilfred Watson, approached the various implications of the verbal cliché and of the archetype. One major facet in McLuhan's overall framework introduced in this book that is seldom noticed is the provision of a new term that actually succeeds the global village; the \"global theater\".\n\nIn McLuhan's terms, a cliché is a \"normal\" action, phrase, etc. which becomes so often used that we are \"anesthetized\" to its effects.\n\nAn example of this given by McLuhan is Eugène Ionesco's play \"The Bald Soprano\", whose dialogue consists entirely of phrases Ionesco pulled from an Assimil language book. \"Ionesco originally put all these idiomatic English clichés into literary French which presented the English in the most absurd aspect possible.\"\n\nMcLuhan's \"archetype\" \"is a quoted extension, medium, technology or environment.\" \"Environment\" would also include the kinds of \"awareness\" and cognitive shifts brought upon people by it, not totally unlike the psychological context Carl Jung described.\n\nMcLuhan also posits that there is a factor of interplay between the cliché and the archetype, or a \"doubleness\":\n\nAnother theme of the Wake [\"Finnegans Wake\"] that helps in the understanding of the paradoxical shift from cliché to archetype is 'past time are pastimes.' The dominant technologies of one age become the games and pastimes of a later age. In the 20th century, the number of 'past times' that are simultaneously available is so vast as to create cultural anarchy. When all the cultures of the world are simultaneously present, the work of the artist in the elucidation of form takes on new scope and new urgency. Most men are pushed into the artist's role. The artist cannot dispense with the principle of 'doubleness' or 'interplay' because this type of hendiadys dialogue is essential to the very structure of consciousness, awareness, and autonomy.\n\nMcLuhan relates the cliché-to-archetype process to the Theater of the Absurd:\nPascal, in the seventeenth century, tells us that the heart has many reasons of which the head knows nothing. The Theater of the Absurd is essentially a communicating to the head of some of the silent languages of the heart which in two or three hundred years it has tried to forget all about. In the seventeenth century world the languages of the heart were pushed down into the unconscious by the dominant print cliché.\n\nThe \"languages of the heart\", or what McLuhan would otherwise define as oral culture, were thus made archetype by means of the printing press, and turned into cliché.\n\nThe satellite medium, McLuhan states, encloses the Earth in a man-made environment, which \"ends 'Nature' and turns the globe into a repertory theater to be programmed.\" All previous environments (book, newspaper, radio, etc.) and their artifacts are retrieved under these conditions (\"past times are pastimes\"). McLuhan thereby meshes this into the term \"global theater\". It serves as an update to his older concept of the global village, which, in its own definitions, can be said to be subsumed into the overall condition described by that of the global theater.\n\nIn his 1989 posthumous book, \"The Global Village\", McLuhan, collaborating with Bruce R. Powers, provided a strong conceptual framework for understanding the cultural implications of the technological advances associated with the rise of a worldwide electronic network. This is a major work of McLuhan's because it contains the most extensive elaboration of his concept of Acoustic Space, and it provides a critique of standard 20th century communication models like the Shannon–Weaver model. McLuhan distinguishes between the existing worldview of Visual Space – a linear, quantitative, classically geometric model – and that of Acoustic Space – a holistic, qualitative order with a complex intricate paradoxical topology. \"Acoustic Space has the basic character of a sphere whose focus or center is simultaneously everywhere and whose margin is nowhere.\" The transition from Visual to Acoustic Space was not automatic with the advent of the global network, but would have to be a conscious project. The \"universal environment of simultaneous electronic flow\" inherently favors right-brain Acoustic Space, yet we are held back by habits of adhering to a fixed point of view. There are no boundaries to sound. We hear from all directions at once. Yet Acoustic and Visual Space are in fact inseparable. The resonant interval is the invisible borderline between Visual and Acoustic Space. This is like the television camera that the Apollo 8 astronauts focused on the Earth after they had orbited the moon.\n\nReading, writing, and hierarchical ordering are associated with the left brain, as are the linear concept of time and phonetic literacy. The left brain is the locus of analysis, classification, and rationality. The right brain is the locus of the spatial, tactile, and musical. \"Comprehensive awareness\" results when the two sides of the brain are in true balance. Visual Space is associated with the simplified worldview of Euclidean geometry, the intuitive three dimensions useful for the architecture of buildings and the surveying of land. It is too rational and has no grasp of the acoustic. Acoustic Space is multisensory.\n\nMcLuhan writes about robotism in the context of Japanese Zen Buddhism and how it can offer us new ways of thinking about technology. The Western way of thinking about technology is too much related to the left hemisphere of our brain, which has a rational and linear focus. What he called robotism might better be called androidism in the wake of \"Blade Runner\" and the novels of Philip K. Dick. Robotism-androidism emerges from the further development of the right hemisphere of the brain, creativity and a new relationship to spacetime (most humans are still living in 17th century classical Newtonian physics spacetime). Robots-androids will have much greater flexibility than humans have had until now, in both mind and body. Robots-androids will teach humanity this new flexibility. And this flexibility of androids (what McLuhan calls robotism) has a strong affinity with Japanese culture and life. McLuhan quotes from Ruth Benedict, \"The Chrysanthemum and the Sword\", an anthropological study of Japanese culture published in 1946: \"Occidentals cannot easily credit the ability of the Japanese to swing from one behavior to another without psychic cost. Such extreme possibilities are not included in our experience. Yet in Japanese life the contradictions, as they seem to us, are as deeply based in their view of life as our uniformities are in ours.\" The ability to live in the present and instantly readjust.\n\n\"All Western scientific models of communication are—like the Shannon–Weaver model—linear, sequential, and logical as a reflection of the late medieval emphasis on the Greek notion of efficient causality.\" McLuhan and Powers criticize the Shannon-Weaver model of communication as emblematic of left-hemisphere bias and linearity, descended from a print-era perversion of Aristotle's notion of efficient causality.\n\nA third term of \"The Global Village\" that McLuhan and Powers develop at length is The Tetrad. McLuhan had begun development on the Tetrad as early as 1974. The tetrad an analogical, simultaneous four-fold pattern of transformation. \"At full maturity the tetrad reveals the metaphoric structure of the artifact as having two figures and two grounds in dynamic and analogical relationship to each other.\" Like the camera focused on the Earth by the Apollo 8 astronauts, the tetrad reveals figure (Moon) and ground (Earth) simultaneously. The right-brain hemisphere thinking is the capability of being in many places at the same time. Electricity is acoustic. It is simultaneously everywhere. The Tetrad, with its fourfold Möbius topological structure of enhancement, reversal, retrieval and obsolescence, is mobilized by McLuhan and Powers to illuminate the media or technological inventions of cash money, the compass, the computer, the database, the satellite, and the global media network.\n\nIn \"Laws of Media\" (1988), published posthumously by his son Eric, McLuhan summarized his ideas about media in a concise tetrad of media effects. The tetrad is a means of examining the effects on society of any technology (i.e., any medium) by dividing its effects into four categories and displaying them simultaneously. McLuhan designed the tetrad as a pedagogical tool, phrasing his laws as questions with which to consider any medium:\n\n\nThe laws of the tetrad exist simultaneously, not successively or chronologically, and allow the questioner to explore the \"grammar and syntax\" of the \"language\" of media. McLuhan departs from his mentor Harold Innis in suggesting that a medium \"overheats\", or reverses into an opposing form, when taken to its extreme.\n\nVisually, a tetrad can be depicted as four diamonds forming an X, with the name of a medium in the centre. The two diamonds on the left of a tetrad are the \"Enhancement\" and \"Retrieval\" qualities of the medium, both \"Figure\" qualities. The two diamonds on the right of a tetrad are the \"Obsolescence\" and \"Reversal\" qualities, both \"Ground\" qualities.\n\nUsing the example of radio:\n\n\nMcLuhan adapted the Gestalt psychology idea of a \"figure and a ground\", which underpins the meaning of \"The medium is the message\". He used this concept to explain how a form of communications technology, the medium or \"figure\", necessarily operates through its context, or \"ground\".\n\nMcLuhan believed that in order to grasp fully the effect of a new technology, one must examine figure (medium) and ground (context) together, since neither is completely intelligible without the other. McLuhan argued that we must study media in their historical context, particularly in relation to the technologies that preceded them. The present environment, itself made up of the effects of previous technologies, gives rise to new technologies, which, in their turn, further affect society and individuals.\n\nAll technologies have embedded within them their own assumptions about time and space. The message which the medium conveys can only be understood if the medium and the environment in which the medium is used—and which, simultaneously, it effectively creates—are analysed together. He believed that an examination of the figure-ground relationship can offer a critical commentary on culture and society.\n\nAfter the publication of \"Understanding Media\", McLuhan received an astonishing amount of publicity, making him perhaps the most publicized English teacher in the twentieth century and arguably the most controversial. This publicity began with the work of two California advertising executives, Howard Gossage and Gerald Feigen who used personal funds to fund their practice of \"genius scouting.\" Much enamoured with McLuhan's work, Feigen and Gossage arranged for McLuhan to meet with editors of several major New York magazines in May 1965 at the Lombardy Hotel in New York. Philip Marchand reports that, as a direct consequence of these meetings, McLuhan was offered the use of an office in the headquarters of both \"Time\" and \"Newsweek\", any time he needed it.\n\nIn August 1965, Feigen and Gossage held what they called a \"McLuhan festival\" in the offices of Gossage's advertising agency in San Francisco. During this \"festival\", McLuhan met with advertising executives, members of the mayor's office, and editors from the \"San Francisco Chronicle\" and \"Ramparts\" magazine. More significant was the presence at the festival of Tom Wolfe, who wrote about McLuhan in a subsequent article, \"What If He Is Right?\", published in \"New York Magazine\" and Wolfe's own \"The Pump House Gang\". According to Feigen and Gossage, their work had only a moderate effect on McLuhan's eventual celebrity: they claimed that their work only \"probably speeded up the recognition of [McLuhan's] genius by about six months.\" In any case, McLuhan soon became a fixture of media discourse. \"Newsweek\" magazine did a cover story on him; articles appeared in \"Life Magazine\", \"Harper's\", \"Fortune\", \"Esquire\", and others. Cartoons about him appeared in \"The New Yorker\". In 1969, \"Playboy\" magazine published a lengthy interview with him. In a running gag on the popular sketch comedy \"Rowan & Martin's Laugh-In\", the \"poet\" Henry Gibson would randomly say, \"Marshall McLuhan, what are you doin'?\"\n\nMcLuhan was credited with coining the phrase \"Turn on, tune in, drop out\" by its popularizer, Timothy Leary, in the 1960s. In a 1988 interview with Neil Strauss, Leary stated that slogan was \"given to him\" by McLuhan during a lunch in New York City. Leary said McLuhan \"was very much interested in ideas and marketing, and he started singing something like, 'Psychedelics hit the spot / Five hundred micrograms, that’s a lot,' to the tune of a Pepsi commercial. Then he started going, 'Tune in, turn on, and drop out.'\"\n\nDuring his lifetime and afterward, McLuhan heavily influenced cultural critics, thinkers, and media theorists such as Neil Postman, Jean Baudrillard, Timothy Leary, Terence McKenna, William Irwin Thompson, Paul Levinson, Douglas Rushkoff, Jaron Lanier, Hugh Kenner, and John David Ebert, as well as political leaders such as Pierre Elliott Trudeau and Jerry Brown. Andy Warhol was paraphrasing McLuhan with his now famous \"15 minutes of fame\" quote. When asked in the 1970s for a way to sedate violences in Angola, he suggested a massive spread of TV devices. The character \"Brian O'Blivion\" in David Cronenberg's 1983 film Videodrome is a \"media oracle\" based on McLuhan. In 1991, McLuhan was named as the \"patron saint\" of \"Wired Magazine\" and a quote of his appeared on the masthead for the first ten years of its publication. He is mentioned by name in a Peter Gabriel-penned lyric in the song \"Broadway Melody of 1974\". This song appears on the concept album \"The Lamb Lies Down on Broadway\", from progressive rock band Genesis. The lyric is: \"Marshall McLuhan, casual viewin' head buried in the sand.\" McLuhan is also jokingly referred to during an episode of \"The Sopranos\" entitled \"House Arrest\". Despite his death in 1980, someone claiming to be McLuhan was posting on a \"Wired\" mailing list in 1996. The information this individual provided convinced one writer for \"Wired\" that \"if the poster was not McLuhan himself, it was a bot programmed with an eerie command of McLuhan's life and inimitable perspective.\"\n\nA new centre known as the McLuhan Program in Culture and Technology, formed soon after his death in 1980, was the successor to McLuhan's Centre for Culture and Technology at the University of Toronto. Since 1994, it has been part of the University of Toronto Faculty of Information and in 2008 the McLuhan Program in Culture and Technology incorporated in the Coach House Institute. The first director was literacy scholar and OISE Professor David R. Olsen. From 1983 until 2008, the McLuhan Program was under the direction of Dr. Derrick de Kerckhove who was McLuhan's student and translator. From 2008 through 2015 Professor Dominique Scheffel-Dunand of York University served Director of the Program.\nIn 2011 at the time of his centenary the Coach House Institute established a Marshall McLuhan Centenary Fellowship program in his honor, and each year appoints up to four fellows for a maximum of two years. In May 2016 the Coach House Institute was renamed the McLuhan Centre for Culture and Technology; its Interim Director was Seamus Ross (2015–16). Sarah Sharma, an Associate Professor of Media Theory from the Institute of Communication, Culture, Information and Technology (ICCIT) and the Faculty of Information (St. George), began a five-year term as director of the Coach House (2017- ). Professor Sharma's research and teaching focuses on feminist approaches to technology, including issues related to temporality and media. Professor Sharma's thematic for the 2017-2018 Monday Night Seminars at the McLuhan Centre is MsUnderstanding Media which extends and introduces feminist approaches to technology to McLuhan's formulations of technology and culture.\n\nIn Toronto, Marshall McLuhan Catholic Secondary School is named after him.\n\nThis is a partial list of works cited in this article. See Bibliography of Marshall McLuhan for a more comprehensive list of works by and about McLuhan.\n\n\n\n\n"}
{"id": "19549", "url": "https://en.wikipedia.org/wiki?curid=19549", "title": "Masochism", "text": "Masochism\n\nMasochism is the practice of seeking pain because it is pleasurable, named for Leopold von Sacher-Masoch from Lviv. It may also refer to:\n\n\n"}
{"id": "19550", "url": "https://en.wikipedia.org/wiki?curid=19550", "title": "Multiple inheritance", "text": "Multiple inheritance\n\nMultiple inheritance is a feature of some object-oriented computer programming languages in which an object or class can inherit characteristics and features from more than one parent object or parent class. It is distinct from single inheritance, where an object or class may only inherit from one particular object or class.\n\nMultiple inheritance has been a sensitive issue for many years, with opponents pointing to its increased complexity and ambiguity in situations such as the \"diamond problem\", where it may be ambiguous as to which parent class a particular feature is inherited from if more than one parent class implements said feature. This can be addressed in various ways, including using virtual inheritance. Alternate methods of object composition not based on inheritance such as mixins and traits have also been proposed to address the ambiguity.\n\nIn object-oriented programming (OOP), \"inheritance\" describes a relationship between two classes in which one class (the \"child\" class) \"subclasses\" the \"parent\" class. The child inherits methods and attributes of the parent, allowing for shared functionality. For example, one might create a variable class \"Mammal\" with features such as eating, reproducing, etc.; then define a child class \"Cat\" that inherits those features without having to explicitly program them, while adding new features like \"chasing mice\".\n\nMultiple inheritance allows programmers to use more than one totally orthogonal hierarchy simultaneously, such as allowing \"Cat\" to inherit from \"Cartoon character\" and \"Pet\" and \"Mammal\" and access features from within all of those classes.\n\nLanguages that support multiple inheritance include: C++, Common Lisp (via Common Lisp Object System (CLOS)), EuLisp (via The EuLisp Object System TELOS), Curl, Dylan, Eiffel, Logtalk, Object REXX, Scala (via use of mixin classes), OCaml, Perl, Perl 6, POP-11, Python, and Tcl (built-in from 8.6 or via Incremental Tcl (Incr Tcl) in earlier versions).\n\nIBM System Object Model (SOM) runtime supports multiple inheritance, and any programming language targeting SOM can implement new SOM classes inherited from multiple bases.\n\nSome object-oriented languages, such as Java, C#, and Ruby implement \"single inheritance\", although protocols, or \"interfaces,\" provide some of the functionality of true multiple inheritance.\n\nPHP uses traits classes to inherit specific method implementations. Ruby uses modules to inherit multiple methods.\n\nThe \"diamond problem\" (sometimes referred to as the \"deadly diamond of death\") is an ambiguity that arises when two classes B and C inherit from A, and class D inherits from both B and C. If there is a method in A that B and C have overridden, and D does not override it, then which version of the method does D inherit: that of B, or that of C?\n\nFor example, in the context of GUI software development, a class codice_1 may inherit from both classes codice_2 (for appearance) and codice_3 (for functionality/input handling), and classes codice_2 and codice_3 both inherit from the codice_6 class. Now if the codice_7 method is called for a codice_1 object and there is no such method in the codice_1 class but there is an overridden codice_7 method in codice_2 or codice_3 (or both), which method should be eventually called?\n\nIt is called the \"diamond problem\" because of the shape of the class inheritance diagram in this situation. In this case, class A is at the top, both B and C separately beneath it, and D joins the two together at the bottom to form a diamond shape.\n\nLanguages have different ways of dealing with these problems of repeated inheritance.\n\n\nLanguages that allow only single inheritance, where a class can only derive from one base class, do not have the diamond problem. The reason for this is that such languages have at most one implementation of any method at any level in the inheritance chain regardless of the repetition or placement of methods. Typically these languages allow classes to implement multiple protocols, called interfaces in Java. These protocols define methods but do not provide concrete implementations. This strategy has been used by ActionScript, C#, D, Java, Nemerle, Object Pascal (Free Pascal and Delphi), Objective-C, Smalltalk, Swift and PHP. All these languages allow classes to implement multiple protocols.\n\nMoreover, languages such as Ada, Objective-C, C#, Delphi/Free Pascal, Java, Swift and PHP allow multiple-inheritance of interfaces (called protocols in Objective-C and Swift). Interfaces are like abstract base classes that specify method signatures without implementing any behavior. (\"Pure\" interfaces such as the ones in Java up to version 7 do not permit any implementation or instance data in the interface.) Nevertheless, even when several interfaces declare the same method signature, as soon as that method is implemented (defined) anywhere in the inheritance chain, it overrides any implementation of that method in the chain above it (in its superclasses). Hence, at any given level in the inheritance chain, there can be at most one implementation of any method. Thus, single-inheritance method implementation does not exhibit the Diamond Problem even with multiple-inheritance of interfaces. With the introduction of default implementation for interfaces in Java 8, it is still possible to generate a Diamond Problem, although this will only appear as a compile-time error.\n\n\n\n"}
{"id": "19552", "url": "https://en.wikipedia.org/wiki?curid=19552", "title": "Media studies", "text": "Media studies\n\nFor a history of the field, see \"History of media studies\".\n\nMedia is studied as a broad subject in most states in Australia, with the state of Victoria being world leaders in curriculum development . Media studies in Australia was first developed as an area of study in Victorian universities in the early 1960s, and in secondary schools in the mid 1960s.\n\nToday, almost all Australian universities teach media studies. According to the Government of Australia's \"Excellence in Research for Australia\" report, the leading universities in the country for media studies (which were ranked well above World standards by the report's scoring methodology) are Monash University, QUT, RMIT, University of Melbourne, University of Queensland and UTS.\n\nIn secondary schools, an early film studies course first began being taught as part of the Victorian junior secondary curriculum during the mid 1960s. And, by the early 1970s, an expanded media studies course was being taught. The course became part of the senior secondary curriculum (later known as the Victorian Certificate of Education or \"VCE\") in the 1980s. It has since become, and continues to be, a strong component of the VCE. Notable figures in the development of the Victorian secondary school curriculum were the long time Rusden College media teacher Peter Greenaway (not the British film director), Trevor Barr (who authored one of the first media text books \"Reflections of Reality\") and later John Murray (who authored \"The Box in the Corner\", \"In Focus\", and \"10 Lessons in Film Appreciation\").\n\nToday, Australian states and territories that teach media studies at a secondary level are Australian Capital Territory, Northern Territory, Queensland, South Australia, Victoria and Western Australia. Media studies does not appear to be taught in the state of New South Wales at a secondary level.\n\nIn Victoria, the VCE media studies course is structured as: Unit 1 - Representation, Technologies of Representation, and New Media; Unit 2 - Media Production, Australian Media Organisations; Unit 3 - Narrative Texts, Production Planning; and Unit 4 - Media Process, Social Values, and Media Influence. Media studies also form a major part of the primary and junior secondary curriculum, and includes areas such as photography, print media and television.\n\nVictoria also hosts the peak media teaching body known as ATOM which publishes \"Metro\" and \"Screen Education\" magazines.\n\nIn Canada, media studies and communication studies are incorporated in the same departments and cover a wide range of approaches (from critical theory to organizations to research-creation and political economy, for example). Over time, research developed to employ theories and methods from cultural studies, philosophy, political economy, gender, sexuality and race theory, management, rhetoric, film theory, sociology, and anthropology. Harold Innis and Marshall McLuhan are famous Canadian scholars for their contributions to the fields of media ecology and political economy in the 20th century. They were both important members of the Toronto School of Communication at the time. More recently, the School of Montreal and its founder James R. Taylor significantly contributed to the field of organizational communication by focusing on the ontological processes of organizations.\n\nCarleton University and the University of Western Ontario, 1945 and 1946 prospectively, created Journalism specific programs or schools. A Journalism specific program was also created at Ryerson in 1950. The first communication programs in Canada were started at Ryerson and Concordia Universities. The Radio and Television Arts program at Ryerson were started in the 1950s, while the Film, Media Studies/Media Arts, and Photography programs also originated from programs started in the 1950s. The Communication studies department at Concordia was created in the late 1960s. Ryerson's Radio and Television, Film, Media and Photography programs were renowned by the mid 1970s, and its programs were being copied by other colleges and universities nationally and Internationally.\n\nToday, most universities offer undergraduate degrees in Media and Communication Studies, and many Canadian scholars actively contribute to the field, among which: Brian Massumi (philosophy, cultural studies), Kim Sawchuk (cultural studies, feminist, ageing studies), Carrie Rentschler (feminist theory), and François Cooren (organizational communication).\n\nIn his book “Understanding Media, The Extensions of Man”, media theorist Marshall McLuhan suggested that \"the medium is the message\", and that all human artefacts and technologies are media. His book introduced the usage of terms such as “media” into our language along with other precepts, among them “global village” and “Age of Information”. A medium is anything that mediates our interaction with the world or other humans. Given this perspective, media study is not restricted to just media of communications but all forms of technology. Media and their users form an ecosystem and the study of this ecosystem is known as media ecology.\n\nMcLuhan says that the “technique of fragmentation that is the essence of machine technology” shaped the restructuring of human work and association and “the essence of automation technology is the opposite”. He uses an example of the electric light to make this connection and to explain “the medium is the message”. The electric light is pure information and it is a medium without a message unless it is used to spell out some verbal ad or a name. The characteristic of all media means the “content” of any medium is always another medium. For example, the content of writing is speech, the written word is the content of print, and print is the content of the telegraph. The change that the medium or technology introduces into human affairs is the “message”. If the electric light is used for Friday night football or to light up your desk you could argue that the content of the electric light is these activities. The fact that it is the medium that shapes and controls the form of human association and action makes it the message. The electric light is over looked as a communication medium because it doesn’t have any content. It is not until the electric light is used to spell a brand name that it is recognized as medium. Similar to radio and other mass media electric light eliminates time and space factors in human association creating deeper involvement. McLuhan compared the “content” to a juicy piece of meat being carried by a burglar to distract the “watchdog of the mind”. The effect of the medium is made strong because it is given another media “content”. The content of a movie is a book, play or maybe even an opera.\n\nMcLuhan talks about media being “hot” or “cold” and touches on the principle that distinguishes them from one another. A hot medium (i.e., radio or Movie) extends a single sense in “high definition”. High definition means the state of being well filled with data. A cool medium (i.e., Telephone and TV) is considered “low definition” because a small amount of data/information is given and has to be filled in. Hot media are low in participation and cool media are high in participation. Hot media are low in participation because it is giving most of the information and it excludes. Cool media are high in participation because it gives you information but you have to fill in the blanks and it is inclusive. He used lecturing as an example for hot media and seminars as an example for low media. If you use a hot medium in a hot or cool culture makes a difference.\n\nThere are two universities in China that specialize in media studies. Communication University of China, formerly known as the Beijing Broadcasting Institute, that dates back to 1954. CUC has 15,307 full-time students, including 9264 undergraduates, 3512 candidates for doctor and master's degrees and 16780 students in programs of continuing education. The other university known for media studies in China is Zhejiang University of Media and Communications (ZUMC) which has campuses in Hangzhou and Tongxiang. Almost 10,000 full-time students are currently studying in over 50 programs at the 13 Colleges and Schools of ZUMC. Both institutions have produced some of China's brightest broadcasting talents for television as well as leading journalists at magazines and newspapers.\n\nThere is no university specialized on journalism and media studies, but there are seven public universities which have a department of media stuides. Three biggest are based in Prague (Charles University), Brno (Masaryk University) and Olomouc (Palacký University). There are another nine private universities and colleges which has media studies department.\n\nOne prominent French media critic is the sociologist Pierre Bourdieu who wrote among other books \"On Television\" (New Press, 1999). Bourdieu's analysis is that television provides far less autonomy, or freedom, than we think. In his view, the market (which implies the hunt for higher advertising revenue) not only imposes uniformity and banality, but also a form of invisible censorship. When, for example, television producers \"pre-interview\" participants in news and public affairs programs, to ensure that they will speak in simple, attention-grabbing terms, and when the search for viewers leads to an emphasis on the sensational and the spectacular, people with complex or nuanced views are not allowed a hearing.\n\nIn Germany two main branches of media theory or media studies can be identified.\n\nThe first major branch of media theory has its roots in the humanities and cultural studies, such as film studies (\"Filmwissenschaft\"), theater studies (\"Theaterwissenschaft\") and German language and literature studies (\"Germanistik\") as well as Comparative Literature Studies (\"Komparatistik\"). This branch has broadened out substantially since the 1990s. And it is on this initial basis that a culturally-based media studies (often emphasised more recently through the disciplinary title \"Medienkulturwissenschaft\") in Germany has primarily developed and established itself. \n\nThis plurality of perspectives make it difficult to single out one particular site where this branch of Medienwissenschaft originated. While the Frankfurt-based theatre scholar, Hans-Theis Lehmanns term \"post dramatic theater\" points directly to the increased blending of co-presence and mediatized material in the German theater (and elsewhere) since the 1970s, the field of theater studies from the 1990s onwards at the Freie Universität Berlin, led in particular by Erika Fischer-Lichte, showed particular interest in the ways in which theatricality influenced notions of performativity in aesthetic events. Within the field of Film Studies, again, both Frankfurt and Berlin were dominant in the development of new perspectives on moving image media. Heide Schlüpman in Frankfurt and Gertrud Koch, first in Bochum then in Berlin, were key theorists contributing to an aesthetic theory of the cinema (Schlüpmann) as \"dispositif\" and the moving image as medium, particularly in the context of illuion (Koch). Many scholars who became known as media scholars in Germany originally were scholars of German, such as Friedrich Kittler, who taught at the Humboldt Universität zu Berlin, completed both his dissertation and habilitation in the context of \"Germanistik\". One of the early publications in this new direction is a volume edited by Helmut Kreuzer, \"Literature Studies - Media Studies\" (\"Literaturwissenschaft – Medienwissenschaft\"), which summarizes the presentations given at the Düsseldorfer Germanistentag 1976.\n\nThe second branch of media studies in Germany is comparable to Communication Studies. Pioneered by Elisabeth Noelle-Neumann in the 1940s, this branch studies mass media, its institutions and its effects on society and individuals. The German Institute for Media and Communication Policy, founded in 2005 by media scholar Lutz Hachmeister, is one of the few independent research institutions that is dedicated to issues surrounding media and communications policies.\n\nThe term \"Wissenschaft\" cannot be translated straightforwardly as \"studies\", as it calls to mind both scientific methods and the humanities. Accordingly, German media theory combines philosophy, psychoanalysis, history, and scienctific studies with media-specific research.\n\n\"Medienwissenschaften\" is currently one of the most popular courses of study at universities in Germany, with many applicants mistakenly assuming that studying it will automatically lead to a career in TV or other media. This has led to widespread disillusionment, with students blaming the universities for offering highly theoretical course content. The universities maintain that practical journalistic training is not the aim of the academic studies they offer.\n\nMedia Studies is a fast growing academic field in India, with several dedicated departments and research institutes. With a view to making the best use of communication facilities for information, publicity and development, the Government of India in 1962-63 sought the advice of the Ford Foundation/UNESCO team of internationally known mass communication specialists who recommended the setting up of a national institute for training, teaching and research in mass communication. Anna University was the first university to start Master of Science in Electronic Media programmes. It offers a five-year integrated programme and a two-year programme in Electronic Media. The Department of Media Sciences was started in January 2002, branching off from the UGC's Educational Multimedia Research Centre (EMMRC). National Institute of Open Schooling, the world's largest open schooling system, offers Mass Communication as a subject of studies at senior secondary level. All the major universities in the country have mass media and journalism studies departments. Centre for the Study of Developing Societies (CSDS), Delhi has media studies as one of their major emphasis. Centre for Internet and Society, Bangaluru that does interdisciplinary research on internet and digital technologies also is worth mentioning.\nMain scholars who are working on Indian media include Arvind Rajagopal, Ravi Sundaram, Robin Jeffrey, Sevanti Ninan, Shohini Ghosh, and Usha M. Rodrigues and Maya Ranganathan. The work of Nalin Mehta on the expansion of private television channels in India, Amelia Bonea's research on the history of telegraph and journalism, and Shiju Sam Varughese's work on science and mass media open new areas of research in Indian media studies.\n\nIn the Netherlands, media studies are split into several academic courses such as (applied) communication sciences, communication- and information sciences, communication and media, media and culture or theater, film and television sciences. Whereas communication sciences focuses on the way people communicate, be it mediated or unmediated, media studies tends to narrow the communication down to just mediated communication. However, it would be a mistake to consider media studies a specialism of communication sciences, since media make up just a small portion of the overall course. Indeed, both studies tend to borrow elements from one another.\n\nCommunication sciences (or a derivative thereof) can be studied at Erasmus University Rotterdam, Radboud University, Tilburg University, University of Amsterdam, University of Groningen, University of Twente, Roosevelt Academy, University of Utrecht, VU University Amsterdam and Wageningen University and Research Centre.\n\nMedia studies (or something similar) can be studied at the University of Amsterdam, VU University Amsterdam, Erasmus University Rotterdam, University of Groningen and the University of Utrecht.\n\nMedia studies in New Zealand is healthy, especially due to renewed activity in the country's film industry and is taught at both secondary and tertiary education institutes. Media studies in NZ can be regarded as a singular success, with the subject well-established in the tertiary sector (such as Screen and Media Studies at the University of Waikato; Media Studies, Victoria University of Wellington; Film, Television and Media Studies, University of Auckland; Media Studies, Massey University; Communication Studies, University of Otago). Different Media Studies courses can offer students a range of specialisations- such as cultural studies, media theory and analysis, practical film-making, journalism and communications studies. But what makes the case of New Zealand particularly significant in respect of Media Studies is that for more than a decade it has been a nationally mandated and very popular subject in secondary (high) schools, taught across three years in a very structured and developmental fashion, with Scholarship in Media Studies available for academically gifted students. According to the New Zealand Ministry of Education Subject Enrolment figures 229 New Zealand schools offered Media Studies as a subject in 2016, representing more than 14,000 students. \n\nIn Pakistan, media studies programs are widely offered. University of the Punjab Lahore is the oldest department. Later on University of Karachi, Peshawar University, BZU Multaan, Islamia University Bahwalpur also started communication programs. Now, newly established universities are also offering mass communication program in which University of Gujrat emerged as a leading department. Bahria University which is established by Pakistan Navy is also offering BS in media studies.\n\nIn Switzerland, media and communication studies are offered by several higher education institutions including the International University in Geneva, Zurich University of Applied Sciences, University of Lugano, University of Fribourg and others.\n\nIn the United Kingdom, media studies developed in the 1960s from the academic study of English, and from literary criticism more broadly. The key date, according to Andrew Crisell, is 1959:\n\nWhen Joseph Trenaman left the BBC's Further Education Unit to become the first holder of the Granada Research Fellowship in Television at Leeds University. Soon after in 1966, the Centre for Mass Communication Research was founded at Leicester University, and degree programmes in media studies began to sprout at polytechnics and other universities during the 1970s and 1980s.\n\nJames Halloran at Leicester University is credited with much influence in the development of media studies and communication studies, as the head of the university's Centre for Mass Communication Research, and founder of the International Association for Media and Communication Research. Media Studies is now taught all over the UK. It is taught at Key Stages 1– 3, Entry Level, GCSE and at A level and the Scottish Qualifications Authority offers formal qualifications at a number of different levels. It is offered through a large area of exam boards including AQA and WJEC.\n\nMuch research in the field of news media studies has been led by the Reuters Institute for the Study of Journalism. Details of the research projects and results are published in the RISJ annual report.\n\nMass communication, Communication studies or simply 'Communication' may be more popular names than “media studies” for academic departments in the United States. However, the focus of such programs sometimes excludes certain media—film, book publishing, video games, etc. The title “media studies” may be used alone, to designate film studies and rhetorical or critical theory, or it may appear in combinations like “media studies and communication” to join two fields or emphasize a different focus. It is a very broad study as media has many platforms in the modern world. Social Media is an industry that has gotten a lot of attention in recent years. Our primary form of entertainment is no longer our TVs but we have access to a screen about worldwide events all the time.\nIn 1999, the MIT Comparative Media Studies program started under the leadership of Henry Jenkins, since growing into a graduate program, MIT's largest humanities major, and, following a 2012 merger with the Writing and Humanistic Studies program, a roster of twenty faculty, including Pulitzer Prize-winning author Junot Diaz, science fiction writer Joe Haldeman, games scholar T. L. Taylor, and media scholars William Uricchio (a CMS co-founder), Edward Schiappa, and Heather Hendershot. Now named Comparative Media Studies/Writing, the department places an emphasis on what Jenkins and colleagues had termed \"applied humanities\": it hosts several research groups for civic media, digital humanities, games, computational media, documentary, and mobile design, and these groups are used to provide graduate students with research assistantships to cover the cost of tuition and living expenses. The incorporation of Writing and Humanistic Studies also placed MIT's Science Writing program, Writing Across the Curriculum, and Writing and Communications Center under the same roof.\n\nFormerly an interdisciplinary major at the University of Virginia the Department of Media Studies was officially established in 2001 and has quickly grown to wide recognition. This is partly thanks to the acquisition of Professor Siva Vaidhyanathan, a cultural historian and media scholar, as well as the Inaugural Verklin Media Policy and Ethics Conference, endowed by the CEO of Canoe Ventures and UVA alumnus David Verklin. In 2010, a group of undergraduate students in the Media Studies Department established the Movable Type Academic Journal, the first ever undergraduate academic journal of its kind. The department is expanding rapidly and doubled in size in 2011.\n\nBrooklyn College, part of the City University of New York, has been offering graduate studies in television and media since 1961. Currently, the Department of Television and Radio administers an MS in Media Studies, and hosts the Center for the Study of World Television.\n\nThe University of Southern California has three distinct centers for media studies: the Center for Visual Anthropology (founded in 1984), the Institute for Media Literacy at the School of Cinematic Arts (founded in 1998) and the Annenberg School for Communication and Journalism (founded in 1971).\n\nUniversity of California, Irvine had in Mark Poster one of the first and foremost theorists of media culture in the US, and can boast a strong Department of Film & Media Studies. University of California, Berkeley has three institutional structures within which media studies can take place: the department of Film and Media (formerly Film Studies Program), including famous theorists as Mary Ann Doane and Linda Williams, the Center for New Media, and a long established interdisciplinary program formerly titled Mass Communications, which recently changed its name to Media Studies, dropping any connotations which accompany the term “Mass” in the former title. Until recently, Radford University in Virginia used the title \"media studies\" for a department that taught practitioner-oriented major concentrations in journalism, advertising, broadcast production and Web design. In 2008, those programs were combined with a previous department of communication (speech and public relations) to create a School of Communication. (A media studies major at Radford still means someone concentrating on journalism, broadcasting, advertising or Web production.)\n\nThe University of Denver has a renowned program for digital media studies. It is an interdisciplinary program combining Communications, Computer Science, and the arts.\n\n\n"}
{"id": "19553", "url": "https://en.wikipedia.org/wiki?curid=19553", "title": "Microprocessor", "text": "Microprocessor\n\nA microprocessor is a computer processor that incorporates the functions of a central processing unit on a single integrated circuit (IC), or at most a few integrated circuits. The microprocessor is a multipurpose, clock driven, register based, digital integrated circuit that accepts binary data as input, processes it according to instructions stored in its memory, and provides results as output. Microprocessors contain both combinational logic and sequential digital logic. Microprocessors operate on numbers and symbols represented in the binary number system.\n\nThe integration of a whole CPU onto a single chip or on a few chips greatly reduced the cost of processing power, increasing efficiency. Integrated circuit processors are produced in large numbers by highly automated processes, resulting in a low per-unit cost. Single-chip processors increase reliability because there are many fewer electrical connections that could fail. As microprocessor designs improve, the cost of manufacturing a chip (with smaller components built on a semiconductor chip the same size) generally stays the same according to Rock's law.\n\nBefore microprocessors, small computers had been built using racks of circuit boards with many medium- and small-scale integrated circuits. Microprocessors combined this into one or a few large-scale ICs. Continued increases in microprocessor capacity have since rendered other forms of computers almost completely obsolete (see history of computing hardware), with one or more microprocessors used in everything from the smallest embedded systems and handheld devices to the largest mainframes and supercomputers.\n\nThe internal arrangement of a microprocessor varies depending on the age of the design and the intended purposes of the microprocessor. The complexity of an integrated circuit (IC) is bounded by physical limitations on the number of transistors that can be put onto one chip, the number of package terminations that can connect the processor to other parts of the system, the number of interconnections it is possible to make on the chip, and the heat that the chip can dissipate. Advancing technology makes more complex and powerful chips feasible to manufacture.\n\nA minimal hypothetical microprocessor might include only an arithmetic logic unit (ALU) and a control logic section. The ALU performs operations such as addition, subtraction, and operations such as AND or OR. Each operation of the ALU sets one or more flags in a status register, which indicate the results of the last operation (zero value, negative number, overflow, or others). The control logic retrieves instruction codes from memory and initiates the sequence of operations required for the ALU to carry out the instruction. A single operation code might affect many individual data paths, registers, and other elements of the processor.\n\nAs integrated circuit technology advanced, it was feasible to manufacture more and more complex processors on a single chip. The size of data objects became larger; allowing more transistors on a chip allowed word sizes to increase from 4- and 8-bit words up to today's 64-bit words. Additional features were added to the processor architecture; more on-chip registers sped up programs, and complex instructions could be used to make more compact programs. Floating-point arithmetic, for example, was often not available on 8-bit microprocessors, but had to be carried out in software. Integration of the floating point unit first as a separate integrated circuit and then as part of the same microprocessor chip sped up floating point calculations.\n\nOccasionally, physical limitations of integrated circuits made such practices as a bit slice approach necessary. Instead of processing all of a long word on one integrated circuit, multiple circuits in parallel processed subsets of each data word. While this required extra logic to handle, for example, carry and overflow within each slice, the result was a system that could handle, for example, 32-bit words using integrated circuits with a capacity for only four bits each.\n\nThe ability to put large numbers of transistors on one chip makes it feasible to integrate memory on the same die as the processor. This CPU cache has the advantage of faster access than off-chip memory and increases the processing speed of the system for many applications. Processor clock frequency has increased more rapidly than external memory speed, except in the recent past, so cache memory is necessary if the processor is not delayed by slower external memory.\n\nA microprocessor is a general-purpose entity. Several specialized processing devices have followed from the technology:\n\n\n32-bit processors have more digital logic than narrower processors, so 32-bit (and wider) processors produce more digital noise and have higher static consumption than narrower processors. Reducing digital noise improves ADC conversion results. So, 8- or 16-bit processors can be better than 32-bit processors for system on a chip and microcontrollers that require extremely low-power electronics, or are part of a mixed-signal integrated circuit with noise-sensitive on-chip analog electronics such as high-resolution analog to digital converters, or both.\n\nNevertheless, trade-offs apply: running 32-bit arithmetic on an 8-bit chip could end up using more power, as the chip must execute software with multiple instructions. Modern microprocessors go into low power states when possible, and an 8-bit chip running 32-bit calculations would be active for more cycles. This creates a delicate balance between software, hardware and use patterns, and costs.\n\nWhen manufactured in a similar process, 8-bit microprocessors use less power when operating and less power when sleeping than 32-bit microprocessors.\n\nHowever, a 32-bit microprocessor may use less average power than an 8-bit microprocessor when the application requires certain operations such as floating-point math\nthat take many more clock cycles on an 8-bit microprocessor than on a 32-bit microprocessor, so the 8-bit microprocessor spends more time in high-power operating mode.\n\nThousands of items that were traditionally not computer-related include microprocessors. These include large and small household appliances, cars (and their accessory equipment units), car keys, tools and test instruments, toys, light switches/dimmers and electrical circuit breakers, smoke alarms, battery packs, and hi-fi audio/visual components (from DVD players to phonograph turntables). Such products as cellular telephones, DVD video system and HDTV broadcast systems fundamentally require consumer devices with powerful, low-cost, microprocessors. Increasingly stringent pollution control standards effectively require automobile manufacturers to use microprocessor engine management systems to allow optimal control of emissions over the widely varying operating conditions of an automobile. Non-programmable controls would require complex, bulky, or costly implementation to achieve the results possible with a microprocessor.\n\nA microprocessor control program (embedded software) can be easily tailored to different needs of a product line, allowing upgrades in performance with minimal redesign of the product. Different features can be implemented in different models of a product line at negligible production cost.\n\nMicroprocessor control of a system can provide control strategies that would be impractical to implement using electromechanical controls or purpose-built electronic controls. For example, an engine control system in an automobile can adjust ignition timing based on engine speed, load on the engine, ambient temperature, and any observed tendency for knocking—allowing an automobile to operate on a range of fuel grades.\n\nThe advent of low-cost computers on integrated circuits has transformed modern society. General-purpose microprocessors in personal computers are used for computation, text editing, multimedia display, and communication over the Internet. Many more microprocessors are part of embedded systems, providing digital control over myriad objects from appliances to automobiles to cellular phones and industrial process control.\n\nThe first use of the term \"microprocessor\" is attributed to Viatron Computer Systems describing the custom integrated circuit used in their System 21 small computer system announced in 1968.\n\nBy the late 1960s, designers were striving to integrate the central processing unit (CPU) functions of a computer onto a handful of very-large-scale integration metal-oxide semiconductor chips, called microprocessor unit (MPU) chipsets. Building on an earlier Busicom design from 1969, Intel introduced the first commercial microprocessor, the 4-bit Intel 4004, in 1971, followed by its 8-bit microprocessor 8008 in 1972. In 1969, Lee Boysel, based on b8-bit arithmetic logic units (3800/3804) he designed earlier at Fairchild, created the Four-Phase Systems Inc. AL-1, an 8-bit CPU slice that was expandable to 32-bits. In 1970, Steve Geller and Ray Holt of Garrett AiResearch designed the MP944 chipset to implement the F-14A Central Air Data Computer on six metal-gate chips fabricated by AMI.\n\nThe first microprocessors emerged in the early 1970s and were used for electronic calculators, using binary-coded decimal (BCD) arithmetic on 4-bit words. Other embedded uses of 4-bit and 8-bit microprocessors, such as terminals, printers, various kinds of automation etc., followed soon after. Affordable 8-bit microprocessors with 16-bit addressing also led to the first general-purpose microcomputers from the mid-1970s on.\n\nSince the early 1970s, the increase in capacity of microprocessors has followed Moore's law; this originally suggested that the number of components that can be fitted onto a chip doubles every year. With present technology, it is actually every two years, and as a result Moore later changed the period to two years.\n\nThree projects delivered a microprocessor at about the same time: Garrett AiResearch's Central Air Data Computer (CADC), Texas Instruments' TMS 1000 (September 1971) and Intel's 4004 (November 1971, based on an earlier 1969 Busicom design). Arguably, Four-Phase Systems AL1 microprocessor was also delivered in 1969.\n\nIn 1968, Garrett AiResearch (who employed designers Ray Holt and Steve Geller) was invited to produce a digital computer to compete with electromechanical systems then under development for the main flight control computer in the US Navy's new F-14 Tomcat fighter. The design was complete by 1970, and used a MOS-based chipset as the core CPU. The design was significantly (approximately 20 times) smaller and much more reliable than the mechanical systems it competed against, and was used in all of the early Tomcat models. This system contained \"a 20-bit, pipelined, parallel multi-microprocessor\". The Navy refused to allow publication of the design until 1997. For this reason the CADC, and the MP944 chipset it used, are fairly unknown. Ray Holt's autobiographical story of this design and development is presented in the book: The Accidental Engineer. \nRay Holt graduated from California Polytechnic University in 1968, and began his computer design career with the CADC. From its inception, it was shrouded in secrecy until 1998 when at Holt's request, the US Navy allowed the documents into the public domain. Since then people have debated whether this was the first microprocessor. Holt has stated that no one has compared this microprocessor with those that came later. According to Parab et al. (2007), This convergence of DSP and microcontroller architectures is known as a digital signal controller.\n\nThe Four-Phase Systems AL1 was an 8-bit bit slice chip containing eight registers and an ALU. It was designed by Lee Boysel in 1969. At the time, it formed part of a nine-chip, 24-bit CPU with three AL1s, but it was later called a microprocessor when, in response to 1990s litigation by Texas Instruments, a demonstration system was constructed where a single AL1 formed part of a courtroom demonstration computer system, together with RAM, ROM, and an input-output device.\n\nIn 1971, Pico Electronics and General Instrument (GI) introduced their first collaboration in ICs, a complete single chip calculator IC for the Monroe/Litton Royal Digital III calculator. This chip could also arguably lay claim to be one of the first microprocessors or microcontrollers having ROM, RAM and a RISC instruction set on-chip. The layout for the four layers of the PMOS process was hand drawn at x500 scale on mylar film, a significant task at the time given the complexity of the chip.\n\nPico was a spinout by five GI design engineers whose vision was to create single chip calculator ICs. They had significant previous design experience on multiple calculator chipsets with both GI and Marconi-Elliott. The key team members had originally been tasked by Elliott Automation to create an 8-bit computer in MOS and had helped establish a MOS Research Laboratory in Glenrothes, Scotland in 1967.\n\nCalculators were becoming the largest single market for semiconductors so Pico and GI went on to have significant success in this burgeoning market. GI continued to innovate in microprocessors and microcontrollers with products including the CP1600, IOB1680 and PIC1650. In 1987, the GI Microelectronics business was spun out into the Microchip PIC microcontroller business.\n\nThe Intel 4004 is generally regarded as the first commercially available microprocessor, and cost . The first known advertisement for the 4004 is dated November 15, 1971 and appeared in \"Electronic News\". The microprocessor was designed by a team consisting of Italian engineer Federico Faggin, American engineers Marcian Hoff and Stanley Mazor, and Japanese engineer Masatoshi Shima.\n\nThe project that produced the 4004 originated in 1969, when Busicom, a Japanese calculator manufacturer, asked Intel to build a chipset for high-performance desktop calculators. Busicom's original design called for a programmable chip set consisting of seven different chips. Three of the chips were to make a special-purpose CPU with its program stored in ROM and its data stored in shift register read-write memory. Ted Hoff, the Intel engineer assigned to evaluate the project, believed the Busicom design could be simplified by using dynamic RAM storage for data, rather than shift register memory, and a more traditional general-purpose CPU architecture. Hoff came up with a four-chip architectural proposal: a ROM chip for storing the programs, a dynamic RAM chip for storing data, a simple I/O device and a 4-bit central processing unit (CPU). Although not a chip designer, he felt the CPU could be integrated into a single chip, but as he lacked the technical know-how the idea remained just a wish for the time being.\n\nWhile the architecture and specifications of the MCS-4 came from the interaction of Hoff with Stanley Mazor, a software engineer reporting to him, and with Busicom engineer Masatoshi Shima, during 1969, Mazor and Hoff moved on to other projects. In April 1970, Intel hired Italian engineer Federico Faggin as project leader, a move that ultimately made the single-chip CPU final design a reality (Shima meanwhile designed the Busicom calculator firmware and assisted Faggin during the first six months of the implementation). Faggin, who originally developed the silicon gate technology (SGT) in 1968 at Fairchild Semiconductor and designed the world’s first commercial integrated circuit using SGT, the Fairchild 3708, had the correct background to lead the project into what would become the first commercial general purpose microprocessor. Since SGT was his very own invention, Faggin also used it to create his new methodology for random logic design that made it possible to implement a single-chip CPU with the proper speed, power dissipation and cost. The manager of Intel's MOS Design Department was Leslie L. Vadász at the time of the MCS-4 development but Vadász's attention was completely focused on the mainstream business of semiconductor memories so he left the leadership and the management of the MCS-4 project to Faggin, who was ultimately responsible for leading the 4004 project to its realization. Production units of the 4004 were first delivered to Busicom in March 1971 and shipped to other customers in late 1971.\n\nGilbert Hyatt was awarded a patent claiming an invention pre-dating both TI and Intel, describing a \"microcontroller\". The patent was later invalidated, but not before substantial royalties were paid out.\n\nThe Intel 4004 was followed in 1972 by the Intel 8008, the world's first 8-bit microprocessor. The 8008 was not, however, an extension of the 4004 design, but instead the culmination of a separate design project at Intel, arising from a contract with Computer Terminals Corporation, of San Antonio TX, for a chip for a terminal they were designing, the Datapoint 2200—fundamental aspects of the design came not from Intel but from CTC. In 1968, CTC's Vic Poor and Harry Pyle developed the original design for the instruction set and operation of the processor. In 1969, CTC contracted two companies, Intel and Texas Instruments, to make a single-chip implementation, known as the CTC 1201. In late 1970 or early 1971, TI dropped out being unable to make a reliable part. In 1970, with Intel yet to deliver the part, CTC opted to use their own implementation in the Datapoint 2200, using traditional TTL logic instead (thus the first machine to run \"8008 code\" was not in fact a microprocessor at all and was delivered a year earlier). Intel's version of the 1201 microprocessor arrived in late 1971, but was too late, slow, and required a number of additional support chips. CTC had no interest in using it. CTC had originally contracted Intel for the chip, and would have owed them for their design work. To avoid paying for a chip they did not want (and could not use), CTC released Intel from their contract and allowed them free use of the design. Intel marketed it as the 8008 in April, 1972, as the world's first 8-bit microprocessor. It was the basis for the famous \"Mark-8\" computer kit advertised in the magazine \"Radio-Electronics\" in 1974. This processor had an 8-bit data bus and a 14-bit address bus.\n\nThe 8008 was the precursor to the successful Intel 8080 (1974), which offered improved performance over the 8008 and required fewer support chips. Federico Faggin conceived and designed it using high voltage N channel MOS. The Zilog Z80 (1976) was also a Faggin design, using low voltage N channel with depletion load and derivative Intel 8-bit processors: all designed with the methodology Faggin created for the 4004. Motorola released the competing 6800 in August 1974, and the similar MOS Technology 6502 in 1975 (both designed largely by the same people). The 6502 family rivaled the Z80 in popularity during the 1980s.\n\nA low overall cost, small packaging, simple computer bus requirements, and sometimes the integration of extra circuitry (e.g. the Z80's built-in memory refresh circuitry) allowed the home computer \"revolution\" to accelerate sharply in the early 1980s. This delivered such inexpensive machines as the Sinclair ZX81, which sold for . A variation of the 6502, the MOS Technology 6510 was used in the Commodore 64 and yet another variant, the 8502, powered the Commodore 128.\n\nThe Western Design Center, Inc (WDC) introduced the CMOS WDC 65C02 in 1982 and licensed the design to several firms. It was used as the CPU in the Apple IIe and IIc personal computers as well as in medical implantable grade pacemakers and defibrillators, automotive, industrial and consumer devices. WDC pioneered the licensing of microprocessor designs, later followed by ARM (32-bit) and other microprocessor intellectual property (IP) providers in the 1990s.\n\nMotorola introduced the MC6809 in 1978. It was an ambitious and well thought-through 8-bit design that was source compatible with the 6800, and implemented using purely hard-wired logic (subsequent 16-bit microprocessors typically used microcode to some extent, as CISC design requirements were becoming too complex for pure hard-wired logic).\n\nAnother early 8-bit microprocessor was the Signetics 2650, which enjoyed a brief surge of interest due to its innovative and powerful instruction set architecture.\n\nA seminal microprocessor in the world of spaceflight was RCA's RCA 1802 (aka CDP1802, RCA COSMAC) (introduced in 1976), which was used on board the \"Galileo\" probe to Jupiter (launched 1989, arrived 1995). RCA COSMAC was the first to implement CMOS technology. The CDP1802 was used because it could be run at very low power, and because a variant was available fabricated using a special production process, silicon on sapphire (SOS), which provided much better protection against cosmic radiation and electrostatic discharge than that of any other processor of the era. Thus, the SOS version of the 1802 was said to be the first radiation-hardened microprocessor.\n\nThe RCA 1802 had a static design, meaning that the clock frequency could be made arbitrarily low, or even stopped. This let the \"Galileo\" spacecraft use minimum electric power for long uneventful stretches of a voyage. Timers or sensors would awaken the processor in time for important tasks, such as navigation updates, attitude control, data acquisition, and radio communication. Current versions of the Western Design Center 65C02 and 65C816 have static cores, and thus retain data even when the clock is completely halted.\n\nThe Intersil 6100 family consisted of a 12-bit microprocessor (the 6100) and a range of peripheral support and memory ICs. The microprocessor recognised the DEC PDP-8 minicomputer instruction set. As such it was sometimes referred to as the CMOS-PDP8. Since it was also produced by Harris Corporation, it was also known as the Harris HM-6100. By virtue of its CMOS technology and associated benefits, the 6100 was being incorporated into some military designs until the early 1980s.\n\nThe first multi-chip 16-bit microprocessor was the National Semiconductor IMP-16, introduced in early 1973. An 8-bit version of the chipset was introduced in 1974 as the IMP-8.\n\nOther early multi-chip 16-bit microprocessors include one that Digital Equipment Corporation (DEC) used in the LSI-11 OEM board set and the packaged PDP 11/03 minicomputer—and the Fairchild Semiconductor MicroFlame 9440, both introduced in 1975–76. In 1975, National introduced the first 16-bit single-chip microprocessor, the National Semiconductor PACE, which was later followed by an NMOS version, the INS8900.\n\nAnother early single-chip 16-bit microprocessor was TI's TMS 9900, which was also compatible with their TI-990 line of minicomputers. The 9900 was used in the TI 990/4 minicomputer, the Texas Instruments TI-99/4A home computer, and the TM990 line of OEM microcomputer boards. The chip was packaged in a large ceramic 64-pin DIP package, while most 8-bit microprocessors such as the Intel 8080 used the more common, smaller, and less expensive plastic 40-pin DIP. A follow-on chip, the TMS 9980, was designed to compete with the Intel 8080, had the full TI 990 16-bit instruction set, used a plastic 40-pin package, moved data 8 bits at a time, but could only address 16 KB. A third chip, the TMS 9995, was a new design. The family later expanded to include the 99105 and 99110.\n\nThe Western Design Center (WDC) introduced the CMOS 65816 16-bit upgrade of the WDC CMOS 65C02 in 1984. The 65816 16-bit microprocessor was the core of the Apple IIgs and later the Super Nintendo Entertainment System, making it one of the most popular 16-bit designs of all time.\n\nIntel \"upsized\" their 8080 design into the 16-bit Intel 8086, the first member of the x86 family, which powers most modern PC type computers. Intel introduced the 8086 as a cost-effective way of porting software from the 8080 lines, and succeeded in winning much business on that premise. The 8088, a version of the 8086 that used an 8-bit external data bus, was the microprocessor in the first IBM PC. Intel then released the 80186 and 80188, the 80286 and, in 1985, the 32-bit 80386, cementing their PC market dominance with the processor family's backwards compatibility. The 80186 and 80188 were essentially versions of the 8086 and 8088, enhanced with some onboard peripherals and a few new instructions. Although Intel's 80186 and 80188 were not used in IBM PC type designs, second source versions from NEC, the V20 and V30 frequently were. The 8086 and successors had an innovative but limited method of memory segmentation, while the 80286 introduced a full-featured segmented memory management unit (MMU). The 80386 introduced a flat 32-bit memory model with paged memory management.\n\nThe 16-bit Intel x86 processors up to and including the 80386 do not include floating-point units (FPUs). Intel introduced the 8087, 80187, 80287 and 80387 math coprocessors to add hardware floating-point and transcendental function capabilities to the 8086 through 80386 CPUs. The 8087 works with the 8086/8088 and 80186/80188, the 80187 works with the 80186 but not the 80188, the 80287 works with the 80286 and the 80387 works with the 80386. The combination of an x86 CPU and an x87 coprocessor forms a single multi-chip microprocessor; the two chips are programmed as a unit using a single integrated instruction set. The 8087 and 80187 coprocessors are connected in parallel with the data and address buses of their parent processor and directly execute instructions intended for them. The 80287 and 80387 coprocessors are interfaced to the CPU through I/O ports in the CPU's address space, this is transparent to the program, which does not need to know about or access these I/O ports directly; the program accesses the coprocessor and its registers through normal instruction opcodes.\n\n16-bit designs had only been on the market briefly when 32-bit implementations started to appear.\n\nThe most significant of the 32-bit designs is the Motorola MC68000, introduced in 1979. The 68k, as it was widely known, had 32-bit registers in its programming model but used 16-bit internal data paths, three 16-bit Arithmetic Logic Units, and a 16-bit external data bus (to reduce pin count), and externally supported only 24-bit addresses (internally it worked with full 32 bit addresses). In PC-based IBM-compatible mainframes the MC68000 internal microcode was modified to emulate the 32-bit System/370 IBM mainframe. Motorola generally described it as a 16-bit processor. The combination of high performance, large (16 megabytes or 2 bytes) memory space and fairly low cost made it the most popular CPU design of its class. The Apple Lisa and Macintosh designs made use of the 68000, as did a host of other designs in the mid-1980s, including the Atari ST and Commodore Amiga.\n\nThe world's first single-chip fully 32-bit microprocessor, with 32-bit data paths, 32-bit buses, and 32-bit addresses, was the AT&T Bell Labs BELLMAC-32A, with first samples in 1980, and general production in 1982. After the divestiture of AT&T in 1984, it was renamed the WE 32000 (WE for Western Electric), and had two follow-on generations, the WE 32100 and WE 32200. These microprocessors were used in the AT&T 3B5 and 3B15 minicomputers; in the 3B2, the world's first desktop super microcomputer; in the \"Companion\", the world's first 32-bit laptop computer; and in \"Alexander\", the world's first book-sized super microcomputer, featuring ROM-pack memory cartridges similar to today's gaming consoles. All these systems ran the UNIX System V operating system.\n\nThe first commercial, single chip, fully 32-bit microprocessor available on the market was the HP FOCUS.\n\nIntel's first 32-bit microprocessor was the iAPX 432, which was introduced in 1981, but was not a commercial success. It had an advanced capability-based object-oriented architecture, but poor performance compared to contemporary architectures such as Intel's own 80286 (introduced 1982), which was almost four times as fast on typical benchmark tests. However, the results for the iAPX432 was partly due to a rushed and therefore suboptimal Ada compiler.\n\nMotorola's success with the 68000 led to the MC68010, which added virtual memory support. The MC68020, introduced in 1984 added full 32-bit data and address buses. The 68020 became hugely popular in the Unix supermicrocomputer market, and many small companies (e.g., Altos, Charles River Data Systems, Cromemco) produced desktop-size systems. The MC68030 was introduced next, improving upon the previous design by integrating the MMU into the chip. The continued success led to the MC68040, which included an FPU for better math performance. The 68050 failed to achieve its performance goals and was not released, and the follow-up MC68060 was released into a market saturated by much faster RISC designs. The 68k family faded from use in the early 1990s.\n\nOther large companies designed the 68020 and follow-ons into embedded equipment. At one point, there were more 68020s in embedded equipment than there were Intel Pentiums in PCs. The ColdFire processor cores are derivatives of the 68020.\n\nDuring this time (early to mid-1980s), National Semiconductor introduced a very similar 16-bit pinout, 32-bit internal microprocessor called the NS 16032 (later renamed 32016), the full 32-bit version named the NS 32032. Later, National Semiconductor produced the NS 32132, which allowed two CPUs to reside on the same memory bus with built in arbitration. The NS32016/32 outperformed the MC68000/10, but the NS32332—which arrived at approximately the same time as the MC68020—did not have enough performance. The third generation chip, the NS32532, was different. It had about double the performance of the MC68030, which was released around the same time. The appearance of RISC processors like the AM29000 and MC88000 (now both dead) influenced the architecture of the final core, the NS32764. Technically advanced—with a superscalar RISC core, 64-bit bus, and internally overclocked—it could still execute Series 32000 instructions through real-time translation.\n\nWhen National Semiconductor decided to leave the Unix market, the chip was redesigned into the Swordfish Embedded processor with a set of on chip peripherals. The chip turned out to be too expensive for the laser printer market and was killed. The design team went to Intel and there designed the Pentium processor, which is very similar to the NS32764 core internally. The big success of the Series 32000 was in the laser printer market, where the NS32CG16 with microcoded BitBlt instructions had very good price/performance and was adopted by large companies like Canon. By the mid-1980s, Sequent introduced the first SMP server-class computer using the NS 32032. This was one of the design's few wins, and it disappeared in the late 1980s. The MIPS R2000 (1984) and R3000 (1989) were highly successful 32-bit RISC microprocessors. They were used in high-end workstations and servers by SGI, among others. Other designs included the Zilog Z80000, which arrived too late to market to stand a chance and disappeared quickly.\n\nThe ARM first appeared in 1985. This is a RISC processor design, which has since come to dominate the 32-bit embedded systems processor space due in large part to its power efficiency, its licensing model, and its wide selection of system development tools. Semiconductor manufacturers generally license cores and integrate them into their own system on a chip products; only a few such vendors are licensed to modify the ARM cores. Most cell phones include an ARM processor, as do a wide variety of other products. There are microcontroller-oriented ARM cores without virtual memory support, as well as symmetric multiprocessor (SMP) applications processors with virtual memory.\n\nFrom 1993 to 2003, the 32-bit x86 architectures became increasingly dominant in desktop, laptop, and server markets, and these microprocessors became faster and more capable. Intel had licensed early versions of the architecture to other companies, but declined to license the Pentium, so AMD and Cyrix built later versions of the architecture based on their own designs. During this span, these processors increased in complexity (transistor count) and capability (instructions/second) by at least three orders of magnitude. Intel's Pentium line is probably the most famous and recognizable 32-bit processor model, at least with the public at broad.\n\nWhile 64-bit microprocessor designs have been in use in several markets since the early 1990s (including the Nintendo 64 gaming console in 1996), the early 2000s saw the introduction of 64-bit microprocessors targeted at the PC market.\n\nWith AMD's introduction of a 64-bit architecture backwards-compatible with x86, x86-64 (also called AMD64), in September 2003, followed by Intel's near fully compatible 64-bit extensions (first called IA-32e or EM64T, later renamed Intel 64), the 64-bit desktop era began. Both versions can run 32-bit legacy applications without any performance penalty as well as new 64-bit software. With operating systems Windows XP x64, Windows Vista x64, Windows 7 x64, Linux, BSD, and macOS that run 64-bit natively, the software is also geared to fully utilize the capabilities of such processors. The move to 64 bits is more than just an increase in register size from the IA-32 as it also doubles the number of general-purpose registers.\n\nThe move to 64 bits by PowerPC had been intended since the architecture's design in the early 90s and was not a major cause of incompatibility. Existing integer registers are extended as are all related data pathways, but, as was the case with IA-32, both floating point and vector units had been operating at or above 64 bits for several years. Unlike what happened when IA-32 was extended to x86-64, no new general purpose registers were added in 64-bit PowerPC, so any performance gained when using the 64-bit mode for applications making no use of the larger address space is minimal.\n\nIn 2011, ARM introduced a new 64-bit ARM architecture.\n\nIn the mid-1980s to early 1990s, a crop of new high-performance reduced instruction set computer (RISC) microprocessors appeared, influenced by discrete RISC-like CPU designs such as the IBM 801 and others. RISC microprocessors were initially used in special-purpose machines and Unix workstations, but then gained wide acceptance in other roles.\n\nThe first commercial RISC microprocessor design was released in 1984, by MIPS Computer Systems, the 32-bit R2000 (the R1000 was not released). In 1986, HP released its first system with a PA-RISC CPU. In 1987, in the non-Unix Acorn computers' 32-bit, then cache-less, ARM2-based Acorn Archimedes became the first commercial success using the ARM architecture, then known as Acorn RISC Machine (ARM); first silicon ARM1 in 1985. The R3000 made the design truly practical, and the R4000 introduced the world's first commercially available 64-bit RISC microprocessor. Competing projects would result in the IBM POWER and Sun SPARC architectures. Soon every major vendor was releasing a RISC design, including the AT&T CRISP, AMD 29000, Intel i860 and Intel i960, Motorola 88000, DEC Alpha.\n\nIn the late 1990s, only two 64-bit RISC architectures were still produced in volume for non-embedded applications: SPARC and Power ISA, but as ARM has become increasingly powerful, in the early 2010s, it became the third RISC architecture in the general computing segment.\n\nA different approach to improving a computer's performance is to add extra processors, as in symmetric multiprocessing designs, which have been popular in servers and workstations since the early 1990s. Keeping up with Moore's law is becoming increasingly challenging as chip-making technologies approach their physical limits. In response, microprocessor manufacturers look for other ways to improve performance so they can maintain the momentum of constant upgrades.\n\nA multi-core processor is a single chip that contains more than one microprocessor core. Each core can simultaneously execute processor instructions in parallel. This effectively multiplies the processor's potential performance by the number of cores, if the software is designed to take advantage of more than one processor core. Some components, such as bus interface and cache, may be shared between cores. Because the cores are physically close to each other, they can communicate with each other much faster than separate (off-chip) processors in a multiprocessor system, which improves overall system performance.\n\nIn 2001, IBM introduced the first commercial multi-core processor, the monolithic two-core POWER4. Personal computers did not receive multi-core processors until the 2005 introduction, of the two-core Intel Pentium D. The Pentium D, however, was not a monolithic multi-core processor. It was constructed from two dies, each containing a core, packaged on a multi-chip module. The first monolithic multi-core processor in the personal computer market was the AMD Athlon X2, which was introduced a few weeks after the Pentium D. , dual- and quad-core processors are widely used in home PCs and laptops, while quad-, six-, eight-, ten-, twelve-, and sixteen-core processors are common in the professional and enterprise markets with workstations and servers.\n\nSun Microsystems has released the Niagara and Niagara 2 chips, both of which feature an eight-core design. The Niagara 2 supports more threads and operates at 1.6 GHz.\n\nHigh-end Intel Xeon processors that are on the LGA 775, LGA 1366, and LGA 2011 sockets and high-end AMD Opteron processors that are on the C32 and G34 sockets are DP (dual processor) capable, as well as the older Intel Core 2 Extreme QX9775 also used in an older Mac Pro by Apple and the Intel Skulltrail motherboard. AMD's G34 motherboards can support up to four CPUs and Intel's LGA 1567 motherboards can support up to eight CPUs.\n\nModern desktop computers support systems with multiple CPUs, but few applications outside of the professional market can make good use of more than four cores. Both Intel and AMD currently offer fast quad, hex and octa-core desktop CPUs, making multi-CPU systems obsolete for many purposes.\nThe desktop market has been in a transition towards quad-core CPUs since Intel's Core 2 Quad was released and are now common, although dual-core CPUs are still more prevalent. Older or mobile computers are less likely to have more than two cores than newer desktops. Not all software is optimised for multi-core CPUs, making fewer, more powerful cores preferable.\n\nAMD offers CPUs with more cores for a given amount of money than similarly priced Intel CPUs—but the AMD cores are somewhat slower, so the two trade blows in different applications depending on how well-threaded the programs running are. For example, Intel's cheapest Sandy Bridge quad-core CPUs often cost almost twice as much as AMD's cheapest Athlon II, Phenom II, and FX quad-core CPUs but Intel has dual-core CPUs in the same price ranges as AMD's cheaper quad-core CPUs. In an application that uses one or two threads, the Intel dual-core CPUs outperform AMD's similarly priced quad-core CPUs—and if a program supports three or four threads the cheap AMD quad-core CPUs outperform the similarly priced Intel dual-core CPUs.\n\nHistorically, AMD and Intel have switched places as the company with the fastest CPU several times. Intel currently leads on the desktop side of the computer CPU market, with their Sandy Bridge and Ivy Bridge series. In servers, AMD's new Opterons seem to have superior performance for their price point. This means that AMD are currently more competitive in low- to mid-end servers and workstations that more effectively use fewer cores and threads.\n\nTaken to the extreme, this trend also includes manycore designs, with hundreds of cores, with qualitatively different architectures.\n\nIn 1997, about 55% of all CPUs sold in the world were 8-bit microcontrollers, of which over 2 billion were sold.\n\nIn 2002, less than 10% of all the CPUs sold in the world were 32-bit or more. Of all the 32-bit CPUs sold, about 2% are used in desktop or laptop personal computers. Most microprocessors are used in embedded control applications such as household appliances, automobiles, and computer peripherals. Taken as a whole, the average price for a microprocessor, microcontroller, or DSP is just over .\n\nIn 2003, about billion worth of microprocessors were manufactured and sold. Although about half of that money was spent on CPUs used in desktop or laptop personal computers, those count for only about 2% of all CPUs sold. The quality-adjusted price of laptop microprocessors improved −25% to −35% per year in 2004–2010, and the rate of improvement slowed to −15% to −25% per year in 2010–2013.\n\nAbout 10 billion CPUs were manufactured in 2008. Most new CPUs produced each year are embedded.\n\n"}
{"id": "19555", "url": "https://en.wikipedia.org/wiki?curid=19555", "title": "Molecule", "text": "Molecule\n\nA molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However, in quantum physics, organic chemistry, and biochemistry, the term \"molecule\" is often used less strictly, also being applied to polyatomic ions.\n\nIn the kinetic theory of gases, the term \"molecule\" is often used for any gaseous particle regardless of its composition. According to this definition, noble gas atoms are considered molecules as they are monatomic molecules.\n\nA molecule may be homonuclear, that is, it consists of atoms of one chemical element, as with oxygen (O); or it may be heteronuclear, a chemical compound composed of more than one element, as with water (HO). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are typically not considered single molecules.\n\nMolecules as components of matter are common in organic substances (and therefore biochemistry). They also make up most of the oceans and atmosphere. However, the majority of familiar solid substances on Earth, including most of the minerals that make up the crust, mantle, and core of the Earth, contain many chemical bonds, but are \"not\" made of identifiable molecules. Also, no typical molecule can be defined for ionic crystals (salts) and covalent crystals (network solids), although these are often composed of repeating unit cells that extend either in a plane (such as in graphene) or three-dimensionally (such as in diamond, quartz, or sodium chloride). The theme of repeated unit-cellular-structure also holds for most condensed phases with metallic bonding, which means that solid metals are also not made of molecules. In glasses (solids that exist in a vitreous disordered state), atoms may also be held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating units that characterizes crystals.\n\nThe science of molecules is called \"molecular chemistry\" or \"molecular physics\", depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term \"unstable molecule\" is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.\n\nAccording to Merriam-Webster and the Online Etymology Dictionary, the word \"molecule\" derives from the Latin \"moles\" or small unit of mass.\n\nThe definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.\n\nMolecules are held together by either covalent bonding or ionic bonding. Several types of non-metal elements exist only as molecules in the environment. For example, hydrogen only exists as hydrogen molecule. A molecule of a compound is made out of two or more elements.\n\nA covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed \"shared pairs\" or \"bonding pairs\", and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed \"covalent bonding\".\n\nIonic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed \"electrovalence\" in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH or SO. Basically, an ionic bond is the transfer of electrons from a metal to a non-metal for both atoms to obtain a full valence shell.\n\nMost molecules are far too small to be seen with the naked eye, but there are exceptions. DNA, a macromolecule, can reach macroscopic sizes, as can molecules of many polymers. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.\n\nThe smallest molecule is the diatomic hydrogen (H), with a bond length of 0.74 Å.\n\nEffective molecular radius is the size a molecule displays in solution.\nThe table of permselectivity for different substances contains examples.\n\nThe chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and \"plus\" (+) and \"minus\" (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.\n\nA compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethyl alcohol or ethanol is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.\n\nThe molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.\n\nThe empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula CH, but the simplest integer ratio of elements is CH.\n\nThe molecular mass can be calculated from the chemical formula and is expressed in conventional atomic mass units equal to 1/12 of the mass of a neutral carbon-12 (C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.\n\nFor molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.\n\nMolecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.\n\nMolecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to Planck's formula). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.\nSpectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).\n\nMicrowave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures changes in vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy actually measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.\n\nThe study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H, and the simplest of all the chemical bonds is the one-electron bond. H is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.\n\nWhen trying to define rigorously whether an arrangement of atoms is \"sufficiently stable\" to be considered a molecule, IUPAC suggests that it \"must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state\". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.\n\nWhether or not an arrangement of atoms is \"sufficiently stable\" to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.\n\n\n"}
{"id": "19556", "url": "https://en.wikipedia.org/wiki?curid=19556", "title": "Mode (music)", "text": "Mode (music)\n\nIn the theory of Western music, a mode is a type of musical scale coupled with a set of characteristic melodic behaviors. Musical modes have been a part of western musical thought since the Middle Ages, and were inspired by the theory of ancient Greek music. The name \"mode\" is derived from the Latin word \"modus,\" \"measure, standard, manner, way, size, limit of quantity, method\" (; OED).\n\nRegarding the concept of mode as applied to pitch relationships generally, Harold S. Powers proposed mode as a general term but limited for melody types, which were based on the modal interpretation of ancient Greek octave species called \"tonos\" (τόνος) or \"harmonia\" (ἁρμονία), with \"most of the area between ... being in the domain of mode\" . This synthesis between tonus as a church tone and the older meaning associated with an octave species was done by medieval theorists for the Western monodic plainchant tradition (see Hucbald and Aurelian). It is generally assumed that Carolingian theorists imported monastic Octoechos propagated in the patriarchates of Jerusalem (Mar Saba) and Constantinople (Stoudios Monastery) which meant the eight echoi they used for the composition of hymns (e.g., ), though direct adaptations of Byzantine chants in the surviving Gregorian repertoire are extremely rare.\n\nSince the end of the eighteenth century, the term \"mode\" has also applied to pitch structures in non-European musical cultures, sometimes with doubtful compatibility . The concept is also heavily used in regard to the Western polyphony before advent of the so-called common-practice music, as for example \"modale Mehrstimmigkeit\" by Carl Dahlhaus or \"Tonarten\" of the 16th and 17th centuries found by Bernhard Meier (; ).\n\nThe word encompasses several additional meanings, however. Authors from the ninth century until the early eighteenth century (e.g. Guido of Arezzo) sometimes employed the Latin \"modus\" for interval. In the theory of late-medieval mensural polyphony (e.g. Franco of Cologne), \"modus\" is a rhythmic relationship between long and short values or a pattern made from them ; in mensural music most often theorists applied it to division of longa into 3 or 2 breves.\n\nA \"scale\" is series of musical pitches in a distinct order.\n\nThe concept of \"mode\" in Western music theory has three successive stages: in Gregorian chant theory, in Renaissance polyphonic theory, and in tonal harmonic music of the common practice period. In all three contexts, \"mode\" incorporates the idea of the diatonic scale, but differs from it by also involving an element of melody type. This concerns particular repertories of short musical figures or groups of tones within a certain scale so that, depending on the point of view, mode takes on the meaning of either a \"particularized scale\" or a \"generalized tune\". Modern musicological practice has extended the concept of mode to earlier musical systems, such as those of Ancient Greek music, Jewish cantillation, and the Byzantine system of \"octoechos\", as well as to other non-Western types of music (; ).\n\nBy the early 19th century, the word \"mode\" had taken on an additional meaning, in reference to the difference between major and minor keys, specified as \"major mode\" and \"minor mode\". At the same time, composers were beginning to conceive of \"modality\" as something outside of the major/minor system that could be used to evoke religious feelings or to suggest folk-music idioms .\n\nEarly Greek treatises describe three interrelated concepts that are related to the later, medieval idea of \"mode\": (1) scales (or \"systems\"), (2) \"tonos\"—pl. \"tonoi\"—(the more usual term used in medieval theory for what later came to be called \"mode\"), and (3) \"harmonia\" (harmony)—pl. \"harmoniai\"—this third term subsuming the corresponding \"tonoi\" but not necessarily the converse .\n\nThe Greek scales in the Aristoxenian tradition were (; ):\n\n\nThese names are derived from Ancient Greek subgroups (Dorians), one small region in central Greece (Locris), and certain neighboring (non-Greek) peoples from Asia Minor (Lydia, Phrygia). The association of these ethnic names with the octave species appears to precede Aristoxenus, who criticized their application to the \"tonoi\" by the earlier theorists whom he called the Harmonicists .\n\nDepending on the positioning (spacing) of the interposed tones in the tetrachords, three \"genera\" of the seven octave species can be recognized. The diatonic genus (composed of tones and semitones), the chromatic genus (semitones and a minor third), and the enharmonic genus (with a major third and two quarter tones or dieses) . The framing interval of the perfect fourth is fixed, while the two internal pitches are movable. Within the basic forms, the intervals of the chromatic and diatonic genera were varied further by three and two \"shades\" (\"chroai\"), respectively (; ).\n\nIn contrast to the medieval modal system, these scales and their related \"tonoi\" and \"harmoniai\" appear to have had no hierarchical relationships amongst the notes that could establish contrasting points of tension and rest, although the \"mese\" (\"middle note\") may have had some sort of gravitational function .\n\nThe term \"tonos\" (pl. \"tonoi\") was used in four senses: \"as note, interval, region of the voice, and pitch. We use it of the region of the voice whenever we speak of Dorian, or Phrygian, or Lydian, or any of the other tones\" . Cleonides attributes thirteen \"tonoi\" to Aristoxenus, which represent a progressive transposition of the entire system (or scale) by semitone over the range of an octave between the Hypodorian and the Hypermixolydian . Aristoxenus's transpositional \"tonoi\", according to , were named analogously to the octave species, supplemented with new terms to raise the number of degrees from seven to thirteen. However, according to the interpretation of at least three modern authorities, in these transpositional \"tonoi\" the Hypodorian is the lowest, and the Mixolydian next-to-highest—the reverse of the case of the octave species (; ; ), with nominal base pitches as follows (descending order):\n\nPtolemy, in his \"Harmonics\", ii.3–11, construed the \"tonoi\" differently, presenting all seven octave species within a fixed octave, through chromatic inflection of the scale degrees (comparable to the modern conception of building all seven modal scales on a single tonic). In Ptolemy's system, therefore there are only seven \"tonoi\" (; ). Pythagoras also construed the intervals arithmetically (if somewhat more rigorously, initially allowing for 1:1 = Unison, 2:1 = Octave, 3:2 = Fifth, 4:3 = Fourth and 5:4 = Major Third within the octave). In their diatonic genus, these \"tonoi\" and corresponding \"harmoniai\" correspond with the intervals of the familiar modern major and minor scales. See Pythagorean tuning and Pythagorean interval.\n\nIn music theory the Greek word \"harmonia\" can signify the enharmonic genus of tetrachord, the seven octave species, or a style of music associated with one of the ethnic types or the \"tonoi\" named by them .\n\nParticularly in the earliest surviving writings, \"harmonia\" is regarded not as a scale, but as the epitome of the stylised singing of a particular district or people or occupation . When the late 6th-century poet Lasus of Hermione referred to the Aeolian \"harmonia\", for example, he was more likely thinking of a melodic style characteristic of Greeks speaking the Aeolic dialect than of a scale pattern . By the late fifth century BC these regional types are being described in terms of differences in what is called \"harmonia\"—a word with several senses, but here referring to the pattern of intervals between the notes sounded by the strings of a lyra or a kithara. However, there is no reason to suppose that, at this time, these tuning patterns stood in any straightforward and organised relations to one another. It was only around the year 400 that attempts were made by a group of theorists known as the harmonicists to bring these \"harmoniai\" into a single system, and to express them as orderly transformations of a single structure. Eratocles was the most prominent of the harmonicists, though his ideas are known only at second hand, through Aristoxenus, from whom we learn they represented the \"harmoniai\" as cyclic reorderings of a given series of intervals within the octave, producing seven octave species. We also learn that Eratocles confined his descriptions to the enharmonic genus .\n\nIn \"The Republic\", Plato uses the term inclusively to encompass a particular type of scale, range and register, characteristic rhythmic pattern, textual subject, etc. (Mathiesen 2001a, 6(iii)(e)). He held that playing music in a particular \"harmonia\" would incline one towards specific behaviors associated with it, and suggested that soldiers should listen to music in Dorian or Phrygian \"harmoniai\" to help make them stronger, but avoid music in Lydian, Mixolydian or Ionian \"harmoniai\", for fear of being softened. Plato believed that a change in the musical modes of the state would cause a wide-scale social revolution (Plato, Rep. III.10–III.12 = 398C–403C)\n\nThe philosophical writings of Plato and Aristotle (c. 350 BC) include sections that describe the effect of different \"harmoniai\" on mood and character formation. For example, Aristotle in the \"Politics\" (viii:1340a:40–1340b:5):\n\nAristotle continues by describing the effects of rhythm, and concludes about the combined effect of rhythm and \"harmonia\" (viii:1340b:10–13): \n\nThe word \"ethos\" (ἦθος) in this context means \"moral character\", and Greek ethos theory concerns the ways in which music can convey, foster, and even generate ethical states .\n\nSome treatises also describe \"melic\" composition (μελοποιΐα), \"the employment of the materials subject to harmonic practice with due regard to the requirements of each of the subjects under consideration\" —which, together with the scales, \"tonoi\", and \"harmoniai\" resemble elements found in medieval modal theory . According to Aristides Quintilianus (\"On Music\", i.12), melic composition is subdivided into three classes: dithyrambic, nomic, and tragic. These parallel his three classes of rhythmic composition: systaltic, diastaltic and hesychastic. Each of these broad classes of melic composition may contain various subclasses, such as erotic, comic and panegyric, and any composition might be elevating (diastaltic), depressing (systaltic), or soothing (hesychastic) .\n\nAccording to Mathiesen, music as a performing art was called melos, which in its perfect form (μέλος τέλειον) comprised not only the melody and the text (including its elements of rhythm and diction) but also stylized dance movement. Melic and rhythmic composition (respectively, μελοποιΐα and ῥυθμοποιΐα) were the processes of selecting and applying the various components of melos and rhythm to create a complete work. Aristides Quintilianus: \nTonaries, which are lists of chant titles grouped by mode, appear in western sources around the turn of the 9th century. The influence of developments in Byzantium, from Jerusalem and Damascus, for instance the works of Saints John of Damascus (d. 749) and Cosmas of Maiouma (; ), are still not fully understood. The eight-fold division of the Latin modal system, in a four-by-two matrix, was certainly of Eastern provenance, originating probably in Syria or even in Jerusalem, and was transmitted from Byzantine sources to Carolingian practice and theory during the 8th century. However, the earlier Greek model for the Carolingian system was probably ordered like the later Byzantine \"oktōēchos\", that is, with the four principal (authentic) modes first, then the four plagals, whereas the Latin modes were always grouped the other way, with the authentics and plagals paired .\n\nThe 6th century scholar Boethius had translated Greek music theory treatises by Nicomachus and Ptolemy into Latin . Later authors created confusion by applying mode as described by Boethius to explain plainchant modes, which were a wholly different system . In his \"De institutione musica\", book 4 chapter 15, Boethius, like his Hellenistic sources, twice used the term \"harmonia\" to describe what would likely correspond to the later notion of \"mode\", but also used the word \"modus\"—probably translating the Greek word τρόπος (\"tropos\"), which he also rendered as Latin \"tropus\"—in connection with the system of transpositions required to produce seven diatonic octave species , so the term was simply a means of describing transposition and had nothing to do with the church modes .\n\nLater, 9th-century theorists applied Boethius’s terms \"tropus\" and \"modus\" (along with \"tonus\") to the system of church modes. The treatise \"De Musica\" (or \"De harmonica institutione\") of Hucbald synthesized the three previously disparate strands of modal theory: chant theory, the Byzantine \"oktōēchos\" and Boethius's account of Hellenistic theory . The late 9th-/early 10th-century compilation known as the \"Alia musica\" imposed the seven octave transpositions, known as \"tropus\" and described by Boethius, onto the eight church modes , but its compilator also mentions the Greek (Byzantine) echoi translated by the Latin term \"sonus\". Thus, the names of the modes became associated with the eight church tones and their modal formulas, but this medieval interpretation does actually not fit to the concept of the Ancient Greek harmonics treatises. The understanding of mode today does often not reflect that it is made of different concepts which cannot fit altogether.\nAccording to Carolingian theorists the eight church modes, or Gregorian modes, can be divided into four pairs, where each pair shares the \"final\" note and the four notes above the final, but they have different intervals concerning the species of the fifth. If the octave is completed by adding three notes above the fifth, the mode is termed \"authentic\", but if the octave is completed by adding three notes below, it is called \"plagal\" (from Greek πλάγιος, \"oblique, sideways\"). Otherwise explained: if the melody moves mostly above the final, with an occasional cadence to the sub-final, the mode is authentic. Plagal modes shift range and also explore the fourth below the final as well as the fifth above. In both cases, the strict ambitus of the mode is one octave. A melody that remains confined to the mode's ambitus is called \"perfect\"; if it falls short of it, \"imperfect\"; if it exceeds it, \"superfluous\"; and a melody that combines the ambituses of both the plagal and authentic is said to be in a \"mixed mode\" .\n\nAlthough the earlier (Greek) model for the Carolingian system was probably ordered like the Byzantine \"oktōēchos\", with the four authentic modes first, followed by the four plagals, the earliest extant sources for the Latin system are organized in four pairs of authentic and plagal modes sharing the same final: protus authentic/plagal, deuterus authentic/plagal, tritus authentic/plagal, and tetrardus authentic/plagal .\n\nEach mode has, in addition to its final, a \"reciting tone\", sometimes called the \"dominant\" (; ). It is also sometimes called the \"tenor\", from Latin \"tenere\" \"to hold\", meaning the tone around which the melody principally centres . The reciting tones of all authentic modes began a fifth above the final, with those of the plagal modes a third above. However, the reciting tones of modes 3, 4, and 8 rose one step during the tenth and eleventh centuries with 3 and 8 moving from B to C (half step) and that of 4 moving from G to A (whole step) .\n\nAfter the reciting tone, every mode is distinguished by scale degrees called \"mediant\" and \"participant\". The mediant is named from its position between the final and reciting tone. In the authentic modes it is the third of the scale, unless that note should happen to be B, in which case C substitutes for it. In the plagal modes, its position is somewhat irregular. The participant is an auxiliary note, generally adjacent to the mediant in authentic modes and, in the plagal forms, coincident with the reciting tone of the corresponding authentic mode (some modes have a second participant) .\n\nOnly one accidental is used commonly in Gregorian chant—B may be lowered by a half-step to B. This usually (but not always) occurs in modes V and VI, as well as in the upper tetrachord of IV, and is optional in other modes except III, VII and VIII .\n\nIn 1547, the Swiss theorist Henricus Glareanus published the \"Dodecachordon\", in which he solidified the concept of the church modes, and added four additional modes: the Aeolian (mode 9), Hypoaeolian (mode 10), Ionian (mode 11), and Hypoionian (mode 12). A little later in the century, the Italian Gioseffo Zarlino at first adopted Glarean's system in 1558, but later (1571 and 1573) revised the numbering and naming conventions in a manner he deemed more logical, resulting in the widespread promulgation of two conflicting systems. Zarlino's system reassigned the six pairs of authentic–plagal mode numbers to finals in the order of the natural hexachord, C D E F G A, and transferred the Greek names as well, so that modes 1 through 8 now became C-authentic to F-plagal, and were now called by the names Dorian to Hypomixolydian. The pair of G modes were numbered 9 and 10 and were named Ionian and Hypoionian, while the pair of A modes retained both the numbers and names (11, Aeolian, and 12 Hypoaeolian) of Glarean's system. While Zarlino's system became popular in France, Italian composers preferred Glarean's scheme because it retained the traditional eight modes, while expanding them. Luzzasco Luzzaschi was an exception in Italy, in that he used Zarlino’s new system .\n\nIn the late-eighteenth and nineteenth centuries, some chant reformers (notably the editors of the Mechlin, Pustet-Ratisbon (Regensburg), and Rheims-Cambrai Office-Books, collectively referred to as the Cecilian Movement) renumbered the modes once again, this time retaining the original eight mode numbers and Glareanus's modes 9 and 10, but assigning numbers 11 and 12 to the modes on the final B, which they named Locrian and Hypolocrian (even while rejecting their use in chant). The Ionian and Hypoionian modes (on C) become in this system modes 13 and 14 .\n\nGiven the confusion between ancient, medieval, and modern terminology, \"today it is more consistent and practical to use the traditional designation of the modes with numbers one to eight\" (), using Roman numeral (I–VIII), rather than using the pseudo-Greek naming system. Medieval terms, first used in Carolingian treatises, later in Aquitanian tonaries, are still used by scholars today: the Greek ordinals (\"first\", \"second\", etc.) transliterated into the Latin alphabet protus (πρῶτος), deuterus (δεύτερος), tritus (τρίτος), and tetrardus (τέταρτος). In practice they can be specified as authentic or as plagal like \"protus authentus / plagalis\".\n\nA mode indicated a primary pitch (a final); the organization of pitches in relation to the final; suggested range; melodic formulas associated with different modes; location and importance of cadences; and affect (i.e., emotional effect/character). Liane Curtis writes that \"Modes should not be equated with scales: principles of melodic organization, placement of cadences, and emotional affect are essential parts of modal content\" in Medieval and Renaissance music (, in ).\n\nCarl lists \"three factors that form the respective starting points for the modal theories of Aurelian of Réôme, Hermannus Contractus, and Guido of Arezzo:\nThe oldest medieval treatise regarding modes is \"Musica disciplina\" by Aurelian of Réôme (dating from around 850) while Hermannus Contractus was the first to define modes as partitionings of the octave . However, the earliest Western source using the system of eight modes is the Tonary of St Riquier, dated between about 795 and 800 .\n\nVarious interpretations of the \"character\" imparted by the different modes have been suggested. Three such interpretations, from Guido of Arezzo (995–1050), Adam of Fulda (1445–1505), and Juan de Espinosa Medrano (1632–1688), follow:\n\nThe modern Western modes consist of seven scales related to the familiar major and minor keys.\n\nAlthough the names of the modern modes are Greek and some have names used in ancient Greek theory for some of the \"harmoniai\", the names of the modern modes are conventional and do not indicate a link between them and ancient Greek theory, and they do not present the sequences of intervals found even in the diatonic genus of the Greek octave species sharing the same name.\n\nModern Western modes use the same set of notes as the major scale, in the same order, but starting from one of its seven degrees in turn as a \"tonic\", and so present a different sequence of whole and half steps. The interval sequence of the major scale being W-W-H-W-W-W-H, where \"H\" means a semitone (half step) and \"W\" means a whole tone (whole step), it is thus possible to generate the following scales:\n\nFor the sake of simplicity, the examples shown above are formed by natural notes (also called \"white-notes\", as they can be played using the white keys of a piano keyboard). However, any transposition of each of these scales is a valid example of the corresponding mode. In other words, transposition preserves mode.\n\nEach mode has characteristic intervals and chords that give it its distinctive sound.\nThe following is an analysis of each of the seven modern modes. The examples are provided in a key signature with no sharps or flats (scales composed of natural notes).\n\nIonian may arbitrarily be designated the first mode. It is the modern major scale. The example composed of natural notes begins on C, and is also known as the C-major scale: \n\nDorian is the second mode. The example composed of natural notes begins on D: \n\nThe Dorian mode is very similar to the modern natural minor scale (see Aeolian mode below). The only difference with respect to the natural minor scale is in the sixth scale degree, which is a major sixth (M6) above the tonic, rather than a minor sixth (m6).\n\n\nPhrygian is the third mode. The example composed of natural notes starts on E: \nThe Phrygian mode is very similar to the modern natural minor scale (see Aeolian mode below). The only difference with respect to the natural minor scale is in the second scale degree, which is a minor second (m2) above the tonic, rather than a major second (M2).\n\n\nLydian is the fourth mode. The example composed of natural notes starts on F: \nThe single tone that differentiates this scale from the major scale (Ionian mode) is its fourth degree, which is an augmented fourth (A4) above the tonic (F), rather than a perfect fourth (P4).\n\n\nMixolydian is the fifth mode. The example composed of natural notes begins on G: \nThe single tone that differentiates this scale from the major scale (Ionian mode), is its seventh degree, which is a minor seventh (m7) above the tonic (G), rather than a major seventh (M7). Therefore, the seventh scale degree becomes a subtonic to the tonic because it is now a whole tone lower than the tonic, in contrast to the seventh degree in the major scale, which is a semitone tone lower than the tonic (leading-tone).\n\n\nAeolian is the sixth mode. It is also called the natural minor scale. The example composed of natural notes begins on A, and is also known as the A natural-minor scale:\n\nLocrian is the seventh mode. The example composed of natural notes begins on B:\nThe distinctive scale degree here is the diminished fifth (d5). This makes the tonic triad diminished, so this mode is the only one in which the chords built on the tonic and dominant scale degrees have their roots separated by a diminished, rather than perfect, fifth. Similarly the tonic seventh chord is half-diminished.\n\nThe modes can be arranged in the following sequence, which follows the circle of fifths. In this sequence, each mode has one more lowered interval relative to the tonic than the mode preceding it. Thus taking Lydian as reference, Ionian (major) has a lowered fourth; Mixolydian, a lowered fourth and seventh; Dorian, a lowered fourth, seventh, and third; Aeolian (Natural Minor), a lowered fourth, seventh, third, and sixth; Phrygian, a lowered fourth, seventh, third, sixth, and second; and Locrian, a lowered fourth, seventh, third, sixth, second, and fifth. Put another way, the augmented fourth of the Lydian scale has been reduced to a perfect fourth in Ionian, the major seventh in Ionian, to a minor seventh in Mixolydian, etc.\nThe first three modes are sometimes called major (; ; ), the next three minor (; ; ), and the last one diminished (Locrian), according to the quality of their tonic triads. The Locrian mode is traditionally considered theoretical rather than practical because the triad built on the first scale degree is diminished. Because diminished triads are not consonant they do not lend themselves to cadential endings and cannot be tonicized according to traditional practice.\n\nThe intervals (above the respective tonics) shown in the above table for Ionian mode, which is used for major keys, are all perfect or major. In contrast, it is not the case that the intervals shown for Aeolian mode, which is used for minor keys, are all perfect or minor, though that is the case for Phrygian mode. See major and minor intervals for more information.\n\nThe Ionian mode () corresponds to the major scale. Scales in the Lydian mode () are major scales with an augmented fourth. The Mixolydian mode () corresponds to the major scale with a minor seventh.\n\nThe Aeolian mode () is identical to the natural minor scale. The Dorian mode () corresponds to the natural minor scale with a major sixth. The Phrygian mode () corresponds to the natural minor scale with a minor second.\n\nThe Locrian () is neither a major nor a minor mode because, although its third scale degree is minor, the fifth degree is diminished instead of perfect. For this reason it is sometimes called a \"diminished\" scale, though in jazz theory this term is also applied to the octatonic scale. This interval is enharmonically equivalent to the augmented fourth found between scale-degrees 1 and 4 in the Lydian mode and is also referred to as the tritone.\n\nUse and conception of modes or modality today is different from that in early music. As Jim Samson explains, \"Clearly any comparison of medieval and modern modality would recognize that the latter takes place against a background of some three centuries of harmonic tonality, permitting, and in the nineteenth century requiring, a dialogue between modal and diatonic procedure\" . Indeed, when 19th-century composers revived the modes, they rendered them more strictly than Renaissance composers had, to make their qualities distinct from the prevailing major-minor system. Renaissance composers routinely sharped leading tones at cadences and lowered the fourth in the Lydian mode .\n\nThe Ionian, or Iastian (; ; ; ; ; ; ; ) mode is another name for the major scale used in much Western music. The Aeolian forms the base of the most common Western minor scale; in modern practice the Aeolian mode is differentiated from the minor by using only the seven notes of the Aeolian scale. By contrast, minor mode compositions of the common practice period frequently raise the seventh scale degree by a semitone to strengthen the cadences, and in conjunction also raise the sixth scale degree by a semitone to avoid the awkward interval of an augmented second. This is particularly true of vocal music .\n\nTraditional folk music provides countless examples of modal melodies. For example, Irish traditional music makes extensive usage not only of the major mode, but also the Mixolydian, Dorian, and Aeolian modes . Much Flamenco music is in the Phrygian mode, though frequently with the third and seventh degrees raised by a semitone .\n\nA significant amount of modern film music is based on the Lydian, particularly the works of John Williams.\n\nZoltán Kodály, Gustav Holst, Manuel de Falla use modal elements as modifications of a diatonic background, while in the music of Debussy and Béla Bartók modality replaces diatonic tonality ().\n\nWhile the term \"mode\" is still most commonly understood to refer to Ionian, Dorian, Phrygian, Lydian, Mixolydian, Aeolian, or Locrian scales, in modern music theory the word is sometimes applied to scales other than the diatonic. This is seen, for example, in \"melodic minor\" scale harmony, which is based on the seven rotations of the ascending melodic minor scale, yielding some interesting scales as shown below. The \"chord\" row lists tetrads that can be built from the pitches in the given mode (Jazz notation Δ is for M). See also Avoid note.\n\nThe number of possible modes for any intervallic set is dictated by the pattern of intervals in the scale. For scales built of a pattern of intervals that only repeats at the octave (like the diatonic set), the number of modes is equal to the number of notes in the scale. Scales with a recurring interval pattern smaller than an octave, however, have only as many modes as notes within that subdivision: e.g., the diminished scale, which is built of alternating whole and half steps, has only two distinct modes, since all odd-numbered modes are equivalent to the first (starting with a whole step) and all even-numbered modes are equivalent to the second (starting with a half step). The chromatic and whole-tone scales, each containing only steps of uniform size, have only a single mode each, as any rotation of the sequence results in the same sequence. Another general definition excludes these equal-division scales, and defines modal scales as subsets of them: \"If we leave out certain steps of a[n equal-step] scale we get a modal construction\" (Karlheinz Stockhausen, in ). In \"Messiaen's narrow sense, \"a mode is any scale\" made up from the 'chromatic total,' the twelve tones of the tempered system\" .\n\n\n\n\n\n"}
{"id": "19559", "url": "https://en.wikipedia.org/wiki?curid=19559", "title": "Mechanics", "text": "Mechanics\n\nMechanics (Greek ) is that area of science concerned with the behaviour of physical bodies when subjected to forces or displacements, and the subsequent effects of the bodies on their environment.\nThe scientific discipline has its origins in Ancient Greece with the writings of Aristotle and Archimedes (see History of classical mechanics and Timeline of classical mechanics). During the early modern period, scientists such as Galileo, Kepler, and Newton laid the foundation for what is now known as classical mechanics.\nIt is a branch of classical physics that deals with particles that are either at rest or are moving with velocities significantly less than the speed of light. \nIt can also be defined as a branch of science which deals with the motion of and forces on objects. The field is yet less widely understood in terms of quantum theory. \n\nHistorically, classical mechanics came first and quantum mechanics is a comparatively recent development. Classical mechanics originated with Isaac Newton's laws of motion in Philosophiæ Naturalis Principia Mathematica; Quantum Mechanics was developed in the early 20th century. Both are commonly held to constitute the most certain knowledge that exists about physical nature. \n\nClassical mechanics has especially often been viewed as a model for other so-called exact sciences. Essential in this respect is the extensive use of mathematics in theories, as well as the decisive role played by experiment in generating and testing them.\n\nQuantum mechanics is of a bigger scope, as it encompasses classical mechanics as a sub-discipline which applies under certain restricted circumstances. According to the correspondence principle, there is no contradiction or conflict between the two subjects, each simply pertains to specific situations. The correspondence principle states that the behavior of systems described by quantum theories reproduces classical physics in the limit of large quantum numbers. Quantum mechanics has superseded classical mechanics at the foundation level and is indispensable for the explanation and prediction of processes at the molecular, atomic, and sub-atomic level. However, for macroscopic processes classical mechanics is able to solve problems which are unmanageably difficult in quantum mechanics and hence remains useful and well used.\nModern descriptions of such behavior begin with a careful definition of such quantities as displacement (distance moved), time, velocity, acceleration, mass, and force. Until about 400 years ago, however, motion was explained from a very different point of view. For example, following the ideas of Greek philosopher and scientist Aristotle, scientists reasoned that a cannonball falls down because its natural position is in the Earth; the sun, the moon, and the stars travel in circles around the earth because it is the nature of heavenly objects to travel in perfect circles.\n\nOften cited as father to modern science, Galileo brought together the ideas of other great thinkers of his time and began to calculate motion in terms of distance traveled from some starting position and the time that it took. He showed that the speed of falling objects increases steadily during the time of their fall. This acceleration is the same for heavy objects as for light ones, provided air friction (air resistance) is discounted. The English mathematician and physicist Isaac Newton improved this analysis by defining force and mass and relating these to acceleration. For objects traveling at speeds close to the speed of light, Newton’s laws were superseded by Albert Einstein’s theory of relativity. [A sentence illustrating the computational complication of Einstein's theory of relativity.] For atomic and subatomic particles, Newton’s laws were superseded by quantum theory. For everyday phenomena, however, Newton’s three laws of motion remain the cornerstone of dynamics, which is the study of what causes motion.\n\nIn analogy to the distinction between quantum and classical mechanics, Einstein's general and special theories of relativity have expanded the scope of Newton and Galileo's formulation of mechanics. The differences between relativistic and Newtonian mechanics become significant and even dominant as the velocity of a massive body approaches the speed of light. For instance, in Newtonian mechanics, Newton's laws of motion specify that F = \"ma, whereas in relativistic mechanics and Lorentz transformations, which were first discovered by Hendrik Lorentz, F = γ\"ma (where γ is the Lorentz factor, which is almost equal to 1 for low speeds).\n\nRelativistic corrections are also needed for quantum mechanics, although general relativity has not been integrated. The two theories remain incompatible, a hurdle which must be overcome in developing a theory of everything.\n\nThe main theory of mechanics in antiquity was Aristotelian mechanics. A later developer in this tradition is Hipparchus.\n\nIn the Middle Ages, Aristotle's theories were criticized and modified by a number of figures, beginning with John Philoponus in the 6th century. A central problem was that of projectile motion, which was discussed by Hipparchus and Philoponus. This led to the development of the theory of impetus by 14th-century French priest Jean Buridan, which developed into the modern theories of inertia, velocity, acceleration and momentum. This work and others was developed in 14th-century England by the Oxford Calculators such as Thomas Bradwardine, who studied and formulated various laws regarding falling bodies.\n\nOn the question of a body subject to a constant (uniform) force, the 12th-century Jewish-Arab Nathanel (Iraqi, of Baghdad) stated that constant force imparts constant acceleration, while the main properties are uniformly accelerated motion (as of falling bodies) was worked out by the 14th-century Oxford Calculators.\n\nTwo central figures in the early modern age are Galileo Galilei and Isaac Newton. Galileo's final statement of his mechanics, particularly of falling bodies, is his \"Two New Sciences\" (1638). Newton's 1687 \"Philosophiæ Naturalis Principia Mathematica\" provided a detailed mathematical account of mechanics, using the newly developed mathematics of calculus and providing the basis of Newtonian mechanics.\n\nThere is some dispute over priority of various ideas: Newton's \"Principia\" is certainly the seminal work and has been tremendously influential, and the systematic mathematics therein did not and could not have been stated earlier because calculus had not been developed. However, many of the ideas, particularly as pertain to inertia (impetus) and falling bodies had been developed and stated by earlier researchers, both the then-recent Galileo and the less-known medieval predecessors. Precise credit is at times difficult or contentious because scientific language and standards of proof changed, so whether medieval statements are \"equivalent\" to modern statements or \"sufficient\" proof, or instead \"similar\" to modern statements and \"hypotheses\" is often debatable.\n\nTwo main modern developments in mechanics are general relativity of Einstein, and quantum mechanics, both developed in the 20th century based in part on earlier 19th-century ideas. The development in the modern continuum mechanics, particularly in the areas of elasticity, plasticity, fluid dynamics, electrodynamics and thermodynamics of deformable media, started in the second half of the 20th century.\n\nThe often-used term body needs to stand for a wide assortment of objects, including particles, projectiles, spacecraft, stars, parts of machinery, parts of solids, parts of fluids (gases and liquids), etc.\n\nOther distinctions between the various sub-disciplines of mechanics, concern the nature of the bodies being described. Particles are bodies with little (known) internal structure, treated as mathematical points in classical mechanics. Rigid bodies have size and shape, but retain a simplicity close to that of the particle, adding just a few so-called degrees of freedom, such as orientation in space.\n\nOtherwise, bodies may be semi-rigid, i.e. elastic, or non-rigid, i.e. fluid. These subjects have both classical and quantum divisions of study.\n\nFor instance, the motion of a spacecraft, regarding its orbit and attitude (rotation), is described by the relativistic theory of classical mechanics, while the analogous movements of an atomic nucleus are described by quantum mechanics.\n\nThe following are two lists of various subjects that are studied in mechanics.\n\nNote that there is also the \"theory of fields\" which constitutes a separate discipline in physics, formally treated as distinct from mechanics, whether classical fields or quantum fields. But in actual practice, subjects belonging to mechanics and fields are closely interwoven. Thus, for instance, forces that act on particles are frequently derived from fields (electromagnetic or gravitational), and particles generate fields by acting as sources. In fact, in quantum mechanics, particles themselves are fields, as described theoretically by the wave function.\n\nThe following are described as forming classical mechanics:\n\nThe following are catgorized as being part of quantum mechanics:\n\n\n\n\n"}
{"id": "19562", "url": "https://en.wikipedia.org/wiki?curid=19562", "title": "Mandelbrot set", "text": "Mandelbrot set\n\nThe Mandelbrot set is the set of complex numbers formula_1 for which the function formula_2 does not diverge when iterated from formula_3, i.e., for which the sequence formula_4, formula_5, etc., remains bounded in absolute value.\nIts definition and name are due to Adrien Douady, in tribute to the mathematician Benoit Mandelbrot. The set is connected to a Julia set, and related Julia sets produce similarly complex fractal shapes. \n\nMandelbrot set images may be created by sampling the complex numbers and testing, for each sample point formula_1, whether the sequence formula_7 goes to infinity (in practice -- whether it leaves some predetermined bounded neighborhood of 0 after a predetermined number of iterations). Treating the real and imaginary parts of formula_1 as image coordinates on the complex plane, pixels may then be coloured according to how soon the sequence formula_9 crosses an arbitrarily chosen threshold, with a special color (usually black) used for the values of formula_1 for which the sequence has not crossed the threshold after the predetermined number of iterations (this is necessary to clearly distinguish the Mandelbrot set image from the image of its complement). If formula_1 is held constant and the initial value of formula_12—denoted by formula_13—is variable instead, one obtains the corresponding Julia set for each point formula_1 in the parameter space of the simple function. \n\nImages of the Mandelbrot set exhibit an elaborate and infinitely complicated boundary that reveals progressively ever-finer recursive detail at increasing magnifications. The \"style\" of this repeating detail depends on the region of the set being examined. The set's boundary also incorporates smaller versions of the main shape, so the fractal property of self-similarity applies to the entire set, and not just to its parts.\n\nThe Mandelbrot set has become popular outside mathematics both for its aesthetic appeal and as an example of a complex structure arising from the application of simple rules. It is one of the best-known examples of mathematical visualization and mathematical beauty.\n\nThe Mandelbrot set has its place in complex dynamics, a field first investigated by the French mathematicians Pierre Fatou and Gaston Julia at the beginning of the 20th century. This fractal was first defined and drawn in 1978 by Robert W. Brooks and Peter Matelski as part of a study of Kleinian groups. On 1 March 1980, at IBM's Thomas J. Watson Research Center in Yorktown Heights, New York, Benoit Mandelbrot first saw a visualization of the set.\n\nMandelbrot studied the parameter space of quadratic polynomials in an article that appeared in 1980. The mathematical study of the Mandelbrot set really began with work by the mathematicians Adrien Douady and John H. Hubbard, who established many of its fundamental properties and named the set in honor of Mandelbrot for his influential work in fractal geometry.\n\nThe mathematicians Heinz-Otto Peitgen and Peter Richter became well known for promoting the set with photographs, books, and an internationally touring exhibit of the German Goethe-Institut.\n\nThe cover article of the August 1985 \"Scientific American\" introduced a wide audience to the algorithm for computing the Mandelbrot set. The cover featured an image located at -0.909 + -0.275 and was created by Peitgen, et al. The Mandelbrot set became prominent in the mid-1980s as a computer graphics demo, when personal computers became powerful enough to plot and display the set in high resolution.\n\nThe work of Douady and Hubbard coincided with a huge increase in interest in complex dynamics and abstract mathematics, and the study of the Mandelbrot set has been a centerpiece of this field ever since. An exhaustive list of all who have contributed to the understanding of this set since then is long but would include Mikhail Lyubich, Curt McMullen, John Milnor, Mitsuhiro Shishikura and Jean-Christophe Yoccoz.\n\nThe Mandelbrot set is the set of values of \"c\" in the complex plane for which the orbit of 0 under iteration of the quadratic map\n\nremains bounded. That is, a complex number \"c\" is part of the Mandelbrot set if, when starting with \"z\" = 0 and applying the iteration repeatedly, the absolute value of \"z\" remains bounded however large \"n\" gets. This can also be represented as\n\nFor example, letting \"c\" = 1 gives the sequence 0, 1, 2, 5, 26, ..., which tends to infinity. As this sequence is unbounded, 1 is not an element of the Mandelbrot set. On the other hand, \"c\" = −1 gives the sequence 0, −1, 0, −1, 0, ..., which is bounded, and so −1 belongs to the Mandelbrot set.\n\nThe Mandelbrot set formula_18 is defined by a family of complex quadratic polynomials\n\ngiven by\n\nwhere formula_1 is a complex parameter. For each formula_1, one considers the behavior of the sequence\n\nobtained by iterating formula_24 starting at critical point formula_25, which either escapes to infinity or stays within a disk of some finite radius. The Mandelbrot set is defined as the set of all points formula_1 such that the above sequence does \"not\" escape to infinity.\nMore formally, if formula_27 denotes the \"n\"th iterate of formula_24 (i.e. formula_24 composed with itself \"n\" times), the Mandelbrot set is the subset of the complex plane given by\n\nAs explained below, it is in fact possible to simplify this definition by taking formula_31.\n\nMathematically, the Mandelbrot set is just a set of complex numbers. A given complex number \"c\" either belongs to \"M\" or it does not. A picture of the Mandelbrot set can be made by coloring all the points formula_1 that belong to \"M\" black, and all other points white. The more colorful pictures usually seen are generated by coloring points not in the set according to which term in the sequence formula_33 is the first term with an absolute value greater than a certain cutoff value, usually 2. The list of colors used are always predefined by the program used or created by the user, the next color in the list is chosen when the iteration count rises. See the section on computer drawings below for more details.\n\nThe Mandelbrot set can also be defined as the connectedness locus of the family of polynomials formula_24. That is, it is the subset of the complex plane consisting of those parameters formula_1 for which the Julia set of formula_36 is connected.\n\nformula_37 is a polynomial in c and its leading terms settle down as n grows large enough. These terms are given by the Catalan numbers. The polynomials formula_37 are bounded by the generating function for the Catalan numbers and tend to it as n goes to infinity.\n\nThe Mandelbrot set is a compact set, since it is closed and contained in the closed disk of radius 2 around the origin. More specifically, a point formula_1 belongs to the Mandelbrot set if and only if\n\nIn other words, if the absolute value of formula_37 ever becomes larger than 2, the sequence will escape to infinity in a short amount of time.\nThe intersection of formula_18 with the real axis is precisely the interval [−2, 1/4]. The parameters along this interval can be put in one-to-one correspondence with those of the real logistic family,\nThe correspondence is given by\n\nIn fact, this gives a correspondence between the entire parameter space of the logistic family and that of the Mandelbrot set.\n\nDouady and Hubbard have shown that the Mandelbrot set is connected. In fact, they constructed an explicit conformal isomorphism between the complement of the Mandelbrot set and the complement of the closed unit disk. Mandelbrot had originally conjectured that the Mandelbrot set is disconnected. This conjecture was based on computer pictures generated by programs that are unable to detect the thin filaments connecting different parts of formula_18. Upon further experiments, he revised his conjecture, deciding that formula_18 should be connected. There also exists a topological proof to the connectedness that was discovered in 2001 by Jeremy Kahn.\n\nThe dynamical formula for the uniformisation of the complement of the Mandelbrot set, arising from Douady and Hubbard's proof of the connectedness of formula_18, gives rise to external rays of the Mandelbrot set. These rays can be used to study the Mandelbrot set in combinatorial terms and form the backbone of the Yoccoz parapuzzle.\n\nThe boundary of the Mandelbrot set is exactly the bifurcation locus of the quadratic family; that is, the set of parameters formula_1 for which the dynamics changes abruptly under small changes of formula_50 It can be constructed as the limit set of a sequence of plane algebraic curves, the \"Mandelbrot curves\", of the general type known as polynomial lemniscates. The Mandelbrot curves are defined by setting \"p\" = \"z\", \"p\" = \"p\" + \"z\", and then interpreting the set of points |\"p\"(\"z\")| = 2 in the complex plane as a curve in the real Cartesian plane of degree 2 in \"x\" and \"y\". These algebraic curves appear in images of the Mandelbrot set computed using the \"escape time algorithm\" mentioned below.\n\nUpon looking at a picture of the Mandelbrot set, one immediately notices the large cardioid-shaped region in the center. This \"main cardioid\"\nis the region of parameters formula_1 for which formula_36 has an attracting fixed point. It consists of all parameters of the form\nfor some formula_54 in the open unit disk.\n\nTo the left of the main cardioid, attached to it at the point formula_55, a circular-shaped bulb is visible. This bulb consists of those parameters formula_1 for which formula_36 has an attracting cycle of period 2. This set of parameters is an actual circle, namely that of radius 1/4 around −1.\n\nThere are infinitely many other bulbs tangent to the main cardioid: for every rational number formula_58, with \"p\" and \"q\" coprime, there is such a bulb that is tangent at the parameter\n\nThis bulb is called the \"formula_58-bulb\" of the Mandelbrot set. It consists of parameters that have an attracting cycle of period formula_61 and combinatorial rotation number formula_58. More precisely, the formula_61 periodic Fatou components containing the attracting cycle all touch at a common point (commonly called the \"formula_64-fixed point\"). If we label these components formula_65 in counterclockwise orientation, then formula_36 maps the component formula_67 to the component formula_68.\n\nThe change of behavior occurring at formula_69 is known as a bifurcation: the attracting fixed point \"collides\" with a repelling period \"q\"-cycle. As we pass through the bifurcation parameter into the formula_58-bulb, the attracting fixed point turns into a repelling fixed point (the formula_64-fixed point), and the period \"q\"-cycle becomes attracting.\nAll the bulbs we encountered in the previous section were interior components of\nthe Mandelbrot set in which the maps formula_36 have an attracting periodic cycle. Such components are called \"hyperbolic components\".\n\nIt is conjectured that these are the \"only\" interior regions of formula_18. This problem, known as \"density of hyperbolicity\", may be the most important open problem in the field of complex dynamics. Hypothetical non-hyperbolic components of the Mandelbrot set are often referred to as \"queer\" or ghost components.\nFor \"real\" quadratic polynomials, this question was answered positively in the 1990s independently by Lyubich and by Graczyk and Świątek. (Note that hyperbolic components intersecting the real axis correspond exactly to periodic windows in the Feigenbaum diagram. So this result states that such windows exist near every parameter in the diagram.)\n\nNot every hyperbolic component can be reached by a sequence of direct bifurcations from the main cardioid of the Mandelbrot set. However, such a component \"can\" be reached by a sequence of direct bifurcations from the main cardioid of a little Mandelbrot copy (see below).\n\nEach of the hyperbolic components has a \"center\", which is a point \"c\" such that the inner Fatou domain for formula_24 has a super-attracting cycle – that is, that the attraction is infinite (see the image ). This means that the cycle contains the critical point 0, so that 0 is iterated back to itself after some iterations. We therefore have that formula_36formula_76 for some \"n\". If we call this polynomial formula_77 (letting it depend on \"c\" instead of \"z\"), we have that formula_78 and that the degree of formula_77 is formula_80. We can therefore construct the centers of the hyperbolic components by successively solving the equations formula_81. The number of new centers produced in each step is given by Sloane's .\n\nIt is conjectured that the Mandelbrot set is locally connected. This famous conjecture is known as \"MLC\" (for \"Mandelbrot locally connected\"). By the work of Adrien Douady and John H. Hubbard, this conjecture would result in a simple abstract \"pinched disk\" model of the Mandelbrot set. In particular, it would imply the important \"hyperbolicity conjecture\" mentioned above.\n\nThe work of Jean-Christophe Yoccoz established local connectivity of the Mandelbrot set at all finitely renormalizable parameters; that is, roughly speaking those contained only in finitely many small Mandelbrot copies. Since then, local connectivity has been proved at many other points of formula_18, but the full conjecture is still open.\n\n The Mandelbrot set is self-similar under magnification in the neighborhoods of the Misiurewicz points. It is also conjectured to be self-similar around generalized Feigenbaum points (e.g., −1.401155 or −0.1528 + 1.0397\"i\"), in the sense of converging to a limit set.\nThe Mandelbrot set in general is not strictly self-similar but it is quasi-self-similar, as small slightly different versions of itself can be found at arbitrarily small scales.\n\nThe little copies of the Mandelbrot set are all slightly different, mostly because of the thin threads connecting them to the main body of the set.\n\nThe Hausdorff dimension of the boundary of the Mandelbrot set equals 2 as determined by a result of Mitsuhiro Shishikura. It is not known whether the boundary of the Mandelbrot set has positive planar Lebesgue measure.\n\nIn the Blum-Shub-Smale model of real computation, the Mandelbrot set is not computable, but its complement is computably enumerable. However, many simple objects (\"e.g.\", the graph of exponentiation) are also not computable in the BSS model. At present, it is unknown whether the Mandelbrot set is computable in models of real computation based on computable analysis, which correspond more closely to the intuitive notion of \"plotting the set by a computer\". Hertling has shown that the Mandelbrot set is computable in this model if the hyperbolicity conjecture is true.\n\nAs a consequence of the definition of the Mandelbrot set, there is a close correspondence between the geometry of the Mandelbrot set at a given point and the structure of the corresponding Julia set. For instance, a point is in the Mandelbrot set exactly when the corresponding Julia set is connected.\n\nThis principle is exploited in virtually all deep results on the Mandelbrot set. For example, Shishikura proved that, for a dense set of parameters in the boundary of the Mandelbrot set, the Julia set has Hausdorff dimension two, and then transfers this information to the parameter plane. Similarly, Yoccoz first proved the local connectivity of Julia sets, before establishing it for the Mandelbrot set at the corresponding parameters. Adrien Douady phrases this principle as:\n\nFor every rational number formula_58, where \"p\" and \"q\" are relatively prime, a hyperbolic component of period \"q\" bifurcates from the main cardioid. The part of the Mandelbrot set connected to the main cardioid at this bifurcation point is called the \"p\"/\"q\"-limb. Computer experiments suggest that the diameter of the limb tends to zero like formula_84. The best current estimate known is the \"Yoccoz-inequality\", which states that the size tends to zero like formula_85.\n\nA period-\"q\" limb will have \"q\" − 1 \"antennae\" at the top of its limb. We can thus determine the period of a given bulb by counting these antennas.\n\nIn an attempt to demonstrate that the thickness of the \"p\"/\"q\"-limb is zero, David Boll carried out a computer experiment in 1991, where he computed the number of iterations required for the series to diverge for z = formula_86 (formula_87 being the location thereof). As the series doesn't diverge for the exact value of z = formula_87, the number of iterations required increases with a small ε. It turns out that multiplying the value of ε with the number of iterations required yields an approximation of π that becomes better for smaller ε. For example, for ε = 0.0000001 the number of iterations is 31415928 and the product is 3.1415928.\n\nThe Fibonacci sequence is a sequence such that every number in the sequence is the sum of the two previous numbers and can be found in the Mandelbrot Set. Starting with the period one cardioid and the period two bulb, the sequence of bulb periods where each successive bulb is the largest bulb attached to the cardioid between the previous two bulbs follows the Fibonacci sequence. For instance, the period three bulb is the next largest bulb after the period two bulb, the period five bulb is the largest between the period two and period three bulbs, and the period eight bulb is the largest between the period three and period five bulbs. These correspond with the numbers of the Fibonacci sequence beginning with 1, 2, 3, 5, 8, 13, 21, and so on.\n\nThe Mandelbrot set shows more intricate detail the closer one looks or magnifies the image, usually called \"zooming in\". The following example of an image sequence zooming to a selected \"c\" value gives an impression of the infinite richness of different geometrical structures and explains some of their typical rules.\n\nThe magnification of the last image relative to the first one is about 10 to 1. Relating to an ordinary monitor, it represents a section of a Mandelbrot set with a diameter of 4 million kilometers. Its border would show an astronomical number of different fractal structures.\n\nThe seahorse \"body\" is composed by 25 \"spokes\" consisting of two groups of 12 \"spokes\" each and one \"spoke\" connecting to the main cardioid. These two groups can be attributed by some kind of metamorphosis to the two \"fingers\" of the \"upper hand\" of the Mandelbrot set; therefore, the number of \"spokes\" increases from one \"seahorse\" to the next by 2; the \"hub\" is a so-called Misiurewicz point. Between the \"upper part of the body\" and the \"tail\" a distorted small copy of the Mandelbrot set called satellite may be recognized.\nThe islands above seem to consist of infinitely many parts like Cantor sets, as is actually the case for the corresponding Julia set \"J\". However, they are connected by tiny structures, so that the whole represents a simply connected set. The tiny structures meet each other at a satellite in the center that is too small to be recognized at this magnification. The value of \"c\" for the corresponding \"J\" is not that of the image center but, relative to the main body of the Mandelbrot set, has the same position as the center of this image relative to the satellite shown in the 6th zoom step.\n\nMultibrot sets are bounded sets found in the complex plane for members of the general monic univariate polynomial family of recursions\n\nFor an integer d, these sets are connectedness loci for the Julia sets built from the same formula. The full cubic connectedness locus has also been studied; here one considers the two-parameter recursion formula_90, whose two critical points are the complex square roots of the parameter \"k\". A parameter is in the cubic connectedness locus if both critical points are stable. For general families of holomorphic functions, the \"boundary\" of the Mandelbrot set generalizes to the bifurcation locus, which is a natural object to study even when the connectedness locus is not useful.\n\nThe Multibrot set is obtained by varying the value of the exponent \"d\". The article has a video that shows the development from \"d\" = 0 to 7, at which point there are 6 i.e. (\"d\" − 1) lobes around the perimeter. A similar development with negative exponents results in (1 − \"d\") clefts on the inside of a ring.\n\nThere is no perfect extension of the Mandelbrot set into 3D. This is because there is no 3D analogue of the complex numbers for it to iterate on. However, there is an extension of the complex numbers into 4 dimensions, called the quaternions, that creates a perfect extension of the Mandelbrot set and the Julia sets into 4 dimensions. These can then be either cross-sectioned or projected into a 3D structure.\n\nOf particular interest is the tricorn fractal, the connectedness locus of the anti-holomorphic family\n\nThe tricorn (also sometimes called the \"Mandelbar\") was encountered by Milnor in his study of parameter slices of real cubic polynomials. It is \"not\" locally connected. This property is inherited by the connectedness locus of real cubic polynomials.\n\nAnother non-analytic generalization is the Burning Ship fractal, which is obtained by iterating the following :\n\nThere are many programs used to generate the Mandelbrot set and other fractals, some of which are described in fractal-generating software. These programs use a variety of algorithms to determine the color of individual pixels and achieve efficient computation.\n\nThe simplest algorithm for generating a representation of the Mandelbrot set is known as the \"escape time\" algorithm. A repeating calculation is performed for each \"x\", \"y\" point in the plot area and based on the behavior of that calculation, a color is chosen for that pixel.\n\nThe \"x\" and \"y\" locations of each point are used as starting values in a repeating, or iterating calculation (described in detail below). The result of each iteration is used as the starting values for the next. The values are checked during each iteration to see whether they have reached a critical \"escape\" condition, or \"bailout\". If that condition is reached, the calculation is stopped, the pixel is drawn, and the next \"x\", \"y\" point is examined. For some starting values, escape occurs quickly, after only a small number of iterations. For starting values very close to but not in the set, it may take hundreds or thousands of iterations to escape. For values within the Mandelbrot set, escape will never occur. The programmer or user must choose how much iteration, or \"depth\", they wish to examine. The higher the maximal number of iterations, the more detail and subtlety emerge in the final image, but the longer time it will take to calculate the fractal image.\n\nEscape conditions can be simple or complex. Because no complex number with a real or imaginary part greater than 2 can be part of the set, a common bailout is to escape when either coefficient exceeds 2. A more computationally complex method that detects escapes sooner, is to compute distance from the origin using the Pythagorean theorem, i.e., to determine the absolute value, or \"modulus\", of the complex number. If this value exceeds 2, or equivalently, when the sum of the squares of the real and imaginary parts exceed 4, the point has reached escape. More computationally intensive rendering variations include the Buddhabrot method, which finds escaping points and plots their iterated coordinates.\n\nThe color of each point represents how quickly the values reached the escape point. Often black is used to show values that fail to escape before the iteration limit, and gradually brighter colors are used for points that escape. This gives a visual representation of how many cycles were required before reaching the escape condition.\n\nTo render such an image, the region of the complex plane we are considering is subdivided into a certain number of pixels. To color any such pixel, let formula_1 be the midpoint of that pixel. We now iterate the critical point 0 under formula_36, checking at each step whether the orbit point has modulus larger than 2. When this is the case, we know that formula_1 does not belong to the Mandelbrot set, and we color our pixel according to the number of iterations used to find out. Otherwise, we keep iterating up to a fixed number of steps, after which we decide that our parameter is \"probably\" in the Mandelbrot set, or at least very close to it, and color the pixel black.\n\nIn pseudocode, this algorithm would look as follows. The algorithm does not use complex numbers and manually simulates complex-number operations using two real numbers, for those who do not have a complex data type. The program may be simplified if the programming language includes complex-data-type operations.\n\nHere, relating the pseudocode to formula_1, formula_12 and formula_36:\nand so, as can be seen in the pseudocode in the computation of \"x\" and \"y\":\n\nTo get colorful images of the set, the assignment of a color to each value of the number of executed iterations can be made using one of a variety of functions (linear, exponential, etc.). One practical way, without slowing down calculations, is to use the number of executed iterations as an entry to a look-up color palette table initialized at startup. If the color table has, for instance, 500 entries, then the color selection is \"n\" mod 500, where \"n\" is the number of iterations.\n\nA more complex coloring method involves using a histogram, which keeps track of how many pixels reached each iteration number, from 1 to \"n\". This method will equally distribute colors to the same overall area, and, importantly, is independent of the maximal number of iterations chosen.\n\nFirst, create an array of size \"n\". For each pixel, which took \"i\" iterations, find the \"i\"th element and increment it. This creates the histogram during computation of the image. Then, when finished, perform a second \"rendering\" pass over each pixel, utilizing the completed histogram. If you had a continuous color palette ranging from 0 to 1, you could find the normalized color of each pixel as follows, using the variables from above.\nThis method may be combined with the smooth coloring method below for more aesthetically pleasing images.\n\nThe escape time algorithm is popular for its simplicity. However, it creates bands of color, which, as a type of aliasing, can detract from an image's aesthetic value. This can be improved using an algorithm known as \"normalized iteration count\", which provides a smooth transition of colors between iterations. The algorithm associates a real number formula_104 with each value of \"z\" by using the connection of the iteration number with the potential function. This function is given by\n\nwhere \"z\" is the value after \"n\" iterations and \"P\" is the power for which \"z\" is raised to in the Mandelbrot set equation (\"z\" = \"z\" + \"c\", \"P\" is generally 2).\n\nIf we choose a large bailout radius \"N\" (e.g., 10), we have that\n\nfor some real number formula_107, and this is\n\nand as \"n\" is the first iteration number such that |\"z\"| > \"N\", the number we subtract from \"n\" is in the interval <nowiki>[0, 1)</nowiki>.\n\nFor the coloring we must have a cyclic scale of colors (constructed mathematically, for instance) and containing \"H\" colors numbered from 0 to \"H\" − 1 (\"H\" = 500, for instance). We multiply the real number formula_107 by a fixed real number determining the density of the colors in the picture, take the integral part of this number modulo \"H\", and use it to look up the corresponding color in the color table.\n\nFor example, modifying the above pseudocode and also using the concept of linear interpolation would yield\nOne can compute the distance from point \"c\" (in exterior or interior) to nearest point on the boundary of the Mandelbrot set.\n\nThe proof of the connectedness of the Mandelbrot set in fact gives a formula for the uniformizing map of the complement of formula_18 (and the derivative of this map). By the Koebe quarter theorem, one can then estimate the distance between the midpoint of our pixel and the Mandelbrot set up to a factor of 4.\n\nIn other words, provided that the maximal number of iterations is sufficiently high, one obtains a picture of the Mandelbrot set with the following properties:\n\nThe distance estimate \"b\" of a pixel \"c\" (a complex number) from the Mandelbrot set is given by\n\nwhere \n\nThe idea behind this formula is simple: When the equipotential lines for the potential function formula_123 lie close, the number formula_124 is large, and conversely, therefore the equipotential lines for the function formula_125 should lie approximately regularly.\n\nFrom a mathematician's point of view, this formula only works in limit where \"n\" goes to infinity, but very reasonable estimates can be found with just a few additional iterations after the main loop exits.\n\nOnce \"b\" is found, by the Koebe 1/4-theorem, we know that there is no point of the Mandelbrot set with distance from \"c\" smaller than \"b/4\".\n\nThe distance estimation can be used for drawing of the boundary of the Mandelbrot set, see the article Julia set.\n\nIt is also possible to estimate the distance of a limitly periodic (i.e., inner) point to the boundary of the Mandelbrot set. The estimate is given by\n\nwhere\n\nAnalogous to the exterior case, once \"b\" is found, we know that all points within the distance of \"b\"/4 from \"c\" are inside the Mandelbrot set.\n\nThere are two practical problems with the interior distance estimate: first, we need to find formula_13 precisely, and second, we need to find formula_127 precisely.\nThe problem with formula_13 is that the convergence to formula_13 by iterating formula_24 requires, theoretically, an infinite number of operations.\nThe problem with any given formula_127 is that, sometimes, due to rounding errors, a period is falsely identified to be an integer multiple of the real period (e.g., a period of 86 is detected, while the real period is only 43=86/2). In such case, the distance is overestimated, i.e., the reported radius could contain points outside the Mandelbrot set.\n\nOne way to improve calculations is to find out beforehand whether the given point lies within the cardioid or in the period-2 bulb. Before passing the complex value through the escape time algorithm, first check that:\n\nwhere \"x\" represents the real value of the point and \"y\" the imaginary value. The first two equations determine that the point is within the cardioid, the last the period-2 bulb.\n\nThe cardioid test can equivalently be performed without the square root:\n\n3rd- and higher-order buds do not have equivalent tests, because they are not perfectly circular. However, it is possible to find whether the points are within circles inscribed within these higher-order bulbs, preventing many, though not all, of the points in the bulb from being iterated.\n\nTo prevent having to do huge numbers of iterations for points in the set, one can perform periodicity checking. Check whether a point reached in iterating a pixel has been reached before. If so, the pixel cannot diverge and must be in the set.\n\nPeriodicity checking is, of course, a trade-off. The need to remember points costs memory and \"data management\" instructions, whereas it saves \"computational\" instructions.\n\nHowever, checking against only one previous iteration can detect many periods with little performance overhead. For example, within the while loop of the pseudocode above, make the following modifications.\n\nIt can be shown that if a solid shape can be drawn on the Mandelbrot set, with all the border colors being the same, then the shape can be filled in with that color. This is a result of the Mandelbrot set being simply connected. Boundary tracing works by following the lemniscates of the various iteration levels (colored bands) all around the set, and then filling the entire band at once. This can be a good speed increase, because it means that large numbers of points can be skipped.\n\nA similar method operating on the same principle uses rectangles instead of arbitrary border shapes. It is usually faster than boundary tracing because it requires fewer calculations to work out the rectangle. It is inefficient, however, because boundaries are not rectangular, and so some areas can be missed. This issue can be minimized by creating a recursive algorithm that, if a rectangle border fails, will subdivide it into four smaller rectangles and test those, and either fill each or subdivide again and repeat the process.\n\nHowever, this only works using discrete colors in the escape time algorithm. It will not work for smooth/continuous coloring.\n\nThe horizontal symmetry of the Mandelbrot set allows for portions of the rendering process to be skipped upon the presence of the real axis in the final image. However, regardless of the portion that gets mirrored, the same number of points will be rendered. \n\nSimple programs and scripts generally tend to set the escape value to two. This process can be improved by utilizing the distance from the origin and the point being rendered with the Pythagorean theorem by summing the squares of the real and imaginary portions of formula_12 and then escaping if the value is larger or equal to four. The result of this optimization is a faster rendering of the image.\n\nVery highly magnified images require more than the standard 64–128 or so bits of precision that most hardware floating-point units provide, requiring renderers to use slow \"bignum\" or \"arbitrary-precision\" math libraries to calculate. However, this can be sped up by the exploitation of perturbation theory. Given\n\nas the iteration, and a small epsilon and delta, it is the case that\n\nor\n\nso if one defines\n\none can calculate a single point (e.g. the center of an image) using high-precision arithmetic (\"z\"), giving a \"reference orbit\", and then compute many points around it in terms of various initial offsets delta plus the above iteration for epsilon, where epsilon-zero is set to 0. For most iterations, epsilon does not need more than 16 significant figures, and consequently hardware floating-point may be used to get a mostly accurate image. There will often be some areas where the orbits of points diverge enough from the reference orbit that extra precision is needed on those points, or else additional local high-precision-calculated reference orbits are needed. By measuring the orbit distance between the reference point and the point calculated with low precision, it can be detected that it is not possible to calculate the point correctly, and the calculation can be stopped. These incorrect points can later be re-calculated e.g. from another closer reference point.\n\nFurther, it is possible to approximate the starting values for the low-precision points with a truncated Taylor series, which often enables a significant amount of iterations to be skipped.\nRenderers implementing these techniques are publicly available and offer speedups for highly magnified images by around two orders of magnitude.\n\nAn alternate explanation of the above: \n\nFor the central point in the disc formula_163 and its iterations formula_164, and an arbitrary point in the disc formula_165 and its iterations formula_166, it is possible to define the following iterative relationship:\n\nWith formula_168. Successive iterations of formula_169 can be found using the following:\n\nNow from the original definition:\n\nIt follows that:\n\nAs the iterative relationship relates an arbitrary point to the central point by a very small change formula_176, then most of the iterations of formula_169 are also small and can be calculated using floating point hardware.\n\nHowever, for every arbitrary point in the disc it is possible to calculate a value for a given formula_178 without having to iterate through the sequence from formula_179, by expressing formula_169 as a power series of formula_176.\n\nWith formula_183.\n\nNow given the iteration equation of formula_184, it is possible to calculate the coefficients of the power series for each formula_169:\n\nTherefore it follows that:\n\nThe cooefficents in the power series can be calculated as iterative series using only values from the central point's iterations formula_193, and do not change for any arbitrary point in the disc. If formula_176 is very small, formula_169 should be calculable to sufficient accuracy using only a few terms of the power series. As the Mandlebrot Escape Contours are 'continuous' over the complex plane, if a points escape time has been calculated, then the escape time of that points neighbours should be similar. Interpolation of the neighbouring points should provide a good estimation of where to start in the formula_169 series. \n\nFurther, separate interpolation of both real axis points and imaginary axis points should provide both an upper and lower bound for the point being calculated. If both results are the same (i.e. both escape or dot not escape) then the difference formula_197 can be used to recuse until both an upper and lower bound can be established. If floating point hardware can be used to iterate the formula_184 series, then there exists a relation between how many iterations can be achieved in the time it takes to use BigNum software to compute a given formula_169. If the difference between the bounds is greater than the number of iterations, it is possible to perform binomial search using BigNum software, successively halving the gap until it becomes more time efficient to find the escape value using floating point hardware.\n\nThe Mandelbrot set is considered by many the most popular fractal, and has been such referenced several times in popular culture.\n\n\n\n"}
{"id": "19565", "url": "https://en.wikipedia.org/wiki?curid=19565", "title": "Michael Mann", "text": "Michael Mann\n\nMichael Kenneth Mann (born February 5, 1943) is an American film director, screenwriter, and producer of film and television who is best known for his distinctive brand of stylized crime drama. His most acclaimed works include the crime films \"Thief\" (1981), \"Manhunter\" (1986), \"Heat\" (1995), and \"Collateral\" (2004), the historical drama \"The Last of the Mohicans\" (1992), and the docudrama \"The Insider\" (1999). He is also known for his role as executive producer on the popular TV series \"Miami Vice\" (1984–89), which he later adapted into a 2006 feature film.\n\nFor his work, he has received nominations from international organizations and juries, including those at the British Academy of Film and Television Arts, Cannes and the Academy of Motion Picture Arts and Sciences. \"Total Film\" ranked Mann No. 28 on its list of the 100 Greatest Directors Ever, \"Sight and Sound\" ranked him No. 5 on their list of the 10 Best Directors of the Last 25 Years, and \"Entertainment Weekly\" ranked Mann No. 8 on their 25 Greatest Active Film Directors list.\n\nMann was born February 5, 1943, in Chicago, Illinois, to a family of Russian Jewish ancestry. He is the son of grocers Esther and Jack Mann.\n\nHe received a B.A. in English at the University of Wisconsin–Madison where he developed interests in history, philosophy and architecture. It was at this time that he saw Stanley Kubrick's \"Dr. Strangelove\" and fell in love with movies. In a recent \"L.A. Weekly\" interview, he describes the film's impact on him: \"It said to my whole generation of filmmakers that you could make an individual statement of high integrity and have that film be successfully seen by a mass audience all at the same time. In other words, you didn't have to be making \"Seven Brides for Seven Brothers\" if you wanted to work in the mainstream film industry, or be reduced to niche filmmaking if you wanted to be serious about cinema. So that's what Kubrick meant, aside from the fact that \"Strangelove\" was a revelation.\" His daughter Ami Canaan Mann is also a film director and producer.\n\nMann later moved to London in the mid 1960s to go to graduate school in cinema. He went on to receive a graduate degree at the London Film School in 1967. He spent seven years in the United Kingdom going to film school and then working on commercials along with contemporaries Alan Parker, Ridley Scott and Adrian Lyne. In 1968, footage he shot of the Paris student revolt for a documentary, \"Insurrection\", aired on NBC's \"First Tuesday\" news program and he developed his '68 experiences into the short film \"Jaunpuri\" which won the Jury Prize at Cannes in 1970.\n\nMann returned to United States after divorcing his first wife in 1971. He went on to direct a road trip documentary, \"17 Days Down the Line\". Three years later, \"Hawaii Five-O\" veteran Robert Lewin gave Mann a shot and a crash course on television writing and story structure. Mann wrote four episodes of \"Starsky and Hutch\" (three in the first series and one in the second) and the pilot episode for \"Vega$\". Around this time, he worked on a show called \"Police Story\" with cop-turned-novelist Joseph Wambaugh. \"Police Story\" concentrated on the detailed realism of a real cop's life and taught Mann that first-hand research was essential to bring authenticity to his work. His first feature movie was a television special called \"The Jericho Mile\", which was released theatrically in Europe. It won the Emmy for best MOW in 1979 and the DGA Best Director award.\n\nHis television work also includes being the executive producer on \"Miami Vice\" and \"Crime Story\". Contrary to popular belief, he was not the creator of these shows, but the executive producer and showrunner. They were produced by his production company and his cinematic influence is felt throughout each show in terms of casting and style. Mann is now known primarily as a feature film director. He has a distinctive style that is reflected in his works: his trademarks are intricate scene setups, during \"Miami Vice\" to such an extent that a whole scene was completely color-coordinated, from props to backgrounds to actors' wardrobes, as well as powerfully-lit night scenes and combining exterior filming in such a way that shots of completely unrelated filming locations can appear as being of the same building or landmark. In terms of sound, he is known for unusual scores, such as Tangerine Dream in \"Thief\" or the new-age score to \"Manhunter\". Dante Spinotti is a frequent cinematographer of Mann's pictures.\n\nMann's first cinema feature as director was \"Thief\" (1981) starring James Caan, a relatively accurate depiction of thieves that operated in New York City and Chicago at that time. Mann used actual former professional burglars to keep the technical scenes as genuine as possible. His next film \"The Keep\" (1983), a supernatural thriller set in Nazi-occupied Romania, was an uncharacteristic choice. Though it was a commercial flop, the film has since attained cult status amongst fans.\n\nIn 1986, Mann was the first to bring Thomas Harris' character of serial killer Hannibal Lecter to the screen with \"Manhunter\", his adaptation of the novel \"Red Dragon\", which starred Brian Cox as a more down-to-earth Hannibal. In an interview on the \"Manhunter\" DVD, star William Petersen comments that because Mann is so focused on his creations, it takes several years for him to complete a film; Petersen believes that this is why Mann does not make films very often.\n\nHe gained widespread recognition in 1992 for his film adaptation of James Fenimore Cooper's novel into the epic film \"The Last of the Mohicans\". His biggest critical successes in the 1990s began with the release of \"Heat\" in 1995 and \"The Insider\" in 1999. The films, which featured Al Pacino with Robert De Niro in \"Heat\" and Al Pacino and Russell Crowe in \"The Insider\", showcased Mann's cinematic style and adeptness at creating rich, complex storylines as well as directing actors. \"The Insider\" was nominated for seven Academy Awards as a result, including a nomination for Mann's direction.\n\nWith his next film, \"Ali\" (2001), starring Will Smith, he started experimenting with digital cameras, while Smith was nominated for an Academy Award for his performance. For his crime film \"Collateral\", which cast Tom Cruise against type by giving him the role of a hitman, Mann shot all of the exterior scenes digitally so that he could achieve more depth and detail during the night scenes while shooting most of the interiors on film stock. In 2004, Mann produced \"The Aviator\", based on the life of Howard Hughes, which he had developed with Leonardo DiCaprio. \"The Aviator\" was nominated for an Academy Award for Best Picture but lost to \"Million Dollar Baby\". After \"Collateral\", Mann directed the film adaptation of \"Miami Vice\" which he also executive produced. It stars a completely new cast with Colin Farrell as Don Johnson's character Sonny Crockett, and Jamie Foxx filling Philip Michael Thomas' shoes.\n\nMann served as a producer with Peter Berg as director for \"The Kingdom\" and \"Hancock\". \"Hancock\" stars Will Smith as a hard-drinking superhero who has fallen out of favor with the public and who begins to have a relationship with the wife (Charlize Theron) of a public relations expert (Jason Bateman), who is helping him to repair his image. Mann also makes a cameo appearance in the film as an executive. In the fall of 2007, Mann directed two commercials for Nike. The ad campaign \"Leave Nothing\" features football action scenes with former NFL players Shawne Merriman and Steven Jackson.\n\nIn 2009, Mann wrote and directed \"Public Enemies\" for Universal Pictures, about the Depression-era crime wave, based on Brian Burrough's nonfiction book, \"Public Enemies: America's Greatest Crime Wave and the Birth of the FBI, 1933–34\". It starred Johnny Depp and Christian Bale. Depp played John Dillinger in the film, and Bale played Melvin Purvis, the FBI agent in charge of capturing Dillinger.\n\nIn January 2010 it was reported by \"Variety\" that Mann, alongside David Milch, would serve as co-executive producer of new TV series \"Luck\". The series was an hour-long HBO production, and Mann directed the series' pilot. Although initially renewed for a second season after the airing of the pilot, it was eventually cancelled due to the death of three horses during production.\n\nOn February 14, 2013, it was announced that Mann had been developing an untitled thriller film with screenwriter Morgan Davis Foehl for over a year, for Legendary Pictures. In May 2013, Mann started filming the thriller, named \"Blackhat\", in Los Angeles, Kuala Lumpur, Hong Kong and Jakarta. The film, starring Chris Hemsworth as a hacker who gets released from prison to pursue a cyberterrorist across the globe, was released on January 16, 2015 by Universal. It received mixed reviews and was a commercial disaster, although many critics included it in their year-end \"best-of\" lists.\n\nIn January 2015, it was reported in \"The New Yorker\" that Mann is developing a film about Ferrari founder Enzo Ferrari. Christian Bale was originally cast to play the title character, but later dropped out citing health concerns due to rapid weight gain required to shoot as per schedule. Financing is coming from Vendian Entertainment in the United States and China's Bliss Media. Distribution rights have already been purchased by Paramount for the US and Bliss Media for China.\n\nMann's films often feature male protagonists, usually highly gifted and independent-minded professionals who struggle to reconcile their mental lives with the demands, both benign and malign, of the exterior world. Importantly, his films often involve a tragic rather than a happy ending, such as in \"Miami Vice\", when of the two undercover police officers, one has his girlfriend (also an undercover officer) come out of a coma and the other tearfully separates from his romantic interest. Mann's films contain fast-paced, artful scenes that strongly depend on powerful music, where often two opposing sides intermix, such as undercover policework and undercover drug trafficking, so that it is hard to distinguish between the two. For example, in \"Heat\", the police detective invites the criminal to meet for coffee, where they discuss their affairs like old business partners. Often it is hard to distinguish between opposing sides (police vs. criminals, etc.), where the actions, dress, and mannerisms of the characters are extremely similar. Also, Mann's work often involves landscapes and modes where the heroic protagonists occupy a somewhat secret world, away from ordinary concerns (law, life and death, money, daily-life survival duties, family duties, and so on), where the secret world may or may not coincide with ordinary reality. Protagonists often find impassioned romantic interests which are severed under tragic situations near the end of the film (\"Last of the Mohicans\", \"Heat\", \"Collateral\", \"Miami Vice, Public Enemies\"). Overall, Mann's films mix artistry (via music, stylishness and emotional intensity) with strong violence and noir-like stoicism.\n\nMann directed the 2002 \"Lucky Star\" advertisement for Mercedes-Benz, which took the form of a film trailer for a purported thriller featuring Benicio del Toro. Mann also directed the 2008 promotional video for Ferrari's California sports car. In 2009 Mann also directed a commercial for Nike that featured several stylistic cues, most notably the use of \"Promontory\" from the soundtrack of \"The Last of the Mohicans\".\n\nMann received an Emmy in 1979 for Outstanding Writing in a Limited Series or a Special for \"The Jericho Mile\". The following year he was honored by the Directors Guild of America for Outstanding Directorial Achievement for \"The Jericho Mile\". In 1990, he won another Emmy for Outstanding Miniseries for \"\". Mann was the recipient of the Humanitas Prize and the Writers Guild of America's Paul Selvin Award in 2000 for \"The Insider\". In 2005, he received the BAFTA Film Award for co-producing \"The Aviator\".\n\nTo date he has received four Academy Award nominations: in 2000, the Best Adapted Screenplay, Best Director and Best Motion Picture of the Year all for \"The Insider\", in 2005 Mann received a nomination for co-producing Martin Scorsese's \"The Aviator\".\n\n\n\n"}
{"id": "19566", "url": "https://en.wikipedia.org/wiki?curid=19566", "title": "Main-group element", "text": "Main-group element\n\nIn chemistry and atomic physics, the main group is the group of elements whose lightest members are represented by helium, lithium, beryllium, boron, carbon, nitrogen, oxygen, and fluorine as arranged in the periodic table of the elements. The main group includes the elements (except hydrogen, which is sometimes not included) in groups 1 and 2 (s-block), and groups 13 to 18 (p-block). The s-block elements are primarily characterised by one main oxidation state, and the p-block elements, when they have multiple oxidation states, often have common oxidation states separated by two units.\n\nMain-group elements (with some of the lighter transition metals) are the most abundant elements on earth, in the solar system, and in the universe. They are sometimes also called the representative elements.\n\nGroup 12 elements are often considered to be transition metals; however, zinc (Zn), cadmium (Cd), and mercury (Hg) share some properties of both groups, and many scientists believe they should be included in the main group. Occasionally, even the group 3 elements as well as the lanthanides and actinides have been included, because especially the group 3 elements and lanthanides are electropositive elements with only one main oxidation state like the group 1 and 2 elements. The position of the actinides is more questionable, but the most common and stable of them, thorium (Th) and uranium (U), are similar to main-group elements as thorium is an electropositive element with only one main oxidation state (+4), and uranium has two main ones separated by two oxidation units (+4 and +6).\n\nIn older nomenclature the main-group elements are groups IA and IIA, and groups IIIB to 0 (CAS groups IIIA to VIIIA). Group 12 is labelled as group IIB in both systems. Group 3 is labelled as group IIIA in the older nomenclature (CAS group IIIB).\n\n"}
{"id": "19567", "url": "https://en.wikipedia.org/wiki?curid=19567", "title": "Microscopy", "text": "Microscopy\n\nMicroscopy is the technical field of using microscopes to view objects and areas of objects that cannot be seen with the naked eye (objects that are not within the resolution range of the normal eye). There are three well-known branches of microscopy: optical, electron, and scanning probe microscopy, along with the emerging field of X-ray microscopy.\n\nOptical microscopy and electron microscopy involve the diffraction, reflection, or refraction of electromagnetic radiation/electron beams interacting with the specimen, and the collection of the scattered radiation or another signal in order to create an image. This process may be carried out by wide-field irradiation of the sample (for example standard light microscopy and transmission electron microscopy) or by scanning a fine beam over the sample (for example confocal laser scanning microscopy and scanning electron microscopy). Scanning probe microscopy involves the interaction of a scanning probe with the surface of the object of interest. The development of microscopy revolutionized biology, gave rise to the field of histology and so remains an essential technique in the life and physical sciences. X-ray microscopy is three-dimensional and non-destructive, allowing for repeated imaging of the same sample for in situ or 4D studies, and providing the ability to \"see inside\" the sample being studied before sacrificing it to higher resolution techniques. A 3D X-ray microscope uses the technique of computed tomography (microCT), rotating the sample 360 degrees and reconstructing the images. CT is typically carried out with a flat panel display. A 3D X-ray microscope employs a range of objectives, e.g., from 4X to 40X, and can also include a flat panel.\n\nThe field of microscopy (optical microscopy) dates back to at least the 17th-century. Earlier microscopes, single lens magnifying glasses with limited magnification, date at least as far back as the wide spread use of lenses in eyeglasses in the 13th century but more advanced compound microscopes first appeared in Europe around 1620 The earliest practitioners of microscopy include Galileo Galilei, who found in 1610 that he could close focus his telescope to view small objects close up and Cornelis Drebbel, who may have invented the compound microscope around 1620 Antonie van Leeuwenhoek developed a very high magnification simple microscope in the 1670's and is often considered to be the first acknowledged microscopist and microbiologist.\n\nOptical or light microscopy involves passing visible light transmitted through or reflected from the sample through a single lens or multiple lenses to allow a magnified view of the sample. The resulting image can be detected directly by the eye, imaged on a photographic plate, or captured digitally. The single lens with its attachments, or the system of lenses and imaging equipment, along with the appropriate lighting equipment, sample stage, and support, makes up the basic light microscope. The most recent development is the digital microscope, which uses a CCD camera to focus on the exhibit of interest. The image is shown on a computer screen, so eye-pieces are unnecessary.\n\nLimitations of standard optical microscopy (bright field microscopy) lie in three areas;\n\n\nLive cells in particular generally lack sufficient contrast to be studied successfully, since the internal structures of the cell are colorless and transparent. The most common way to increase contrast is to stain the different structures with selective dyes, but this often involves killing and fixing the sample. Staining may also introduce artifacts, which are apparent structural details that are caused by the processing of the specimen and are thus not legitimate features of the specimen. In general, these techniques make use of differences in the refractive index of cell structures. Bright field microscopy is comparable to looking through a glass window: one sees not the glass but merely the dirt on the glass. There is a difference, as glass is a denser material, and this creates a difference in phase of the light passing through. The human eye is not sensitive to this difference in phase, but clever optical solutions have been devised to change this difference in phase into a difference in amplitude (light intensity).\n\nIn order to improve specimen contrast or highlight certain structures in a sample, special techniques must be used. A huge selection of microscopy techniques are available to increase contrast or label a sample.\n\n<gallery caption=\"Four examples of transillumination techniques used to generate contrast in a sample of tissue paper. 1.559 μm/pixel.\" align=\"center\">\nImage:Paper_Micrograph_Bright.png|Bright field illumination, sample contrast comes from absorbance of light in the sample.\nImage:Paper_Micrograph_Cross-Polarised.png|Cross-polarized light illumination, sample contrast comes from rotation of polarized light through the sample.\nImage:Paper_Micrograph_Dark.png|Dark field illumination, sample contrast comes from light scattered by the sample.\nImage:Paper_Micrograph_Phase.png|Phase contrast illumination, sample contrast comes from interference of different path lengths of light through the sample.\n</gallery>\n\nBright field microscopy is the simplest of all the light microscopy techniques. Sample illumination is via transmitted white light, i.e. illuminated from below and observed from above. Limitations include low contrast of most biological samples and low apparent resolution due to the blur of out-of-focus material. The simplicity of the technique and the minimal sample preparation required are significant advantages.\n\nThe use of oblique (from the side) illumination gives the image a three-dimensional (3D) appearance and can highlight otherwise invisible features. A more recent technique based on this method is \"Hoffmann's modulation contrast\", a system found on inverted microscopes for use in cell culture. Oblique illumination suffers from the same limitations as bright field microscopy (low contrast of many biological samples; low apparent resolution due to out of focus objects).\n\nDark field microscopy is a technique for improving the contrast of unstained, transparent specimens. Dark field illumination uses a carefully aligned light source to minimize the quantity of directly transmitted (unscattered) light entering the image plane, collecting only the light scattered by the sample. Dark field can dramatically improve image contrast – especially of transparent objects – while requiring little equipment setup or sample preparation. However, the technique suffers from low light intensity in final image of many biological samples and continues to be affected by low apparent resolution.\n\"Rheinberg illumination\" is a special variant of dark field illumination in which transparent, colored filters are inserted just before the condenser so that light rays at high aperture are differently colored than those at low aperture (i.e., the background to the specimen may be blue while the object appears self-luminous red). Other color combinations are possible, but their effectiveness is quite variable.\n\nDispersion staining is an optical technique that results in a colored image of a colorless object. This is an optical staining technique and requires no stains or dyes to produce a color effect. There are five different microscope configurations used in the broader technique of dispersion staining. They include brightfield Becke line, oblique, darkfield, phase contrast, and objective stop dispersion staining.\n\nMore sophisticated techniques will show proportional differences in optical density. Phase contrast is a widely used technique that shows differences in refractive index as difference in contrast. It was developed by the Dutch physicist Frits Zernike in the 1930s (for which he was awarded the Nobel Prize in 1953). The nucleus in a cell for example will show up darkly against the surrounding cytoplasm. Contrast is excellent; however it is not for use with thick objects. Frequently, a halo is formed even around small objects, which obscures detail. The system consists of a circular annulus in the condenser, which produces a cone of light. This cone is superimposed on a similar sized ring within the phase-objective. Every objective has a different size ring, so for every objective another condenser setting has to be chosen. The ring in the objective has special optical properties: it, first of all, reduces the direct light in intensity, but more importantly, it creates an artificial phase difference of about a quarter wavelength. As the physical properties of this direct light have changed, interference with the diffracted light occurs, resulting in the phase contrast image. One disadvantage of phase-contrast microscopy is halo formation (halo-light ring).\n\nSuperior and much more expensive is the use of interference contrast. Differences in optical density will show up as differences in relief. A nucleus within a cell will actually show up as a globule in the most often used differential interference contrast system according to Georges Nomarski. However, it has to be kept in mind that this is an \"optical effect\", and the relief does not necessarily resemble the true shape. Contrast is very good and the condenser aperture can be used fully open, thereby reducing the depth of field and maximizing resolution.\n\nThe system consists of a special prism (Nomarski prism, Wollaston prism) in the condenser that splits light in an ordinary and an extraordinary beam. The spatial difference between the two beams is minimal (less than the maximum resolution of the objective). After passage through the specimen, the beams are reunited by a similar prism in the objective.\n\nIn a homogeneous specimen, there is no difference between the two beams, and no contrast is being generated. However, near a refractive boundary (say a nucleus within the cytoplasm), the difference between the ordinary and the extraordinary beam will generate a relief in the image. Differential interference contrast requires a polarized light source to function; two polarizing filters have to be fitted in the light path, one below the condenser (the polarizer), and the other above the objective (the analyzer).\n\nNote: In cases where the optical design of a microscope produces an appreciable lateral separation of the two beams we have the case of classical interference microscopy, which does not result in relief images, but can nevertheless be used for the quantitative determination of mass-thicknesses of microscopic objects.\n\nAn additional technique using interference is interference reflection microscopy (also known as reflected interference contrast, or RIC). It relies on cell adhesion to the slide to produce an interference signal. If there is no cell attached to the glass, there will be no interference.\n\nInterference reflection microscopy can be obtained by using the same elements used by DIC, but without the prisms. Also, the light that is being detected is reflected and not transmitted as it is when DIC is employed.\n\nWhen certain compounds are illuminated with high energy light, they emit light of a lower frequency. This effect is known as fluorescence. Often specimens show their characteristic autofluorescence image, based on their chemical makeup.\n\nThis method is of critical importance in the modern life sciences, as it can be extremely sensitive, allowing the detection of single molecules. Many different fluorescent dyes can be used to stain different structures or chemical compounds. One particularly powerful method is the combination of antibodies coupled to a fluorophore as in immunostaining. Examples of commonly used fluorophores are fluorescein or rhodamine.\n\nThe antibodies can be tailor-made for a chemical compound. For example, one strategy often in use is the artificial production of proteins, based on the genetic code (DNA). These proteins can then be used to immunize rabbits, forming antibodies which bind to the protein. The antibodies are then coupled chemically to a fluorophore and used to trace the proteins in the cells under study.\n\nHighly efficient fluorescent proteins such as the green fluorescent protein (GFP) have been developed using the molecular biology technique of gene fusion, a process that links the expression of the fluorescent compound to that of the target protein. This combined fluorescent protein is, in general, non-toxic to the organism and rarely interferes with the function of the protein under study. Genetically modified cells or organisms directly express the fluorescently tagged proteins, which enables the study of the function of the original protein in vivo.\n\nGrowth of protein crystals results in both protein and salt crystals. Both are colorless and microscopic. Recovery of the protein crystals requires imaging which can be done by the intrinsic fluorescence of the protein or by using transmission microscopy. Both methods require an ultraviolet microscope as protein absorbs light at 280 nm. Protein will also fluorescence at approximately 353 nm when excited with 280 nm light.\n\nSince fluorescence emission differs in wavelength (color) from the excitation light, an ideal fluorescent image shows only the structure of interest that was labeled with the fluorescent dye. This high specificity led to the widespread use of fluorescence light microscopy in biomedical research. Different fluorescent dyes can be used to stain different biological structures, which can then be detected simultaneously, while still being specific due to the individual color of the dye.\n\nTo block the excitation light from reaching the observer or the detector, filter sets of high quality are needed. These typically consist of an excitation filter selecting the range of excitation wavelengths, a dichroic mirror, and an emission filter blocking the excitation light. Most fluorescence microscopes are operated in the Epi-illumination mode (illumination and detection from one side of the sample) to further decrease the amount of excitation light entering the detector.\n\nAn example of fluorescence microscopy today is two-photon or multi-photon imaging. Two photon imaging allows imaging of living tissues up to a very high depth by enabling greater excitation light penetration and reduced background emission signal.\n\nSee also:\ntotal internal reflection fluorescence microscope\nNeuroscience\n\nConfocal microscopy uses a scanning point of light and a pinhole to prevent out of focus light from reaching the detector. Compared to full sample illumination, confocal microscopy gives slightly higher resolution, and significantly improves optical sectioning. Confocal microscopy is, therefore, commonly used where 3D structure is important.\n\nUsing a plane of light formed by focusing light through a cylindrical lens at a narrow angle or by scanning a line of light in a plane perpendicular to the axis of objective, high resolution optical sections can be taken. Single plane illumination, or light sheet illumination, is also accomplished using beam shaping techniques incorporating multiple-prism beam expanders. The images are captured by CCDs. These variants allow very fast and high signal to noise ratio image capture.\n\nWide-field multiphoton microscopy refers to an optical non-linear imaging technique tailored for ultrafast imaging in which a large area of the object is illuminated and imaged without the need for scanning. High intensities are required to induce non-linear optical processes such as two-photon fluorescence or second harmonic generation. In scanning multiphoton microscopes the high intensities are achieved by tightly focusing the light, and the image is obtained by stage- or beam-scanning the sample. In wide-field multiphoton microscopy the high intensities are best achieved using an optically amplified pulsed laser source to attain a large field of view (~100 µm). The image in this case is obtained as a single frame with a CCD without the need of scanning, making the technique particularly useful to visualize dynamic processes simultaneously across the object of interest. With wide-field multiphoton microscopy the frame rate can be increased up to a 1000-fold compared to multiphoton scanning microscopy.\n\nFluorescence microscopy is a powerful technique to show specifically labeled structures within a complex environment and to provide three-dimensional information of biological structures. However, this information is blurred by the fact that, upon illumination, all fluorescently labeled structures emit light, irrespective of whether they are in focus or not. So an image of a certain structure is always blurred by the contribution of light from structures that are out of focus. This phenomenon results in a loss of contrast especially when using objectives with a high resolving power, typically oil immersion objectives with a high numerical aperture.\n\nHowever, blurring is not caused by random processes, such as light scattering, but can be well defined by the optical properties of the image formation in the microscope imaging system. If one considers a small fluorescent light source (essentially a bright spot), light coming from this spot spreads out further from our perspective as the spot becomes more out of focus. Under ideal conditions, this produces an \"hourglass\" shape of this point source in the third (axial) dimension. This shape is called the point spread function (PSF) of the microscope imaging system. Since any fluorescence image is made up of a large number of such small fluorescent light sources, the image is said to be \"convolved by the point spread function\". The mathematically modeled PSF of a terahertz laser pulsed imaging system is shown on the right.\n\nThe output of an imaging system can be described using the equation:\n\nformula_1\n\nWhere is the additive noise. Knowing this point spread function means that it is possible to reverse this process to a certain extent by computer-based methods commonly known as deconvolution microscopy. There are various algorithms available for 2D or 3D deconvolution. They can be roughly classified in \"nonrestorative\" and \"restorative\" methods. While the nonrestorative methods can improve contrast by removing out-of-focus light from focal planes, only the restorative methods can actually reassign light to its proper place of origin. Processing fluorescent images in this manner can be an advantage over directly acquiring images without out-of-focus light, such as images from confocal microscopy, because light signals otherwise eliminated become useful information. For 3D deconvolution, one typically provides a series of images taken from different focal planes (called a Z-stack) plus the knowledge of the PSF, which can be derived either experimentally or theoretically from knowing all contributing parameters of the microscope.\n\nA multitude of super-resolution microscopy techniques have been developed in recent times which circumvent the diffraction barrier.\n\nThis is mostly achieved by imaging a sufficiently static sample multiple times and either modifying the excitation light or observing stochastic changes in the image. The deconvolution methods described in the previous section, which removes the PSF induced blur and assigns a mathematically 'correct' origin of light, are used, albeit with slightly different understanding of what the value of a pixel mean. Assuming \"most of the time\", one single fluorophore contributes to one single blob on one single taken image, the blobs in the images can be replaced with their calculated position, vastly improving resolution to well below the diffraction limit.\n\nTo realize such assumption, Knowledge of and chemical control over fluorophore photophysics is at the core of these techniques, by which resolutions of ~20 nanometers are regularly obtained.\n\nSerial time encoded amplified microscopy (STEAM) is an imaging method that provides ultrafast shutter speed and frame rate, by using optical image amplification to circumvent the fundamental trade-off between sensitivity and speed, and a single-pixel photodetector to eliminate the need for a detector array and readout time limitations The method is at least 1000 times faster than the state-of-the-art CCD and CMOS cameras. Consequently, it is potentially useful for a broad range of scientific, industrial, and biomedical applications that require high image acquisition rates, including real-time diagnosis and evaluation of shockwaves, microfluidics, MEMS, and laser surgery. \n\nMost modern instruments provide simple solutions for micro-photography and image recording electronically. However such capabilities are not always present and the more experienced microscopist will, in many cases, still prefer a hand drawn image to a photograph. This is because a microscopist with knowledge of the subject can accurately convert a three-dimensional image into a precise two-dimensional drawing. In a photograph or other image capture system however, only one thin plane is ever in good focus.\n\nThe creation of careful and accurate micrographs requires a microscopical technique using a monocular eyepiece. It is essential that both eyes are open and that the eye that is not observing down the microscope is instead concentrated on a sheet of paper on the bench besides the microscope. With practice, and without moving the head or eyes, it is possible to accurately record the observed details by tracing round the observed shapes by simultaneously \"seeing\" the pencil point in the microscopical image.\n\nPracticing this technique also establishes good general microscopical technique. It is always less tiring to observe with the microscope focused so that the image is seen at infinity and with both eyes open at all times.\n\nMicrospectroscopy:spectroscopy with a microscope\n\nAs resolution depends on the wavelength of the light. Electron microscopy has been developed since the 1930s that use electron beams instead of light. Because of the much smaller wavelength of the electron beam, resolution is far higher.\n\nThough less common, X-ray microscopy has also been developed since the late 1940s. The resolution of X-ray microscopy lies between that of light microscopy and electron microscopy.\n\nUntil the invention of sub-diffraction microscopy, the wavelength of the light limited the resolution of traditional microscopy to around 0.2 micrometers. In order to gain higher resolution, the use of an electron beam with a far smaller wavelength is used in electron microscopes.\n\nElectron microscopes equipped for X-ray spectroscopy can provide qualitative and quantitative elemental analysis. This type of electron microscope, also known as analytical electron microscope, can be a very powerful characterisation tool for investigation of nanomaterials.\n\nThis is a sub-diffraction technique. Examples of scanning probe microscopes are the atomic force microscope (AFM), the Scanning tunneling microscope, the photonic force microscope and the recurrence tracking microscope. All such methods use the physical contact of a solid probe tip to scan the surface of an object, which is supposed to be almost flat.\n\nUltrasonic force microscopy (UFM) has been developed in order to improve the details and image contrast on \"flat\" areas of interest where AFM images are limited in contrast. The combination of AFM-UFM allows a near field acoustic microscopic image to be generated. The AFM tip is used to detect the ultrasonic waves and overcomes the limitation of wavelength that occurs in acoustic microscopy. By using the elastic changes under the AFM tip, an image of much greater detail than the AFM topography can be generated.\n\nUltrasonic force microscopy allows the local mapping of elasticity in atomic force microscopy by the application of ultrasonic vibration to the cantilever or sample. In an attempt to analyze the results of ultrasonic force microscopy in a quantitative fashion, a force-distance curve measurement is done with ultrasonic vibration applied to the cantilever base, and the results are compared with a model of the cantilever dynamics and tip-sample interaction based on the finite-difference technique.\n\nUltraviolet microscopes have two main purposes. The first is to utilize the shorter wavelength of ultraviolet electromagnetic energy to improve the image resolution beyond that of the diffraction limit of standard optical microscopes. This technique is used for non-destructive inspection of devices with very small features such as those found in modern semiconductors. The second application for UV microscopes is contrast enhancement where the response of individual samples is enhanced, relative to their surrounding, due to the interaction of light with the molecules within the sample itself. One example is in the growth of protein crystals. Protein crystals are formed in salt solutions. As salt and protein crystals are both formed in the growth process, and both are commonly transparent to the human eye, they cannot be differentiated with a standard optical microscope. As the tryptophan of protein absorbs light at 280 nm, imaging with a UV microscope with 280 nm bandpass filters makes it simple to differentiate between the two types of crystals. The protein crystals appear dark while the salt crystals are transparent.\n\nThe term \"infrared microscopy\" refers to microscopy performed at infrared wavelengths. In the typical instrument configuration, a Fourier Transform Infrared Spectrometer (FTIR) is combined with an optical microscope and an infrared detector. The infrared detector can be a single point detector, a linear array or a 2D focal plane array. The FTIR provides the ability to perform chemical analysis via infrared spectroscopy and the microscope and point or array detector enable this chemical analysis to be spatially resolved, i.e. performed at different regions of the sample. As such, the technique is also called infrared microspectroscopy (an alternative architecture involves the combination of a tuneable infrared light source and single point detector on a flying objective). This technique is frequently used for infrared chemical imaging, where the image contrast is determined by the response of individual sample regions to particular IR wavelengths selected by the user, usually specific IR absorption bands and associated molecular resonances . A key limitation of conventional infrared microspectroscopy is that the spatial resolution is diffraction-limited. Specifically the spatial resolution is limited to a figure related to the wavelength of the light. For practical IR microscopes, the spatial resolution is limited to 1-3X the wavelength, depending on the specific technique and instrument used. For mid-IR wavelengths, this sets a practical spatial resolution limit of ~3-30 μm.\n\nIR versions of sub-diffraction microscopy (see above) also exist. These include IR NSOM, photothermal microspectroscopy, and atomic force microscope based infrared spectroscopy (AFM-IR).\n\nIn digital holographic microscopy (DHM), interfering wave fronts from a coherent (monochromatic) light-source are recorded on a sensor. The image is digitally reconstructed by a computer from the recorded hologram. Besides the ordinary bright field image, a phase shift image is created.\n\nDHM can operate both in reflection and transmission mode. In reflection mode, the phase shift image provides a relative distance measurement and thus represents a topography map of the reflecting surface. In transmission mode, the phase shift image provides a label-free quantitative measurement of the optical thickness of the specimen. Phase shift images of biological cells are very similar to images of stained cells and have successfully been analyzed by high content analysis software.\n\nA unique feature of DHM is the ability to adjust focus after the image is recorded, since all focus planes are recorded simultaneously by the hologram. This feature makes it possible to image moving particles in a volume or to rapidly scan a surface. Another attractive feature is DHM’s ability to use low cost optics by correcting optical aberrations by software.\n\nDigital pathology is an image-based information environment enabled by computer technology that allows for the management of information generated from a digital slide. Digital pathology is enabled in part by virtual microscopy, which is the practice of converting glass slides into digital slides that can be viewed, managed, and analyzed.\n\nLaser microscopy is a rapidly growing field that uses laser illumination sources in various forms of microscopy. For instance, laser microscopy focused on biological applications uses ultrashort pulse lasers, in a number of techniques labeled as nonlinear microscopy, saturation microscopy, and two-photon excitation microscopy.\n\nHigh-intensity, short-pulse laboratory x-ray lasers have been under development for several years. When this technology comes to fruition, it will be possible to obtain magnified three-dimensional images of elementary biological structures in the living state at a precisely defined instant. For optimum contrast between water and protein and for best sensitivity and resolution, the laser should be tuned near the nitrogen line at about 0.3 nanometers. Resolution will be limited mainly by the hydrodynamic expansion that occurs while the necessary number of photons is being registered. Thus, while the specimen is destroyed by the exposure, its configuration can be captured before it explodes.\n\nScientists have been working on practical designs and prototypes for x-ray holographic microscopes, despite the prolonged development of the appropriate laser.\n\nA microscopy technique relying on the photoacoustic effect, i.e. the generation of (ultra)sound caused by light absorption.\nA focused and intensity modulated laser beam is raster scanned over a sample. The generated (ultra)sound is detected via an ultrasound transducer. Commonly, piezoelectric ultrasound transducers are employed.\n\nThe image contrast is related to the sample's absorption coefficient formula_2. This is in contrast to bright or dark field microscopy, where the image contrast is due to transmittance or scattering. In principle, the contrast of fluorescence microscopy is proportional to the sample's absorption too. However, in fluorescence microscopy the fluorescence quantum yield formula_3 needs to be unequal to zero in order that a signal can be detected. In photoacoustic microscopy, however, every absorbing substance gives a photoacoustic signal formula_4 which is proportional to\n\nformula_5\n\nHere formula_6 is the Grüneisen coefficient, formula_7 is the laser's photon energy and formula_8 is the sample's band gap energy. Therefore, photoacoustic microscopy seems well suited as a complementary technique to fluorescence microscopy, as a high fluorescence quantum yield leads to high fluorescence signals and a low fluorescence quantum yield leads to high photoacoustic signals.\n\nNeglecting non-linear effects, the lateral resolution is limited by the Abbe diffraction limit:\n\nformula_9\n\nwhere formula_10 is the wavelength of the excitation laser and is the numerical aperture of the objective lens. The Abbe diffraction limit holds if the incoming wave front is parallel. In reality, however, the laser beam profile is Gaussian. Therefore, in order to the calculate the achievable resolution, formulas for truncated Gaussian beams have to be used.\n\n\"Amateur Microscopy\" is the investigation and observation of biological and non-biological specimens for recreational purposes. Collectors of minerals, insects, seashells, and plants may use microscopes as tools to uncover features that help them classify their collected items. Other amateurs may be interested in observing the life found in pond water and of other samples. Microscopes may also prove useful for the water quality assessment for people that keep a home aquarium. Photographic documentation and drawing of the microscopic images are additional tasks that augment the spectrum of tasks of the amateur. There are even competitions for photomicrograph art. Participants of this pastime may either use commercially prepared microscopic slides or engage in the task of specimen preparation.\n\nWhile microscopy is a central tool in the documentation of biological specimens, it is, in general, insufficient to justify the description of a new species based on microscopic investigations alone. Often genetic and biochemical tests are necessary to confirm the discovery of a new species. A laboratory and access to academic literature is a necessity, which is specialized and, in general, not available to amateurs. There is, however, one huge advantage that amateurs have above professionals: time to explore their surroundings. Often, advanced amateurs team up with professionals to validate their findings and (possibly) describe new species.\n\nIn the late 1800s, amateur microscopy became a popular hobby in the United States and Europe. Several 'professional amateurs' were being paid for their sampling trips and microscopic explorations by philanthropists, to keep them amused on the Sunday afternoon (e.g., the diatom specialist A. Grunow, being paid by (among others) a Belgian industrialist). Professor John Phin published \"Practical Hints on the Selection and Use of the Microscope (Second Edition, 1878),\" and was also the editor of the \"American Journal of Microscopy.\"\n\nExamples of amateur microscopy images:\nMicroscopy has many applications in the forensic sciences; it provides precision, quality, accuracy, and reproducibility of results. These applications are almost limitless. This is due to the ability of microscope to detect, resolve and image the smallest items of evidence, often without any alteration or destruction. The microscope is used to identify and compare fibers, hairs, soils, and dust…etc.\n\nThe aim of any microscope is to magnify images or photos of a small object and to see fine details. In forensic; the type of specimen, the information one wishes to obtain from it and the type of microscope chosen for the task will determine if the sample preparation is required. For example, ink lines, blood stains or bullets, no treatment is required and the evidence shows directly from appropriate microscope without any form of sample preparation, but for traces of particular matter, the sample preparation must be done before microscopical examination occurs.\n\nA variety of microscopes are used in forensic science laboratory. The light microscopes are the most use in forensic and these microscopes use photons to form images, these microscopes which are most applicable for examining forensic specimens as mentioned before are as follows:\n\n1. The compound microscope\n\n2. The comparison microscope\n\n3. The stereoscopic microscope\n\n4. The polarizing microscope\n\n5. The micro spectrophotometer\n\nThis diversity of the types of microscopes in forensic applications comes mainly from their magnification ranges, which are (1- 1200X), (50 -30,000X) and (500- 250,000X) for the optical microscopy, SEM and TEM respectively.\n\n\n\n"}
{"id": "19568", "url": "https://en.wikipedia.org/wiki?curid=19568", "title": "Microscope", "text": "Microscope\n\nA microscope (from the , \"mikrós\", \"small\" and , \"skopeîn\", \"to look\" or \"see\") is an instrument used to see objects that are too small to be seen by the naked eye. Microscopy is the science of investigating small objects and structures using such an instrument. Microscopic means invisible to the eye unless aided by a microscope.\n\nThere are many types of microscopes, and they may be grouped in different ways. One way is to describe the way the instruments interact with a sample to create images, either by sending a beam of light or electrons to a sample in its optical path, or by scanning across, and a short distance from the surface of a sample using a probe. The most common microscope (and the first to be invented) is the optical microscope, which uses light to pass through a sample to produce an image. Other major types of microscopes are the fluorescence microscope, the electron microscope (both the transmission electron microscope and the scanning electron microscope) and the various types of scanning probe microscopes.\n\nAlthough objects resembling lenses date back 4000 years and there are Greek accounts of the optical properties of water-filled spheres (5th century BC) followed by many centuries of writings on optics, the earliest known use of simple microscopes (magnifying glasses) dates back to the widespread use of lenses in eyeglasses in the 13th century. The earliest known examples of compound microscopes, which combine an objective lens near the specimen with an eyepiece to view a real image, appeared in Europe around 1620. The inventor is unknown although many claims have been made over the years. Several revolve around the spectacle-making centers in the Netherlands including claims it was invented in 1590 by Zacharias Janssen (claim made by his son) and/or Zacharias' father, Hans Martens, claims it was invented by their neighbor and rival spectacle maker, Hans Lippershey (who applied for the first telescope patent in 1608), and claims it was invented by expatriate Cornelis Drebbel who was noted to have a version in London in 1619. Galileo Galilei (also sometimes cited as compound microscope inventor) seems to have found after 1610 that he could close focus his telescope to view small objects and, after seeing a compound microscope built by Drebbel exhibited in Rome in 1624, built his own improved version. Giovanni Faber coined the name \"microscope\" for the compound microscope Galileo submitted to the Accademia dei Lincei in 1625 (Galileo had called it the \"\"occhiolino\"\" or \"\"little eye\"\").\n\nThe first detailed account of the microscopic anatomy of organic tissue based on the use of a microscope did not appear until 1644, in Giambattista Odierna's \"L'occhio della mosca\", or \"The Fly's Eye\".\n\nThe microscope was still largely a novelty until the 1660s and 1670s when naturalists in Italy, the Netherlands and England began using them to study biology. Italian scientist Marcello Malpighi, called the father of histology by some historians of biology, began his analysis of biological structures with the lungs. Robert Hooke's \"Micrographia\" had a huge impact, largely because of its impressive illustrations. A significant contribution came from Antonie van Leeuwenhoek who achieved up to 300 times magnification using a simple single lens microscope. He sandwiched a very small glass ball lens between the holes in two metal plates riveted together, and with an adjustable-by-screws needle attached to mount the specimen. Then, Van Leeuwenhoek re-discovered red blood cells (after Jan Swammerdam) and spermatozoa, and helped popularise the use of microscopes to view biological ultrastructure. On 9 October 1676, van Leeuwenhoek reported the discovery of micro-organisms.\n\nThe performance of a light microscope depends on the quality and correct use of the condensor lens system to focus light on the specimen and the objective lens to capture the light from the specimen and form an image. Early instruments were limited until this principle was fully appreciated and developed from the late 19th to very early 20th century, and until electric lamps were available as light sources. In 1893 August Köhler developed a key principle of sample illumination, Köhler illumination, which is central to achieving the theoretical limits of resolution for the light microscope. This method of sample illumination produces even lighting and overcomes the limited contrast and resolution imposed by early techniques of sample illumination. Further developments in sample illumination came from the discovery of phase contrast by Frits Zernike in 1953, and differential interference contrast illumination by Georges Nomarski in 1955; both of which allow imaging of unstained, transparent samples.\n\nIn the early 20th century a significant alternative to the light microscope was developed, an instrument that uses a beam of electrons rather than light to generate an image. The German physicist, Ernst Ruska, working with electrical engineer Max Knoll, developed the first prototype electron microscope in 1931, a transmission electron microscope (TEM). The transmission electron microscope works on similar principles to an optical microscope but uses electrons in the place of light and electromagnets in the place of glass lenses. Use of electrons, instead of light, allows for much higher resolution.\n\nDevelopment of the transmission electron microscope was quickly followed in 1935 by the development of the scanning electron microscope by Max Knoll. Although TEMs were being used for research before WWII, and became popular afterwards, the SEM was not commercially available until 1965.\n\nTransmission electron microscopes became popular following the Second World War. Ernst Ruska, working at Siemens, developed the first commercial transmission electron microscope and, in the 1950s, major scientific conferences on electron microscopy started being held. In 1965, the first commercial scanning electron microscope was developed by Professor Sir Charles Oatley and his postgraduate student Gary Stewart, and marketed by the Cambridge Instrument Company as the \"Stereoscan\".\n\nOne of the latest discoveries made about using an electron microscope is the ability to identify a virus. Since this microscope produces a visible, clear image of small organelles, in an electron microscope there is no need for reagents to see the virus or harmful cells, resulting in a more efficient way to detect pathogens.\n\nFrom 1981 to 1983 Gerd Binnig and Heinrich Rohrer worked at IBM in Zurich, Switzerland to study the quantum tunnelling phenomenon. They created a practical instrument, a scanning probe microscope from quantum tunnelling theory, that read very small forces exchanged between a probe and the surface of a sample. The probe approaches the surface so closely that electrons can flow continuously between probe and sample, making a current from surface to probe. The microscope was not initially well received due to the complex nature of the underlying theoretical explanations. In 1984 Jerry Tersoff and D.R. Hamann, while at AT&T's Bell Laboratories in Murray Hill, New Jersey began publishing articles that tied theory to the experimental results obtained by the instrument. This was closely followed in 1985 with functioning commercial instruments, and in 1986 with Gerd Binnig, Quate, and Gerber's invention of the atomic force microscope, then Binnig's and Rohrer's Nobel Prize in Physics for the SPM.\n\nNew types of scanning probe microscope have continued to be developed as the ability to machine ultra-fine probes and tips has advanced.\n\nThe most recent developments in light microscope largely centre on the rise of fluorescence microscopy in biology. During the last decades of the 20th century, particularly in the post-genomic era, many techniques for fluorescent staining of cellular structures were developed. The main groups of techniques involve targeted chemical staining of particular cell structures, for example, the chemical compound DAPI to label DNA, use of antibodies conjugated to fluorescent reporters, see\nimmunofluorescence, and fluorescent proteins, such as green fluorescent protein. These techniques use these different fluorophores for analysis of cell structure at a molecular level in both live and fixed samples.\n\nThe rise of fluorescence microscopy drove the development of a major modern microscope design, the confocal microscope. The principle was patented in 1957 by Marvin Minsky, although laser technology limited practical application of the technique. It was not until 1978 when Thomas and Christoph Cremer developed the first practical confocal laser scanning microscope and the technique rapidly gained popularity through the 1980s.\n\nMuch current research (in the early 21st century) on optical microscope techniques is focused on development of superresolution analysis of fluorescently labelled samples. Structured illumination can improve resolution by around two to four times and techniques like stimulated emission depletion (STED) microscopy are approaching the resolution of electron microscopes. This occurs because the diffraction limit is occurred from light or excitation, which makes the resolution must be doubled to become super saturated. Stefan Hell was awarded the 2014 Nobel Prize in Chemistry for the development of the STED technique, along with Eric Betzig and William Moerner who adapted fluorescence microscopy for single-molecule visualization.\n\nX-ray microscopes are instruments that use electromagnetic radiation usually in the soft X-ray band to image objects. Technological advances in X-ray lens optics in the early 1970s made the instrument a viable imaging choice. They are often used in tomography (see micro-computed tomography) to produce three dimensional images of objects, including biological materials that have not been chemically fixed. Currently research is being done to improve optics for hard X-rays which have greater penetrating power.\n\nMicroscopes can be separated into several different classes. One grouping is based on what interacts with the sample to generate the image, i.e., light or photons (optical microscopes), electrons (electron microscopes) or a probe (scanning probe microscopes). Alternatively, microscopes can be classified based on whether they analyze the sample via a scanning point (confocal optical microscopes, scanning electron microscopes and scanning probe microscopes) or analyze the sample all at once (wide field optical microscopes and transmission electron microscopes).\n\nWide field optical microscopes and transmission electron microscopes both use the theory of lenses (optics for light microscopes and electromagnet lenses for electron microscopes) in order to magnify the image generated by the passage of a wave transmitted through the sample, or reflected by the sample. The waves used are electromagnetic (in optical microscopes) or electron beams (in electron microscopes). Resolution in these microscopes is limited by the wavelength of the radiation used to image the sample, where shorter wavelengths allow for a higher resolution.\n\nScanning optical and electron microscopes, like the confocal microscope and scanning electron microscope, use lenses to focus a spot of light or electrons onto the sample then analyze the signals generated by the beam interacting with the sample. The point is then scanned over the sample to analyze a rectangular region. Magnification of the image is achieved by displaying the data from scanning a physically small sample area on a relatively large screen. These microscopes have the same resolution limit as wide field optical, probe, and electron microscopes.\n\nScanning probe microscopes also analyze a single point in the sample and then scan the probe over a rectangular sample region to build up an image. As these microscopes do not use electromagnetic or electron radiation for imaging they are not subject to the same resolution limit as the optical and electron microscopes described above.\n\nThe most common type of microscope (and the first invented) is the optical microscope. This is an optical instrument containing one or more lenses producing an enlarged image of a sample placed in the focal plane. Optical microscopes have refractive glass (occasionally plastic or quartz), to focus light on the eye or on to another light detector. Mirror-based optical microscopes operate in the same manner. Typical magnification of a light microscope, assuming visible range light, is up to 1250x with a theoretical resolution limit of around 0.250 micrometres or 250 nanometres. This limits practical magnification to ~1500x. Specialized techniques (e.g., scanning confocal microscopy, Vertico SMI) may exceed this magnification but the resolution is diffraction limited. The use of shorter wavelengths of light, such as ultraviolet, is one way to improve the spatial resolution of the optical microscope, as are devices such as the near-field scanning optical microscope.\n\nSarfus is a recent optical technique that increases the sensitivity of a standard optical microscope to a point where it is possible to directly visualize nanometric films (down to 0.3 nanometre) and isolated nano-objects (down to 2 nm-diameter). The technique is based on the use of non-reflecting substrates for cross-polarized reflected light microscopy.\n\nUltraviolet light enables the resolution of microscopic features as well as the imaging of samples that are transparent to the eye. Near infrared light can be used to visualize circuitry embedded in bonded silicon devices, since silicon is transparent in this region of wavelengths.\n\nIn fluorescence microscopy many wavelengths of light ranging from the ultraviolet to the visible can be used to cause samples to fluoresce which allows viewing by eye or with specifically sensitive cameras. Phase contrast microscopy is an optical microscopy illumination technique in which small phase shifts in the light passing through a transparent specimen are converted into amplitude or contrast changes in the image. The use of phase contrast does not require staining to view the slide. This microscope technique made it possible to study the cell cycle in live cells.\n\nThe traditional optical microscope has more recently evolved into the digital microscope. In addition to, or instead of, directly viewing the object through the eyepieces, a type of sensor similar to those used in a digital camera is used to obtain an image, which is then displayed on a computer monitor. These sensors may use CMOS or charge-coupled device (CCD) technology, depending on the application.\n\nDigital microscopy with very low light levels to avoid damage to vulnerable biological samples is available using sensitive photon-counting digital cameras. It has been demonstrated that a light source providing pairs of entangled photons may minimize the risk of damage to the most light-sensitive samples. In this application of ghost imaging to photon-sparse microscopy, the sample is illuminated with infrared photons, each of which is spatially correlated with an entangled partner in the visible band for efficient imaging by a photon-counting camera.\n\nThe two major types of electron microscopes are transmission electron microscopes (TEMs) and scanning electron microscopes (SEMs). They both have series of electromagnetic and electrostatic lenses to focus a high energy beam of electrons on a sample. In a TEM the electrons pass through the sample, analogous to basic optical microscopy. This requires careful sample preparation, since electrons are scattered strongly by most materials. The samples must also be very thin (50–100 nm) in order for the electrons to pass through it. Cross-sections of cells stained with osmium and heavy metals reveal clear organelle membranes and proteins such as ribosomes. With a 0.1 nm level of resolution, detailed views of viruses (20–300 nm) and a strand of DNA (2 nm in width) can be obtained. In contrast, the SEM has raster coils to scan the surface of bulk objects with a fine electron beam. Therefore, the specimen do not necessarily need to be sectioned, but require coating with a substance such as a heavy metal. This allows three-dimensional views of the surface of samples.\n\nThe different types of scanning probe microscopes arise from the many different types of interactions that occur when a small probe of some type is scanned over and interacts with a specimen. These interactions or modes can be recorded or mapped as function of location on the surface to form a characterization map. The three most common types of scanning probe microscopes are atomic force microscopes (AFM), near-field scanning optical microscopes (MSOM or SNOM, scanning near-field optical microscopy), and scanning tunneling microscopes (STM). An atomic force microscope has a fine probe, usually of silicon or silicon nitride, attached to a cantilever; the probe is scanned over the surface of the sample, and the forces that cause an interaction between the probe and the surface of the sample are measured and mapped. A near-field scanning optical microscope is similar to an AFM but its probe consists of a light source in an optical fiber covered with a tip that has usually an aperture for the light to pass through. The microscope can capture either transmitted or reflected light to measure very localized optical properties of the surface, commonly of a biological specimen. Scanning tunneling microscopes have a metal tip with a single apical atom; the tip is attached to a tube through which a current flows. The tip is scanned over the surface of a conductive sample until a tunneling current flows; the current is kept constant by computer movement of the tip and an image is formed by the recorded movements of the tip.\nScanning acoustic microscopes use sound waves to measure variations in acoustic impedance. Similar to Sonar in principle, they are used for such jobs as detecting defects in the subsurfaces of materials including those found in integrated circuits. On February 4, 2013, Australian engineers built a \"quantum microscope\" which provides unparalleled precision.\n\n"}
{"id": "19570", "url": "https://en.wikipedia.org/wiki?curid=19570", "title": "Midrash", "text": "Midrash\n\nMidrash (; ; pl. \"\") is biblical exegesis by ancient Judaic authorities, using a mode of interpretation prominent in the Talmud.\n\nMidrash and rabbinic readings \"discern value in texts, words, and letters, as potential revelatory spaces,\" writes the Reverend and Hebrew scholar Wilda C. Gafney. \"They reimagine dominant narratival readings while crafting new ones to stand alongside—not replace—former readings. Midrash also asks questions of the text; sometimes it provides answers, sometimes it leaves the reader to answer the questions.\"\n\nVanessa Lovelace defines midrash as \"a Jewish mode of interpretation that not only engages the words of the text, behind the text, and beyond the text, but also focuses on each letter, and the words left unsaid by each line.\"\n\nThe term is also used of a rabbinic work that interprets Scripture in that manner. Such works contain early interpretations and commentaries on the Written Torah and Oral Torah (spoken law and sermons), as well as non-legalistic rabbinic literature (') and occasionally Jewish religious laws ('), which usually form a running commentary on specific passages in the Hebrew Scripture (\"\").\n\n\"Midrash\", especially if capitalized, can refer to a specific compilation of these rabbinic writings composed between 400 and 1200 CE.\n\nAccording to Gary Porson and Jacob Neusner, \"midrash\" has three technical meanings: 1) Judaic biblical interpretation; 2) the method used in interpreting; 3) a collection of such interpretations.\n\nThe Hebrew word \"midrash\" is derived from the root of the verb \"darash\" (), which means \"resort to, seek, seek with care, enquire, require\", forms of which appear frequently in the Bible.\n\nThe word \"midrash\" occurs twice in the Hebrew Bible: 2 Chronicles 13:22 \"in the \"midrash\" of the prophet Iddo\", and 24:27 \"in the \"midrash\" of the book of the kings\". KJV and ESV translate the word as \"story\" in both instances; the Septuagint translates it as βιβλίον (book) in the first, as γραφή (writing) in the second. The meaning of the Hebrew word in these contexts is uncertain: it has been interpreted as referring to \"a body of authoritative narratives, or interpretations thereof, concerning historically important figures\" and seems to refer to a \"book\", perhaps even a \"book of interpretation\", which might make its use a foreshadowing of the technical sense that the rabbis later gave to the word.\n\nSince the early Middle Ages the function of much of midrashic interpretation has been distinguished from that of \"peshat\", straight or direct interpretation aiming at the original literal meaning of a scriptural text.\n\nA definition of \"midrash\" repeatedly quoted by other scholars is that given by Gary G. Porton in 1981: \"a type of literature, oral or written, which stands in direct relationship to a fixed, canonical text, considered to be the authoritative and revealed word of God by the midrashist and his audience, and in which this canonical text is explicitly cited or clearly alluded to\".\n\nLieve M. Teugels, who would limit midrash to rabbinic literature, offered a definition of midrash as \"rabbinic interpretation of Scripture that bears the lemmatic form\", a definition that, unlike Porton's, has not been adopted by others. While some scholars agree with the limitation of the term \"midrash\" to rabbinic writings, others apply it also to certain Qumran writings, to parts of the New Testament, and of the Hebrew Bible (in particular the superscriptions of the Psalms, Deuteronomy, and Chronicles), and even modern compositions are called midrashim.\n\nMidrash is now viewed more as method than genre, although the rabbinic midrashim do constitute a distinct literary genre.\n\nAccording to the Encyclopaedia Britannica, \"Midrash was initially a philological method of interpreting the literal meaning of biblical texts. In time it developed into a sophisticated interpretive system that reconciled apparent biblical contradictions, established the scriptural basis of new laws, and enriched biblical content with new meaning. Midrashic creativity reached its peak in the schools of Rabbi Ishmael and Akiba, where two different hermeneutic methods were applied. The first was primarily logically oriented, making inferences based upon similarity of content and analogy. The second rested largely upon textual scrutiny, assuming that words and letters that seem superfluous teach something not openly stated in the text.\"\n\nMany different exegetical methods are employed in an effort to derive deeper meaning from a text. This is not limited to the traditional thirteen textual tools attributed to the Tanna Rabbi Ishmael, which are used in the interpretation of \"halakha\" (Jewish law). The presence of words or letters which are seen to be apparently superfluous, and the chronology of events, parallel narratives or what are seen as other textual \"anomalies\" are often used as a springboard for interpretation of segments of Biblical text. In many cases, a handful of lines in the Biblical narrative may become a long philosophical discussion\n\nJacob Neusner distinguishes three midrash processes:\n\nNumerous Jewish midrashim previously preserved in manuscript form have been published in print, including those denominated as smaller or minor midrashim. Bernard H. Mehlman and Seth M. Limmer deprecate this usage on the grounds that the term \"minor\" seems judgmental and \"small\" is inappropriate for midrashim some of which are lengthy. They propose instead the term \"medieval midrashim\", since the period of their production extended from the twilight of the rabbinic age to the dawn of the Age of Enlightenment.\n\nGenerally speaking, rabbinic midrashim either focus on religious law and practice (\"halakha\") or interpret biblical narrative in relation to non-legal ethics or theology, creating homilies and parables based on the text. In the latter case they are described as \"aggadic\".\n\n\"Midrash halakha\" is the name given to a group of tannaitic expositions on the first four books of the Hebrew Bible. These midrashim, written in Mishnahic Hebrew, clearly distinguish between the Biblical texts that they discuss, and the rabbinic interpretation of that text. They often go well beyond simple interpretation and derive or provide support for halakha. This work is based on pre-set assumptions about the sacred and divine nature of the text, and the belief in the legitimacy that accords with rabbinic interpretation.\n\nAlthough this material treats the biblical texts as the authoritative word of God, it is clear that not all of the Hebrew Bible was fixed in its wording at this time, as some verses that are cited differ from the Masoretic, and accord with the Septuagint, or Samaritan Torah instead.\n\nWith the growing canonization of the contents of the Hebrew Bible, both in terms of the books that it contained, and the version of the text in them, and an acceptance that new texts could not be added, there came a need to produce material that would clearly differentiate between that text, and rabbinic interpretation of it. By collecting and compiling these thoughts they could be presented in a manner which helped to refute claims that they were only human interpretations. The argument being that by presenting the various collections of different schools of thought each of which relied upon close study of the text, the growing difference between early biblical law, and its later rabbinic interpretation could be reconciled.\n\nMidrashim which seek to explain the non-legal portions of the Hebrew Bible are sometimes referred to as \"aggadah\" or \"haggadah\".\n\nAggadic discussions of the non-legal parts of Scripture are characterized by a much greater freedom of exposition than the halakhic midrashim (midrashim on Jewish law). Aggadic expositors availed themselves of various techniques, including sayings of prominent rabbis. These aggadic explanations could be philosophical or mystical disquisitions concerning angels, demons, paradise, hell, the messiah, Satan, feasts and fasts, parables, legends, satirical assaults on those who practice idolatry, etc.\n\nSome of these midrashim entail mystical teachings. The presentation is such that the midrash is a simple lesson to the uninitiated, and a direct allusion, or analogy, to a mystical teaching for those educated in this area.\n\nAn example of a midrashic interpretation:\n\n\n\n\nA wealth of literature and artwork has been created in the 20th and 21st centuries by people aspiring to create \"contemporary midrash\". Forms include poetry, prose, Bibliodrama (the acting out of Bible stories), murals, masks, and music, among others. The Institute for Contemporary Midrash was formed to facilitate these reinterpretations of sacred texts. The institute hosted several week-long intensives between 1995 and 2004, and published eight issues of \"Living Text: The Journal of Contemporary Midrash\" from 1997 to 2000.\n\nAccording to Carol Bakhos, recent studies that use literary-critical tools to concentrate on the cultural and literary aspects of midrash have led to a rediscovery of the importance of these texts for finding insights into the rabbinic culture that created them. Midrash is increasingly seen as a literary and cultural construction, responsive to literary means of analysis.\n\nReverend Wilda C. Gafney has coined and expanded on \"womanist midrash\", a particular practice and method that Gafney defines as: \"[...] A set of interpretive practices, including translation, exegesis, and biblical narratives, that attends to marginalized characters in biblical narratives, specially women and girls, intentionally including and centering on non-Israelite peoples and enslaved persons. Womanist midrash listens to and for their voices in and through the Hebrew Bible, while acknowledging that often the text does not speak, or even intend to speak, to or for them, let alone hear them. In the tradition of rabbinic midrash and contemporary feminist biblical scholarship, womanist midrash offers names for anonymized characters and crafts/listens to/gives voice to those characters.\" Gafney's analysis also draws parallels between midrash as a Jewish exegetical practice and African American Christian practices of biblical interpretation, wherein both practices privilege \"sacred imaginative interrogation,\" or \"sanctified imagination\" as it is referred to in the black preaching tradition, when exegeting and interpreting the text. \"Like classical and contemporary Jewish midrash, the sacred imagination [as practiced in black preaching traditions] tells us the story behind the story, the story between the lines on the page,\" Gafney writes.\n\nFrank Kermode has written that midrash is an imaginative way of \"updating, enhancing, augmenting, explaining, and justifying the sacred text\". Because the Tanakh came to be seen as unintelligible or even offensive, midrash could be used as a means of rewriting it in a way that both makes it more acceptable to later ethical standards and renders it less obviously implausible.\n\nJames L. Kugel, in \"The Bible as It Was\" (Cambridge, Massachusetts: Harvard University Press, 1997), examines a number of early Jewish and Christian texts that comment on, expand, or re-interpret passages from the first five books of the Tanakh between the third century BCE and the second century CE.\n\nKugel traces how and why biblical interpreters produced new meanings by the use of exegesis on ambiguities, syntactical details, unusual or awkward vocabulary, repetitions, etc. in the text. As an example, Kugel examines the different ways in which the biblical story that God's instructions are not to be found in heaven (Deut 30:12) has been interpreted. Baruch 3:29-4:1 states that this means that divine wisdom is not available anywhere other than in the Torah. Targum Neophyti (Deut 30:12) and b. Baba Metzia 59b claim that this text means that Torah is no longer hidden away, but has been given to humans who are then responsible for following it.\n\n\n"}
{"id": "19571", "url": "https://en.wikipedia.org/wiki?curid=19571", "title": "Missouri", "text": "Missouri\n\nMissouri is a state in the Midwestern United States. With over six million residents, it is the 18th-most populous state of the Union. The largest urban areas are St. Louis, Kansas City, Springfield, and Columbia; the capital is Jefferson City. The state is the 21st-most extensive in area. In the South are the Ozarks, a forested highland, providing timber, minerals, and recreation. The Missouri River, after which the state is named, flows through the center of the state into the Mississippi River, which makes up Missouri's eastern border.\n\nHumans have inhabited the land now known as Missouri for at least 12,000 years. The Mississippian culture built cities and mounds, before declining in the 14th century. When European explorers arrived in the 17th century they encountered the Osage and Missouria nations. The French established Louisiana, a part of New France, and founded Ste. Genevieve in 1735 and St. Louis in 1764. After a brief period of Spanish rule, the United States acquired the Louisiana Purchase in 1803. Americans from the Upland South, including enslaved African Americans, rushed into the new Missouri Territory. Missouri was admitted as a slave state as part of the Missouri Compromise. Many from Virginia, Kentucky, and Tennessee settled in the Boonslick area of Mid-Missouri. Soon after, heavy German immigration formed the Missouri Rhineland.\n\nMissouri played a central role in the westward expansion of the United States, as memorialized by the Gateway Arch. The Pony Express, Oregon Trail, Santa Fe Trail, and California Trail all began in Missouri. As a border state, Missouri's role in the American Civil War was complex and there were many conflicts within. After the war, both Greater St. Louis and the Kansas City metropolitan area became centers of industrialization and business. Today, the state is divided into 114 counties and the independent city of St. Louis.\n\nMissouri's culture blends elements from the Midwestern and Southern United States. The musical styles of ragtime, Kansas City jazz, and St. Louis Blues developed in Missouri. The well-known Kansas City-style barbecue, and lesser-known St. Louis-style barbecue, can be found across the state and beyond. St. Louis is also a major center of beer brewing; Anheuser-Busch is the largest producer in the world. Missouri wine is produced in the nearby Missouri Rhineland and Ozarks. Missouri's alcohol laws are among the most permissive in the United States. Outside of the state's major cities, popular tourist destinations include the Lake of the Ozarks, Table Rock Lake, and Branson.\n\nWell-known Missourians include U.S. President Harry S. Truman, Mark Twain, Walt Disney, Chuck Berry, and Nelly. Some of the largest companies based in the state include Cerner, Express Scripts, Monsanto, Emerson Electric, Edward Jones, H&R Block, Wells Fargo Advisors, and O'Reilly Auto Parts. Missouri has been called the \"Mother of the West\" and the \"Cave State\"; however, Missouri's most famous nickname is the \"Show Me State.\"\n\nThe state is named for the Missouri River, which was named after the indigenous Missouri Indians, a Siouan-language tribe. It is said that they were called the \"ouemessourita\" (\"wimihsoorita\"), meaning \"those who have dugout canoes\", by the Miami-Illinois language speakers. This appears to be folk etymology—the Illinois spoke an Algonquian language and the closest approximation that can be made in that of their close neighbors, the Ojibwe, is \"\"You Ought to Go Downriver & Visit Those People.\"\" This would be an odd occurrence, as the French who first explored & attempted to settle the Mississippi River usually got their translations during that time fairly accurate, often giving things French names that were exact translations of the native tongue(s).\n\nAssuming Missouri were deriving from the Siouan language, it would translate as \"\"It connects to the side of it,\"\" in reference to the river itself. This isn't entirely likely either, as this would be coming out as \"\"Maya Sunni\"\" (Mah-yah soo-nee) Most likely, though, the name Missouri comes from Chiwere language, a fairly unique Siouan dialect spoken by people who resided in the modern day states of Wisconsin, Iowa, South Dakota, Missouri & Nebraska.\n\nThe name \"Missouri\" has several different pronunciations even among its present-day natives, the two most common being and . Further pronunciations also exist in Missouri or elsewhere in the United States, involving the realization of the first syllable as either or ; the medial consonant as either or ; the vowel in the second syllable as either or ; and the third syllable as , , centralized ), or nothing. Any combination of these phonetic realizations may be observed coming from speakers of American English.\n\nThe linguistic history was treated definitively by Donald M. Lance, who acknowledged that the question is sociologically complex, but that no pronunciation could be declared \"correct\", nor could any be clearly defined as native or outsider, rural or urban, southern or northern, educated or otherwise. Politicians often employ multiple pronunciations, even during a single speech, to appeal to a greater number of listeners. Often, informal respellings of the state's name, such as \"Missour-\"ee\"\" or \"Missour-\"uh\"\", are used informally to phonetically distinguish pronunciations.\n\nThere is no official state nickname. However, Missouri's unofficial nickname is the \"Show Me State\", which appears on its license plates. This phrase has several origins. One is popularly ascribed to a speech by Congressman Willard Vandiver in 1899, who declared that \"I come from a state that raises corn and cotton, cockleburs and Democrats, and frothy eloquence neither convinces nor satisfies me. I'm from Missouri, and you have got to show me.\" This is in keeping with the saying \"I'm from Missouri\" which means \"I'm skeptical of the matter and not easily convinced.\" However, according to researchers, the phrase \"show me\" was already in use before the 1890s. Another one states that it is a reference to Missouri miners who were taken to Leadville, Colorado to replace striking workers. Since the new men were unfamiliar with the mining methods, they required frequent instruction.\n\nOther nicknames for Missouri include \"The Lead State\", \"The Bullion State\", \"The Ozark State\", \"The Mother of the West\", \"The Iron Mountain State\", and \"Pennsylvania of the West\". It is also known as the \"Cave State\" because there are more than 6,000 recorded caves in the state (second to Tennessee). Perry County is the county with the largest number of caves and the single longest cave.\n\nThe official state motto is , which means \"Let the welfare of the people be the supreme law.\"\n\nMissouri is landlocked and borders eight different states as does its neighbor, Tennessee. No state in the U.S. touches more than eight. Missouri is bounded by Iowa on the north; by Illinois, Kentucky, and Tennessee across the Mississippi River on the east; on the south by Arkansas; and by Oklahoma, Kansas, and Nebraska (the last across the Missouri River) on the west. Whereas the northern and southern boundaries are straight lines, the Missouri Bootheel protrudes southerly into Arkansas. The two largest rivers are the Mississippi (which defines the eastern boundary of the state) and the Missouri River (which flows from west to east through the state) essentially connecting the two largest metros of Kansas City and St. Louis.\n\nAlthough today it is usually considered part of the Midwest, Missouri was historically seen by many as a border state, chiefly because of the settlement of migrants from the South and its status as a slave state before the Civil War, balanced by the influence of St. Louis. The counties that made up \"Little Dixie\" were those along the Missouri River in the center of the state, settled by Southern migrants who held the greatest concentration of slaves.\n\nIn 2005, Missouri received 16,695,000 visitors to its national parks and other recreational areas totaling , giving it $7.41 million in annual revenues, 26.6% of its operating expenditures.\n\nNorth of, and in some cases just south of, the Missouri River lie the Northern Plains that stretch into Iowa, Nebraska, and Kansas. Here, rolling hills remain from the glaciation that once extended from the Canadian Shield to the Missouri River. Missouri has many large river bluffs along the Mississippi, Missouri, and Meramec Rivers. Southern Missouri rises to the Ozark Mountains, a dissected plateau surrounding the Precambrian igneous St. Francois Mountains. This region also hosts karst topography characterized by high limestone content with the formation of sinkholes and caves.\nThe southeastern part of the state is known as the Missouri Bootheel region, which is part of the Mississippi Alluvial Plain or Mississippi embayment. This region is the lowest, flattest, warmest, and wettest part of the state. It is also among the poorest, as the economy there is mostly agricultural. It is also the most fertile, with cotton and rice crops predominant. The Bootheel was the epicenter of the four New Madrid Earthquakes of 1811 and 1812.\n\nMissouri generally has a humid continental climate with cold snowy winters and hot, humid, and wet summers. In the southern part of the state, particularly in the Bootheel, the climate becomes humid subtropical. Located in the interior United States, Missouri often experiences extreme temperatures. Without high mountains or oceans nearby to moderate temperature, its climate is alternately influenced by air from the cold Arctic and the hot and humid Gulf of Mexico. Missouri's highest recorded temperature is at Warsaw and Union on July 14, 1954, while the lowest recorded temperature is also at Warsaw on February 13, 1905.\n\nLocated in Tornado Alley, Missouri also receives extreme weather in the form of severe thunderstorms and tornadoes. The most recent tornado in the state to cause damage and casualties was the 2011 Joplin tornado, which destroyed roughly one-third of the city of Joplin. The tornado caused an estimated $1–3 billion in damages, killed 159 (+1 non-tornadic), and injured over 1,000 people. It was the first EF5 to hit the state since 1957 and the deadliest in the U.S. since 1947, making it the seventh deadliest tornado in American history and 27th deadliest in the world. St. Louis and its suburbs also have a history of experiencing particularly severe tornadoes, the most recent memorable one being an EF4 tornado that damaged Lambert-St. Louis International Airport on April 22, 2011. One of the worst tornadoes in American history struck St. Louis on May 27, 1896, killing at least 255 and causing $10 mil. damage ($3.9 bil. damage in 2009) or $ in today's dollars.\n\nMissouri is home to a diversity of both flora and fauna. There is a large amount of fresh water present due to the Mississippi River, Missouri River, Table Rock Lake and Lake of the Ozarks, with numerous smaller tributary rivers, streams, and lakes. North of the Missouri River, the state is primarily rolling hills of the Great Plains, whereas south of the Missouri River, the state is dominated by the Oak-Hickory Central U.S. hardwood forest.\n\nIndigenous peoples inhabited Missouri for thousands of years before European exploration and settlement. Archaeological excavations along the rivers have shown continuous habitation for more than 7,000 years. Beginning before 1000 CE, there arose the complex Mississippian culture, whose people created regional political centers at present-day St. Louis and across the Mississippi River at Cahokia, near present-day Collinsville, Illinois. Their large cities included thousands of individual residences, but they are known for their surviving massive earthwork mounds, built for religious, political and social reasons, in platform, ridgetop and conical shapes. Cahokia was the center of a regional trading network that reached from the Great Lakes to the Gulf of Mexico. The civilization declined by 1400 CE, and most descendants left the area long before the arrival of Europeans. St. Louis was at one time known as Mound City by the European Americans, because of the numerous surviving prehistoric mounds, since lost to urban development. The Mississippian culture left mounds throughout the middle Mississippi and Ohio river valleys, extending into the southeast as well as the upper river.\n\nThe first European settlers were mostly ethnic French Canadians, who created their first settlement in Missouri at present-day Ste. Genevieve, about an hour south of St. Louis. They had migrated about 1750 from the Illinois Country. They came from colonial villages on the east side of the Mississippi River, where soils were becoming exhausted and there was insufficient river bottom land for the growing population. Sainte-Geneviève became a thriving agricultural center, producing enough surplus wheat, corn and tobacco to ship tons of grain annually downriver to Lower Louisiana for trade. Grain production in the Illinois Country was critical to the survival of Lower Louisiana and especially the city of New Orleans.\n\nSt. Louis was founded soon after by French fur traders, Pierre Laclède and stepson Auguste Chouteau from New Orleans in 1764. From 1764 to 1803, European control of the area west of the Mississippi to the northernmost part of the Missouri River basin, called Louisiana, was assumed by the Spanish as part of the Viceroyalty of New Spain, due to Treaty of Fontainebleau (in order to have Spain join with France in the war against England). The arrival of the Spanish in St. Louis was in September 1767.\n\nSt. Louis became the center of a regional fur trade with Native American tribes that extended up the Missouri and Mississippi rivers, which dominated the regional economy for decades. Trading partners of major firms shipped their furs from St. Louis by river down to New Orleans for export to Europe. They provided a variety of goods to traders, for sale and trade with their Native American clients. The fur trade and associated businesses made St. Louis an early financial center and provided the wealth for some to build fine houses and import luxury items. Its location near the confluence of the Illinois River meant it also handled produce from the agricultural areas. River traffic and trade along the Mississippi were integral to the state's economy, and as the area's first major city, St. Louis expanded greatly after the invention of the steamboat and the increased river trade.\n\nNapoleon Bonaparte had gained Louisiana for French ownership from Spain in 1800 under the Treaty of San Ildefonso, after it had been a Spanish colony since 1762. But the treaty was kept secret. Louisiana remained nominally under Spanish control until a transfer of power to France on November 30, 1803, just three weeks before the cession to the United States.\n\nPart of the 1803 Louisiana Purchase by the United States, Missouri earned the nickname \"Gateway to the West\" because it served as a major departure point for expeditions and settlers heading to the West during the 19th century. St. Charles, just west of St. Louis, was the starting point and the return destination of the Lewis and Clark Expedition, which ascended the Missouri River in 1804, in order to explore the western lands to the Pacific Ocean. St. Louis was a major supply point for decades, for parties of settlers heading west.\n\nAs many of the early settlers in western Missouri migrated from the Upper South, they brought enslaved African Americans as agricultural laborers, and they desired to continue their culture and the institution of slavery. They settled predominantly in 17 counties along the Missouri River, in an area of flatlands that enabled plantation agriculture and became known as \"Little Dixie.\" In 1821 the former Missouri Territory was admitted as a slave state, in accordance with the Missouri Compromise, and with a temporary state capital in St. Charles. In 1826, the capital was shifted to its current, permanent location of Jefferson City, also on the Missouri River.\n\nThe state was rocked by the 1811–12 New Madrid earthquakes. Casualties were few due to the sparse population.\n\nOriginally the state's western border was a straight line, defined as the meridian passing through the Kawsmouth, the point where the Kansas River enters the Missouri River. The river has moved since this designation. This line is known as the Osage Boundary. In 1836 the Platte Purchase was added to the northwest corner of the state after purchase of the land from the native tribes, making the Missouri River the border north of the Kansas River. This addition increased the land area of what was already the largest state in the Union at the time (about to Virginia's 65,000 square miles, which then included West Virginia).\nIn the early 1830s, Mormon migrants from northern states and Canada began settling near Independence and areas just north of there. Conflicts over religion and slavery arose between the 'old settlers' (mainly from the South) and the Mormons (mainly from the North). The Mormon War erupted in 1838. By 1839, with the help of an \"Extermination Order\" by Governor Lilburn Boggs, the old settlers forcefully expelled the Mormons from Missouri and confiscated their lands.\n\nConflicts over slavery exacerbated border tensions among the states and territories. From 1838 to 1839, a border dispute with Iowa over the so-called Honey Lands resulted in both states' calling-up of militias along the border.\n\nWith increasing migration, from the 1830s to the 1860s Missouri's population almost doubled with every decade. Most of the newcomers were American-born, but many Irish and German immigrants arrived in the late 1840s and 1850s. As a majority were Catholic, they set up their own religious institutions in the state, which had been mostly Protestant. Having fled famine and oppression in Ireland, and revolutionary upheaval in Germany, the immigrants were not sympathetic to slavery. Many settled in cities, where they created a regional and then state network of Catholic churches and schools. Nineteenth-century German immigrants created the wine industry along the Missouri River and the beer industry in St. Louis.\n\nMost Missouri farmers practiced subsistence farming before the American Civil War. The majority of those who held slaves had fewer than five each. Planters, defined by some historians as those holding twenty slaves or more, were concentrated in the counties known as \"Little Dixie\", in the central part of the state along the Missouri River. The tensions over slavery chiefly had to do with the future of the state and nation. In 1860, enslaved African Americans made up less than 10% of the state's population of 1,182,012. In order to control the flooding of farmland and low-lying villages along the Mississippi, the state had completed construction of of levees along the river by 1860.\n\nAfter the secession of Southern states began in 1861, the Missouri legislature called for the election of a special convention on secession. The convention voted decisively to remain within the Union. Pro-Southern Governor Claiborne F. Jackson ordered the mobilization of several hundred members of the state militia who had gathered in a camp in St. Louis for training. Alarmed at this action, Union General Nathaniel Lyon struck first, encircling the camp and forcing the state troops to surrender. Lyon directed his soldiers, largely non-English-speaking German immigrants, to march the prisoners through the streets, and they opened fire on the largely hostile crowds of civilians who gathered around them. Soldiers killed unarmed prisoners as well as men, women and children of St. Louis in the incident that became known as the \"St. Louis Massacre\".\n\nThese events heightened Confederate support within the state. Governor Jackson appointed Sterling Price, president of the convention on secession, as head of the new Missouri State Guard. In the face of Union General Lyon's rapid advance through the state, Jackson and Price were forced to flee the capital of Jefferson City on June 14, 1861. In the town of Neosho, Missouri, Jackson called the state legislature into session. They enacted a secession ordinance. However, even under the Southern view of secession, only the state convention had the power to secede. Since the convention was dominated by unionists, and the state was more pro-Union than pro-Confederate in any event, the ordinance of secession adopted by the legislature is generally given little credence. The Confederacy nonetheless recognized it on October 30, 1861.\n\nWith the elected governor absent from the capital and the legislators largely dispersed, the state convention was reassembled with most of its members present, save 20 that fled south with Jackson's forces. The convention declared all offices vacant, and installed Hamilton Gamble as the new governor of Missouri. President Lincoln's administration immediately recognized Gamble's government as the legal Missouri government. The federal government's decision enabled raising pro-Union militia forces for service within the state as well as volunteer regiments for the Union Army.\n\nFighting ensued between Union forces and a combined army of General Price's Missouri State Guard and Confederate troops from Arkansas and Texas under General Ben McCulloch. After winning victories at the battle of Wilson's Creek and the siege of Lexington, Missouri and suffering losses elsewhere, the Confederate forces retreated to Arkansas and later Marshall, Texas, in the face of a largely reinforced Union Army.\n\nThough regular Confederate troops staged some large-scale raids into Missouri, the fighting in the state for the next three years consisted chiefly of guerrilla warfare. \"Citizen soldiers\" or insurgents such as Captain William Quantrill, Frank and Jesse James, the Younger brothers, and William T. Anderson made use of quick, small-unit tactics. Pioneered by the Missouri Partisan Rangers, such insurgencies also arose in portions of the Confederacy occupied by the Union during the Civil War. Historians have portrayed stories of the James brothers' outlaw years as an American \"Robin Hood\" myth. The vigilante activities of the Bald Knobbers of the Ozarks in the 1880s were an unofficial continuation of insurgent mentality long after the official end of the war, and they are a favorite theme in Branson's self-image.\n\nThe Progressive Era (1890s to 1920s) saw numerous prominent leaders from Missouri trying to end corruption and modernize politics, government and society. Joseph \"Holy Joe\" Folk was a key leader who made a strong appeal to middle class and rural evangelical Protestants. Folk was elected governor as a progressive reformer and Democrat in the 1904 election. He promoted what he called \"the Missouri Idea\", the concept of Missouri as a leader in public morality through popular control of law and strict enforcement. He successfully conducted antitrust prosecutions, ended free railroad passes for state officials, extended bribery statutes, improved election laws, required formal registration for lobbyists, made racetrack gambling illegal, and enforced the Sunday-closing law. He helped enact Progressive legislation, including an initiative and referendum provision, regulation of elections, education, employment and child labor, railroads, food, business, and public utilities. A number of efficiency-oriented examiner boards and commissions were established during Folk's administration, including many agricultural boards and the Missouri library commission.\n\nBetween the Civil War and the end of World War II, Missouri transitioned from a rural economy to a hybrid industrial-service-agricultural economy as the Midwest rapidly industrialized. The expansion of railroads to the West transformed Kansas City into a major transportation hub within the nation. The growth of the Texas cattle industry along with this increased rail infrastructure and the invention of the refrigerated boxcar also made Kansas City a major meatpacking center, as large cattle drives from Texas brought herds of cattle to Dodge City and other Kansas towns. There, the cattle were loaded onto trains destined for Kansas City, where they were butchered and distributed to the eastern markets. The first half of the twentieth century was the height of Kansas City's prominence and its downtown became a showcase for stylish Art Deco skyscrapers as construction boomed.\n\nIn 1930, there was a diphtheria epidemic in the area around Springfield, which killed approximately 100 people. Serum was rushed to the area, and medical personnel stopped the epidemic.\n\nDuring the mid-1950s and 1960s, St. Louis and Kansas City suffered deindustrialization and loss of jobs in railroads and manufacturing, as did other Midwestern industrial cities. In 1956 St. Charles claims to be the site of the first interstate highway project. Such highway construction made it easy for middle-class residents to leave the city for newer housing developed in the suburbs, often former farmland where land was available at lower prices. These major cities have gone through decades of readjustment to develop different economies and adjust to demographic changes. Suburban areas have developed separate job markets, both in knowledge industries and services, such as major retail malls.\n\nIn 2014, Missouri received national attention for the protests and riots that followed the shooting of Michael Brown by a police officer of Ferguson, which led Governor Jay Nixon to call out the Missouri National Guard. A grand jury declined to indict the officer, and the U.S. Department of Justice concluded, after careful investigation, that the police officer legitimately feared for his safety. However, in a separate investigation, the Department of Justice also found that the Ferguson Police Department and the City of Ferguson relied on unconstitutional practices in order to balance the city's budget through racially motivated excessive fines and punishments, that the Ferguson police \"had used excessive and dangerous force and had disproportionately targeted blacks,\" and that the municipal court \"emphasized revenue over public safety, leading to routine breaches of citizens' constitutional guarantees of due process and equal protection under the law.\"\n\nA series of student protests at the University of Missouri against what the protesters viewed as poor response by the administration to racist incidents on campus began in September 2015.\n\nOn June 7, 2017, the National Association for the Advancement of Colored People issued a warning to prospective African-American travelers to Missouri. This is the first NAACP warning ever covering an entire state. According to a 2018 report by the Missouri Attorney General's office, for the past 18 years, \"African Americans, Hispanics and other people of color are disproportionately affected by stops, searches and arrests.\" The same report found that the biggest discrepancy was in 2017, when \"black motorists were 85% more likely to be pulled over in traffic stops\".\n\nThe United States Census Bureau estimates that the population of Missouri was 6,083,672 on July 1, 2015, a 1.58% increase since the 2010 United States Census.\n\nMissouri had a population of 5,988,927, according to the 2010 Census; an increase of 392,369 (7.0 percent) since the year 2000. From 2000 to 2007, this includes a natural increase of 137,564 people since the last census (480,763 births less 343,199 deaths), and an increase of 88,088 people due to net migration into the state. Immigration from outside the United States resulted in a net increase of 50,450 people, and migration within the country produced a net increase of 37,638 people. Over half of Missourians (3,294,936 people, or 55.0%) live within the state's two largest metropolitan areas–St. Louis and Kansas City. The state's population density 86.9 in 2009, is also closer to the national average (86.8 in 2009) than any other state.\n\nIn 2011, the racial composition of the state was:\n\nIn 2011, 3.7% of the total population was of Hispanic or Latino origin (they may be of any race).\n\nThe U.S. Census of 2010 found that the population center of the United States is in Texas County, while the 2000 Census found the mean population center to be in Phelps County. The center of population of Missouri is in Osage County, in the city of Westphalia.\n\nIn 2004, the population included 194,000 foreign-born (3.4 percent of the state population).\n\nThe five largest ancestry groups in Missouri are: German (27.4 percent), Irish (14.8 percent), English (10.2 percent), American (8.5 percent) and French (3.7 percent).\n\nGerman Americans are an ancestry group present throughout Missouri. African Americans are a substantial part of the population in St. Louis (56.6% of African Americans in the state lived in St. Louis or St. Louis County as of the 2010 census), Kansas City, Boone County and in the southeastern Bootheel and some parts of the Missouri River Valley, where plantation agriculture was once important. Missouri Creoles of French ancestry are concentrated in the Mississippi River Valley south of St. Louis (see Missouri French). Kansas City is home to large and growing immigrant communities from Latin America esp. Mexico and Colombia, Africa (i.e. Sudan, Somalia and Nigeria), and Southeast Asia including China and the Philippines; and Europe like the former Yugoslavia (see Bosnian American). A notable Cherokee Indian population exists in Missouri.\n\nIn 2004, 6.6 percent of the state's population was reported as younger than 5 years old, 25.5 percent younger than 18, and 13.5 percent was 65 or older. Females were approximately 51.4 percent of the population. 81.3 percent of Missouri residents were high school graduates (more than the national average), and 21.6 percent had a bachelor's degree or higher. 3.4 percent of Missourians were foreign-born, and 5.1 percent reported speaking a language other than English at home.\n\nIn 2010, there were 2,349,955 households in Missouri, with 2.45 people per household. The home ownership rate was 70.0 percent, and the median value of an owner-occupied housing unit was $137,700. The median household income for 2010 was $46,262, or $24,724 per capita. There were 14.0 percent (1,018,118) of Missourians living below the poverty line in 2010.\n\nThe mean commute time to work was 23.8 minutes.\n\nIn 2011, 28.1% of Missouri's population younger than age 1 were minorities.\n\n\"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number.\"\n\n\nThe vast majority of people in Missouri speak English. Approximately 5.1% of the population reported speaking a language other than English at home. The Spanish language is spoken in small Latino communities in the St. Louis and Kansas City Metro areas.\n\nMissouri is home to an endangered dialect of the French language known as Missouri French. Speakers of the dialect, who call themselves \"Créoles\", are descendants of the French pioneers who settled the area then known as the Illinois Country beginning in the late 17th century. It developed in isolation from French speakers in Canada and Louisiana, becoming quite distinct from the varieties of Canadian French and Louisiana Creole French. Once widely spoken throughout the area, Missouri French is now nearly extinct, with only a few elderly speakers able to use it.\n\nAccording to a Pew Research study conducted in 2014, 80% of Missourians identify with a religion. 77% affiliate with Christianity and its various denominations, and the other 3% are adherents of non-Christian religions. The remaining 20% have no religion, with 2% specifically identifying as atheists and 3% identifying as agnostics (the other 15% do not identify as \"anything in particular\").\n\nBroken down, the religious demographics of Missouri are as follows:\n\nThe largest denominations by number of adherents in 2010 were the Southern Baptist Convention with 749,685; the Roman Catholic Church with 724,315; and the United Methodist Church with 226,409.\n\nAmong the other denominations there are approximately 93,000 Mormons in 253 congregations, 25,000 Jewish adherents in 21 synagogues, 12,000 Muslims in 39 masjids, 7,000 Buddhists in 34 temples, 20,000 Hindus in 17 temples, 2,500 Unitarians in 9 congregations, 2,000 Baha'i in 17 temples, 5 Sikh temples, a Zoroastrian temple, a Jain temple and an uncounted number of neopagans.\n\nSeveral religious organizations have headquarters in Missouri, including the Lutheran Church–Missouri Synod, which has its headquarters in Kirkwood, as well as the United Pentecostal Church International in Hazelwood, both outside St. Louis.\n\nIndependence, near Kansas City, is the headquarters for the Community of Christ (formerly the Reorganized Church of Jesus Christ of Latter Day Saints), the Church of Christ (Temple Lot) and the group Remnant Church of Jesus Christ of Latter Day Saints. This area and other parts of Missouri are also of significant religious and historical importance to The Church of Jesus Christ of Latter-day Saints (LDS Church), which maintains several sites and visitors centers.\n\nSpringfield is the headquarters of the Assemblies of God USA and the Baptist Bible Fellowship International. The General Association of General Baptists has its headquarters in Poplar Bluff. The Unity Church is headquartered in Unity Village.\n\nHindu Temple of St. Louis is the largest Hindu Temple in Missouri, serving over 14,000 Hindus.\n\nThe U.S. Department of Commerce’s Bureau of Economic Analysis estimated Missouri's 2016 gross state product at $299.1 billion, ranking 22nd among U.S. states. Per capita personal income in 2006 was $32,705, ranking 26th in the nation. Major industries include aerospace, transportation equipment, food processing, chemicals, printing/publishing, electrical equipment, light manufacturing, financial services and beer.\n\nThe agriculture products of the state are beef, soybeans, pork, dairy products, hay, corn, poultry, sorghum, cotton, rice, and eggs. Missouri is ranked 6th in the nation for the production of hogs and 7th for cattle. Missouri is ranked in the top five states in the nation for production of soy beans, and it is ranked fourth in the nation for the production of rice. In 2001, there were 108,000 farms, the second-largest number in any state after Texas. Missouri actively promotes its rapidly growing wine industry. According to the Missouri Partnership, Missouri's agriculture industry contributes $33 billion in GDP to Missouri's economy, and generates $88 billion in sales and more than 378,000 jobs.\n\nMissouri has vast quantities of limestone. Other resources mined are lead, coal, and crushed stone. Missouri produces the most lead of all of the states. Most of the lead mines are in the central eastern portion of the state. Missouri also ranks first or near first in the production of lime, a key ingredient in Portland cement.\n\nMissouri also has a growing science, agricultural technology and biotechnology field. Monsanto, one of the largest biotech companies in America, is based in St. Louis.\n\nTourism, services and wholesale/retail trade follow manufacturing in importance. Tourism benefits from the many rivers, lakes, caves, parks, etc. throughout the state. In addition to a network of state parks, Missouri is home to the Gateway Arch National Park in St. Louis and the Ozark National Scenic Riverways National Park. A much-visited show cave is Meramec Caverns in Stanton, Missouri. \n\nMissouri is the only state in the Union to have two Federal Reserve Banks: one in Kansas City (serving western Missouri, Kansas, Nebraska, Oklahoma, Colorado, northern New Mexico, and Wyoming) and one in St. Louis (serving eastern Missouri, southern Illinois, southern Indiana, western Kentucky, western Tennessee, northern Mississippi, and all of Arkansas).\nThe state's seasonally adjusted unemployment rate in April 2017 was 3.9 percent. In 2017, Missouri became a right-to-work state, but in August 2018, Missouri voters rejected a right-to-work law with 67% to 33%.\n\nPersonal income is taxed in ten different earning brackets, ranging from 1.5% to 6.0%. Missouri's sales tax rate for most items is 4.225% with some additional local levies. More than 2,500 Missouri local governments rely on property taxes levied on real property (real estate) and personal property.\n\nMost personal property is exempt, except for motorized vehicles. Exempt real estate includes property owned by governments and property used as nonprofit cemeteries, exclusively for religious worship, for schools and colleges and for purely charitable purposes. There is no inheritance tax and limited Missouri estate tax related to federal estate tax collection.\n\nIn 2017, the Tax Foundation rated Missouri as having the 5th-best corporate tax index, and the 15th-best overall tax climate. Missouri's corporate income tax rate is 6.25%; however, 50% of federal income tax payments may be deducted before computing taxable income, leading to an effective rate of 5.2%.\n\nIn 2012, Missouri had roughly 22,000 MW of installed electricity generation capacity. In 2011, 82% of Missouri's electricity was generated by coal. Ten percent was generated from the state's only nuclear power plant, the Callaway Plant in Callaway County, northeast of Jefferson City. Five percent was generated by natural gas. One percent was generated by hydroelectric sources, such as the dams for Truman Lake and Lake of the Ozarks. Missouri has a small but growing amount of wind and solar power—wind capacity increased from 309 MW in 2009 to 459 MW in 2011, while photovoltaics have increased from 0.2 MW to 1.3 MW over the same period. As of 2016, Missouri's solar installations had reached 141 MW.\n\nOil wells in Missouri produced 120,000 barrels of crude oil in fiscal 2012. There are no oil refineries in Missouri.\n\nMissouri has two major airport hubs: Lambert–St. Louis International Airport and Kansas City International Airport. Southern Missouri has the Springfield–Branson National Airport (SGF) with multiple non-stop destinations. Residents of Mid-Missouri use Columbia Regional Airport (COU) to fly to Chicago (ORD), Dallas (DFW) or Denver (DEN).\n\nTwo of the nation's three busiest rail centers are in Missouri. Kansas City is a major railroad hub for BNSF Railway, Norfolk Southern Railway, Kansas City Southern Railway, and Union Pacific Railroad, and every class 1 railroad serves Missouri. Kansas City is the second largest freight rail center in the US (but is first in the amount of tonnage handled). Like Kansas City, St. Louis is a major destination for train freight. Springfield remains an operational hub for BNSF Railway.\n\nAmtrak passenger trains serve Kansas City, La Plata, Jefferson City, St. Louis, Lee's Summit, Independence, Warrensburg, Hermann, Washington, Kirkwood, Sedalia, and Poplar Bluff. A proposed high-speed rail route in Missouri as part of the Chicago Hub Network has received $31 million in funding.\n\nThe only urban light rail/subway system operating in Missouri is MetroLink, which connects the city of St. Louis with suburbs in Illinois and St. Louis County. It is one of the largest systems (by track mileage) in the United States. The KC Streetcar in downtown Kansas City opened in May 2016.\n\nThe Gateway Multimodal Transportation Center in St. Louis is the largest active multi-use transportation center in the state. It is in downtown St. Louis, next to the historic Union Station complex. It serves as a hub center/station for MetroLink, the MetroBus regional bus system, Greyhound, Amtrak, and taxi services.\n\nMany cities have regular fixed-route systems, and many rural counties have rural public transit services. Greyhound and Trailways provide inter-city bus service in Missouri. Megabus serves St. Louis, but discontinued service to Columbia and Kansas City in 2015.\n\nThe Mississippi River and Missouri River are commercially navigable over their entire lengths in Missouri. The Missouri was channelized through dredging and jettys and the Mississippi was given a series of locks and dams to avoid rocks and deepen the river. St. Louis is a major destination for barge traffic on the Mississippi.\n\nFollowing the passage of Amendment 3 in late 2004, the Missouri Department of Transportation (MoDOT) began its Smoother, Safer, Sooner road-building program with a goal of bringing of highways up to good condition by December 2007. From 2006 to 2010 traffic deaths have decreased annually from 1,257 in 2005, to 1,096 in 2006, to 992 for 2007, to 960 for 2008, to 878 in 2009, to 821 in 2010.\n\nThe current Constitution of Missouri, the fourth constitution for the state, was adopted in 1945. It provides for three branches of government: the legislative, judicial, and executive branches. The legislative branch consists of two bodies: the House of Representatives and the Senate. These bodies comprise the Missouri General Assembly.\n\nThe House of Representatives has 163 members who are apportioned based on the last decennial census. The Senate consists of 34 members from districts of approximately equal populations. The judicial department comprises the Supreme Court of Missouri, which has seven judges, the Missouri Court of Appeals (an intermediate appellate court divided into three districts), sitting in Kansas City, St. Louis, and Springfield, and 45 Circuit Courts which function as local trial courts. The executive branch is headed by the Governor of Missouri and includes five other statewide elected offices. Following the death of Tom Schweich in 2015, only one of Missouri's statewide elected offices are held by Democrats.\n\nHarry S Truman (1884–1972), the 33rd President of the United States (Democrat, 1945–1953), was born in Lamar. He was a judge in Jackson County and then represented the state in the United States Senate for ten years, before being elected Vice-President in 1944. He lived in Independence after retiring.\n\nMissouri was widely regarded as a bellwether in American politics, often making it a swing state. The state had a longer stretch of supporting the winning presidential candidate than any other state, having voted with the nation in every election from 1904 to 2004 with a single exception: 1956, when Democratic candidate Adlai Stevenson of neighboring Illinois lost the election despite carrying Missouri. However, in recent years, areas of the state outside Kansas City, St. Louis, and Columbia have shifted heavily to the right, and so the state is no longer considered a bellwether by most analysts. Missouri twice voted against Democrat Barack Obama, who won in 2008 and 2012. Missouri voted for Romney by nearly 10% in 2012.\n\nOn October 24, 2012, there were 4,190,936 registered voters. At the state level, both Democratic Senator Claire McCaskill and Democratic Governor Jay Nixon were re-elected. On November 8, 2016, there were 4,223,787 registered voters, with 2,811,549 voting (66.6%).\n\nMissouri has been known for its population's generally \"stalwart, conservative, noncredulous\" attitude toward regulatory regimes, which is one of the origins of the state's unofficial nickname, the \"Show-Me State\". As a result, and combined with the fact that Missouri is one of America's leading alcohol states, regulation of alcohol and tobacco in Missouri is among the most laissez-faire in America. For 2013, the annual \"Freedom in the 50 States\" study prepared by the Mercatus Center at George Mason University ranked Missouri as #3 in America for alcohol freedom and #1 for tobacco freedom (#7 for freedom overall). The study notes that Missouri's \"alcohol regime is one of the least restrictive in the United States, with no blue laws and taxes well below average\", and that \"Missouri ranks best in the nation on tobacco freedom\".\n\nMissouri law makes it \"an improper employment practice\" for an employer to refuse to hire, to fire, or otherwise to disadvantage any person because that person lawfully uses alcohol and/or tobacco products when he or she is not at work.\n\nWith a large German immigrant population and the development of a brewing industry, Missouri always has had among the most permissive alcohol laws in the United States. It never enacted statewide prohibition. Missouri voters rejected prohibition in three separate referenda in 1910, 1912, and 1918. Alcohol regulation did not begin in Missouri until 1934.\n\nToday, alcohol laws are controlled by the state government, and local jurisdictions are prohibited from going beyond those state laws. Missouri has no statewide open container law or prohibition on drinking in public, no alcohol-related blue laws, no local option, no precise locations for selling liquor by the package (allowing even drug stores and gas stations to sell any kind of liquor), and no differentiation of laws based on alcohol percentage. State law protects persons from arrest or criminal penalty for public intoxication.\n\nMissouri law expressly prohibits any jurisdiction from going dry. Missouri law also expressly allows parents and guardians to serve alcohol to their children. The Power & Light District in Kansas City is one of the few places in the United States where a state law explicitly allows persons over the age of 21 to possess and consume open containers of alcohol in the street (as long as the beverage is in a plastic cup).\n\nAs for tobacco (as of July 2016), Missouri has the lowest cigarette excise taxes in the United States, at 17 cents per pack, and the state electorate voted in 2002, 2006, 2012, and twice in 2016 to keep it that way. In 2007, \"Forbes\" named Missouri's largest metropolitan area, St. Louis, America's \"best city for smokers\".\n\nAccording to the Centers for Disease Control and Prevention, in 2008 Missouri had the fourth highest percentage of adult smokers among U.S states, at 24.5%. Although Missouri's minimum age for purchase and distribution of tobacco products is 18, tobacco products can be distributed to persons under 18 by family members on private property.\n\nNo statewide smoking ban ever has been seriously entertained before the Missouri General Assembly, and in October 2008, a statewide survey by the Missouri Department of Health and Senior Services found that only 27.5% of Missourians support a statewide ban on smoking in all bars and restaurants. Missouri state law permits restaurants seating less than 50 people, bars, bowling alleys, and billiard parlors to decide their own smoking policies, without limitation.\n\nMissouri has 114 counties and one independent city (St. Louis).\n\nThe largest county by size is Texas County (1,179 sq. miles) and Shannon County is second (1,004 sq. miles). Worth County is the smallest (266 sq. miles). The independent city of St. Louis has only of area. St. Louis City is the most densely populated area (5,140.1 per sq. mi.) in Missouri.\n\nThe largest county by population (2012 estimate) is St. Louis County (1,000,438 residents), with Jackson County second (677,377 residents), St. Charles third (368,666), and St. Louis fourth (318,172). Worth County is the least populous with 2,171 (2010 census) residents.\n\nJefferson City is the capital of Missouri.\n\nThe five largest cities in Missouri are Kansas City, St. Louis, Springfield, Columbia, and Independence.\n\nSt. Louis is the principal city of the largest metropolitan area in Missouri, composed of 17 counties and the independent city of St. Louis; eight of those counties lie in Illinois. As of 2017 St. Louis was the 21st-largest metropolitan area in the nation with 2.81 million people. However, if ranked using Combined Statistical Area, it is 19th-largest with 2.91 million people in 2017. Some of the major cities making up the St. Louis Metro area in Missouri are O'Fallon, St. Charles, St. Peters, Florissant, Chesterfield, Wentzville, Wildwood, University City, and Ballwin.\n\nKansas City is Missouri's largest city and the principal city of the fifteen-county Kansas City Metropolitan Statistical Area, including six counties in the state of Kansas. As of 2017, it was the 30th-largest metropolitan area in the nation, with 2.13 million people. In the Combined Statistical Area in 2017, it ranked 25th with 2.47 million. Some of the other major cities comprising the Kansas City metro area in Missouri include Independence, Lee's Summit, Blue Springs, Liberty, Raytown, Gladstone, and Grandview.\n\nSpringfield is Missouri's third-largest city and the principal city of the Springfield-Branson Metropolitan Area, which has a population of 549,423 and includes seven counties in southwestern Missouri. Branson is a major tourist attraction in the Ozarks of southwestern Missouri. Some of the other major cities comprising the Springfield-Branson metro area include Nixa, Ozark, and Republic.\n\nThe Missouri State Board of Education has general authority over all public education in the state of Missouri. It is made up of eight citizens appointed by the governor and confirmed by the Missouri Senate.\n\nEducation is compulsory from ages seven to seventeen, and it is required that any parent, guardian or other person with custody of a child between the ages of seven and seventeen the compulsory attendance age for the district, must ensure that the child is enrolled in and regularly attends public, private, parochial school, home school or a combination of schools for the full term of the school year. Compulsory attendance also ends when children complete sixteen credits in high school.\n\nChildren in Missouri between the ages of five and seven are not required to be enrolled in school. However, if they are enrolled in a public school their parent, guardian or custodian must ensure that they regularly attend.\n\nMissouri schools are commonly but not exclusively divided into three tiers of primary and secondary education: elementary school, middle school or junior high school and high school. The public schools system includes kindergarten to 12th grade. District territories are often complex in structure. In some cases, elementary, middle and junior high schools of a single district feed into high schools in another district. High school athletics and competitions are governed by the Missouri State High School Activities Association (MSHSAA).\n\nHomeschooling is legal in Missouri and is an option to meet the compulsory education requirement. It is neither monitored nor regulated by the state's Department of Elementary and Secondary Education\n\nAnother gifted school is the Missouri Academy of Science, Mathematics and Computing, which is at the Northwest Missouri State University.\n\nThe University of Missouri System is Missouri's statewide public university system. The flagship institution and largest university in the state is the University of Missouri in Columbia. The others in the system are University of Missouri–Kansas City, University of Missouri–St. Louis, and Missouri University of Science and Technology in Rolla.\n\nDuring the late nineteenth and early twentieth century the state established a series of normal schools in each region of the state, originally named after the geographic districts: Northeast Missouri State University (now Truman State University) (1867), Central Missouri State University (now the University of Central Missouri) (1871), Southeast Missouri State University (1873), Southwest Missouri State University (now Missouri State University) (1905), Northwest Missouri State University (1905), Missouri Western State University (1915),Maryville University (1872) and Missouri Southern State University (1937). Lincoln University and Harris–Stowe State University were established in the mid-nineteenth century and are historically black colleges and universities.\n\nAmong private institutions Washington University in St. Louis and Saint Louis University are two top ranked schools in the US. There are numerous junior colleges, trade schools, church universities and other private universities in the state. A.T. Still University was the first osteopathic medical school in the world. Hannibal–LaGrange University in Hannibal, Missouri, was one of the first colleges west of the Mississippi (founded 1858 in LaGrange, Missouri, and moved to Hannibal in 1928).\n\nThe state funds a $2000, renewable merit-based scholarship, Bright Flight, given to the top three percent of Missouri high school graduates who attend a university in-state.\n\nThe 19th century border wars between Missouri and Kansas have continued as a sports rivalry between the University of Missouri and University of Kansas. The rivalry was chiefly expressed through football and basketball games between the two universities, but since Missouri left the Big 12 Conference in 2012, the teams no longer regularly play one another. It was the oldest college rivalry west of the Mississippi River and the second-oldest in the nation. Each year when the universities met to play, the game was coined the \"Border War.\" An exchange occurred following the game where the winner took a historic Indian War Drum, which had been passed back and forth for decades. Though Missouri and Kansas no longer have an annual game after the University of Missouri moved to the Southeastern Conference, tension still exists between the two schools.\n\nMany well-known musicians were born or have lived in Missouri. These include guitarist and rock pioneer Chuck Berry, singer and actress Josephine Baker, \"Queen of Rock\" Tina Turner, pop singer-songwriter Sheryl Crow, Michael McDonald of the Doobie Brothers, and rappers Nelly, Chingy and Akon, all of whom are either current or former residents of St. Louis.\n\nCountry singers from Missouri include New Franklin native Sara Evans, Cantwell native Ferlin Husky, West Plains native Porter Wagoner, Tyler Farr of Garden City, and Mora native Leroy Van Dyke, along with bluegrass musician Rhonda Vincent, a native of Greentop. Rapper Eminem was born in St. Joseph and also lived in Savannah and Kansas City. Ragtime composer Scott Joplin lived in St. Louis and Sedalia. Jazz saxophonist Charlie Parker lived in Kansas City. Rock and Roll singer Steve Walsh of the group Kansas was born in St. Louis and grew up in St. Joseph.\n\nThe Kansas City Symphony and the St. Louis Symphony Orchestra are the state's major orchestras. The latter is the nation's second-oldest symphony orchestra and achieved prominence in recent years under conductor Leonard Slatkin. Branson is well known for its music theaters, most of which bear the name of a star performer or musical group.\n\nMissouri is the native state of Mark Twain. His novels \"The Adventures of Tom Sawyer\" and \"The Adventures of Huckleberry Finn\" are set in his boyhood hometown of Hannibal. Authors Kate Chopin, T. S. Eliot and Tennessee Williams were from St. Louis. Kansas City-born writer William Least Heat-Moon resides in Rocheport. He is best known for \"Blue Highways\", a chronicle of his travels to small towns across America, which was on The New York Times Bestseller list for 42 weeks in 1982–1983.\n\nFilmmaker, animator, and businessman Walt Disney spent part of his childhood in the Linn County town of Marceline before settling in Kansas City. Disney began his artistic career in Kansas City, where he founded the Laugh-O-Gram Studio.\n\nSeveral film versions of Mark Twain's novels \"The Adventures of Tom Sawyer\" and \"The Adventures of Huckleberry Finn\" have been made. \"Meet Me in St. Louis\", a musical involving the 1904 St. Louis World's Fair, starred Judy Garland. Part of the 1983 road movie \"National Lampoon's Vacation\" was shot on location in Missouri, for the Griswolds' trip from Chicago to Los Angeles. The Thanksgiving holiday film \"Planes, Trains, and Automobiles\" was partially shot at Lambert–St. Louis International Airport. \"White Palace\" was filmed in St. Louis. The award-winning 2010 film \"Winter's Bone\" was shot in the Ozarks of Missouri. \"Up in the Air\" starring George Clooney was filmed in St. Louis. John Carpenter's \"Escape from New York\" was filmed in St. Louis during the early 1980s due to the large number of abandoned buildings in the city. The 1973 movie \"Paper Moon\", which starred Ryan and Tatum O'Neal, was partly filmed in St. Joseph. Most of HBO's film \"Truman\" (1995) was filmed in Kansas City, Independence, and the surrounding area; Gary Sinise won an Emmy for his portrayal of Harry Truman in the film. \"Ride With the Devil\" (1999), starring Jewel and Tobey Maguire, was filmed in the countryside of Jackson County (where the historic events of the film actually took place). \"Gone Girl\", a 2014 film starring Ben Affleck, Rosamund Pike, Neil Patrick Harris, and Tyler Perry, was filmed in Cape Girardeau.\n\nMissouri hosted the 1904 Summer Olympics at St. Louis, the first time the games were hosted in the United States.\n\nProfessional major league teams\n\nFormer professional major league teams\n\n\n"}
{"id": "19574", "url": "https://en.wikipedia.org/wiki?curid=19574", "title": "Monitor", "text": "Monitor\n\nMonitor or monitor may refer to:\n\n\n\n\n\n\n\n\n\n"}
{"id": "19577", "url": "https://en.wikipedia.org/wiki?curid=19577", "title": "Moses", "text": "Moses\n\nMoses () was a prophet according to the teachings of the Abrahamic religions; however, scholarly consensus sees Moses as a legendary figure and not a historical person.\n\nAccording to the Hebrew Bible, he was adopted by an Egyptian princess, and later in life became the leader of the Israelites and lawgiver, to whom the authorship of the Torah, or acquisition of the Torah from Heaven is traditionally attributed. Also called \"Moshe Rabbenu\" in Hebrew (, \"lit.\" \"Moses our Teacher\"), he is the most important prophet in Judaism. He is also an important prophet in Christianity, Islam, the Bahá'í Faith, and a number of other Abrahamic religions.\n\nAccording to the Book of Exodus, Moses was born in a time when his people, the Israelites, an enslaved minority, were increasing in numbers and the Egyptian Pharaoh was worried that they might ally themselves with Egypt's enemies. Moses' Hebrew mother, Jochebed, secretly hid him when the Pharaoh ordered all newborn Hebrew boys to be killed in order to reduce the population of the Israelites. Through the Pharaoh's daughter (identified as Queen Bithia in the Midrash), the child was adopted as a foundling from the Nile river and grew up with the Egyptian royal family. After killing an Egyptian slavemaster (because the slavemaster was smiting a Hebrew), Moses fled across the Red Sea to Midian, where he encountered The Angel of the Lord, speaking to him from within a burning bush on Mount Horeb (which he regarded as the Mountain of God).\n\nGod sent Moses back to Egypt to demand the release of the Israelites from slavery. Moses said that he could not speak eloquently, so God allowed Aaron, his brother, to become his spokesperson. After the Ten Plagues, Moses led the Exodus of the Israelites out of Egypt and across the Red Sea, after which they based themselves at Mount Sinai, where Moses received the Ten Commandments. After 40 years of wandering in the desert, Moses died within sight of the Promised Land on Mount Nebo.\n\nJerome gives 1592 BCE, and James Ussher 1571 BCE as Moses' birth year. In the Book of Deuteronomy, Moses was called \"the man of God\".\n\nThe name of Moses belongs to 1st millennium Egyptian. The Biblical account of Moses' birth provides him with a folk etymology to explain the ostensible meaning of his name. He is said to have received it from the Pharaoh's daughter: \"he became her son. She named him Moses (Moshe), saying, 'I drew him out (\"meshitihu\") of the water. This explanation links it to a verb \"mashah\", meaning \"to draw out\", which makes the Pharaoh's daughter's declaration a play on words. The princess made a grammatical mistake which is prophetic of his future role in legend, as someone who will \"draw the people of Israel out of Egypt through the waters of the Red Sea.\"\n\nSeveral etymologies have been proposed. An Egyptian root \"msy\", \"child of\", has been considered as a possible etymology, arguably an abbreviation of a theophoric name, as for example in Egyptian names like Thutmoses (Thoth created him) and Ramesses (Ra created him), with the god's name omitted. Abraham Yahuda, based on the spelling given in the Tanakh, argues that it combines \"water\" or \"seed\" and \"pond, expanse of water\", thus yielding the sense of \"child of the Nile\" (\"mw-še\").\n\nThe Hebrew etymology in the Biblical story may reflect an attempt to cancel out traces of Moses' Egyptian origins. The Egyptian character of his name was recognized as such by ancient Jewish writers like Philo of Alexandria and Josephus. Philo linked Mōēsēs () to the Egyptian (Coptic) word for water (\"mou\"/μῶυ), while Josephus, in his Antiquities of the Jews, claimed that the second element, \"-esês\", meant 'those who are saved'. The problem of how an Egyptian princess, known to Josephus as Thermutis (identified as Tharmuth) and in later Jewish tradition as Bithiah, could have known Hebrew puzzled medieval Jewish commentators like Abraham ibn Ezra and Hezekiah ben Manoah, known also as Hizkuni. Hizkuni suggested she either converted or took a tip from Jochebed.\n\nThe Israelites had settled in the Land of Goshen in the time of Joseph and Jacob, but a new pharaoh arose who oppressed the children of Israel. At this time Moses was born to his father Amram, son of Kehath the Levite, who entered Egypt with Jacob's household; his mother was Jochebed (also Yocheved), who was kin to Kehath. Moses had one older (by seven years) sister, Miriam, and one older (by three years) brother, Aaron.\nThe Pharaoh had commanded that all male Hebrew children born would be drowned in the river Nile, but Moses' mother placed him in an ark and concealed the ark in the bulrushes by the riverbank, where the baby was discovered and adopted by Pharaoh's daughter, and raised as an Egyptian. One day after Moses had reached adulthood he killed an Egyptian who was beating a Hebrew. Moses, in order to escape the Pharaoh's death penalty, fled to Midian (a desert country south of Judah), where he married Zipporah.\n\nThere, on Mount Horeb, God appeared to Moses as a burning bush, revealed to Moses his name YHWH (probably pronounced Yahweh) and commanded him to return to Egypt and bring his chosen people (Israel) out of bondage and into the Promised Land (Canaan). During the journey, God tried to kill Moses because he had not circumcised his son, but Zipporah saved his life. Moses returned to carry out God's command, but God caused the Pharaoh to refuse, and only after God had subjected Egypt to ten plagues did the Pharaoh relent. Moses led the Israelites to the border of Egypt, but there God hardened the Pharaoh's heart once more, so that he could destroy the Pharaoh and his army at the Red Sea Crossing as a sign of his power to Israel and the nations.\n\nAfter defeating the Amalekites in Rephidim, Moses led the Israelites to biblical Mount Sinai, where he was given the Ten Commandments from God, written on stone tablets. However, since Moses remained a long time on the mountain, some of the people feared that he might be dead, so they made a statue of a golden calf and worshiped it, thus disobeying and angering God and Moses. Moses, out of anger, broke the tablets, and later ordered the elimination of those who had worshiped the golden statue, which was melted down and fed to the idolaters. He also wrote the ten commandments on a new set of tablets. Later at Mount Sinai, Moses and the elders entered into a covenant, by which Israel would become the people of YHWH, obeying his laws, and YHWH would be their god. Moses delivered the laws of God to Israel, instituted the priesthood under the sons of Moses' brother Aaron, and destroyed those Israelites who fell away from his worship. In his final act at Sinai, God gave Moses instructions for the Tabernacle, the mobile shrine by which he would travel with Israel to the Promised Land.\n\nFrom Sinai, Moses led the Israelites to the Desert of Paran on the border of Canaan. From there he sent twelve spies into the land. The spies returned with samples of the land's fertility, but warned that its inhabitants were giants. The people were afraid and wanted to return to Egypt, and some rebelled against Moses and against God. Moses told the Israelites that they were not worthy to inherit the land, and would wander the wilderness for forty years until the generation who had refused to enter Canaan had died, so that it would be their children who would possess the land.\n\nWhen the forty years had passed, Moses led the Israelites east around the Dead Sea to the territories of Edom and Moab. There they escaped the temptation of idolatry, conquered the lands of Og and Sihon in Transjordan, received God's blessing through Balaam the prophet, and massacred the Midianites, who by the end of the Exodus journey had become the enemies of the Israelites due to their notorious role in enticing the Israelites to sin against God. Moses was twice given notice that he would die before entry to the Promised Land: in Numbers 27:13, once he had seen the Promised Land from a viewpoint on Mount Abarim, and again in Numbers 31:1 once battle with the Midianites had been won.\n\nOn the banks of the Jordan River, in sight of the land, Moses assembled the tribes. After recalling their wanderings he delivered God's laws by which they must live in the land, sang a song of praise and pronounced a blessing on the people, and passed his authority to Joshua, under whom they would possess the land. Moses then went up Mount Nebo to the top of Pisgah, looked over the promised land of Israel spread out before him, and died, at the age of one hundred and twenty. More humble than any other man (Num. 12:3), \"there hath not arisen a prophet since in Israel like unto Moses, whom YHWH knew face to face\" (Deuteronomy 34:10). The New Testament states that after Moses' death, Michael the Archangel and the Devil disputed over his body (Epistle of Jude 1:9).\n\nMoses is honoured among Jews today as the \"lawgiver of Israel\", and he delivers several sets of laws in the course of the four books. The first is the Covenant Code (Exodus –), the terms of the covenant which God offers to the Israelites at biblical Mount Sinai. Embedded in the covenant are the Decalogue (the Ten Commandments, Exodus 20:1–17) and the Book of the Covenant (Exodus 20:22–23:19). The entire Book of Leviticus constitutes a second body of law, the Book of Numbers begins with yet another set, and the Book of Deuteronomy another. \n\nMoses has traditionally been regarded as the author of those four books and the Book of Genesis, which together comprise the Torah, the first section of the Hebrew Bible.\n\nThe modern scholarly consensus is that the figure of Moses is legendary, and not historical, although a \"Moses-like figure may have existed somewhere in the southern Transjordan in the mid-late 13th century B.C.\" Certainly no Egyptian sources mention Moses or the events of Exodus–Deuteronomy, nor has any archaeological evidence been discovered in Egypt or the Sinai wilderness to support the story in which he is the central figure. The story of his discovery picks up a familiar motif in ancient Near Eastern mythological accounts of the ruler who rises from humble origins: Thus Sargon of Akkad's Akkadian account of his own origins runs;\n<poem>My mother, the high priestess, conceived; in secret she bore me\nShe set me in a basket of rushes, with bitumen she sealed my lid\nShe cast me into the river which rose over me.</poem>\n\nThe tradition of Moses as a lawgiver and culture hero of the Israelites may go back to the 7th-century BCE sources of the Deuteronomist, which might conserve earlier traditions. Kenneth Kitchen, described as a distinguished but lonely voice among British Egyptologists on the subject, argues that there is an historic core behind the Exodus, with Egyptian corvée labour exacted from Hebrews during the imperialist control exercised by the Egyptian Empire over Canaan from the time of the Thutmosides down to the revolt against Merneptah and Rameses III. Kitchen argued that there is a lack of factual evidence to deny his existence. Martin Noth called the Deuteronomic description of Moses' burial a ″lone historical tradition.″ William Albright believed in the essential historicity of the biblical tales of Moses and the Exodus, accepting however that the core narrative had been overlaid by legendary accretions. Biblical minimalists such as Philip R. Davies and Niels Peter Lemche regard all biblical books, and the stories of an Exodus, united monarchy, exile and return as fictions composed by a social elite in Yehud in the Persian period or even later, the purpose being to legitimize a return to indigenous roots.\n\nDespite the imposing fame associated with Moses, no source mentions him until he emerges in texts associated with the Babylonian exile. A theory developed by Cornelius Tiele in 1872, which had proved influential, argued that Yahweh was a Midianite god, introduced to the Israelites by Moses, whose father-in-law Jethro was a Midianite priest. It was to such a Moses that Yahweh reveals his real name, hidden from the Patriarchs who knew him only as El Shaddai. Against this view is the modern consensus that most of the Israelites were native to Palestine. Martin Noth argued that the Pentateuch uses the figure of Moses, originally linked to legends of a Transjordan conquest, as a narrative bracket or late redactional device to weld together 4 of the 5, originally independent, themes of that work.\n\nThe name King Mesha of Moab has been linked to that of Moses. Mesha also is associated with narratives of an exodus and a conquest, and several motifs in stories about him are shared with the Exodus tale and that regarding Israel's war with Moab (2 Kings 3). Moab rebels against oppression, like Moses, leads his people out of Israel, as Moses does from Egypt, and his first-born son is slaughtered at the wall of Kir-hareseth as the firstborn of Israel are condemned to slaughter in the Exodus story, \"an infernal passover that delivers Mesha while wrath burns against his enemies\".\n\nAn Egyptian version of the tale that crosses over with the Moses story is found in Manetho who, according to the summary in Josephus, wrote that a certain Osarseph, a Heliopolitan priest, became overseer of a band of lepers, when Amenophis, following indications by Amenhotep, son of Hapu, had all the lepers in Egypt quarantined in order to cleanse the land so that he might see the gods. The lepers are bundled into Avaris, the former capital of the Hyksos, where Osarseph prescribes for them everything forbidden in Egypt, while proscribing everything permitted in Egypt. They invite the Hyksos to reinvade Egypt, rule with them for 13 years – Osarseph then assumes the name Moses – and are then driven out.\n\nNon-biblical writings about Jews, with references to the role of Moses, first appear at the beginning of the Hellenistic period, from 323 BCE to about 146 BCE. Shmuel notes that \"a characteristic of this literature is the high honour in which it holds the peoples of the East in general and some specific groups among these peoples.\"\n\nIn addition to the Judeo-Roman or Judeo-Hellenic historians Artapanus, Eupolemus, Josephus, and Philo, a few non-Jewish historians including Hecataeus of Abdera (quoted by Diodorus Siculus), Alexander Polyhistor, Manetho, Apion, Chaeremon of Alexandria, Tacitus and Porphyry also make reference to him. The extent to which any of these accounts rely on earlier sources is unknown. Moses also appears in other religious texts such as the Mishnah (c. 200 CE), Midrash (200–1200 CE), and the Quran (c. 610–653).\n\nThe figure of Osarseph in Hellenistic historiography is a renegade Egyptian priest who leads an army of lepers against the pharaoh and is finally expelled from Egypt, changing his name to Moses.\n\nThe earliest existing reference to Moses in Greek literature occurs in the Egyptian history of Hecataeus of Abdera (4th century BCE). All that remains of his description of Moses are two references made by Diodorus Siculus, wherein, writes historian Arthur Droge, he \"describes Moses as a wise and courageous leader who left Egypt and colonized Judaea.\" Among the many accomplishments described by Hecataeus, Moses had founded cities, established a temple and religious cult, and issued laws:\nDroge also points out that this statement by Hecataeus was similar to statements made subsequently by Eupolemus.\n\nThe Jewish historian Artapanus of Alexandria (2nd century BCE), portrayed Moses as a cultural hero, alien to the Pharaonic court. According to theologian John Barclay, the Moses of Artapanus \"clearly bears the destiny of the Jews, and in his personal, cultural and military splendor, brings credit to the whole Jewish people.\"\nArtapanus goes on to relate how Moses returns to Egypt with Aaron, and is imprisoned, but miraculously escapes through the name of YHWH in order to lead the Exodus. This account further testifies that all Egyptian temples of Isis thereafter contained a rod, in remembrance of that used for Moses' miracles. He describes Moses as 80 years old, \"tall and ruddy, with long white hair, and dignified.\"\n\nSome historians, however, point out the \"apologetic nature of much of Artapanus' work,\" with his addition of extra-biblical details, such as his references to Jethro: the non-Jewish Jethro expresses admiration for Moses' gallantry in helping his daughters, and chooses to adopt Moses as his son.\n\nStrabo, a Greek historian, geographer and philosopher, in his \"Geographica\" (c. 24 CE), wrote in detail about Moses, whom he considered to be an Egyptian who deplored the situation in his homeland, and thereby attracted many followers who respected the deity. He writes, for example, that Moses opposed the picturing of the deity in the form of man or animal, and was convinced that the deity was an entity which encompassed everything – land and sea:\nIn Strabo's writings of the history of Judaism as he understood it, he describes various stages in its development: from the first stage, including Moses and his direct heirs; to the final stage where \"the Temple of Jerusalem continued to be surrounded by an aura of sanctity.\" Strabo's \"positive and unequivocal appreciation of Moses' personality is among the most sympathetic in all ancient literature.\" His portrayal of Moses is said to be similar to the writing of Hecataeus who \"described Moses as a man who excelled in wisdom and courage.\"\n\nEgyptologist Jan Assmann concludes that Strabo was the historian \"who came closest to a construction of Moses' religion as monotheistic and as a pronounced counter-religion.\" It recognized \"only one divine being whom no image can represent... [and] the only way to approach this god is to live in virtue and in justice.\"\n\nThe Roman historian Tacitus (c. 56–120 CE) refers to Moses by noting that the Jewish religion was monotheistic and without a clear image. His primary work, wherein he describes Jewish philosophy, is his \"Histories\" (c. 100), where, according to Arthur Murphy, as a result of the Jewish worship of one God, \"pagan mythology fell into contempt.\" Tacitus states that, despite various opinions current in his day regarding the Jews' ethnicity, most of his sources are in agreement that there was an Exodus from Egypt. By his account, the Pharaoh Bocchoris, suffering from a plague, banished the Jews in response to an oracle of the god Zeus-Amun.\nIn this version, Moses and the Jews wander through the desert for only six days, capturing the Holy Land on the seventh.\n\nThe Septuagint, the Greek version of the Hebrew Bible, influenced Longinus, who may have been the author of the great book of literary criticism, \"On the Sublime\". The date of composition is unknown, but it is commonly assigned to the late Ist century C.E.\n\nThe writer quotes Genesis in a \"style which presents the nature of the deity in a manner suitable to his pure and great being,\" however he does not mention Moses by name, calling him 'no chance person' () but \"the Lawgiver\" (, thesmothete) of the Jews,\" a term that puts him on a par with Lycurgus and Minos. Aside from a reference to Cicero, Moses is the only non-Greek writer quoted in the work, contextually he is put on a par with Homer, and he is described \"with far more admiration than even Greek writers who treated Moses with respect, such as Hecataeus and Strabo.\n\nIn Josephus' (37 – c. 100 CE) \"Antiquities of the Jews\", Moses is mentioned throughout. For example Book VIII Ch. IV, describes Solomon's Temple, also known as the First Temple, at the time the Ark of the Covenant was first moved into the newly built temple:\nAccording to Feldman, Josephus also attaches particular significance to Moses' possession of the \"cardinal virtues of wisdom, courage, temperance, and justice.\" He also includes piety as an added fifth virtue. In addition, he \"stresses Moses' willingness to undergo toil and his careful avoidance of bribery. Like Plato's philosopher-king, Moses excels as an educator.\"\n\nNumenius, a Greek philosopher who was a native of Apamea, in Syria, wrote during the latter half of the 2nd century CE. Historian Kennieth Guthrie writes that \"Numenius is perhaps the only recognized Greek philosopher who explicitly studied Moses, the prophets, and the life of Jesus...\" He describes his background:\nThe Christian saint and religious philosopher Justin Martyr (103–165 CE) drew the same conclusion as Numenius, according to other experts. Theologian Paul Blackham notes that Justin considered Moses to be \"more trustworthy, profound and truthful because he is \"older\" than the Greek philosophers.\" He quotes him:\n\nMost of what is known about Moses from the Bible comes from the books of Exodus, Leviticus, Numbers and Deuteronomy. The majority of scholars consider the compilation of these books to go back to the Persian period, 538–332 BCE, but based on earlier written and oral traditions. There is a wealth of stories and additional information about Moses in the Jewish apocrypha and in the genre of rabbinical exegesis known as Midrash, as well as in the primary works of the Jewish oral law, the Mishnah and the Talmud. Moses is also given a number of bynames in Jewish tradition. The Midrash identifies Moses as one of seven biblical personalities who were called by various names. Moses' other names were: Jekuthiel (by his mother), Heber (by his father), Jered (by Miriam), Avi Zanoah (by Aaron), Avi Gedor (by Kohath), Avi Soco (by his wet-nurse), Shemaiah ben Nethanel (by people of Israel). Moses is also attributed the names Toviah (as a first name), and Levi (as a family name) (Vayikra Rabbah 1:3), Heman, Mechoqeiq (lawgiver) and Ehl Gav Ish (Numbers 12:3). In another exegesis, Moses had ascended to the first heaven until the seventh, even visited Paradise and Hell alive, after he saw the Divine vision in Mount Horeb.\n\nJewish historians who lived at Alexandria, such as Eupolemus, attributed to Moses the feat of having taught the Phoenicians their alphabet, similar to legends of Thoth. Artapanus of Alexandria explicitly identified Moses not only with Thoth/Hermes, but also with the Greek figure Musaeus (whom he called \"the teacher of Orpheus\"), and ascribed to him the division of Egypt into 36 districts, each with its own liturgy. He named the princess who adopted Moses as Merris, wife of Pharaoh Chenephres.\n\nJewish tradition considers Moses to be the greatest prophet who ever lived. Despite his importance, Judaism stresses that Moses was a human being, and is therefore not to be worshipped. Only God is worthy of worship in Judaism.\n\nTo Orthodox Jews, Moses is called \"Moshe Rabbenu, `Eved HaShem, Avi haNeviim zya\"a\": \"Our Leader Moshe, Servant of God, Father of all the Prophets (may his merit shield us, amen)\". In the orthodox view, Moses received not only the Torah, but also the revealed (written and oral) and the hidden (the \"`hokhmat nistar\" teachings, which gave Judaism the Zohar of the Rashbi, the Torah of the Ari haQadosh and all that is discussed in the Heavenly Yeshiva between the Ramhal and his masters).\n\nArising in part from his age of death (120 according to Deut. 34:7) and that \"his eye had not dimmed, and his vigor had not diminished,\" the phrase \"may you live to 120\" has become a common blessing among Jews, especially since 120 is elsewhere stated as the maximum age for Noah's descendants (one interpretation of ).\n\nMoses is mentioned more often in the New Testament than any other Old Testament figure. For Christians, Moses is often a symbol of God's law, as reinforced and expounded on in the teachings of Jesus. New Testament writers often compared Jesus' words and deeds with Moses' to explain Jesus' mission. In Acts 7:39–43, 51–53, for example, the rejection of Moses by the Jews who worshipped the golden calf is likened to the rejection of Jesus by the Jews that continued in traditional Judaism.\n\nMoses also figures in several of Jesus' messages. When he met the Pharisee Nicodemus at night in the third chapter of the Gospel of John, he compared Moses' lifting up of the bronze serpent in the wilderness, which any Israelite could look at and be healed, to his own lifting up (by his death and resurrection) for the people to look at and be healed. In the sixth chapter, Jesus responded to the people's claim that Moses provided them \"manna\" in the wilderness by saying that it was not Moses, but God, who provided. Calling himself the \"bread of life\", Jesus stated that He was provided to feed God's people. \n\nMoses, along with Elijah, is presented as meeting with Jesus in all three Synoptic Gospels of the Transfiguration of Jesus in Matthew 17, Mark 9, and Luke 9, respectively. Jesus refers to the scribes and the Pharisees of the Temple as \"seated in the chair of Moses\" (, \"epi tēs Mōuseōs kathedras\") \n\nHis relevance to modern Christianity has not diminished. Moses is considered to be a saint by several churches; and is commemorated as a prophet in the respective Calendars of Saints of the Eastern Orthodox Church, the Roman Catholic Church, and the Lutheran churches on September 4. In Eastern Orthodox liturgics for September 4, Moses is commemorated as the \"Holy Prophet and God-seer Moses, on Mount Nebo\". The Orthodox Church also commemorates him on the Sunday of the Forefathers, two Sundays before the Nativity.\n\nThe Armenian Apostolic Church commemorates him as one of the Holy Forefathers in their Calendar of Saints on July 30.\n\nMembers of The Church of Jesus Christ of Latter-day Saints (colloquially called Mormons) generally view Moses in the same way that other Christians do. However, in addition to accepting the biblical account of Moses, Mormons include Selections from the Book of Moses as part of their scriptural canon. This book is believed to be the translated writings of Moses, and is included in the Pearl of Great Price.\n\nLatter-day Saints are also unique in believing that Moses was taken to heaven without having tasted death (translated). In addition, Joseph Smith and Oliver Cowdery stated that on April 3, 1836, Moses appeared to them in the Kirtland Temple (located in Kirtland, Ohio) in a glorified, immortal, physical form and bestowed upon them the \"keys of the gathering of Israel from the four parts of the earth, and the leading of the ten tribes from the land of the north.\"\n\nMoses is mentioned more in the Quran than any other individual and his life is narrated and recounted more than that of any other Islamic prophet. In general, Moses is described in ways which parallel the Islamic prophet Muhammad, and \"his character exhibits some of the main themes of Islamic theology,\" including the \"moral injunction that we are to submit ourselves to God.\"\n\nMoses is defined in the Quran as both prophet (\"nabi\") and messenger (\"rasul\"), the latter term indicating that he was one of those prophets who brought a scripture and law to his people. \n\nHuston Smith describes an account in the Quran of meetings in heaven between Moses and Muhammad, which Huston states were \"one of the crucial events in Muhammad's life,\" and resulted in Muslims observing 5 daily prayers.\n\nMoses is mentioned 502 times in the Quran; passages mentioning Moses include 2.49–61, 7.103–60, 10.75–93, 17.101–04, 20.9–97, 26.10–66, 27.7–14, 28.3–46, 40.23–30, 43.46–55, 44.17–31, and 79.15–25. and many others. Most of the key events in Moses' life which are narrated in the Bible are to be found dispersed through the different Surahs of the Quran, with a story about meeting Khidr which is not found in the Bible.\n\nIn the Moses story related by the Quran, Jochebed is commanded by God to place Moses in an ark and cast him on the waters of the Nile, thus abandoning him completely to God's protection. The Pharaoh's wife Asiya, not his daughter, found Moses floating in the waters of the Nile. She convinced the Pharaoh to keep him as their son because they were not blessed with any children.\nThe Quran's account has emphasized Moses' mission to invite the Pharaoh to accept God's divine message as well as give salvation to the Israelites. According to the Quran, Moses encourages the Israelites to enter Canaan, but they are unwilling to fight the Canaanites, fearing certain defeat. Moses responds by pleading to Allah that he and his brother Aaron be separated from the rebellious Israelites. After which the Israelites are made to wander for 40 years.\n\nAccording to Islamic tradition, Moses is buried at Maqam El-Nabi Musa, Jericho.\n\nMoses is one of the most important of God's messengers in the Bahá'í Faith being designated a Manifestation of God. An epithet of Moses in Baha'i scriptures is the One Who Conversed with God.\n\nImportant figures in the Baha’i religion, such as Abdul’l-Baha, have highlighted the fact that Moses, like Abraham, had none of the makings of a great man of history, but through God's assistance he was able achieve many great things. He is described as having been \"for a long time a shepherd in the wilderness,\" of having had a stammer, and of being \"much hated and detested\" by the Pharaoh and the ancient Egyptians of his time. He is said to have been raised in an oppressive household, and to have been known, in Egypt, as a man who had committed murder – though he had done so in order to prevent an act of cruelty.\n\nNevertheless, like Abraham, through the assistance of God, he achieved great things and gained renown even beyond the Levant. Chief among these achievements was the freeing of his people, the Hebrews, from bondage in Egypt and leading \"them to the Holy Land.\" He is viewed as the one who bestowed on Israel 'the religious and the civil law' which gave them \"honour among all nations,\" and which spread their fame to different parts of the world.\n\nFurthermore, through the law, Moses is believed to have led the Hebrews 'to the highest possible degree of civilization at that period.’ Abdul’l-Baha asserts that the ancient Greek philosophers regarded \"the illustrious men of Israel as models of perfection.\" Chief among these philosophers, he says, was Socrates who \"visited Syria, and took from the children of Israel the teachings of the Unity of God and of the immortality of the soul.\"\n\nMoses is further described as paving the way for Bahá'u'lláh and his ultimate revelation, and as a teacher of truth, whose teachings were in line with the customs of his time.\n\nIn a metaphorical sense in the Christian tradition, a \"Moses\" has been referred to as the leader who delivers the people from a terrible situation. Among the Presidents of the United States known to have used the symbolism of Moses were Harry S. Truman, Jimmy Carter, Ronald Reagan, Bill Clinton, George W. Bush and Barack Obama, who referred to his supporters as \"the Moses generation.\"\n\nIn subsequent years, theologians linked the Ten Commandments with the formation of early democracy. Scottish theologian William Barclay described them as \"the universal foundation of all things… the law without which nationhood is impossible. …Our society is founded upon it. Pope Francis addressed the United States Congress in 2015 stating that all people need to \"keep alive their sense of unity by means of just legislation... [and] the figure of Moses leads us directly to God and thus to the transcendent dignity of the human being.\n\nReferences to Moses were used by the Puritans, who relied on the story of Moses to give meaning and hope to the lives of Pilgrims seeking religious and personal freedom in America. John Carver was the first governor of Plymouth colony and first signer of the Mayflower Compact, which he wrote in 1620 during the ship \"Mayflower\"'s three-month voyage. He inspired the Pilgrims with a \"sense of earthly grandeur and divine purpose,\" notes historian Jon Meacham, and was called the \"Moses of the Pilgrims.\" Early American writer James Russell Lowell noted the similarity of the founding of America by the Pilgrims to that of ancient Israel by Moses:\nFollowing Carver's death the following year, William Bradford was made governor. He feared that the remaining Pilgrims would not survive the hardships of the new land, with half their people having already died within months of arriving. Bradford evoked the symbol of Moses to the weakened and desperate Pilgrims to help calm them and give them hope: \"Violence will break all. Where is the meek and humble spirit of Moses?\" William G. Dever explains the attitude of the Pilgrims: \"We considered ourselves the 'New Israel,' particularly we in America. And for that reason we knew who we were, what we believed in and valued, and what our 'manifest destiny' was.\"\n\nOn July 4, 1776, immediately after the Declaration of Independence was officially passed, the Continental Congress asked John Adams, Thomas Jefferson, and Benjamin Franklin to design a seal that would clearly represent a symbol for the new United States. They chose the symbol of Moses leading the Israelites to freedom. The Founding Fathers of the United States inscribed the words of Moses on the Liberty Bell: \"Proclaim Liberty thro' all the Land to all the Inhabitants thereof.\" (Leviticus 25)\n\nUpon the death of George Washington in 1799, two thirds of his eulogies referred to him as \"America's Moses,\" with one orator saying that \"Washington has been the same to us as Moses was to the Children of Israel.\"\n\nBenjamin Franklin, in 1788, saw the difficulties that some of the newly independent American states were having in forming a government, and proposed that until a new code of laws could be agreed to, they should be governed by \"the laws of Moses,\" as contained in the Old Testament. He justified his proposal by explaining that the laws had worked in biblical times: \"The Supreme Being… having rescued them from bondage by many miracles, performed by his servant Moses, he personally delivered to that chosen servant, in the presence of the whole nation, a constitution and code of laws for their observance.\n\nJohn Adams, 2nd President of the United States, stated why he relied on the laws of Moses over Greek philosophy for establishing the United States Constitution: \"As much as I love, esteem, and admire the Greeks, I believe the Hebrews have done more to enlighten and civilize the world. Moses did more than all their legislators and philosophers. Swedish historian Hugo Valentin credited Moses as the \"first to proclaim the rights of man.\"\n\nHistorian Gladys L. Knight describes how leaders who emerged during and after the period in which slavery in the United States was legal often personified the Moses symbol. \"The symbol of Moses was empowering in that it served to amplify a need for freedom.\" Therefore, when Abraham Lincoln was assassinated in 1865 after the passage of the amendment to the Constitution outlawing slavery, Black Americans said they had lost \"their Moses\". Lincoln biographer Charles Carleton Coffin writes, \"The millions whom Abraham Lincoln delivered from slavery will ever liken him to Moses, the deliverer of Israel.\" Similarly, Harriet Tubman, who rescued approximately seventy enslaved family and friends, was also described as the \"Moses\" of her people.\n\nIn the 1960s, a leading figure in the civil rights movement was Martin Luther King Jr., who was called \"a modern Moses,\" and often referred to Moses in his speeches: \"The struggle of Moses, the struggle of his devoted followers as they sought to get out of Egypt. This is something of the story of every people struggling for freedom.\"\n\nLiterature\n\nArt\nMichelangelo's statue of Moses in the Church of San Pietro in Vincoli, Rome, is one of the most familiar masterpieces in the world. The horns the sculptor included on Moses' head are the result of a mistranslation of the Hebrew Bible into the Latin Vulgate Bible with which Michelangelo was familiar. The Hebrew word taken from \"Exodus\" means either a \"horn\" or an \"irradiation.\" Experts at the Archaeological Institute of America show that the term was used when Moses \"returned to his people after seeing as much of the Glory of the Lord as human eye could stand,\" and his face \"reflected radiance.\" In early Jewish art, moreover, Moses is often \"shown with rays coming out of his head.\"\n\nAnother author explains, \"When Saint Jerome translated the Old Testament into Latin, he thought no one but Christ should glow with rays of light – so he advanced the secondary translation. However, writer J. Stephen Lang points out that Jerome's version actually described Moses as \"giving off hornlike rays,\" and he \"rather clumsily translated it to mean 'having horns.'\" It has also been noted that he had Moses seated on a throne, yet Moses was never given the title of a King nor ever sat on such thrones.\n\nMoses is depicted in several U.S. government buildings because of his legacy as a lawgiver. In the Library of Congress stands a large statue of Moses alongside a statue of the Paul the Apostle. Moses is one of the 23 lawgivers depicted in marble bas-reliefs in the chamber of the U.S. House of Representatives in the United States Capitol. The plaque's overview states: \"Moses (c. 1350–1250 B.C.) Hebrew prophet and lawgiver; transformed a wandering people into a nation; received the Ten Commandments.\"\n\nThe other twenty-two figures have their profiles turned to Moses, which is the only forward-facing bas-relief.\n\nMoses appears eight times in carvings that ring the Supreme Court Great Hall ceiling. His face is presented along with other ancient figures such as Solomon, the Greek god Zeus and the Roman goddess of wisdom, Minerva. The Supreme Court Building's east pediment depicts Moses holding two tablets. Tablets representing the Ten Commandments can be found carved in the oak courtroom doors, on the support frame of the courtroom's bronze gates and in the library woodwork. A controversial image is one that sits directly above the Chief Justice of the United States' head. In the center of the 40-foot-long Spanish marble carving is a tablet displaying Roman numerals I through X, with some numbers partially hidden.\n\nFilm and television\n\nThomas Paine and Numbers 31:13–18<br>\nIn the late eighteenth century, the deist Thomas Paine commented at length on Moses' Laws in \"The Age of Reason\" (1794, 1795, and 1807). Paine considered Moses to be a \"detestable villain\", and cited as an example of his \"unexampled atrocities\". In the passage, the Jewish army had returned from conquering the Midianites, and Moses has gone down to meet it:\n\nThe prominent atheist Richard Dawkins also made reference to these verses in his 2006 book, \"The God Delusion\", concluding that Moses was \"not a great role model for modern moralists\".\n\nRabbi Joel Grossman argued that the story is a \"powerful fable of lust and betrayal\", and that Moses' execution of the women was a symbolic condemnation of those who seek to turn sex and desire to evil purposes. Alan Levin, an educational specialist with the Reform movement, has similarly suggested that the story should be taken as a cautionary tale, to \"warn successive generations of Jews to watch their own idolatrous behavior\".\n\nHowever, some Jewish sources defend Moses' role. The Chasam Sofer emphasizes that this war was not fought at Moses' behest, but was commanded by God as an act of revenge against the Midianite women, who, according to the Biblical account, had seduced the Israelites and led them to sin. In \"Legend of the Jews\", Phinehas son of Eleazar defend their innocent action in leaving the women remain alive because Moses instructed them to take revenge \"only to the Midianites,\" without mentioning \"Midianite women.\" As God had also commanded them to be a holy nation, the \"polluted\" or unvirgin women should not be preferred among sons of Israel, therefore the \"pure\" or virgin women are more sacred for themselves.\n\n\nInformational notes\nCitations\nFurther reading\n\n"}
{"id": "19579", "url": "https://en.wikipedia.org/wiki?curid=19579", "title": "Mississippi River", "text": "Mississippi River\n\nThe Mississippi River is the largest river of the United States and the chief river of the second-largest drainage system on the North American continent, second only to the Hudson Bay drainage system. Its source is Lake Itasca in northern Minnesota and it flows generally south for to the Mississippi River Delta in the Gulf of Mexico. With its many tributaries, the Mississippi's watershed drains all or parts of 32 U.S. states and two Canadian provinces between the Rocky and Appalachian Mountains. The main stem is entirely within the United States; the total drainage basin is , of which only about one percent is in Canada. The Mississippi ranks as the fourth-longest and fifteenth-largest river by discharge in the world. The river either borders or passes through the states of Minnesota, Wisconsin, Iowa, Illinois, Missouri, Kentucky, Tennessee, Arkansas, Mississippi, and Louisiana.\n\nNative Americans have lived along the Mississippi River and its tributaries for thousands of years. Most were hunter-gatherers, but some, such as the Mound Builders, formed prolific agricultural societies. The arrival of Europeans in the 16th century changed the native way of life as first explorers, then settlers, ventured into the basin in increasing numbers. The river served first as a barrier, forming borders for New Spain, New France, and the early United States, and then as a vital transportation artery and communications link. In the 19th century, during the height of the ideology of manifest destiny, the Mississippi and several western tributaries, most notably the Missouri, formed pathways for the western expansion of the United States.\n\nFormed from thick layers of the river's silt deposits, the Mississippi embayment is one of the most fertile regions of the United States; steamboats were widely used in the 19th and early 20th centuries to ship agricultural and industrial goods. During the American Civil War, the Mississippi's capture by Union forces marked a turning point towards victory, due to the river's strategic importance to the Confederate war effort. Because of substantial growth of cities and the larger ships and barges that replaced steamboats, the first decades of the 20th century saw the construction of massive engineering works such as levees, locks and dams, often built in combination. A major focus of this work has been to prevent the lower Mississippi from shifting into the channel of the Atchafalaya River and bypassing New Orleans.\n\nSince the 20th century, the Mississippi River has also experienced major pollution and environmental problems – most notably elevated nutrient and chemical levels from agricultural runoff, the primary contributor to the Gulf of Mexico dead zone.\n\nThe word Mississippi itself comes from \"Misi zipi\", the French rendering of the Anishinaabe (Ojibwe or Algonquin) name for the river, \"Misi-ziibi\" (Great River).\n\nIn the 18th century, the river was the primary western boundary of the young United States, and since the country's expansion westward, the Mississippi River has been widely considered a convenient if approximate dividing line between the Eastern, Southern, and Midwestern United States, and the Western United States. This is exemplified by the Gateway Arch in St. Louis and the phrase \"Trans-Mississippi\" as used in the name of the Trans-Mississippi Exposition.\n\nIt is common to qualify a regionally superlative landmark in relation to it, such as \"the highest peak east of the Mississippi\" or \"the oldest city west of the Mississippi\". The FCC also uses it as the dividing line for broadcast call-signs, which begin with W to the east and K to the west, mixing together in media markets along the river.\n\nThe geographical setting of the Mississippi River includes considerations of the course of the river itself, its watershed, its outflow, its prehistoric and historic course changes, and possibilities of future course changes. The New Madrid Seismic Zone along the river is also noteworthy. These various basic geographical aspects of the river in turn underlie its human history and present uses of the waterway and its adjacent lands.\n\nThe Mississippi River can be divided into three sections: the Upper Mississippi, the river from its headwaters to the confluence with the Missouri River; the Middle Mississippi, which is downriver from the Missouri to the Ohio River; and the Lower Mississippi, which flows from the Ohio to the Gulf of Mexico.\n\nThe Upper Mississippi runs from its headwaters to its confluence with the Missouri River at St. Louis, Missouri. It is divided into two sections:\n\nThe source of the Upper Mississippi branch is traditionally accepted as Lake Itasca, above sea level in Itasca State Park in Clearwater County, Minnesota. The name \"Itasca\" was chosen to designate the \"true head\" of the Mississippi River as a combination of the last four letters of the Latin word for truth (\"veritas\") and the first two letters of the Latin word for head (\"caput\"). However, the lake is in turn fed by a number of smaller streams.\n\nFrom its origin at Lake Itasca to St. Louis, Missouri, the waterway's flow is moderated by 43 dams. Fourteen of these dams are located above Minneapolis in the headwaters region and serve multiple purposes, including power generation and recreation. The remaining 29 dams, beginning in downtown Minneapolis, all contain locks and were constructed to improve commercial navigation of the upper river. Taken as a whole, these 43 dams significantly shape the geography and influence the ecology of the upper river. Beginning just below Saint Paul, Minnesota, and continuing throughout the upper and lower river, the Mississippi is further controlled by thousands of wing dikes that moderate the river's flow in order to maintain an open navigation channel and prevent the river from eroding its banks.\n\nThe head of navigation on the Mississippi is the Coon Rapids Dam in Coon Rapids, Minnesota. Before it was built in 1913, steamboats could occasionally go upstream as far as Saint Cloud, Minnesota, depending on river conditions.\n\nThe uppermost lock and dam on the Upper Mississippi River is the Upper St. Anthony Falls Lock and Dam in Minneapolis. Above the dam, the river's elevation is . Below the dam, the river's elevation is . This drop is the largest of all the Mississippi River locks and dams. The origin of the dramatic drop is a waterfall preserved adjacent to the lock under an apron of concrete. Saint Anthony Falls is the only true waterfall on the entire Mississippi River. The water elevation continues to drop steeply as it passes through the gorge carved by the waterfall.\n\nAfter the completion of the St. Anthony Falls Lock and Dam in 1963, the river's head of navigation moved upstream, to the Coon Rapids Dam. However, the Locks were closed in 2015 to control the spread of invasive Asian carp, making Minneapolis once again the site of the head of navigation of the river.\n\nThe Upper Mississippi has a number of natural and artificial lakes, with its widest point being Lake Winnibigoshish, near Grand Rapids, Minnesota, over across. Lake Onalaska, created by Lock and Dam No. 7, near La Crosse, Wisconsin, is more than wide. Lake Pepin, a natural lake formed behind the delta of the Chippewa River of Wisconsin as it enters the Upper Mississippi, is more than wide.\n\nBy the time the Upper Mississippi reaches Saint Paul, Minnesota, below Lock and Dam No. 1, it has dropped more than half its original elevation and is above sea level. From St. Paul to St. Louis, Missouri, the river elevation falls much more slowly, and is controlled and managed as a series of pools created by 26 locks and dams.\n\nThe Upper Mississippi River is joined by the Minnesota River at Fort Snelling in the Twin Cities; the St. Croix River near Prescott, Wisconsin; the Cannon River near Red Wing, Minnesota; the Zumbro River at Wabasha, Minnesota; the Black, La Crosse, and Root rivers in La Crosse, Wisconsin; the Wisconsin River at Prairie du Chien, Wisconsin; the Rock River at the Quad Cities; the Iowa River near Wapello, Iowa; the Skunk River south of Burlington, Iowa; and the Des Moines River at Keokuk, Iowa. Other major tributaries of the Upper Mississippi include the Crow River in Minnesota, the Chippewa River in Wisconsin, the Maquoketa River and the Wapsipinicon River in Iowa, and the Illinois River in Illinois.\n\nThe Upper Mississippi is largely a multi-thread stream with many bars and islands. From its confluence with the St. Croix River downstream to Dubuque, Iowa, the river is entrenched, with high bedrock bluffs lying on either side. The height of these bluffs decreases to the south of Dubuque, though they are still significant through Savanna, Illinois. This topography contrasts strongly with the Lower Mississippi, which is a meandering river in a broad, flat area, only rarely flowing alongside a bluff (as at Vicksburg, Mississippi).\n\nThe Mississippi River is known as the Middle Mississippi from the Upper Mississippi River's confluence with the Missouri River at St. Louis, Missouri, for to its confluence with the Ohio River at Cairo, Illinois.\n\nThe Middle Mississippi is relatively free-flowing. From St. Louis to the Ohio River confluence, the Middle Mississippi falls over for an average rate of . At its confluence with the Ohio River, the Middle Mississippi is above sea level. Apart from the Missouri and Meramec rivers of Missouri and the Kaskaskia River of Illinois, no major tributaries enter the Middle Mississippi River.\n\nThe Mississippi River is called the Lower Mississippi River from its confluence with the Ohio River to its mouth at the Gulf of Mexico, a distance of about . At the confluence of the Ohio and the Middle Mississippi, the long-term mean discharge of the Ohio at Cairo, Illinois is , while the long-term mean discharge of the Mississippi at Thebes, Illinois (just upriver from Cairo) is . Thus, by volume, the main branch of the Mississippi River system at Cairo can be considered to be the Ohio River (and the Allegheny River further upstream), rather than the Middle Mississippi.\n\nIn addition to the Ohio River, the major tributaries of the Lower Mississippi River are the White River, flowing in at the White River National Wildlife Refuge in east central Arkansas; the Arkansas River, joining the Mississippi at Arkansas Post; the Big Black River in Mississippi; and the Yazoo River, meeting the Mississippi at Vicksburg, Mississippi. The widest point of the Mississippi River is in the Lower Mississippi portion where it exceeds in width in several places.\n\nDeliberate water diversion at the Old River Control Structure in Louisiana allows the Atchafalaya River in Louisiana to be a major distributary of the Mississippi River, with 30% of the Mississippi flowing to the Gulf of Mexico by this route, rather than continuing down the Mississippi's current channel past Baton Rouge and New Orleans on a longer route to the Gulf. Although the Red River is commonly thought to be a tributary, it is actually not, because its water flows separately into the Gulf of Mexico through the Atchafalaya River.\n\nThe Mississippi River has the world's fourth-largest drainage basin (\"watershed\" or \"catchment\"). The basin covers more than , including all or parts of 32 U.S. states and two Canadian provinces. The drainage basin empties into the Gulf of Mexico, part of the Atlantic Ocean. The total catchment of the Mississippi River covers nearly 40% of the landmass of the continental United States. The highest point within the watershed is also the highest point of the Rocky Mountains, Mount Elbert at .\n\nIn the United States, the Mississippi River drains the majority of the area between the crest of the Rocky Mountains and the crest of the Appalachian Mountains, except for various regions drained to Hudson Bay by the Red River of the North; to the Atlantic Ocean by the Great Lakes and the Saint Lawrence River; and to the Gulf of Mexico by the Rio Grande, the Alabama and Tombigbee rivers, the Chattahoochee and Appalachicola rivers, and various smaller coastal waterways along the Gulf.\n\nThe Mississippi River empties into the Gulf of Mexico about downstream from New Orleans. Measurements of the length of the Mississippi from Lake Itasca to the Gulf of Mexico vary somewhat, but the United States Geological Survey's number is . The retention time from Lake Itasca to the Gulf is typically about 90 days.\n\nThe Mississippi River discharges at an annual average rate of between 200 and 700 thousand cubic feet per second (7,000–20,000 m/s). Although it is the fifth-largest river in the world by volume, this flow is a small fraction of the output of the Amazon, which moves nearly 7 million cubic feet per second (200,000 m/s) during wet seasons. On average, the Mississippi has only 8% the flow of the Amazon River.\n\nFresh river water flowing from the Mississippi into the Gulf of Mexico does not mix into the salt water immediately. The images from NASA's MODIS (to the right) show a large plume of fresh water, which appears as a dark ribbon against the lighter-blue surrounding waters. These images demonstrate that the plume did not mix with the surrounding sea water immediately. Instead, it stayed intact as it flowed through the Gulf of Mexico, into the Straits of Florida, and entered the Gulf Stream. The Mississippi River water rounded the tip of Florida and traveled up the southeast coast to the latitude of Georgia before finally mixing in so thoroughly with the ocean that it could no longer be detected by MODIS.\n\nBefore 1900, the Mississippi River transported an estimated 400 million metric tons of sediment per year from the interior of the United States to coastal Louisiana and the Gulf of Mexico. During the last two decades, this number was only 145 million metric tons per year. The reduction in sediment transported down the Mississippi River is the result of engineering modification of the Mississippi, Missouri, and Ohio rivers and their tributaries by dams, meander cutoffs, river-training structures, and bank revetments and soil erosion control programs in the areas drained by them.\n\nOver geologic time, the Mississippi River has experienced numerous large and small changes to its main course, as well as additions, deletions, and other changes among its numerous tributaries, and the lower Mississippi River has used different pathways as its main channel to the Gulf of Mexico across the delta region.\n\nThrough a natural process known as avulsion or delta switching, the lower Mississippi River has shifted its final course to the mouth of the Gulf of Mexico every thousand years or so. This occurs because the deposits of silt and sediment begin to clog its channel, raising the river's level and causing it to eventually find a steeper, more direct route to the Gulf of Mexico. The abandoned distributaries diminish in volume and form what are known as bayous. This process has, over the past 5,000 years, caused the coastline of south Louisiana to advance toward the Gulf from . The currently active delta lobe is called the Birdfoot Delta, after its shape, or the Balize Delta, after La Balize, Louisiana, the first French settlement at the mouth of the Mississippi.\n\nThe current form of the Mississippi River basin was largely shaped by the Laurentide Ice Sheet of the most recent Ice Age. The southernmost extent of this enormous glaciation extended well into the present-day United States and Mississippi basin. When the ice sheet began to recede, hundreds of feet of rich sediment were deposited, creating the flat and fertile landscape of the Mississippi Valley. During the melt, giant glacial rivers found drainage paths into the Mississippi watershed, creating such features as the Minnesota River, James River, and Milk River valleys. When the ice sheet completely retreated, many of these \"temporary\" rivers found paths to Hudson Bay or the Arctic Ocean, leaving the Mississippi Basin with many features \"over-sized\" for the existing rivers to have carved in the same time period.\n\nIce sheets during the Illinoian Stage, about 300,000 to 132,000 years before present, blocked the Mississippi near Rock Island, Illinois, diverting it to its present channel farther to the west, the current western border of Illinois. The Hennepin Canal roughly follows the ancient channel of the Mississippi downstream from Rock Island to Hennepin, Illinois. South of Hennepin, to Alton, Illinois, the current Illinois River follows the ancient channel used by the Mississippi River before the Illinoian Stage.\n\nTimeline of outflow course changes\n\nIn March 1876, the Mississippi suddenly changed course near the settlement of Reverie, Tennessee, leaving a small part of Tipton County, Tennessee, attached to Arkansas and separated from the rest of Tennessee by the new river channel. Since this event was an avulsion, rather than the effect of incremental erosion and deposition, the state line still follows the old channel.\n\nThe town of Kaskaskia, Illinois once stood on a peninsula at the confluence of the Mississippi and Kaskaskia (Okaw) Rivers. Founded as a French colonial community, it later became the capital of the Illinois Territory and was the first state capital of Illinois until 1819. Beginning in 1844, successive flooding caused the Mississippi River to slowly encroach east. A major flood in 1881 caused it to overtake the lower 10 miles of the Kaskaskia River, forming a new Mississippi channel and cutting off the town from the rest of the state. Later flooding destroyed most of the remaining town, including the original State House. Today, the remaining 2,300 acre island and community of 14 residents is known as an enclave of Illinois and is accessible only from the Missouri side.\n\nThe New Madrid Seismic Zone, along the Mississippi River near New Madrid, Missouri, between Memphis and St. Louis, is related to an aulacogen (failed rift) that formed at the same time as the Gulf of Mexico. This area is still quite active seismically. Four great earthquakes in 1811 and 1812, estimated at approximately 8 on the Richter magnitude scale, had tremendous local effects in the then sparsely settled area, and were felt in many other places in the Midwestern and eastern U.S. These earthquakes created Reelfoot Lake in Tennessee from the altered landscape near the river.\n\nWhen measured from its traditional source at Lake Itasca, the Mississippi has a length of . When measured from its longest stream source (most distant source from the sea), Brower's Spring in Montana, the source of the Missouri River, it has a length of , making it the fourth longest river in the world after the Nile, Amazon, and Yangtze. When measured by the largest stream source (by water volume), the Ohio River, by extension the Allegheny River, would be the source, and the Mississippi would begin in Pennsylvania.\n\nAt its source at Lake Itasca, the Mississippi River is about 3 feet deep. The average depth of the Mississippi River between Saint Paul and Saint Louis is between deep, the deepest part being Lake Pepin, which averages deep and has a maximum depth of . Between Saint Lois, Missouri, where the Missouri River joins and Cairo, Illinois, the depth averages . Below Cairo, where the Ohio River joins, the depth averages deep. The deepest part of the river is in New Orleans, where it reaches deep.\n\nThe Mississippi River runs through or along 10 states, from Minnesota to Louisiana, and is used to define portions of these states borders, with Wisconsin, Illinois, Kentucky, Tennessee, and Mississippi along the east side of the river, and Iowa, Missouri, and Arkansas along its west side. Substantial parts of both Minnesota and Louisiana are on either side of the river, although the Mississippi defines part of the boundary of each of these states.\n\nIn all of these cases, the middle of the riverbed at the time the borders were established was used as the line to define the borders between adjacent states. In various areas, the river has since shifted, but the state borders have not changed, still following the former bed of the Mississippi River as of their establishment, leaving several small isolated areas of one state across the new river channel, contiguous with the adjacent state. Also, due to a meander in the river, a small part of western Kentucky is contiguous with Tennessee, but isolated from the rest of its state.\n\nMany of the communities along the Mississippi River are listed below; most have either historic significance or cultural lore connecting them to the river. They are sequenced from the source of the river to its end.\n\nThe road crossing highest on the Upper Mississippi is a simple steel culvert, through which the river (locally named \"Nicolet Creek\") flows north from Lake Nicolet under \"Wilderness Road\" to the West Arm of Lake Itasca, within Itasca State Park.\n\nThe earliest bridge across the Mississippi River was built in 1855. It spanned the river in Minneapolis where the current Hennepin Avenue Bridge is located. No highway or railroad tunnels cross under the Mississippi River.\n\nThe first railroad bridge across the Mississippi was built in 1856. It spanned the river between the Rock Island Arsenal in Illinois and Davenport, Iowa. Steamboat captains of the day, fearful of competition from the railroads, considered the new bridge a hazard to navigation. Two weeks after the bridge opened, the steamboat \"Effie Afton\" rammed part of the bridge, setting it on fire. Legal proceedings ensued, with Abraham Lincoln defending the railroad. The lawsuit went to the Supreme Court of the United States, which ruled in favor of the railroad.\n\nBelow is a general overview of selected Mississippi bridges which have notable engineering or landmark significance, with their cities or locations. They are sequenced from the Upper Mississippi's source to the Lower Mississippi's mouth.\n\n\n\n\n\nA clear channel is needed for the barges and other vessels that make the main stem Mississippi one of the great commercial waterways of the world. The task of maintaining a navigation channel is the responsibility of the United States Army Corps of Engineers, which was established in 1802. Earlier projects began as early as 1829 to remove snags, close off secondary channels and excavate rocks and sandbars.\n\nSteamboats entered trade in the 1820s, so the period 1830–1850 became the golden age of steamboats. As there were few roads or rails in the lands of the Louisiana Purchase, river traffic was an ideal solution. Cotton, timber and food came down the river, as did Appalachian coal. The port of New Orleans boomed as it was the trans-shipment point to deep sea ocean vessels. As a result, the image of the twin stacked, wedding cake Mississippi steamer entered into American mythology. Steamers worked the entire route from the trickles of Montana, to the Ohio River; down the Missouri and Tennessee, to the main channel of the Mississippi. Only with the arrival of the railroads in the 1880s did steamboat traffic diminish. Steamboats remained a feature until the 1920s. Most have been superseded by pusher tugs. A few survive as icons—the Delta Queen and the River Queen for instance.\n\nA series of 29 locks and dams on the upper Mississippi, most of which were built in the 1930s, is designed primarily to maintain a channel for commercial barge traffic. The lakes formed are also used for recreational boating and fishing. The dams make the river deeper and wider but do not stop it. No flood control is intended. During periods of high flow, the gates, some of which are submersible, are completely opened and the dams simply cease to function. Below St. Louis, the Mississippi is relatively free-flowing, although it is constrained by numerous levees and directed by numerous wing dams.\n\nOn the lower Mississippi, from Baton Rouge to the mouth of the Mississippi, the navigation depth is , allowing container ships and cruise ships to dock at the Port of New Orleans and bulk cargo ships shorter than air draft that fit under the Huey P. Long Bridge to traverse the Mississippi to Baton Rouge. There is a feasibility study to dredge this portion of the river to to allow New Panamax ship depths.\n\nIn 1829, there were surveys of the two major obstacles on the upper Mississippi, the Des Moines Rapids and the Rock Island Rapids, where the river was shallow and the riverbed was rock. The Des Moines Rapids were about long and just above the mouth of the Des Moines River at Keokuk, Iowa. The Rock Island Rapids were between Rock Island and Moline, Illinois. Both rapids were considered virtually impassable.\n\nIn 1848, the Illinois and Michigan Canal was built to connect the Mississippi River to Lake Michigan via the Illinois River near Peru, Illinois. The canal allowed shipping between these important waterways. In 1900, the canal was replaced by the Chicago Sanitary and Ship Canal. The second canal, in addition to shipping, also allowed Chicago to address specific health issues (typhoid fever, cholera and other waterborne diseases) by sending its waste down the Illinois and Mississippi river systems rather than polluting its water source of Lake Michigan.\n\nThe Corps of Engineers recommended the excavation of a channel at the Des Moines Rapids, but work did not begin until after Lieutenant Robert E. Lee endorsed the project in 1837. The Corps later also began excavating the Rock Island Rapids. By 1866, it had become evident that excavation was impractical, and it was decided to build a canal around the Des Moines Rapids. The canal opened in 1877, but the Rock Island Rapids remained an obstacle. In 1878, Congress authorized the Corps to establish a channel to be obtained by building wing dams which direct the river to a narrow channel causing it to cut a deeper channel, by closing secondary channels and by dredging. The channel project was complete when the Moline Lock, which bypassed the Rock Island Rapids, opened in 1907.\n\nTo improve navigation between St. Paul, Minnesota, and Prairie du Chien, Wisconsin, the Corps constructed several dams on lakes in the headwaters area, including Lake Winnibigoshish and Lake Pokegama. The dams, which were built beginning in the 1880s, stored spring run-off which was released during low water to help maintain channel depth.\n\nIn 1907, Congress authorized a channel project on the Mississippi River, which was not complete when it was abandoned in the late 1920s in favor of the channel project.\n\nIn 1913, construction was complete on Lock and Dam No. 19 at Keokuk, Iowa, the first dam below St. Anthony Falls. Built by a private power company (Union Electric Company of St. Louis) to generate electricity (originally for streetcars in St. Louis), the Keokuk dam was one of the largest hydro-electric plants in the world at the time. The dam also eliminated the Des Moines Rapids. Lock and Dam No. 1 was completed in Minneapolis, Minnesota in 1917. Lock and Dam No. 2, near Hastings, Minnesota, was completed in 1930.\n\nBefore the Great Mississippi Flood of 1927, the Corps's primary strategy was to close off as many side channels as possible to increase the flow in the main river. It was thought that the river's velocity would scour off bottom sediments, deepening the river and decreasing the possibility of flooding. The 1927 flood proved this to be so wrong that communities threatened by the flood began to create their own levee breaks to relieve the force of the rising river.\n\nThe Rivers and Harbors Act of 1930 authorized the channel project, which called for a navigation channel feet deep and wide to accommodate multiple-barge tows. This was achieved by a series of locks and dams, and by dredging. Twenty-three new locks and dams were built on the upper Mississippi in the 1930s in addition to the three already in existence.\n\nUntil the 1950s, there was no dam below Lock and Dam 26 at Alton, Illinois. Chain of Rocks Lock (Lock and Dam No. 27), which consists of a low-water dam and an canal, was added in 1953, just below the confluence with the Missouri River, primarily to bypass a series of rock ledges at St. Louis. It also serves to protect the St. Louis city water intakes during times of low water.\n\nU.S. government scientists determined in the 1950s that the Mississippi River was starting to switch to the Atchafalaya River channel because of its much steeper path to the Gulf of Mexico. Eventually the Atchafalaya River would capture the Mississippi River and become its main channel to the Gulf of Mexico, leaving New Orleans on a side channel. As a result, the U.S. Congress authorized a project called the Old River Control Structure, which has prevented the Mississippi River from leaving its current channel that drains into the Gulf via New Orleans.\n\nBecause the large scale of high-energy water flow threatened to damage the structure, an auxiliary flow control station was built adjacent to the standing control station. This $300 million project was completed in 1986 by the Corps of Engineers. Beginning in the 1970s, the Corps applied hydrological transport models to analyze flood flow and water quality of the Mississippi. Dam 26 at Alton, Illinois, which had structural problems, was replaced by the Mel Price Lock and Dam in 1990. The original Lock and Dam 26 was demolished.\n\nThe Corps now actively creates and maintains spillways and floodways to divert periodic water surges into backwater channels and lakes, as well as route part of the Mississippi's flow into the Atchafalaya Basin and from there to the Gulf of Mexico, bypassing Baton Rouge and New Orleans. The main structures are the Birds Point-New Madrid Floodway in Missouri; the Old River Control Structure and the Morganza Spillway in Louisiana, which direct excess water down the west and east sides (respectively) of the Atchafalaya River; and the Bonnet Carré Spillway, also in Louisiana, which directs floodwaters to Lake Pontchartrain (see diagram). Some experts blame urban sprawl for increases in both the risk and frequency of flooding on the Mississippi River.\n\nSome of the pre-1927 strategy is still in use today, with the Corps actively cutting the necks of horseshoe bends, allowing the water to move faster and reducing flood heights.\n\nThe area of the Mississippi River basin was first settled by hunting and gathering Native American peoples and is considered one of the few independent centers of plant domestication in human history. Evidence of early cultivation of sunflower, a goosefoot, a marsh elder and an indigenous squash dates to the 4th millennium BC. The lifestyle gradually became more settled after around 1000 BC during what is now called the Woodland period, with increasing evidence of shelter construction, pottery, weaving and other practices. A network of trade routes referred to as the Hopewell interaction sphere was active along the waterways between about 200 and 500 AD, spreading common cultural practices over the entire area between the Gulf of Mexico and the Great Lakes. A period of more isolated communities followed, and agriculture introduced from Mesoamerica based on the Three Sisters (maize, beans and squash) gradually came to dominate. After around 800 AD there arose an advanced agricultural society today referred to as the Mississippian culture, with evidence of highly stratified complex chiefdoms and large population centers. The most prominent of these, now called Cahokia, was occupied between about 600 and 1400 AD and at its peak numbered between 8,000 and 40,000 inhabitants, larger than London, England of that time. At the time of first contact with Europeans, Cahokia and many other Mississippian cities had dispersed, and archaeological finds attest to increased social stress.\n\nModern American Indian nations inhabiting the Mississippi basin include Cheyenne, Sioux, Ojibwe, Potawatomi, Ho-Chunk, Fox, Kickapoo, Tamaroa, Moingwena, Quapaw and Chickasaw.\n\nThe word \"Mississippi\" itself comes from \"Messipi\", the French rendering of the Anishinaabe (Ojibwe or Algonquin) name for the river, \"Misi-ziibi\" (Great River). The Ojibwe called Lake Itasca \"Omashkoozo-zaaga'igan\" (Elk Lake) and the river flowing out of it \"Omashkoozo-ziibi\" (Elk River). After flowing into Lake Bemidji, the Ojibwe called the river \"Bemijigamaag-ziibi\" (River from the Traversing Lake). After flowing into Cass Lake, the name of the river changes to \"Gaa-miskwaawaakokaag-ziibi\" (Red Cedar River) and then out of Lake Winnibigoshish as \"Wiinibiigoonzhish-ziibi\" (Miserable Wretched Dirty Water River), \"Gichi-ziibi\" (Big River) after the confluence with the Leech Lake River, then finally as \"Misi-ziibi\" (Great River) after the confluence with the Crow Wing River. After the expeditions by Giacomo Beltrami and Henry Schoolcraft, the longest stream above the juncture of the Crow Wing River and \"Gichi-ziibi\" was named \"Mississippi River\". The Mississippi River Band of Chippewa Indians, known as the \"Gichi-ziibiwininiwag\", are named after the stretch of the Mississippi River known as the \"Gichi-ziibi\". The Cheyenne, one of the earliest inhabitants of the upper Mississippi River, called it the \"Máʼxe-éʼometaaʼe\" (Big Greasy River) in the Cheyenne language. The Arapaho name for the river is \"Beesniicíe\". The Pawnee name is \"Kickaátit\".\n\nThe Mississippi was spelled during French Louisiana and was also known as the Rivière Saint-Louis.\n\nOn May 8, 1541, Spanish explorer Hernando de Soto became the first recorded European to reach the Mississippi River, which he called \"Río del Espíritu Santo\" (\"River of the Holy Spirit\"), in the area of what is now Mississippi. In Spanish, the river is called \"Río Mississippi\".\n\nFrench explorers Louis Jolliet and Jacques Marquette began exploring the Mississippi in the 17th century. Marquette traveled with a Sioux Indian who named it \"Ne Tongo\" (\"Big river\" in Sioux language) in 1673. Marquette proposed calling it the \"River of the Immaculate Conception\".\n\nWhen Louis Jolliet explored the Mississippi Valley in the 17th century, natives guided him to a quicker way to return to French Canada via the Illinois River. When he found the Chicago Portage, he remarked that a canal of \"only half a league\" (less than 2 miles (3.2 km), 3 km) would join the Mississippi and the Great Lakes. In 1848, the continental divide separating the waters of the Great Lakes and the Mississippi Valley was breached by the Illinois and Michigan canal via the Chicago River. This both accelerated the development, and forever changed the ecology of the Mississippi Valley and the Great Lakes.\n\nIn 1682, René-Robert Cavelier, Sieur de La Salle and Henri de Tonti claimed the entire Mississippi River Valley for France, calling the river \"Colbert River\" after Jean-Baptiste Colbert and the region \"La Louisiane\", for King Louis XIV. On March 2, 1699, Pierre Le Moyne d'Iberville rediscovered the mouth of the Mississippi, following the death of La Salle. The French built the small fort of La Balise there to control passage.\n\nIn 1718, about upriver, New Orleans was established along the river crescent by Jean-Baptiste Le Moyne, Sieur de Bienville, with construction patterned after the 1711 resettlement on Mobile Bay of Mobile, the capital of French Louisiana at the time.\n\nFollowing Britain's victory in the Seven Years War the Mississippi became the border between the British and Spanish Empires. The Treaty of Paris (1763) gave Great Britain rights to all land east of the Mississippi and Spain rights to land west of the Mississippi. Spain also ceded Florida to Britain to regain Cuba, which the British occupied during the war. Britain then divided the territory into East and West Florida.\n\nArticle 8 of the Treaty of Paris (1783) states, \"The navigation of the river Mississippi, from its source to the ocean, shall forever remain free and open to the subjects of Great Britain and the citizens of the United States\". With this treaty, which ended the American Revolutionary War, Britain also ceded West Florida back to Spain to regain the Bahamas, which Spain had occupied during the war. In 1800, under duress from Napoleon of France, Spain ceded an undefined portion of West Florida to France. When France then sold the Louisiana Territory to the U.S. in 1803, a dispute arose again between Spain and the U.S. on which parts of West Florida exactly had Spain ceded to France, which would in turn decide which parts of West Florida were now U.S. property versus Spanish property. These aspirations ended when Spain was pressured into signing Pinckney's Treaty in 1795.\n\nFrance reacquired 'Louisiana' from Spain in the secret Treaty of San Ildefonso in 1800. The United States then secured effective control of the river when it bought the Louisiana Territory from France in the Louisiana Purchase of 1803. The last serious European challenge to U.S. control of the river came at the conclusion of War of 1812 when British forces mounted an attack on New Orleans – the attack was repulsed by an American army under the command of General Andrew Jackson.\n\nIn the Treaty of 1818, the U.S. and Great Britain agreed to fix the border running from the Lake of the Woods to the Rocky Mountains along the 49th parallel north. In effect, the U.S. ceded the northwestern extremity of the Mississippi basin to the British in exchange for the southern portion of the Red River basin.\n\nSo many settlers traveled westward through the Mississippi river basin, as well as settled in it, that Zadok Cramer wrote a guide book called \"The Navigator\", detailing the features and dangers and navigable waterways of the area. It was so popular that he updated and expanded it through 12 editions over a period of 25 years.\n\nThe colonization of the area was barely slowed by the three earthquakes in 1811 and 1812, estimated at approximately 8 on the Richter magnitude scale, that were centered near New Madrid, Missouri.\n\nMark Twain's book, \"Life on the Mississippi\", covered the steamboat commerce which took place from 1830 to 1870 on the river before more modern ships replaced the steamer. The book was published first in serial form in \"Harper's Weekly\" in seven parts in 1875. The full version, including a passage from the then unfinished \"Adventures of Huckleberry Finn\" and works from other authors, was published by James R. Osgood & Company in 1885.\n\nThe first steamboat to travel the full length of the Lower Mississippi from the Ohio River to New Orleans was the \"New Orleans\" in December 1811. Its maiden voyage occurred during the series of New Madrid earthquakes in 1811–12. The Upper Mississippi was treacherous, unpredictable and to make traveling worse, the area was not properly mapped out or surveyed. Until the 1840s only two trips a year to the Twin Cities landings were made by steamboats which suggests it was not very profitable.\n\nSteamboat transport remained a viable industry, both in terms of passengers and freight until the end of the first decade of the 20th century. Among the several Mississippi River system steamboat companies was the noted Anchor Line, which, from 1859 to 1898, operated a luxurious fleet of steamers between St. Louis and New Orleans.\n\nItalian explorer Giacomo Beltrami, wrote about his journey on the Virginia, which was the first steam boat to make it to Fort St. Anthony in Minnesota. He referred to his voyage as a promenade that was once a journey on the Mississippi. The steamboat era changed the economic and political life of the Mississippi, as well as the nature of travel itself. The Mississippi was completely changed by the steamboat era as it transformed into a flourishing tourists trade.\n\nControl of the river was a strategic objective of both sides in the American Civil War. In 1862 Union forces coming down the river successfully cleared Confederate defenses at Island Number 10 and Memphis, Tennessee, while Naval forces coming upriver from the Gulf of Mexico captured New Orleans, Louisiana. The remaining major Confederate stronghold was on the heights overlooking the river at Vicksburg, Mississippi, and the Union's Vicksburg Campaign (December 1862 to July 1863), and the fall of Port Hudson, completed control of the lower Mississippi River. The Union victory ending the Siege of Vicksburg on July 4, 1863, was pivotal to the Union's final victory of the Civil War.\n\nThe \"Big Freeze\" of 1918–19 blocked river traffic north of Memphis, Tennessee, preventing transportation of coal from southern Illinois. This resulted in widespread shortages, high prices, and rationing of coal in January and February.\n\nIn the spring of 1927, the river broke out of its banks in 145 places, during the Great Mississippi Flood of 1927 and inundated to a depth of up to .\n\nIn 1962 and 1963, industrial accidents spilled of soybean oil into the Mississippi and Minnesota rivers. The oil covered the Mississippi River from St. Paul to Lake Pepin, creating an ecological disaster and a demand to control water pollution.\n\nOn October 20, 1976, the automobile ferry, \"MV George Prince\", was struck by a ship traveling upstream as the ferry attempted to cross from Destrehan, Louisiana, to Luling, Louisiana. Seventy-eight passengers and crew died; only eighteen survived the accident.\n\nIn 1988, the water level of the Mississippi fell to below zero on the Memphis gauge. The remains of wooden-hulled water craft were exposed in an area of on the bottom of the Mississippi River at West Memphis, Arkansas. They dated to the late 19th to early 20th centuries. The State of Arkansas, the Arkansas Archeological Survey, and the Arkansas Archeological Society responded with a two-month data recovery effort. The fieldwork received national media attention as good news in the middle of a drought.\n\nThe Great Flood of 1993 was another significant flood, primarily affecting the Mississippi above its confluence with the Ohio River at Cairo, Illinois.\n\nTwo portions of the Mississippi were designated as American Heritage Rivers in 1997: the lower portion around Louisiana and Tennessee, and the upper portion around Iowa, Illinois, Minnesota and Missouri. The Nature Conservancy's project called \"America's Rivershed Initiative\" announced a 'report card' assessment of the entire basin in October 2015 and gave the grade of D+. The assessment noted the aging navigation and flood control infrastructure along with multiple environmental problems.\n\nIn 2002, Slovenian long-distance swimmer Martin Strel swam the entire length of the river, from Minnesota to Louisiana, over the course of 68 days. In 2005, the Source to Sea Expedition paddled the Mississippi and Atchafalaya Rivers to benefit the Audubon Society's Upper Mississippi River Campaign.\n\nGeologists believe that the lower Mississippi could take a new course to the Gulf. Either of two new routes—through the Atchafalaya Basin or through Lake Pontchartrain—might become the Mississippi's main channel if flood-control structures are overtopped or heavily damaged during a severe flood.\n\nFailure of the Old River Control Structure, the Morganza Spillway, or nearby levees would likely re-route the main channel of the Mississippi through Louisiana's Atchafalaya Basin and down the Atchafalaya River to reach the Gulf of Mexico south of Morgan City in southern Louisiana. This route provides a more direct path to the Gulf of Mexico than the present Mississippi River channel through Baton Rouge and New Orleans. While the risk of such a diversion is present during any major flood event, such a change has so far been prevented by active human intervention involving the construction, maintenance, and operation of various levees, spillways, and other control structures by the U.S. Army Corps of Engineers.\nThe Old River Control Structure, between the present Mississippi River channel and the Atchafalaya Basin, sits at the normal water elevation and is ordinarily used to divert 30% of the Mississippi's flow to the Atchafalaya River. There is a steep drop here away from the Mississippi's main channel into the Atchafalaya Basin. If this facility were to fail during a major flood, there is a strong concern the water would scour and erode the river bottom enough to capture the Mississippi's main channel. The structure was nearly lost during the 1973 flood, but repairs and improvements were made after engineers studied the forces at play. In particular, the Corps of Engineers made many improvements and constructed additional facilities for routing water through the vicinity. These additional facilities give the Corps much more flexibility and potential flow capacity than they had in 1973, which further reduces the risk of a catastrophic failure in this area during other major floods, such as that of 2011.\n\nBecause the Morganza Spillway is slightly higher and well back from the river, it is normally dry on both sides. Even if it failed at the crest during a severe flood, the flood waters would have to erode to normal water levels before the Mississippi could permanently jump channel at this location. During the 2011 floods, the Corps of Engineers opened the Morganza Spillway to 1/4 of its capacity to allow 150,000 ft/sec of water to flood the Morganza and Atchafalaya floodways and continue directly to the Gulf of Mexico, bypassing Baton Rouge and New Orleans. In addition to reducing the Mississippi River crest downstream, this diversion reduced the chances of a channel change by reducing stress on the other elements of the control system.\n\nSome geologists have noted that the possibility for course change into the Atchafalaya also exists in the area immediately north of the Old River Control Structure. Army Corps of Engineers geologist Fred Smith once stated, \"The Mississippi wants to go west. 1973 was a forty-year flood. The big one lies out there somewhere—when the structures can't release all the floodwaters and the levee is going to have to give way. That is when the river's going to jump its banks and try to break through.\"\n\nAnother possible course change for the Mississippi River is a diversion into Lake Pontchartrain near New Orleans. This route is controlled by the Bonnet Carré Spillway, built to reduce flooding in New Orleans. This spillway and an imperfect natural levee about 4–6 meters (12 to 20 feet) high are all that prevents the Mississippi from taking a new, shorter course through Lake Pontchartrain to the Gulf of Mexico. Diversion of the Mississippi's main channel through Lake Pontchartrain would have consequences similar to an Atchafalaya diversion, but to a lesser extent, since the present river channel would remain in use past Baton Rouge and into the New Orleans area.\n\nThe sport of water skiing was invented on the river in a wide region between Minnesota and Wisconsin known as Lake Pepin. Ralph Samuelson of Lake City, Minnesota, created and refined his skiing technique in late June and early July 1922. He later performed the first water ski jump in 1925 and was pulled along at by a Curtiss flying boat later that year.\n\nThere are seven National Park Service sites along the Mississippi River. The Mississippi National River and Recreation Area is the National Park Service site dedicated to protecting and interpreting the Mississippi River itself. The other six National Park Service sites along the river are (listed from north to south):\n\nThe Mississippi basin is home to a highly diverse aquatic fauna and has been called the \"mother fauna\" of North American fresh water.\n\nAbout 375 fish species are known from the Mississippi basin, far exceeding other North Hemisphere river basin exclusively within temperate/subtropical regions, except the Yangtze. Within the Mississippi basin, streams that have their source in the Appalachian and Ozark highlands contain especially many species. Among the fish species in the basin are numerous endemics, as well as relicts such as paddlefish, sturgeon, gar and bowfin.\n\nBecause of its size and high species diversity, the Mississippi basin is often divided into subregions. The Upper Mississippi River alone is home to about 120 fish species, including walleye, sauger, large mouth bass, small mouth bass, white bass, northern pike, bluegill, crappie, channel catfish, flathead catfish, common shiner, freshwater drum and shovelnose sturgeon.\n\nIn addition to fish, several species of turtles (such as snapping, musk, mud, map, cooter, painted and softshell turtles), American alligator, aquatic amphibians (such as hellbender, mudpuppy, three-toed amphiuma and lesser siren), and cambarid crayfish (such as the red swamp crayfish) are native to the Mississippi basin.\n\nNumerous introduced species are found in the Mississippi and some of these are invasive. Among the introductions are fish such as Asian carp, including the silver carp that have become infamous for outcompeting native fish and their potentially dangerous jumping behavior. They have spread throughout much of the basin, even approaching (but not yet invading) the Great Lakes. The Minnesota Department of Natural Resources has designated much of the Mississippi River in the state as infested waters by the exotic species zebra mussels and Eurasian watermilfoil.\n\n\n\n"}
{"id": "19581", "url": "https://en.wikipedia.org/wiki?curid=19581", "title": "Men in black", "text": "Men in black\n\nIn popular culture and UFO conspiracy theories, men in black (MIB) are supposed men dressed in black suits who claim to be quasi-government agents who harass or threaten UFO witnesses to keep them quiet about what they have seen. It is sometimes implied that they may be aliens themselves. The term is also frequently used to describe mysterious men working for unknown organizations, as well as various branches of government allegedly designed to protect secrets or perform other strange activities. The term is generic, used for any unusual, threatening or strangely behaved individual whose appearance on the scene can be linked in some fashion with a UFO sighting. Several alleged encounters with the men in black have been reported by UFO researchers and enthusiasts.\n\nStories about allegedly real-life men in black inspired the semi-comic science fiction \"Men in Black\" franchise of comic books, films, and other media.\n\nFolklorist James R. Lewis compares accounts of men in black with tales of people encountering Lucifer and speculates that they can be considered a kind of \"psychological drama\".\n\nMen in black figure prominently in ufology and UFO folklore. In the 1950s and 1960s, UFOlogists adopted a conspiratorial mindset and began to fear they would be subject to organized intimidation in retaliation for discovering \"the truth of the UFOs\".\n\nIn 1947, Harold Dahl claimed to have been warned not to talk about his alleged UFO sighting on Maury Island by a man in a dark suit. In the mid-1950s, the ufologist Albert K. Bender claimed he was visited by men in dark suits who threatened and warned him not to continue investigating UFOs. Bender maintained that the men in black were secret government agents who had been given the task of suppressing evidence of UFOs. The ufologist John Keel claimed to have had encounters with men in black and referred to them as \"demonic supernaturals\" with \"dark skin and/or ‘exotic’ facial features\". According to the ufologist Jerome Clark, reports of men in black represent \"experiences\" that \"don't seem to have occurred in the world of consensus reality\".\n\nHistorian Aaron Gulyas wrote, \"during the 1970s, 1980s, and 1990s, UFO conspiracy theorists would incorporate the Men in Black into their increasingly complex and paranoid visions\".\n\nIn his article, \"Gray Barker: My Friend, the Myth-Maker,\" John C. Sherwood claims that, in the late 1960s, at the age of 18, he cooperated when Gray Barker urged him to develop a hoax—which Barker subsequently published—about what Barker called \"blackmen\", three mysterious UFO inhabitants who silenced Sherwood's pseudonymous identity, \"Dr. Richard H. Pratt\".\n\n\n\n"}
{"id": "19582", "url": "https://en.wikipedia.org/wiki?curid=19582", "title": "May 7", "text": "May 7\n\n\n\n"}
{"id": "19583", "url": "https://en.wikipedia.org/wiki?curid=19583", "title": "Monomer", "text": "Monomer\n\nA monomer ( ; \"mono-\", \"one\" + \"-mer\", \"part\") is a molecule that \"can undergo polymerization thereby contributing constitutional units to the essential structure of a macromolecule\". Large numbers of monomers combine to form polymers in a process called polymerization.\n\nMonomers can be classified in many ways. They can be subdivided into two broad classes, depending on the kind of the polymer that they form. Monomers that participate in condensation polymerization have a different stoichiometry than monomers that participate in addition polymerization: \n\nOther classifications include:\n\nThe polymerization of one kind of monomer gives a homopolymer. Many polymers are copolymers, meaning that they are derived from two different monomers. In the case of condensation polymerizations, the ratio of comonomers is usually 1:1. For example, the formation of many nylons requires equal amounts of a dicarboxylic acid and diamine. In the case of addition polymerizations, the comonomer content is often only a few percent. For example, small amounts of 1-octene monomer are copolymerized with ethylene to give specialized polyethylene.\n\n\nThe term \"monomeric protein\" may also be used to describe one of the proteins making up a multiprotein complex. \n\nSome of the main biopolymers are listed below:\nFor \"proteins\", the monomers are amino acids. Polymerization occurs at ribosomes. Usually about 20 types of amino acid monomers are used to produce proteins. Hence proteins are not homopolymers.\n\nFor polynucleic acids (DNA/RNA), the monomers are nucleotides, each of which is made of a pentose sugar, a nitrogenous base and a phosphate group. Nucleotide monomers are found in the cell nucleus. Four types of nucleotide monomers are precursors to DNA and four different nucleotide monomers are precursors to RNA.\n\nFor carbohydrates, the monomers are monosaccharides. The most abundant natural monomer is glucose, which is linked by glycosidic bonds into the polymers cellulose, starch, and glycogen.\n\nIsoprene is a natural monomer that polymerizes to form natural rubber, most often \"cis-\"1,4-polyisoprene, but also \"trans-\"1,4-polymer. Synthetic rubbers are often based on butadiene, which is structurally related to isoprene.\n\n\n"}
{"id": "19588", "url": "https://en.wikipedia.org/wiki?curid=19588", "title": "Mitochondrion", "text": "Mitochondrion\n\nThe mitochondrion (plural mitochondria) is a double-membrane-bound organelle found in most eukaryotic organisms. Some cells in some multicellular organisms may, however, lack them (for example, mature mammalian red blood cells). A number of unicellular organisms, such as microsporidia, parabasalids, and diplomonads, have also reduced or transformed their mitochondria into other structures. To date, only one eukaryote, \"Monocercomonoides\", is known to have completely lost its mitochondria. The word mitochondrion comes from the Greek , , \"thread\", and , , \"granule\" or \"grain-like\". Mitochondria generate most of the cell's supply of adenosine triphosphate (ATP), used as a source of chemical energy.\n\nMitochondria are commonly between 0.75 and 3 μm in diameter but vary considerably in size and structure. Unless specifically stained, they are not visible. In addition to supplying cellular energy, mitochondria are involved in other tasks, such as signaling, cellular differentiation, and cell death, as well as maintaining control of the cell cycle and cell growth. Mitochondrial biogenesis is in turn temporally coordinated with these cellular processes. Mitochondria have been implicated in several human diseases, including mitochondrial disorders, cardiac dysfunction, heart failure and autism.\n\nThe number of mitochondria in a cell can vary widely by organism, tissue, and cell type. For instance, red blood cells have no mitochondria, whereas liver cells can have more than 2000. The organelle is composed of compartments that carry out specialized functions. These compartments or regions include the outer membrane, the intermembrane space, the inner membrane, and the cristae and matrix.\n\nAlthough most of a cell's DNA is contained in the cell nucleus, the mitochondrion has its own independent genome that shows substantial similarity to bacterial genomes. Mitochondrial proteins (proteins transcribed from mitochondrial DNA) vary depending on the tissue and the species. In humans, 615 distinct types of protein have been identified from cardiac mitochondria, whereas in rats, 940 proteins have been reported. The mitochondrial proteome is thought to be dynamically regulated.\n\nThe first observations of intracellular structures that probably represented mitochondria were published in the 1840s. Richard Altmann, in 1890, established them as cell organelles and called them \"bioblasts\". The term \"mitochondria\" was coined by Carl Benda in 1898. Leonor Michaelis discovered that Janus green can be used as a supravital stain for mitochondria in 1900. In 1904, Friedrich Meves, made the first recorded observation of mitochondria in plants in cells of the white waterlily, \"Nymphaea alba\" and in 1908, along with Claudius Regaud, suggested that they contain proteins and lipids. Benjamin F. Kingsbury, in 1912, first related them with cell respiration, but almost exclusively based on morphological observations. In 1913, particles from extracts of guinea-pig liver were linked to respiration by Otto Heinrich Warburg, which he called \"grana\". Warburg and Heinrich Otto Wieland, who had also postulated a similar particle mechanism, disagreed on the chemical nature of the respiration. It was not until 1925, when David Keilin discovered cytochromes, that the respiratory chain was described.\n\nIn 1939, experiments using minced muscle cells demonstrated that cellular respiration using one oxygen atom can form two adenosine triphosphate (ATP) molecules, and, in 1941, the concept of the phosphate bonds of ATP being a form of energy in cellular metabolism was developed by Fritz Albert Lipmann. In the following years, the mechanism behind cellular respiration was further elaborated, although its link to the mitochondria was not known. The introduction of tissue fractionation by Albert Claude allowed mitochondria to be isolated from other cell fractions and biochemical analysis to be conducted on them alone. In 1946, he concluded that cytochrome oxidase and other enzymes responsible for the respiratory chain were isolated to the mitochondria. Eugene Kennedy and Albert Lehninger discovered in 1948 that mitochondria are the site of oxidative phosphorylation in eukaryotes. Over time, the fractionation method was further developed, improving the quality of the mitochondria isolated, and other elements of cell respiration were determined to occur in the mitochondria.\n\nThe first high-resolution electron micrographs appeared in 1952, replacing the Janus Green stains as the preferred way of visualising the mitochondria. This led to a more detailed analysis of the structure of the mitochondria, including confirmation that they were surrounded by a membrane. It also showed a second membrane inside the mitochondria that folded up in ridges dividing up the inner chamber and that the size and shape of the mitochondria varied from cell to cell.\n\nThe popular term \"powerhouse of the cell\" was coined by Philip Siekevitz in 1957.\n\nIn 1967, it was discovered that mitochondria contained ribosomes. In 1968, methods were developed for mapping the mitochondrial genes, with the genetic and physical map of yeast mitochondrial DNA being completed in 1976.\n\nThere are two hypotheses about the origin of mitochondria: endosymbiotic and autogenous. The endosymbiotic hypothesis suggests that mitochondria were originally prokaryotic cells, capable of implementing oxidative mechanisms that were not possible for eukaryotic cells; they became endosymbionts living inside the eukaryote. In the autogenous hypothesis, mitochondria were born by splitting off a portion of DNA from the nucleus of the eukaryotic cell at the time of divergence with the prokaryotes; this DNA portion would have been enclosed by membranes, which could not be crossed by proteins. Since mitochondria have many features in common with bacteria, the endosymbiotic hypothesis is more widely accepted.\n\nA mitochondrion contains DNA, which is organized as several copies of a single, usually circular, chromosome. This mitochondrial chromosome contains genes for redox proteins, such as those of the respiratory chain. The CoRR hypothesis proposes that this co-location is required for redox regulation. The mitochondrial genome codes for some RNAs of ribosomes, and the 22 tRNAs necessary for the translation of mRNAs into protein. The circular structure is also found in prokaryotes. The proto-mitochondrion was probably closely related to the \"Rickettsia\". However, the exact relationship of the ancestor of mitochondria to the alphaproteobacteria and whether the mitochondrion was formed at the same time or after the nucleus, remains controversial. For example, it has been suggested that the SAR11 clade of bacteria shares a relatively recent common ancestor with the mitochondria, while phylogenomic analyses indicate that mitochondria evolved from a proteobacteria lineage that branched off before the divergence of all sampled alphaproteobacteria.\nThe ribosomes coded for by the mitochondrial DNA are similar to those from bacteria in size and structure. They closely resemble the bacterial 70S ribosome and not the 80S cytoplasmic ribosomes, which are coded for by nuclear DNA.\n\nThe endosymbiotic relationship of mitochondria with their host cells was popularized by Lynn Margulis. The endosymbiotic hypothesis suggests that mitochondria descended from bacteria that somehow survived endocytosis by another cell, and became incorporated into the cytoplasm. The ability of these bacteria to conduct respiration in host cells that had relied on glycolysis and fermentation would have provided a considerable evolutionary advantage. This symbiotic relationship probably developed 1.7 to 2 billion years ago.\nA few groups of unicellular eukaryotes have only vestigial mitochondria or derived structures: the microsporidians, metamonads, and archamoebae. These groups appear as the most primitive eukaryotes on phylogenetic trees constructed using rRNA information, which once suggested that they appeared before the origin of mitochondria. However, this is now known to be an artifact of long-branch attraction—they are derived groups and retain genes or organelles derived from mitochondria (e.g., mitosomes and hydrogenosomes).\n\nMonocercomonoides appear to have lost their mitochondria completely and at least some of the mitochondrial functions seem to be carried out by cytoplasmic proteins now\".\"\n\nA mitochondrion contains outer and inner membranes composed of phospholipid bilayers and proteins. The two membranes have different properties. Because of this double-membraned organization, there are five distinct parts to a mitochondrion. They are:\n\nMitochondria stripped of their outer membrane are called mitoplasts.\n\nThe outer mitochondrial membrane, which encloses the entire organelle, is 60 to 75 angstroms (Å) thick. It has a protein-to-phospholipid ratio similar to that of the eukaryotic plasma membrane (about 1:1 by weight). It contains large numbers of integral membrane proteins called porins. These porins form channels that allow molecules of 5000 daltons or less in molecular weight to freely diffuse from one side of the membrane to the other. Larger proteins can enter the mitochondrion if a signaling sequence at their N-terminus binds to a large multisubunit protein called translocase in the outer membrane, which then actively moves them across the membrane. Mitochondrial pro-proteins are imported through specialised translocation complexes. The outer membrane also contains enzymes involved in such diverse activities as the elongation of fatty acids, oxidation of epinephrine, and the degradation of tryptophan. These enzymes include monoamine oxidase, rotenone-insensitive NADH-cytochrome c-reductase, kynurenine hydroxylase and fatty acid Co-A ligase. Disruption of the outer membrane permits proteins in the intermembrane space to leak into the cytosol, leading to certain cell death. The mitochondrial outer membrane can associate with the endoplasmic reticulum (ER) membrane, in a structure called MAM (mitochondria-associated ER-membrane). This is important in the ER-mitochondria calcium signaling and is involved in the transfer of lipids between the ER and mitochondria.\n\nThe intermembrane space is the space between the outer membrane and the inner membrane. It is also known as perimitochondrial space. Because the outer membrane is freely permeable to small molecules, the concentrations of small molecules, such as ions and sugars, in the intermembrane space is the same as in the cytosol. However, large proteins must have a specific signaling sequence to be transported across the outer membrane, so the protein composition of this space is different from the protein composition of the cytosol. One protein that is localized to the intermembrane space in this way is cytochrome c.\n\nThe inner mitochondrial membrane contains proteins with five types of functions:\n\nIt contains more than 151 different polypeptides, and has a very high protein-to-phospholipid ratio (more than 3:1 by weight, which is about 1 protein for 15 phospholipids). The inner membrane is home to around 1/5 of the total protein in a mitochondrion. In addition, the inner membrane is rich in an unusual phospholipid, cardiolipin. This phospholipid was originally discovered in cow hearts in 1942, and is usually characteristic of mitochondrial and bacterial plasma membranes. Cardiolipin contains four fatty acids rather than two, and may help to make the inner membrane impermeable. Unlike the outer membrane, the inner membrane doesn't contain porins, and is highly impermeable to all molecules. Almost all ions and molecules require special membrane transporters to enter or exit the matrix. Proteins are ferried into the matrix via the translocase of the inner membrane (TIM) complex or via Oxa1. In addition, there is a membrane potential across the inner membrane, formed by the action of the enzymes of the electron transport chain.\n\nThe inner mitochondrial membrane is compartmentalized into numerous cristae, which expand the surface area of the inner mitochondrial membrane, enhancing its ability to produce ATP. For typical liver mitochondria, the area of the inner membrane is about five times as large as the outer membrane. This ratio is variable and mitochondria from cells that have a greater demand for ATP, such as muscle cells, contain even more cristae. These folds are studded with small round bodies known as F particles or oxysomes. These are not simple random folds but rather invaginations of the inner membrane, which can affect overall chemiosmotic function.\n\nOne recent mathematical modeling study has suggested that the optical properties of the cristae in filamentous mitochondria may affect the generation and propagation of light within the tissue.\n\nThe matrix is the space enclosed by the inner membrane. It contains about 2/3 of the total protein in a mitochondrion. The matrix is important in the production of ATP with the aid of the ATP synthase contained in the inner membrane. The matrix contains a highly concentrated mixture of hundreds of enzymes, special mitochondrial ribosomes, tRNA, and several copies of the mitochondrial DNA genome. Of the enzymes, the major functions include oxidation of pyruvate and fatty acids, and the citric acid cycle.\n\nMitochondria have their own genetic material, and the machinery to manufacture their own RNAs and proteins (\"see: protein biosynthesis\"). A published human mitochondrial DNA sequence revealed 16,569 base pairs encoding 37 genes: 22 tRNA, 2 rRNA, and 13 peptide genes. The 13 mitochondrial peptides in humans are integrated into the inner mitochondrial membrane, along with proteins encoded by genes that reside in the host cell's nucleus.\n\nThe mitochondria-associated ER membrane (MAM) is another structural element that is increasingly recognized for its critical role in cellular physiology and homeostasis. Once considered a technical snag in cell fractionation techniques, the alleged ER vesicle contaminants that invariably appeared in the mitochondrial fraction have been re-identified as membranous structures derived from the MAM—the interface between mitochondria and the ER. Physical coupling between these two organelles had previously been observed in electron micrographs and has more recently been probed with fluorescence microscopy. Such studies estimate that at the MAM, which may comprise up to 20% of the mitochondrial outer membrane, the ER and mitochondria are separated by a mere 10–25 nm and held together by protein tethering complexes.\n\nPurified MAM from subcellular fractionation has been shown to be enriched in enzymes involved in phospholipid exchange, in addition to channels associated with Ca signaling. These hints of a prominent role for the MAM in the regulation of cellular lipid stores and signal transduction have been borne out, with significant implications for mitochondrial-associated cellular phenomena, as discussed below. Not only has the MAM provided insight into the mechanistic basis underlying such physiological processes as intrinsic apoptosis and the propagation of calcium signaling, but it also favors a more refined view of the mitochondria. Though often seen as static, isolated 'powerhouses' hijacked for cellular metabolism through an ancient endosymbiotic event, the evolution of the MAM underscores the extent to which mitochondria have been integrated into overall cellular physiology, with intimate physical and functional coupling to the endomembrane system.\n\nThe MAM is enriched in enzymes involved in lipid biosynthesis, such as phosphatidylserine synthase on the ER face and phosphatidylserine decarboxylase on the mitochondrial face. Because mitochondria are dynamic organelles constantly undergoing fission and fusion events, they require a constant and well-regulated supply of phospholipids for membrane integrity. But mitochondria are not only a destination for the phospholipids they finish synthesis of; rather, this organelle also plays a role in inter-organelle trafficking of the intermediates and products of phospholipid biosynthetic pathways, ceramide and cholesterol metabolism, and glycosphingolipid anabolism.\n\nSuch trafficking capacity depends on the MAM, which has been shown to facilitate transfer of lipid intermediates between organelles. In contrast to the standard vesicular mechanism of lipid transfer, evidence indicates that the physical proximity of the ER and mitochondrial membranes at the MAM allows for lipid flipping between opposed bilayers. Despite this unusual and seemingly energetically unfavorable mechanism, such transport does not require ATP. Instead, in yeast, it has been shown to be dependent on a multiprotein tethering structure termed the ER-mitochondria encounter structure, or ERMES, although it remains unclear whether this structure directly mediates lipid transfer or is required to keep the membranes in sufficiently close proximity to lower the energy barrier for lipid flipping.\n\nThe MAM may also be part of the secretory pathway, in addition to its role in intracellular lipid trafficking. In particular, the MAM appears to be an intermediate destination between the rough ER and the Golgi in the pathway that leads to very-low-density lipoprotein, or VLDL, assembly and secretion. The MAM thus serves as a critical metabolic and trafficking hub in lipid metabolism.\n\nA critical role for the ER in calcium signaling was acknowledged before such a role for the mitochondria was widely accepted, in part because the low affinity of Ca channels localized to the outer mitochondrial membrane seemed to contradict this organelle's purported responsiveness to changes in intracellular Ca flux. But the presence of the MAM resolves this apparent contradiction: the close physical association between the two organelles results in Ca microdomains at contact points that facilitate efficient Ca transmission from the ER to the mitochondria. Transmission occurs in response to so-called \"Ca puffs\" generated by spontaneous clustering and activation of IP3R, a canonical ER membrane Ca channel.\n\nThe fate of these puffs—in particular, whether they remain restricted to isolated locales or integrated into Ca waves for propagation throughout the cell—is determined in large part by MAM dynamics. Although reuptake of Ca by the ER (concomitant with its release) modulates the intensity of the puffs, thus insulating mitochondria to a certain degree from high Ca exposure, the MAM often serves as a firewall that essentially buffers Ca puffs by acting as a sink into which free ions released into the cytosol can be funneled. This Ca tunneling occurs through the low-affinity Ca receptor VDAC1, which recently has been shown to be physically tethered to the IP3R clusters on the ER membrane and enriched at the MAM. The ability of mitochondria to serve as a Ca sink is a result of the electrochemical gradient generated during oxidative phosphorylation, which makes tunneling of the cation an exergonic process. Normal, mild calcium influx from cytosol into the mitochondrial matrix causes transient depolarization that is corrected by pumping out protons.\n\nBut transmission of Ca is not unidirectional; rather, it is a two-way street. The properties of the Ca pump SERCA and the channel IP3R present on the ER membrane facilitate feedback regulation coordinated by MAM function. In particular, the clearance of Ca by the MAM allows for spatio-temporal patterning of Ca signaling because Ca alters IP3R activity in a biphasic manner. SERCA is likewise affected by mitochondrial feedback: uptake of Ca by the MAM stimulates ATP production, thus providing energy that enables SERCA to reload the ER with Ca for continued Ca efflux at the MAM. Thus, the MAM is not a passive buffer for Ca puffs; rather it helps modulate further Ca signaling through feedback loops that affect ER dynamics.\n\nRegulating ER release of Ca at the MAM is especially critical because only a certain window of Ca uptake sustains the mitochondria, and consequently the cell, at homeostasis. Sufficient intraorganelle Ca signaling is required to stimulate metabolism by activating dehydrogenase enzymes critical to flux through the citric acid cycle. However, once Ca signaling in the mitochondria passes a certain threshold, it stimulates the intrinsic pathway of apoptosis in part by collapsing the mitochondrial membrane potential required for metabolism. Studies examining the role of pro- and anti-apoptotic factors support this model; for example, the anti-apoptotic factor Bcl-2 has been shown to interact with IP3Rs to reduce Ca filling of the ER, leading to reduced efflux at the MAM and preventing collapse of the mitochondrial membrane potential post-apoptotic stimuli. Given the need for such fine regulation of Ca signaling, it is perhaps unsurprising that dysregulated mitochondrial Ca has been implicated in several neurodegenerative diseases, while the catalogue of tumor suppressors includes a few that are enriched at the MAM.\n\nRecent advances in the identification of the tethers between the mitochondrial and ER membranes suggest that the scaffolding function of the molecular elements involved is secondary to other, non-structural functions. In yeast, ERMES, a multiprotein complex of interacting ER- and mitochondrial-resident membrane proteins, is required for lipid transfer at the MAM and exemplifies this principle. One of its components, for example, is also a constituent of the protein complex required for insertion of transmembrane beta-barrel proteins into the lipid bilayer. However, a homologue of the ERMES complex has not yet been identified in mammalian cells. Other proteins implicated in scaffolding likewise have functions independent of structural tethering at the MAM; for example, ER-resident and mitochondrial-resident mitofusins form heterocomplexes that regulate the number of inter-organelle contact sites, although mitofusins were first identified for their role in fission and fusion events between individual mitochondria. Glucose-related protein 75 (grp75) is another dual-function protein. In addition to the matrix pool of grp75, a portion serves as a chaperone that physically links the mitochondrial and ER Ca channels VDAC and IP3R for efficient Ca transmission at the MAM. Another potential tether is Sigma-1R, a non-opioid receptor whose stabilization of ER-resident IP3R may preserve communication at the MAM during the metabolic stress response.\n\nThe MAM is a critical signaling, metabolic, and trafficking hub in the cell that allows for the integration of ER and mitochondrial physiology. Coupling between these organelles is not simply structural but functional as well and critical for overall cellular physiology and homeostasis. The MAM thus offers a perspective on mitochondria that diverges from the traditional view of this organelle as a static, isolated unit appropriated for its metabolic capacity by the cell. Instead, this mitochondrial-ER interface emphasizes the integration of the mitochondria, the product of an endosymbiotic event, into diverse cellular processes.\n\nMitochondria (and related structures) are found in all eukaryotes (except one—the Oxymonad \"Monocercomonoides\" sp.). Although commonly depicted as bean-like structures they form a highly dynamic network in the majority of cells where they constantly undergo fission and fusion. The population of all the mitochondria of a given cell constitutes the chondriome. Mitochondria vary in number and location according to cell type. A single mitochondrion is often found in unicellular organisms. Conversely, the chondriome size of human liver cells is large, with about 1000–2000 mitochondria per cell, making up 1/5 of the cell volume. The mitochondrial content of otherwise similar cells can vary substantially in size and membrane potential, with differences arising from sources including uneven partitioning at cell divisions, leading to extrinsic differences in ATP levels and downstream cellular processes. The mitochondria can be found nestled between myofibrils of muscle or wrapped around the sperm flagellum. Often, they form a complex 3D branching network inside the cell with the cytoskeleton. The association with the cytoskeleton determines mitochondrial shape, which can affect the function as well: different structures of the mitochondrial network may afford the population a variety of physical, chemical, and signalling advantages or disadvantages. Mitochondria in cells are always distributed along microtubules and the distribution of these organelles is also correlated with the endoplasmic reticulum. Recent evidence suggests that vimentin, one of the components of the cytoskeleton, is also critical to the association with the cytoskeleton.\n\nThe most prominent roles of mitochondria are to produce the energy currency of the cell, ATP (i.e., phosphorylation of ADP), through respiration, and to regulate cellular metabolism. The central set of reactions involved in ATP production are collectively known as the citric acid cycle, or the Krebs cycle. However, the mitochondrion has many other functions in addition to the production of ATP.\n\nA dominant role for the mitochondria is the production of ATP, as reflected by the large number of proteins in the inner membrane for this task. This is done by oxidizing the major products of glucose: pyruvate, and NADH, which are produced in the cytosol. This type of cellular respiration known as aerobic respiration, is dependent on the presence of oxygen. When oxygen is limited, the glycolytic products will be metabolized by anaerobic fermentation, a process that is independent of the mitochondria. The production of ATP from glucose has an approximately 13-times higher yield during aerobic respiration compared to fermentation. Plant mitochondria can also produce a limited amount of ATP without oxygen by using the alternate substrate nitrite. ATP crosses out through the inner membrane with the help of a specific protein, and across the outer membrane via porins. ADP returns via the same route.\n\nPyruvate molecules produced by glycolysis are actively transported across the inner mitochondrial membrane, and into the matrix where they can either be oxidized and combined with coenzyme A to form CO, acetyl-CoA, and NADH, or they can be carboxylated (by pyruvate carboxylase) to form oxaloacetate. This latter reaction ”fills up” the amount of oxaloacetate in the citric acid cycle, and is therefore an anaplerotic reaction, increasing the cycle’s capacity to metabolize acetyl-CoA when the tissue's energy needs (e.g. in muscle) are suddenly increased by activity.\n\nIn the citric acid cycle, all the intermediates (e.g. citrate, iso-citrate, alpha-ketoglutarate, succinate, fumarate, malate and oxaloacetate) are regenerated during each turn of the cycle. Adding more of any of these intermediates to the mitochondrion therefore means that the additional amount is retained within the cycle, increasing all the other intermediates as one is converted into the other. Hence, the addition of any one of them to the cycle has an anaplerotic effect, and its removal has a cataplerotic effect. These anaplerotic and cataplerotic reactions will, during the course of the cycle, increase or decrease the amount of oxaloacetate available to combine with acetyl-CoA to form citric acid. This in turn increases or decreases the rate of ATP production by the mitochondrion, and thus the availability of ATP to the cell.\n\nAcetyl-CoA, on the other hand, derived from pyruvate oxidation, or from the beta-oxidation of fatty acids, is the only fuel to enter the citric acid cycle. With each turn of the cycle one molecule of acetyl-CoA is consumed for every molecule of oxaloacetate present in the mitochondrial matrix, and is never regenerated. It is the oxidation of the acetate portion of acetyl-CoA that produces CO and water, with the energy thus released captured in the form of ATP.\n\nIn the liver, the carboxylation of cytosolic pyruvate into intra-mitochondrial oxaloacetate is an early step in the gluconeogenic pathway, which converts lactate and de-aminated alanine into glucose, under the influence of high levels of glucagon and/or epinephrine in the blood. Here, the addition of oxaloacetate to the mitochondrion does not have a net anaplerotic effect, as another citric acid cycle intermediate (malate) is immediately removed from the mitochondrion to be converted into cytosolic oxaloacetate, which is ultimately converted into glucose, in a process that is almost the reverse of glycolysis.\n\nThe enzymes of the citric acid cycle are located in the mitochondrial matrix, with the exception of succinate dehydrogenase, which is bound to the inner mitochondrial membrane as part of Complex II. The citric acid cycle oxidizes the acetyl-CoA to carbon dioxide, and, in the process, produces reduced cofactors (three molecules of NADH and one molecule of FADH) that are a source of electrons for the \"electron transport chain\", and a molecule of GTP (that is readily converted to an ATP).\n\nThe redox energy from NADH and FADH is transferred to oxygen (O) in several steps via the electron transport chain. These energy-rich molecules are produced within the matrix via the citric acid cycle but are also produced in the cytoplasm by glycolysis. Reducing equivalents from the cytoplasm can be imported via the malate-aspartate shuttle system of antiporter proteins or feed into the electron transport chain using a glycerol phosphate shuttle. Protein complexes in the inner membrane (NADH dehydrogenase (ubiquinone), cytochrome c reductase, and cytochrome c oxidase) perform the transfer and the incremental release of energy is used to pump protons (H) into the intermembrane space. This process is efficient, but a small percentage of electrons may prematurely reduce oxygen, forming reactive oxygen species such as superoxide. This can cause oxidative stress in the mitochondria and may contribute to the decline in mitochondrial function associated with the aging process.\n\nAs the proton concentration increases in the intermembrane space, a strong electrochemical gradient is established across the inner membrane. The protons can return to the matrix through the ATP synthase complex, and their potential energy is used to synthesize ATP from ADP and inorganic phosphate (P). This process is called chemiosmosis, and was first described by Peter Mitchell who was awarded the 1978 Nobel Prize in Chemistry for his work. Later, part of the 1997 Nobel Prize in Chemistry was awarded to Paul D. Boyer and John E. Walker for their clarification of the working mechanism of ATP synthase.\n\nUnder certain conditions, protons can re-enter the mitochondrial matrix without contributing to ATP synthesis. This process is known as \"proton leak\" or \"mitochondrial uncoupling\" and is due to the facilitated diffusion of protons into the matrix. The process results in the unharnessed potential energy of the proton electrochemical gradient being released as heat. The process is mediated by a proton channel called thermogenin, or UCP1. Thermogenin is a 33 kDa protein first discovered in 1973. Thermogenin is primarily found in brown adipose tissue, or brown fat, and is responsible for non-shivering thermogenesis. Brown adipose tissue is found in mammals, and is at its highest levels in early life and in hibernating animals. In humans, brown adipose tissue is present at birth and decreases with age.\n\nThe concentrations of free calcium in the cell can regulate an array of reactions and is important for signal transduction in the cell. Mitochondria can transiently store calcium, a contributing process for the cell's homeostasis of calcium.\n\nCa influx to the mitochondrial matrix has recently been implicated as a mechanism to regulate respiratory bioenergetics by allowing the electrochemical potential across the membrane to transiently \"pulse\" from ΔΨ-dominated to pH-dominated, facilitating a reduction of oxidative stress. In neurons, concomitant increases in cytosolic and mitochondrial calcium act to synchronize neuronal activity with mitochondrial energy metabolism. Mitochondrial matrix calcium levels can reach the tens of micromolar levels, which is necessary for the activation of isocitrate dehydrogenase, one of the key regulatory enzymes of the Krebs cycle.\n\nMitochondria play a central role in many other metabolic tasks, such as:\n\nSome mitochondrial functions are performed only in specific types of cells. For example, mitochondria in liver cells contain enzymes that allow them to detoxify ammonia, a waste product of protein metabolism. A mutation in the genes regulating any of these functions can result in mitochondrial diseases.\n\nThe relationship between cellular proliferation and mitochondria has been investigated using cervical cancer HeLa cells. Tumor cells require an ample amount of ATP (Adenosine triphosphate) in order to synthesize bioactive compounds such as lipids, proteins, and nucleotides for rapid cell proliferation. The majority of ATP in tumor cells is generated via the oxidative phosphorylation pathway (OxPhos). Interference with OxPhos have shown to cause cell cycle arrest suggesting that mitochondria play a role in cell proliferation. Mitochondrial ATP production is also vital for cell division in addition to other basic functions in the cell including the regulation of cell volume, solute concentration, and cellular architecture. ATP levels differ at various stages of the cell cycle suggesting that there is a relationship between the abundance of ATP and the cell's ability to enter a new cell cycle. ATP's role in the basic functions of the cell make the cell cycle sensitive to changes in the availability of mitochondrial derived ATP. The variation in ATP levels at different stages of the cell cycle support the hypothesis that mitochondria play an important role in cell cycle regulation. Although the specific mechanisms between mitochondria and the cell cycle regulation is not well understood, studies have shown that low energy cell cycle checkpoints monitor the energy capability before committing to another round of cell division.\n\nMitochondria contain their own genome, an indication that they are derived from bacteria through endosymbiosis. However, the ancestral endosymbiont genome has lost most of its genes so that the mitochondrial genome (mitogenome) is one of the most reduced genomes across organisms.\n\nThe human mitochondrial genome is a circular DNA molecule of about 16 kilobases. It encodes 37 genes: 13 for subunits of respiratory complexes I, III, IV and V, 22 for mitochondrial tRNA (for the 20 standard amino acids, plus an extra gene for leucine and serine), and 2 for rRNA. One mitochondrion can contain two to ten copies of its DNA.\n\nAs in prokaryotes, there is a very high proportion of coding DNA and an absence of repeats. Mitochondrial genes are transcribed as multigenic transcripts, which are cleaved and polyadenylated to yield mature mRNAs. Not all proteins necessary for mitochondrial function are encoded by the mitochondrial genome; most are coded by genes in the cell nucleus and the corresponding proteins are imported into the mitochondrion. The exact number of genes encoded by the nucleus and the mitochondrial genome differs between species. Most mitochondrial genomes are circular, although exceptions have been reported. In general, mitochondrial DNA lacks introns, as is the case in the human mitochondrial genome; however, introns have been observed in some eukaryotic mitochondrial DNA, such as that of yeast and protists, including \"Dictyostelium discoideum\". Between protein-coding regions, tRNAs are present. During transcription, the tRNAs acquire their characteristic L-shape that gets recognized and cleaved by specific enzymes. Mitochondrial tRNA genes have different sequences from the nuclear tRNAs but lookalikes of mitochondrial tRNAs have been found in the nuclear chromosomes with high sequence similarity.\n\nIn animals, the mitochondrial genome is typically a single circular chromosome that is approximately 16 kb long and has 37 genes. The genes, while highly conserved, may vary in location. Curiously, this pattern is not found in the human body louse (\"Pediculus humanus\"). Instead, this mitochondrial genome is arranged in 18 minicircular chromosomes, each of which is 3–4 kb long and has one to three genes. This pattern is also found in other sucking lice, but not in chewing lice. Recombination has been shown to occur between the minichromosomes. The reason for this difference is not known.\n\nWhile slight variations on the standard genetic code had been predicted earlier, none was discovered until 1979, when researchers studying human mitochondrial genes determined that they used an alternative code. However, the mitochondria of many other eukaryotes, including most plants, use the standard code. Many slight variants have been discovered since, including various alternative mitochondrial codes. Further, the AUA, AUC, and AUU codons are all allowable start codons.\n\nSome of these differences should be regarded as pseudo-changes in the genetic code due to the phenomenon of RNA editing, which is common in mitochondria. In higher plants, it was thought that CGG encoded for tryptophan and not arginine; however, the codon in the processed RNA was discovered to be the UGG codon, consistent with the standard genetic code for tryptophan. Of note, the arthropod mitochondrial genetic code has undergone parallel evolution within a phylum, with some organisms uniquely translating AGG to lysine.\n\nMitochondrial genomes have far fewer genes than the bacteria from which they are thought to be descended. Although some have been lost altogether, many have been transferred to the nucleus, such as the respiratory complex II protein subunits. This is thought to be relatively common over evolutionary time. A few organisms, such as the \"Cryptosporidium\", actually have mitochondria that lack any DNA, presumably because all their genes have been lost or transferred. In \"Cryptosporidium\", the mitochondria have an altered ATP generation system that renders the parasite resistant to many classical mitochondrial inhibitors such as cyanide, azide, and atovaquone.\n\nMitochondria divide by binary fission, similar to bacterial cell division. The regulation of this division differs between eukaryotes. In many single-celled eukaryotes, their growth and division is linked to the cell cycle. For example, a single mitochondrion may divide synchronously with the nucleus. This division and segregation process must be tightly controlled so that each daughter cell receives at least one mitochondrion. In other eukaryotes (in mammals for example), mitochondria may replicate their DNA and divide mainly in response to the energy needs of the cell, rather than in phase with the cell cycle. When the energy needs of a cell are high, mitochondria grow and divide. When the energy use is low, mitochondria are destroyed or become inactive. In such examples, and in contrast to the situation in many single celled eukaryotes, mitochondria are apparently randomly distributed to the daughter cells during the division of the cytoplasm. Understanding of mitochondrial dynamics, which is described as the balance between mitochondrial fusion and fission, has revealed that functional and structural alterations in mitochondrial morphology are important factors in pathologies associated with several disease conditions.\n\nThe hypothesis of mitochondrial binary fission has relied on the visualization by fluorescence microscopy and conventional transmission electron microscopy (TEM). The resolution of fluorescence microscopy(~200 nm) is insufficient to distinguish structural details, such as double mitochondrial membrane in mitochondrial division or even to distinguish individual mitochondria when several are close together. Conventional TEM has also some technical limitations in verifying mitochondrial division. Cryo-electron tomography was recently used to visualize mitochondrial division in frozen hydrated intact cells. It revealed that mitochondria divide by budding.\n\nAn individual's mitochondrial genes are not inherited by the same mechanism as nuclear genes. Typically, the mitochondria are inherited from one parent only. In humans, when an egg cell is fertilized by a sperm, the egg nucleus and sperm nucleus each contribute equally to the genetic makeup of the zygote nucleus. In contrast, the mitochondria, and therefore the mitochondrial DNA, usually come from the egg only. The sperm's mitochondria enter the egg, but do not contribute genetic information to the embryo. Instead, paternal mitochondria are marked with ubiquitin to select them for later destruction inside the embryo. The egg cell contains relatively few mitochondria, but it is these mitochondria that survive and divide to populate the cells of the adult organism. Mitochondria are, therefore, in most cases inherited only from mothers, a pattern known as maternal inheritance. This mode is seen in most organisms, including the majority of animals. However, mitochondria in some species can sometimes be inherited paternally. This is the norm among certain coniferous plants, although not in pine trees and yews. For Mytilids, paternal inheritance only occurs within males of the species. It has been suggested that it occurs at a very low level in humans. There is a recent suggestion that mitochondria that shorten male lifespan stay in the system because they are inherited only through the mother. By contrast, natural selection weeds out mitochondria that reduce female survival as such mitochondria are less likely to be passed on to the next generation. Therefore, it is suggested that human females and female animals tend to live longer than males. The authors claim that this is a partial explanation.\n\nUniparental inheritance leads to little opportunity for genetic recombination between different lineages of mitochondria, although a single mitochondrion can contain 2–10 copies of its DNA. For this reason, mitochondrial DNA is usually thought to reproduce by binary fission. What recombination does take place maintains genetic integrity rather than maintaining diversity. However, there are studies showing evidence of recombination in mitochondrial DNA. It is clear that the enzymes necessary for recombination are present in mammalian cells. Further, evidence suggests that animal mitochondria can undergo recombination. The data are a bit more controversial in humans, although indirect evidence of recombination exists. If recombination does not occur, the whole mitochondrial DNA sequence represents a single haplotype, which makes it useful for studying the evolutionary history of populations.\n\nEntities undergoing uniparental inheritance and with little to no recombination may be expected to be subject to Muller's ratchet, the inexorable accumulation of deleterious mutations until functionality is lost. Animal populations of mitochondria avoid this buildup through a developmental process known as the mtDNA bottleneck. The bottleneck exploits stochastic processes in the cell to increase in the cell-to-cell variability in mutant load as an organism develops: a single egg cell with some proportion of mutant mtDNA thus produces an embryo where different cells have different mutant loads. Cell-level selection may then act to remove those cells with more mutant mtDNA, leading to a stabilisation or reduction in mutant load between generations. The mechanism underlying the bottleneck is debated, with a recent mathematical and experimental metastudy providing evidence for a combination of random partitioning of mtDNAs at cell divisions and random turnover of mtDNA molecules within the cell.\n\nMitochondria can repair oxidative DNA damage by mechanisms that are analogous to those occurring in the cell nucleus. The proteins that are employed in mtDNA repair are encoded by nuclear genes, and are translocated to the mitochondria. The DNA repair pathways in mammalian mitochondria include base excision repair, double-strand break repair, direct reversal and mismatch repair. Also DNA damages may be bypassed, rather than repaired, by translesion synthesis.\n\nOf the several DNA repair process in mitochondria, the base excision repair pathway is the one that has been most comprehensively studied. Base excision repair is carried out by a sequence of enzymatic catalyzed steps that include recognition and excision of a damaged DNA base, removal of the resulting abasic site, end processing, gap filling and ligation. A common damage in mtDNA that is repaired by base excision repair is 8-oxoguanine produced by the oxidation of guanine.\n\nDouble-strand breaks can be repaired by homologous recombinational repair in both mammalian mtDNA and plant mtDNA. Double-strand breaks in mtDNA can also be repaired by microhomology-mediated end joining. Although there is evidence for the repair processes of direct reversal and mismatch repair in mtDNA, these processes are still not well characterized.\n\nThe near-absence of genetic recombination in mitochondrial DNA makes it a useful source of information for scientists involved in population genetics and evolutionary biology. Because all the mitochondrial DNA is inherited as a single unit, or haplotype, the relationships between mitochondrial DNA from different individuals can be represented as a gene tree. Patterns in these gene trees can be used to infer the evolutionary history of populations. The classic example of this is in human evolutionary genetics, where the molecular clock can be used to provide a recent date for mitochondrial Eve. This is often interpreted as strong support for a recent modern human expansion out of Africa. Another human example is the sequencing of mitochondrial DNA from Neanderthal bones. The relatively large evolutionary distance between the mitochondrial DNA sequences of Neanderthals and living humans has been interpreted as evidence for the lack of interbreeding between Neanderthals and anatomically modern humans.\n\nHowever, mitochondrial DNA reflects only the history of the females in a population and so may not represent the history of the population as a whole. This can be partially overcome by the use of paternal genetic sequences, such as the non-recombining region of the Y-chromosome. In a broader sense, only studies that also include nuclear DNA can provide a comprehensive evolutionary history of a population.\n\nRecent measurements of the molecular clock for mitochondrial DNA reported a value of 1 mutation every 7884 years dating back to the most recent common ancestor of humans and apes, which is consistent with estimates of mutation rates of autosomal DNA (10 per base per generation.\n\nDamage and subsequent dysfunction in mitochondria is an important factor in a range of human diseases due to their influence in cell metabolism. Mitochondrial disorders often present themselves as neurological disorders, including autism. They can also manifest as myopathy, diabetes, multiple endocrinopathy, and a variety of other systemic disorders. Diseases caused by mutation in the mtDNA include Kearns-Sayre syndrome, MELAS syndrome and Leber's hereditary optic neuropathy. In the vast majority of cases, these diseases are transmitted by a female to her children, as the zygote derives its mitochondria and hence its mtDNA from the ovum. Diseases such as Kearns-Sayre syndrome, Pearson syndrome, and progressive external ophthalmoplegia are thought to be due to large-scale mtDNA rearrangements, whereas other diseases such as MELAS syndrome, Leber's hereditary optic neuropathy, myoclonic epilepsy with ragged red fibers (MERRF), and others are due to point mutations in mtDNA.\n\nIn other diseases, defects in nuclear genes lead to dysfunction of mitochondrial proteins. This is the case in Friedreich's ataxia, hereditary spastic paraplegia, and Wilson's disease. These diseases are inherited in a dominance relationship, as applies to most other genetic diseases. A variety of disorders can be caused by nuclear mutations of oxidative phosphorylation enzymes, such as coenzyme Q10 deficiency and Barth syndrome. Environmental influences may interact with hereditary predispositions and cause mitochondrial disease. For example, there may be a link between pesticide exposure and the later onset of Parkinson's disease. Other pathologies with etiology involving mitochondrial dysfunction include schizophrenia, bipolar disorder, dementia, Alzheimer's disease, Parkinson's disease, epilepsy, stroke, cardiovascular disease, chronic fatigue syndrome, retinitis pigmentosa, and diabetes mellitus.\n\nMitochondria-mediated oxidative stress plays a role in cardiomyopathy in Type 2 diabetics. Increased fatty acid delivery to the heart increases fatty acid uptake by cardiomyocytes, resulting in increased fatty acid oxidation in these cells. This process increases the reducing equivalents available to the electron transport chain of the mitochondria, ultimately increasing reactive oxygen species (ROS) production. ROS increases uncoupling proteins (UCPs) and potentiate proton leakage through the adenine nucleotide translocator (ANT), the combination of which uncouples the mitochondria. Uncoupling then increases oxygen consumption by the mitochondria, compounding the increase in fatty acid oxidation. This creates a vicious cycle of uncoupling; furthermore, even though oxygen consumption increases, ATP synthesis does not increase proportionally because the mitochondria is uncoupled. Less ATP availability ultimately results in an energy deficit presenting as reduced cardiac efficiency and contractile dysfunction. To compound the problem, impaired sarcoplasmic reticulum calcium release and reduced mitochondrial reuptake limits peak cytosolic levels of the important signaling ion during muscle contraction. The decreased intra-mitochondrial calcium concentration increases dehydrogenase activation and ATP synthesis. So in addition to lower ATP synthesis due to fatty acid oxidation, ATP synthesis is impaired by poor calcium signaling as well, causing cardiac problems for diabetics.\n\nGiven the role of mitochondria as the cell's powerhouse, there may be some leakage of the high-energy electrons in the respiratory chain to form reactive oxygen species. This was thought to result in significant oxidative stress in the mitochondria with high mutation rates of mitochondrial DNA (mtDNA). Hypothesized links between aging and oxidative stress are not new and were proposed in 1956, which was later refined into the mitochondrial free radical theory of aging. A vicious cycle was thought to occur, as oxidative stress leads to mitochondrial DNA mutations, which can lead to enzymatic abnormalities and further oxidative stress.\n\nA number of changes can occur to mitochondria during the aging process. Tissues from elderly patients show a decrease in enzymatic activity of the proteins of the respiratory chain. However, mutated mtDNA can only be found in about 0.2% of very old cells. Large deletions in the mitochondrial genome have been hypothesized to lead to high levels of oxidative stress and neuronal death in Parkinson's disease.\n\nMadeleine L'Engle's 1973 science fantasy novel \"A Wind in the Door\" prominently features the mitochondria of main character Charles Wallace Murry, as being inhabited by creatures known as the farandolae. The novel also features other characters travelling inside one of Murry's mitochondria.\n\nThe 1995 horror fiction novel \"Parasite Eve\" by Hideaki Sena depicts mitochondria as having some consciousness and mind control abilities, attempting to use these to overtake eukaryotes as the dominant life form. This text was adapted into an eponymous film, video game, and video game sequel all involving a similar premise.\n\nIn the \"Star Wars\" franchise, microorganisms referred to as \"midi-chlorians\" give some characters the ability to sense and use the Force. George Lucas, director of the 1999 film \"\", in which midi-chlorians were introduced, described them as \"a loose depiction of mitochondria\". The non-fictional bacteria genus \"Midichloria\" was later named after the midi-chlorians of \"Star Wars\".\n\nAs a result of the mitochondrion's prominence in modern science education, the phrase \"the mitochondria is the powerhouse of the cell\" became a popular Internet meme. The meme is used to imply that secondary education places an insufficient focus on life skills, compared to academic knowledge such as the role of the mitochondrion, which has been considered comparatively impractical.\n\nGeneral\n\n"}
