{"id": "14099", "url": "https://en.wikipedia.org/wiki?curid=14099", "title": "History of Africa", "text": "History of Africa\n\nThe history of Africa begins with the emergence of hominids, archaic humans and—at least 200,000 years ago—anatomically modern humans (\"Homo sapiens\"), in East Africa, and continues unbroken into the present as a patchwork of diverse and politically developing nation states. The earliest known recorded history arose in the Kingdom of Kush, and later in Ancient Egypt, the Sahel, the Maghreb and the Horn of Africa.\n\nFollowing the desertification of the Sahara, North African history became entwined with the Middle East and Southern Europe while the Bantu expansion swept from modern day Cameroon (West Africa) across much of the sub-Saharan continent in waves between around 1000 BC and 0 AD, creating a linguistic commonality across much of the central and Southern continent.\n\nDuring the Middle Ages, Islam spread west from Arabia to Egypt, crossing the Maghreb and the Sahel. Some notable pre-colonial states and societies in Africa include the Ajuran Empire, D'mt, Adal Sultanate, Warsangali Sultanate, Kingdom of Nri, Nok culture, Mali Empire, Songhai Empire, Benin Empire, Oyo Empire, Ashanti Empire, Ghana Empire, Mossi Kingdoms, Mutapa Empire, Kingdom of Mapungubwe, Kingdom of Sine, Kingdom of Sennar, Kingdom of Saloum, Kingdom of Baol, Kingdom of Cayor, Kingdom of Zimbabwe, Kingdom of Kongo, Empire of Kaabu, Kingdom of Ile Ife, Ancient Carthage, Numidia, Mauretania, and the Aksumite Empire. At its peak, prior to European colonialism, it is estimated that Africa had up to 10,000 different states and autonomous groups with distinct languages and customs.<ref name=\"http://newswatch.nationalgeographic.com/2013/10/31/getting-to-know-africa-50-facts/\">Africa Information</ref>\n\nFrom the mid-7th century, the Arab slave trade saw Muslim Arabs enslave Africans. Following an armistice between the Rashidun Caliphate and the Kingdom of Makuria after the Second Battle of Dongola in 652 AD, they were transported, along with Asians and Europeans, across the Red Sea, Indian Ocean, and Sahara Desert.\n\nFrom the late 15th century, Europeans joined the slave trade. One could say the Portuguese led in partnership with other Europeans. That includes the triangular trade, with the Portuguese initially acquiring slaves through trade and later by force as part of the Atlantic slave trade. They transported enslaved West, Central, and Southern Africans overseas. Subsequently, European colonization of Africa developed rapidly from around 10% (1870) to over 90% (1914) in the Scramble for Africa (1881–1914). However following struggles for independence in many parts of the continent, as well as a weakened Europe after the Second World War , decolonization took place across the continent, culminating in the 1960 Year of Africa.\n\nAfrica's pre-colonial history has been challenging to research, mostly due to the almost extreme lack of documentation and architecture that the continents of Europe and Asia are so richly dense in. Disciplines such as the recording of oral history, historical linguistics, archaeology and genetics have been crucial.\n\nThe first known hominids evolved in Africa. According to paleontology, the early hominids' skull anatomy was similar to that of the gorilla and the chimpanzee, great apes that also evolved in Africa, but the hominids had adopted a bipedal locomotion which freed their hands. This gave them a crucial advantage, enabling them to live in both forested areas and on the open savanna at a time when Africa was drying up and the savanna was encroaching on forested areas. This would have occurred 10 to 5 million years ago, but these claims are controversial because biologists and genetics have humans appearing around the last 70 thousand to 200 thousand years. https://web.archive.org/web/20150907140051/http://genome.wellcome.ac.uk/doc_WTD020876.html\n\nBy 4 million years ago, several australopithecine hominid species had developed throughout Southern, Eastern and Central Africa. They were tool users, and makers of tools. They scavenged for meat and were omnivores.\n\nBy approximately 3.3 million years ago, primitive stone tools were first used to scavenge kills made by other predators and to harvest carrion and marrow from their bones. In hunting, \"Homo habilis\" was probably not capable of competing with large predators and was still more prey than hunter. \"H. habilis\" probably did steal eggs from nests and may have been able to catch small game and weakened larger prey (cubs and older animals). The tools were classed as Oldowan.\n\nAround 1.8 million years ago, \"Homo ergaster\" first appeared in the fossil record in Africa. From \"Homo ergaster\", \"Homo erectus\" evolved 1.5 million years ago. Some of the earlier representatives of this species were still fairly small-brained and used primitive stone tools, much like \"H. habilis\". The brain later grew in size, and \"H. erectus\" eventually developed a more complex stone tool technology called the Acheulean. Possibly the first hunters, \"H. erectus\" mastered the art of making fire and was the first hominid to leave Africa, colonizing most of Afro-Eurasia and perhaps later giving rise to \"Homo floresiensis\". Although some recent writers have suggested that \"Homo georgicus\" was the first and primary hominid ever to live outside Africa, many scientists consider \"H. georgicus\" to be an early and primitive member of the \"H. erectus\" species.\nThe fossil record shows \"Homo sapiens\" living in Southern and Eastern Africa at least 200,000 to 150,000 years ago. Around 40,000 years ago, the species' expansion out of Africa launched the colonization of the planet by modern human beings. By 10,000 BC, \"Homo sapiens\" had spread to most corners of Afro-Eurasia. Their disperals are traced by linguistic, cultural and genetic evidence. The earliest physical evidence of astronomical activity appears to be a lunar calendar found on the Ishango bone dated to between 23,000 and 18,000 BC.\n\nScholars have argued that warfare was absent throughout much of humanity's prehistoric past, and that it emerged from more complex political systems as a result of sedentism, agricultural farming, etc. However, the findings at the site of Nataruk in Turkana County, Kenya, where the remains of 27 individuals who died as the result of an intentional attack by another group 10,000 years ago, suggest that inter-human conflict has a much longer history.\n\nAround 16,000 BC, from the Red Sea Hills to the northern Ethiopian Highlands, nuts, grasses and tubers were being collected for food. By 13,000 to 11,000 BC, people began collecting wild grains. This spread to Western Asia, which domesticated its wild grains, wheat and barley. Between 10,000 and 8000 BC, Northeast Africa was cultivating wheat and barley and raising sheep and cattle from Southwest Asia. A wet climatic phase in Africa turned the Ethiopian Highlands into a mountain forest. Omotic speakers domesticated enset around 6500–5500 BC. Around 7000 BC, the settlers of the Ethiopian highlands domesticated donkeys, and by 4000 BC domesticated donkeys had spread to Southwest Asia. Cushitic speakers, partially turning away from cattle herding, domesticated teff and finger millet between 5500 and 3500 BC.\n\nIn the steppes and savannahs of the Sahara and Sahel in Northern West Africa, the Nilo-Saharan speakers and Mandé peoples started to collect and domesticate wild millet, African rice and sorghum between 8000 and 6000 BC. Later, gourds, watermelons, castor beans, and cotton were also collected and domesticated. The people started capturing wild cattle and holding them in circular thorn hedges, resulting in domestication. They also started making pottery and built stone settlements (see Tichitt and Oualata). Fishing, using bone-tipped harpoons, became a major activity in the numerous streams and lakes formed from the increased rains. Mande peoples have been credited with the independent development of agriculture about 3000–4000 BC.\n\nIn West Africa, the wet phase ushered in an expanding rainforest and wooded savanna from Senegal to Cameroon. Between 9000 and 5000 BC, Niger–Congo speakers domesticated the oil palm and raffia palm. Two seed plants, black-eyed peas and voandzeia (African groundnuts), were domesticated, followed by okra and kola nuts. Since most of the plants grew in the forest, the Niger–Congo speakers invented polished stone axes for clearing forest.\n\nMost of Southern Africa was occupied by pygmy peoples and Khoisan who engaged in hunting and gathering. Some of the oldest rock art was produced by them.\n\nFor several hundred thousand years the Sahara has alternated between desert and savanna grassland in a 41,000 year cycle caused by changes (\"precession\") in the Earth's axis as it rotates around the sun which change the location of the North African Monsoon. When the North African monsoon is at its strongest annual precipitation and subsequent vegetation in the Sahara region increase, resulting in conditions commonly referred to as the \"green Sahara\". For a relatively weak North African monsoon, the opposite is true, with decreased annual precipitation and less vegetation resulting in a phase of the Sahara climate cycle known as the \"desert Sahara\". The Sahara has been a desert for several thousand years, and is expected to become green again in about 15,000 years time (17,000 AD).\n\nJust prior to Saharan desertification, the communities that developed south of Egypt, in what is now Sudan, were full participants in the Neolithic revolution and lived a settled to semi-nomadic lifestyle, with domesticated plants and animals. It has been suggested that megaliths found at Nabta Playa are examples of the world's first known archaeoastronomical devices, predating Stonehenge by some 1,000 years. The sociocultural complexity observed at Nabta Playa and expressed by different levels of authority within the society there has been suggested as forming the basis for the structure of both the Neolithic society at Nabta and the Old Kingdom of Egypt.\nBy 5000 BC, Africa entered a dry phase, and the climate of the Sahara region gradually became drier. The population trekked out of the Sahara region in all directions, including towards the Nile Valley below the Second Cataract, where they made permanent or semipermanent settlements. A major climatic recession occurred, lessening the heavy and persistent rains in Central and Eastern Africa. Since then, dry conditions have prevailed in Eastern Africa.\n\nArchaeological finds in Central Africa have been discovered dating back to over 100,000 years. Extensive walled sites and settlements have recently been found in Zilum, Chad approximately southwest of Lake Chad dating to the first millennium BC.\n\nTrade and improved agricultural techniques supported more sophisticated societies, leading to the early civilizations of Sao, Kanem, Bornu, Shilluk, Baguirmi, and Wadai.\n\nAround 1,000 BC, Bantu migrants had reached the Great Lakes Region in Central Africa. Halfway through the first millennium BC, the Bantu had also settled as far south as what is now Angola.\n\nThe first metals to be smelted in Africa were lead, copper, and bronze in the fourth millennium BC.\n\nCopper was smelted in Egypt during the predynastic period, and bronze came into use after 3,000 BC at the latest in Egypt and Nubia. Nubia was a major source of copper as well as gold. The use of gold and silver in Egypt dates back to the predynastic period.\n\nIn the Aïr Mountains, present-day Niger, copper was smelted independently of developments in the Nile valley between 3,000 and 2,500 BC. The process used was unique to the region, indicating that it was not brought from outside the region; it became more mature by about 1,500 BC.\n\nBy the 1st millennium BC, iron working had been introduced in Northwestern Africa, Egypt, and Nubia. According to Zangato an Holl, there is evidence of iron-smelting in the Central African Republic and Cameroon that may date back to 3,000 to 2,500 BC. In 670 BC, Nubians were pushed out of Egypt by Assyrians using iron weapons, after which the use of iron in the Nile valley became widespread.\n\nThe theory of iron spreading to Sub-Saharan Africa via the Nubian city of Meroe is no longer widely accepted. Metalworking in West Africa has been dated as early as 2,500 BC at Egaro west of the Termit in Niger, and iron working was practiced there by 1,500 BC. In Central Africa, there is evidence that iron working may have been practiced as early as the 3rd millennium BC. Iron smelting was developed in the area between Lake Chad and the African Great Lakes between 1,000 and 600 BC, long before it reached Egypt. Before 500 BC, the Nok culture in the Jos Plateau was already smelting iron.\n\nThe ancient history of North Africa is inextricably linked to that of the Ancient Near East. This is particularly true of Ancient Egypt and Nubia. In the Horn of Africa the Kingdom of Aksum ruled modern-day Eritrea, northern Ethiopia and the coastal area of the western part of the Arabian Peninsula. The Ancient Egyptians established ties with the Land of Punt in 2,350 BC. Punt was a trade partner of Ancient Egypt and it is believed that it was located in modern-day Somalia, Djibouti or Eritrea. Phoenician cities such as Carthage were part of the Mediterranean Iron Age and classical antiquity. Sub-Saharan Africa developed more or less independently in those times. \n\nAfter the desertification of the Sahara, settlement became concentrated in the Nile Valley, where numerous sacral chiefdoms appeared. The regions with the largest population pressure were in the Nile Delta region of Lower Egypt, in Upper Egypt, and also along the second and third cataracts of the Dongola Reach of the Nile in Nubia. This population pressure and growth was brought about by the cultivation of southwest Asian crops, including wheat and barley, and the raising of sheep, goats, and cattle. Population growth led to competition for farm land and the need to regulate farming. Regulation was established by the formation of bureaucracies among sacral chiefdoms. The first and most powerful of the chiefdoms was Ta-Seti, founded around 3,500 BC. The idea of sacral chiefdom spread throughout Upper and Lower Egypt.\nLater consolidation of the chiefdoms into broader political entities began to occur in Upper and Lower Egypt, culminating into the unification of Egypt into one political entity by Narmer (Menes) in 3,100 BC. Instead of being viewed as a sacral chief, he became a divine king. The henotheism, or worship of a single god within a polytheistic system, practiced in the sacral chiefdoms along Upper and Lower Egypt, became the polytheistic Ancient Egyptian religion. Bureaucracies became more centralized under the pharaohs, run by viziers, governors, tax collectors, generals, artists, and technicians. They engaged in tax collecting, organizing of labor for major public works, and building irrigation systems, pyramids, temples, and canals. During the Fourth Dynasty (2,620–2,480 BC), long distance trade was developed, with the Levant for timber, with Nubia for gold and skins, with Punt for frankincense, and also with the western Libyan territories. For most of the Old Kingdom, Egypt developed her fundamental systems, institutions and culture, always through the central bureaucracy and by the divinity of the Pharaoh.\n\nAfter the fourth millennium BC, Egypt started to extend direct military and political control over her southern and western neighbors. By 2,200 BC, the Old Kingdom's stability was undermined by rivalry among the governors of the nomes who challenged the power of pharaohs and by invasions of Asiatics into the Nile Delta. The First Intermediate Period had begun, a time of political division and uncertainty.\n\nMiddle Kingdom of Egypt arose when Mentuhotep II of Eleventh Dynasty unified Egypt once again between 2041 and 2016 BC beginning with his conquering of Tenth Dynasty in 2041 BC. Pyramid building resumed, long-distance trade re-emerged, and the center of power moved from Memphis to Thebes. Connections with the southern regions of Kush, Wawat and Irthet at the second cataract were made stronger. Then came the Second Intermediate Period, with the invasion of the Hyksos on horse-drawn chariots and utilizing bronze weapons, a technology heretofore unseen in Egypt. Horse-drawn chariots soon spread to the west in the inhabitable Sahara and North Africa. The Hyksos failed to hold on to their Egyptian territories and were absorbed by Egyptian society. This eventually led to one of Egypt's most powerful phases, the New Kingdom (1,580–1,080 BC), with the Eighteenth Dynasty. Egypt became a superpower controlling Nubia and Judea while exerting political influence on the Libyans to the West and on the Mediterranean.\n\nAs before, the New Kingdom ended with invasion from the west by Libyan princes, leading to the Third Intermediate Period. Beginning with Shoshenq I, the Twenty-second Dynasty was established. It ruled for two centuries.\n\nTo the south, Nubian independence and strength was being reasserted. This reassertion led to the conquest of Egypt by Nubia, begun by Kashta and completed by Piye (Pianhky, 751–730 BC) and Shabaka (716–695 BC). This was the birth of the Twenty-fifth Dynasty of Egypt. The Nubians tried to re-establish Egyptian traditions and customs. They ruled Egypt for a hundred years. This was ended by an Assyrian invasion, with Taharqa experiencing the full might of Assyrian iron weapons. The Nubian pharaoh Tantamani was the last of the Twenty-fifth dynasty.\n\nWhen the Assyrians and Nubians left, a new Twenty-sixth Dynasty emerged from Sais. It lasted until 525 BC, when Egypt was invaded by the Persians. Unlike the Assyrians, the Persians stayed. In 332, Egypt was conquered by Alexander the Great. This was the beginning of the Ptolemaic dynasty, which ended with Roman conquest in 30 BC. Pharaonic Egypt had come to an end.\n\nAround 3,500 BC, one of the first sacral kingdoms to arise in the Nile was Ta-Seti, located in northern Nubia. Ta-Seti was a powerful sacral kingdom in the Nile Valley at the 1st and 2nd cataracts that exerted an influence over nearby chiefdoms based on pictorial representation ruling over Upper Egypt. Ta-Seti traded as far as Syro-Palestine, as well as with Egypt. Ta-Seti exported gold, copper, ostrich feathers, ebony and ivory to the Old Kingdom. By the 32nd century BC, Ta-Seti was in decline. After the unification of Egypt by Narmer in 3,100 BC, Ta-Seti was invaded by the Pharaoh Hor-Aha of the First Dynasty, destroying the final remnants of the kingdom. Ta-Seti is affiliated with the A-Group Culture known to archaeology.\nSmall sacral kingdoms continued to dot the Nubian portion of the Nile for centuries after 3,000 BC. Around the latter part of the third millennium, there was further consolidation of the sacral kingdoms. Two kingdoms in particular emerged: the Sai kingdom, immediately south of Egypt, and the Kingdom of Kerma at the third cataract. Sometime around the 18th century BC, the Kingdom of Kerma conquered the Kingdom of Sai, becoming a serious rival to Egypt. Kerma occupied a territory from the first cataract to the confluence of the Blue Nile, White Nile, and Atbarah River. About 1,575 to 1,550 BC, during the latter part of the Seventeenth Dynasty, the Kingdom of Kerma invaded Egypt. The Kingdom of Kerma allied itself with the Hyksos invasion of Egypt.\n\nEgypt eventually re-energized under the Eighteenth Dynasty and conquered the Kingdom of Kerma or Kush, ruling it for almost 500 years. The Kushites were Egyptianized during this period. By 1100 BC, the Egyptians had withdrawn from Kush. The region regained independence and reasserted its culture. Kush built a new religion around Amun and made Napata its spiritual center. In 730 BC, the Kingdom of Kush invaded Egypt, taking over Thebes and beginning the Nubian Empire. The empire extended from Palestine to the confluence of the Blue Nile, the White Nile, and River Atbara.\n\nIn 760 BC, the Kushites were expelled from Egypt by iron-wielding Assyrians. Later, the administrative capital was moved from Napata to Meröe, developing a new Nubian culture. Initially, Meroites were highly Egyptianized, but they subsequently began to take on distinctive features. Nubia became a center of iron-making and cotton cloth manufacturing. Egyptian writing was replaced by the Meroitic alphabet. The lion god Apedemak was added to the Egyptian pantheon of gods. Trade links to the Red Sea increased, linking Nubia with Mediterranean Greece. Its architecture and art diversified, with pictures of lions, ostriches, giraffes, and elephants. Eventually, with the rise of Aksum, Nubia's trade links were broken and it suffered environmental degradation from the tree cutting required for iron production. In 350 AD, the Aksumite king Ezana brought Meröe to an end.\n\nThe Egyptians referred to the people west of the Nile, ancestral to the Berbers, as Libyans. The Libyans were agriculturalists like the Mauri of Morocco and the Numidians of central and eastern Algeria and Tunis. They were also nomadic, having the horse, and occupied the arid pastures and desert, like the Gaetuli. Berber desert nomads were typically in conflict with Berber coastal agriculturalists.\n\nThe Phoenicians were Mediterranean seamen in constant search for valuable metals such as copper, gold, tin, and lead. They began to populate the North African coast with settlementstrading and mixing with the native Berber population. In 814 BC, Phoenicians from Tyre established the city of Carthage. By 600 BC, Carthage had become a major trading entity and power in the Mediterranean, largely through trade with tropical Africa. Carthage's prosperity fostered the growth of the Berber kingdoms, Numidia and Mauretania. Around 500 BC, Carthage provided a strong impetus for trade with Sub-Saharan Africa. Berber middlemen, who had maintained contacts with Sub-Saharan Africa since the desert had desiccated, utilized pack animals to transfer products from oasis to oasis. Danger lurked from the Garamantes of Fez, who raided caravans. Salt and metal goods were traded for gold, slaves, beads, and ivory.\nThe Carthaginians were rivals to the Greeks and Romans. Carthage fought the Punic Wars, three wars with Rome: the First Punic War (264 to 241 BC), over Sicily; the Second Punic War (218 to 201 BC), in which Hannibal invaded Europe; and the Third Punic War (149 to 146 BC). Carthage lost the first two wars, and in the third it was destroyed, becoming the Roman province of Africa, with the Berber Kingdom of Numidia assisting Rome. The Roman province of Africa became a major agricultural supplier of wheat, olives, and olive oil to imperial Rome via exorbitant taxation. Two centuries later, Rome brought the Berber kingdoms of Numidia and Mauretania under its authority. In the 420's AD, Vandals invaded North Africa and Rome lost her territories. The Berber kingdoms subsequently regained their independence.\n\nChristianity gained a foothold in Africa at Alexandria in the 1st century AD and spread to Northwest Africa. By 313 AD, with the Edict of Milan, all of Roman North Africa was Christian. Egyptians adopted Monophysite Christianity and formed the independent Coptic Church. Berbers adopted Donatist Christianity. Both groups refused to accept the authority of the Roman Catholic Church. \n\nAs Carthaginian power grew, its impact on the indigenous population increased dramatically. Berber civilization was already at a stage in which agriculture, manufacturing, trade, and political organization supported several states. Trade links between Carthage and the Berbers in the interior grew, but territorial expansion also resulted in the enslavement or military recruitment of some Berbers and in the extraction of tribute from others. By the early 4th century BC, Berbers formed one of the largest element, with Gauls, of the Carthaginian army. In the Revolt of the Mercenaries, Berber soldiers participated from 241 to 238 BC after being unpaid following the defeat of Carthage in the First Punic War. Berbers succeeded in obtaining control of much of Carthage's North African territory, and they minted coins bearing the name Libyan, used in Greek to describe natives of North Africa. The Carthaginian state declined because of successive defeats by the Romans in the Punic Wars; in 146 BC the city of Carthage was destroyed. As Carthaginian power waned, the influence of Berber leaders in the hinterland grew. By the 2nd century BC, several large but loosely administered Berber kingdoms had emerged. Two of them were established in Numidia, behind the coastal areas controlled by Carthage. West of Numidia lay Mauretania, which extended across the Moulouya River in Morocco to the Atlantic Ocean. The high point of Berber civilization, unequaled until the coming of the Almohads and Almoravid dynasty more than a millennium later, was reached during the reign of Masinissa in the 2nd century BC. After Masinissa's death in 148 BC, the Berber kingdoms were divided and reunited several times. Masinissa's line survived until 24 AD, when the remaining Berber territory was annexed to the Roman Empire.\n\nThe ancestors of the Somali people were an important link in the Horn of Africa connecting the region's commerce with the rest of the ancient world. Somali sailors and merchants were the main suppliers of frankincense, myrrh and spices, all of which were valuable luxuries to the Ancient Egyptians, Phoenicians, Mycenaeans and Babylonians.\n\nIn the classical era, several flourishing Somali city-states such as Opone, Mosylon, Cape Guardafui, and Malao competed with the Sabaeans, Parthians and Axumites for the rich Indo–Greco-Roman trade.\n\nIncreases in urbanization and in the area under cultivation during Roman rule caused wholesale dislocations of the Berber society, forcing nomad tribes to settle or to move from their traditional rangelands. Sedentary tribes lost their autonomy and connection with the land. Berber opposition to the Roman presence was nearly constant. The Roman emperor Trajan established a frontier in the south by encircling the Aurès and Nemencha mountains and building a line of forts from Vescera (modern Biskra) to Ad Majores (Hennchir Besseriani, southeast of Biskra). The defensive line extended at least as far as Castellum Dimmidi (modern Messaâd, southwest of Biskra), Roman Algeria's southernmost fort. Romans settled and developed the area around Sitifis (modern Sétif) in the 2nd century, but farther west the influence of Rome did not extend beyond the coast and principal military roads until much later. \n\nThe Roman military presence of North Africa remained relatively small, consisting of about 28,000 troops and auxiliaries in Numidia and the two Mauretanian provinces. Starting in the 2nd century AD, these garrisons were manned mostly by local inhabitants.\n\nAside from Carthage, urbanization in North Africa came in part with the establishment of settlements of veterans under the Roman emperors Claudius (reigned 41–54), Nerva (96–98), and Trajan (98–117). In Algeria such settlements included Tipasa, Cuicul or Curculum (modern Djemila, northeast of Sétif), Thamugadi (modern Timgad, southeast of Sétif), and Sitifis (modern Sétif). The prosperity of most towns depended on agriculture. Called the \"granary of the empire\", North Africa became one of the largest exporters of grain in the empire, shipping to the provinces which did not produce cereals, like Italy and Greece. Other crops included fruit, figs, grapes, and beans. By the 2nd century AD, olive oil rivaled cereals as an export item.\n\nThe beginnings of the Roman imperial decline seemed less serious in North Africa than elsewhere. However, uprisings did take place. In 238 AD, landowners rebelled unsuccessfully against imperial fiscal policies. Sporadic tribal revolts in the Mauretanian mountains followed from 253 to 288, during the Crisis of the Third Century. The towns also suffered economic difficulties, and building activity almost ceased.\n\nThe towns of Roman North Africa had a substantial Jewish population. Some Jews had been deported from Judea or Palestine in the 1st and 2nd centuries AD for rebelling against Roman rule; others had come earlier with Punic settlers. In addition, a number of Berber tribes had converted to Judaism.\n\nChristianity arrived in the 2nd century and soon gained converts in the towns and among slaves. More than eighty bishops, some from distant frontier regions of Numidia, attended the Council of Carthage (256) in 256. By the end of the 4th century, the settled areas had become Christianized, and some Berber tribes had converted \"en masse\".\n\nA division in the church that came to be known as the Donatist heresy began in 313 among Christians in North Africa. The Donatists stressed the holiness of the church and refused to accept the authority to administer the sacraments of those who had surrendered the scriptures when they were forbidden under the Emperor Diocletian (reigned 284–305). The Donatists also opposed the involvement of Constantine the Great (reigned 306–337) in church affairs in contrast to the majority of Christians who welcomed official imperial recognition.\n\nThe occasionally violent Donatist controversy has been characterized as a struggle between opponents and supporters of the Roman system. The most articulate North African critic of the Donatist position, which came to be called a heresy, was Augustine, bishop of Hippo Regius. Augustine maintained that the unworthiness of a minister did not affect the validity of the sacraments because their true minister was Jesus Christ. In his sermons and books Augustine, who is considered a leading exponent of Christian dogma, evolved a theory of the right of orthodox Christian rulers to use force against schismatics and heretics. Although the dispute was resolved by a decision of an imperial commission in Carthage in 411, Donatist communities continued to exist as late as the 6th century.\n\nA decline in trade weakened Roman control. Independent kingdoms emerged in mountainous and desert areas, towns were overrun, and Berbers, who had previously been pushed to the edges of the Roman Empire, returned.\n\nDuring the Vandalic War, Belisarius, general of the Byzantine emperor Justinian I based in Constantinople, landed in North Africa in 533 with 16,000 men and within a year destroyed the Vandal Kingdom. Local opposition delayed full Byzantine control of the region for twelve years, however, and when imperial control came, it was but a shadow of the control exercised by Rome. Although an impressive series of fortifications were built, Byzantine rule was compromised by official corruption, incompetence, military weakness, and lack of concern in Constantinople for African affairs, which made it an easy target for the Arabs during the Early Muslim conquests . As a result, many rural areas reverted to Berber rule. \n\nThe earliest state in Eritrea and northern Ethiopia, Dʿmt, dates from around the 8th and 7th centuries BC. D'mt traded through the Red Sea with Egypt and the Mediterranean, providing frankincense. By the 5th and 3rd centuries, D'mt had declined, and several successor states took its place. Later there was greater trade with South Arabia, mainly with the port of Saba. Adulis became an important commercial center in the Ethiopian Highlands. The interaction of the peoples in the two regions, the southern Arabia Sabaeans and the northern Ethiopians, resulted in the Ge'ez culture and language and eventual development of the Ge'ez script. Trade links increased and expanded from the Red Sea to the Mediterranean, with Egypt, Greece, and Rome, to the Black Sea, and to Persia, India, and China. Aksum was known throughout those lands. By the 5th century BC, the region was very prosperous, exporting ivory, hippopotamus hides, gold dust, spices, and live elephants. It imported silver, gold, olive oil, and wine. Aksum manufactured glass crystal, brass, and copper for export. A powerful Aksum emerged, unifying parts of eastern Sudan, northern Ethiopia (Tigre), and Eritrea. Its kings built stone palatial buildings and were buried under megalithic monuments. By 300 AD, Aksum was minting its own coins in silver and gold.\n\nIn 331 AD, King Ezana (320–350 AD) was converted to Miaphysite Christianity which believes in one united divine-human nature of Christ], supposedly by Frumentius and Aedesius, who became stranded on the Red Sea coast. Some scholars believed the process was more complex and gradual than a simple conversion. Around 350, the time Ezana sacked Meroe, the Syrian monastic tradition took root within the Ethiopian church.\n\nIn the 6th century Aksum was powerful enough to add Saba on the Arabian peninsula to her empire. At the end of the 6th century, the Sasanian Empire pushed Aksum out of the peninsula. With the spread of Islam through Western Asia and Northern Africa, Aksum's trading networks in the Mediterranean faltered. The Red Sea trade diminished as it was diverted to the Persian Gulf and dominated by Arabs, causing Aksum to decline. By 800 AD, the capital was moved south into the interior highlands, and Aksum was much diminished.\n\nIn the western Sahel the rise of settled communities occurred largely as a result of the domestication of millet and of sorghum. Archaeology points to sizable urban populations in West Africa beginning in the 2nd millennium BC. Symbiotic trade relations developed before the trans-Saharan trade, in response to the opportunities afforded by north-south diversity in ecosystems across deserts, grasslands, and forests. The agriculturists received salt from the desert nomads. The desert nomads acquired meat and other foods from pastoralists and farmers of the grasslands and from fishermen on the Niger River. The forest-dwellers provided furs and meat.\nDhar Tichitt and Oualata in present-day Mauritania figure prominently among the early urban centers, dated to 2,000 BC. About 500 stone settlements litter the region in the former savannah of the Sahara. Its inhabitants fished and grew millet. It has been found that the Soninke of the Mandé peoples were responsible for constructing such settlements. Around 300 BC the region became more desiccated and the settlements began to decline, most likely relocating to Koumbi Saleh. Architectural evidence and the comparison of pottery styles suggest that Dhar Tichitt was related to the subsequent Ghana Empire. Djenné-Djenno (in present-day Mali) was settled around 300 BC, and the town grew to house a sizable Iron Age population, as evidenced by crowded cemeteries. Living structures were made of sun-dried mud. By 250 BC Djenné-Djenno had become a large, thriving market town.\n\nFarther south, in central Nigeria, around 1,000 BC, the Nok culture developed on the Jos Plateau. It was a highly centralized community. The Nok people produced lifelike representations in terracotta, including human heads, elephants, and other animals. By 500 BC they were smelting iron. By 200 AD the Nok culture had vanished. Based on stylistic similarities with the Nok terracottas, the bronze figurines of the Yoruba kingdom of Ife and those of the Bini kingdom of Benin are now believed to be continuations of the traditions of the earlier Nokite culture.\n\nThe Bantu expansion involved a significant movement of people in African history and in the settling of the continent. People speaking Bantu languages (a branch of the Niger–Congo family) began in the second millennium BC to spread from Cameroon eastward to the Great Lakes region. In the first millennium BC, Bantu languages spread from the Great Lakes to southern and east Africa. One early movement headed south to the upper Zambezi valley in the 2nd century BC. Then Bantu-speakers pushed westward to the savannahs of present-day Angola and eastward into Malawi, Zambia, and Zimbabwe in the 1st century AD. The second thrust from the Great Lakes was eastward, 2,000 years ago, expanding to the Indian Ocean coast, Kenya and Tanzania. The eastern group eventually met the southern migrants from the Great Lakes in Malawi, Zambia, and Zimbabwe. Both groups continued southward, with eastern groups continuing to Mozambique and reaching Maputo in the 2nd century AD, and expanding as far as Durban. By the later first millennium AD, the expansion had reached the Great Kei River in present-day South Africa. Sorghum, a major Bantu crop, could not thrive under the winter rainfall of Namibia and the western Cape. Khoisan people inhabited the remaining parts of southern Africa.\n\nThe Sao civilization flourished from c. the sixth century BC to as late as the 16th century AD in Central Africa. The Sao lived by the Chari River south of Lake Chad in territory that later became part of Cameroon and Chad. They are the earliest people to have left clear traces of their presence in the territory of modern Cameroon. Today, several ethnic groups of northern Cameroon and southern Chad but particularly the Sara people claim descent from the civilization of the Sao. Sao artifacts show that they were skilled workers in bronze, copper, and iron. Finds include bronze sculptures and terra cotta statues of human and animal figures, coins, funerary urns, household utensils, jewelry, highly decorated pottery, and spears. The largest Sao archaeological finds have been made south of Lake Chad.\n\nThe Kanem Empire was centered in the Chad Basin. It was known as the Kanem Empire from the 9th century AD onward and lasted as the independent kingdom of Bornu until 1893. At its height it encompassed an area covering not only much of Chad, but also parts of modern southern Libya, eastern Niger, northeastern Nigeria, northern Cameroon, parts of South Sudan and the Central African Republic. The history of the Empire is mainly known from the Royal Chronicle or \"Girgam\" discovered in 1851 by the German traveller Heinrich Barth. Kanem rose in the 8th century in the region to the north and east of Lake Chad. The Kanem empire went into decline, shrank, and in the 14th century was defeated by Bilala invaders from the Lake Fitri region.\n\nAround the 9th century AD, the central Sudanic Empire of Kanem, with its capital at Njimi, was founded by the Kanuri-speaking nomads. Kanem arose by engaging in the trans-Saharan trade. It exchanged slaves captured by raiding the south for horses from North Africa, which in turn aided in the acquisition of slaves. By the late 11th century, the Islamic Sayfawa (Saifawa) dynasty was founded by Humai (Hummay) ibn Salamna. The Sayfawa Dynasty ruled for 771 years, making it one of the longest-lasting dynasties in human history. In addition to trade, taxation of local farms around Kanem became a source of state income. Kanem reached its peak under \"Mai\" (king) Dunama Dibalemi ibn Salma (1210–1248). The empire reportedly was able to field 40,000 cavalry, and it extended from Fezzan in the north to the Sao state in the south. Islam became firmly entrenched in the empire. Pilgrimages to Mecca were common; Cairo had hostels set aside specifically for pilgrims from Kanem.\n\nThe Kanuri people led by the Sayfuwa migrated to the west and south of the lake, where they established the Bornu Empire. By the late 16th century the Bornu empire had expanded and recaptured the parts of Kanem that had been conquered by the Bulala. Satellite states of Bornu included the Damagaram in the west and Baguirmi to the southeast of Lake Chad.\nAround 1400, the Sayfawa Dynasty moved its capital to Bornu, a tributary state southwest of Lake Chad with a new capital Birni Ngarzagamu. Overgrazing had caused the pastures of Kanem to become too dry. In addition, political rivalry from the Bilala clan was becoming intense. Moving to Bornu better situated the empire to exploit the trans-Saharan trade and to widen its network in that trade. Links to the Hausa states were also established, providing horses and salt from Bilma for Akan gold. Mai Ali Gazi ibn Dunama (c. 1475–1503) defeated the Bilala, reestablishing complete control of Kanem.\nDuring the early 16th century, the Sayfawa Dynasty solidified its hold on the Bornu population after much rebellion. In the latter half of the 16th century, \"Mai\" Idris Alooma modernized its military, in contrast to the Songhai Empire. Turkish mercenaries were used to train the military. The Sayfawa Dynasty were the first monarchs south of the Sahara to import firearms. The empire controlled all of the Sahel from the borders of Darfur in the east to Hausaland to the west. Friendly relationship was established with the Ottoman Empire via Tripoli. The \"Mai\" exchanged gifts with the Ottoman sultan.\nDuring the 17th and 18th centuries, not much is known about Bornu. During the 18th century, it became a center of Islamic learning. However, Bornu's army became outdated by not importing new arms, and Kamembu had also begun its decline. The power of the \"mai\" was undermined by droughts and famine that were becoming more intense, internal rebellion in the pastoralist north, growing Hausa power, and the importation of firearms which made warfare more bloody. By 1841, the last \"mai\" was deposed, bringing to an end the long-lived Sayfawa Dynasty. In its place, the al-Kanemi dynasty of the \"shehu\" rose to power.\n\nThe Shilluk Kingdom was centered in South Sudan from the 15th century from along a strip of land along the western bank of the White Nile, from Lake No to about 12° north latitude. The capital and royal residence was in the town of Fashoda. The kingdom was founded during the mid-15th century AD by its first ruler, Nyikang. During the 19th century, the Shilluk Kingdom faced decline following military assaults from the Ottoman Empire and later British and Sudanese colonization in Anglo-Egyptian Sudan.\n\nThe Kingdom of Baguirmi existed as an independent state during the 16th and 17th centuries southeast of Lake Chad in what is now the country of Chad. Baguirmi emerged to the southeast of the Kanem-Bornu Empire. The kingdom's first ruler was Mbang Birni Besse. Later in his reign, the Bornu Empire conquered and made the state a tributary.\n\nThe Wadai Empire was centered on Chad and the Central African Republic from the 17th century. The Tunjur people founded the Wadai Kingdom to the east of Bornu in the 16th century. In the 17th century there was a revolt of the Maba people who established a Muslim dynasty.\n\nAt first Wadai paid tribute to Bornu and Durfur, but by the 18th century Wadai was fully independent and had become an aggressor against its neighbors.To the west of Bornu, by the 15th century the Kingdom of Kano had become the most powerful of the Hausa Kingdoms, in an unstable truce with the Kingdom of Katsina to the north. Both were absorbed into the Sokoto Caliphate during the Fulani Jihad of 1805, which threatened Bornu itself.\n\nSometime between 1300 and 1400 AD, Kongolo Mwamba (Nkongolo) from the Balopwe clan unified the various Luba peoples, near Lake Kisale. He founded the Kongolo Dynasty, which was later ousted by Kalala Ilunga. Kalala expanded the kingdom west of Lake Kisale. A new centralized political system of spiritual kings (\"balopwe\") with a court council of head governors and sub-heads all the way to village heads. The \"balopwe\" was the direct communicator with the ancestral spirits and chosen by them. Conquered states were integrated into the system and represented in the court, with their titles. The authority of the \"balopwe\" resided in his spiritual power rather than his military authority. The army was relatively small. The Luba was able to control regional trade and collect tribute for redistribution. Numerous offshoot states were formed with founders claiming descent from the Luba. The Luba political system spread throughout Central Africa, southern Uganda, Rwanda, Burundi, Malawi, Zambia, Zimbabwe, and the western Congo. Two major empires claiming Luba descent were the Lunda Empire and Maravi Empire. The Bemba people and Basimba people of northern Zambia were descended from Luba migrants who arrived in Zambia during the 17th century.\n\nIn the 1450s, a Luba from the royal family Ilunga Tshibinda married Lunda queen Rweej and united all Lunda peoples. Their son \"mulopwe\" Luseeng expanded the kingdom. His son Naweej expanded the empire further and is known as the first Lunda emperor, with the title \"mwato yamvo\" (\"mwaant yaav\", \"mwant yav\"), the Lord of Vipers. The Luba political system was retained, and conquered peoples were integrated into the system. The \"mwato yamvo\" assigned a \"cilool\" or \"kilolo\" (royal adviser) and tax collector to each state conquered.\n\nNumerous states claimed descent from the Lunda. The Imbangala of inland Angola claimed descent from a founder, Kinguri, brother of Queen Rweej, who could not tolerate the rule of \"mulopwe\" Tshibunda. \"Kinguri\" became the title of kings of states founded by Queen Rweej's brother. The Luena (Lwena) and Lozi (Luyani) in Zambia also claim descent from Kinguri. During the 17th century, a Lunda chief and warrior called Mwata Kazembe set up an Eastern Lunda kingdom in the valley of the Luapula River. The Lunda's western expansion also saw claims of descent by the Yaka and the Pende. The Lunda linked Central Africa with the western coast trade. The kingdom of Lunda came to an end in the 19th century when it was invaded by the Chokwe, who were armed with guns.\n\nBy the 15th century AD, the farming Bakongo people (\"ba\" being the plural prefix) were unified as the Kingdom of Kongo under a ruler called the \"manikongo\", residing in the fertile Pool Malebo area on the lower Congo River. The capital was M'banza-Kongo. With superior organization, they were able to conquer their neighbors and extract tribute. They were experts in metalwork, pottery, and weaving raffia cloth. They stimulated interregional trade via a tribute system controlled by the \"manikongo\". Later, maize (corn) and cassava (manioc) would be introduced to the region via trade with the Portuguese at their ports at Luanda and Benguela. The maize and cassava would result in population growth in the region and other parts of Africa, replacing millet as a main staple.\nBy the 16th century, the \"manikongo\" held authority from the Atlantic in the west to the Kwango River in the east. Each territory was assigned a \"mani-mpembe\" (provincial governor) by the \"manikongo\". In 1506, Afonso I (1506–1542), a Christian, took over the throne. Slave trading increased with Afonso's wars of conquest. About 1568 to 1569, the Jaga invaded Kongo, laying waste to the kingdom and forcing the \"manikongo\" into exile. In 1574, Manikongo Álvaro I was reinstated with the help of Portuguese mercenaries. During the latter part of the 1660s, the Portuguese tried to gain control of Kongo. Manikongo António I (1661–1665), with a Kongolese army of 5,000, was destroyed by an army of Afro-Portuguese at the Battle of Mbwila. The empire dissolved into petty polities, fighting among each other for war captives to sell into slavery.\n\nKongo gained captives from the Kingdom of Ndongo in wars of conquest. Ndongo was ruled by the \"ngola\". Ndongo would also engage in slave trading with the Portuguese, with São Tomé being a transit point to Brazil. The kingdom was not as welcoming as Kongo; it viewed the Portuguese with great suspicion and as an enemy. The Portuguese in the latter part of the 16th century tried to gain control of Ndongo but were defeated by the Mbundu. Ndongo experienced depopulation from slave raiding. The leaders established another state at Matamba, affiliated with Queen Nzinga, who put up a strong resistance to the Portuguese until coming to terms with them. The Portuguese settled along the coast as trade dealers, not venturing on conquest of the interior. Slavery wreaked havoc in the interior, with states initiating wars of conquest for captives. The Imbangala formed the slave-raiding state of Kasanje, a major source of slaves during the 17th and 18th centuries.\n\nThe birth of Islam opposite Somalia's Red Sea coast meant that Somali merchants and sailors living on the Arabian Peninsula gradually came under the influence of the new religion through their converted Arab Muslim trading partners. With the migration of Muslim families from the Islamic world to Somalia in the early centuries of Islam, and the peaceful conversion of the Somali population by Somali Muslim scholars in the following centuries, the ancient city-states eventually transformed into Islamic Mogadishu, Berbera, Zeila, Barawa and Merka, which were part of the \"Berber\" (the medieval Arab term for the ancestors of the modern Somalis) civilization. The city of Mogadishu came to be known as the \"City of Islam\" and controlled the East African gold trade for several centuries.\nDuring this period, sultanates such as the Ajuran Empire and the Sultanate of Mogadishu, and republics like Barawa, Merca and Hobyo and their respective ports flourished and had a lucrative foreign commerce with ships sailing to and coming from Arabia, India, Venice, Persia, Egypt, Portugal and as far away as China. Vasco da Gama, who passed by Mogadishu in the 15th century, noted that it was a large city with houses four or five stories high and big palaces in its centre, in addition to many mosques with cylindrical minarets.\n\nIn the 16th century, Duarte Barbosa noted that many ships from the Kingdom of Cambaya in modern-day India sailed to Mogadishu with cloth and spices, for which they in return received gold, wax, and ivory. Barbosa also highlighted the abundance of meat, wheat, barley, horses, and fruit in the coastal markets, which generated enormous wealth for the merchants. Mogadishu, the center of a thriving weaving industry known as \"toob benadir\" (specialized for the markets in Egypt and Syria), together with Merca and Barawa, served as a transit stop for Swahili merchants from Mombasa and Malindi and for the gold trade from Kilwa. Jewish merchants from the Strait of Hormuz brought their Indian textiles and fruit to the Somali coast to exchange for grain and wood.\n\nTrading relations were established with Malacca in the 15th century, with cloth, ambergris, and porcelain being the main commodities of the trade. Giraffes, zebras, and incense were exported to the Ming Empire of China, which established Somali merchants as leaders in the commerce between the Asia and Africa and influenced the Chinese language with borrowings from the Somali language in the process. Hindu merchants from Surat and southeast African merchants from Pate, seeking to bypass both the Portuguese blockade and Omani meddling, used the Somali ports of Merca and Barawa (which were out of the two powers' jurisdiction) to conduct their trade in safety and without any problems.\n\nThe Zagwe dynasty ruled many parts of modern Ethiopia and Eritrea from approximately 1137 to 1270. The name of the dynasty comes from the Cushitic speaking Agaw of northern Ethiopia. From 1270 AD and on for many centuries, the Solomonic dynasty ruled the Ethiopian Empire. \n\nIn the early 15th century Ethiopia sought to make diplomatic contact with European kingdoms for the first time since Aksumite times. A letter from King Henry IV of England to the Emperor of Abyssinia survives. In 1428, the Emperor Yeshaq I sent two emissaries to Alfonso V of Aragon, who sent return emissaries who failed to complete the return trip.\n\nThe first continuous relations with a European country began in 1508 with the Kingdom of Portugal under Emperor Lebna Dengel, who had just inherited the throne from his father. This proved to be an important development, for when the empire was subjected to the attacks of the Adal general and imam, Ahmad ibn Ibrahim al-Ghazi (called \"\"Grañ\"\", or \"the Left-handed\"), Portugal assisted the Ethiopian emperor by sending weapons and four hundred men, who helped his son Gelawdewos defeat Ahmad and re-establish his rule. This Abyssinian–Adal War was also one of the first proxy wars in the region as the Ottoman Empire, and Portugal took sides in the conflict.\n\nWhen Emperor Susenyos converted to Roman Catholicism in 1624, years of revolt and civil unrest followed resulting in thousands of deaths. The Jesuit missionaries had offended the Orthodox faith of the local Ethiopians, and on June 25, 1632, Susenyos's son, Emperor Fasilides, declared the state religion to again be Ethiopian Orthodox Christianity and expelled the Jesuit missionaries and other Europeans.\n\nBy 711 AD, the Umayyad Caliphate had conquered all of North Africa. By the 10th century, the majority of the population of North Africa was Muslim.\n\nBy the 9th century AD, the unity brought about by the Islamic conquest of North Africa and the expansion of Islamic culture came to an end. Conflict arose as to who should be the successor of the prophet. The Umayyads had initially taken control of the Caliphate, with their capital at Damascus. Later, the Abbasids had taken control, moving the capital to Baghdad. The Berber people, being independent in spirit and hostile to outside interference in their affairs and to Arab exclusivity in orthodox Islam, adopted Shi'ite and Kharijite Islam, both considered unorthodox and hostile to the authority of the Abbasid Caliphate. Numerous Kharijite kingdoms came and fell during the 8th and 9th centuries, asserting their independence from Baghdad. In the early 10th century, Shi'ite groups from Syria, claiming descent from Muhammad's daughter Fatimah, founded the Fatimid Dynasty in the Maghreb. By 950, they had conquered all of the Maghreb and by 969 all of Egypt. They had immediately broken away from Baghdad.\n\nIn an attempt to bring about a purer form of Islam among the Sanhaja Berbers, Abdallah ibn Yasin founded the Almoravid movement in present-day Mauritania and Western Sahara. The Sanhaja Berbers, like the Soninke, practiced an indigenous religion alongside Islam. Abdallah ibn Yasin found ready converts in the Lamtuna Sanhaja, who were dominated by the Soninke in the south and the Zenata Berbers in the north. By the 1040s, all of the Lamtuna was converted to the Almoravid movement. With the help of Yahya ibn Umar and his brother Abu Bakr ibn Umar, the sons of the Lamtuna chief, the Almoravids created an empire extending from the Sahel to the Mediterranean. After the death of Abdallah ibn Yassin and Yahya ibn Umar, Abu Bakr split the empire in half, between himself and Yusuf ibn Tashfin, because it was too big to be ruled by one individual. Abu Bakr took the south to continue fighting the Soninke, and Yusuf ibn Tashfin took the north, expanding it to southern Spain. The death of Abu Bakr in 1087 saw a breakdown of unity and increase military dissension in the south. This caused a re-expansion of the Soninke. The Almoravids were once held responsible for bringing down the Ghana Empire in 1076, but this view is no longer credited.\n\nDuring the 10th through 13th centuries, there was a large-scale movement of bedouins out of the Arabian Peninsula. About 1050, a quarter of a million Arab nomads from Egypt moved into the Maghreb. Those following the northern coast were referred to as Banu Hilal. Those going south of the Atlas Mountains were the Banu Sulaym. This movement spread the use of the Arabic language and hastened the decline of the Berber language and the Arabisation of North Africa. Later an Arabised Berber group, the Hawwara, went south to Nubia via Egypt.\n\nIn the 1140s, Abd al-Mu'min declared jihad on the Almoravids, charging them with decadence and corruption. He united the northern Berbers against the Almoravids, overthrowing them and forming the Almohad Empire. During this period, the Maghreb became thoroughly Islamised and saw the spread of literacy, the development of algebra, and the use of the number zero and decimals. By the 13th century, the Almohad states had split into three rival states. Muslim states were largely extinguished in the Iberian Peninsula by the Christian kingdoms of Castile, Aragon, and Portugal. Around 1415, Portugal engaged in a \"reconquista\" of North Africa by capturing Ceuta, and in later centuries Spain and Portugal acquired other ports on the North African coast. In 1492, at the end of the Granada War, Spain defeated Muslims in the Emirate of Granada, effectively ending eight centuries of Muslim domination in southern Iberia.\n\nPortugal and Spain took the ports of Tangiers, Algiers, Tripoli, and Tunis. This put them in direct competition with the Ottoman Empire, which re-took the ports using Turkish corsairs (pirates and privateers). The Turkish corsairs would use the ports for raiding Christian ships, a major source of booty for the towns. Technically, North Africa was under the control of the Ottoman Empire, but only the coastal towns were fully under Istanbul's control. Tripoli benefited from trade with Borno. The pashas of Tripoli traded horses, firearms, and armor via Fez with the sultans of the Bornu Empire for slaves.\n\nIn the 16th century, an Arab nomad tribe that claimed descent from Muhammad's daughter, the Saadis, conquered and united Morocco. They prevented the Ottoman Empire from reaching to the Atlantic and expelled Portugal from Morocco's western coast. Ahmad al-Mansur brought the state to the height of its power. He invaded Songhay in 1591, to control the gold trade, which had been diverted to the western coast of Africa for European ships and to the east, to Tunis. Morocco's hold on Songhay diminished in the 17th century. In 1603, after Ahmad's death, the kingdom split into the two sultanates of Fes and Marrakesh. Later it was reunited by Moulay al-Rashid, founder of the Alaouite Dynasty (1672–1727). His brother and successor, Ismail ibn Sharif (1672–1727), strengthened the unity of the country by importing slaves from the Sudan to build up the military.\n\nIn 642 AD, the Rashidun Caliphate conquered Byzantine Egypt.\n\nEgypt under the Fatimid Caliphate was prosperous. Dams and canals were repaired, and wheat, barley, flax, and cotton production increased. Egypt became a major producer of linen and cotton cloth. Its Mediterranean and Red Sea trade increased. Egypt also minted a gold currency called the Fatimid dinar, which was used for international trade. The bulk of revenues came from taxing the fellahin (peasant farmers), and taxes were high. Tax collecting was leased to Berber overlords, who were soldiers who had taken part in the Fatimid conquest in 969 AD. The overlords paid a share to the caliphs and retained what was left. Eventually, they became landlords and constituted a settled land aristocracy.\n\nTo fill the military ranks, Mamluk Turkish slave cavalry and Sudanese slave infantry were used. Berber freemen were also recruited. In the 1150s, tax revenues from farms diminished. The soldiers revolted and wreaked havoc in the countryside, slowed trade, and diminished the power and authority of the Fatimid caliphs.\n\nDuring the 1160s, Fatimid Egypt came under threat from European crusaders. Out of this threat, a Kurdish general named Ṣalāḥ ad-Dīn Yūsuf ibn Ayyūb (Saladin), with a small band of professional soldiers, emerged as an outstanding Muslim defender. Saladin defeated the Christian crusaders at Egypt's borders and recaptured Jerusalem in 1187. On the death of Al-Adid, the last Fatimid caliph, in 1171, Saladin became the ruler of Egypt, ushering in the Ayyubid Dynasty. Under his rule, Egypt returned to Sunni Islam, Cairo became an important center of Arab Islamic learning, and Mamluk slaves were increasingly recruited from Turkey and southern Russia for military service. Support for the military was tied to the \"iqta\", a form of land taxation in which soldiers were given ownership in return for military service.\n\nOver time, Mamluk slave soldiers became a very powerful landed aristocracy, to the point of getting rid of the Ayyubid dynasty in 1250 and establishing a Mamluk dynasty. The more powerful Mamluks were referred to as \"amirs\". For 250 years, Mamluks controlled all of Egypt under a military dictatorship. Egypt extended her territories to Syria and Palestine, thwarted the crusaders, and halted a Mongol invasion in 1260 at the Battle of Ain Jalut. Mamluk Egypt came to be viewed as a protector of Islam, and of Medina and Mecca. Eventually the \"iqta\" system declined and proved unreliable for providing an adequate military. The Mamluks started viewing their \"iqta\" as hereditary and became attuned to urban living. Farm production declined, and dams and canals lapsed into disrepair. Mamluk military skill and technology did not keep pace with new technology of handguns and cannons.\n\nWith the rise of the Ottoman Empire, Egypt was easily defeated. In 1517, at the end of an Ottoman–Mamluk War, Egypt became part of the Ottoman Empire. The Istanbul government revived the \"iqta\" system. Trade was reestablished in the Red Sea, but it could not completely connect with the Indian Ocean trade because of growing Portuguese presence. During the 17th and 18th centuries, hereditary Mamluks regained power. The leading Mamluks were referred to as \"beys\". Pashas, or viceroys, represented the Istanbul government in name only, operating independently. During the 18th century, dynasties of pashas became established. The government was weak and corrupt.\n\nIn 1798, Napoleon invaded Egypt. The local forces had little ability to resist the French conquest. However, the British Empire and the Ottoman Empire were able to remove French occupation in 1801. These events marked the beginning of a 19th-century Anglo-Franco rivalry over Egypt.\n\nAfter Ezana of Aksum sacked Meroe, people associated with the site of Ballana moved into Nubia from the southwest and founded three kingdoms: Makuria, Nobatia, and Alodia. They would rule for 200 years. Makuria was above the third cataract, along the Dongola Reach with its capital at Dongola. Nobadia was to the north with its capital at Faras, and Alodia was to the south with its capital at Soba. Makuria eventually absorbed Nobadia. The people of the region converted to Monophysite Christianity around 500 to 600 CE. The church initially started writing in Coptic, then in Greek, and finally in Old Nubian, a Nilo-Saharan language. The church was aligned with the Egyptian Coptic Church.\n\nBy 641, Egypt was conquered by the Rashidun Caliphate. This effectively blocked Christian Nubia and Aksum from Mediterranean Christendom. In 651–652, Arabs from Egypt invaded Christian Nubia. Nubian archers soundly defeated the invaders. The Baqt (or Bakt) Treaty was drawn, recognizing Christian Nubia and regulating trade. The treaty controlled relations between Christian Nubia and Islamic Egypt for almost six hundred years.\n\nBy the 13th century, Christian Nubia began its decline. The authority of the monarchy was diminished by the church and nobility. Arab bedouin tribes began to infiltrate Nubia, causing further havoc. \"Fakirs\" (holy men) practicing Sufism introduced Islam into Nubia. By 1366, Nubia had become divided into petty fiefdoms when it was invaded by Mamluks. During the 15th century, Nubia was open to Arab immigration. Arab nomads intermingled with the population and introduced the Arab culture and the Arabic language. By the 16th century, Makuria and Nobadia had been Islamized. During the 16th century, Abdallah Jamma headed an Arab confederation that destroyed Soba, capital of Alodia, the last holdout of Christian Nubia. Later Alodia would fall under the Funj Sultanate.\n\nDuring the 15th century, Funj herders migrated north to Alodia and occupied it. Between 1504 and 1505, the kingdom expanded, reaching its peak and establishing its capital at Sennar under Badi II Abu Daqn (c. 1644–1680). By the end of the 16th century, the Funj had converted to Islam. They pushed their empire westward to Kordofan. They expanded eastward, but were halted by Ethiopia. They controlled Nubia down to the 3rd Cataract. The economy depended on captured enemies to fill the army and on merchants travelling through Sennar. Under Badi IV (1724–1762), the army turned on the king, making him nothing but a figurehead. In 1821, the Funj were conquered by Muhammad Ali (1805–1849), Pasha of Egypt.\n\nSettlements of Bantu-speaking peoples who were iron-using agriculturists and herdsmen were present south of the Limpopo River by the 4th or 5th century CE, displacing and absorbing the original Khoisan speakers. They slowly moved south, and the earliest ironworks in modern-day KwaZulu-Natal Province are believed to date from around 1050. The southernmost group was the Xhosa people, whose language incorporates certain linguistic traits from the earlier Khoi-San people, reaching the Great Fish River in today's Eastern Cape Province.\n\nThe Kingdom of Mapungubwe was the first state in Southern Africa, with its capital at Mapungubwe. The state arose in the 12th century CE. Its wealth came from controlling the trade in ivory from the Limpopo Valley, copper from the mountains of northern Transvaal, and gold from the Zimbabwe Plateau between the Limpopo and Zambezi rivers, with the Swahili merchants at Chibuene. By the mid-13th century, Mapungubwe was abandoned.\n\nAfter the decline of Mapungubwe, Great Zimbabwe rose on the Zimbabwe Plateau. \"Zimbabwe\" means stone building. Great Zimbabwe was the first city in Southern Africa and was the center of an empire, consolidating lesser Shona polities. Stone building was inherited from Mapungubwe. These building techniques were enhanced and came into maturity at Great Zimbabwe, represented by the wall of the Great Enclosure. The dry-stack stone masonry technology was also used to build smaller compounds in the area. Great Zimbabwe flourished by trading with Swahili Kilwa and Sofala. The rise of Great Zimbabwe parallels the rise of Kilwa. Great Zimbabwe was a major source of gold. Its royal court lived in luxury, wore Indian cotton, surrounded themselves with copper and gold ornaments, and ate on plates from as far away as Persia and China. Around the 1420s and 1430s, Great Zimbabwe was on decline. The city was abandoned by 1450. Some have attributed the decline to the rise of the trading town Ingombe Ilede.\n\nA new chapter of Shona history ensued. Nyatsimba Mutota, a northern Shona king of the Karanga, engaged in conquest. He and his son Mutope conquered the Zimbabwe Plateau, going through Mozambique to the east coast, linking the empire to the coastal trade. They called their empire \"Wilayatu 'l Mu'anamutapah\" or \"mwanamutapa\" (Lord of the Plundered Lands), or the Kingdom of Mutapa. \"Monomotapa\" was the Portuguese corruption. They did not build stone structures; the northern Shonas had no traditions of building in stone. After the death of Matope in 1480, the empire split into two small empires: Torwa in the south and Mutapa in the north. The split occurred over rivalry from two Shona lords, Changa and Togwa, with the \"mwanamutapa\" line. Changa was able to acquire the south, forming the Kingdom of Butua with its capital at Khami.\n\nThe Mutapa Empire continued in the north under the \"mwenemutapa\" line. During the 16th century the Portuguese were able to establish permanent markets up the Zambezi River in an attempt to gain political and military control of Mutapa. They were partially successful. In 1628, a decisive battle allowed them to put a puppet \"mwanamutapa\" named Mavura, who signed treaties that gave favorable mineral export rights to the Portuguese. The Portuguese were successful in destroying the \"mwanamutapa\" system of government and undermining trade. By 1667, Mutapa was in decay. Chiefs would not allow digging for gold because of fear of Portuguese theft, and the population declined.\n\nThe Kingdom of Butua was ruled by a \"changamire\", a title derived from the founder, Changa. Later it became the Rozwi Empire. The Portuguese tried to gain a foothold but were thrown out of the region in 1693, by Changamire Dombo. The 17th century was a period of peace and prosperity. The Rozwi Empire fell into ruins in the 1830s from invading Nguni from Natal.\n\nBy 1500 AD, most of southern Africa had established states. In northwestern Namibia, the Ovambo engaged in farming and the Herero engaged in herding. As cattle numbers increased, the Herero moved southward to central Namibia for grazing land. A related group, the Ovambanderu, expanded to Ghanzi in northwestern Botswana. The Nama, a Khoi-speaking, sheep-raising group, moved northward and came into contact with the Herero; this would set the stage for much conflict between the two groups. The expanding Lozi states pushed the Mbukushu, Subiya, and Yei to Botei, Okavango, and Chobe in northern Botswana.\n\nThe development of Sotho–Tswana states based on the highveld, south of the Limpopo River, began around 1000 CE. The chief's power rested on cattle and his connection to the ancestor. This can be seen in the Toutswemogala Hill settlements with stone foundations and stone walls, north of the highveld and south of the Vaal River. Northwest of the Vaal River developed early Tswana states centered on towns of thousands of people. When disagreements or rivalry arose, different groups moved to form their own states.\n\nSoutheast of the Drakensberg mountains lived Nguni-speaking peoples (Zulu, Xhosa, Swazi, and Ndebele). They too engaged in state building, with new states developing from rivalry, disagreements, and population pressure causing movement into new regions. This 19th-century process of warfare, state building and migration later became known as the Mfecane (Nguni) or Difaqane (Sotho). Its major catalyst was the consolidation of the Zulu Kingdom. They were metalworkers, cultivators of millet, and cattle herders.\n\nThe Khoisan lived in the southwestern Cape Province, where winter rainfall is plentiful. Earlier Khoisan populations were absorbed by Bantu peoples, such as the Sotho and Nguni, but the Bantu expansion stopped at the region with winter rainfall. Some Bantu languages have incorporated the click consonant of the Khoisan languages. The Khoisan traded with their Bantu neighbors, providing cattle, sheep, and hunted items. In return, their Bantu speaking neighbors traded copper, iron, and tobacco.\n\nBy the 16th century, the Dutch East India Company established a replenishing station at Table Bay for restocking water and purchasing meat from the Khoikhoi. The Khoikhoi received copper, iron, tobacco, and beads in exchange. In order to control the price of meat and stock and make service more consistent, the Dutch established a permanent settlement at Table Bay in 1652. They grew fresh fruit and vegetables and established a hospital for sick sailors. To increase produce, the Dutch decided to increase the number of farms at Table Bay by encouraging freeburgher \"boers\" (farmers) on lands worked initially by slaves from West Africa. The land was taken from Khoikhoi grazing land, triggering the first Khoikhoi-Dutch war in 1659. No victors emerged, but the Dutch assumed a \"right of conquest\" by which they claimed all of the cape. In a series of wars pitting the Khoikhoi against each other, the Boers assumed all Khoikhoi land and claimed all their cattle. The second Khoikoi-Dutch war (1673–1677) was a cattle raid. The Khoikhoi also died in thousands from European diseases.\n\nBy the 18th century, the cape colony had grown, with slaves coming from Madagascar, Mozambique, and Indonesia. The settlement also started to expand northward, but Khoikhoi resistance, raids, and guerrilla warfare slowed the expansion during the 18th century. Boers who started to practice pastoralism were known as \"trekboers\". A common source of \"trekboer\" labor was orphan children who were captured during raids and whose parents had been killed.\n\nAccording to the theory of recent African origin of modern humans, the mainstream position held within the scientific community, all humans originate from either Southeast Africa or the Horn of Africa. During the first millennium CE, Nilotic and Bantu-speaking peoples moved into the region.\n\nFollowing the Bantu Migration, on the coastal section of Southeast Africa, a mixed Bantu community developed through contact with Muslim Arab and Persian traders, leading to the development of the mixed Arab, Persian and African Swahili City States. The Swahili culture that emerged from these exchanges evinces many Arab and Islamic influences not seen in traditional Bantu culture, as do the many Afro-Arab members of the Bantu Swahili people. With its original speech community centered on the coastal parts of Tanzania (particularly Zanzibar) and Kenya—a seaboard referred to as the Swahili Coast—the Bantu Swahili language contains many Arabic language loan-words as a consequence of these interactions.\n\nThe earliest Bantu inhabitants of the Southeast coast of Kenya and Tanzania encountered by these later Arab and Persian settlers have been variously identified with the trading settlements of Rhapta, Azania and Menouthias referenced in early Greek and Chinese writings from 50 AD to 500 AD, ultimately giving rise to the name for Tanzania. These early writings perhaps document the first wave of Bantu settlers to reach Southeast Africa during their migration.\n\nHistorically, the Swahili people could be found as far north as northern Kenya and as far south as the Ruvuma River in Mozambique. Arab geographers referred to the Swahili coast as the land of the \"zanj\" (blacks).\n\nAlthough once believed to be the descendants of Persian colonists, the ancient Swahili are now recognized by most historians, historical linguists, and archaeologists as a Bantu people who had sustained important interactions with Muslim merchants, beginning in the late 7th and early 8th centuries AD.\nMedieval Swahili kingdoms are known to have had island trade ports, described by Greek historians as \"metropolises\", and to have established regular trade routes with the Islamic world and Asia. Ports such as Mombasa, Zanzibar, and Kilwa were known to Chinese sailors under Zheng He and medieval Islamic geographers such as the Berber traveller Abu Abdullah ibn Battuta. The main Swahili exports were ivory, slaves, and gold. They traded with Arabia, India, Persia, and China.\n\nThe Portuguese arrived in 1498. On a mission to economically control and Christianize the Swahili coast, the Portuguese attacked Kilwa first in 1505 and other cities later. Because of Swahili resistance, the Portuguese attempt at establishing commercial control was never successful. By the late 17th century, Portuguese authority on the Swahili coast began to diminish. With the help of Omani Arabs, by 1729 the Portuguese presence had been removed. The Swahili coast eventually became part of the Sultanate of Oman. Trade recovered, but it did not regain the levels of the past.\n\nThe Urewe culture developed and spread in and around the Lake Victoria region of Africa during the African Iron Age. The culture's earliest dated artifacts are located in the Kagera Region of Tanzania, and it extended as far west as the Kivu region of the Democratic Republic of the Congo, as far east as the Nyanza and Western provinces of Kenya, and north into Uganda, Rwanda and Burundi. Sites from the Urewe culture date from the Early Iron Age, from the 5th century BC to the 6th century AD.\n\nThe origins of the Urewe culture are ultimately in the Bantu expansion originating in Cameroon. Research into early Iron Age civilizations in Sub-Saharan Africa has been undertaken concurrently with studies on African linguistics on Bantu expansion. The Urewe culture may correspond to the Eastern subfamily of Bantu languages, spoken by the descendants of the first wave of Bantu peoples to settle East Africa. At first sight, Urewe seems to be a fully developed civilization recognizable through its distinctive, stylish earthenware and highly technical and sophisticated iron working techniques. Given our current level of knowledge, neither seems to have developed or altered for nearly 2,000 years. However, minor local variations in the ceramic ware can be observed.\n\nUrewe is the name of the site in Kenya brought to prominence through the publication in 1948 of Mary Leakey's archaeological findings. She described the early Iron Age period in the Great Lakes region in Central East Africa around Lake Victoria.\n\nMadagascar was apparently first settled by Austronesian speakers from Southeast Asia before the 6th century AD and subsequently by Bantu speakers from the east African mainland in the 6th or 7th century, according to archaeological and linguistic data. The Austronesians introduced banana and rice cultivation, and the Bantu speakers introduced cattle and other farming practices. About the year 1000, Arab and Indian trade settlement were started in northern Madagascar to exploit the Indian Ocean trade. By the 14th century, Islam was introduced on the island by traders. Madagascar functioned in the East African medieval period as a contact port for the other Swahili seaport city-states such as Sofala, Kilwa, Mombasa, and Zanzibar.\n\nSeveral kingdoms emerged after the 15th century: the Sakalava Kingdom (16th century) on the west coast, Tsitambala Kingdom (17th century) on the east coast, and Merina (15th century) in the central highlands. By the 19th century, Merina controlled the whole island. In 1500, the Portuguese were the first Europeans on the island, raiding the trading settlements.\n\nThe British and later the French arrived. During the latter part of the 17th century, Madagascar was a popular transit point for pirates. Radama I (1810–1828) invited Christian missionaries in the early 19th century. Queen Ranavalona I \"the Cruel\" (1828–1861) banned the practice of Christianity in the kingdom, and an estimated 150,000 Christians perished. Under Radama II (1861–1863), Madagascar took a French orientation, with great commercial concession given to the French. In 1895, in the second Franco-Hova War, the French invaded Madagascar, taking over Antsiranana (Diego Suarez) and declaring Madagascar a protectorate.\n\nBetween the 14th and 15th centuries, large Southeast African kingdoms and states emerged, such as the Buganda and Karagwe Kingdoms of Uganda and Tanzania.\n\nBy 1000 AD, numerous states had arisen on the Lake Plateau among the Great Lakes of East Africa. Cattle herding, cereal growing, and banana cultivation were the economic mainstays of these states. The Ntusi and Bigo earthworks are representative of one of the first states, the Bunyoro kingdom, which oral tradition stipulates was part of the Empire of Kitara that dominated the whole Lakes region. A Luo ethnic elite, from the Bito clan, ruled over the Bantu-speaking Nyoro people. The society was essentially Nyoro in its culture, based on the evidence from pottery, settlement patterns, and economic specialization.\n\nThe Bito clan claimed legitimacy by being descended from the Bachwezi clan, who were said to have ruled the Empire of Kitara. However, very little is known about Kitara; some scholars even question its historical existence. Most founding leaders of the various polities in the lake region seem to have claimed descent from the Bachwezi. There are now 13 million Tara who are part of the second African loss,(Nafi and Uma are two losses).\n\nThe Buganda kingdom was founded by Kato Kintu around the 14th century AD. Kato Kintu may have migrated to the northwest of Lake Victoria as early as 1000 BC. Buganda was ruled by the \"kabaka\" with a \"bataka\" composed of the clan heads. Over time, the \"kabakas\" diluted the authority of the \"bataka\", with Buganda becoming a centralized monarchy. By the 16th century, Buganda was engaged in expansion but had a serious rival in Bunyoro. By the 1870s, Buganda was a wealthy nation-state. The \"kabaka\" ruled with his \"Lukiko\" (council of ministers). Buganda had a naval fleet of a hundred vessels, each manned by thirty men. Buganda supplanted Bunyoro as the most important state in the region. However, by the early 20th century, Buganda became a province of the British Uganda Protectorate.\n\nSoutheast of Bunyoro, near Lake Kivu at the bottom of the western rift, the Kingdom of Rwanda was founded, perhaps during the 17th century. Tutsi (BaTutsi) pastoralists formed the elite, with a king called the \"mwami\". The Hutu (BaHutu) were farmers. Both groups spoke the same language, but there were strict social norms against marrying each other and interaction. According to oral tradition, the Kingdom of Rwanda was founded by Mwami Ruganzu II (Ruganzu Ndori) (c. 1600–1624), with his capital near Kigali. It took 200 years to attain a truly centralized kingdom under Mwami Kigeli IV (Kigeri Rwabugiri) (1840–1895). Subjugation of the Hutu proved more difficult than subduing the Tutsi. The last Tutsi chief gave up to Mwami Mutara II (Mutara Rwogera) (1802–1853) in 1852, but the last Hutu holdout was conquered in the 1920s by Mwami Yuhi V (Yuli Musinga) (1896–1931).\n\nSouth of the Kingdom of Rwanda was the Kingdom of Burundi. It was founded by the Tutsi chief Ntare Rushatsi (c. 1657–1705). Like Rwanda, Burundi was built on cattle raised by Tutsi pastoralists, crops from Hutu farmers, conquest, and political innovations. Under Mwami Ntari Rugaamba (c. 1795–1852), Burundi pursued an aggressive expansionist policy, one based more on diplomacy than force.\n\nThe Maravi claimed descent from Karonga (\"kalonga\"), who took that title as king. The Maravi connected Central Africa to the east coastal trade, with Swahili Kilwa. By the 17th century, the Maravi Empire encompassed all the area between Lake Malawi and the mouth of the Zambezi River. The \"karonga\" was Mzura, who did much to extend the empire. Mzura made a pact with the Portuguese to establish a 4,000-man army to attack the Shona in return for aid in defeating his rival Lundi, a chief of the Zimba. In 1623, he turned on the Portuguese and assisted the Shona. In 1640, he welcomed back the Portuguese for trade. The Maravi Empire did not long survive the death of Mzura. By the 18th century, it had broken into its previous polities.\n\nThe Ghana Empire may have been an established kingdom as early as the 8th century AD, founded among the Soninke by Dinge Cisse. Ghana was first mentioned by Arab geographer Al-Farazi in the late 8th century. Ghana was inhabited by urban dwellers and rural farmers. The urban dwellers were the administrators of the empire, who were Muslims, and the \"Ghana\" (king), who practiced traditional religion. Two towns existed, one where the Muslim administrators and Berber-Arabs lived, which was connected by a stone-paved road to the king's residence. The rural dwellers lived in villages, which joined together into broader polities that pledged loyalty to the \"Ghana.\" The \"Ghana\" was viewed as divine, and his physical well-being reflected on the whole society. Ghana converted to Islam around 1050, after conquering Aoudaghost.\n\nThe Ghana Empire grew wealthy by taxing the trans-Saharan trade that linked Tiaret and Sijilmasa to Aoudaghost. Ghana controlled access to the goldfields of Bambouk, southeast of Koumbi Saleh. A percentage of salt and gold going through its territory was taken. The empire was not involved in production.\n\nBy the 11th century, Ghana was in decline. It was once thought that the sacking of Koumbi Saleh by Berbers under the Almoravid dynasty in 1076 was the cause. This is no longer accepted. Several alternative explanations are cited. One important reason is the transfer of the gold trade east to the Niger River and the Taghaza Trail, and Ghana's consequent economic decline. Another reason cited is political instability through rivalry among the different hereditary polities.\nThe empire came to an end in 1230, when Takrur in northern Senegal took over the capital.\n\nThe Mali Empire began in the 13th century AD, when a Mande (Mandingo) leader, Sundiata (Lord Lion) of the Keita clan, defeated Soumaoro Kanté, king of the Sosso or southern Soninke, at the Battle of Kirina in c. 1235. Sundiata continued his conquest from the fertile forests and Niger Valley, east to the Niger Bend, north into the Sahara, and west to the Atlantic Ocean, absorbing the remains of the Ghana Empire. Sundiata took on the title of \"mansa\". He established the capital of his empire at Niani.\n\nAlthough the salt and gold trade continued to be important to the Mali Empire, agriculture and pastoralism was also critical. The growing of sorghum, millet, and rice was a vital function. On the northern borders of the Sahel, grazing cattle, sheep, goats, and camels were major activities. Mande society was organize around the village and land. A cluster of villages was called a \"kafu\", ruled by a \"farma\". The \"farma\" paid tribute to the \"mansa\". A dedicated army of elite cavalry and infantry maintained order, commanded by the royal court. A formidable force could be raised from tributary regions, if necessary.\n\nConversion to Islam was a gradual process. The power of the \"mansa\" depended on upholding traditional beliefs and a spiritual foundation of power. Sundiata initially kept Islam at bay. Later \"mansas\" were devout Muslims but still acknowledged traditional deities and took part in traditional rituals and festivals, which were important to the Mande. Islam became a court religion under Sundiata's son Uli I (1225–1270). \"Mansa\" Uli made a pilgrimage to Mecca, becoming recognized within the Muslim world. The court was staffed with literate Muslims as secretaries and accountants. Muslim traveller Ibn Battuta left vivid descriptions of the empire.\n\nMali reached the peak of its power and extent in the 14th century, when \"Mansa\" Musa (1312–1337) made his famous \"hajj\" to Mecca with 500 slaves, each holding a bar of gold worth 500 mitqals. \"Mansa\" Musa's \"hajj\" devalued gold in Mamluk Egypt for a decade. He made a great impression on the minds of the Muslim and European world. He invited scholars and architects like Ishal al-Tuedjin (al-Sahili) to further integrate Mali into the Islamic world.\n\nThe Mali Empire saw an expansion of learning and literacy. In 1285, Sakura, a freed slave, usurped the throne. This \"mansa\" drove the Tuareg out of Timbuktu and established it as a center of learning and commerce. The book trade increased, and book copying became a very respectable and profitable profession. Timbuktu and Djenné became important centers of learning within the Islamic world.\n\nAfter the reign of Mansa Suleyman (1341–1360), Mali began its spiral downward. Mossi cavalry raided the exposed southern border. Tuareg harassed the northern border in order to retake Timbuktu. Fulani (Fulbe) eroded Mali's authority in the west by establishing the independent Imamate of Futa Toro, a successor to the kingdom of Takrur. Serer and Wolof alliances were broken. In 1545 to 1546, the Songhai Empire took Niani. After 1599, the empire lost the Bambouk goldfields and disintegrated into petty polities.\n\nThe Songhai people are descended from fishermen on the Middle Niger River. They established their capital at Kukiya in the 9th century AD and at Gao in the 12th century. The Songhai speak a Nilo-Saharan language.\n\nSonni Ali, a Songhai, began his conquest by capturing Timbuktu in 1468 from the Tuareg. He extended the empire to the north, deep into the desert, pushed the Mossi further south of the Niger, and expanded southwest to Djenne. His army consisted of cavalry and a fleet of canoes. Sonni Ali was not a Muslim, and he was portrayed negatively by Berber-Arab scholars, especially for attacking Muslim Timbuktu. After his death in 1492, his heirs were deposed by General Muhammad Ture, a Muslim of Soninke origins\n\nMuhammad Ture (1493–1528) founded the Askiya Dynasty, \"askiya\" being the title of the king. He consolidated the conquests of Sonni Ali. Islam was used to extend his authority by declaring jihad on the Mossi, reviving the trans-Saharan trade, and having the Abbasid \"shadow\" caliph in Cairo declare him as caliph of Sudan. He established Timbuktu as a great center of Islamic learning. Muhammad Ture expanded the empire by pushing the Tuareg north, capturing Aïr in the east, and capturing salt-producing Taghaza. He brought the Hausa states into the Songhay trading network. He further centralized the administration of the empire by selecting administrators from loyal servants and families and assigning them to conquered territories. They were responsible for raising local militias. Centralization made Songhay very stable, even during dynastic disputes. Leo Africanus left vivid descriptions of the empire under Askiya Muhammad. Askiya Muhammad was deposed by his son in 1528. After much rivalry, Muhammad Ture's last son Askiya Daoud (1529–1582) assumed the throne.\n\nIn 1591, Morocco invaded the Songhai Empire under Ahmad al-Mansur of the Saadi Dynasty in order to secure the goldfields of the Sahel. At the Battle of Tondibi, the Songhai army was defeated. The Moroccans captured Djenne, Gao, and Timbuktu, but they were unable to secure the whole region. Askiya Nuhu and the Songhay army regrouped at Dendi in the heart of Songhai territory where a spirited guerrilla resistance sapped the resources of the Moroccans, who were dependent upon constant resupply from Morocco. Songhai split into several states during the 17th century.\n\nMorocco found its venture unprofitable. The gold trade had been diverted to Europeans on the coast. Most of the trans-Saharan trade was now diverted east to Bornu. Expensive equipment purchased with gold had to be sent across the Sahara, an unsustainable scenario. The Moroccans who remained married into the population and were referred to as \"Arma\" or \"Ruma\". They established themselves at Timbuktu as a military caste with various fiefs, independent from Morocco. Amid the chaos, other groups began to assert themselves, including the Fulani of Futa Tooro who encroached from the west. The Bambara Empire, one of the states that broke from Songhai, sacked Gao. In 1737, the Tuareg massacred the \"Arma\".\n\nThe Fulani were migratory people. They moved from Mauritania and settled in Futa Tooro, Futa Djallon, and subsequently throughout the rest of West Africa. By the 14th century CE, they had converted to Islam. During the 16th century, they established themselves at Macina in southern Mali. During the 1670s, they declared jihads on non-Muslims. Several states were formed from these jihadist wars, at Futa Toro, Futa Djallon, Macina, Oualia, and Bundu. The most important of these states was the Sokoto Caliphate or Fulani Empire.\n\nIn the city of Gobir, Usman dan Fodio (1754–1817) accused the Hausa leadership of practicing an impure version of Islam and of being morally corrupt. In 1804, he launched the Fulani War as a jihad among a population that was restless about high taxes and discontented with its leaders. Jihad fever swept northern Nigeria, with strong support among both the Fulani and the Hausa. Usman created an empire that included parts of northern Nigeria, Benin, and Cameroon, with Sokoto as its capital. He retired to teach and write and handed the empire to his son Muhammed Bello. The Sokoto Caliphate lasted until 1903 when the British conquered northern Nigeria.\n\nThe Akan speak a Kwa language. The speakers of Kwa languages are believed to have come from East/Central Africa, before settling in the Sahel. By the 12th century, the Akan Kingdom of Bonoman (Bono State) was established. During the 13th century, when the gold mines in modern-day Mali started to dry up, Bonoman and later other Akan states began to rise to prominence as the major players in the Gold trade. It was Bonoman and other Akan kingdoms like Denkyira, Akyem, Akwamu which were the predecessors to what became the all-powerful Empire of Ashanti. When and how the Ashante got to their present location is debatable. What is known is that by the 17th century an Akan people were identified as living in a state called Kwaaman. The location of the state was north of Lake Bosomtwe. The state's revenue was mainly derived from trading in gold and kola nuts and clearing forest to plant yams. They built towns between the Pra and Ofin rivers. They formed alliances for defense and paid tribute to Denkyira one of the more powerful Akan states at that time along with Adansi and Akwamu. During the 16th century, Ashante society experienced sudden changes, including population growth because of cultivation of New World plants such as cassava and maize and an increase in the gold trade between the coast and the north.\n\nBy the 17th century, Osei Kofi Tutu I (c. 1695–1717), with help of Okomfo Anokye, unified what became the Ashante into a confederation with the Golden Stool as a symbol of their unity and spirit. Osei Tutu engaged in a massive territorial expansion. He built up the Ashante army based on the Akan state of Akwamu, introducing new organization and turning a disciplined militia into an effective fighting machine. In 1701, the Ashante conquered Denkyira, giving them access to the coastal trade with Europeans, especially the Dutch. Opoku Ware I (1720–1745) engaged in further expansion, adding other southern Akan states to the growing empire. He turned north adding Techiman, Banda, Gyaaman, and Gonja, states on the Black Volta. Between 1744 and 1745, \"Asantehene\" Opoku attacked the powerful northern state of Dagomba, gaining control of the important middle Niger trade routes. Kusi Obodom (1750–1764) succeeded Opoku. He solidified all the newly won territories. Osei Kwadwo (1777–1803) imposed administrative reforms that allowed the empire to be governed effectively and to continue its military expansion. Osei Kwame Panyin (1777–1803), Osei Tutu Kwame (1804–1807), and Osei Bonsu (1807–1824) continued territorial consolidation and expansion. The Ashante Empire included all of present-day Ghana and large parts of the Ivory Coast.\n\nThe \"ashantehene\" inherited his position from his mother. He was assisted at the capital, Kumasi, by a civil service of men talented in trade, diplomacy, and the military, with a head called the \"Gyaasehene\". Men from Arabia, Sudan, and Europe were employed in the civil service, all of them appointed by the \"ashantehene\". At the capital and in other towns, the \"ankobia\" or special police were used as bodyguards to the \"ashantehene\", as sources of intelligence, and to suppress rebellion. Communication throughout the empire was maintained via a network of well-kept roads from the coast to the middle Niger and linking together other trade cities.\n\nFor most of the 19th century, the Ashante Empire remained powerful. It was later destroyed in 1900 by British superior weaponry and organization following the four Anglo-Ashanti wars.\n\nThe Dahomey Kingdom was founded in the early 17th century when the Aja people of the Allada kingdom moved northward and settled among the Fon. They began to assert their power a few years later. In so doing they established the Kingdom of Dahomey, with its capital at Agbome. King Houegbadja (c. 1645–1685) organized Dahomey into a powerful centralized state. He declared all lands to be owned of the king and subject to taxation. Primogeniture in the kingship was established, neutralizing all input from village chiefs. A \"cult of kingship\" was established. A captive slave would be sacrificed annually to honor the royal ancestors. During the 1720s, the slave-trading states of Whydah and Allada were taken, giving Dahomey direct access to the slave coast and trade with Europeans. King Agadja (1708–1740) attempted to end the slave trade by keeping the slaves on plantations producing palm oil, but the European profits on slaves and Dahomey's dependency on firearms were too great. In 1730, under king Agaja, Dahomey was conquered by the Oyo Empire, and Dahomey had to pay tribute. Taxes on slaves were mostly paid in cowrie shells. During the 19th century, palm oil was the main trading commodity. France conquered Dahomey during the Second Franco-Dahomean War (1892–1894) and established a colonial government there. Most of the troops who fought against Dahomey were native Africans.\n\nTraditionally, the Yoruba people viewed themselves as the inhabitants of a united empire, in contrast to the situation today, in which \"Yoruba\" is the cultural-linguistic designation for speakers of a language in the Niger–Congo family. The name comes from a Hausa word to refer to the Oyo Empire. The first Yoruba state was Ile-Ife, said to have been founded around 1000 AD by a supernatural figure, the first \"oni\" Oduduwa. Oduduwa's sons would be the founders of the different city-states of the Yoruba, and his daughters would become the mothers of the various Yoruba \"obas\", or kings. Yoruba city-states were usually governed by an \"oba\" and an \"iwarefa\", a council of chiefs who advised the \"oba.\" by the 18th century, the Yoruba city-states formed a loose confederation, with the \"Oni\" of Ife as the head and Ife as the capital. As time went on, the individual city-states became more powerful with their \"obas\" assuming more powerful spiritual positions and diluting the authority of the \"Oni\" of Ife. Rivalry became intense among the city-states.\n\nThe Oyo Empire rose in the 16th century. The Oyo state had been conquered in 1550 by the kingdom of Nupe, which was in possession of cavalry, an important tactical advantage. The \"alafin\" (king) of Oyo was sent into exile. After returning, \"Alafin\" Orompoto (c. 1560–1580) built up an army based on heavily armed cavalry and long-service troops. This made them invincible in combat on the northern grasslands and in the thinly wooded forests. By the end of the 16th century, Oyo had added the western region of the Niger to the hills of Togo, the Yoruba of Ketu, Dahomey, and the Fon nation.\n\nA governing council served the empire, with clear executive divisions. Each acquired region was assigned a local administrator. Families served in king-making capacities. Oyo, as a northern Yoruba kingdom, served as middle-man in the north-south trade and connecting the eastern forest of Guinea with the western and central Sudan, the Sahara, and North Africa. The Yoruba manufactured cloth, ironware, and pottery, which were exchanged for salt, leather, and most importantly horses from the Sudan to maintain the cavalry. Oyo remained strong for two hundred years. It became a protectorate of Great Britain in 1888, before further fragmenting into warring factions. The Oyo state ceased to exist as any sort of power in 1896.\n\nThe Kwa Niger–Congo speaking Edo people had established the Benin Empire by the middle of the 15th century. It was engaged in political expansion and consolidation from its very beginning. Under \"Oba\" (king) Ewuare (c. 1450–1480 AD), the state was organized for conquest. He solidified central authority and initiated 30 years of war with his neighbors. At his death, the Benin Empire extended to Dahomey in the west, to the Niger Delta in the east, along the west African coast, and to the Yoruba towns in the north.\n\nEwuare's grandson \"Oba\" Esigie (1504–1550) eroded the power of the \"uzama\" (state council) and increased contact and trade with Europeans, especially with the Portuguese who provided a new source of copper for court art.\nThe \"oba\" ruled with the advice of the \"uzama\", a council consisting of chiefs of powerful families and town chiefs of different guilds. Later its authority was diminished by the establishment of administrative dignitaries. Women wielded power. The queen mother who produced the future \"oba\" wielded immense influence.\n\nBenin was never a significant exporter of slaves, as Alan Ryder's book Benin and the Europeans showed. By the early 18th century, it was wrecked with dynastic disputes and civil wars. However, it regained much of its former power in the reigns of Oba Eresoyen and Oba Akengbuda. After the 16th century, Benin mainly exported pepper, ivory, gum, and cotton cloth to the Portuguese and Dutch who resold it to other African societies on the coast. In 1897, the British sacked the city.\n\nThe Niger Delta comprised numerous city-states with numerous forms of government. These city-states were protected by the waterways and thick vegetation of the delta. The region was transformed by trade in the 17th century. The delta's city-states were comparable to those of the Swahili people in East Africa. Some, like Bonny, Kalabari, and Warri, had kings. Others, like Brass, were republics with small senates, and those at Cross River and Old Calabar were ruled by merchants of the \"ekpe\" society. The \"ekpe\" society regulated trade and made rules for members known as house systems. Some of these houses, like the Pepples of Bonny, were well known in the Americas and Europe.\n\nThe Igbo lived east of the delta (but with the Anioma on the west of the Niger River). The Kingdom of Nri rose in the 9th century, with the \"Eze\" Nri being its leader. It was a political entity composed of villages, and each village was autonomous and independent with its own territory and name, each recognized by its neighbors. Villages were democratic with all males and sometimes females a part of the decision-making process. Graves at Igbo-Ukwu (800 AD) contained brass artifacts of local manufacture and glass beads from Egypt or India, indicative of extraregional trade.\n\nBy the 1850s, British and German missionaries and traders had penetrated present-day Namibia. Herero and Nama competed for guns and ammunition, providing cattle, ivory, and ostrich feathers. The Germans were more firmly established than the British in the region. By 1884, the Germans declared the coastal region from the Orange River to the Kunene River a German protectorate, part of German South West Africa. They pursued an aggressive policy of land expansion for white settlements. They exploited rivalry between the Nama and Herero.\n\nThe Herero entered into an alliance with the Germans, thinking they could get an upper hand on the Nama. The Germans set up a garrison at the Herero capital and started allocating Herero land for white settlements, including the best grazing land in the central plateau, and made tax and labor demands. The Herero and Ovambanderu rebelled, but the rebellion was crushed and leaders were executed. Between 1896 and 1897, rinderpest crippled the economic backbone of the Herero and Nama economy and slowed white expansion. The Germans continued the policy of making Namibia a white settlement by seizing land and cattle, and even trying to export Herero labor to South Africa.\n\nIn 1904, the Herero rebelled. German General Lothar von Trotha implemented an extermination policy at the Battle of Waterberg, which drove the Herero west of the Kalahari Desert. At the end of 1905, only 16,000 Herero were alive, out of a previous population of 80,000. Nama resistance was crushed in 1907. All Nama and Herero cattle and land were confiscated from the very diminished population, with remaining Nama and Herero assuming a subordinate position. Labor had to be imported from among the Ovambo.\n\nA moment of great disorder in southern Africa was the \"Mfecane\", \"the crushing.\" It was started by the northern Nguni kingdoms of Mthethwa, Ndwandwe, and Swaziland over scarce resource and famine. When Dingiswayo of Mthethwa died, Shaka of the Zulu people took over. He established the Zulu Kingdom, asserting authority over the Ndwandwe and pushing the Swazi north. The scattering Ndwandwe and Swazi caused the Mfecane to spread. During the 1820s, Shaka expanded the empire all along the Drakensberg foothills, with tribute being paid as far south as the Tugela and Umzimkulu rivers. He replaced the chiefs of conquered polities with \"indunas\", responsible to him. He introduced a centralized, dedicated, and disciplined military force not seen in the region, with a new weapon in the short stabbing-spear.\n\nIn 1828, Shaka was assassinated by his half brother Dingane, who lacked the military genius and leadership skills of Shaka. Voortrekkers tried to occupy Zulu land in 1838. In the early months they were defeated, but the survivors regrouped at the Ncome River and soundly defeated the Zulu. However, the Voortrekkers dared not settle Zulu land. Dingane was killed in 1840 during a civil war. His brother Mpande took over and strengthened Zulu territories to the north. In 1879 the Zulu Kingdom was invaded by Britain in a quest to control all of South Africa. The Zulu Kingdom was victorious at the Battle of Isandlwana but was defeated at the Battle of Ulundi.\n\nOne of the major states to emerge from the Mfecane was the Sotho Kingdom founded at Thaba Bosiu by Moshoeshoe I around 1821 to 1822. It was a confederation of different polities that accepted the absolute authority of Moshoeshoe. During the 1830s, the kingdom invited missionaries as a strategic means of acquiring guns and horses from the Cape. The Orange Free State slowly diminished the kingdom but never completely defeated it. In 1868, Moshoeshoe asked that the Sotho Kingdom be annexed by Britain, to save the remnant. It became the British protectorate of Basutoland.\n\nThe arrival of the ancestors of the Tswana-speakers who came to control the region (from the Vaal River to Botswana) has yet to be dated precisely although AD 600 seems to be a consensus estimate. This massive cattle-raising complex prospered until 1300 AD or so. All these various peoples were connected to trade routes that ran via the Limpopo River to the Indian Ocean, and trade goods from Asia such as beads made their way to Botswana most likely in exchange for ivory, gold, and rhinoceros horn.\nThe first written records relating to modern-day Botswana appear in 1824. What these records show is that the Bangwaketse had become the predominant power in the region. Under the rule of Makaba II, the Bangwaketse kept vast herds of cattle in well-protected desert areas, and used their military prowess to raid their neighbours. Other chiefdoms in the area, by this time, had capitals of 10,000 or so and were fairly prosperous. This equilibrium came to end during the Mfecane period, 1823–1843, when a succession of invading peoples from South Africa entered the country. Although the Bangwaketse were able to defeat the invading Bakololo in 1826, over time all the major chiefdoms in Botswana were attacked, weakened, and impoverished. The Bakololo and Amandebele raided repeatedly, and took large numbers of cattle, women, and children from the Batswana—most of whom were driven into the desert or sanctuary areas such as hilltops and caves. Only after 1843, when the Amandebele moved into western Zimbabwe, did this threat subside.\nDuring the 1840s and 1850s trade with Cape Colony-based merchants opened up and enabled the Batswana chiefdoms to rebuild. The Bakwena, Bangwaketse, Bangwato and Batawana cooperated to control the lucrative ivory trade, and then used the proceeds to import horses and guns, which in turn enabled them to establish control over what is now Botswana. This process was largely complete by 1880, and thus the Bushmen, the Bakalanga, the Bakgalagadi, the Batswapong and other current minorities were subjugated by the Batswana. Following the Great Trek, Afrikaners from the Cape Colony established themselves on the borders of Botswana in the Transvaal. In 1852 a coalition of Tswana chiefdoms led by Sechele I resisted Afrikaner incursions, and after about eight years of intermittent tensions and hostilities, eventually came to a peace agreement in Potchefstroom in 1860. From that point on, the modern-day border between South Africa and Botswana was agreed on, and the Afrikaners and Batswana traded and worked together peacefully.\nIn the 1820s, refugees from the Zulu expansion under Shaka came into contact with the Basotho people residing on the highveld.\nIn 1823, those pressures caused one group of Basotho, the Kololo, to migrate north, past the Okavango Swamp and across the Zambezi into Barotseland, now part of Zambia. In 1845, the Kololo conquered Barotseland.\n\nAt about the same time, the Boers began to encroach upon Basotho territory. After the Cape Colony had been ceded to Britain at the conclusion of the Napoleonic Wars, the \"voortrekkers\" (\"pioneers\") were farmers who opted to leave the former Dutch colony and moved inland where they eventually established independent polities.\n\nAt the time of these developments, Moshoeshoe I gained control of the Basotho kingdoms of the southern Highveld. Universally praised as a skilled diplomat and strategist, he was able to wield the disparate refugee groups escaping the Difaqane into a cohesive nation.\nHis inspired leadership helped his small nation to survive the dangers and pitfalls (the Zulu hegemony, the inward expansion of the voortrekkers and the designs of imperial Britain) that destroyed other indigenous South African kingdoms during the 19th century \n\nIn 1822, Moshoeshoe established his capital at Butha-Buthe, an easily defendable mountain in the northern Drakensberg mountains, laying the foundations of the eventual Kingdom of Lesotho. His capital was later moved to Thaba Bosiu\n\nTo deal with the encroaching voortrekker groups, Moshoeshoe encouraged French missionary activity in his kingdom. Missionaries sent by the Paris Evangelical Missionary Society provided the King with foreign affairs counsel and helped to facilitate the purchase of modern weapons.\n\nAside from acting as state ministers, missionaries (primarily Casalis and Arbousset) played a vital role in delineating Sesotho orthography and printing Sesotho language materials between 1837 and 1855. The first Sesotho translation of the Bible appeared in 1878.\n\nIn 1868, after losing the western lowlands to the Boers during the Free State–Basotho Wars; Moshoeshoe successfully appealed to Queen Victoria to proclaim Lesotho (then known as Basotuland) a protectorate of Britain and the British administration was placed in Maseru, the site of Lesotho's current capital. Local chieftains retained power over internal affairs while Britain was responsible for foreign affairs and the defence of the protectorate. In 1869, the British sponsored a process by which the borders of Basutoland were finally demarcated.\nWhile many clans had territory within Basotuland, large numbers of Sesotho speakers resided in areas allocated to the Orange Free State, the sovereign voortrekker republic that bordered the Basotho kingdom.\n\nBy the 19th century, most Khoikhoi territory was under Boer control. The Khoikhoi had lost economic and political independence and had been absorbed into Boer society. The Boers spoke Afrikaans, a language or dialect derived from Dutch, and no longer called themselves Boers but Afrikaners. Some Khoikhoi were used as commandos in raids against other Khoikhoi and later Xhosa. A mixed Khoi, slave, and European population called the Cape Coloureds, who were outcasts within colonial society, also arose. Khoikhoi who lived far on the frontier included the Kora, Oorlams, and Griqua. In 1795, the British took over the cape colony from the Dutch.\n\nIn the 1830s, Boers embarked on a journey of expansion, east of the Great Fish River into the Zuurveld. They were referred to as \"Voortrekkers\". They founded republics of the Transvaal and Orange Free State, mostly in areas of sparse population that had been diminished by the \"Mfecane/Difaqane\". Unlike the Khoisan, the Bantu states were not conquered by the Afrikaners, because of population density and greater unity. Additionally, they began to arm themselves with guns acquired through trade at the cape. In some cases, as in the Xhosa/Boer Wars, Boers were removed from Xhosa lands. It required a dedicated imperial military force to subdue the Bantu-speaking states. In 1901, the Boer republics were defeated by Britain in the Second Boer War. The defeat however consummated many Afrikaners' ambition: South Africa would be under white rule. The British placed all power—legislative, executive, administrative—in English and Afrikaner hands.\n\nBetween 1878 and 1898, European states partitioned and conquered most of Africa. For 400 years, European nations had mainly limited their involvement to trading stations on the African coast. Few dared venture inland from the coast; those that did, like the Portuguese, often met defeats and had to retreat to the coast. Several technological innovations helped to overcome this 400-year pattern. One was the development of repeating rifles, which were easier and quicker to load than muskets. Artillery was being used increasingly. In 1885, Hiram S. Maxim developed the maxim gun, the model of the modern-day machine gun. European states kept these weapons largely among themselves by refusing to sell these weapons to African leaders.\n\nAfrican germs took numerous European lives and deterred permanent settlements. Diseases such as yellow fever, sleeping sickness, yaws, and leprosy made Africa a very inhospitable place for Europeans. The deadliest disease was malaria, endemic throughout Tropical Africa. In 1854, the discovery of quinine and other medical innovations helped to make conquest and colonization in Africa possible.\n\nStrong motives for conquest of Africa were at play. Raw materials were needed for European factories. Europe in the early part of the 19th century was undergoing its Industrial Revolution. Nationalist rivalries and prestige were at play. Acquiring African colonies would show rivals that a nation was powerful and significant. These factors culminated in the Scramble for Africa.\nKnowledge of Africa increased. Numerous European explorers began to explore the continent. Mungo Park traversed the Niger River. James Bruce travelled through Ethiopia and located the source of the Blue Nile. Richard Francis Burton was the first European at Lake Tanganyika. Samuel White Baker explored the Upper Nile. John Hanning Speke located a source of the Nile at Lake Victoria. Other significant European explorers included Heinrich Barth, Henry Morton Stanley (coiner of the term \"Dark Continent\" for Africa in an 1878 book), Silva Porto, Alexandre de Serpa Pinto, Rene Caille, Friedrich Gerhard Rohlfs, Gustav Nachtigal, George Schweinfurth, and Joseph Thomson. The most famous of the explorers was David Livingstone, who explored southern Africa and traversed the continent from the Atlantic at Luanda to the Indian Ocean at Quelimane. European explorers made use of African guides and servants, and established long-distance trading routes were used.\n\nMissionaries attempting to spread Christianity also increased European knowledge of Africa. Between 1884 and 1885, European nations met at the Berlin West Africa Conference to discuss the partitioning of Africa. It was agreed that European claims to parts of Africa would only be recognised if Europeans provided effective occupation. In a series of treaties in 1890–1891, colonial boundaries were completely drawn. All of Sub-Saharan Africa was claimed by European powers, except for Ethiopia (Abyssinia) and Liberia.\n\nThe European powers set up a variety of different administrations in Africa, reflecting different ambitions and degrees of power. In some areas, such as parts of British West Africa, colonial control was tenuous and intended for simple economic extraction, strategic power, or as part of a long term development plan. In other areas, Europeans were encouraged to settle, creating settler states in which a European minority dominated. Settlers only came to a few colonies in sufficient numbers to have a strong impact. British settler colonies included British East Africa (now Kenya), Northern and Southern Rhodesia, (Zambia and Zimbabwe, respectively), and South Africa, which already had a significant population of European settlers, the Boers. France planned to settle Algeria and eventually incorporate it into the French state on an equal basis with the European provinces. Algeria's proximity across the Mediterranean allowed plans of this scale.\n\nIn most areas colonial administrations did not have the manpower or resources to fully administer the territory and had to rely on local power structures to help them. Various factions and groups within the societies exploited this European requirement for their own purposes, attempting to gain positions of power within their own communities by cooperating with Europeans. One aspect of this struggle included what Terence Ranger has termed the \"invention of tradition.\" In order to legitimize their own claims to power in the eyes of both the colonial administrators and their own people, native elites would essentially manufacture \"traditional\" claims to power, or ceremonies. As a result, many societies were thrown into disarray by the new order.\n\nFollowing the Scramble for Africa, an early but secondary focus for most colonial regimes was the suppression of slavery and the slave trade. By the end of the colonial period they were mostly successful in this aim, though slavery is still very active in Africa.\n\nAs a part of the Scramble for Africa, France had the establishment of a continuous west-east axis of the continent as an objective, in contrast with the British north-south axis. Tensions between Britain and France reached tinder stage in Africa. At several points war was possible, but never happened. The most serious episode was the Fashoda Incident of 1898. French troops tried to claim an area in the Southern Sudan, and a much more powerful British force purporting to be acting in the interests of the Khedive of Egypt arrived to confront them. Under heavy pressure the French withdrew securing British control over the area. The status quo was recognised by an agreement between the two states acknowledging British control over Egypt, while France became the dominant power in Morocco, but France suffered a humiliating defeat overall.\n\nBelgium\n\n\nFrance\nGermany\n\nItaly\n\nPortugal\nSpain\nUnited Kingdom\nIndependent states\n\nIn the 1880s the European powers had divided up almost all of Africa, with only Ethiopia, Liberia and Darwiish (the successor state of Dhulbahante garaadship) maintaining their independence throughout the Scramble for Africa. They ruled until after World War II when forces of nationalism grew much stronger. In the 1950s and 1960s the colonial holdings became independent states. The process was usually peaceful but there were several long bitter bloody civil wars, as in Algeria, Kenya and elsewhere. Across Africa the powerful new force of nationalism drew upon the organizational skills that natives learned in the British and French and other armies in the world wars. It led to organizations that were not controlled by or endorsed by either the colonial powers not the traditional local power structures that were collaborating with the colonial powers. Nationalistic organizations began to challenge both the traditional and the new colonial structures and finally displaced them. Leaders of nationalist movements took control when the European authorities exited; many ruled for decades or until they died off. These structures included political, educational, religious, and other social organizations. In recent decades, many African countries have undergone the triumph and defeat of nationalistic fervor, changing in the process the loci of the centralizing state power and patrimonial state.\n\nWith the vast majority of the continent under the colonial control of European governments, the World Wars were significant events in the geopolitical history of Africa. Africa was a theater of war and saw fighting in both wars. More important in most regions, the total war footing of colonial powers impacted the governance of African colonies, through resource allocation, conscription, and taxation. In World War I there were several campaigns in Africa, including the Togoland Campaign, the Kamerun Campaign, the South West Africa campaign, and the East African campaign. In each, Allied forces, primarily British, but also French, Belgian, South African, and Portuguese, sought to force the Germans out of their African colonies. In each, German forces were badly outnumbered and, due to Allied naval superiority, were cut off from reinforcement or resupply. The Allies eventually conquered all German colonies; German forces in East Africa managed to avoid surrender thorughout the war, though they could not hold any territory after 1917. After World War I, former German colonies in Africa were taken over by France, Belgium, and the British Empire.\n\nAfter World War I, colonial powers continued to consolidate their control over their African territories. In some areas, particularly in Southern and East Africa, large settler populations were successful in pressing for additional devolution of administration, so-called \"home rule\" by the white settlers. In many cases, settler regimes were harsher on African populations, tending to see them more as a threat to political power, as opposed to colonial regimes which had generally endeavored to coopt local populations into economic production. The Great Depression strongly affected Africa's non-subsistence economy, much of which was based on commodity production for Western markets. As demand increased in the late 1930s, Africa's economy rebounded as well.\n\nAfrica was the site of one of the first instances of fascist territorial expansions in the 1930s. Italy had attempted to conquer Ethiopia in the 1890s but had been rebuffed in the First Italo-Ethiopian War. Ethiopia lay between two Italian colonies, Italian Somaliland and Eritrea and was invaded in October 1935. With an overwhelming advantage in armor and aircraft, by May 1936, Italian forces had occupied the capital of Addis Ababa and effectively declared victory. Ethiopia and their other colonies were consolidated into Italian East Africa.\nAfrica was a large continent whose geography gave it strategic importance during the war. North Africa was the scene of major British and American campaigns against Italy and Germany; East Africa was the scene of a major British campaign against Italy. The vast geography provided major transportation routes linking the United States to the Middle East and Mediterranean regions. The sea route around South Africa was heavily used even though it added 40 days to voyages that had to avoid the dangerous Suez region. Lend Lease supplies to Russia often came this way. Internally, long-distance road and railroad connections facilitated the British war effort. The Union of Africa had dominion status and was largely self-governing, the other British possessions were ruled by the colonial office, usually with close ties to local chiefs and kings. Italian holdings were the target of successful British military campaigns. The Belgian Congo, and two other Belgian colonies, were major exporters. In terms of numbers and wealth, the British -controlled the richest portions of Africa, and made extensive use not only of the geography, but the manpower, and the natural resources. Civilian colonial officials made a special effort to upgrade the African infrastructure, promote agriculture, integrate colonial Africa with the world economy, and recruit over a half million soldiers.\n\nBefore the war, Britain had made few plans for the utilization of Africa, but it quickly set up command structures. The Army set up the West Africa Command, which recruited 200,000 soldiers. The East Africa Command was created in September 1941 to support the overstretched Middle East Command. It provided the largest number of men, over 320,000, chiefly from Kenya, Tanganyika, and Uganda. The Southern Command was the domain of South Africa. The Royal Navy set up the South Atlantic Command based in Sierra Leone, that became one of the main convoy assembly points. The RAF Coastal Command had major submarine-hunting operations based in West Africa, while a smaller RAF command Dealt with submarines in the Indian Ocean. Ferrying aircraft from North America and Britain was the major mission of the Western Desert Air Force. In addition smaller more localized commands were set up throughout the war.\n\nBefore 1939, the military establishments were very small throughout British Africa, and largely consisted of whites, who comprised under two percent of the population outside South Africa. As soon as the war began, newly created African units were set up, primarily by the Army. The new recruits were almost always volunteers, usually provided in close cooperation with local tribal leaders. During the war, military pay scales far exceeded what civilians natives could earn, especially when food, housing and clothing allowances are included. The largest numbers were in construction units, called Pioneer units, with over 82,000 soldiers.. The RAF and Navy also did some recruiting. The volunteers did some fighting, a great deal of guard duty, and construction work. 80,000 served in the Middle East. A special effort was made not to challenge white supremacy, certainly before the war, and to a large extent during the war itself. Nevertheless, the soldiers were drilled and train to European standards, given strong doses of propaganda, and learn leadership and organizational skills that proved essential to the formation of nationalistic and independence movements after 1945. There were minor episodes of discontent, but nothing serious, among the natives. Afrikaner nationalism was a factor in South Africa, But the proto-German Afrikaner prime minister was replaced in 1939 by Jan Smuts, an Afrikaner who was an enthusiastic supporter of the British Empire. His government closely cooperated with London and raised 340,000 volunteers (190,000 were white, or about one-third of the eligible white men).\nAs early as 1857, the French established volunteer units of black soldiers in sub- Sahara Africa, termed the \"tirailleurs senegalais.\" They served in military operations throughout the Empire, including 171,000 soldiers in World War I and 160,000 in World War II. About 90,000 became POWs in Germany. The veterans played a central role in the postwar independence movement in French Africa. \n\nauthorities in West Africa declared allegiance to the Vichy regime, as did the colony of French Gabon Vichy forces defeated a Free French Forces invasion of French West Africa in the two battles of Dakar in July and September 1940. Gabon fell to Free France after the Battle of Gabon in November 1940, but West Africa remained under Vichy control until November 1942. Vichy forces tried to resist the overwhelming Allied landings in North Africa (operation \"Torch\") in November 1942. Vichy Admiral François Darlan suddenly switched sides and the fighting ended. The Allies gave Darlan control of North African French forces in exchange for support from both French North Africa as well as French West Africa. Vichy was now eliminated as a factor in Africa. Darlan was assassinated in December, and the two factions of Free French, led by Charles de Gaulle and Henri Giraud, jockeyed for power. De Gaulle finally won out.\n\nSince Germany had lost its African colonies following World War I, World War II did not reach Africa until Italy joined the war on June 10, 1940, controlling Libya and Italian East Africa. With the fall of France on June 25, most of France's colonies in North and West Africa were controlled by the Vichy government, though much of Central Africa fell under Free French control after some fighting between Vichy and Free French forces at the Battle of Dakar and the Battle of Gabon. After the fall of France, Africa was the only active theater for ground combat until the Italian invasion of Greece in October. In the Western Desert campaign Italian forces from Libya sought to overrun Egypt, controlled by the British. Simultaneously, in the East African campaign, Italian East African forces overran British Somaliland and some British outposts in Kenya and Anglo-Egyptian Sudan. When Italy's efforts to conquer Egypt (including the crucial Suez Canal) and Sudan fell short, they were unable to reestablish supply to Italian East Africa. Without the ability to reinforce or resupply and surrounded by Allied possessions, Italian East Africa was conquered by mainly British and South African forces in 1941. In North Africa, the Italians soon requested help from the Germans who sent a substantial force under General Rommel. With German help, the Axis forces regained the upper hand but were unable to break through British defenses in two tries at El Alamein. In late 1942, Allied forces, mainly Americans and Canadians, invaded French North Africa in Operation Torch, where Vichy French forces initially surprised them with their resistance but were convinced to stop fighting after three days. The second front relieved pressure on the British in Egypt who began pushing west to meet up with the Torch forces, eventually pinning German and Italian forces in Tunisia, which was conquered by May 1943 in the Tunisia campaign, ending the war in Africa. The only other significant operations occurred in the French colony of Madagascar, which was invaded by the British in May 1942 to deny its ports to the Axis (potentially the Japanese who had reached the eastern Indian Ocean). The French garrisons in Madagascar surrendered in November 1942.\n\nThe decolonization of Africa started with Libya in 1951, although Liberia, South Africa, Egypt and Ethiopia were already independent. Many countries followed in the 1950s and 1960s, with a peak in 1960 with the Year of Africa, which saw 17 African nations declare independence, including a large part of French West Africa. Most of the remaining countries gained independence throughout the 1960s, although some colonizers (Portugal in particular) were reluctant to relinquish sovereignty, resulting in bitter wars of independence which lasted for a decade or more. The last African countries to gain formal independence were Guinea-Bissau (1974), Mozambique (1975) and Angola (1975) from Portugal; Djibouti from France in 1977; Zimbabwe from the United Kingdom in 1980; and Namibia from South Africa in 1990. Eritrea later split off from Ethiopia in 1993.\n\nThe Mau Mau Uprising took place in Kenya from 1952 until 1956 but was put down by British and local forces. A state of emergency remained in place until 1960. Kenya became independent in 1963, and Jomo Kenyatta served as its first president.\n\nThe early 1960s also signaled the start of major clashes between the Hutus and the Tutsis in Rwanda and Burundi. In 1994 this culminated in the Rwandan Genocide, a conflict in which over 800,000 people were murdered.\n\nMoroccan nationalism developed during the 1930s; the Istiqlal Party was formed, pushing for independence. In 1953 sultan Mohammed V of Morocco called for independence. On March 2, 1956, Morocco became independent of France. Mohammed V became ruler of independent Morocco.\n\nIn 1954, Algeria formed the National Liberation Front (FLN) as it split from France. This resulted in the Algerian War, which lasted until independence negotiations in 1962. Muhammad Ahmed Ben Bella was elected President of Algeria. Over a million French nationals, predominantly Pied-Noirs, left the country, crippling the economy.\n\nIn 1934, the \"Neo Destour\" (New Constitution) party was founded by Habib Bourguiba pushing for independence in Tunisia. Tunisia became independent in 1955. Its \"bey\" was deposed and Habib Bourguiba elected as President of Tunisia.\n\nIn 1954, Gamal Abdel Nasser deposed the monarchy of Egypt in the Egyptian Revolution of 1952 and came to power as Prime Minister of Egypt. Muammar Gaddafi led the 1969 Libyan coup d'état which deposed Idris of Libya. Gaddafi remained in power until his death in the Libyan Civil War of 2011.\n\nEgypt was involved in several wars against Israel and was allied with other Arab countries. The first was the 1948 Arab–Israeli War, right after the state of Israel was founded. Egypt went to war again in the Six-Day War of 1967 and lost the Sinai Peninsula to Israel. They went to war yet again in the Yom Kippur War of 1973. In 1979, President of Egypt Anwar Sadat and Prime Minister of Israel Menachem Begin signed the Camp David Accords, which gave back the Sinai Peninsula to Egypt in exchange for the recognition of Israel. The accords are still in effect today. In 1981, Sadat was assassinated by members of the Egyptian Islamic Jihad under Khalid Islambouli. The assassins were Islamists who targeted Sadat for his signing of the Accords.\n\nIn 1948 the apartheid laws were started in South Africa by the dominant National Party. These were largely a continuation of existing policies; the difference was the policy of \"separate development\" (Apartheid). Where previous policies had only been disparate efforts to economically exploit the African majority, Apartheid represented an entire philosophy of separate racial goals, leading to both the divisive laws of 'petty apartheid,' and the grander scheme of African homelands.\n\nIn 1994, the South African government abolished Apartheid. South Africans elected Nelson Mandela of the African National Congress in the South African general election, 1994, the country's first multiracial presidential election.\n\nFollowing World War II, nationalist movements arose across West Africa, most notably in Ghana under Kwame Nkrumah. In 1957, Ghana became the first sub-Saharan colony to achieve its independence, followed the next year by France's colonies; by 1974, West Africa's nations were entirely autonomous. Since independence, many West African nations have been plagued by corruption and instability, with notable civil wars in Nigeria, Sierra Leone, Liberia, and Ivory Coast, and a succession of military coups in Ghana and Burkina Faso. Many states have failed to develop their economies despite enviable natural resources, and political instability is often accompanied by undemocratic government.\n\nSee also 2014 Ebola virus epidemic in Sierra Leone, 2014 Ebola virus epidemic in Guinea, and 2014 Ebola virus epidemic in Liberia\n\nThe first historical studies in English appeared in the 1890s, and followed one of four approaches. 1) The territorial narrative was typically written by a veteran soldier or civil servant who gave heavy emphasis to what he had seen. 2) The \"apologia\" were essays designed to justify British policies. 3) Popularizers tried to reach a large audience. 4) Compendia appeared designed to combine academic and official credentials. Professional scholarship appeared around 1900, and began with the study of business operations, typically using government documents and unpublished archives.\n\nThe economic approach was widely practiced in the 1930s, primarily to provide descriptions of the changes underway in the previous half-century. In 1935, American historian William L. Langer published \"The Diplomacy of Imperialism: 1890–1902\", a book that is still widely cited. In 1939, Oxford professor Reginald Coupland published \"The Exploitation of East Africa, 1856–1890: The Slave Trade and the Scramble\", another popular treatment.\n\nWorld War II diverted most scholars to wartime projects and accounted for a pause in scholarship during the 1940s.\n\nBy the 1950s many African students were studying in British universities, and they produced a demand for new scholarship, and started themselves to supply it as well. Oxford University became the main center for African studies, with activity as well at Cambridge University and the London School of Economics. The perspective of British government policymakers or international business operations slowly gave way to a new interest in the activities of the natives, especially nationalistic movements and the growing demand for independence. The major breakthrough came from Ronald Robinson and John Andrew Gallagher, especially with their studies of the impact of free trade on Africa. In 1985 \"The Oxford History of South Africa\" (2 vols.) was published, attempting to synthesize the available materials. In 2013, \"The Oxford Handbook of Modern African History\" was published, bringing the scholarship up to date.\n\n\n\n\n\n\n"}
{"id": "14104", "url": "https://en.wikipedia.org/wiki?curid=14104", "title": "History of Oceania", "text": "History of Oceania\n\nThe History of Oceania includes the history of Australia, New Zealand, Papua New Guinea, Fiji and other Pacific island nations.\n\nThe prehistory of Oceania is divided into the prehistory of each of its major areas: Polynesia, Micronesia, Melanesia, and Australasia, and these vary greatly as to when they were first inhabited by humans—from 70,000 years ago (Australasia) to 3,000 years ago (Polynesia).\n\nThe Polynesian people are considered to be by linguistic, archaeological and human genetic ancestry a subset of the sea-migrating Austronesian people and tracing Polynesian languages places their prehistoric origins in the Malay Archipelago, and ultimately, in Taiwan. Between about 3000 and 1000 BCE speakers of Austronesian languages began spreading from Taiwan into Island South-East Asia, as tribes whose natives were thought to have arrived through South China about 8,000 years ago to the edges of western Micronesia and on into Melanesia, although they are different from the Han Chinese who now form the majority of people in China and Taiwan. There are three theories regarding the spread of humans across the Pacific to Polynesia. These are outlined well by Kayser \"et al.\" (2000) and are as follows:\n\nIn the archaeological record there are well-defined traces of this expansion which allow the path it took to be followed and dated with some certainty. It is thought that by roughly 1400 BCE, \"Lapita Peoples\", so-named after their pottery tradition, appeared in the Bismarck Archipelago of north-west Melanesia. This culture is seen as having adapted and evolved through time and space since its emergence \"Out of Taiwan\". They had given up rice production, for instance, after encountering and adapting to breadfruit in the Bird's Head area of New Guinea. In the end, the most eastern site for Lapita archaeological remains recovered so far has been through work on the archaeology in Samoa. The site is at Mulifanua on Upolu. The Mulifanua site, where 4,288 pottery shards have been found and studied, has a \"true\" age of c. 1000 BCE based on C14 dating. A 2010 study places the beginning of the human archaeological sequences of Polynesia in Tonga at 900 BCE, the small differences in dates with Samoa being due to differences in radiocarbon dating technologies between 1989 and 2010, the Tongan site apparently predating the Samoan site by some few decades in real time.\n\nWithin a mere three or four centuries between about 1300 and 900 BCE, the Lapita archaeological culture spread 6,000 kilometres further to the east from the Bismarck Archipelago, until it reached as far as Fiji, Tonga, and Samoa. The area of Tonga, Fiji, and Samoa served as a gateway into the rest of the Pacific region known as Polynesia. Ancient Tongan mythologies recorded by early European explorers report the islands of 'Ata and Tongatapu as the first islands being hauled to the surface from the deep ocean by Maui.\n\nThe \"Tuʻi Tonga Empire\" or \"Tongan Empire\" in Oceania are descriptions sometimes given to Tongan expansionism and projected hegemony dating back to 950 CE, but at its peak during the period 1200–1500. While modern researchers and cultural experts attest to widespread Tongan influence and evidences of transoceanic trade and exchange of material and non-material cultural artifacts, empirical evidence of a true political empire ruled for any length of time by successive rulers is lacking.\n\nModern archeology, anthropology and linguistic studies confirm widespread Tongan cultural influence ranging widely through East 'Uvea, Rotuma, Futuna, Samoa and Niue, parts of Micronesia (Kiribati, Pohnpei), Vanuatu, and New Caledonia and the Loyalty Islands, and while some academics prefer the term \"maritime chiefdom\", others argue that, while very different from examples elsewhere, \"...\"empire\" is probably the most convenient term.\"\n\nPottery art from Fijian towns shows that Fiji was settled before or around 3500 to 1000 BC, although the question of Pacific migration still lingers. It is believed that the Lapita people or the ancestors of the Polynesians settled the islands first but not much is known of what became of them after the Melanesians arrived; they may have had some influence on the new culture, and archaeological evidence shows that they would have then moved on to Tonga, Samoa and even Hawai'i.\n\nThe first settlements in Fiji were started by voyaging traders and settlers from the west about 5000 years ago. Lapita pottery shards have been found at numerous excavations around the country. Aspects of Fijian culture are similar to the Melanesian culture of the western Pacific but have a stronger connection to the older Polynesian cultures. Across from east to west, Fiji has been a nation of many languages. Fiji's history was one of settlement but also of mobility.\n\nOver the centuries, a unique Fijian culture developed. Constant warfare and cannibalism between warring tribes were quite rampant and very much part of everyday life. In later centuries, the ferocity of the cannibal lifestyle deterred European sailors from going near Fijian waters, giving Fiji the name \"Cannibal Isles\"; as a result, Fiji remained unknown to the rest of the world.\n\nEarly European visitors to Easter Island recorded the local oral traditions about the original settlers. In these traditions, Easter Islanders claimed that a chief Hotu Matu'a arrived on the island in one or two large canoes with his wife and extended family. They are believed to have been Polynesian. There is considerable uncertainty about the accuracy of this legend as well as the date of settlement. Published literature suggests the island was settled around 300–400 CE, or at about the time of the arrival of the earliest settlers in Hawaii.\n\nSome scientists say that Easter Island was not inhabited until 700–800 CE. This date range is based on glottochronological calculations and on three radiocarbon dates from charcoal that appears to have been produced during forest clearance activities.\n\nMoreover, a recent study which included radiocarbon dates from what is thought to be very early material suggests that the island was settled as recently as 1200 CE. This seems to be supported by a 2006 study of the island's deforestation, which could have started around the same time. A large now extinct palm, \"Paschalococos disperta\", related to the Chilean wine palm \"(Jubaea chilensis)\", was one of the dominant trees as attested by fossil evidence; this species, whose sole occurrence was Easter Island, became extinct due to deforestation by the early settlers.\n\nMicronesia began to be settled several millennia ago, although there are competing theories about the origin and arrival of the first settlers. There are numerous difficulties with conducting archaeological excavations in the islands, due to their size, settlement patterns and storm damage. As a result, much evidence is based on linguistic analysis. The earliest archaeological traces of civilization have been found on the island of Saipan, dated to 1500 BCE or slightly before.\n\nThe ancestors of the Micronesians settled there over 4,000 years ago. A decentralized chieftain-based system eventually evolved into a more centralized economic and religious culture centered on Yap and Pohnpei. The prehistory of many Micronesian islands such as Yap are not known very well.\n\nOn Pohnpei, pre-colonial history is divided into three eras: \"Mwehin Kawa\" or \"Mwehin Aramas\" (Period of Building, or Period of Peopling, before c. 1100); \"Mwehin Sau Deleur\" (Period of the Lord of Deleur, c. 1100 to c. 1628); and \"Mwehin Nahnmwarki\" (Period of the Nahnmwarki, c. 1628 to c. 1885). Pohnpeian legend recounts that the Saudeleur rulers, the first to bring government to Pohnpei, were of foreign origin. The Saudeleur centralized form of absolute rule is characterized in Pohnpeian legend as becoming increasingly oppressive over several generations. Arbitrary and onerous demands, as well as a reputation for offending Pohnpeian deities, sowed resentment among Pohnpeians. The Saudeleur Dynasty ended with the invasion of Isokelekel, another semi-mythical foreigner, who replaced the Saudeleur rule with the more decentralized \"nahnmwarki\" system in existence today. Isokelekel is regarded as the creator of the modern Pohnpeian \"nahnmwarki\" social system and the father of the Pompeian people.\n\nConstruction of Nan Madol, a megalithic complex made from basalt lava logs in Pohnpei began as early as 1200 CE. Nan Madol is offshore of Temwen Island near Pohnpei, consists of a series of small artificial islands linked by a network of canals, and is often called the \"Venice of the Pacific\". It is located near the island of Pohnpei and was the ceremonial and political seat of the Saudeleur Dynasty that united Pohnpei's estimated 25,000 people until its centralized system collapsed amid the invasion of Isokelekel. Isokelekel and his descendants initially occupied the stone city, but later abandoned it.\n\nThe first people of the Northern Mariana Islands navigated to the islands at some period between 4000 BCE to 2000 BCE from South-East Asia. They became known as the Chamorros, and spoke an Austronesian language called Chamorro. The ancient Chamorro left a number of megalithic ruins, including Latte stone. The Refaluwasch, or Carolinian, people came to the Marianas in the 1800s from the Caroline Islands. Micronesian colonists gradually settled the Marshall Islands during the 2nd millennium BCE, with inter-island navigation made possible using traditional stick charts.\n\nThe first settlers of Australia, New Guinea, and the large islands just to the east arrived between 50,000 and 30,000 years ago, when Neanderthals still roamed Europe. The original inhabitants of the group of islands now named Melanesia were likely the ancestors of the present-day Papuan-speaking people. Migrating from South-East Asia, they appear to have occupied these islands as far east as the main islands in the Solomon Islands archipelago, including Makira and possibly the smaller islands farther to the east.\n\nParticularly along the north coast of New Guinea and in the islands north and east of New Guinea, the Austronesian people, who had migrated into the area somewhat more than 3,000 years ago, came into contact with these pre-existing populations of Papuan-speaking peoples. In the late 20th century, some scholars theorized a long period of interaction, which resulted in many complex changes in genetics, languages, and culture among the peoples. Kayser, et al. proposed that, from this area, a very small group of people (speaking an Austronesian language) departed to the east to become the forebears of the Polynesian people.\n\nHowever, the theory is contradicted by the findings of a genetic study published by Temple University in 2008; based on genome scans and evaluation of more than 800 genetic markers among a wide variety of Pacific peoples, it found that neither Polynesians nor Micronesians have much genetic relation to Melanesians. Both groups are strongly related genetically to East Asians, particularly Taiwanese aborigines. It appeared that, having developed their sailing outrigger canoes, the Polynesian ancestors migrated from East Asia, moved through the Melanesian area quickly on their way, and kept going to eastern areas, where they settled. They left little genetic evidence in Melanesia.\n\nThe study found a high rate of genetic differentiation and diversity among the groups living within the Melanesian islands, with the peoples distinguished by island, language, topography, and geography among the islands. Such diversity developed over their tens of thousands of years of settlement before the Polynesian ancestors ever arrived at the islands. For instance, populations developed differently in coastal areas, as opposed to those in more isolated mountainous valleys.\n\nAdditional DNA analysis has taken research into new directions, as more human species have been discovered since the late 20th century. Based on his genetic studies of the Denisova hominin, an ancient human species discovered in 2010, Svante Pääbo claims that ancient human ancestors of the Melanesians interbred in Asia with these humans. He has found that people of New Guinea share 4–6% of their genome with the Denisovans, indicating this exchange. The Denisovans are considered cousin to the Neanderthals; both groups are now understood to have migrated out of Africa, with the Neanderthals going into Europe, and the Denisovans heading east about 400,000 years ago. This is based on genetic evidence from a fossil found in Siberia. The evidence from Melanesia suggests their territory extended into south Asia, where ancestors of the Melanesians developed.\n\nMelanesians of some islands are one of the few non-European peoples, and the only dark-skinned group of people outside Australia, known to have blond hair.\n\nIndigenous Australians are the original inhabitants of the Australian continent and nearby islands. Indigenous Australians migrated from Africa to Asia around 70,000 years ago and arrived in Australia around 50,000 years ago. The Torres Strait Islanders are indigenous to the Torres Strait Islands, which are at the northernmost tip of Queensland near Papua New Guinea. The term \"Aboriginal\" is traditionally applied to only the indigenous inhabitants of mainland Australia and Tasmania, along with some of the adjacent islands, i.e.: the \"first peoples\". \"Indigenous Australians\" is an inclusive term used when referring to both Aboriginal and Torres Strait islanders.\n\nThe earliest definite human remains found to date are that of Mungo Man, which have been dated at about 40,000 years old, but the time of arrival of the ancestors of Indigenous Australians is a matter of debate among researchers, with estimates dating back as far as 125,000 years ago. There is great diversity among different Indigenous communities and societies in Australia, each with its own unique mixture of cultures, customs and languages. In present-day Australia these groups are further divided into local communities.\n\nOceania was first explored by Europeans from the 16th century onwards. Portuguese navigators, between 1512 and 1526, reached the Moluccas (by António de Abreu and Francisco Serrão in 1512), Timor, the Aru Islands (Martim A. Melo Coutinho), the Tanimbar Islands, some of the Caroline Islands (by Gomes de Sequeira in 1525), and west Papua New Guinea (by Jorge de Menezes in 1526). In 1519 a Castilian ('Spanish') expedition led by Ferdinand Magellan sailed down the east coast of South America, found and sailed through the strait that bears his name and on 28 November 1520 entered the ocean which he named \"Pacific\". The three remaining ships, led by Magellan and his captains Duarte Barbosa and João Serrão, then sailed north and caught the trade winds which carried them across the Pacific to the Philippines where Magellan was killed. One surviving ship led by Juan Sebastián Elcano returned west across the Indian Ocean and the other went north in the hope of finding the westerlies and reaching Mexico. Unable to find the right winds, it was forced to return to the East Indies. The Magellan-Elcano expedition achieved the first circumnavigation of the world and reached the Philippines, the Mariana Islands and other islands of Oceania.\n\nFrom 1527 to 1595 a number of other large Spanish expeditions crossed the Pacific Ocean, leading to the discovery of the Marshall Islands and Palau in the North Pacific, as well as Tuvalu, the Marquesas, the Solomon Islands archipelago, the Cook Islands and the Admiralty Islands in the South Pacific.\n\nIn 1565, Spanish navigator Andrés de Urdaneta found a wind system that would allow ships to sail eastward from Asia, back to the Americas. From then until 1815 the annual Manila Galleons crossed the Pacific from Mexico to the Philippines and back, in the first transpacific trade route in history. Combined with the Spanish Atlantic or West Indies Fleet, the Manila Galleons formed one of the first global maritime exchange in human history, linking Seville in Spain with Manila in the Philippines, via Mexico.\n\nLater, in the quest for Terra Australis, Spanish explorers in the 17th century discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Luís Vaz de Torres. In 1668 the Spanish founded a colony on Guam as a resting place for west-bound galleons. For a long time this was the only non-coastal European settlement in the Pacific.\n\nThe Dutch were the first non-natives to undisputedly explore and chart coastlines of Australia, Tasmania, New Zealand, Tonga, Fiji, Samoa, and Easter Island. Verenigde Oostindische Compagnie (or VOC) was a major force behind the (c. 1590s–1720s) and Netherlandish cartography (c. 1570s–1670s). In the 17th century, the VOC's navigators and explorers charted almost three-quarters of the Australian coastline, except the east coast.\n\nAbel Tasman was the first known European explorer to reach the islands of Van Diemen's Land (now Tasmania) and New Zealand, and to sight the Fiji islands. His navigator François Visscher, and his merchant Isaack Gilsemans, mapped substantial portions of Australia, New Zealand, Tonga and the Fijian islands.\nOn 24 November 1642 Abel Tasman sighted the west coast of Tasmania, north of Macquarie Harbour. He named his discovery Van Diemen's Land after Antonio van Diemen, Governor-General of the Dutch East Indies. then claimed formal possession of the land on 3 December 1642.\nAfter some exploration, Tasman had intended to proceed in a northerly direction but as the wind was unfavourable he steered east. On 13 December they sighted land on the north-west coast of the South Island, New Zealand, becoming the first Europeans to do so. Tasman named it \"Staten Landt\" on the assumption that it was connected to an island (Staten Island, Argentina) at the south of the tip of South America. Proceeding north and then east, he stopped to gather water, but one of his boats was attacked by Māori in a double hulled waka (canoes) and four of his men were attacked and killed by mere. As Tasman sailed out of the bay he was again attacked, this time by 11 waka . The waka approached the Zeehan which fired and hit one Māori who fell down. Canister shot hit the side of a waka.\n\nArcheological research has shown the Dutch had tried to land at a major agricultural area, which the Māori may have been trying to protect. Tasman named the bay \"Murderers' Bay\" (now known as Golden Bay) and sailed north, but mistook Cook Strait for a bight (naming it \"Zeehaen's Bight\"). Two names he gave to New Zealand landmarks still endure, Cape Maria van Diemen and Three Kings Islands, but \"Kaap Pieter Boreels\" was renamed by Cook 125 years later to Cape Egmont.\n\nEn route back to Batavia, Tasman came across the Tongan archipelago on 20 January 1643. While passing the Fiji Islands Tasman's ships came close to being wrecked on the dangerous reefs of the north-eastern part of the Fiji group. He charted the eastern tip of Vanua Levu and Cikobia before making his way back into the open sea. He eventually turned north-west to New Guinea, and arrived at Batavia on 15 June 1643. For over a century after Tasman's voyages, until the era of James Cook, Tasmania and New Zealand were not visited by Europeans—mainland Australia was visited, but usually only by accident.\n\nIn 1766 the Royal Society engaged James Cook to travel to the Pacific Ocean to observe and record the transit of Venus across the Sun. The expedition sailed from England on 26 August 1768, rounded Cape Horn and continued westward across the Pacific to arrive at Tahiti on 13 April 1769, where the observations of the Venus Transit were made. Once the observations were completed, Cook opened the sealed orders which were additional instructions from the Admiralty for the second part of his voyage: to search the south Pacific for signs of the postulated rich southern continent of \"Terra Australis\".\n\nWith the help of a Tahitian named Tupaia, who had extensive knowledge of Pacific geography, Cook managed to reach New Zealand on 6 October 1769, leading only the second group of Europeans to do so (after Abel Tasman over a century earlier, in 1642). Cook mapped the complete New Zealand coastline, making only some minor errors (such as calling Banks Peninsula an island, and thinking Stewart Island/Rakiura was a peninsula of the South Island). He also identified Cook Strait, which separates the North Island from the South Island, and which Tasman had not seen.\n\nCook then voyaged west, reaching the south-eastern coast of Australia on 19 April 1770, and in doing so his expedition became the first recorded Europeans to have encountered its eastern coastline. On 23 April he made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point, noting in his journal: \"\"…and were so near the Shore as to distinguish several people upon the Sea beach they appear'd to be of a very dark or black Colour but whether this was the real colour of their skins or the C[l]othes they might have on I know not\".\" On 29 April Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal.\n\nAfter his departure from Botany Bay he continued northwards. After a grounding mishap on the Great Barrier Reef, the voyage continued, sailing through Torres Strait before returning to England via Batavia, the Cape of Good Hope, and Saint Helena.\n\nIn 1772 the Royal Society commissioned Cook to search for the hypothetical Terra Australis again. On his first voyage, Cook had demonstrated by circumnavigating New Zealand that it was not attached to a larger landmass to the south. Although he charted almost the entire eastern coastline of Australia, showing it to be continental in size, the Terra Australis was believed by the Royal Society to lie further south.\n\nCook commanded on this voyage, while Tobias Furneaux commanded its companion ship, . Cook's expedition circumnavigated the globe at an extreme southern latitude, becoming one of the first to cross the Antarctic Circle (17 January 1773). In the Antarctic fog, \"Resolution\" and \"Adventure\" became separated. Furneaux made his way to New Zealand, where he lost some of his men during an encounter with Māori, and eventually sailed back to Britain, while Cook continued to explore the Antarctic, reaching 71°10'S on 31 January 1774.\n\nCook almost encountered the mainland of Antarctica, but turned towards Tahiti to resupply his ship. He then resumed his southward course in a second fruitless attempt to find the supposed continent. On this leg of the voyage he brought a young Tahitian named Omai, who proved to be somewhat less knowledgeable about the Pacific than Tupaia had been on the first voyage. On his return voyage to New Zealand in 1774, Cook landed at the Friendly Islands, Easter Island, Norfolk Island, New Caledonia, and Vanuatu.\n\nBefore returning to England, Cook made a final sweep across the South Atlantic from Cape Horn. He then turned north to South Africa, and from there continued back to England. His reports upon his return home put to rest the popular myth of Terra Australis.\n\nOn his last voyage, Cook again commanded HMS \"Resolution\", while Captain Charles Clerke commanded . The voyage was ostensibly planned to return the Pacific Islander, Omai to Tahiti, or so the public were led to believe. The trip's principal goal was to locate a North-West Passage around the American continent. After dropping Omai at Tahiti, Cook travelled north and in 1778 became the first European to visit the Hawaiian Islands. After his initial landfall in January 1778 at Waimea harbour, Kauai, Cook named the archipelago the \"Sandwich Islands\" after the fourth Earl of Sandwich—the acting First Lord of the Admiralty.\n\nFrom the Sandwich Islands Cook sailed north and then north-east to explore the west coast of North America north of the Spanish settlements in Alta California. Cook explored and mapped the coast all the way to the Bering Strait, on the way identifying what came to be known as Cook Inlet in Alaska. In a single visit, Cook charted the majority of the North American north-west coastline on world maps for the first time, determined the extent of Alaska, and closed the gaps in Russian (from the West) and Spanish (from the South) exploratory probes of the Northern limits of the Pacific.\n\nCook returned to Hawaii in 1779. After sailing around the archipelago for some eight weeks, he made landfall at Kealakekua Bay, on 'Hawaii Island', largest island in the Hawaiian Archipelago. Cook's arrival coincided with the \"Makahiki\", a Hawaiian harvest festival of worship for the Polynesian god Lono. Coincidentally the form of Cook's ship, HMS \"Resolution\", or more particularly the mast formation, sails and rigging, resembled certain significant artefacts that formed part of the season of worship. Similarly, Cook's clockwise route around the island of Hawaii before making landfall resembled the processions that took place in a clockwise direction around the island during the Lono festivals. It has been argued (most extensively by Marshall Sahlins) that such coincidences were the reasons for Cook's (and to a limited extent, his crew's) initial deification by some Hawaiians who treated Cook as an incarnation of Lono. Though this view was first suggested by members of Cook's expedition, the idea that any Hawaiians understood Cook to be Lono, and the evidence presented in support of it, were challenged in 1992.\n\nAfter a month's stay, Cook resumed his exploration of the Northern Pacific. Shortly after leaving Hawaii Island, however, the \"Resolution\" foremast broke, so the ships returned to Kealakekua Bay for repairs. Tensions rose, and a number of quarrels broke out between the Europeans and Hawaiians. On 14 February 1779, at Kealakekua Bay, some Hawaiians took one of Cook's small boats. As thefts were quite common in Tahiti and the other islands, Cook would have taken hostages until the stolen articles were returned. He attempted to take as hostage the King of Hawaiʻi, Kalaniʻōpuʻu. The Hawaiians prevented this, and Cook's men had to retreat to the beach. As Cook turned his back to help launch the boats, he was struck on the head by the villagers and then stabbed to death as he fell on his face in the surf. Hawaiian tradition says that he was killed by a chief named Kalaimanokahoʻowaha or Kanaʻina. The Hawaiians dragged his body away. Four of Cook's men were also killed and two others were wounded in the confrontation.\n\nThe esteem which the islanders nevertheless held for Cook caused them to retain his body. Following their practice of the time, they prepared his body with funerary rituals usually reserved for the chiefs and highest elders of the society. The body was disembowelled, baked to facilitate removal of the flesh, and the bones were carefully cleaned for preservation as religious icons in a fashion somewhat reminiscent of the treatment of European saints in the Middle Ages. Some of Cook's remains, thus preserved, were eventually returned to his crew for a formal burial at sea.\n\nClerke assumed leadership of the expedition. Following the death of Clerke, \"Resolution\" and \"Discovery\" returned home in October 1780 commanded by John Gore, a veteran of Cook's first voyage, and Captain James King. After their arrival in England, King completed Cook's account of the voyage.\n\nIn 1789 the Mutiny on the Bounty against William Bligh led to several of the mutineers escaping the Royal Navy and settling on Pitcairn Islands, which later became a British colony. Britain also established colonies in Australia in 1788, New Zealand in 1840 and Fiji in 1872, with much of Oceania becoming part of the British Empire.\n\nThe Gilbert Islands (now known as Kiribati) and the Ellice Islands (now known as Tuvalu) came under Britain's sphere of influence in the late 19th century. The Ellice Islands were administered as British protectorate by a Resident Commissioner from 1892 to 1916 as part of the British Western Pacific Territories (BWPT), and later as part of the Gilbert and Ellice Islands colony from 1916 to 1974.\n\nAmong the last islands in Oceania to be colonised was Niue (1900). In 1887, King Fata-a-iki, who reigned Niue from 1887 to 1896, offered to cede sovereignty to the British Empire, fearing the consequences of annexation by a less benevolent colonial power. The offer was not accepted until 1900. Niue was a British protectorate, but the UK's direct involvement ended in 1901 when New Zealand annexed the island.\n\nFrench Catholic missionaries arrived on Tahiti in 1834; their expulsion in 1836 caused France to send a gunboat in 1838. In 1842, Tahiti and Tahuata were declared a French protectorate, to allow Catholic missionaries to work undisturbed. The capital of Papeetē was founded in 1843. In 1880, France annexed Tahiti, changing the status from that of a protectorate to that of a colony.\n\nOn 24 September 1853, under orders from Napoleon III, Admiral Febvrier Despointes took formal possession of New Caledonia and Port-de-France (Nouméa) was founded 25 June 1854. A few dozen free settlers settled on the west coast in the following years. New Caledonia became a penal colony, and from the 1860s until the end of the transportations in 1897, about 22,000 criminals and political prisoners were sent to New Caledonia, among them many Communards, including Henri de Rochefort and Louise Michel. Between 1873 and 1876, 4,200 political prisoners were \"relegated\" in New Caledonia. Only forty of them settled in the colony, the rest returned to France after being granted amnesty in 1879 and 1880.\n\nIn the 1880s, France claimed the Tuamotu Archipelago, which formerly belonged to the Pōmare Dynasty, without formally annexing it. Having declared a protectorate over Tahuata in 1842, the French regarded the entire Marquesas Islands as French. In 1885, France appointed a governor and established a general council, thus giving it the proper administration for a colony. The islands of Rimatara and Rūrutu unsuccessfully lobbied for British protection in 1888, so in 1889 they were annexed by France. Postage stamps were first issued in the colony in 1892. The first official name for the colony was \"Établissements de l'Océanie\" (Settlements in Oceania); in 1903 the general council was changed to an advisory council and the colony's name was changed to \"Établissements Français de l'Océanie\" (French Settlements in Oceania).\n\nThe Spanish explorer Alonso de Salazar landed in the Marshall Islands in 1529. They were named by Krusenstern, after English explorer John Marshall, who visited them together with Thomas Gilbert in 1788, en route from Botany Bay to Canton (two ships of the First Fleet). The Marshall Islands were claimed by Spain in 1874.\n\nIn 1606 Luís Vaz de Torres explored the southern coast of New Guinea from Milne Bay to the Gulf of Papua including Orangerie Bay which he named \"Bahía de San Lorenzo\". His expedition also discovered Basilaki Island naming it \"Tierra de San Buenaventura\", which he claimed for Spain in July 1606. On 18 October his expedition reached the western part of the island in present-day Indonesia, and also claimed the territory for the King of Spain.\n\nA successive European claim occurred in 1828, when the Netherlands formally claimed the western half of the island as Netherlands New Guinea. In 1883, following a short-lived French annexation of New Ireland, the British colony of Queensland annexed south-eastern New Guinea. However, the Queensland government's superiors in the United Kingdom revoked the claim, and (formally) assumed direct responsibility in 1884, when Germany claimed north-eastern New Guinea as the protectorate of German New Guinea (also called Kaiser-Wilhelmsland).\n\nThe first Dutch government posts were established in 1898 and in 1902: Manokwari on the north coast, Fak-Fak in the west and Merauke in the south at the border with British New Guinea. The German, Dutch and British colonial administrators each attempted to suppress the still-widespread practices of inter-village warfare and headhunting within their respective territories.\n\nIn 1905 the British government transferred some administrative responsibility over south-east New Guinea to Australia (which renamed the area \"Territory of Papua\"); and in 1906, transferred all remaining responsibility to Australia. During World War I, Australian forces seized German New Guinea, which in 1920 became the Territory of New Guinea,\nto be administered by Australia under a League of Nations mandate. The territories under Australian administration became collectively known as The Territories of Papua and New Guinea (until February 1942).\n\nGermany established colonies in New Guinea in 1884, and Samoa in 1900.\n\nFollowing papal mediation and German compensation of $4.5 million, Spain recognized a German claim in 1885. Germany established a protectorate and set up trading stations on the islands of Jaluit and Ebon to carry out the flourishing copra (dried coconut meat) trade. Marshallese Iroij (high chiefs) continued to rule under indirect colonial German administration.\n\nThe United States also expanded into the Pacific, beginning with Baker Island and Howland Island in 1857, and with Hawaii becoming a U.S. territory in 1898. Disagreements between the US, Germany and UK over Samoa led to the Tripartite Convention of 1899.\n\nSamoa aligned its interests with the United States in a Deed of Succession, signed by the \"Tui Manúʻa\" (supreme chief of Manúʻa) on 16 July 1904 at the Crown residence of the Tuimanuʻa called the \"Faleula\" in the place called Lalopua (from Official documents of the Tuimanuʻa government, 1893; Office of the Governor, 2004).\n\nCession followed the Tripartite Convention of 1899 that partitioned the eastern islands of Samoa (including Tutuila and the Manúʻa Group) from the western islands of Samoa (including ʻUpolu and Savaiʻi).\n\nAt the beginning of World War I, Japan assumed control of the Marshall Islands. The Japanese headquarters was established at the German center of administration, Jaluit. On 31 January 1944, during World War II, American forces landed on Kwajalein atoll and U.S. Marines and Army troops later took control of the islands from the Japanese on 3 February, following intense fighting on Kwajalein and Enewetak atolls. In 1947, the United States, as the occupying power, entered into an agreement with the UN Security Council to administer much of Micronesia, including the Marshall Islands, as the Trust Territory of the Pacific Islands.\n\nDuring World War II, Japan colonized many Oceanic colonies by wresting control from western powers.\n\nThe Samoan Crisis was a confrontation standoff between the United States, Imperial Germany and Great Britain from 1887–1889 over control of the Samoan Islands during the Samoan Civil War. The incident involved three American warships, USS , and and three German warships, SMS \"Adler\", SMS \"Olga\", and SMS \"Eber\", keeping each other at bay over several months in Apia harbor, which was monitored by the British warship .\n\nThe standoff ended on 15 and 16 March when a cyclone wrecked all six warships in the harbor. \"Calliope\" was able to escape the harbor and survived the storm. Robert Louis Stevenson witnessed the storm and its aftermath at Apia and later wrote about what he saw. The Samoan Civil War continued, involving Germany, United States and Britain, eventually resulting, via the Tripartite Convention of 1899, in the partition of the Samoan Islands into American Samoa and German Samoa.\n\nThe Asian and Pacific Theatre of World War I was a conquest of German colonial possession in the Pacific Ocean and China. The most significant military action was the Siege of Tsingtao in what is now China, but smaller actions were also fought at Battle of Bita Paka and Siege of Toma in German New Guinea.\n\nAll other German and Austrian possessions in Asia and the Pacific fell without bloodshed. Naval warfare was common; all of the colonial powers had naval squadrons stationed in the Indian or Pacific Oceans. These fleets operated by supporting the invasions of German held territories and by destroying the East Asia Squadron.\n\nOne of the first land offensives in the Pacific theatre was the Occupation of German Samoa in August 1914 by New Zealand forces. The campaign to take Samoa ended without bloodshed after over 1,000 New Zealanders landed on the German colony, supported by an Australian and French naval squadron.\n\nAustralian forces attacked German New Guinea in September 1914: 500 Australians encountered 300 Germans and native policemen at the Battle of Bita Paka; the Allies won the day and the Germans retreated to Toma. A company of Australians and a British warship besieged the Germans and their colonial subjects, ending with a German surrender.\n\nAfter the fall of Toma, only minor German forces were left in New Guinea and these generally capitulated once met by Australian forces. In December 1914, one German officer near Angorum attempted resist the occupation with thirty native police but his force deserted him after they fired on an Australian scouting party and he was subsequently captured.\n\nGerman Micronesia, the Marianas, the Carolines and the Marshall Islands also fell to Allied forces during the war.\n\nThe Pacific front saw major action during the Second World War, mainly between the belligerents Japan and the United States.\n\nThe attack on Pearl Harbor was a surprise military strike conducted by the Imperial Japanese Navy against the United States naval base at Pearl Harbor, Hawaii, on the morning of 7 December 1941 (8 December in Japan). The attack led to the United States' entry into World War II.\n\nThe attack was intended as a preventive action in order to keep the U.S. Pacific Fleet from interfering with military actions the Empire of Japan was planning in South-East Asia against overseas territories of the United Kingdom, the Netherlands, and the United States. There were simultaneous Japanese attacks on the U.S.-held Philippines and on the British Empire in Malaya, Singapore, and Hong Kong.\n\nThe Japanese subsequently invaded New Guinea, the Solomon Islands and other Pacific islands. The Japanese were turned back at the Battle of the Coral Sea and the Kokoda Track campaign before they were finally defeated in 1945.\n\nSome of the most prominent Oceanic battlegrounds were the Solomon Islands campaign, the Air raids on Darwin, the Kokada Track, and the Borneo campaign.\n\nIn 1940 the administration of French Polynesia recognised the Free French Forces and many Polynesians served in World War II. Unknown at the time to French and Polynesians, the Konoe Cabinet in Imperial Japan on 16 September 1940 included French Polynesia among the many territories which were to become Japanese possessions in the post-war world—though in the course of the war in the Pacific the Japanese were not able to launch an actual invasion of the French islands.\n\nSome of the most intense fighting of the Second World War occurred in the Solomons. The most significant of the Allied Forces' operations against the Japanese Imperial Forces was launched on 7 August 1942, with simultaneous naval bombardments and amphibious landings on the Florida Islands at Tulagi and Red Beach on Guadalcanal.\n\nThe Guadalcanal Campaign became an important and bloody campaign fought in the Pacific War as the Allies began to repulse Japanese expansion. Of strategic importance during the war were the coastwatchers operating in remote locations, often on Japanese held islands, providing early warning and intelligence of Japanese naval, army and aircraft movements during the campaign.\n\n\"The Slot\" was a name for New Georgia Sound, when it was used by the Tokyo Express to supply the Japanese garrison on Guadalcanal. Of more than 36,000 Japanese on Guadalcanal, about 26,000 were killed or missing, 9,000 died of disease, and 1,000 were captured.\n\nThe Kokoda Track campaign was a campaign consisting of a series of battles fought between July and November 1942 between Japanese and Allied—primarily Australian—forces in what was then the Australian territory of Papua. Following a landing near Gona, on the north coast of New Guinea, Japanese forces attempted to advance south overland through the mountains of the Owen Stanley Range to seize Port Moresby as part of a strategy of isolating Australia from the United States. Initially only limited Australian forces were available to oppose them, and after making rapid progress the Japanese South Seas Force clashed with under strength Australian forces at Awala, forcing them back to Kokoda. A number of Japanese attacks were subsequently fought off by the Australian Militia, yet they began to withdraw over the Owen Stanley Range, down the Kokoda Track.\n\nIn sight of Port Moresby itself, the Japanese began to run out of momentum against the Australians who began to receive further reinforcements. Having outrun their supply lines and following the reverses suffered by the Japanese at Guadalcanal, the Japanese were now on the defensive, marking the limit of the Japanese advance southwards. The Japanese subsequently withdrew to establish a defensive position on the north coast, but they were followed by the Australians who recaptured Kokoda on 2 November. Further fighting continued into November and December as the Australian and United States forces assaulted the Japanese beachheads, in what later became known as the Battle of Buna–Gona.\n\nDue to its low population, Oceania was a popular location for atmospheric and underground nuclear tests. Tests were conducted in various locations by the United Kingdom (Operation Grapple and Operation Antler), the United States (Bikini atoll and the Marshall Islands) and France (Moruroa), often with devastating consequences for the inhabitants.\n\nFrom 1946 to 1958, the Marshall Islands served as the Pacific Proving Grounds for the United States, and was the site of 67 nuclear tests on various atolls. The world's first hydrogen bomb, codenamed \"Mike\", was tested at the Enewetak atoll in the Marshall Islands on 1 November (local date) in 1952, by the United States.\n\nIn 1954, fallout from the American Castle Bravo hydrogen bomb test in the Marshall Islands was such that the inhabitants of the Rongelap Atoll were forced to abandon their island. Three years later the islanders were allowed to return, but suffered abnormally high levels of cancer. They were evacuated again in 1985 and in 1996 given $45 million in compensation.\n\nA series of British tests were also conducted in the 1950s at Maralinga in South Australia, forcing the removal of the Pitjantjatjara and Yankunytjatjara peoples from their ancestral homelands.\n\nIn 1962, France's early nuclear testing ground of Algeria became independent and the atoll of Moruroa in the Tuamotu Archipelago was selected as the new testing site. Moruroa atoll became notorious as a site of French nuclear testing, primarily because tests were carried out there after most Pacific testing had ceased. These tests were opposed by most other nations in Oceania. The last atmospheric test was conducted in 1974, and the last underground test in 1996.\n\nFrench nuclear testing in the Pacific was controversial in the 1980s, in 1985 French agents caused the Sinking of the Rainbow Warrior in Auckland to prevent it from arriving at the test site in Moruroa. In September 1995, France stirred up widespread protests by resuming nuclear testing at Fangataufa atoll after a three-year moratorium. The last test was on 27 January 1996. On 29 January 1996, France announced that it would accede to the Comprehensive Test Ban Treaty, and no longer test nuclear weapons.\n\nFiji has suffered several coups d'état: military in 1987 and 2006 and civilian in 2000. All were ultimately due to ethnic tension between indigenous Fijians and Indo-Fijians, who originally came to the islands as indentured labour in the late nineteenth and early twentieth century. The 1987 coup followed the election of a multi-ethnic coalition, which Lieutenant Colonel Sitiveni Rabuka overthrew, claiming racial discrimination against ethnic Fijians. The coup was denounced by the United Nations and Fiji was expelled from the Commonwealth of Nations.\n\nThe 2000 coup was essentially a repeat of the 1987 affair, although it was led by civilian George Speight, apparently with military support. Commodore Frank Bainimarama, who was opposed to Speight, then took over and appointed a new Prime Minister. Speight was later tried and convicted for treason. Many indigenous Fijians were unhappy at the treatment of Speight and his supporters, feeling that the coup had been legitimate. In 2006 the Fijian parliament attempted to introduce a series of bills which would have, amongst other things, pardoned those involved in the 2000 coup. Bainimarama, concerned that the legal and racial injustices of the previous coups would be perpetuated, staged his own coup. It was internationally condemned, and Fiji again suspended from the Commonwealth.\n\nIn 2006 the then Australia Defence Minister, Brendan Nelson, warned Fijian officials of an Australian Naval fleet within proximity of Fiji that would respond to any attacks against its citizens.\n\nThe Australian government estimated that anywhere between 15,000 and 20,000 people could have died in the Bougainville Civil War. More conservative estimates put the number of combat deaths as 1–2,000.\n\nFrom 1975, there were attempts by the Bougainville Province to secede from Papua New Guinea. These were resisted by Papua New Guinea primarily because of the presence in Bougainville of the Panguna mine, which was vital to Papua New Guinea's economy. The Bougainville Revolutionary Army began attacking the mine in 1988, forcing its closure the following year. Further BRA activity led to the declaration of a state of emergency and the conflict continued until about 2005, when successionist leader and self-proclaimed King of Bougainville Francis Ona died of malaria. Peacekeeping troops led by Australia have been in the region since the late 1990s, and a referendum on independence will be held in the 2010s.\n\nIn 1946, French Polynesians were granted French citizenship and the islands' status was changed to an overseas territory; the islands' name was changed in 1957 to Polynésie Française (French Polynesia).\n\nAustralia and New Zealand became dominions in the 20th century, adopting the Statute of Westminster Act in 1942 and 1947 respectively, marking their legislative independence from the United Kingdom. Hawaii became a U.S. state in 1959.\n\nSamoa became the first pacific nation to gain independence in 1962, Fiji and Tonga became independent in 1970, with many other nations following in the 1970s and 1980s. The South Pacific Forum was founded in 1971, which became the Pacific Islands Forum in 2000. Bougainville Island, geographically part of the Solomon Islands archipelago but politically part of Papua New Guinea, tried unsuccessfully to become independent in 1975, and a civil war followed in the early 1990s, with it later being granted autonomy.\n\nOn 1 May 1979, in recognition of the evolving political status of the Marshall Islands, the United States recognized the constitution of the Marshall Islands and the establishment of the Government of the Republic of the Marshall Islands. The constitution incorporates both American and British constitutional concepts.\n\nIn 1852, French Polynesia was granted partial internal autonomy; in 1984, the autonomy was extended. French Polynesia became a full overseas collectivity of France in 2004.\n\nBetween 2001 and 2007 Australia's Pacific Solution policy transferred asylum seekers to several Pacific nations, including the Nauru detention centre. Australia, New Zealand and other nations took part in the Regional Assistance Mission to Solomon Islands from 2003 after a request for aid.\n\n\n"}
{"id": "14105", "url": "https://en.wikipedia.org/wiki?curid=14105", "title": "Hanseatic League", "text": "Hanseatic League\n\nThe Hanseatic League (; Middle Low German: \"Hanse\", \"Düdesche Hanse\", \"Hansa\"; Standard German: \"Deutsche Hanse\"; Latin: \"Hansa Teutonica\") was a commercial and defensive confederation of merchant guilds and market towns in Northwestern and Central Europe. Growing from a few North German towns in the late 1100s, the league came to dominate Baltic maritime trade for three centuries along the coasts of Northern Europe. Hansa territories stretched from the Baltic to the North Sea and inland during the Late Middle Ages, and diminished slowly after 1450.\n\n\"Hanse\", later spelled as \"Hansa\", was the Old High German word for a convoy, and this word was applied to bands of merchants traveling between the Hanseatic cities - whether by land or by sea.\n\nMerchant circles established the league to protect the guilds' economic interests and diplomatic privileges in their affiliated cities and countries, as well as along the trade routes which the merchants used. The Hanseatic cities had their own legal system and operated their own armies for mutual protection and aid. Despite this, the organization was not a state, nor could it be called a confederation of city-states; only a very small number of the cities within the league enjoyed autonomy and liberties comparable to those of a free imperial city.\n\nHistorians generally trace the origins of the Hanseatic League to the rebuilding of the north German town of Lübeck in 1159 by the powerful Henry the Lion, Duke of Saxony and Bavaria, after he had captured the area from Adolf II, Count of Schauenburg and Holstein. Exploratory trading adventures, raids, and piracy had occurred earlier throughout the Baltic region—the sailors of Gotland sailed up rivers as far away as Novgorod, for example—but the scale of international trade in the Baltic area remained insignificant before the growth of the Hanseatic League.\n\nGerman cities achieved domination of trade in the Baltic with striking speed during the 13th century, and Lübeck became a central node in the seaborne trade that linked the areas around the North and Baltic seas. The hegemony of Lübeck peaked during the 15th century.\n\nLübeck became a base for merchants from Saxony and Westphalia trading eastward and northward. Well before the term \"Hanse\" appeared in a document in 1267, merchants in different cities began to form guilds, or \"Hansa\", with the intention of trading with towns overseas, especially in the economically less-developed eastern Baltic. This area was a source of timber, wax, amber, resins, and furs, along with rye and wheat brought down on barges from the hinterland to port markets. The towns raised their own armies, with each guild required to provide levies when needed. The Hanseatic cities came to the aid of one another, and commercial ships often had to be used to carry soldiers and their arms.\n\nVisby functioned as the leading centre in the Baltic before the Hansa. Sailing east, Visby merchants established a trading post at Novgorod called Gutagard (also known as Gotenhof) in 1080. Merchants from northern Germany also stayed in the early period of the Gotlander settlement. Later they established their own trading station in Novgorod, known as , which was further up river, in the first half of the 13th century. In 1229, German merchants at Novgorod were granted certain privileges that made their positions more secure.\n\nHansa societies worked to remove restrictions to trade for their members. Before the official foundation of the league in 1356, the word \"Hanse\" did not occur in the Baltic language. Gotlanders used the word \"varjag\". The earliest remaining documentary mention, although without a name, of a specific German commercial federation is from London in 1157. That year, the merchants of the Hansa in Cologne convinced Henry II, King of England, to free them from all tolls in London and allow them to trade at fairs throughout England. The \"Queen of the Hansa\", Lübeck, where traders were required to trans-ship goods between the North Sea and the Baltic, gained imperial privileges to become a free imperial city in 1226, as its potential trading partner Hamburg had in 1189.\n\nIn 1241, Lübeck, which had access to the Baltic and North seas' fishing grounds, formed an alliance—a precursor to the league—with Hamburg, another trading city, that controlled access to salt-trade routes from Lüneburg. The allied cities gained control over most of the salt-fish trade, especially the Scania Market; Cologne joined them in the Diet of 1260. In 1266, Henry III of England granted the Lübeck and Hamburg Hansa a charter for operations in England, and the Cologne Hansa joined them in 1282 to form the most powerful Hanseatic colony in London. Much of the drive for this co-operation came from the fragmented nature of existing territorial governments, which failed to provide security for trade. Over the next 50 years the Hansa itself emerged with formal agreements for confederation and co-operation covering the west and east trade routes. The principal city and linchpin remained Lübeck; with the first general diet of the Hansa held there in 1356, the Hanseatic League acquired an official structure.\n\nLübeck's location on the Baltic provided access for trade with Scandinavia and Kievan Rus', putting it in direct competition with the Scandinavians who had previously controlled most of the Baltic trade routes. A treaty with the Visby Hansa put an end to this competition: through this treaty the Lübeck merchants also gained access to the inland Russian port of Novgorod, where they built a trading post or \"Kontor\" (literally: \"office\"). Although such alliances formed throughout the Holy Roman Empire, the league never became a closely managed formal organisation. Assemblies of the Hanseatic towns met irregularly in Lübeck for a \"Hansetag\" (Hanseatic diet), from 1356 onwards, but many towns chose not to attend nor to send representatives and decisions were not binding on individual cities. Over the period, a network of alliances grew to include a flexible roster of 70 to 170 cities.\n\nThe league succeeded in establishing additional \"Kontors\" in Bruges (Flanders), Bergen (Norway), and London (England). These trading posts became significant enclaves. The London \"Kontor\", established in 1320, stood west of London Bridge near Upper Thames Street, the site now occupied by Cannon Street station. It grew into a significant walled community with its own warehouses, weighhouse, church, offices and houses, reflecting the importance and scale of trading activity on the premises. The first reference to it as the Steelyard (\"der Stahlhof\") occurs in 1422.\n\nStarting with trade in coarse woollen fabrics, the Hanseatic League had the effect of bringing both commerce and industry to northern Germany. As trade increased, newer and finer woollen and linen fabrics, and even silks, were manufactured in northern Germany. The same refinement of products out of cottage industry occurred in other fields, e.g. etching, wood carving, armour production, engraving of metals, and wood-turning. The century-long monopolization of sea navigation and trade by the Hanseatic League ensured that the Renaissance arrived in northern Germany long before the rest of Europe.\n\nIn addition to the major \"Kontors\", individual Hanseatic ports had a representative merchant and warehouse. In England this happened in Boston, Bristol, Bishop's Lynn (now King's Lynn, which features the sole remaining Hanseatic warehouse in England), Hull, Ipswich, Norwich, Yarmouth (now Great Yarmouth), and York.\n\nThe league primarily traded timber, furs, resin (or tar), flax, honey, wheat, and rye from the east to Flanders and England with cloth (and, increasingly, manufactured goods) going in the other direction. Metal ore (principally copper and iron) and herring came southwards from Sweden.\n\nGerman colonists in the 12th and 13th centuries settled in numerous cities on and near the east Baltic coast, such as Elbing (Elbląg), Thorn (Toruń), Reval (Tallinn), Riga, and Dorpat (Tartu), which became members of the Hanseatic League, and some of which still retain many Hansa buildings and bear the style of their Hanseatic days. Most were granted Lübeck law (\"Lübisches Recht\"), after the league's most prominent town. The law provided that they had to appeal in all legal matters to Lübeck's city council. The Livonian Confederation incorporated modern-day Estonia and parts of Latvia and had its own Hanseatic parliament (diet); all of its major towns became members of the Hanseatic League. The dominant language of trade was Middle Low German, a dialect with significant impact for countries involved in the trade, particularly the larger Scandinavian languages, Estonian, and Latvian.\n\nThe league had a fluid structure, but its members shared some characteristics; most of the Hansa cities either started as independent cities or gained independence through the collective bargaining power of the league, though such independence remained limited. The Hanseatic free cities owed allegiance directly to the Holy Roman Emperor, without any intermediate family tie of obligation to the local nobility.\n\nAnother similarity involved the cities' strategic locations along trade routes. At the height of its power in the late 14th century, the merchants of the Hanseatic League succeeded in using their economic clout, and sometimes their military might—trade routes required protection and the league's ships sailed well-armed—to influence imperial policy.\n\nThe league also wielded power abroad. Between 1361 and 1370, it waged war against Denmark. Initially unsuccessful, Hanseatic towns in 1368 allied in the Confederation of Cologne, sacked Copenhagen and Helsingborg, and forced Valdemar IV, King of Denmark, and his son-in-law Haakon VI, King of Norway, to grant the league 15% of the profits from Danish trade in the subsequent peace treaty of Stralsund in 1370, thus gaining an effective trade and economic monopoly in Scandinavia. This favourable treaty marked the height of Hanseatic power. After the Danish-Hanseatic War (1426–1435) and the Bombardment of Copenhagen (1428), the commercial privileges were renewed in the Treaty of Vordingborg in 1435.\n\nThe Hansa also waged a vigorous campaign against pirates. Between 1392 and 1440, maritime trade of the league faced danger from raids of the Victual Brothers and their descendants, privateers hired in 1392 by Albert of Mecklenburg, King of Sweden, against Margaret I, Queen of Denmark. In the Dutch–Hanseatic War (1438–41), the merchants of Amsterdam sought and eventually won free access to the Baltic and broke the Hanseatic monopoly. As an essential part of protecting their investment in the ships and their cargoes, the League trained pilots and erected lighthouses.\n\nMost foreign cities confined the Hanseatic traders to certain trading areas and to their own trading posts. They seldom interacted with the local inhabitants, except when doing business. Many locals, merchant and noble alike, envied the power of the league and tried to diminish it. For example, in London, the local merchants exerted continuing pressure for the revocation of privileges. The refusal of the Hansa to offer reciprocal arrangements to their English counterparts exacerbated the tension. King Edward IV of England reconfirmed the league's privileges in the Treaty of Utrecht (1474) despite the latent hostility, in part thanks to the significant financial contribution the league made to the Yorkist side during the Wars of the Roses. In 1597, Queen Elizabeth I of England expelled the league from London, and the Steelyard closed the following year. Ivan III of Russia closed the Hanseatic \"Kontor\" at Novgorod in 1494. The very existence of the league and its privileges and monopolies created economic and social tensions that often crept over into rivalries between league members.\n\nThe economic crises of the late 15th century did not spare the Hansa. Nevertheless, its eventual rivals emerged in the form of the territorial states, whether new or revived, and not just in the west: Poland triumphed over the Teutonic Knights in 1466; Ivan III, Grand Prince of Moscow, ended the entrepreneurial independence of Hansa's Novgorod \"Kontor\" in 1478—it closed completely and finally in 1494. New vehicles of credit were imported from Italy, where double-entry book-keeping was invented in 1492, and outpaced the Hansa economy, in which silver coins changed hands rather than bills of exchange.\n\nIn the 15th century, tensions between the Prussian region and the \"Wendish\" cities (Lübeck and its eastern neighbours) increased. Lübeck was dependent on its role as centre of the Hansa, being on the shore of the sea without a major river. It was on the entrance of the land route to Hamburg, but this land route could be bypassed by sea travel around Denmark and through the Kattegat. Prussia's main interest, on the other hand, was the export of bulk products like grain and timber, which were very important for England, the Low Countries, and, later on, also for Spain and Italy.\n\nIn 1454, the year of the marriage of Elisabeth of Austria to the Jagiellonian king, the towns of the Prussian Confederation rose up against the dominance of the Teutonic Order and asked Casimir IV, King of Poland, for help. Gdańsk (Danzig), Thorn and Elbing became part of the Kingdom of Poland, (from 1466–1569 referred to as Royal Prussia, region of Poland) by the Second Peace of Thorn (1466). Poland in turn was heavily supported by the Holy Roman Empire through family connections and by military assistance under the Habsburgs. Kraków, then the capital of Poland, had a loose association with the Hansa. The lack of customs borders on the River Vistula after 1466 helped to gradually increase Polish grain exports, transported to the sea down the Vistula, from per year, in the late 15th century, to over in the 17th century. The Hansa-dominated maritime grain trade made Poland one of the main areas of its activity, helping Danzig to become the Hansa's largest city.\n\nThe member cities took responsibility for their own protection. In 1567, a Hanseatic League agreement reconfirmed previous obligations and rights of league members, such as common protection and defense against enemies. The Prussian Quartier cities of Thorn, Elbing, Königsberg and Riga and Dorpat also signed. When pressed by the King of Poland–Lithuania, Danzig remained neutral and would not allow ships running for Poland into its territory. They had to anchor somewhere else, such as at Pautzke (Puck).\n\nA major economic advantage for the Hansa was its control of the shipbuilding market, mainly in Lübeck and in Danzig. The Hansa sold ships everywhere in Europe, including Italy. They drove out the Dutch, because Holland wanted to favour Bruges as a huge staple market at the end of a trade route. When the Dutch started to become competitors of the Hansa in shipbuilding, the Hansa tried to stop the flow of shipbuilding technology from Hanseatic towns to Holland. Danzig, a trading partner of Amsterdam, attempted to forestall the decision. Dutch ships sailed to Danzig to take grain from the city directly, to the dismay of Lübeck. Hollanders also circumvented the Hanseatic towns by trading directly with north German princes in non-Hanseatic towns. Dutch freight costs were much lower than those of the Hansa, and the Hansa were excluded as middlemen.\n\nWhen Bruges, Antwerp and Holland all became part of the Duchy of Burgundy they actively tried to take over the monopoly of trade from the Hansa, and the staples market from Bruges was transferred to Amsterdam. The Dutch merchants aggressively challenged the Hansa and met with much success. Hanseatic cities in Prussia, Livonia, supported the Dutch against the core cities of the Hansa in northern Germany. After several naval wars between Burgundy and the Hanseatic fleets, Amsterdam gained the position of leading port for Polish and Baltic grain from the late 15th century onwards. The Dutch regarded Amsterdam's grain trade as the mother of all trades (\"Moedernegotie\").\n\nNuremberg in Franconia developed an overland route to sell formerly Hansa-monopolised products from Frankfurt via Nuremberg and Leipzig to Poland and Russia, trading Flemish cloth and French wine in exchange for grain and furs from the east. The Hansa profited from the Nuremberg trade by allowing Nurembergers to settle in Hanseatic towns, which the Franconians exploited by taking over trade with Sweden as well. The Nuremberger merchant Albrecht Moldenhauer was influential in developing the trade with Sweden and Norway, and his sons Wolf Moldenhauer and Burghard Moldenhauer established themselves in Bergen and Stockholm, becoming leaders of the local Hanseatic activities.\n\nAt the start of the 16th century, the league found itself in a weaker position than it had known for many years. The rising Swedish Empire had taken control of much of the Baltic Sea. Denmark had regained control over its own trade, the \"Kontor\" in Novgorod had closed, and the \"Kontor\" in Bruges had become effectively moribund. The individual cities making up the league had also started to put self-interest before their common Hanseatic interests. Finally, the political authority of the German princes had started to grow, constraining the independence of the merchants and Hanseatic towns.\n\nThe league attempted to deal with some of these issues: it created the post of Syndic in 1556 and elected Heinrich Sudermann as a permanent official with legal training, who worked to protect and extend the diplomatic agreements of the member towns. In 1557 and 1579 revised agreements spelled out the duties of towns and some progress was made. The Bruges \"Kontor\" moved to Antwerp and the Hansa attempted to pioneer new routes. However the league proved unable to prevent the growing mercantile competition, and so a long decline commenced. The Antwerp \"Kontor\" closed in 1593, followed by the London \"Kontor\" in 1598. The Bergen \"Kontor\" continued until 1754; of all the \"Kontore\", only its buildings, the \"Bryggen\", survive.\n\nThe gigantic warship \"Adler von Lübeck\" was constructed for military use against Sweden during the Northern Seven Years' War (1563–70) but was never put to military use, epitomizing the vain attempts of Lübeck to uphold its long-privileged commercial position in a changing economic and political climate.\n\nBy the late 16th century, the league had imploded and could no longer deal with its own internal struggles. The social and political changes that accompanied the Protestant Reformation included the rise of Dutch and English merchants and the pressure of the Ottoman Empire upon the Holy Roman Empire and its trade routes. Only nine members attended the last formal meeting in 1669 and only three (Lübeck, Hamburg and Bremen) remained as members until its demise in 1862, in the wake of the creation of the German Empire under Kaiser Wilhelm I. Hence, only Lübeck, Hamburg, and Bremen retain the words \"Hanseatic City\" in their official German titles.\n\nDespite its collapse, several cities still maintained the link to the Hanseatic League. Dutch cities including Groningen, Deventer, Kampen, Zutphen and Zwolle, and a number of German cities including Bremen, Demmin, Greifswald, Hamburg, Lübeck, Lüneburg, Rostock, Stade, Stralsund and Wismar still call themselves \"Hanse\" cities (their car license plates are prefixed \"H\", e.g. –\"HB\"– for \"Hansestadt Bremen\"). Hamburg and Bremen continue to style themselves officially as \"free Hanseatic cities\", with Lübeck named \"Hanseatic City\" (Rostock's football team is named F.C. Hansa Rostock in memory of the city's trading past). For Lübeck in particular, this anachronistic tie to a glorious past remained especially important in the 20th century. In 1937, the Nazi Party removed this privilege through the Greater Hamburg Act possibly because the \"Senat\" of Lübeck did not permit Adolf Hitler to speak in Lübeck during his 1932 election campaign. He held the speech in Bad Schwartau, a small village on the outskirts of Lübeck. Subsequently, he referred to Lübeck as \"the small city close to Bad Schwartau.\"\n\nAfter the EU enlargement to the East in May 2004 there were some experts who wrote about the resurrection of the Baltic Hansa.\n\nThe legacy of the Hansa is remembered today in several names: the German airline Lufthansa (i.e., \"Air Hansa\"); F.C. Hansa Rostock; Hanze University of Applied Sciences, Groningen, Netherlands; Hanze oil production platform, Netherlands; the Hansa Brewery in Bergen and the Hanse Sail in Rostock. DDG Hansa was a major German shipping company from 1881 until its bankruptcy in 1980. Hansabank in the Baltic states has been rebranded into Swedbank).\n\nThere are two museums in Europe dedicated specifically to the history of the Hanseatic League: the European Hansemuseum in Lübeck and the Hanseatic Museum and Schøtstuene in Bergen.\n\nThe members of the Hanseatic League were Low German merchants, whose towns were, with the exception of Dinant, where these merchants held citizenship. Not all towns with Low German merchant communities were members of the league (e.g., Emden, Memel (today Klaipėda), Viborg (today Vyborg) and Narva never joined). However, Hanseatic merchants could also come from settlements without German town law—the premise for league membership was birth to German parents, subjection to German law, and a commercial education. The league served to advance and defend the common interests of its heterogeneous members: commercial ambitions such as enhancement of trade, and political ambitions such as ensuring maximum independence from the noble territorial rulers.\n\nDecisions and actions of the Hanseatic League were the consequence of a consensus-based procedure. If an issue arose, the league's members were invited to participate in a central meeting, the \"Tagfahrt\" (\"meeting ride\", sometimes also referred to as \"Hansetag\", since 1358). The member communities then chose envoys (\"Ratssendeboten\") to represent their local consensus on the issue at the \"Tagfahrt\". Not every community sent an envoy, delegates were often entitled to represent a set of communities. Consensus-building on local and \"Tagfahrt\" levels followed the Low Saxon tradition of \"Einung\", where consensus was defined as absence of protest: after a discussion, the proposals which gained sufficient support were dictated aloud to the scribe and passed as binding \"Rezess\" if the attendees did not object; those favouring alternative proposals unlikely to get sufficient support were obliged to remain silent during this procedure. If consensus could not be established on a certain issue, it was found instead in the appointment of a number of league members who were then empowered to work out a compromise.\n\nThe Hanseatic \"Kontore\", which operated like an early stock exchange,\neach had their own treasury, court and seal. Like the guilds, the \"Kontore\" were led by \"Ältermänner\" (\"eldermen\", or English aldermen). The Stalhof \"Kontor\", as a special case, had a Hanseatic and an English \"Ältermann\". In 1347 the \"Kontor\" of Brussels modified its statute to ensure an equal representation of the league's members. To that end, member communities from different regions were pooled into three circles (\"Drittel\" (\"third [part]\"): the Wendish and Saxon Drittel, the Westphalian and Prussian Drittel as well as the Gothlandian, Livonian and Swedish Drittel. The merchants from their respective \"Drittel\" would then each choose two \"Ältermänner\" and six members of the Eighteen Men's Council (\"Achtzehnmännerrat\") to administer the \"Kontor\" for a set period of time. In 1356, during a Hanseatic meeting in preparation of the first \"Tagfahrt\", the league confirmed this statute. The league in general gradually adopted and institutionalized the division into \"Drittel\" (see table).\n\nThe \"Tagfahrt\" or \"Hansetag\" was the only central institution of the Hanseatic League. However, with the division into \"Drittel\", the members of the respective subdivisions frequently held a \"Dritteltage\" (\"\"Drittel\" meeting\") to work out common positions which could then be presented at a \"Tagfahrt\". On a more local level, league members also met, and while such regional meetings were never formalized into a Hanseatic institution, they gradually gained importance in the process of preparing and implementing \"Tagfahrt\" decisions.\n\nFrom 1554, the division into \"Drittel\" was modified to reduce the circles' heterogeneity, to enhance the collaboration of the members on a local level and thus to make the league's decision-making process more efficient. The number of circles rose to four, so they were called \"Quartiere\" (quarters):\n\nThis division was however not adopted by the \"Kontore\", who, for their purposes (like \"Ältermänner\" elections), grouped the league members in different ways (e.g., the division adopted by the Stahlhof in London in 1554 grouped the league members into \"Dritteln\", whereby Lübeck merchants represented the Wendish, Pomeranian Saxon and several Westphalian towns, Cologne merchants represented the Cleves, Mark, Berg and Dutch towns, while Danzig merchants represented the Prussian and Livonian towns).\n\nThe names of the Quarters have been abbreviated in the following table:\n\nKontor: The Kontore were foreign trading posts of the League, not cities that were Hanseatic members, and are set apart in a separate table below.\n\nThe remaining column headings are as follows:\n\n\n\nIn 1980, former Hanseatic League members established a \"new Hanse\" in Zwolle. This league is open to all former Hanseatic League members and cities that share a Hanseatic Heritage. In 2012 the New Hanseatic league had 187 members. This includes twelve Russian cities, most notably Novgorod, which was a major Russian trade partner of the Hansa in the Middle Ages. The \"new Hanse\" fosters and develops business links, tourism and cultural exchange.\n\nThe headquarters of the New Hansa is in Lübeck, Germany. The current President of the Hanseatic League of New Time is Bernd Saxe, Mayor of Lübeck.\n\nEach year one of the member cities of the New Hansa hosts the Hanseatic Days of New Time international festival.\n\nIn 2006 King's Lynn became the first English member of the newly formed new Hanseatic League. It was joined by Hull in 2012 and Boston in 2016.\n\nThe so-called \"New Hanseatic League\" was established in February 2018 by finance ministers from Denmark, Estonia, Finland, Ireland, Latvia, Lithuania, the Netherlands and Sweden through the signing of a two-page foundational document which set out the countries' \"shared views and values in the discussion on the architecture of the EMU.\"\n\n\n\n"}
{"id": "14107", "url": "https://en.wikipedia.org/wiki?curid=14107", "title": "Harvard (disambiguation)", "text": "Harvard (disambiguation)\n\nHarvard University is a university in Cambridge, Massachusetts, USA.\n\nHarvard may also refer to:\n\n\n\n\n"}
{"id": "14108", "url": "https://en.wikipedia.org/wiki?curid=14108", "title": "Historical African place names", "text": "Historical African place names\n\nThis is a list of historical African place names. The names on the left are linked to the corresponding subregion(s) from History of Africa.\n\nSee also: List of extinct countries, empires, etc.\n"}
{"id": "14109", "url": "https://en.wikipedia.org/wiki?curid=14109", "title": "Horror fiction", "text": "Horror fiction\n\nHorror is a genre of speculative fiction which is intended to frighten, scare, disgust, or startle its readers by inducing feelings of horror and terror. Literary historian J. A. Cuddon defined the horror story as \"a piece of fiction in prose of variable length... which shocks, or even frightens the reader, or perhaps induces a feeling of repulsion or loathing\". It creates an eerie and frightening atmosphere. Horror is frequently supernatural, though it can be non-supernatural. Often the central menace of a work of horror fiction can be interpreted as a metaphor for the larger fears of a society.\n\nThe horror genre has ancient origins with roots in folklore and religious traditions, focusing on death, the afterlife, evil, the demonic and the principle of the thing embodied in the person. These were manifested in stories of beings such as witches, vampires, werewolves and ghosts. European horror fiction became established through works by the Ancient Greeks and Ancient Romans. The well-known 19th century novel about Frankenstein was greatly influenced by the story of Hippolytus, where Asclepius revives him from death. Euripides wrote plays based on the story, \"Hippolytos Kalyptomenos\" and \"Hippolytus\". Plutarch's \"The Lives of the Noble Grecians and Romans: \"Cimon\" describes the spirit of a murderer, Damon, who himself was murdered in a bathhouse in Chaeronea.\n\nPliny the Younger tells the tale of Athenodorus Cananites who bought a haunted house in Athens. Athenodorus was cautious since the house was inexpensive. While writing a book on philosophy, he was visited by a spectre bound in chains. The figure disappeared in the courtyard; the following day, the magistrates dug it up to find an unmarked grave.\n\nThe earliest recording of an official accusation of Satanism by the Church took place in Toulouse in AD 1022 against a couple of clerics. Werewolf stories were popular in medieval French literature. One of Marie de France's twelve lais is a werewolf story titled \"Bisclavret\". \n\nThe Countess Yolande commissioned a werewolf story titled \"Guillaume de Palerme\". Anonymous writers penned two werewolf stories, \"Biclarel\" and \"Melion\".\n\nMuch horror fiction derives from the cruellest personages of the 15th century. Dracula can be traced to the Prince of Wallachia Vlad III whose alleged war crimes were published in German pamphlets. A 1499 pamphlet published by Markus Ayrer is most notable for its woodcut imagery. The alleged serial killer spree of Giles de Rais have been seen as the inspiration for \"Bluebeard\". The motif of the vampiress is most notably derived from the real life noblewoman and murderess, Elizabeth Bathory, and helped usher in the emergence of horror fiction in the 18th century, such as through László Turóczi's 1729 book \"Tragica Historia\".\n\nThe 18th century saw the gradual development of Romanticism and the Gothic horror genre. It drew on the written and material heritage of the Late Middle Ages, finding its form with Horace Walpole's seminal and controversial 1874 novel \"The Castle of Otranto\". In fact, the first edition was published disguised as an actual medieval romance from Italy, discovered and republished by a fictitious translator. Once revealed as modern, many found it anachronistic, reactionary, or simply in poor taste — but it proved immediately popular. \"Otranto\" inspired \"Vathek\" (1786) by William Beckford, \"A Sicilian Romance\" (1790), \"The Mysteries of Udolpho\" (1794) and \"The Italian\" (1796) by Ann Radcliffe and \"The Monk\" (1797) by Matthew Lewis. A significant amount of horror fiction of this era was written by women and marketed towards a female audience, a typical scenario being a resourceful female menaced in a gloomy castle.\n\nThe Gothic tradition blossomed into the genre modern readers call horror literature in the 19th century. Influential works and characters that continue resonating in fiction and film today saw their genesis in the Brothers Grimm's \"Hänsel und Gretel\" (1812), Mary Shelley's \"Frankenstein\" (1818), Washington Irving's \"The Legend of Sleepy Hollow\" (1820), Jane C. Loudon's \"\" (1827), Victor Hugo's \"The Hunchback of Notre Dame\" (1831), Thomas Peckett Prest's \"Varney the Vampire\" (1847), Nathaniel Hawthorne's \"The Scarlet Letter\" (1850), the works of Edgar Allan Poe, the works of Sheridan Le Fanu, Robert Louis Stevenson's \"Strange Case of Dr Jekyll and Mr Hyde\" (1886), Oscar Wilde's \"The Picture of Dorian Gray\" (1890), H. G. Wells' \"The Invisible Man\" (1897), and Bram Stoker's \"Dracula\" (1897). Each of these works created an enduring icon of horror seen in later re-imaginings on the page, stage and screen.\n\nA proliferation of cheap periodicals around turn of the century led to a boom in horror writing. For example, Gaston Leroux serialized his \"Le Fantôme de l'Opéra\" before it was a novel in 1910. One writer who specialized in horror fiction for mainstream pulps such as \"All-Story Magazine\" was Tod Robbins, whose fiction deals with themes of madness and cruelty. Later, specialist publications emerged to give horror writers an outlet, prominent among them \"Weird Tales\" and \"Unknown Worlds\".\n\nInfluential horror writers of the early 20th century made inroads in these mediums. Particularly, the venerated horror author H. P. Lovecraft, and his enduring Cthulhu Mythos pioneered the genre of cosmic horror, and M. R. James is credited with redefining the ghost story in that era.\n\nThe serial murderer became a recurring theme. Yellow journalism and sensationalism of various murderers, such as Jack the Ripper, and lesser so, Carl Panzram, Fritz Haarman, and Albert Fish, all perpetuated this phenomenon. The trend continued in the postwar era, partly renewed after the murders committed by Ed Gein. In 1959, Robert Bloch, inspired by the murders, wrote \"Psycho\". The crimes committed in 1969 by the Manson family influenced the slasher theme in horror fiction of the 1970s. In 1981, Thomas Harris wrote \"Red Dragon\", introducing Dr. Hannibal Lecter. In 1988, Harris's sequel appeared, \"The Silence of the Lambs\".\n\nEarly cinema was inspired by many aspects of horror literature, and started a strong tradition of horror films and subgenres that continues to this day. Up until the graphic depictions of violence and gore on the screen commonly associated with 1960s and 1970s slasher films and splatter films, comic books such as those published by EC Comics (famous for series such as \"Tales From The Crypt\") in the 1950s satisfied readers' quests for horror imagery that the silver screen could not provide. This imagery made these comics controversial, and as a consequence they were frequently censored.\n\nThe modern zombie tale dealing with the motif of the living dead harks back to works including H.P. Lovecraft's stories \"Cool Air\" (1925), \"In The Vault\" (1926), and \"The Outsider\" (1926), and Dennis Wheatley's \"Strange Conflict\" (1941). Richard Matheson's novel \"I Am Legend\" (1954) influenced an entire genre of apocalyptic zombie fiction emblematised by the films of George A. Romero.\nOne of the best-known late-20th century horror writers is Stephen King, known for \"Carrie\", \"The Shining\", \"It\", \"Misery\" and many more. Beginning in the 1970s, King's stories have attracted a large audience, for which he was awarded by the U.S. National Book Foundation in 2003. Other popular horror authors of the period included Anne Rice, Brian Lumley, James Herbert, Dean Koontz, Clive Barker, Ramsey Campbell, and Peter Straub.\n\nBest-selling book series of contemporary times exist in genres related to horror fiction, such as the werewolf fiction urban fantasy Kitty Norville books by Carrie Vaughn (2005 onward). Horror elements continue to expand outside the genre. The alternate history of more traditional historical horror in Dan Simmons's 2007 novel \"The Terror\" sits on bookstore shelves next to genre mash ups such as \"Pride and Prejudice and Zombies\" (2009) and historical fantasy and horror comics such as \"Hellblazer\" (1993 onward) and Mike Mignola's Hellboy (1993 onward). Horror also serves as one of the central genres in more complex modern works such as Mark Z. Danielewski's \"House of Leaves\" (2000), a finalist for the National Book Award. There are many horror novels for teens, such as The Monstrumologist by Rick Yancey (2009).\n\nOne of the defining traits of the genre of horror is that it provokes a response; emotional, psychological or physical, within readers that causes them to react with fear. One of H.P. Lovecraft's most famous quotes about the genre is that: \"The oldest and strongest emotion of mankind is fear, and the oldest and strongest kind of fear is fear of the unknown.\" the first sentence from his seminal essay, \"Supernatural Horror in Literature\".\n\nIn her essay \"Elements of Aversion\", Elizabeth Barrette articulates the need by some for horror tales in a modern world:\nIn a sense similar to the reason a person seeks out the controlled thrill of a roller coaster, readers in the modern era seek out feelings of horror and terror to feel a sense of excitement. However, she adds that horror fiction is one of the few mediums where readers seek out a form of art that forces themselves to confront ideas and images they \"might rather ignore ... [to challenge] preconceptions of all kinds.\"\n\nOne can see the confrontation of ideas readers and characters would \"rather ignore\" throughout literature, in famous moments such as Hamlet's musings about the skull of Yorick and its implications of the mortality of humanity and the gruesome end that bodies inevitably come to. In horror fiction, the confrontation with the gruesome is often a metaphor for the problems facing the current generation of the author.\n\nStephanie Demetrakopoulos illustrates a common interpretation of one of the benchmarks of the canon of horror literature. Tina Broussard in an annotated bibliography of Dracula surmises Demetrakopoulos' thesis:\n\nIt is a now commonly accepted viewpoint that the horror elements of Dracula's portrayal of vampirism are metaphors for sexuality in a repressed Victorian era. But this is merely one of many interpretations of the metaphor of Dracula. Judith Halberstam postulates many of these in her essay \"Technologies of Monstrosity: Bram Stoker's Dracula\". She writes:\n\nHalberstram articulates a view of Dracula as manifesting the growing perception of the aristocracy as an evil and outdated notion to be defeated. The depiction of a multinational band of protagonists using the latest technologies (such as a telegraph) to quickly share, collate, and act upon new information is what leads to the destruction of the Vampire. This is one of many interpretations of the metaphor of only one central figure of the canon of horror fiction, as over a dozen possible metaphors are referenced in the analysis, from the religious to the anti-semitic.\n\nNoël Carroll's \"Philosophy of Horror\" postulates that a modern piece of horror fiction's \"monster\", villain, or a more inclusive menace must exhibit the following two traits:\n\nIn addition to those essays and articles shown above, scholarship on horror fiction is almost as old as horror fiction itself. In 1826, the gothic novelist Ann Radcliffe published an essay distinguishing two elements of horror fiction, \"terror\" and \"horror.\" Whereas terror is a feeling of dread that takes place before an event happens, horror is a feeling of revulsion or disgust after an event has happened. Radcliffe describes terror as that which \"expands the soul and awakens the faculties to a high degree of life,\" whereas horror is described as that which \"freezes and nearly annihilates them.\"\n\nModern scholarship on horror fiction draws upon a range of sources. In their historical studies of the gothic novel, both Devandra Varma and S.L. Varnado make reference to the theologian Rudolf Otto, whose concept of the \"numinous\" was originally used to describe religious experience.\n\nAchievements in horror fiction are recognized by numerous awards. The Horror Writer's Association presents the Bram Stoker Awards for Superior Achievement, named in honor of Bram Stoker, author of the seminal horror novel \"Dracula\". The Australian Horror Writers Association presents annual Australian Shadows Awards. The International Horror Guild Award was presented annually to works of horror and dark fantasy from 1995 to 2008. The Shirley Jackson Awards are literary awards for outstanding achievement in the literature of psychological suspense, horror and the dark fantastic works. Other important awards for horror literature are as subcategories included within general awards for fantasy and science fiction in such awards as the Aurealis Award.\n\nSome writers of fiction normally classified as \"horror\" nevertheless dislike the term, considering it too lurid. They instead use the terms dark fantasy or Gothic fantasy for supernatural horror, or \"psychological thriller\" for non-supernatural horror.\n\n\n\n"}
{"id": "14110", "url": "https://en.wikipedia.org/wiki?curid=14110", "title": "Holomorphic function", "text": "Holomorphic function\n\nIn mathematics, a holomorphic function is a complex-valued function of one or more complex variables that is, at every point of its domain, complex differentiable in a neighbourhood of the point. The existence of a complex derivative in a neighbourhood is a very strong condition, for it implies that any holomorphic function is actually infinitely differentiable and equal, locally, to its own Taylor series (\"analytic\"). Holomorphic functions are the central objects of study in complex analysis.\n\nThough the term \"analytic function\" is often used interchangeably with \"holomorphic function\", the word \"analytic\" is defined in a broader sense to denote any function (real, complex, or of more general type) that can be written as a convergent power series in a neighbourhood of each point in its domain. The fact that all holomorphic functions are complex analytic functions, and vice versa, is a major theorem in complex analysis.\n\nHolomorphic functions are also sometimes referred to as \"regular functions\". A holomorphic function whose domain is the whole complex plane is called an entire function. The phrase \"holomorphic at a point \"z\"\" means not just differentiable at \"z\", but differentiable everywhere within some neighbourhood of \"z\" in the complex plane.\n\nGiven a complex-valued function \"f\" of a single complex variable, the derivative of \"f\" at a point \"z\" in its domain is defined by the limit\n\nThis is the same as the definition of the derivative for real functions, except that all of the quantities are complex. In particular, the limit is taken as the complex number \"z\" approaches \"z\", and must have the same value for any sequence of complex values for \"z\" that approach \"z\" on the complex plane. If the limit exists, we say that \"f\" is complex-differentiable at the point \"z\". This concept of complex differentiability shares several properties with real differentiability: it is linear and obeys the product rule, quotient rule, and chain rule.\n\nIf \"f\" is \"complex differentiable\" at \"every\" point \"z\" in an open set \"U\", we say that \"f\" is holomorphic on \"U\". We say that \"f\" is holomorphic at the point \"z\" if it is holomorphic on some neighbourhood of \"z\". We say that \"f\" is holomorphic on some non-open set \"A\" if it is holomorphic in an open set containing \"A\". As a pathological non-example, the function given by \"f\"(\"z\") = |\"z\"| is complex differentiable at exactly one point (\"z\" = 0), and for this reason, it is \"not\" holomorphic at 0 because there is no open set around 0 on which \"f\" is complex differentiable. \n\nThe relationship between real differentiability and complex differentiability is the following. If a complex function is holomorphic, then \"u\" and \"v\" have first partial derivatives with respect to \"x\" and \"y\", and satisfy the Cauchy–Riemann equations:\n\nor, equivalently, the Wirtinger derivative of \"f\" with respect to the complex conjugate of \"z\" is zero:\n\nwhich is to say that, roughly, \"f\" is functionally independent from the complex conjugate of \"z\".\n\nIf continuity is not given, the converse is not necessarily true. A simple converse is that if \"u\" and \"v\" have \"continuous\" first partial derivatives and satisfy the Cauchy–Riemann equations, then \"f\" is holomorphic. A more satisfying converse, which is much harder to prove, is the Looman–Menchoff theorem: if \"f\" is continuous, \"u\" and \"v\" have first partial derivatives (but not necessarily continuous), and they satisfy the Cauchy–Riemann equations, then \"f\" is holomorphic.\n\nThe word \"holomorphic\" was introduced by two of Cauchy's students, Briot (1817–1882) and Bouquet (1819–1895), and derives from the Greek ὅλος (\"holos\") meaning \"entire\", and μορφή (\"morphē\") meaning \"form\" or \"appearance\".\n\nToday, the term \"holomorphic function\" is sometimes preferred to \"analytic function\", as the latter is a more general concept. This is also because an important result in complex analysis is that every holomorphic function is complex analytic, a fact that does not follow obviously from the definitions. The term \"analytic\" is however also in wide use.\n\nBecause complex differentiation is linear and obeys the product, quotient, and chain rules; the sums, products and compositions of holomorphic functions are holomorphic, and the quotient of two holomorphic functions is holomorphic wherever the denominator is not zero.\n\nIf one identifies C with R, then the holomorphic functions coincide with those functions of two real variables with continuous first derivatives which solve the Cauchy–Riemann equations, a set of two partial differential equations.\n\nEvery holomorphic function can be separated into its real and imaginary parts, and each of these is a solution of Laplace's equation on R. In other words, if we express a holomorphic function \"f\"(\"z\") as both \"u\" and \"v\" are harmonic functions, where v is the harmonic conjugate of u.\n\nCauchy's integral theorem implies that the contour integral of every holomorphic function along a loop vanishes:\n\nHere \"γ\" is a rectifiable path in a simply connected open subset \"U\" of the complex plane C whose start point is equal to its end point, and is a holomorphic function.\n\nCauchy's integral formula states that every function holomorphic inside a disk is completely determined by its values on the disk's boundary. Furthermore: Suppose \"U\" is an open subset of C, is a holomorphic function and the closed disk is completely contained in \"U\". Let γ be the circle forming the boundary of \"D\". Then for every \"a\" in the interior of \"D\":\n\nwhere the contour integral is taken counter-clockwise.\n\nThe derivative \"f\"′(\"a\") can be written as a contour integral using Cauchy's differentiation formula:\n\nfor any simple loop positively winding once around \"a\", and\n\nfor infinitesimal positive loops γ around \"a\".\n\nIn regions where the first derivative is not zero, holomorphic functions are conformal in the sense that they preserve angles and the shape (but not size) of small figures.\n\nEvery holomorphic function is analytic. That is, a holomorphic function \"f\" has derivatives of every order at each point \"a\" in its domain, and it coincides with its own Taylor series at \"a\" in a neighbourhood of \"a\". In fact, \"f\" coincides with its Taylor series at \"a\" in any disk centred at that point and lying within the domain of the function.\n\nFrom an algebraic point of view, the set of holomorphic functions on an open set is a commutative ring and a complex vector space. Additionally, the set of holomorphic functions in an open set U is an integral domain if and only if the open set U is connected. In fact, it is a locally convex topological vector space, with the seminorms being the suprema on compact subsets.\n\nFrom a geometric perspective, a function \"f\" is holomorphic at \"z\" if and only if its exterior derivative \"df\" in a neighbourhood \"U\" of \"z\" is equal to \"f\"′(\"z\") \"dz\" for some continuous function \"f\"′. It follows from\n\nthat \"df\"′ is also proportional to \"dz\", implying that the derivative \"f\"′ is itself holomorphic and thus that \"f\" is infinitely differentiable. Similarly, the fact that implies that any function \"f\" that is holomorphic on the simply connected region \"U\" is also integrable on \"U\". (For a path γ from \"z\" to \"z\" lying entirely in \"U\", define\n\nin light of the Jordan curve theorem and the generalized Stokes' theorem, \"F\"(\"z\") is independent of the particular choice of path γ, and thus \"F\"(\"z\") is a well-defined function on \"U\" having and .)\n\nAll polynomial functions in \"z\" with complex coefficients are holomorphic on C, and so are sine, cosine and the exponential function. (The trigonometric functions are in fact closely related to and can be defined via the exponential function using Euler's formula). The principal branch of the complex logarithm function is holomorphic on the set The square root function can be defined as\n\nand is therefore holomorphic wherever the logarithm log(\"z\") is. The function 1/\"z\" is holomorphic on \n\nAs a consequence of the Cauchy–Riemann equations, a real-valued holomorphic function must be constant. Therefore, the absolute value of \"z\", the argument of \"z\", the real part of \"z\" and the imaginary part of \"z\" are not holomorphic. Another typical example of a continuous function which is not holomorphic is the complex conjugate formed by complex conjugation.\n\nThe definition of a holomorphic function generalizes to several complex variables in a straightforward way. Let \"D\" denote an open subset of C, and let . The function \"f\" is analytic at a point \"p\" in \"D\" if there exists an open neighbourhood of \"p\" in which \"f\" is equal to a convergent power series in \"n\" complex variables. Define \"f\" to be holomorphic if it is analytic at each point in its domain. Osgood's lemma shows (using the multivariate Cauchy integral formula) that, for a continuous function \"f\", this is equivalent to \"f\" being holomorphic in each variable separately (meaning that if any coordinates are fixed, then the restriction of \"f\" is a holomorphic function of the remaining coordinate). The much deeper Hartogs' theorem proves that the continuity hypothesis is unnecessary: \"f\" is holomorphic if and only if it is holomorphic in each variable separately.\n\nMore generally, a function of several complex variables that is square integrable over every compact subset of its domain is analytic if and only if it satisfies the Cauchy–Riemann equations in the sense of distributions.\n\nFunctions of several complex variables are in some basic ways more complicated than functions of a single complex variable. For example, the region of convergence of a power series is not necessarily an open ball; these regions are Reinhardt domains, the simplest example of which is a polydisk. However, they also come with some fundamental restrictions. Unlike functions of a single complex variable, the possible domains on which there are holomorphic functions that cannot be extended to larger domains are highly limited. Such a set is called a domain of holomorphy.\n\nThe concept of a holomorphic function can be extended to the infinite-dimensional spaces of functional analysis. For instance, the Fréchet or Gâteaux derivative can be used to define a notion of a holomorphic function on a Banach space over the field of complex numbers.\n\n"}
{"id": "14113", "url": "https://en.wikipedia.org/wiki?curid=14113", "title": "History of Algeria", "text": "History of Algeria\n\nMuch of the history of Algeria has taken place \non the fertile coastal plain of North Africa, which is often called the Maghreb (or Maghrib). North Africa served as a transit region for people moving towards Europe or the Middle East, thus, the region's inhabitants have been influenced by populations from other areas, including the Carthaginians, Romans, and Vandals. The region was conquered by the Muslims in the early 8th century AD, but broke off from the Umayyad Caliphate after the Berber Revolt of 740. Later, various Berbers, Arabs, Persian Muslim states, Sunni, Shia or Ibadi communities were established that ruled parts of modern-day of Algeria: including the Rustamids, Ifranids, Fatimids, Maghrawas, Zirids, Hammadids, Almoravid, Almohads, Hafsids, and Ziyyanids. During the Ottoman period, Algiers was the center of the Barbary slave trade which led to many naval conflicts. The last significant events in the country's recent history have been the Algerian War and Algerian Civil War.\n\nEvidence of the early human occupation of Algeria is demonstrated by the discovery of 1.8 million year old Oldowan stone tools found at Ain Hanech in 1992. In 1954 fossilised \"Homo erectus\" bones were discovered by C. Arambourg at Ternefine that are 700,000 years old. Neolithic civilization (marked by animal domestication and subsistence agriculture) developed in the Saharan and Mediterranean Maghrib between 6000 and 2000 BC. This type of economy, richly depicted in the Tassili n'Ajjer cave paintings in southeastern Algeria, predominated in the Maghrib until the classical period. The amalgam of peoples of North Africa coalesced eventually into a distinct native population, the Berbers lacked a written language and hence tended to be overlooked or marginalized in historical accounts.\n\nSince 4000 BC, the indigenous peoples of northern Africa successfully resisted Phoenician, Roman, Vandal, Byzantine, Turkish, and French invaders but accepted Islam between the 7th to 9th century, and Arabic is now the language spoken by a majority in the country.\n\nPhoenician traders arrived on the North African coast around 900 BC and established Carthage (in present-day Tunisia) around 800 BC. During the classical period, Berber civilization was already at a stage in which agriculture, manufacturing, trade, and political organization supported several states. Trade links between Carthage and the Berbers in the interior grew, but territorial expansion also resulted in the enslavement or military recruitment of some Berbers and in the extraction of tribute from others.\n\nThe Carthaginian state declined because of successive defeats by the Romans in the Punic Wars, and in 146 BC, the city of Carthage was destroyed. As Carthaginian power waned, the influence of Berber leaders in the hinterland grew.\n\nBy the 2nd century BC, several large but loosely administered Berber kingdoms had emerged. After that, king Masinissa managed to unify Numidia under his rule.\n\nMadghacen was a king of independent kingdoms of the Numidians, between 12 and 3 BC.\n\nBerber territory was annexed by the Roman Empire in AD 24. Increases in urbanization and in the area under cultivation during Roman rule caused wholesale dislocations of Berber society, and Berber opposition to the Roman presence was nearly constant. The prosperity of most towns depended on agriculture, and the region was known as the \"breadbasket of the empire\".\n\nChristianity arrived in the 2nd century. By the end of the 4th century, the settled areas had become christianized, and some Berber tribes had converted en masse.\n\nFrom the 8th century Umayyad conquest of North Africa led by Musa bin Nusayr, Arab colonization started. The 11th century invasion of migrants from the Arabian peninsula brought oriental tribal customs. The introduction of Islam and Arabic had a profound impact on North Africa. The new religion and language introduced changes in social and economic relations, and established links with the Arab world through acculturation and assimilation.\n\nAccording to historians of the Middle Ages, the Berbers are divided into two branches, both are from their ancestor Mazigh. The two branches Botr and Barnès are divided into tribes, and each Maghreb region is made up of several tribes. The large Berber tribes or peoples are Sanhaja, Houara, Zenata, Masmuda, Kutama, Awarba, Barghawata ... etc. Each tribe is divided into sub tribes. All these tribes have independence and territorial decisions.\n\nSeveral Berber dynasties emerged during the Middle Ages: - In North Africa, Sudan, in Andalusia, Italy, in Mali, Niger, Senegal and Egypt. Ibn Khaldoun made a table of Berber dynasties: Zirid, Banu Ifran, Maghrawa, Almoravid, Hammadid, Almohad Caliphate, Marinid, Zayyanid, Wattasid, Meknes, Hafsid dynasty.\n\nThe invasion of the Banu Hilal Arab tribes in 11th century, sacked Kairouan, and the area under Zirid control was reduced to the coastal region, and the Arab conquests fragmented into petty Bedouin emirates.\n\nThe second Arab military expeditions into the Maghreb, between 642 and 669, resulted in the spread of Islam. The Umayyads (a Muslim dynasty based in Damascus from 661 to 750) recognised that the strategic necessity of dominating the Mediterranean dictated a concerted military effort on the North African front. By 711 Umayyad forces helped by Berber converts to Islam had conquered all of North Africa. In 750 the Abbasids succeeded the Umayyads as Muslim rulers and moved the caliphate to Baghdad. Under the Abbasids, Berber Kharijites Sufri Banu Ifran were opposed to Umayyad and Abbasids. After, the Rustumids (761–909) actually ruled most of the central Maghrib from Tahirt, southwest of Algiers. The imams gained a reputation for honesty, piety, and justice, and the court of Tahirt was noted for its support of scholarship. The Rustumid imams failed, however, to organise a reliable standing army, which opened the way for Tahirt's demise under the assault of the Fatimid dynasty.\n\nWith their interest focused primarily on Egypt and Muslim lands beyond, the Fatimids left the rule of most of Algeria to the Zirids and Hammadid (972–1148), a Berber dynasty that centered significant local power in Algeria for the first time, but who were still at war with Banu Ifran (kingdom of Tlemcen) and Maghraoua (942-1068). This period was marked by constant conflict, political instability, and economic decline. Following a large incursion of Arab Bedouin from Egypt beginning in the first half of the 11th century, the use of Arabic spread to the countryside, and sedentary Berbers were gradually Arabised.\n\nThe Almoravid (\"those who have made a religious retreat\") movement developed early in the 11th century among the Sanhaja Berbers of southern Morocco. The movement's initial impetus was religious, an attempt by a tribal leader to impose moral discipline and strict adherence to Islamic principles on followers. But the Almoravid movement shifted to engaging in military conquest after 1054. By 1106, the Almoravids had conquered the Maghreb as far east as Algiers and Morocco, and Spain up to the Ebro River.\n\nLike the Almoravids, the Almohads (\"unitarians\") found their inspiration in Islamic reform. The Almohads took control of Morocco by 1146, captured Algiers around 1151, and by 1160 had completed the conquest of the central Maghrib. The zenith of Almohad power occurred between 1163 and 1199. For the first time, the Maghrib was united under a local regime, but the continuing wars in Spain overtaxed the resources of the Almohads, and in the Maghrib their position was compromised by factional strife and a renewal of tribal warfare.\n\nIn the central Maghrib, the Abdalwadid founded a dynasty that ruled the Kingdom of Tlemcen in Algeria. For more than 300 years, until the region came under Ottoman suzerainty in the 16th century, the Zayanids kept a tenuous hold in the central Maghrib. Many coastal cities asserted their autonomy as municipal republics governed by merchant oligarchies, tribal chieftains from the surrounding countryside, or the privateers who operated out of their ports. Nonetheless, Tlemcen, the \"pearl of the Maghrib,\" prospered as a commercial center.\n\nThe final triumph of the 700-year Christian reconquest of Spain was marked by the fall of Granada in 1492. Christian Spain imposed its influence on the Maghrib coast by constructing fortified outposts and collecting tribute. But Spain never sought to extend its North African conquests much beyond a few modest enclaves. Privateering was an age-old practice in the Mediterranean, and North African rulers engaged in it increasingly in the late 16th and early 17th centuries because it was so lucrative. Algeria became the privateering city-state par excellence, and two privateer brothers were instrumental in extending Ottoman influence in Algeria. At about the time Spain was establishing its presidios in the Maghrib, the Muslim privateer brothers Aruj and Khair ad Din—the latter known to Europeans as Barbarossa, or Red Beard—were operating successfully off Tunisia. In 1516 Aruj moved his base of operations to Algiers but was killed in 1518. Khair ad Din succeeded him as military commander of Algiers, and the Ottoman sultan gave him the title of beylerbey (provincial governor).\n\nThe Spanish expansionist policy in North Africa begun with the Catholic Monarchs and the regent Cisneros, once the \"Reconquista\" in the Iberian Peninsula was finished. That way, several towns and outposts in the Algerian coast were conquered and occupied: Mers El Kébir (1505), Oran (1509), Algiers (1510) and Bugia (1510). The Spaniards left Algiers in 1529, Bujia in 1554, Mers El Kébir and Oran in 1708. The Spanish returned in 1732 when the armada of the Duke of Montemar was victorious in the Battle of Aïn-el-Turk and took again Oran and Mers El Kébir. Both cities were held until 1792, when they were sold by the king Charles IV to the Bey of Algiers.\n\nUnder Khair ad Din's regency, Algiers became the center of Ottoman authority in the Maghrib. For 300 years, Algeria was a province of the Ottoman Empire under a regency that had Algiers as its capital (see Dey). Subsequently, with the institution of a regular Ottoman administration, governors with the title of pasha ruled. Turkish was the official language, and Arabs and Berbers were excluded from government posts. In 1671 a new leader took power, adopting the title of dey. In 1710 the dey persuaded the sultan to recognize him and his successors as regent, replacing the pasha in that role.\n\nAlthough Algiers remained a part of the Ottoman Empire, the Ottoman government ceased to have effective influence there. European maritime powers paid the tribute demanded by the rulers of the privateering states of North Africa (Algiers, Tunis, Tripoli, and Morocco) to prevent attacks on their shipping. The Napoleonic wars of the early 19th century diverted the attention of the maritime powers from suppressing piracy. But when peace was restored to Europe in 1815, Algiers found itself at war with Spain, the Netherlands, Prussia, Denmark, Russia, and Naples. Algeria and surrounding areas, collectively known as the Barbary States, were responsible for piracy in the Mediterranean Sea, as well as the enslaving of Christians, actions which brought them into the First and Second Barbary War with the United States of America.\n\nNorth African boundaries have shifted during various stages of the conquests. The borders of modern Algeria were created by the French, whose colonization began in 1830 (French invasion began on July 5). To benefit French colonists (many of whom were not in fact of French origin but Italian, Maltese, and Spanish) and nearly the entirety of whom lived in urban areas, northern Algeria was eventually organized into overseas departments of France, with representatives in the French National Assembly. France controlled the entire country, but the traditional Muslim population in the rural areas remained separated from the modern economic infrastructure of the European community.\n\nAs a result of what the French considered an insult to the French consul in Algiers by the Day in 1827, France blockaded Algiers for three years. In 1830, France invaded and occupied the coastal areas of Algeria, citing a diplomatic incident as casus belli. Hussein Dey went into exile. French colonization then gradually penetrated southwards, and came to have a profound impact on the area and its populations. The European conquest, initially accepted in the Algiers region, was soon met by a rebellion, led by Abdel Kadir, which took roughly a decade for the French troops to put down.\nBy 1848 nearly all of northern Algeria was under French control, and the new government of the Second Republic declared the occupied lands an integral part of France. Three \"civil territories\"—Algiers, Oran, and Constantine—were organized as French départements (local administrative units) under a civilian government.\n\nIn addition to enduring the affront of being ruled by a foreign, non-Muslim power, many Algerians lost their lands to the new government or to colonists. Traditional leaders were eliminated, coopted, or made irrelevant, and the traditional educational system was largely dismantled; social structures were stressed to the breaking point. From 1856, native Muslims and Jews were viewed as French subjects, but not French \"citizens\".\n\nHowever, in 1865, Napoleon III allowed them to apply for full French citizenship, a measure that few took, since it involved renouncing the right to be governed by \"sharia\" law in personal matters, and was considered a kind of apostasy; in 1870, French citizenship was made automatic for Jewish natives, a move which largely angered many Muslims, which resulted in the Jews being seen as the accomplices of the colonial power by anti-colonial Algerians. Nonetheless, this period saw progress in health, some infrastructures, and the overall expansion of the economy of Algeria, as well as the formation of new social classes, which, after exposure to ideas of equality and political liberty, would help propel the country to independence.\n\nA new generation of Islamic leadership emerged in Algeria at the time of World War I and grew to maturity during the 1920s and 1930s. Various groups were formed in opposition to French rule, most notable the National Liberation Front (FLN) and the National Algerian Movement.\n\n\"Colons\" (colonists), or, more popularly, \"pieds noirs\" (literally, black feet) dominated the government and controlled the bulk of Algeria's wealth. Throughout the colonial era, they continued to block or delay all attempts to implement even the most modest reforms. But from 1933 to 1936, mounting social, political, and economic crises in Algeria induced the indigenous population to engage in numerous acts of political protest. The government responded with more restrictive laws governing public order and security. Algerian Muslims rallied to the French side at the start of World War II as they had done in World War I. But the colons were generally sympathetic to the collaborationist Vichy regime established following France's defeat by Nazi Germany. After the fall of the Vichy regime in Algeria (November 11, 1942) as a result of Operation Torch, the Free French commander in chief in North Africa slowly rescinded repressive Vichy laws, despite opposition by colon extremists.\n\nIn March 1943, Muslim leader Ferhat Abbas presented the French administration with the Manifesto of the Algerian People, signed by 56 Algerian nationalist and international leaders. The manifesto demanded an Algerian constitution that would guarantee immediate and effective political participation and legal equality for Muslims. Instead, the French administration in 1944 instituted a reform package, based on the 1936 Viollette Plan, that granted full French citizenship only to certain categories of \"meritorious\" Algerian Muslims, who numbered about 60,000. In April 1945 the French had arrested the Algerian nationalist leader Messali Hadj. On May 1 the followers of his Parti du Peuple Algérien (PPA) participated in demonstrations which were violently put down by the police. Several Algerians were killed. The tensions between the Muslim and colon communities exploded on May 8, 1945, V-E Day. When a Muslim march was met with violence, marchers rampaged. The army and police responded by conducting a prolonged and systematic ratissage (literally, raking over) of suspected centers of dissidence. According to official French figures, 1,500 Muslims died as a result of these countermeasures. Other estimates vary from 6,000 to as high as 45,000 killed. Many nationalists drew the conclusion that independence could not be won by peaceful means, and so started organizing for violent rebellion.\n\nIn August 1947, the French National Assembly approved the government-proposed Organic Statute of Algeria. This law called for the creation of an Algerian Assembly with one house representing Europeans and \"meritorious\" Muslims and the other representing the remaining 8 million or more Muslims. Muslim and colon deputies alike abstained or voted against the statute but for diametrically opposed reasons: the Muslims because it fell short of their expectations and the colons because it went too far.\n\nThe Algerian War of Independence (1954–1962), brutal and long, was the most recent major turning point in the country's history. Although often fratricidal, it ultimately united Algerians and seared the value of independence and the philosophy of anticolonialism into the national consciousness. Abusive tactics of the French Army remains a controversial subject in France to this day.\n\nIn the early morning hours of November 1, 1954, the National Liberation Front (Front de Libération Nationale—FLN) launched attacks throughout Algeria in the opening salvo of a war of independence. An important watershed in this war was the massacre of civilians by the FLN near the town of Philippeville in August 1955. The government claimed it killed 1,273 guerrillas in retaliation; according to the FLN, 12,000 Muslims perished in an orgy of bloodletting by the armed forces and police, as well as colon gangs. After Philippeville, all-out war began in Algeria. The FLN fought largely using guerrilla tactics whilst the French counter-insurgency tactics often included severe reprisals and repression.\n\nEventually, protracted negotiations led to a cease-fire signed by France and the FLN on March 18, 1962, at Evian, France. The Evian accords also provided for continuing economic, financial, technical, and cultural relations, along with interim administrative arrangements until a referendum on self-determination could be held. The Evian accords guaranteed the religious and property rights of French settlers, but the perception that they would not be respected led to the exodus of one million \"pieds-noirs\" and \"harkis\".\n\nBetween 350,000 and 1 million Algerians are estimated to have died during the war, and more than 2 million, out of a total Muslim population of 9 or 10 million, were made into refugees or forcibly relocated into government-controlled camps. Much of the countryside and agriculture was devastated, along with the modern economy, which had been dominated by urban European settlers (the \"pied-noirs\"). French sources estimated that at least 70,000 Muslim civilians were killed or abducted and presumed killed, by the FLN during the Algerian War. Nearly one million people of mostly French, Spanish and Italian descent were forced to flee the country at independence due to the unbridgeable rifts opened by the civil war and threats from units of the victorious FLN; along with them fled most Algerians of Jewish descent and those Muslim Algerians who had supported a French Algeria (\"harkis\"). 30–150,000 pro-French Muslims were also killed in Algeria by FLN in post-war reprisals.\n\nThe referendum was held in Algeria on 1 July 1962, and France declared Algeria independent on 3 July. On 8 September 1963, a constitution was adopted by referendum, and later that month, Ahmed Ben Bella was formally elected the first president. The war for independence and its aftermath had severely disrupted Algeria's society and economy. In addition to the physical destruction, the exodus of the \"colons\" deprived the country of most of its managers, civil servants, engineers, teachers, physicians, and skilled workers. The homeless and displaced numbered in the hundreds of thousands, many suffering from illness, and some 70 percent of the workforce was unemployed.\n\nThe months immediately following independence witnessed the pell-mell rush of Algerians, their government, and its officials to claim the property and jobs left behind by the Europeans. In the 1963 March Decrees, Ben Bella declared that all agricultural, industrial, and commercial properties previously owned and operated by Europeans were vacant, thereby legalizing confiscation by the state. A new constitution drawn up under close FLN supervision was approved by nationwide referendum in September 1963, and Ben Bella was confirmed as the party's choice to lead the country for a five-year term.\n\nUnder the new constitution, Ben Bella as president combined the functions of chief of state and head of government with those of supreme commander of the armed forces. He formed his government without needing legislative approval and was responsible for the definition and direction of its policies. There was no effective institutional check on its powers. Opposition leader Hocine Aït-Ahmed quit the National Assembly in 1963 to protest the increasingly dictatorial tendencies of the regime and formed a clandestine resistance movement, the Front of Socialist Forces (Front des Forces Socialistes—FFS) dedicated to overthrowing the Ben Bella regime by force.\n\nLate summer 1963 saw sporadic incidents attributed to the FFS. More serious fighting broke out a year later. The army moved quickly and in force to crush the rebellion. As minister of defense, Houari Boumédienne had no qualms about sending the army to put down regional uprisings because he felt they posed a threat to the state. However, when Ben Bella attempted to co-opt allies from among some of those regionalists, tensions increased between Houari Boumédienne and Ahmed Ben Bella. In 1965 the military toppled Ahmed Ben Bella, and Houari Boumedienne became head of state. The military has dominated Algerian politics until today.\n\nOn 19 June 1965, Houari Boumédienne deposed Ahmed Ben Bella in a military coup d'état that was both swift and bloodless. Ben Bella \"disappeared\", and would not be seen again until he was released from house arrest in 1980 by Boumédienne's successor, Colonel Chadli Bendjedid. Boumédienne immediately dissolved the National Assembly and suspended the 1963 constitution. Political power resided in the Council of the Revolution, a predominantly military body intended to foster cooperation among various factions in the army and the party.\n\nHouari Boumédienne's position as head of government and of state was initially not secure partly because of his lack of a significant power base outside the armed forces; he relied strongly on a network of former associates known as the Oujda group (after his posting as ALN leader in the Moroccan border town of Oujda during the war years), but he could not fully dominate the fractious regime. This situation may have accounted for his deference to collegial rule.\n\nFollowing attempted coups—most notably that of chief-of-staff Col. Tahar Zbiri in December 1967—and a failed assassination attempt in (April 25, 1968), Boumédienne consolidated power and forced military and political factions to submit. He took a systematic, authoritarian approach to state building, arguing that Algeria needed stability and an economic base before any political institutions.\n\nEleven years after Houari Boumédienne took power, after much public debate, a long-promised new constitution was promulgated in November 1976, and Boumédienne was elected president with 95 percent of the cast votes.\n\nBoumédienne's death on December 27, 1978 set off a struggle within the FLN to choose a successor. To break a deadlock between two candidates, Colonel Chadli Bendjedid, a moderate who had collaborated with Boumédienne in deposing Ahmed Ben Bella, was sworn in on February 9, 1979. He was re-elected in 1984 and 1988. After the violent 1988 October Riots, a new constitution was adopted in 1989 that allowed the formation of political associations other than the FLN. It also removed the armed forces, which had run the government since the days of Boumédienne, from a role in the operation of the government.\n\nAmong the scores of parties that sprang up under the new constitution, the militant Islamic Salvation Front (FIS) was the most successful, winning more than 50% of all votes cast in municipal elections in June 1990 as well as in first stage of national legislative elections held in December 1991.\n\nThe surprising first round of success for the fundamentalist FIS party in the December 1991 balloting caused the army to intervene, crack down on the FIS, and postpone subsequent elections. The fundamentalist response has resulted in a continuous low-grade, conflict, the Algerian Civil War, with the secular state apparatus, which nonetheless has allowed elections featuring pro-government and moderate religious-based parties. This civil war lasted until 2002.\n\nIn 1996 a referendum introduced changes to the constitution, enhancing presidential powers and banning Islamist parties. Presidential elections were held in April 1999. Although seven candidates qualified for election, all but Abdelaziz Bouteflika, who appeared to have the support of the military as well as the FLN, withdrew on the eve of the election amid charges of electoral fraud. Bouteflika went on to win with 70 percent of the cast votes.\n\nFollowing his election to a five-year term, Bouteflika concentrated on restoring security and stability to the strife-ridden country. As part of his endeavor, he successfully campaigned to provide amnesty to thousands of members of the banned FIS. The so-called Civil Concord was approved in a nationwide referendum in September 2000. The reconciliation by no means ended all violence, but it reduced violence to manageable levels. An estimated 80% of those fighting the regime accepted the amnesty offer.\n\nThe president also formed national commissions to study reforms of the education system, judiciary, and state bureaucracy. President Bouteflika was rewarded for his efforts at stabilizing the country when he was elected to another five-year term in April 2004, in an election contested by six candidates without military interference. In September 2005, another referendum—this one to consider a proposed Charter for Peace and National Reconciliation—passed by an overwhelming margin. The charter coupled another amnesty offer to all but the most violent participants in the Islamist uprising with an implicit pardon for security forces accused of abuses in fighting the rebels.\n\n\n1. The indigenous peoples of northern Africa were identified by the Romans as \"Berbers\", a word derived from the word \"Barbare\" or \"Barbarian\", but they prefer being called \"Imazighen\".\n2. On the Banu Hilal invasion, see Ibn Khaldoun (v.1).\n\n"}
{"id": "14114", "url": "https://en.wikipedia.org/wiki?curid=14114", "title": "History of Zimbabwe", "text": "History of Zimbabwe\n\nFollowing the Lancaster House Agreement of 1979 there was a transition to internationally recognized majority rule in 1980; the United Kingdom ceremonially granted Zimbabwe independence on 18 April that year. In the 2000s Zimbabwe's economy began to deteriorate due to various factors, including, the imposition of economic sanctions by western countries led by the United Kingdom, and also due to wide spread corruption in government. Economic instability caused a lot of Zimbabweans to move overseas or to neighboring countries. Prior to its recognized independence as Zimbabwe in 1980, the nation had been known by several names: Rhodesia, Southern Rhodesia and Zimbabwe Rhodesia.\n\nPrior to the arrival of Bantu speakers in present-day Zimbabwe the region was populated by ancestors of the San people. The first Bantu-speaking farmers arrived during the Bantu expansion around 2000 years ago.\n\nThese Bantu speakers were the makers of early Iron Age pottery belonging to the Silver Leaves or Matola tradition, third to fifth centuries A.D., found in southeast Zimbabwe. This tradition was part of the eastern stream of Bantu expansion (sometimes called Kwale) which originated west of the Great Lakes, spreading to the coastal regions of southeastern Kenya and north eastern Tanzania, and then southwards to Mozambique, south eastern Zimbabwe and Natal. More substantial in numbers in Zimbabwe were the makers of the Ziwa and Gokomere ceramic wares, of the fourth century A.D. Their early Iron Age ceramic tradition belonged to the highlands facies of the eastern stream, which moved inland to Malawi and Zimbabwe. Imports of beads have been found at Gokomere and Ziwa sites, possibly in return for gold exported to the coast.\n\nA later phase of the Gokomere culture was the Zhizo in southern Zimbabwe. Zhizo communities settled in the Shashe-Limpopo area in the tenth century. Their capital there was Schroda (just across the Limpopo River from Zimbabwe). Many fragments of ceramic figurines have been recovered from there, figures of animals and birds, and also fertility dolls. The inhabitants produced ivory bracelets and other ivory goods. Imported beads found there and at other Zhizo sites, are evidence of trade, probably of ivory and skins, with traders on the Indian Ocean coast.\n\nPottery belonging to a western stream of Bantu expansion (sometimes called Kalundu) has been found at sites in northeastern Zimbabwe, dated from the seventh century. (The western stream originated in the same area as the eastern stream: both belong to the same style system, called by Phillipson the Chifumbadze system, which has general acceptance by archaeologists.) The terms eastern and western streams represent the expansion of the Bantu speaking peoples in terms of their culture. Another question is the branches of the Bantu languages which they spoke. It seems that the makers of the Ziwa/Gokomere wares were not the ancestral speakers of the Shona languages of today's Zimbabwe, who did not arrive in there until around the tenth century, from south of the Limpopo river, and whose ceramic culture belonged to the western stream. The linguist and historian Ehret believes that in view of the similarity of the Ziwa/Gokomere pottery to the Nkope of the ancestral Nyasa language speakers, the Ziwa/Gokomere people spoke a language closely related to the Nyasa group. Their language, whatever it was, was superseded by the ancestral Shona languages, although Ehret says that a set of Nyasa words occur in central Shona dialects today.\n\nThe evidence that the ancestral Shona speakers came from South Africa is that the ceramic styles associated with Shona speakers in Zimbabwe from the thirteenth to the seventeenth centuries can be traced back to western stream (Kalunndu) pottery styles in South Africa. The Ziwa /Gokomere and Zhizo traditions were superseded by Leopards Kopje and Gumanye wares of the Kalundu tradition from the tenth century.\n\nAlthough the western stream Kalundu tradition was ancestral to Shona ceramic wares, the closest relationships of the ancestral Shona language according to many linguists were with a southern division of eastern Bantu – such languages as the southeastern languages (Nguni, Sotho-Tswana, Tsonga), Nyasa and Makwa. While it may well be the case that the people of the western stream spoke a language belonging to a wider Eastern Bantu division, it is a puzzle which remains to be resolved that they spoke a language most closely related to the languages just mentioned, all of which are today spoken in southeastern Africa.\n\nAfter the Shona speaking people moved into the present day Zimbabwe many different dialects developed over time in the different parts of the country. Among these was Kalanga.\nIt is believed that Kalanga speaking societies first emerged in the middle Limpopo valley in the 9th century before moving on to the Zimbabwean highlands. The Zimbabwean plateau eventually became the centre of subsequent Kalanga states. The Kingdom of Mapungubwe was the first in a series of sophisticated trade states developed in Zimbabwe by the time of the first European explorers from Portugal. They traded in gold, ivory and copper for cloth and glass. From about 1250 until 1450, Mapungubwe was eclipsed by the Kingdom of Zimbabwe. This Kalanga state further refined and expanded upon Mapungubwe's stone architecture, which survives to this day at the ruins of the kingdom's capital of Great Zimbabwe. From circa 1450–1760, Zimbabwe gave way to the Kingdom of Mutapa. This Kalanga state ruled much of the area that is known as Zimbabwe today, and parts of central Mozambique. It is known by many names including the Mutapa Empire, also known as Mwenemutapa was known for its gold trade routes with Arabs and the Portuguese. However, Portuguese settlers destroyed the trade and began a series of wars which left the empire in near collapse in the early 17th century. As a direct response to Portuguese aggression in the interior, a new Kalanga state emerged called the Rozwi Empire. Relying on centuries of military, political and religious development, the Rozwi (which means \"destroyers\") removed the Portuguese from the Zimbabwe plateau by force of arms. The Rozwi continued the stone building traditions of the Zimbabwe and Mapungubwe kingdoms while adding guns to its arsenal and developing a professional army to protect its trade routes and conquests. Around 1821, the Zulu general Mzilikazi of the Khumalo clan successfully rebelled from King Shaka and created his own clan, the Ndebele. The Ndebele fought their way northwards into the Transvaal, leaving a trail of destruction in their wake and beginning an era of widespread devastation known as the Mfecane. When Dutch trekboers converged on the Transvaal in 1836, they drove the tribe even further northward. By 1838, the Rozwi Empire, along with the other petty Shona states were conquered by the Ndebele and reduced to vassaldom.\n\nAfter losing their remaining South African lands in 1840, Mzilikazi and his tribe permanently settled the southwest of present-day Zimbabwe in what became known as Matabeleland, establishing Bulawayo as their capital. Mzilikazi then organised his society into a military system with regimental kraals, similar to those of Shaka, which was stable enough to repel further Boer incursions. Mzilikazi died in 1868 and, following a violent power struggle, was succeeded by his son, Lobengula.\n\nIn the 1880s, the British arrived with Cecil Rhodes' British South Africa Company. In 1898, the name Southern Rhodesia was adopted. In 1888, British colonialist Cecil Rhodes obtained a concession for mining rights from King Lobengula of the Ndebele peoples. Cecil Rhodes presented this concession to persuade the government of the United Kingdom to grant a royal charter to his British South Africa Company (BSAC) over Matabeleland, and its subject states such as Mashonaland. Rhodes sought permission to negotiate similar concessions covering all territory between the Limpopo River and Lake Tanganyika, then known as 'Zambesia'. In accordance with the terms of aforementioned concessions and treaties, Cecil Rhodes promoted the colonisation of the region's land, with British control over labour as well as precious metals and other mineral resources. In 1895 the BSAC adopted the name 'Rhodesia' for the territory of Zambesia, in honour of Cecil Rhodes. In 1898, 'Southern Rhodesia' became the official denotation for the region south of the Zambezi, which later became Zimbabwe. The region to the north was administered separately by the BSAC and later named Northern Rhodesia (now Zambia).\n\nThe Shona staged unsuccessful revolts (known as Chimurenga) against encroachment upon their lands, by clients of BSAC and Cecil Rhodes in 1896 and 1897. Following the failed insurrections of 1896–97 the Ndebele and Shona groups became subject to Rhodes's administration thus precipitating European settlement en masse which led to land distribution disproportionately favouring Europeans, displacing the Shona, Ndebele, and other indigenous peoples.\nSouthern Rhodesia became a self-governing British colony in October 1923, subsequent to a referendum held the previous year. Many Rhodesians served on behalf of the United Kingdom during World War II, mainly in the East African Campaign against Axis forces in Italian East Africa.\n\nIn 1953, in the face of African opposition, Britain consolidated the two colonies of Rhodesia with Nyasaland (now Malawi) in the ill-fated Federation of Rhodesia and Nyasaland which was dominated by Southern Rhodesia. Growing African nationalism and general dissent, particularly in Nyasaland, persuaded the UK to dissolve the Union in 1963, forming three colonies. As colonial rule was ending throughout the continent and as African-majority governments assumed control in neighbouring Northern Rhodesia and in Nyasaland, the white-minority Rhodesian government led by Ian Smith made a Unilateral Declaration of Independence (UDI) from the United Kingdom on 11 November 1965. The United Kingdom deemed this an act of rebellion, but did not re-establish control by force. The white minority government declared itself a republic in 1970. A civil war ensued, with Joshua Nkomo's ZAPU and Robert Mugabe's ZANU using assistance from the governments of Zambia and Mozambique. Although Smith's declaration was not recognised by the United Kingdom nor any other foreign power, Southern Rhodesia dropped the designation \"Southern\", and claimed nation status as the Republic of Rhodesia in 1970 although this was not recognised internationally.\n\nThe country gained official independence as Zimbabwe on 18 April 1980. The government held independence celebrations in Rufaro stadium in Salisbury, the capital. Lord Christopher Soames, the last Governor of Southern Rhodesia, watched as Charles, Prince of Wales, gave a farewell salute and the Rhodesian Signal Corps played \"God Save the Queen\". Many foreign dignitaries also attended, including Prime Minister Indira Gandhi of India, President Shehu Shagari of Nigeria, President Kenneth Kaunda of Zambia, President Seretse Khama of Botswana, and Prime Minister Malcolm Fraser of Australia, representing the Commonwealth of Nations. Bob Marley sang 'Zimbabwe', a song he wrote, at the government's invitation in a concert at the country's independence festivities.\n\nPresident Shagari pledged $15 million at the celebration to train Zimbabweans in Zimbabwe and expatriates in Nigeria. Mugabe's government used part of the money to buy newspaper companies owned by South Africans, increasing the government's control over the media. The rest went to training students in Nigerian universities, government workers in the Administrative Staff College of Nigeria in Badagry, and soldiers in the Nigerian Defence Academy in Kaduna. Later that year Mugabe commissioned a report by the BBC on press freedom in Zimbabwe. The BBC issued its report on 26 June, recommending the privatisation of the Zimbabwe Broadcasting Corporation and its independence from political interests. \"See also:\" Foreign relations of Zimbabwe\n\nMugabe's government changed the capital's name from Salisbury to Harare on 18 April 1982 in celebration of the second anniversary of independence. The government renamed the main street in the capital, Jameson Avenue, in honour of Samora Machel, President of Mozambique.\n\nThe new Constitution provided for an executive President as Head of State with a Prime Minister as Head of Government. Reverend Canaan Banana served as the first President. In government amended the Constitution in 1987 to provide for an Executive President and abolished the office of Prime Minister. The constitutional changes came into effect on 1 January 1988 with Robert Mugabe as President. The bicameral Parliament of Zimbabwe had a directly elected House of Assembly and an indirectly elected Senate, partly made up of tribal chiefs. The Constitution established two separate voters rolls, one for the black majority, who had 80% of the seats in Parliament, and the other for whites and other ethnic minorities, such as Coloureds, people of mixed race, and Asians, who held 20%. The government amended the Constitution in 1986, eliminating the voter rolls and replacing the white seats with seats filled by nominated members. Many white MPs joined ZANU which then reappointed them. In 1990 the government abolished the Senate and increased the House of Assembly's membership to include members nominated by the President.\n\nPrime Minister Mugabe kept Peter Walls, the head of the army, in his government and put him in charge of integrating the Zimbabwe People's Revolutionary Army (ZIPRA), Zimbabwe African National Liberation Army (ZANLA), and the Rhodesian Army. While Western media outlets praised Mugabe's efforts at reconciliation with the white minority, tension soon developed. On 17 March 1980, after several unsuccessful assassination attempts Mugabe asked Walls, \"Why are your men trying to kill me?\" Walls replied, \"If they were my men you would be dead.\" BBC news interviewed Walls on 11 August 1980. He told the BBC that he had asked British Prime Minister Margaret Thatcher to annul the 1980 election prior to the official announcement of the result on the grounds that Mugabe used intimidation to win the election. Walls said Thatcher had not replied to his request. On 12 August British government officials denied that they had not responded, saying Antony Duff, Deputy Governor of Salisbury, told Walls on 3 March that Thatcher would not annul the election.\n\nMinister of Information Nathan Shamuyarira said the government would not be \"held ransom by racial misfits\" and told \"all those Europeans who do not accept the new order to pack their bags.\" He also said the government continued to consider taking \"legal or administrative action\" against Walls. Mugabe, returning from a visit with United States President Jimmy Carter in New York City, said, \"One thing is quite clear—we are not going to have disloyal characters in our society.\" Walls returned to Zimbabwe after the interview, telling Peter Hawthorne of \"Time\" magazine, \"To stay away at this time would have appeared like an admission of guilt.\" Mugabe drafted legislation that would exile Walls from Zimbabwe for life and Walls moved to South Africa.\n\nEthnic divisions soon came back to the forefront of national politics. Tension between ZAPU and ZANU erupted with guerrilla activity starting again in Matabeleland in south-western Zimbabwe. Nkomo (ZAPU) left for exile in Britain and did not return until Mugabe guaranteed his safety. In 1982 government security officials discovered large caches of arms and ammunition on properties owned by ZAPU, accusing Nkomo and his followers of plotting to overthrow the government. Mugabe fired Nkomo and his closest aides from the cabinet. Seven MPs, members of the Rhodesian Front, left Smith's party to sit as \"independents\" on 4 March 1982, signifying their dissatisfaction with his policies. As a result of what they saw as persecution of Nkomo and his party, PF-ZAPU supporters, army deserters began a campaign of dissidence against the government. Centring primarily in Matabeleland, home of the Ndebeles who were at the time PF-ZAPU's main followers, this dissidence continued through 1987. It involved attacks on government personnel and installations, armed banditry aimed at disrupting security and economic life in the rural areas, and harassment of ZANU-PF members.\n\nBecause of the unsettled security situation immediately after independence and democratic sentiments, the government kept in force a \"state of emergency\". This gave the government widespread powers under the \"Law and Order Maintenance Act,\" including the right to detain persons without charge which it used quite widely. In 1983 to 1984 the government declared a curfew in areas of Matabeleland and sent in the army in an attempt to suppress members of the Ndebele tribe. The pacification campaign, known as the Gukuruhundi, or strong wind, resulted in at least 20,000 civilian deaths perpetrated by an elite, North Korean-trained brigade, known in Zimbabwe as the Gukurahundi.\n\nZANU-PF increased its majority in the 1985 elections, winning 67 of the 100 seats. The majority gave Mugabe the opportunity to start making changes to the constitution, including those with regard to land restoration. Fighting did not cease until Mugabe and Nkomo reached an agreement in December 1987 whereby ZAPU became part of ZANU-PF and the government changed the constitution to make Mugabe the country's first executive president and Nkomo one of two vice-presidents.\n\nElections in March 1990 resulted in another overwhelming victory for Mugabe and his party, which won 117 of the 120 election seats. Election observers estimated voter turnout at only 54% and found the campaign neither free nor fair, though balloting met international standards. Unsatisfied with a \"de facto\" one-party state, Mugabe called on the ZANU-PF Central Committee to support the creation of a \"de jure\" one-party state in September 1990 and lost. The government began further amending the constitution. The judiciary and human rights advocates fiercely criticised the first amendments enacted in April 1991 because they restored corporal and capital punishment and denied recourse to the courts in cases of compulsory purchase of land by the government. The general health of the civilian population also began to significantly flounder and by 1997 25% of the population of Zimbabwe had been infected by HIV, the AIDS virus.\n\nDuring the 1990s students, trade unionists, and workers often demonstrated to express their discontent with the government. Students protested in 1990 against proposals for an increase in government control of universities and again in 1991 and 1992 when they clashed with police. Trade unionists and workers also criticised the government during this time. In 1992 police prevented trade unionists from holding anti-government demonstrations. In 1994 widespread industrial unrest weakened the economy. In 1996 civil servants, nurses, and junior doctors went on strike over salary issues.\n\nOn 9 December 1997 a national strike paralysed the country. Mugabe was panicked by demonstrations by Zanla ex-combatants, war veterans, who had been the heart of incursions 20 years earlier in the Bush War. He agreed to pay them large gratuities and pensions, which proved to be a wholly unproductive and unbudgeted financial commitment. The discontent with the government spawned draconian government crackdowns which in turn started to destroy both the fabric of the state and of society. This in turn brought with it further discontent within the population. Thus a vicious downward spiral commenced.\n\nAlthough many whites had left Zimbabwe after independence, mainly for neighbouring South Africa, those who remained continued to wield disproportionate control of some sectors of the economy, especially agriculture. In the late-1990s whites accounted for less than 1% of the population but owned 70% of arable land. Mugabe raised this issue of land ownership by white farmers. In a calculated move, he began forcible land redistribution, which brought the government into headlong conflict with the International Monetary Fund. Amid a severe drought in the region, the police and military were instructed not to stop the invasion of white-owned farms by the so-called 'war veterans' and youth militia. This has led to a mass migration of White Zimbabweans out of Zimbabwe. At present almost no arable land is in the possession of white farmers.\n\nThe economy was run along corporatist lines with strict governmental controls on all aspects of the economy. Controls were placed on wages, prices and massive increases in government spending resulting in significant budget deficits. This experiment met with very mixed results and Zimbabwe fell further behind the first world and unemployment. Some market reforms in the 1990s were attempted. A 40 per cent devaluation of the Zimbabwean dollar was allowed to occur and price and wage controls were removed. These policies also failed at that time. Growth, employment, wages, and social service spending contracted sharply, inflation did not improve, the deficit remained well above target, and many industrial firms, notably in textiles and footwear, closed in response to increased competition and high real interest rates. The incidence of poverty in the country increased during this time.\n\nHowever, Zimbabwe began experiencing a period of considerable political and economic upheaval in 1999. Opposition to President Mugabe and the ZANU-PF government grew considerably after the mid-1990s in part due to worsening economic and human rights conditions brought about by crippling economic sanctions imposed by western countries led by Britain in response to land seizures from the White minority farmers. The Movement for Democratic Change (MDC) was established in September 1999 as an opposition party founded by trade unionist Morgan Tsvangirai.\n\nThe MDC's first opportunity to test opposition to the Mugabe government came in February 2000, when a referendum was held on a draft constitution proposed by the government. Among its elements, the new constitution would have permitted President Mugabe to seek two additional terms in office, granted government officials immunity from prosecution, and authorised government seizure of white-owned land. The referendum was handily defeated. Shortly thereafter, the government, through a loosely organised group of war veterans,some of the so-called war veterans judging from their age were not war veterans as they were too young to have fought in the chimurenga, sanctioned an aggressive land redistribution program often characterised by forced expulsion of white farmers and violence against both farmers and farm employees.\n\nParliamentary elections held in June 2000 were marred by localised violence, and claims of electoral irregularities and government intimidation of opposition supporters. Nonetheless, the MDC succeeded in capturing 57 of 120 seats in the National Assembly.\n\nPresidential elections were held in March 2002. In the months leading up to the poll, ZANU-PF, with the support of the army, security services, and especially the so-called 'war veterans', – very few of whom actually fought in the Second Chimurenga against the Smith regime in the 1970s – set about wholesale intimidation and suppression of the MDC-led opposition. Despite strong international criticism, these measures, together with organised subversion of the electoral process, ensured a Mugabe victory . The government's behaviour drew strong criticism from the EU and the USA, which imposed limited sanctions against the leading members of the Mugabe regime. Since the 2002 election, Zimbabwe has suffered further economic difficulty and growing political chaos.\n\nDivisions within the opposition MDC had begun to fester early in the decade, after Morgan Tsvangirai (the president of the MDC) was lured into a government sting operation that videotaped him talking of Mr. Mugabe's removal from power. He was subsequently arrested and put on trial on treason charges. This crippled his control of party affairs and raised questions about his competence. It also catalysed a major split within the party. In 2004 he was acquitted, but not until after suffering serious abuse and mistreatment in prison. The opposing faction was led by Welshman Ncube who was the general secretary of the party. In mid-2004, vigilantes loyal to Mr. Tsvangirai began attacking members who were mostly loyal to Ncube, climaxing in a September raid on the party's Harare headquarters in which the security director was nearly thrown to his death.\n\nAn internal party inquiry later established that aides to Tsvangirai had tolerated, if not endorsed, the violence. Divisive as the violence was, it was a debate over the rule of law that set off the party's final break-up in November 2005. These division severely weakened the opposition. In addition the government employed its own operatives to both spy on each side and to undermine each side via acts of espionage. Zimbabwean parliamentary election, 2005 were held in March 2005 in which ZANU-PF won a two-thirds majority, were again criticised by international observers as being flawed. Mugabe's political operatives were thus able to weaken the opposition internally and the security apparatus of the state was able to destabilise it externally by using violence in anti-Mugabe strongholds to prevent citizens from voting. Some voters were 'turned away' from polling station despite having proper identification, further guaranteeing that the government could control the results. Additionally Mugabe had started to appoint judges sympathetic to the government, making any judicial appeal futile. Mugabe was also able to appoint 30 of the members of parliament.\n\nAs Senate elections approached further opposition splits occurred. Ncube's supporters argued that the M.D.C. should field a slate of candidates; Tsvangirai's argued for a boycott. When party leaders voted on the issue, Ncube's side narrowly won, but Mr. Tsvangirai declared that as president of the party he was not bound by the majority's decision. Again the opposition was weakened. As a result, the elections for a new Senate in November 2005 were largely boycotted by the opposition. Mugabe's party won 24 of the 31 constituencies where elections were held amid low voter turnout. Again, evidence surfaced of voter intimidation and fraud. \n\nIn May 2005 the government began Operation Murambatsvina. It was officially billed to rid urban areas of illegal structures, illegal business enterprises, and criminal activities. In practice its purpose was to punish political opponents. The UN estimates 700,000 people have been left without jobs or homes as a result. Families and traders, especially at the beginning of the operation, were often given no notice before police destroyed their homes and businesses. Others were able to salvage some possessions and building materials but often had nowhere to go, despite the government's statement that people should be returning to their rural homes. Thousands of families were left unprotected in the open in the middle of Zimbabwe's winter., . The government interfered with non-governmental organisation (NGO) efforts to provide emergency assistance to the displaced in many instances. Some families were removed to transit camps, where they had no shelter or cooking facilities and minimal food, supplies, and sanitary facilities. The operation continued into July 2005, when the government began a program to provide housing for the newly displaced.\n\nHuman Rights Watch said the evictions had disrupted treatment for people with HIV/AIDS in a country where 3,000 die from the disease each week and about 1.3 million children have been orphaned. The operation was \"the latest manifestation of a massive human rights problem that has been going on for years\", said Amnesty International. As of September 2006, housing construction fell far short of demand, and there were reports that beneficiaries were mostly civil servants and ruling party loyalists, not those displaced. The government campaign of forced evictions continued in 2006, albeit on a lesser scale.\n\nIn September 2005 Mugabe signed constitutional amendments that reinstituted a national senate (abolished in 1987) and that nationalised all land. This converted all ownership rights into leases. The amendments also ended the right of landowners to challenge government expropriation of land in the courts and marked the end of any hope of returning any land that had been hitherto grabbed by armed land invasions. Elections for the senate in November resulted in a victory for the government. The MDC split over whether to field candidates and partially boycotted the vote. In addition to low turnout there was widespread government intimidation. The split in the MDC hardened into factions, each of which claimed control of the party. The early months of 2006 were marked by food shortages and mass hunger. The sheer extremity of the siltation was revealed by the fact that in the courts, state witnesses said they were too weak from hunger to testify.\n\nIn August 2006 runaway inflation forced the government to replace its existing currency with a revalued one. In December 2006, ZANU-PF proposed the \"harmonisation\" of the parliamentary and presidential election schedules in 2010; the move was seen by the opposition as an excuse to extend Mugabe's term as president until 2010.\n\nMorgan Tsvangirai was badly beaten on 12 March 2007 after being arrested and held at Machipisa Police Station in the Highfield suburb of Harare. The event garnered an international outcry and was considered particularly brutal and extreme, even considering the reputation of Mugabe's government. \"We are very concerned by reports of continuing brutal attacks on opposition activists in Zimbabwe and call on the government to stop all acts of violence and intimidation against opposition activists,\" said Kolawole Olaniyan, Director of Amnesty International's Africa Programme.\n\nThe economy has shrunk by 50% from 2000 to 2007. In September 2007 the inflation rate was put at almost 8,000%, the world's highest. There are frequent power and water outages. Harare's drinking water became unreliable in 2006 and as a consequence dysentery and cholera swept the city in December 2006 and January 2007. Unemployment in formal jobs is running at a record 80%. There was widespread hunger, manipulated by the government so that opposition strongholds suffer the most. Availability of bread was severely constrained after a poor wheat harvest and the closure of all bakeries.\n\nThe country, which used to be one of Africa's richest, became one of its poorest. Many observers now view the country as a 'failed state'. The settlement of the Second Congo War brought back Zimbabwe's substantial military commitment, although some troops remain to secure the mining assets under their control. The government lacks the resources or machinery to deal with the ravages of the HIV/AIDS pandemic, which affects 25% of the population. With all this and the forced and violent removal of white farmers in a brutal land redistribution program, Mugabe has earned himself widespread scorn from the international arena.\n\nThe regime has managed to cling to power by creating wealthy enclaves for government ministers, and senior party members. For example, Borrowdale Brook, a suburb of Harare is an oasis of wealth and privilege. It features mansions, manicured lawns, full shops with fully stocked shelves containing an abundance of fruit and vegetables, big cars and a golf club give is the home to President Mugabe's out-of-town retreat.\n\nZimbabwe's bakeries shut down in October 2007 and supermarkets warned that they would have no bread for the foreseeable future due to collapse in wheat production after the seizure of white-owned farms. The ministry of agriculture has also blamed power shortages for the wheat shortfall, saying that electricity cuts have affected irrigation and halved crop yields per acre. The power shortages are because Zimbabwe relies on Mozambique for some of its electricity and that due to an unpaid bill of $35 million Mozambique had reduced the amount of electrical power it supplies. On 4 December 2007, The United States imposed travel sanctions against 38 people with ties to President Mugabe because they \"played a central role in the regime's escalated human rights abuses.\"\n\nOn 8 December 2007, Mugabe attended a meeting of EU and African leaders in Lisbon, prompting UK Prime Minister Gordon Brown to decline to attend. While German chancellor Angela Merkel criticised Mugabe with her public comments, the leaders of other African countries offered him statements of support.\n\nThe educational system in Zimbabwe which was once regarded as among the best in Africa, went into crisis in 2007 because of the country's economic meltdown. One foreign reporter witnessed hundreds of children at Hatcliffe Extension Primary School in Epworth, 12 miles west of Harare, writing in the dust on the floor because they had no exercise books or pencils. The high school exam system unravelled in 2007. Examiners refused to mark examination papers when they were offered just Z$79 a paper, enough to buy three small candies. Corruption has crept into the system and may explain why in January 2007 thousands of pupils received no marks for subjects they had entered, while others were deemed \"excellent\" in subjects they had not sat. However as of late the education system has recovered and is still considered the best in Southern Africa.\n\nZimbabwe held a presidential election along with a 2008 parliamentary election of 29 March. The three major candidates were incumbent President Robert Mugabe of the Zimbabwe African National Union – Patriotic Front (ZANU-PF), Morgan Tsvangirai of the Movement for Democratic Change – Tsvangirai (MDC-T), and Simba Makoni, an independent. As no candidate received an outright majority in the first round, a second round was held on 27 June 2008 between Tsvangirai (with 47.9% of the first round vote) and Mugabe (43.2%). Tsvangirai withdrew from the second round a week before it was scheduled to take place, citing violence against his party's supporters. The second round went ahead, despite widespread criticism, and led to victory for Mugabe.\n\nBecause of Zimbabwe's dire economic situation the election was expected to provide President Mugabe with his toughest electoral challenge to date. Mugabe's opponents were critical of the handling of the electoral process, and the government was accused of planning to rig the election; Human Rights Watch said that the election was likely to be \"deeply flawed\". After the first round, but before the counting was completed, Jose Marcos Barrica, the head of the Southern African Development Community observer mission, described the election as \"a peaceful and credible expression of the will of the people of Zimbabwe.\"\n\nNo official results were announced for more than a month after the first round. The failure to release results was strongly criticised by the MDC, which unsuccessfully sought an order from the High Court to force their release. An independent projection placed Tsvangirai in the lead, but without the majority needed to avoid a second round. The MDC declared that Tsvangirai won a narrow majority in the first round and initially refused to participate in any second round. ZANU-PF has said that Mugabe will participate in a second round; the party alleged that some electoral officials, in connection with the MDC, fraudulently reduced Mugabe's score, and as a result a recount was conducted.\n\nAfter the recount and the verification of the results, the Zimbabwe Electoral Commission (ZEC) announced on 2 May that Tsvangirai won 47.9% and Mugabe won 43.2%, thereby necessitating a run-off, which was to be held on 27 June 2008. Despite Tsvangirai's continuing claims to have won a first round majority, he refused to participate in the second round. The period following the first round was marked by serious political violence caused by ZANU-PF. ZANU-PF blamed the MDC supporters for perpetrating this violence; Western governments and prominent Western organisations have blamed ZANU-PF for the violence which seems very likely to be true. On 22 June 2008, Tsvangirai announced that he was withdrawing from the run-off, describing it as a \"violent sham\" and saying that his supporters risked being killed if they voted for him. The second round nevertheless went ahead as planned with Mugabe as the only actively participating candidate, although Tsvangirai's name remained on the ballot. Mugabe won the second round by an overwhelming margin and was sworn in for another term as President on 29 June.\n\nThe international reaction to the second round have varied. The United States and states of the European Union have called for increased sanctions. On 11 July, the United Nations Security Council voted to impose sanctions on the Zimbabwe; Russia and China vetoed. The African Union has called for a \"government of national unity.\"\n\nPreliminary talks to set up conditions for official negotiations began between leading negotiators from both parties on 10 July, and on 22 July, the three party leaders met for the first time in Harare to express their support for a negotiated settlement of disputes arising out of the presidential and parliamentary elections. Negotiations between the parties officially began on 25 July and are currently proceeding with very few details released from the negotiation teams in Pretoria, as coverage by the media is barred from the premises where the negotiations are taking place. The talks were mediated by South African President Thabo Mbeki.\n\nOn 15 September 2008, the leaders of the 14-member Southern African Development Community witnessed the signing of the power-sharing agreement, brokered by South African leader Thabo Mbeki. With symbolic handshake and warm smiles at the Rainbow Towers hotel, in Harare, Mugabe and Tsvangirai signed the deal to end the violent political crisis. As provided, Robert Mugabe will remain president, Morgan Tsvangirai will become prime minister, ZANU-PF and the MDC will share control of the police, Mugabe's Zanu (PF) will command the Army, and Arthur Mutambara becomes deputy prime minister.\n\nIn November 2008 the Air Force of Zimbabwe was sent, after some police officers began refusing orders to shoot the illegal miners at Marange diamond fields. Up to 150 of the estimated 30,000 illegal miners were shot from helicopter gunships. In 2008 some Zimbabwean lawyers and opposition politicians from Mutare claimed that Shiri was the prime mover behind the military assaults on illegal diggers in the diamond mines in the east of Zimbabwe. Estimates of the death toll by mid-December range from 83 reported by the Mutare City Council, based on a request for burial ground, to 140 estimated by the (then) opposition Movement for Democratic Change - Tsvangirai party.\n\nIn January 2009, Morgan Tsvangirai announced that he would do as the leaders across Africa had insisted and join a coalition government as prime minister with his nemesis, President Robert Mugabe . On 11 February 2009 Tsvangirai was sworn in as the Prime Minister of Zimbabwe. By 2009 inflation had peaked at 500 billion % per year under the Mugabe government and the Zimbabwe currency was worthless. The opposition shared power with the Mugabe regime between 2009 and 2013, Zimbabwe switched to using the US dollar as currency and the economy improved reaching a growth rate of 10% per year.\n\nIn 2013 the Mugabe government won an election which The Economist described as \"rigged,\" doubled the size of the civil service and embarked on \"...misrule and dazzling corruption.\" However, the United Nations, African Union and SADC endorsed the elections as free and fair.\n\nBy 2016 the economy had collapsed, nationwide protests took place throughout the country and the finance minister admitted \"Right now we literally have nothing.\" \nThere was the introduction of bond notes to literally fight the biting cash crisis and liquidity crunch. Cash became scarce on the market in the year 2017.\n\nOn Wednesday November 15, 2017 the military placed President Mugabe under house arrest and removed him from power. The military stated that the president was safe. The military placed tanks around government buildings in Harare and blocked the main road to the airport. Public opinion in the capital favored the dictators removal although they were uncertain about his replacement with another dictatorship. The Times reported that Emmerson Mnangagwa helped to orchestrate the coup. He had recently been sacked by Mr Mugabe so that the path could be smoothed for Grace Mugabe to replace her husband. A Zimbabwean army officer, Major General Sibusiso Moyo, went on television to say the military was targeting \"criminals\" around President Mugabe but not actively removing the president from power. However the head of the African Union described it as such.\n\nUgandan writer Charles Onyango-Obbo stated on Twitter \"If it looks like a coup, walks like a coup and quacks like a coup, then it's a coup\". Naunihal Singh, an assistant professor at the U.S. Naval War College and author of a book on military coups, described the situation in Zimbabwe as a coup. He tweeted that \"'The President is safe' is a classic coup catch-phrase\" of such an event.\n\nRobert Mugabe resigned 21 November 2017. Second Vice-President Phelekezela Mphoko became the Acting President. Emmerson Mnangagwa was sworn in as President on 24 November 2017.\nGeneral elections were held on 30 July 2018 to elect the president and members of both houses of parliament. Ruling party ZANU-PF won the majority of seats in parliament, incumbent President Emmerson Mnangagwa was declared the winner after receiving 50.8% of votes. The opposition accused the government of rigging the vote. In subsequent riots by MDC supporters, the army opened fire and killed three people, while three others died of their injuries the following day.\nIn January 2019 following a 130% increase in the price of fuel thousands of Zimbabweans protested and the government responded with a coordinated crackdown that resulted in hundreds of arrests and multiple deaths.\n\n\n\n"}
{"id": "14115", "url": "https://en.wikipedia.org/wiki?curid=14115", "title": "History of Russia", "text": "History of Russia\n\nThe History of Russia begins with that of the East Slavs and the Finno-Ugric peoples. The traditional beginning of Russian history is the establishment of Kievan Rus', the first united Eastern Slavic state, in 882. The state adopted Christianity from the Byzantine Empire in 988, beginning the synthesis of Byzantine and Slavic cultures that defined Orthodox Slavic culture for the next millennium. Kievan Rus' ultimately disintegrated as a state due to the Mongol invasions in 1237–1240 along with the resulting deaths of about half the population of Rus'.\n\nAfter the 13th century, Moscow became a cultural center, and by the 18th century, the Tsardom of Russia had grown to become the Russian Empire, stretching from eastern Poland to the Pacific Ocean. Peasant revolts were common, and all were fiercely suppressed. Russian serfdom was abolished in 1861, but the peasants fared poorly and often turned to revolutionary pressures. In the following decades, reform efforts such as the Stolypin reforms, the constitution of 1906, and the State Duma attempted to open and liberalize the economy and political system, but the tsars refused to relinquish autocratic rule or share their power.\n\nThe Russian Revolution in 1917 was triggered by a combination of economic breakdown, war-weariness, and discontent with the autocratic system of government. It initially brought to power a coalition of liberals and moderate socialists, but their failed policies led to seizure of power by the communist Bolsheviks on 25 October. Between 1922 and 1991, the history of Russia is essentially the history of the Soviet Union, effectively an ideologically based state which was roughly conterminous with the Russian Empire before the Treaty of Brest-Litovsk. The approach to the building of socialism, however, varied over different periods in Soviet history, from the mixed economy and diverse society and culture of the 1920s to the command economy and repressions of the Joseph Stalin era to the \"era of stagnation\" in the 1980s. From its first years, government in the Soviet Union was based on the one-party rule of the Communists, as the Bolsheviks called themselves, beginning in March 1918.\n\nBy the mid-1980s, with the weaknesses of its economic and political structures becoming acute, Mikhail Gorbachev embarked on major reforms, which led to the overthrow of the communist party and the breakup of the USSR, leaving Russia again on its own and marking the start of the history of post-Soviet Russia. The Russian Federation began in January 1992 as the legal successor to the USSR. Russia retained its nuclear arsenal but lost its superpower status. Scrapping the socialist central planning and state ownership of property of the socialist era, new leaders, led by President Vladimir Putin, took political and economic power after 2000 and engaged in an energetic foreign policy. Russia's recent annexation of the Crimean peninsula has led to severe economic sanctions imposed by the United States and the European Union.\n\nIn 2006, 1.5-million-year-old Oldowan flint tools were discovered in the Dagestan Akusha region of the north Caucasus, demonstrating the presence of early humans in Russia from a very early time.\nThe discovery of some of the earliest evidence for the presence of anatomically modern humans found anywhere in Europe was reported in 2007 from the deepest levels of the Kostenki archaeological site near the Don River in Russia, which has been dated to at least 40,000 years ago. Arctic Russia was reached by 40,000 years ago.\nThat Russia was also home to some of the last surviving Neanderthals was revealed by the discovery of the partial skeleton of a Neanderthal infant in Mezmaiskaya cave in Adygea, which was carbon dated to only 29,000 years ago. In 2008, Russian archaeologists from the Institute of Archaeology and Ethnology of Novosibirsk, working at the site of Denisova Cave in the Altai Mountains of Siberia, uncovered a 40,000-year-old small bone fragment from the fifth finger of a juvenile hominin, which DNA analysis revealed to be a previously unknown species of human, which was named the Denisova hominin.\n\nDuring the prehistoric eras the vast steppes of Southern Russia were home to tribes of nomadic pastoralists. In classical antiquity, the Pontic Steppe was known as Scythia. Remnants of these long gone steppe cultures were discovered in the course of the 20th century in such places as Ipatovo, Sintashta, Arkaim, and Pazyryk.\n\nIn the later part of the 8th century BCE, Greek merchants brought classical civilization to the trade emporiums in Tanais and Phanagoria. Gelonus was described by Herodotus as a huge (Europe's biggest) earth- and wood-fortified grad inhabited around 500 BC by Heloni and Budini. The Bosporan Kingdom was incorporated as part of the Roman province of Moesia Inferior from 63 to 68 AD, under Emperor Nero. At about the 2nd century AD Goths migrated to the Black Sea, and in the 3rd and 4th centuries AD, a semi-legendary Gothic kingdom of Oium existed in Southern Russia until it was overrun by Huns. Between the 3rd and 6th centuries AD, the Bosporan Kingdom, a Hellenistic polity which succeeded the Greek colonies, was also overwhelmed by successive waves of nomadic invasions, led by warlike tribes which would often move on to Europe, as was the case with the Huns and Turkish Avars.\n\nA Turkic people, the Khazars, ruled the lower Volga basin steppes between the Caspian and Black Seas through to the 8th century. Noted for their laws, tolerance, and cosmopolitanism, the Khazars were the main commercial link between the Baltic and the Muslim Abbasid empire centered in Baghdad. They were important allies of the Byzantine Empire, and waged a series of successful wars against the Arab Caliphates. In the 8th century, the Khazars embraced Judaism.\n\nSome of the ancestors of the modern Russians were the Slavic tribes, whose original home is thought by some scholars to have been the wooded areas of the Pripet Marshes. The Early East Slavs gradually settled Western Russia in two waves: one moving from Kiev towards present-day Suzdal and Murom and another from Polotsk towards Novgorod and Rostov.\n\nFrom the 7th century onwards, East Slavs constituted the bulk of the population in Western Russia and slowly but peacefully assimilated the native Finno-Ugric tribes, such as the Merya, the Muromians, and the Meshchera.\n\nScandinavian Norsemen, known as Vikings in Western Europe and Varangians in the East, combined piracy and trade throughout Northern Europe. In the mid-9th century, they began to venture along the waterways from the eastern Baltic to the Black and Caspian Seas. According to the earliest Russian chronicle, a Varangian named Rurik was elected ruler (\"knyaz\") of Novgorod in about 860, before his successors moved south and extended their authority to Kiev, which had been previously dominated by the Khazars. Oleg, Rurik's son Igor and Igor's son Sviatoslav subsequently subdued all local East Slavic tribes to Kievan rule, destroyed the Khazar khaganate and launched several military expeditions to Byzantium and Persia.\n\nThus, the first East Slavic state, Rus', emerged in the 9th century along the Dnieper River valley. A coordinated group of princely states with a common interest in maintaining trade along the river routes, Kievan Rus' controlled the trade route for furs, wax, and slaves between Scandinavia and the Byzantine Empire along the Volkhov and Dnieper Rivers.\n\nBy the end of the 10th century, the minority Norse military aristocracy had merged with the native Slavic population, which also absorbed Greek Christian influences in the course of the multiple campaigns to loot Tsargrad, or Constantinople. One such campaign claimed the life of the foremost Slavic druzhina leader, Svyatoslav I, who was renowned for having crushed the power of the Khazars on the Volga. At the time, the Byzantine Empire was experiencing a major military and cultural revival; despite its later decline, its culture would have a continuous influence on the development of Russia in its formative centuries.\n\nKievan Rus' is important for its introduction of a Slavic variant of the Eastern Orthodox religion, dramatically deepening a synthesis of Byzantine and Slavic cultures that defined Russian culture for the next thousand years. The region adopted Christianity in 988 by the official act of public baptism of Kiev inhabitants by Prince Vladimir I, who followed the private conversion of his grandmother. Some years later the first code of laws, Russkaya Pravda, was introduced by Yaroslav the Wise. From the onset the Kievan princes followed the Byzantine example and kept the Church dependent on them, even for its revenues, so that the Russian Church and state were always closely linked.\n\nBy the 11th century, particularly during the reign of Yaroslav the Wise, Kievan Rus' displayed an economy and achievements in architecture and literature superior to those that then existed in the western part of the continent. Compared with the languages of European Christendom, the Russian language was little influenced by the Greek and Latin of early Christian writings. This was because Church Slavonic was used directly in liturgy instead.\n\nA nomadic Turkic people, the Kipchaks (also known as the Cumans), replaced the earlier Pechenegs as the dominant force in the south steppe regions neighbouring to Rus' at the end of the 11th century and founded a nomadic state in the steppes along the Black Sea (Desht-e-Kipchak). Repelling their regular attacks, especially on Kiev, which was just one day's ride from the steppe, was a heavy burden for the southern areas of Rus'. The nomadic incursions caused a massive influx of Slavs to the safer, heavily forested regions of the north, particularly to the area known as Zalesye.\n\nKievan Rus' ultimately disintegrated as a state because of in-fighting between members of the princely family that ruled it collectively. Kiev's dominance waned, to the benefit of Vladimir-Suzdal in the north-east, Novgorod in the north, and Halych-Volhynia in the south-west. Conquest by the Mongol Golden Horde in the 13th century was the final blow. Kiev was destroyed. Halych-Volhynia would eventually be absorbed into the Polish–Lithuanian Commonwealth, while the Mongol-dominated Vladimir-Suzdal and independent Novgorod Republic, two regions on the periphery of Kiev, would establish the basis for the modern Russian nation.\n\nThe invading Mongols accelerated the fragmentation of the Rus'. In 1223, the disunited southern princes faced a Mongol raiding party at the Kalka River and were soundly defeated. In 1237–1238 the Mongols burnt down the city of Vladimir (4 February 1238) and other major cities of northeast Russia, routed the Russians at the Sit' River, and then moved west into Poland and Hungary. By then they had conquered most of the Russian principalities. Only the Novgorod Republic escaped occupation and continued to flourish in the orbit of the Hanseatic League.\n\nThe impact of the Mongol invasion on the territories of Kievan Rus' was uneven. The advanced city culture was almost completely destroyed. As older centers such as Kiev and Vladimir never recovered from the devastation of the initial attack, the new cities of Moscow, Tver and Nizhny Novgorod began to compete for hegemony in the Mongol-dominated Russia. Although a Russian army defeated the Golden Horde at Kulikovo in 1380, Mongol domination of the Russian-inhabited territories, along with demands of tribute from Russian princes, continued until about 1480.\n\nAfter the fall of the Khazars in the 10th century, the middle Volga came to be dominated by the mercantile state of Volga Bulgaria, the last vestige of Greater Bulgaria centered at Phanagoria. In the 10th century the Turkic population of Volga Bulgaria converted to Islam, which facilitated its trade with the Middle East and Central Asia. In the wake of the Mongol invasions of the 1230s, Volga Bulgaria was absorbed by the Golden Horde and its population evolved into the modern Chuvashes and Kazan Tatars.\n\nThe Mongols held Russia and Volga Bulgaria in sway from their western capital at Sarai, one of the largest cities of the medieval world. The princes of southern and eastern Russia had to pay tribute to the Mongols of the Golden Horde, commonly called Tatars; but in return they received charters authorizing them to act as deputies to the khans. In general, the princes were allowed considerable freedom to rule as they wished, while the Russian Orthodox Church even experienced a spiritual revival under the guidance of Metropolitan Alexis and Sergius of Radonezh.\n\nTo the Orthodox Church and most princes, the fanatical Northern Crusaders seemed a greater threat to the Russian way of life than the Mongols. In the mid-13th century, Alexander Nevsky, elected prince of Novgorod, acquired heroic status as the result of major victories over the Teutonic Knights and the Swedes. Alexander obtained Mongol protection and assistance in fighting invaders from the west who, hoping to profit from the Russian collapse since the Mongol invasions, tried to grab territory and convert the Russians to Roman Catholicism.\n\nThe Mongols left their impact on the Russians in such areas as military tactics and transportation. Under Mongol occupation, Russia also developed its postal road network, census, fiscal system, and military organization.\n\nDaniil Aleksandrovich, the youngest son of Alexander Nevsky, founded the principality of Moscow (known as Muscovy in English), which first cooperated with and ultimately expelled the Tatars from Russia. Well-situated in the central river system of Russia and surrounded by protective forests and marshes, Moscow was at first only a vassal of Vladimir, but soon it absorbed its parent state.\n\nA major factor in the ascendancy of Moscow was the cooperation of its rulers with the Mongol overlords, who granted them the title of Grand Prince of Moscow and made them agents for collecting the Tatar tribute from the Russian principalities. The principality's prestige was further enhanced when it became the center of the Russian Orthodox Church. Its head, the Metropolitan, fled from Kiev to Vladimir in 1299 and a few years later established the permanent headquarters of the Church in Moscow under the original title of Kiev Metropolitan.\n\nBy the middle of the 14th century, the power of the Mongols was declining, and the Grand Princes felt able to openly oppose the Mongol yoke. In 1380, at Kulikovo on the Don River, the Mongols were defeated, and although this hard-fought victory did not end Tatar rule of Russia, it did bring great fame to the Grand Prince Dmitry Donskoy. Moscow's leadership in Russia was now firmly based and by the middle of the 14th century its territory had greatly expanded through purchase, war, and marriage.\n\nIn the 15th century, the grand princes of Moscow continued to consolidate Russian land to increase their population and wealth. The most successful practitioner of this process was Ivan III, who laid the foundations for a Russian national state. Ivan competed with his powerful northwestern rival, the Grand Duchy of Lithuania, for control over some of the semi-independent Upper Principalities in the upper Dnieper and Oka River basins.\n\nThrough the defections of some princes, border skirmishes, and a long war with the Novgorod Republic, Ivan III was able to annex Novgorod and Tver. As a result, the Grand Duchy of Moscow tripled in size under his rule. During his conflict with Pskov, a monk named Filofei (Philotheus of Pskov) composed a letter to Ivan III, with the prophecy that the latter's kingdom would be the Third Rome. The Fall of Constantinople and the death of the last Greek Orthodox Christian emperor contributed to this new idea of Moscow as 'New Rome' and the seat of Orthodox Christianity.\n\nA contemporary of the Tudors and other \"new monarchs\" in Western Europe, Ivan proclaimed his absolute sovereignty over all Russian princes and nobles. Refusing further tribute to the Tatars, Ivan initiated a series of attacks that opened the way for the complete defeat of the declining Golden Horde, now divided into several Khanates and hordes. Ivan and his successors sought to protect the southern boundaries of their domain against attacks of the Crimean Tatars and other hordes. To achieve this aim, they sponsored the construction of the Great Abatis Belt and granted manors to nobles, who were obliged to serve in the military. The manor system provided a basis for an emerging cavalry based army.\n\nIn this way, internal consolidation accompanied outward expansion of the state. By the 16th century, the rulers of Moscow considered the entire Russian territory their collective property. Various semi-independent princes still claimed specific territories, but Ivan III forced the lesser princes to acknowledge the grand prince of Moscow and his descendants as unquestioned rulers with control over military, judicial, and foreign affairs. Gradually, the Russian ruler emerged as a powerful, autocratic ruler, a tsar. The first Russian ruler to officially crown himself \"Tsar\" was Ivan IV.\n\nIvan III tripled the territory of his state, ended the dominance of the Golden Horde over the Rus', renovated the Moscow Kremlin, and laid the foundations of the Russian state. Biographer Fennell concludes that his reign was \"militarily glorious and economically sound,\" and especially points to his territorial annexations and his centralized control over local rulers. However, Fennell, the leading British specialist on Ivan III, argues that his reign was also \"a period of cultural depression and spiritual barrenness. Freedom was stamped out within the Russian lands. By his bigoted anti-Catholicism Ivan brought down the curtain between Russia and the west. For the sake of territorial aggrandizement he deprived his country of the fruits of Western learning and civilization.\"\nThe development of the Tsar's autocratic powers reached a peak during the reign of Ivan IV (1547–1584), known as \"Ivan the Terrible\". He strengthened the position of the monarch to an unprecedented degree, as he ruthlessly subordinated the nobles to his will, exiling or executing many on the slightest provocation. Nevertheless, Ivan is often seen as a farsighted statesman who reformed Russia as he promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (Zemsky Sobor), curbed the influence of the clergy, and introduced local self-management in rural regions.\n\nAlthough his long Livonian War for control of the Baltic coast and access to the sea trade ultimately proved a costly failure, Ivan managed to annex the Khanates of Kazan, Astrakhan, and Siberia. These conquests complicated the migration of aggressive nomadic hordes from Asia to Europe via the Volga and Urals. Through these conquests, Russia acquired a significant Muslim Tatar population and emerged as a multiethnic and multiconfessional state. Also around this period, the mercantile Stroganov family established a firm foothold in the Urals and recruited Russian Cossacks to colonise Siberia.\n\nIn the later part of his reign, Ivan divided his realm in two. In the zone known as the \"oprichnina\", Ivan's followers carried out a series of bloody purges of the feudal aristocracy (whom he suspected of treachery after the betrayal of prince Kurbsky), culminating in the Massacre of Novgorod in 1570. This combined with the military losses, epidemics, and poor harvests so weakened Russia that the Crimean Tatars were able to sack central Russian regions and burn down Moscow in 1571. In 1572 Ivan abandoned the \"oprichnina\".\n\nAt the end of Ivan IV's reign the Polish–Lithuanian and Swedish armies carried out a powerful intervention in Russia, devastating its northern and northwest regions.\n\nThe death of Ivan's childless son Feodor was followed by a period of civil wars and foreign intervention known as the \"Time of Troubles\" (1606–13). Extremely cold summers (1601–1603) wrecked crops, which led to the Russian famine of 1601–1603 and increased the social disorganization. Boris Godunov's (Борис Годунов) reign ended in chaos, civil war combined with foreign intrusion, devastation of many cities and depopulation of the rural regions. The country rocked by internal chaos also attracted several waves of interventions by the Polish–Lithuanian Commonwealth.\n\nDuring the Polish–Muscovite War (1605–1618), Polish–Lithuanian forces reached Moscow and installed the impostor False Dmitriy I in 1605, then supported False Dmitry II in 1607. The decisive moment came when a combined Russian-Swedish army was routed by the Polish forces under hetman Stanisław Żółkiewski at the Battle of Klushino on . As the result of the battle, the Seven Boyars, a group of Russian nobles, deposed the tsar Vasily Shuysky on , and recognized the Polish prince Władysław IV Vasa as the Tsar of Russia on . The Poles entered Moscow on . Moscow revolted but riots there were brutally suppressed and the city was set on fire.\n\nThe crisis provoked a patriotic national uprising against the invasion, both in 1611 and 1612. Finally, a volunteer army, led by the merchant Kuzma Minin and prince Dmitry Pozharsky, expelled the foreign forces from the capital on .\n\nThe Russian statehood survived the \"Time of Troubles\" and the rule of weak or corrupt Tsars because of the strength of the government's central bureaucracy. Government functionaries continued to serve, regardless of the ruler's legitimacy or the faction controlling the throne. However, the \"Time of Troubles\" provoked by the dynastic crisis resulted in the loss of much territory to the Polish–Lithuanian Commonwealth in the Russo-Polish war, as well as to the Swedish Empire in the Ingrian War.\n\nIn February 1613, with the chaos ended and the Poles expelled from Moscow, a national assembly, composed of representatives from fifty cities and even some peasants, elected Michael Romanov, the young son of Patriarch Filaret, to the throne. The Romanov dynasty ruled Russia until 1917.\n\nThe immediate task of the new dynasty was to restore peace. Fortunately for Moscow, its major enemies, the Polish–Lithuanian Commonwealth and Sweden, were engaged in a bitter conflict with each other, which provided Russia the opportunity to make peace with Sweden in 1617 and to sign a truce with the Polish–Lithuanian Commonwealth in 1619.\n\nRecovery of lost territories began in the mid-17th century, when the Khmelnitsky Uprising (1648–57) in Ukraine against Polish rule brought about the Treaty of Pereyaslav, concluded between Russia and the Ukrainian Cossacks. According to the treaty, Russia granted protection to the Cossacks state in Left-bank Ukraine, formerly under Polish control. This triggered a prolonged Russo-Polish War (1654-1667), which ended with the Treaty of Andrusovo, where Poland accepted the loss of Left-bank Ukraine, Kiev and Smolensk.\n\nRather than risk their estates in more civil war, the boyars cooperated with the first Romanovs, enabling them to finish the work of bureaucratic centralization. Thus, the state required service from both the old and the new nobility, primarily in the military. In return, the tsars allowed the boyars to complete the process of enserfing the peasants.\n\nIn the preceding century, the state had gradually curtailed peasants' rights to move from one landlord to another. With the state now fully sanctioning serfdom, runaway peasants became state fugitives, and the power of the landlords over the peasants \"attached\" to their land had become almost complete. Together the state and the nobles placed an overwhelming burden of taxation on the peasants, whose rate was 100 times greater in the mid-17th century than it had been a century earlier. In addition, middle-class urban tradesmen and craftsmen were assessed taxes, and, like the serfs, they were forbidden to change residence. All segments of the population were subject to military levy and to special taxes.\n\nRiots amongst peasants and citizens of Moscow at this time were endemic, and included the Salt Riot (1648), Copper Riot (1662), and the Moscow Uprising (1682). By far the greatest peasant uprising in 17th-century Europe erupted in 1667. As the free settlers of South Russia, the Cossacks, reacted against the growing centralization of the state, serfs escaped from their landlords and joined the rebels. The Cossack leader Stenka Razin led his followers up the Volga River, inciting peasant uprisings and replacing local governments with Cossack rule. The tsar's army finally crushed his forces in 1670; a year later Stenka was captured and beheaded. Yet, less than half a century later, the strains of military expeditions produced another revolt in Astrakhan, ultimately subdued.\n\nMuch of Russia's expansion occurred in the 17th century, culminating in the first Russian colonisation of the Pacific in the mid-17th century, the Russo-Polish War (1654–67) that incorporated left-bank Ukraine, and the Russian conquest of Siberia. Poland was divided in the 1790–1815 era, with much of the land and population going to Russia. Most of the 19th century growth came from adding territory in Asia, south of Siberia.\n\nPeter the Great (1672–1725) brought autocracy into Russia and played a major role in bringing his country into the European state system. Russia had now become the largest country in the world, stretching from the Baltic Sea to the Pacific Ocean. The vast majority of the land was unoccupied, and travel was slow. Much of its expansion had taken place in the 17th century, culminating in the first Russian settlement of the Pacific in the mid-17th century, the reconquest of Kiev, and the pacification of the Siberian tribes. However, a population of only 14 million was stretched across this vast landscape. With a short growing season grain yields trailed behind those in the West and potato farming was not yet widespread. As a result, the great majority of the population workforce was occupied with agriculture. Russia remained isolated from the sea trade and its internal trade, communication and manufacturing were seasonally dependent.\n\nPeter's first military efforts were directed against the Ottoman Turks. His aim was to establish a Russian foothold on the Black Sea by taking the town of Azov. His attention then turned to the north. Peter still lacked a secure northern seaport except at Archangel on the White Sea, whose harbor was frozen nine months a year. Access to the Baltic was blocked by Sweden, whose territory enclosed it on three sides. Peter's ambitions for a \"window to the sea\" led him in 1699 to make a secret alliance with the Polish–Lithuanian Commonwealth and Denmark against Sweden resulting in the Great Northern War.\n\nThe war ended in 1721 when an exhausted Sweden sued for peace with Russia. Peter acquired four provinces situated south and east of the Gulf of Finland, thus securing his coveted access to the sea. There, in 1703, he had already founded the city that was to become Russia's new capital, Saint Petersburg, as a \"window opened upon Europe\" to replace Moscow, long Russia's cultural center. Russian intervention in the Commonwealth marked, with the Silent Sejm, the beginning of a 200-year domination of that region by the Russian Empire. In celebration of his conquests, Peter assumed the title of emperor, and the Russian Tsardom officially became the Russian Empire in 1721.\n\nPeter reorganized his government based on the latest Western models, molding Russia into an absolutist state. He replaced the old \"boyar\" Duma (council of nobles) with a nine-member senate, in effect a supreme council of state. The countryside was also divided into new provinces and districts. Peter told the senate that its mission was to collect tax revenues. In turn tax revenues tripled over the course of his reign.\n\nAdministrative Collegia (ministries) were established in St. Petersburg, to replace the old governmental departments. In 1722 Peter promulgated his famous Table of ranks. As part of the government reform, the Orthodox Church was partially incorporated into the country's administrative structure, in effect making it a tool of the state. Peter abolished the patriarchate and replaced it with a collective body, the Holy Synod, led by a lay government official. Peter continued and intensified his predecessors' requirement of state service for all nobles.\n\nBy this same time, the once powerful Persian Safavid Empire to the south was heavily declining. Taking advantage of the profitable situation, Peter launched the Russo-Persian War (1722-1723), known as \"The Persian Expedition of Peter the Great\" by Russian histographers, in order to be the first Russian emperor to establish Russian influence in the Caucasus and Caspian Sea region. After considerable success and the capture of many provinces and cities in the Caucasus and northern mainland Persia, the Safavids were forced to hand over the territories to Russia. However, by twelve years later, all the territories were ceded back to Persia, which was now led by the charismatic military genius Nader Shah, as part of the Treaty of Resht and Treaty of Ganja and the Russo-Persian alliance against the Ottoman Empire, the common neighbouring rivalling enemy.\n\nPeter the Great died in 1725, leaving an unsettled succession, but Russia had become a great power by the end of his reign.\n\nInnovative tsars such as Peter the Great and Catherine the Great brought in Western experts, scientists, philosophers, and engineers. Powerful Russians resented their privileged positions and alien ideas. The backlash was especially severe after the Napoleonic wars. It produced a powerful anti-western campaign that \"led to a wholesale purge of Western specialists and their Russian followers in universities, schools, and government service.\"\n\nRussia was in a continuous state of financial crisis. While revenue rose from 9 million rubles in 1724 to 40 million in 1794, expenses grew more rapidly, reaching 49 million in 1794. The budget was allocated 46 percent to the military, 20 percent to government economic activities, 12 percent to administration, and nine percent for the Imperial Court in St. Petersburg. The deficit required borrowing, primarily from Amsterdam; five percent of the budget was allocated to debt payments. Paper money was issued to pay for expensive wars, thus causing inflation. For its spending, Russia obtained a large and glorious army, a very large and complex bureaucracy, and a splendid court that rivaled Paris and London. However, the government was living far beyond its means, and 18th-century Russia remained \"a poor, backward, overwhelmingly agricultural, and illiterate country.\"\n\nPeter I was succeeded by his second wife, Catherine I (1725–1727), who was merely a figurehead for a powerful group of high officials, then by his minor grandson, Peter II (1727–1730), then by his niece, Anna (1730–1740), daughter of Tsar Ivan V.\n\nNearly forty years were to pass before a comparably ambitious ruler appeared on the Russian throne. Catherine II, \"the Great\" (r. 1762–1796), was a German princess who married the German heir to the Russian crown. Finding him incompetent, Catherine tacitly consented to his murder and in 1762 she became ruler. Catherine enthusiastically supported the ideals of The Enlightenment, thus earning the status of an enlightened despot (\"despot\" is not derogatory in this context.) She patronized the arts, science and learning. She contributed to the resurgence of the Russian nobility that began after the death of Peter the Great. Catherine promulgated the Charter to the Gentry reaffirming rights and freedoms of the Russian nobility and abolishing mandatory state service. She seized control of all the church lands, drastically reduced the size of the monasteries, and put the surviving clergy on a tight budget.\n\nCatherine spent heavily to promote an expansive foreign policy. She extended Russian political control over the Polish–Lithuanian Commonwealth with actions, including the support of the Targowica Confederation. The cost of her campaigns, on top of the oppressive social system that required serfs to spend almost all of their time laboring on the land of their lords, provoked a major peasant uprising in 1773. Inspired by a Cossack named Pugachev, with the emphatic cry of \"Hang all the landlords!\", the rebels threatened to take Moscow until Catherine crushed the rebellion. Like the other enlightened despots of Europe, Catherine made certain of her own power and formed an alliance with the nobility.\n\nCatherine successfully waged war against the decaying Ottoman Empire and advanced Russia's southern boundary to the Black Sea. Then, by allying with the rulers of Austria and Prussia, she incorporated the territories of the Polish–Lithuanian Commonwealth, where after a century of Russian rule non-Catholic, mainly Orthodox population prevailed during the Partitions of Poland, pushing the Russian frontier westward into Central Europe. In accordance to the treaty Russia had signed with the Georgians to protect them against any new invasion of their Persian suzerains and further political aspirations, Catherine waged a new war against Persia in 1796 after they had again invaded Georgia and established rule over it about a year prior, and had expelled the newly established Russian garrisons in the Caucasus.\n\nBy the time of her death in 1796, Catherine's expansionist policy had made Russia into a major European power. Alexander I continued this policy, wresting Finland from the weakened kingdom of Sweden in 1809 and Bessarabia from the Ottomans in 1812.\n\nAfter Russian armies liberated allied Georgia from Persian occupation in 1802, they clashed with Persia over control and consolidation over Georgia, as well as the Iranian territories that comprise modern-day Azerbaijan and Dagestan. They also became involved in the Caucasian War against the Caucasian Imamate. In 1813, the war with Persia concluded with a Russian victory, forcing Qajar Iran to cede swaths of its territories in the Caucasus to Russia, which drastically increased its territory in the region. To the south-west, Russia attempted to expand at the expense of the Ottoman Empire, using Georgia at its base for the Caucasus and Anatolian front.\n\nIn European policy, Alexander I switched Russia back and forth four times in 1804–1812 from neutral peacemaker to anti-Napoleon to an ally of Napoleon, winding up in 1812 as Napoleon's enemy. In 1805, he joined Britain in the War of the Third Coalition against Napoleon, but after the massive defeat at the Battle of Austerlitz he switched and formed an alliance with Napoleon by the Treaty of Tilsit (1807) and joined Napoleon's Continental System. He fought a small-scale naval war against Britain, 1807–12. He and Napoleon could never agree, especially about Poland, and the alliance collapsed by 1810.\n\nFurthermore, Russia's economy had been hurt by Napoleon's Continental System, which cut off trade with Britain. As Esdaile notes, \"Implicit in the idea of a Russian Poland was, of course, a war against Napoleon.\" Schroeder says Poland was the root cause of the conflict but Russia's refusal to support the Continental System was also a factor.\n\nThe invasion of Russia was a catastrophe for Napoleon and his 450,000 invasion troops. One major battle was fought at Borodino; casualties were very high but it was indecisive and Napoleon was unable to engage and defeat the Russian armies. He attempted to force the Tsar to terms by capturing Moscow at the onset of winter, even though the French Army had already lost most of its men. The expectation proved futile. The Russians retreated, burning crops and food supplies in a scorched earth policy that multiplied Napoleon's logistic problems. Unprepared for winter warfare, 85%–90% of Napoleon's soldiers died from disease, cold, starvation or by ambush by peasant guerrilla fighters. As Napoleon's forces retreated, Russian troops pursued them into Central and Western Europe and finally captured Paris. Out of a total population of around 43 million people, Russia lost about 1.5 million in the year 1812; of these about 250,000 to 300,000 were soldiers and the rest peasants and serfs.\n\nAfter the final defeat of Napoleon in 1815, Alexander became known as the 'savior of Europe.' He presided over the redrawing of the map of Europe at the Congress of Vienna (1814–15), which made him the king of Congress Poland. He formed the Holy Alliance with Austria and Prussia, to suppress revolutionary movements in Europe that he saw as immoral threats to legitimate Christian monarchs. He helped Austria's Klemens von Metternich in suppressing all national and liberal movements.\n\nAlthough the Russian Empire would play a leading political role as late as 1848, its retention of serfdom precluded economic progress of any significant degree. As West European economic growth accelerated during the Industrial Revolution, sea trade and colonialism which had begun in the second half of the 18th century, Russia began to lag ever farther behind, undermining its ability to field strong armies.\n\nRussia's great power status obscured the inefficiency of its government, the isolation of its people, and its economic backwardness. Following the defeat of Napoleon, Alexander I was willing to discuss constitutional reforms, and though a few were introduced, no thoroughgoing changes were attempted.\n\nThe tsar was succeeded by his younger brother, Nicholas I (1825–1855), who at the onset of his reign was confronted with an uprising. The background of this revolt lay in the Napoleonic Wars, when a number of well-educated Russian officers traveled in Europe in the course of the military campaigns, where their exposure to the liberalism of Western Europe encouraged them to seek change on their return to autocratic Russia. The result was the Decembrist Revolt (December 1825), the work of a small circle of liberal nobles and army officers who wanted to install Nicholas' brother as a constitutional monarch. But the revolt was easily crushed, leading Nicholas to turn away from liberal reforms and champion the reactionary doctrine \"Orthodoxy, Autocracy, and Nationality\".\n\nIn 1826–1828 Russia fought another war against Persia. Russia lost almost all of its recently consolidated territories during the first year but gained them back and won the war on highly favourable terms. At the 1828 Treaty of Turkmenchay, Russia gained Armenia, Nakhchivan, Nagorno-Karabakh, Azerbaijan, and Iğdır. In the 1828–1829 Russo-Turkish War Russia invaded northeastern Anatolia and occupied the strategic Ottoman towns of Erzurum and Gumushane and, posing as protector and saviour of the Greek Orthodox population, received extensive support from the region's Pontic Greeks. Following a brief occupation, the Russian imperial army withdrew back into Georgia. By the 1830s, Russia had conquered all Persian territories and major Ottoman territories in the Caucasus.\n\nIn 1831 Nicholas crushed the November Uprising in Poland. The Russian autocracy gave Polish artisans and gentry reason to rebel in 1863 by assailing the national core values of language, religion, and culture. The resulting January Uprising was a massive Polish revolt, which also was crushed. France, Britain and Austria tried to intervene in the crisis but were unable to do so. The Russian patriotic press used the Polish uprising to unify the Russian nation, claiming it was Russia's God-given mission to save Poland and the world. Poland was punished by losing its distinctive political and judicial rights, with Russianization imposed on its schools and courts.\n\nTsar Nicholas I (reigned 1825–1855) lavished attention on his very large army; with a population of 60–70 million people, the army included a million men. They had outdated equipment and tactics, but the tsar, who dressed like a soldier and surrounded himself with officers, gloried in the victory over Napoleon in 1812 and took enormous pride in its smartness on parade. The cavalry horses, for example, were only trained in parade formations, and did poorly in battle. The glitter and braid masked profound weaknesses that he did not see. He put generals in charge of most of his civilian agencies regardless of their qualifications. An agnostic who won fame in cavalry charges was made supervisor of Church affairs. The Army became the vehicle of upward social mobility for noble youths from non-Russian areas, such as Poland, the Baltic, Finland and Georgia. On the other hand, many miscreants, petty criminals and undesirables were punished by local officials by enlisting them for life in the Army. The conscription system was highly unpopular with people, as was the practice of forcing peasants to house the soldiers for six months of the year. Curtiss finds that \"The pedantry of Nicholas' military system, which stressed unthinking obedience and parade ground evolutions rather than combat training, produced ineffective commanders in time of war.\" His commanders in the Crimean War were old and incompetent, and indeed so were his muskets as the colonels sold the best equipment and the best food.\nFinally the Crimean War at the end of his reign demonstrated to the world what no one had previously realized: Russia was militarily weak, technologically backward, and administratively incompetent. Despite his grand ambitions toward the south and Ottoman Empire, Russia had not built its railroad network in that direction, and communications were bad. The bureaucracy was riddled with graft, corruption and inefficiency and was unprepared for war. The Navy was weak and technologically backward; the Army, although very large, was good only for parades, suffered from colonels who pocketed their men's pay, poor morale, and was even more out of touch with the latest technology as developed by Britain and France. As Fuller notes, \"Russia had been beaten on the Crimean peninsula, and the military feared that it would inevitably be beaten again unless steps were taken to surmount its military weakness.\"\n\nAs Western Europe modernized, after 1840 the issue for Russia became one of direction. Some favored imitating Europe while others renounced the West and called for a return of the traditions of the past. The latter path was championed by Slavophiles, who heaped scorn on the \"decadent\" West. The Slavophiles were opponents of bureaucracy and preferred the collectivism of the medieval Russian \"mir\", or village community, to the individualism of the West.\n\nSince the war against Napoleon, Russia had become deeply involved in the affairs of Europe, as part of the \"Holy Alliance.\" The Holy Alliance was formed to serve as the \"policeman of Europe.\" However, to be the policeman of Europe and maintain the alliance required large armies. Prussia, Austria, Britain and France (the other members of the alliance) lacked large armies and needed Russia to supply the required numbers, which fit the philosophy of Nicholas I. When the Revolutions of 1848 swept Europe, however, Russia was quiet. The Tsar sent his army into Hungary in 1849 at the request of the Austrian Empire and broke the revolt there, while preventing its spread to Russian Poland. The Tsar cracked down on any signs of internal unrest.\n\nRussia expected that in exchange for supplying the troops to be the policeman of Europe, it should have a free hand in dealing with the decaying Ottoman Empire—the \"sick man of Europe.\" In 1853 Russia invaded the Crimea peninsula and other regions, leading to the Crimean War, and Britain and France came to the rescue of the Ottomans. As Fuller notes, \"Russia had been beaten on the Crimean peninsula, and the military feared that it would inevitably be beaten again unless steps were taken to surmount its military weakness.\"\nIn this setting Michael Bakunin would emerge as the father of anarchism. He left Russia in 1842 to Western Europe, where he became active in the socialist movement. After participating in the May Uprising in Dresden of 1849, he was handed over to Russia and sent to Siberia. He escaped in 1861, then began to organize. He argued with Karl Marx over socialism. Marx won and had Bakunin and the anarchists expelled from the First International in 1872. He died in obscurity but other anarchists took up the torch, especially Russian radicals such as Alexander Herzen and Peter Kropotkin.\n\nTsar Nicholas died with his philosophy in dispute. One year earlier, Russia had become involved in the Crimean War, a conflict fought primarily in the Crimean peninsula. Since playing a major role in the defeat of Napoleon, Russia had been regarded as militarily invincible, but, once pitted against a coalition of the great powers of Europe, the reverses it suffered on land and sea exposed the weakness of Tsar Nicholas' regime.\n\nWhen Alexander II came to the throne in 1855, desire for reform was widespread. The most pressing problem confronting the Government was serfdom. In 1859, there were 23 million serfs (out of a total population of 67.1 Million). In anticipation of civil unrest that could ultimately foment a revolution, Alexander II chose to preemptively abolish serfdom with the emancipation reform in 1861, an event which shifted the balance of power away from the landed aristocracy. Emancipation brought a supply of free labor to the cities, stimulated industry, and the middle class grew in number and influence. The freed peasants had to buy land, allotted to them, from the landowners with the state assistance. The Government issued special bonds to the landowners for the land that they had lost, and collected a special tax from the peasants, called redemption payments, at a rate of 5% of the total cost of allotted land yearly. All the land turned over to the peasants was owned collectively by the \"mir\", the village community, which divided the land among the peasants and supervised the various holdings.\n\nAlexander was the most successful Russian reformer since Peter the Great, and was responsible for numerous reforms besides abolishing serfdom. He reorganized the judicial system, setting up elected local judges, abolishing capital punishment, promoting local self-government through the zemstvo system, imposing universal military service, ending some of the privileges of the nobility, and promoting the universities. In foreign policy, he sold Alaska to the United States in 1867, fearing the remote colony would fall into British hands if there was another war. He modernized the military command system. He sought peace, and moved away from bellicose France when Napoleon III fell. He joined with Germany and Austria in the League of the Three Emperors that stabilized the European situation. The Russian Empire expanded in Siberia and in the Caucasus and made gains at the expense of China. Faced with an uprising in Poland in 1863, he stripped that land of its separate Constitution and incorporated it directly into Russia. To counter the rise of a revolutionary and anarchistic movements, he sent thousands of dissidents into exile in Siberia and was proposing additional parliamentary reforms when he was assassinated in 1881.\n\nIn the late 1870s Russia and the Ottoman Empire again clashed in the Balkans. The Russo-Turkish War was popular among the Russian people, who supported the independence of their fellow Orthodox Slavs, the Serbs and the Bulgarians. However, the war increased tension with Austria-Hungary, which also had ambitions in the region. The tsar was disappointed by the results of the Congress of Berlin in 1878, but abided by the agreement. During this period Russia expanded its empire into Central Asia, which was rich in raw materials, conquering the khanates of Kokand, Bokhara, and Khiva, as well as the Trans-Caspian region.\n\nIn the 1860s a movement known as Nihilism developed in Russia. A term originally coined by Ivan Turgenev in his 1862 novel \"Fathers and Sons\", Nihilists favoured the destruction of human institutions and laws, based on the assumption that such institutions and laws are artificial and corrupt. At its core, Russian nihilism was characterized by the belief that the world lacks comprehensible meaning, objective truth, or value. For some time many Russian liberals had been dissatisfied by what they regarded as the empty discussions of the intelligentsia. The Nihilists questioned all old values and shocked the Russian establishment. They moved beyond being purely philosophical to becoming major political forces after becoming involved in the cause of reform. Their path was facilitated by the previous actions of the Decembrists, who revolted in 1825, and the financial and political hardship caused by the Crimean War, which caused large numbers of Russian people to lose faith in political institutions.\n\nThe Nihilists first attempted to convert the aristocracy to the cause of reform. Failing there, they turned to the peasants. Their campaign, which targeted the people instead of the aristocracy or the landed gentry, became known as the Populist movement. It was based upon the belief that the common people possessed the wisdom and peaceful ability to lead the nation.\n\nWhile the Narodnik movement was gaining momentum, the government quickly moved to extirpate it. In response to the growing reaction of the government, a radical branch of the Narodniks advocated and practiced terrorism. One after another, prominent officials were shot or killed by bombs. This represented the ascendancy of anarchism in Russia as a powerful revolutionary force. Finally, after several attempts, Alexander II was assassinated by anarchists in 1881, on the very day he had approved a proposal to call a representative assembly to consider new reforms in addition to the abolition of serfdom designed to ameliorate revolutionary demands.\n\nUnlike his father, the new tsar Alexander III (1881–1894) was throughout his reign a staunch reactionary who revived the maxim of \"Orthodoxy, Autocracy, and National Character\". A committed Slavophile, Alexander III believed that Russia could be saved from chaos only by shutting itself off from the subversive influences of Western Europe. In his reign Russia concluded the union with republican France to contain the growing power of Germany, completed the conquest of Central Asia, and exacted important territorial and commercial concessions from China.\n\nThe tsar's most influential adviser was Konstantin Pobedonostsev, tutor to Alexander III and his son Nicholas, and procurator of the Holy Synod from 1880 to 1895. He taught his royal pupils to fear freedom of speech and press and to hate democracy, constitutions, and the parliamentary system. Under Pobedonostsev, revolutionaries were hunted down and a policy of Russification was carried out throughout the empire.\n\nAlexander was succeeded by his son Nicholas II (1894–1917). The Industrial Revolution, which began to exert a significant influence in Russia, was meanwhile creating forces that would finally overthrow the tsar. Politically, these opposition forces organized into three competing parties: The liberal elements among the industrial capitalists and nobility, who believed in peaceful social reform and a constitutional monarchy, founded the Constitutional Democratic party or \"Kadets\" in 1905. Followers of the Narodnik tradition established the Socialist-Revolutionary Party or \"Esers\" in 1901, advocating the distribution of land among those who actually worked it—the peasants. A third radical group founded the Russian Social Democratic Labour Party or \"RSDLP\" in 1898; this party was the primary exponent of Marxism in Russia. Gathering their support from the radical intellectuals and the urban working class, they advocated complete social, economic and political revolution.\n\nIn 1903 the RSDLP split into two wings: the radical Bolsheviks, led by Vladimir Lenin, and the relatively moderate Mensheviks, led by Yuli Martov. The Mensheviks believed that Russian socialism would grow gradually and peacefully and that the tsar's regime should be succeeded by a democratic republic in which the socialists would cooperate with the liberal bourgeois parties. The Bolsheviks advocated the formation of a small elite of professional revolutionists, subject to strong party discipline, to act as the vanguard of the proletariat in order to seize power by force.\n\nThe disastrous performance of the Russian armed forces in the Russo-Japanese War was a major blow to the Russian State and increased the potential for unrest.\n\nIn January 1905, an incident known as \"Bloody Sunday\" occurred when Father Gapon led an enormous crowd to the Winter Palace in Saint Petersburg to present a petition to the tsar. When the procession reached the palace, Cossacks opened fire on the crowd, killing hundreds. The Russian masses were so aroused over the massacre that a general strike was declared demanding a democratic republic. This marked the beginning of the Russian Revolution of 1905. Soviets (councils of workers) appeared in most cities to direct revolutionary activity.\n\nIn October 1905, Nicholas reluctantly issued the October Manifesto, which conceded the creation of a national Duma (legislature) to be called without delay. The right to vote was extended, and no law was to go into force without confirmation by the Duma. The moderate groups were satisfied; but the socialists rejected the concessions as insufficient and tried to organize new strikes. By the end of 1905, there was disunity among the reformers, and the tsar's position was strengthened for the time being.\n\nThe Archduke Franz Ferdinand of Austro-Hungary was assassinated by Bosnian Serbs on 28 June 1914. An ultimatum followed to Serbia, which was considered a Russian client-state, by Austro-Hungary on 23 July. Russia had no treaty obligation to Serbia, and in long-term perspective, Russia was militarily gaining on Germany and Austro-Hungary, and thus had an incentive to wait. Most Russian leaders wanted to avoid a war. However, in the present crisis they had the support of France, and they feared that the failure to support Serbia would lead to a loss of Russian credibility and a major political defeat to Russia's goals for a leadership role in the Balkans. Tsar Nicholas II mobilised Russian forces on 30 July 1914 to defend Serbia from Austria-Hungary. Christopher Clark states: \"The Russian general mobilisation [of 30 July] was one of the most momentous decisions of the July crisis. This was the first of the general mobilisations. It came at the moment when the German government had not yet even declared the State of Impending War\". Germany responded with her own mobilisation and declaration of War on 1 August 1914. At the opening of hostilities, the Russians took the offensive against both Germany and Austria-Hungary.\n\nThe very large but poorly equipped Russian army fought tenaciously and desperately at times despite its lack of organization and very weak logistics. Casualties were enormous. By 1915, many soldiers were sent to the front unarmed, and told to pick up whatever weapons they could from the battlefield. Nevertheless, the Russian army fought on, and tied down large numbers of Germans and Austrians. When civilians showed a surge of patriotism, the tsar and his entourage failed to exploit it for military benefit. Instead, they relied on slow-moving bureaucracies. In areas where they did advance against the Austrians, they failed to rally the ethnic and religious minorities that were hostile to Austria, such as Poles. The tsar refused to cooperate with the national legislature, the Duma, and listened less to experts than to his wife, who was in thrall to her chief advisor, the so-called holy man Grigori Rasputin. More than two million refugees fled.\n\nRepeated military failures and bureaucratic ineptitude soon turned large segments of the population against the government. The German and Ottoman fleets prevented Russia from importing supplies and exporting goods through the Baltic and Black seas.\n\nBy the middle of 1915 the impact of the war was demoralizing. Food and fuel were in short supply, casualties kept occurring, and inflation was mounting. Strikes increased among low-paid factory workers, and the peasants, who wanted land reforms, were restless. Meanwhile, elite distrust of the regime was deepened by reports that Rasputin was gaining influence; his assassination in late 1916 ended the scandal but did not restore the autocracy's lost prestige.\n\nThe Tsarist system was completely overthrown in February 1917. Rabinowitch argues:\nThe February 1917 revolution...grew out of prewar political and economic instability, technological backwardness, and fundamental social divisions, coupled with gross mismanagement of the war effort, continuing military defeats, domestic economic dislocation, and outrageous scandals surrounding the monarchy.\nIn late February (3 March 1917), a strike occurred in a factory in the capital Petrograd (the new name for Saint Petersburg). On 23 February (8 March) 1917, thousands of female textile workers walked out of their factories protesting the lack of food and calling on other workers to join them. Within days, nearly all the workers in the city were idle, and street fighting broke out. The tsar ordered the Duma to disband, ordered strikers to return to work, and ordered troops to shoot at demonstrators in the streets. His orders triggered the February Revolution, especially when soldiers openly sided with the strikers. The tsar and the aristocracy fell on 2 March, as Nicholas II abdicated.\n\nTo fill the vacuum of authority, the Duma declared a Provisional Government, headed by Prince Lvov, which was collectively known as the Russian Republic. Meanwhile, the socialists in Petrograd organized elections among workers and soldiers to form a soviet (council) of workers' and soldiers' deputies, as an organ of popular power that could pressure the \"bourgeois\" Provisional Government.\nIn July, following a series of crises that undermined their authority with the public, the head of the Provisional Government resigned and was succeeded by Alexander Kerensky, who was more progressive than his predecessor but not radical enough for the Bolsheviks or many Russians discontented with the deepening economic crisis and the continuation of the war. While Kerensky's government marked time, the socialist-led soviet in Petrograd joined with soviets that formed throughout the country to create a national movement.\n\nThe German government provided over 40 million gold marks to subsidize Bolshevik publications and activities subversive of the tsarist government, especially focusing on disgruntled soldiers and workers. In April 1917 Germany provided a special sealed train to carry Vladimir Lenin back to Russia from his exile in Switzerland. After many behind-the-scenes maneuvers, the soviets seized control of the government in November 1917 and drove Kerensky and his moderate provisional government into exile, in the events that would become known as the October Revolution.\n\nWhen the national Constituent Assembly (elected in December 1917) refused to become a rubber stamp of the Bolsheviks, it was dissolved by Lenin's troops and all vestiges of democracy were removed. With the handicap of the moderate opposition removed, Lenin was able to free his regime from the war problem by the harsh Treaty of Brest-Litovsk (1918) with Germany. Russia lost much of her western borderlands. However, when Germany was defeated the Soviet government repudiated the Treaty.\n\nThe Bolshevik grip on power was by no means secure, and a lengthy struggle broke out between the new regime and its opponents, which included the Socialist Revolutionaries, right-wing \"Whites\", and large numbers of peasants. At the same time the Allied powers sent several expeditionary armies to support the anti-Communist forces in an attempt to force Russia to rejoin the world war. The Bolsheviks fought against both these forces and national independence movements in the former Russian Empire. By 1921, they had defeated their internal enemies and brought most of the newly independent states under their control, with the exception of Finland, the Baltic States, the Moldavian Democratic Republic (which joined Romania), and Poland (with whom they had fought the Polish–Soviet War). Finland also annexed the region Pechenga of the Russian Kola peninsula; Soviet Russia and allied Soviet republics conceded the parts of its territory to Estonia (Petseri County and Estonian Ingria), Latvia (Pytalovo), and Turkey (Kars). Poland incorporated the contested territories of Western Belarus and Western Ukraine, the former parts of the Russian Empire (except Galicia) east to Curzon Line.\n\nBoth sides regularly committed brutal atrocities against civilians. During the civil war era White Terror (Russia) for example, Petlyura and Denikin's forces massacred 100,000 to 150,000 Jews in Ukraine and southern Russia. Hundreds of thousands of Jews were left homeless and tens of thousands became victims of serious illness.\n\nEstimates for the total number of people killed during the Red Terror carried out by the Bolsheviks vary widely. One source asserts that the total number of victims of repression and pacification campaigns could be 1.3 million, whereas others gives estimates of at least 10,000 in the initial period of repression and an estimate of 28,000 executions per year from December 1917 to February 1922. The most reliable estimations for the total number of killings put the number at about 100,000, whereas others suggest a figure of 200,000.\n\nThe Russian economy was devastated by the war, with factories and bridges destroyed, cattle and raw materials pillaged, mines flooded and machines damaged. The droughts of 1920 and 1921, as well as the 1921 famine, worsened the disaster still further. Disease had reached pandemic proportions, with 3,000,000 dying of typhus alone in 1920. Millions more also died of widespread starvation. By 1922 there were at least 7,000,000 street children in Russia as a result of nearly ten years of devastation from the Great War and the civil war. Another one to two million people, known as the White émigrés, fled Russia, many with the White Gen. Pyotr Nikolayevich Wrangel—some through the Far East, others west into the newly independent Baltic countries. These émigrés included a large percentage of the educated and skilled population of Russia.\n\nThe history of Russia between 1922 and 1991 is essentially the history of the Union of Soviet Socialist Republics, or Soviet Union. This ideologically based union, established in December 1922 by the leaders of the Russian Communist Party, was roughly coterminous with Russia before the Treaty of Brest-Litovsk. At that time, the new nation included four constituent republics: the Russian SFSR, the Ukrainian SSR, the Belarusian SSR, and the Transcaucasian SFSR.\n\nThe constitution, adopted in 1924, established a federal system of government based on a succession of soviets set up in villages, factories, and cities in larger regions. This pyramid of soviets in each constituent republic culminated in the All-Union Congress of Soviets. However, while it appeared that the congress exercised sovereign power, this body was actually governed by the Communist Party, which in turn was controlled by the Politburo from Moscow, the capital of the Soviet Union, just as it had been under the tsars before Peter the Great.\n\nThe period from the consolidation of the Bolshevik Revolution in 1917 until 1921 is known as the period of war communism. Land, all industry, and small businesses were nationalized, and the money economy was restricted. Strong opposition soon developed. The peasants wanted cash payments for their products and resented having to surrender their surplus grain to the government as a part of its civil war policies. Confronted with peasant opposition, Lenin began a strategic retreat from war communism known as the New Economic Policy (NEP). The peasants were freed from wholesale levies of grain and allowed to sell their surplus produce in the open market. Commerce was stimulated by permitting private retail trading. The state continued to be responsible for banking, transportation, heavy industry, and public utilities.\n\nAlthough the left opposition among the Communists criticized the rich peasants, or kulaks, who benefited from the NEP, the program proved highly beneficial and the economy revived. The NEP would later come under increasing opposition from within the party following Lenin's death in early 1924.\n\nWhile the Russian economy was being transformed, the social life of the people underwent equally drastic changes. From the beginning of the revolution, the government attempted to weaken patriarchal domination of the family. Divorce no longer required court procedure,\nand to make women completely free of the responsibilities of childbearing, abortion was made legal as early as 1920. As a side effect, the emancipation of women increased the labor market. Girls were encouraged to secure an education and pursue a career in the factory or the office. Communal nurseries were set up for the care of small children, and efforts were made to shift the center of people's social life from the home to educational and recreational groups, the soviet clubs.\n\nThe regime abandoned the tsarist policy of discriminating against national minorities in favor of a policy of incorporating the more than two hundred minority groups into Soviet life. Another feature of the regime was the extension of medical services. Campaigns were carried out against typhus, cholera, and malaria; the number of doctors was increased as rapidly as facilities and training would permit; and infant mortality rates rapidly decreased while life expectancy rapidly increased.\n\nIn accordance with Marxist theory, the government also promoted atheism and materialism. It opposed organized religion, especially to break the power of the Russian Orthodox Church, a former pillar of the old tsarist regime and a major barrier to social change. Many religious leaders were sent to internal exile camps. Members of the party were forbidden to attend religious services, and the education system was separated from the Church. Religious teaching was prohibited except in the home, and atheist instruction was stressed in the schools.\n\nThe years from 1929 to 1939 comprised a tumultuous decade in Soviet history—a period of massive industrialization and internal struggles as Joseph Stalin established near total control over Soviet society, wielding virtually unrestrained power. Following Lenin's death Stalin wrestled to gain control of the Soviet Union with rival factions in the Politburo, especially Leon Trotsky's. By 1928, with the Trotskyists either exiled or rendered powerless, Stalin was ready to put a radical programme of industrialisation into action.\n\nIn 1929 Stalin proposed the first five-year plan. Abolishing the NEP, it was the first of a number of plans aimed at swift accumulation of capital resources through the buildup of heavy industry, the collectivization of agriculture, and the restricted manufacture of consumer goods. For the first time in history a government controlled all economic activity.\n\nAs a part of the plan, the government took control of agriculture through the state and collective farms (\"kolkhozes\"). By a decree of February 1930, about one million individual peasants (\"kulaks\") were forced off their land. Many peasants strongly opposed regimentation by the state, often slaughtering their herds when faced with the loss of their land. In some sections they revolted, and countless peasants deemed \"kulaks\" by the authorities were executed. The combination of bad weather, deficiencies of the hastily established collective farms, and massive confiscation of grain precipitated a serious famine, and several million peasants died of starvation, mostly in Ukraine, Kazakhstan and parts of southwestern Russia. The deteriorating conditions in the countryside drove millions of desperate peasants to the rapidly growing cities, fueling industrialization, and vastly increasing Russia's urban population in the space of just a few years.\n\nThe plans received remarkable results in areas aside from agriculture. Russia, in many measures the poorest nation in Europe at the time of the Bolshevik Revolution, now industrialized at a phenomenal rate, far surpassing Germany's pace of industrialization in the 19th century and Japan's earlier in the 20th century.\n\nWhile the Five-Year Plans were forging ahead, Stalin was establishing his personal power. The NKVD gathered in tens of thousands of Soviet citizens to face arrest, deportation, or execution. Of the six original members of the 1920 Politburo who survived Lenin, all were purged by Stalin. Old Bolsheviks who had been loyal comrades of Lenin, high officers in the Red Army, and directors of industry were liquidated in the Great Purges. Purges in other Soviet republics also helped centralize control in the USSR.\n\nStalin's repressions led to the creation of a vast system of internal exile, of considerably greater dimensions than those set up in the past by the tsars. Draconian penalties were introduced and many citizens were prosecuted for fictitious crimes of sabotage and espionage. The labor provided by convicts working in the labor camps of the Gulag system became an important component of the industrialization effort, especially in Siberia. An estimated 18 million people passed through the Gulag system, and perhaps another 15 million had experience of some other form of forced labor.\n\nThe Soviet Union viewed the 1933 accession of fervently anti-Communist Hitler's government to power in Germany with great alarm from the onset, especially since Hitler proclaimed the Drang nach Osten as one of the major objectives in his vision of the German strategy of Lebensraum. The Soviets supported the republicans of Spain who struggled against fascist German and Italian troops in the Spanish Civil War. In 1938–1939, immediately prior to WWII, the Soviet Union successfully fought against Imperial Japan in the Soviet–Japanese border conflicts in the Russian Far East, which led to Soviet-Japanese neutrality and the tense border peace that lasted until August 1945.\n\nIn 1938 Germany annexed Austria and, together with major Western European powers, signed the Munich Agreement following which Germany, Hungary and Poland divided parts of Czechoslovakia between themselves. German plans for further eastward expansion, as well as the lack of resolve from Western powers to oppose it, became more apparent. Despite the Soviet Union strongly opposing the Munich deal and repeatedly reaffirming its readiness to militarily back commitments given earlier to Czechoslovakia, the Western Betrayal led to the end of Czechoslovakia and further increased fears in the Soviet Union of a coming German attack. This led the Soviet Union to rush the modernization of its military industry and to carry out its own diplomatic maneuvers. In 1939 the Soviet Union signed the Molotov–Ribbentrop Pact: a non-aggression pact with Nazi Germany dividing Eastern Europe into two separate spheres of influence. Following the pact, the USSR normalized relations with Nazi Germany and resumed Soviet–German trade.\n\nOn 17 September 1939, sixteen days after the start of World War II and with the victorious Germans having advanced deep into Polish territory, the Red Army invaded eastern Poland, stating as justification the \"need to protect Ukrainians and Belarusians\" there, after the \"cessation of existence\" of the Polish state. As a result, the Belarusian and Ukrainian Soviet republics' western borders were moved westward, and the new Soviet western border was drawn close to the original Curzon line. In the meantime negotiations with Finland over a Soviet-proposed land swap that would redraw the Soviet-Finnish border further away from Leningrad failed, and in December 1939 the USSR invaded Finland, beginning a campaign known as the Winter War (1939–40). The war took a heavy death toll on the Red Army but forced Finland to sign a Moscow Peace Treaty and cede the Karelian Isthmus and Ladoga Karelia. In summer 1940 the USSR issued an ultimatum to Romania forcing it to cede the territories of Bessarabia and Northern Bukovina. At the same time, the Soviet Union also occupied the three formerly independent Baltic states (Estonia, Latvia and Lithuania).\n\nThe peace with Germany was tense, as both sides were preparing for the military conflict, and abruptly ended when the Axis forces led by Germany swept across the Soviet border on 22 June 1941. By the autumn the German army had seized Ukraine, laid a siege of Leningrad, and threatened to capture the capital, Moscow, itself. Despite the fact that in December 1941 the Red Army threw off the German forces from Moscow in a successful counterattack, the Germans retained the strategic initiative for approximately another year and held a deep offensive in the south-eastern direction, reaching the Volga and the Caucasus. However, two major German defeats in Stalingrad and Kursk proved decisive and reversed the course of the entire World War as Germans never regained the strength to sustain their offensive operations and the Soviet Union recaptured the initiative for the rest of the conflict. By the end of 1943, the Red Army had broken through the German siege of Leningrad and liberated much of Ukraine, much of Western Russia and moved into Belarus. By the end of 1944, the front had moved beyond the 1939 Soviet frontiers into eastern Europe. Soviet forces drove into eastern Germany, capturing Berlin in May 1945. The war with Germany thus ended triumphantly for the Soviet Union.\n\nAs agreed at the Yalta Conference, three months after the Victory Day in Europe the USSR launched the Soviet invasion of Manchuria, defeating the Japanese troops in neighboring Manchuria, the last Soviet battle of World War II.\n\nAlthough the Soviet Union was victorious in World War II, the war resulted in around 26–27 million Soviet deaths (estimates vary) and had devastated the Soviet economy in the struggle. Some 1,710 towns and 70,000 settlements were destroyed. The occupied territories suffered from the ravages of German occupation and deportations of slave labor by Germany. Thirteen million Soviet citizens became victims of the repressive policies of Germany and its allies in occupied territories, where people died because of mass murders, famine, absence of elementary medical aid and slave labor. The Nazi Genocide of the Jews, carried out by German \"Einsatzgruppen\" along with local collaborators, resulted in almost complete annihilation of the Jewish population over the entire territory temporarily occupied by Germany and its allies. During the occupation, the Leningrad region lost around a quarter of its population, Soviet Belarus lost from a quarter to a third of its population, and 3.6 million Soviet prisoners of war (of 5.5 million) died in German camps.\n\nCollaboration among the major Allies had won the war and was supposed to serve as the basis for postwar reconstruction and security. However, the conflict between Soviet and U.S. national interests, known as the Cold War, came to dominate the international stage in the postwar period.\n\nThe Cold War emerged from a conflict between Stalin and U.S. President Harry Truman over the future of Eastern Europe during the Potsdam Conference in the summer of 1945. Russia had suffered three devastating Western onslaughts in the previous 150 years during the Napoleonic Wars, the First World War, and the Second World War, and Stalin's goal was to establish a buffer zone of states between Germany and the Soviet Union. Truman charged that Stalin had betrayed the Yalta agreement. With Eastern Europe under Red Army occupation, Stalin was also biding his time, as his own atomic bomb project was steadily and secretly progressing.\n\nIn April 1949 the United States sponsored the North Atlantic Treaty Organization (NATO), a mutual defense pact in which most Western nations pledged to treat an armed attack against one nation as an assault on all. The Soviet Union established an Eastern counterpart to NATO in 1955, dubbed the Warsaw Pact. The division of Europe into Western and Soviet blocks later took on a more global character, especially after 1949, when the U.S. nuclear monopoly ended with the testing of a Soviet bomb and the Communist takeover in China.\n\nThe foremost objectives of Soviet foreign policy were the maintenance and enhancement of national security and the maintenance of hegemony over Eastern Europe. The Soviet Union maintained its dominance over the Warsaw Pact through crushing the Hungarian Revolution of 1956, suppressing the Prague Spring in Czechoslovakia in 1968, and supporting the suppression of the Solidarity movement in Poland in the early 1980s. The Soviet Union opposed the United States in a number of proxy conflicts all over the world, including the Korean War and Vietnam War.\n\nAs the Soviet Union continued to maintain tight control over its sphere of influence in Eastern Europe, the Cold War gave way to \"Détente\" and a more complicated pattern of international relations in the 1970s in which the world was no longer clearly split into two clearly opposed blocs. Less powerful countries had more room to assert their independence, and the two superpowers were partially able to recognize their common interest in trying to check the further spread and proliferation of nuclear weapons in treaties such as SALT I, SALT II, and the Anti-Ballistic Missile Treaty.\n\nU.S.–Soviet relations deteriorated following the beginning of the nine-year Soviet–Afghan War in 1979 and the 1980 election of Ronald Reagan, a staunch anti-communist, but improved as the communist bloc started to unravel in the late 1980s. With the collapse of the Soviet Union in 1991, Russia lost the superpower status that it had won in the Second World War.\n\nIn the power struggle that erupted after Stalin's death in 1953, his closest followers lost out. Nikita Khrushchev solidified his position in a speech before the Twentieth Congress of the Communist Party in 1956 detailing Stalin's atrocities.\n\nIn 1964 Khrushchev was impeached by the Communist Party's Central Committee, charging him with a host of errors that included Soviet setbacks such as the Cuban Missile Crisis. After a period of collective leadership led by Leonid Brezhnev, Alexei Kosygin and Nikolai Podgorny, a veteran bureaucrat, Brezhnev, took Khrushchev's place as Soviet leader. Brezhnev emphasized heavy industry, instituted the Soviet economic reform of 1965, and also attempted to ease relationships with the United States. In the 1960s the USSR became a leading producer and exporter of petroleum and natural gas. Soviet science and industry peaked in the Khrushchev and Brezhnev years. The world's first nuclear power plant was established in 1954 in Obninsk, and the Baikal Amur Mainline was built.\n\nThe Soviet space program, founded by Sergey Korolev, was especially successful. On 4 October 1957 Soviet Union launched the first space satellite Sputnik. On 12 April 1961 Yuri Gagarin became the first human to travel into space in the Soviet spaceship Vostok 1. Other achievements of Russian space program include: the first photo of the far side of the Moon; exploration of Venus; the first spacewalk by Alexey Leonov; first female spaceflight by Valentina Tereshkova. More recently, the Soviet Union produced the world's first space station, Salyut which in 1986 was replaced by Mir, the first consistently inhabited long-term space station, that served from 1986 to 2001.\nWhile all modernized economies were rapidly moving to computerization after 1965, the USSR fell further and further behind. Moscow's decision to copy the IBM 360 of 1965 proved a decisive mistake for it locked scientists into an antiquated system they were unable to improve. They had enormous difficulties in manufacturing the necessary chips reliably and in quantity, in programming workable and efficient programs, in coordinating entirely separate operations, and in providing support to computer users.\n\nOne of the greatest strengths of Soviet economy was its vast supplies of oil and gas; world oil prices quadrupled in the 1973–74, and rose again in 1979–1981, making the energy sector the chief driver of the Soviet economy, and was used to cover multiple weaknesses. At one point, Soviet Premier Alexei Kosygin told the head of oil and gas production, \"things are bad with bread. Give me 3 million tons [of oil] over the plan.\" Former prime minister Yegor Gaidar, an economist looking back three decades, in 2007 wrote:\n\nTwo developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. After the rapid succession of former KGB Chief Yuri Andropov and Konstantin Chernenko, transitional figures with deep roots in Brezhnevite tradition, Mikhail Gorbachev announced perestroika in an attempt to modernize Soviet communism, and made significant changes in the party leadership. However, Gorbachev's social reforms led to unintended consequences. Because of his policy of \"glasnost\", which facilitated public access to information after decades of government repression, social problems received wider public attention, undermining the Communist Party's authority. In the revolutions of 1989 the USSR lost its allies in Eastern Europe. \"Glasnost\" allowed ethnic and nationalist disaffection to reach the surface. Many constituent republics, especially the Baltic republics, Georgian SSR and Moldavian SSR, sought greater autonomy, which Moscow was unwilling to provide. Gorbachev's attempts at economic reform were not sufficient, and the Soviet government left intact most of the fundamental elements of communist economy. Suffering from low pricing of petroleum and natural gas, ongoing war in Afghanistan, outdated industry and pervasive corruption, the Soviet planned economy proved to be ineffective, and by 1990 the Soviet government had lost control over economic conditions. Due to price control, there were shortages of almost all products, reaching their peak in the end of 1991, when people had to stand in long lines and to be lucky enough to buy even the essentials. Control over the constituent republics was also relaxed, and they began to assert their national sovereignty over Moscow.\nThe tension between Soviet Union and Russian SFSR authorities came to be personified in the bitter power struggle between Gorbachev and Boris Yeltsin. Squeezed out of Union politics by Gorbachev in 1987, Yeltsin, who represented himself as a committed democrat, presented a significant opposition to Gorbachev authority. In a remarkable reversal of fortunes, he gained election as chairman of the Russian republic's new Supreme Soviet in May 1990. The following month, he secured legislation giving Russian laws priority over Soviet laws and withholding two-thirds of the budget. In the first Russian presidential election in 1991 Yeltsin became president of the Russian SFSR.\nAt last Gorbachev attempted to restructure the Soviet Union into a less centralized state. However, on 19 August 1991, a coup against Gorbachev, conspired by senior Soviet officials, was attempted. The coup faced wide popular opposition and collapsed in three days, but disintegration of the Union became imminent. The Russian government took over most of the Soviet Union government institutions on its territory. Because of the dominant position of Russians in the Soviet Union, most gave little thought to any distinction between Russia and the Soviet Union before the late 1980s. In the Soviet Union, only Russian SFSR lacked even the paltry instruments of statehood that the other republics possessed, such as its own republic-level Communist Party branch, trade union councils, Academy of Sciences, and the like. The Communist Party of the Soviet Union was banned in Russia in 1991–1992, although no lustration has ever taken place, and many of its members became top Russian officials. However, as the Soviet government was still opposed to market reforms, the economic situation continued to deteriorate. By December 1991, the shortages had resulted in the introduction of food rationing in Moscow and Saint Petersburg for the first time since World War II. Russia received humanitarian food aid from abroad. After the Belavezha Accords, the Supreme Soviet of Russia withdrew Russia from the Soviet Union on 12 December. The Soviet Union officially ended on 25 December 1991, and the Russian Federation (formerly the Russian Soviet Federative Socialist Republic) took power on 26 December. The Russian government lifted price control on January 1992. Prices rose dramatically, but shortages disappeared.\n\nAlthough Yeltsin came to power on a wave of optimism, he never recovered his popularity after endorsing Yegor Gaidar's \"shock therapy\" of ending Soviet-era price controls, drastic cuts in state spending, and an open foreign trade regime in early 1992 (\"see\" Russian economic reform in the 1990s). The reforms immediately devastated the living standards of much of the population. In the 1990s Russia suffered an economic downturn that was, in some ways, more severe than the United States or Germany had undergone six decades earlier in the Great Depression. Hyperinflation hit the ruble, due to monetary overhang from the days of the planned economy.\n\nMeanwhile, the profusion of small parties and their aversion to coherent alliances left the legislature chaotic. During 1993, Yeltsin's rift with the parliamentary leadership led to the September–October 1993 constitutional crisis. The crisis climaxed on 3 October, when Yeltsin chose a radical solution to settle his dispute with parliament: he called up tanks to shell the Russian White House, blasting out his opponents. As Yeltsin was taking the unconstitutional step of dissolving the legislature, Russia came close to a serious civil conflict. Yeltsin was then free to impose the current Russian constitution with strong presidential powers, which was approved by referendum in December 1993. The cohesion of the Russian Federation was also threatened when the republic of Chechnya attempted to break away, leading to the First and Second Chechen Wars.\nEconomic reforms also consolidated a semi-criminal oligarchy with roots in the old Soviet system. Advised by Western governments, the World Bank, and the International Monetary Fund, Russia embarked on the largest and fastest privatization that the world had ever seen in order to reform the fully nationalized Soviet economy. By mid-decade, retail, trade, services, and small industry was in private hands. Most big enterprises were acquired by their old managers, engendering a new rich (Russian tycoons) in league with criminal mafias or Western investors. That being said, there were corporate raiders such as Andrei Volgin engaged in hostile takeovers of corrupt corporations by the mid-1990s.\n\nBy the mid-1990s Russia had a system of multiparty electoral politics. But it was harder to establish a representative government because of two structural problems—the struggle between president and parliament and the anarchic party system.\n\nMeanwhile, the central government had lost control of the localities, bureaucracy, and economic fiefdoms; tax revenues had collapsed. Still in deep depression by the mid-1990s, Russia's economy was hit further by the financial crash of 1998. After the 1998 financial crisis, Yeltsin was at the end of his political career. Just hours before the first day of 2000, Yeltsin made a surprise announcement of his resignation, leaving the government in the hands of the little-known Prime Minister Vladimir Putin, a former KGB official and head of the FSB, the KGB's post-Soviet successor agency. In 2000, the new acting president defeated his opponents in the presidential election on 26 March, and won a landslide 4 years later. In 2001, Putin discussed a possible Russia's entry in NATO with Bill Clinton without result.\n\nInternational observers were alarmed by late 2004 moves to further tighten the presidency's control over parliament, civil society, and regional officeholders. In 2008 Dmitri Medvedev, a former Gazprom chairman and Putin's head of staff, was elected new President of Russia. In 2012, Putin was once again elected as President.\nRussia has had difficulty attracting foreign direct investment and has experienced large capital outflows in the past several years. Russia's long-term problems also include a shrinking workforce, rampant corruption, and underinvestment in infrastructure.\nNevertheless, reversion to a socialist command economy seemed almost impossible.\n\nRussia ended 2006 with its eighth straight year of growth, averaging 6.7% annually since the financial crisis of 1998. Although high oil prices and a relatively cheap ruble initially drove this growth, since 2003 consumer demand and, more recently, investment have played a significant role. Russia is well ahead of most other resource-rich countries in its economic development, with a long tradition of education, science, and industry.\n\nIn 2014, following a referendum, in which separation was favored by a large majority of voters, the Russian leadership announced the accession of Crimea into the Russian Federation.\n\n\n\n\n\n\n\n"}
{"id": "14117", "url": "https://en.wikipedia.org/wiki?curid=14117", "title": "History of Christianity", "text": "History of Christianity\n"}
{"id": "14121", "url": "https://en.wikipedia.org/wiki?curid=14121", "title": "Hertz", "text": "Hertz\n\nThe hertz (symbol: Hz) is the derived unit of frequency in the International System of Units (SI) and is defined as one cycle per second. It is named for Heinrich Rudolf Hertz, the first person to provide conclusive proof of the existence of electromagnetic waves. Hertz are commonly expressed in multiples: kilohertz (10 Hz, kHz), megahertz (10 Hz, MHz), gigahertz (10 Hz, GHz), terahertz (10 Hz, THz), petahertz (10 Hz, PHz), and exahertz (10 Hz, EHz).\n\nSome of the unit's most common uses are in the description of sine waves and musical tones, particularly those used in radio- and audio-related applications. It is also used to describe the speeds at which computers and other electronics are driven.\n\nThe hertz is defined as one cycle per second. The International Committee for Weights and Measures defined the second as \"the duration of 9 192 631 770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the caesium 133 atom\" and then adds: \"It follows that the hyperfine splitting in the ground state of the caesium 133 atom is exactly 9 192 631 770 hertz, ν(hfs Cs) = 9 192 631 770 Hz.\" The dimension of the unit hertz is 1/time (1/T). Expressed in base SI units it is 1/second (1/s).\n\nIn English, \"hertz\" is also used as the plural form. As an SI unit, Hz can be prefixed; commonly used multiples are kHz (kilohertz, 10 Hz), MHz (megahertz, 10 Hz), GHz (gigahertz, 10 Hz) and THz (terahertz, 10 Hz). One hertz simply means \"one cycle per second\" (typically that which is being counted is a complete cycle); 100 Hz means \"one hundred cycles per second\", and so on. The unit may be applied to any periodic event—for example, a clock might be said to tick at 1 Hz, or a human heart might be said to beat at 1.2 Hz. The occurrence rate of aperiodic or stochastic events is expressed in reciprocal second or inverse second (1/s or s) in general or, in the specific case of radioactive decay, in becquerels. Whereas 1 Hz is 1 cycle per second, 1 Bq is 1 aperiodic radionuclide event per second.\n\nEven though angular velocity, angular frequency and the unit hertz all have the dimension 1/s, angular velocity and angular frequency are not expressed in hertz, but rather in an appropriate angular unit such as radians per second. Thus a disc rotating at 60 revolutions per minute (rpm) is said to be rotating at either 2 rad/s \"or\" 1 Hz, where the former measures the angular velocity and the latter reflects the number of \"complete\" revolutions per second. The conversion between a frequency \"f\" measured in hertz and an angular velocity \"ω\" measured in radians per second is\n\nThe hertz is named after the German physicist Heinrich Hertz (1857–1894), who made important scientific contributions to the study of electromagnetism. The name was established by the International Electrotechnical Commission (IEC) in 1930. It was adopted by the General Conference on Weights and Measures (CGPM) (\"Conférence générale des poids et mesures\") in 1960, replacing the previous name for the unit, \"cycles per second\" (cps), along with its related multiples, primarily \"kilocycles per second\" (kc/s) and \"megacycles per second\" (Mc/s), and occasionally \"kilomegacycles per second\" (kMc/s). The term \"cycles per second\" was largely replaced by \"hertz\" by the 1970s. One hobby magazine, \"Electronics Illustrated\", declared their intention to stick with the traditional kc., Mc., etc. units.\n\nSound is a traveling longitudinal wave which is an oscillation of pressure. Humans perceive frequency of sound waves as pitch. Each musical note corresponds to a particular frequency which can be measured in hertz. An infant's ear is able to perceive frequencies ranging from 20 Hz to 20,000 Hz; the average adult human can hear sounds between 20 Hz and 16,000 Hz. The range of ultrasound, infrasound and other physical vibrations such as molecular and atomic vibrations extends from a few femtohertz into the terahertz range and beyond.\n\nElectromagnetic radiation is often described by its frequency—the number of oscillations of the perpendicular electric and magnetic fields per second—expressed in hertz.\n\nRadio frequency radiation is usually measured in kilohertz (kHz), megahertz (MHz), or gigahertz (GHz). Light is electromagnetic radiation that is even higher in frequency, and has frequencies in the range of tens (infrared) to thousands (ultraviolet) of terahertz. Electromagnetic radiation with frequencies in the low terahertz range (intermediate between those of the highest normally usable radio frequencies and long-wave infrared light) is often called terahertz radiation. Even higher frequencies exist, such as that of gamma rays, which can be measured in exahertz (EHz). (For historical reasons, the frequencies of light and higher frequency electromagnetic radiation are more commonly specified in terms of their wavelengths or photon energies: for a more detailed treatment of this and the above frequency ranges, see electromagnetic spectrum.)\n\nIn computers, most central processing units (CPU) are labeled in terms of their clock rate expressed in megahertz (10 Hz) or gigahertz (10 Hz). This specification refers to the frequency of the CPU's master clock signal. This signal is a square wave, which is an electrical voltage that switches between low and high logic values at regular intervals. As the hertz has become the primary unit of measurement accepted by the general populace to determine the performance of a CPU, many experts have criticized this approach, which they claim is an easily manipulable benchmark. Some processors use multiple clock periods to perform a single operation, while others can perform multiple operations in a single cycle. For personal computers, CPU clock speeds have ranged from approximately 1 MHz in the late 1970s (Atari, Commodore, Apple computers) to up to 6 GHz in IBM POWER microprocessors.\n\nVarious computer buses, such as the front-side bus connecting the CPU and northbridge, also operate at various frequencies in the megahertz range.\n\nHigher frequencies than the International System of Units provides prefixes for are believed to occur naturally in the frequencies of the quantum-mechanical vibrations of high-energy, or, equivalently, massive particles, although these are not directly observable and must be inferred from their interactions with other phenomena. By convention, these are typically not expressed in hertz, but in terms of the equivalent quantum energy, which is proportional to the frequency by the factor of Planck's constant.\n\n\n"}
{"id": "14123", "url": "https://en.wikipedia.org/wiki?curid=14123", "title": "Heroic couplet", "text": "Heroic couplet\n\nA heroic couplet is a traditional form for English poetry, commonly used in epic and narrative poetry, and consisting of a rhyming pair of lines in iambic pentameter. Use of the heroic couplet was pioneered by Geoffrey Chaucer in the \"Legend of Good Women\" and the \"Canterbury Tales\", and generally considered to have been perfected by John Dryden and Alexander Pope in the Restoration Age and early 18th century respectively. \n\nA frequently-cited example illustrating the use of heroic couplets is this passage from \"Cooper's Hill\" by John Denham, part of his description of the Thames:\n\nThe term \"heroic couplet\" is sometimes reserved for couplets that are largely \"closed\" and self-contained, as opposed to the enjambed couplets of poets like John Donne. The heroic couplet is often identified with the English Baroque works of John Dryden and Alexander Pope, who used the form for their translations of the epics of Virgil and Homer, respectively. Major poems in the closed couplet, apart from the works of Dryden and Pope, are Samuel Johnson's \"The Vanity of Human Wishes\", Oliver Goldsmith's \"The Deserted Village\", and John Keats's \"Lamia\". The form was immensely popular in the 18th century. The looser type of couplet, with occasional enjambment, was one of the standard verse forms in medieval narrative poetry, largely because of the influence of the Canterbury Tales.\n\nEnglish heroic couplets, especially in Dryden and his followers, are sometimes varied by the use of the occasional alexandrine, or hexameter line, and triplet. Often these two variations are used together to heighten a climax. The breaking of the regular pattern of rhyming pentameter pairs brings about a sense of poetic closure. Here are two examples from Book IV of Dryden's translation of the \"Aeneid\".\nTwentieth-century authors have occasionally made use of the heroic couplet, often as an allusion to the works of poets of previous centuries. An example of this is Vladimir Nabokov's novel \"Pale Fire\", the second section of which is a 999-line, 4-canto poem largely written in loose heroic couplets with frequent enjambment. Here is an example from the first canto:\n"}
{"id": "14127", "url": "https://en.wikipedia.org/wiki?curid=14127", "title": "Höðr", "text": "Höðr\n\nHöðr ( ; often anglicized as Hod, Hoder, or Hodur) is a blind god and a son of Odin and Frigg in Norse mythology. Tricked and guided by Loki, he shot the mistletoe arrow which was to slay the otherwise invulnerable Baldr.\n\nAccording to the \"Prose Edda\" and the \"Poetic Edda\", the goddess Frigg, Baldr's mother, made everything in existence swear never to harm Baldr, except for the mistletoe, which she found too unimportant to ask (alternatively, which she found too young to demand an oath from). The gods amused themselves by trying weapons on Baldr and seeing them fail to do any harm. Loki, the mischief-maker, upon finding out about Baldr's one weakness, made a spear from mistletoe, and helped Höðr shoot it at Baldr. In reaction to this, Odin and the giantess Rindr gave birth to Váli, who grew to adulthood within a day and slew Höðr.\n\nThe Danish historian Saxo Grammaticus recorded an alternative version of this myth in his \"Gesta Danorum\". In this version, the mortal hero Høtherus and the demi-god \"Balderus\" compete for the hand of Nanna. Ultimately, Høtherus slays Balderus.\n\nIn the \"Gylfaginning\" part of Snorri Sturluson's Prose Edda Höðr is introduced in an ominous way.\n\nHöðr is not mentioned again until the prelude to Baldr's death is described. All things except\nthe mistletoe (believed to be harmless) have sworn an oath not to harm Baldr, so the Æsir throw missiles at him for sport.\n\nThe \"Gylfaginning\" does not say what happens to Höðr after this. In fact it specifically states that Baldr cannot be avenged, at least not immediately.\n\nIt does seem, however, that Höðr ends up in Hel one way or another for the last mention of him in \"Gylfaginning\" is in the description of the post-Ragnarök world.\n\nSnorri's source of this knowledge is clearly \"Völuspá\" as quoted below.\n\nIn the \"Skáldskaparmál\" section of the Prose Edda several kennings for Höðr are related.\n\nNone of those kennings, however, are actually found in surviving skaldic poetry. Neither are Snorri's kennings for Váli, which are also of interest in this context.\n\nIt is clear from this that Snorri was familiar with the role of Váli as Höðr's slayer, even though he does not relate that myth in the \"Gylfaginning\" prose. Some scholars have speculated that he found it distasteful since Höðr is essentially innocent in his version of the story.\n\nHöðr is referred to several times in the Poetic Edda, always in the context of Baldr's death. The following strophes are from \"Völuspá\".\n\nThis account seems to fit well with the information in the Prose Edda, but here the role of Baldr's avenging brother is emphasized.\n\nBaldr and Höðr are also mentioned in \"Völuspá\"'s description of the world after Ragnarök.\n\nThe poem \"Vafþrúðnismál\" informs us that the gods who survive Ragnarök are Viðarr, Váli, Móði and Magni with no mention of Höðr and Baldr.\n\nThe myth of Baldr's death is also referred to in another Eddic poem, \"Baldrs draumar\".\n\nHöðr is not mentioned again by name in the Eddas. He is, however, referred to in \"Völuspá in skamma\".\n\nThe name of Höðr occurs several times in skaldic poetry as a part of warrior-kennings. Thus \"Höðr brynju\", \"Höðr of byrnie\", is a warrior and so is \"Höðr víga\", \"Höðr of battle\". Some scholars have found the fact that the poets should want to compare warriors with Höðr to be incongruous with Snorri's description of him as a blind god, unable to harm anyone without assistance. It is possible that this indicates that some of the poets were familiar with other myths about Höðr than the one related in \"Gylfaginning\" - perhaps some where Höðr has a more active role. On the other hand, the names of many gods occur in kennings and the poets might not have been particular in using any god name as a part of a kenning.\n\nIn \"Gesta Danorum\" Hotherus is a human hero of the Danish and Swedish royal lines. He is gifted in swimming, archery, fighting and music and Nanna, daughter of King Gevarus falls in love with him. But at the same time Balderus, son of Othinus, has caught sight of Nanna bathing and fallen violently in love with her. He resolves to slay Hotherus, his rival.\n\nOut hunting, Hotherus is led astray by a mist and meets wood-maidens who control the fortunes of war. They warn him that Balderus has designs on Nanna but also tell him that he shouldn't attack him in battle since he is a demigod. Hotherus goes to consult with King Gevarus and asks him for his daughter. The king replies that he would gladly favour him but that Balderus has already made a like request and he does not want to incur his wrath.\n\nGevarus tells Hotherus that Balderus is invincible but that he knows of one weapon which can defeat him, a sword kept by Mimingus, the satyr of the woods. Mimingus also has another magical artifact, a bracelet that increases the wealth of its owner. Riding through a region of extraordinary cold in a carriage drawn by reindeer, Hotherus captures the satyr with a clever ruse and forces him to yield his artifacts.\n\nHearing about Hotherus's artifacts, Gelderus, king of Saxony, equips a fleet to attack him. Gevarus warns Hotherus of this and tells him where to meet Gelderus in battle. When the battle is joined, Hotherus and his men save their missiles while defending themselves against those of the enemy with a testudo formation. With his missiles exhausted, Gelderus is forced to sue for peace. He is treated mercifully by Hotherus and becomes his ally. Hotherus then gains another ally with his eloquent oratory by helping King Helgo of Hålogaland win a bride.\n\nMeanwhile, Balderus enters the country of king Gevarus armed and sues for Nanna. Gevarus tells him to learn Nanna's own mind. Balderus addresses her with cajoling words but is refused. Nanna tells him that because of the great difference in their nature and stature, since he is a demigod, they are not suitable for marriage.\n\nAs news of Balderus's efforts reaches Hotherus, he and his allies resolve to attack Balderus. A great naval battle ensues where the gods fight on the side of Balderus. Thoro in particular shatters all opposition with his mighty club. When the battle seems lost, Hotherus manages to hew Thoro's club off at the haft and the gods are forced to retreat. Gelderus perishes in the battle and Hotherus arranges a funeral pyre of vessels for him. After this battle Hotherus finally marries Nanna.\n\nBalderus is not completely defeated and shortly afterwards returns to defeat Hotherus in the field. But Balderus's victory is without fruit for he is still without Nanna. Lovesick, he is harassed by phantoms in Nanna's likeness and his health deteriorates so that he cannot walk but has himself drawn around in a cart.\n\nAfter a while Hotherus and Balderus have their third battle and again Hotherus is forced to retreat. Weary of life because of his misfortunes, he plans to retire and wanders into the wilderness. In a cave he comes upon the same maidens he had met at the start of his career. Now they tell him that he can defeat Balderus if he gets a taste of some extraordinary food which had been devised to increase the strength of Balderus.\n\nEncouraged by this, Hotherus returns from exile and once again meets Balderus in the field. After a day of inconclusive fighting, he goes out during the night to spy on the enemy. He finds where Balderus's magical food is prepared and plays the lyre for the maidens preparing it. While they don't want to give him the food, they bestow on him a belt and a girdle which secure victory.\n\nHeading back to his camp, Hotherus meets Balderus and plunges his sword into his side. After three days, Balderus dies from his wound. Many years later, Bous, the son of Othinus and Rinda, avenges his brother by killing Hotherus in a duel.\n\nThere are also two lesser-known DanishLatin chronicles, the \"Chronicon Lethrense\" and the \"Annales Lundenses\", of which the latter is included in the former. These two sources provide a second euhemerized account of Höðr's slaying of Balder.\n\nIt relates that Hother was the king of the Saxons, son of Hothbrod, the daughter of Hadding. Hother first slew Othen's (i.e., Odin's) son Balder in battle and then chased Othen and Thor. Finally, Othen's son Both killed Hother. Hother, Balder, Othen, and Thor were incorrectly considered to be gods.\n\nAccording to the Swedish mythologist and romantic poet Viktor Rydberg, the story of Baldr's death was taken from Húsdrápa, a poem composed by Ulfr Uggason around 990 AD at a feast thrown by the Icelandic Chief Óláfr Höskuldsson to celebrate the finished construction of his new home, Hjarðarholt, the walls of which were filled with symbolic representations of the Baldr myth among others. Rydberg suggested that Höðr was depicted with eyes closed and Loki guiding his aim to indicate that Loki was the true cause of Baldr's death and Höðr was only his \"blind tool.\" Rydberg theorized that the author of the \"Gylfaginning\" then mistook the description of the symbolic artwork in the Húsdrápa as the actual tale of Baldr's death.\n\n\n"}
{"id": "14128", "url": "https://en.wikipedia.org/wiki?curid=14128", "title": "Herat", "text": "Herat\n\nHerāt (; Persian/Pashto: ) is the third-largest city of Afghanistan. It has a population of about 436,300, and serves as the capital of Herat Province, situated in the fertile valley of the Hari River in the western part of the country. It is linked with Kandahar, Kabul, and Mazar-i-Sharif via Highway 1 or the ring road. It is further linked to the city of Mashhad in neighboring Iran through the border town of Islam Qala, and to Mary in Turkmenistan to the north through the border town of Torghundi.\n\nHerat dates back to the Avestan times and was traditionally known for its wine. The city has a number of historic sites, including the Herat Citadel and the Musalla Complex. During the Middle Ages Herat became one of the important cities of Khorasan, as it was known as the \"Pearl of Khorasan\". It has been governed by various Afghan rulers since the early 18th century. In 1717, the city was invaded by the Hotaki forces until they were expelled by the Afsharids in 1729. After Nader Shah's death and Ahmad Shah Durrani's rise to power in 1747, Herat became part of Afghanistan. It witnessed some political disturbances and military invasions during the early half of the 19th century but the 1857 Treaty of Paris ended hostilities of the Anglo-Persian War.\n\nHerat lies on the ancient trade routes of the Middle East, Central and South Asia, and today is a regional hub in western Afghanistan. The roads from Herat to Iran, Turkmenistan, and other parts of Afghanistan are still strategically important. As the gateway to Iran, it collects high amount of customs revenue for Afghanistan. It also has an international airport. The city has high residential density clustered around the core of the city. However, vacant plots account for a higher percentage of the city (21%) than residential land use (18%) and agricultural is the largest percentage of total land use (36%). Today the city is considered to be relatively safe.\n\nHerat dates back to ancient times (its exact age remains unknown). During the period of the Achaemenid Empire (ca. 550-330 BC), the surrounding district was known as \"Haraiva\" (in Old Persian), and in classical sources the region was correspondingly known as Aria (Areia). In the Zoroastrian Avesta, the district is mentioned as \"Haroiva\". The name of the district and its main town is derived from that of the chief river of the region, the Herey River (Old Dari \"Hereyrud\", \"Silken Water\"), which traverses the district and passes some south of modern Herāt. Herey is mentioned in Sanskrit as yellow or golden color equivalent to Persian \"Zard\" meaning Gold (yellow). The naming of a region and its principal town after the main river is a common feature in this part of the world—compare the adjoining districts/rivers/towns of Arachosia and Bactria.\n\nThe district \"Aria\" of the Achaemenid Empire is mentioned in the provincial lists that are included in various royal inscriptions, for instance, in the Behistun inscription of Darius I (ca. 520 BC). Representatives from the district are depicted in reliefs, e.g., at the royal Achaemenid tombs of Naqsh-e Rustam and Persepolis. They are wearing Scythian-style dress (with a tunic and trousers tucked into high boots) and a twisted Bashlyk that covers their head, chin and neck.\n\nHamdallah Mustawfi, composer of the 14th century work \"The Geographical Part of the Nuzhat-al-Qulub\" writes that:\n\nHerodotus described Herat as \"the bread-basket of Central Asia\". At the time of Alexander the Great in 330 BC, Aria was obviously an important district. It was administered by a satrap called Satibarzanes, who was one of the three main Persian officials in the East of the Empire, together with the satrap Bessus of Bactria and Barsaentes of Arachosia. In late 330 BC, Alexander captured the Arian capital that was called Artacoana. The town was rebuilt and the citadel was constructed. Afghanistan became part of the Seleucid Empire after Alexander died.\n\nMost sources suggest that Herat was predominantly Zoroastrian in the pre-Islamic period. It became part of the Parthian Empire in 167 BC. In the Sasanian period (226-652), \"Harēv\" is listed in an inscription on the Ka'ba-i Zartosht at Naqsh-e Rustam; and \"Hariy\" is mentioned in the Pahlavi catalogue of the provincial capitals of the empire. In around 430, the town is also listed as having a Christian community, with a Nestorian bishop.\n\nIn the last two centuries of Sasanian rule, Aria (Herat) had great strategic importance in the endless wars between the Sasanians, the Chionites and the Hephthalites who had been settled in the northern section of Afghanistan since the late 4th century.\n\nAt the time of the Arab invasion in the middle of the 7th century, the Sasanian central power seemed already largely nominal in the province in contrast with the role of the Hephthalites tribal lords, who were settled in the Herat region and in the neighboring districts, mainly in pastoral Bādghis and in Qohestān. It must be underlined, however, that Herat remained one of the three Sasanian mint centers in the east, the other two being Balkh and Marv. The Hephthalites from Herat and some unidentified Turks opposed the Arab forces in a battle of Qohestān in 651-52 AD, trying to block their advance on Nishāpur, but they were defeated\n\nWhen the Arab armies appeared in Khorāsān in the 650s AD, Herāt was counted among the twelve capital towns of the Sasanian Empire. The Arab army under the general command of Ahnaf ibn Qais in its conquest of Khorāsān in 652 seems to have avoided Herāt, but it can be assumed that the city eventually submitted to the Arabs, since shortly afterwards an Arab governor is mentioned there. A treaty was drawn in which the regions of Bādghis and Bushanj were included. As did many other places in Khorāsān, Herāt rebelled and had to be re-conquered several times.\n\nAnother power that was active in the area in the 650s was Tang dynasty China which had embarked on a campaign that culminated in the Conquest of the Western Turks. By 659-661, the Tang claimed a tenuous suzerainty over Herat, the westernmost point of Chinese power in its long history. This hold however would be ephemeral with local Turkish tribes rising in rebellion in 665 and driving out the Tang.\n\nIn 702 AD Yazid ibn al-Muhallab defeated certain Arab rebels, followers of Ibn al-Ash'ath, and forced them out of Herat. The city was the scene of conflicts between different groups of Muslims and Arab tribes in the disorders leading to the establishment of the Abbasid Caliphate. Herat was also a centre of the followers of Ustadh Sis.\n\nIn 870 AD, Yaqub ibn Layth Saffari, a local ruler of the Saffarid dynasty conquered Herat and the rest of the nearby regions in the name of Islam.\n\nThe region of Herāt was under the rule of King Nuh III, the seventh of the Samanid line—at the time of Sebük Tigin and his older son, Mahmud of Ghazni. The governor of Herāt was a noble by the name of \"Faik\", who was appointed by Nuh III. It is said that Faik was a powerful, but insubordinate governor of Nuh III; and had been punished by Nuh III. Faik made overtures to Bogra Khan and Ughar Khan of Khorasan. Bogra Khan answered Faik's call, came to Herāt and became its ruler. The Samanids fled, betrayed at the hands of Faik to whom the defence of Herāt had been entrusted by Nuh III. In 994, Nuh III invited Alp Tigin to come to his aid. Alp Tigin, along with Mahmud of Ghazni, defeated Faik and annexed Herāt, Nishapur and Tous.\n\nHerat was a great trading centre strategically located on trade routes from Mediterranean to India or to China. The city was noted for its textiles during the Abbasid Caliphate, according to many references by geographers. Herāt also had many learned sons such as Ansārī. The city is described by Estakhri and Ibn Hawqal in the 10th century as a prosperous town surrounded by strong walls with plenty of water sources, extensive suburbs, an inner citadel, a congregational mosque, and four gates, each gate opening to a thriving market place. The government building was outside the city at a distance of about a mile in a place called Khorāsānābād. A church was still visible in the countryside northeast of the town on the road to Balkh, and farther away on a hilltop stood a flourishing fire temple, called Sereshk, or Arshak according to Mustawfi.\n\nHerat was a part of the Taherid dominion in Khorāsān until the rise of the Saffarids in Sistān under Ya'qub-i Laith in 861, who, in 862, started launching raids on Herat before besieging and capturing it on 16 August 867, and again in 872. The Saffarids succeeded in expelling the Taherids from Khorasan in 873.\n\nThe Sāmānid dynasty was established in Transoxiana by three brothers, Nuh, Yahyā, and Ahmad. Ahmad Sāmāni opened the way for the Samanid dynasty to the conquest of Khorāsān, including Herāt, which they were to rule for one century. The centralized Samanid administration served as a model for later dynasties. The Samanid power was destroyed in 999 by the Qarakhanids, who were advancing on Transoxiana from the northeast, and by the Ghaznavids, former Samanid retainers, attacking from the southeast.\n\nSultan Maḥmud of Ghazni officially took control of Khorāsān in 998. Herat was one of the six Ghaznavid mints in the region. In 1040, Herat was captured by the Seljuk Empire. Yet, in 1175, it was captured by the Ghurids of Ghor and then came under the Khawarazm Empire in 1214. According to the account of Mustawfi, Herat flourished especially under the Ghurid dynasty in the 12th century. Mustawfi reported that there were \"359 colleges in Herat, 12,000 shops all fully occupied, 6,000 bath-houses; besides caravanserais and mills, also a darwish convent and a fire temple\". There were about 444,000 houses occupied by a settled population. The men were described as \"warlike and carry arms\", and they were Sunni Muslims. The great mosque of Herāt was built by Ghiyas ad-Din Ghori in 1201. In this period Herāt became an important center for the production of metal goods, especially in bronze, often decorated with elaborate inlays in precious metals.\n\nHerat was invaded and destroyed by Genghis Khan's Mongol army in 1221. The city was destroyed a second time and remained in ruins from 1222 to about 1236. In 1244 a local prince Shams al-Din Kart was named ruler of Herāt by the Mongol governor of Khorāsān and in 1255 he was confirmed in his rule by the founder of the Il-Khan dynasty Hulagu. Shams al-Din founded a new dynasty and his successors, especially Fakhr-al-Din and Ghiyath al-Din, built many mosques and other buildings. The members of this dynasty were great patrons of literature and the arts. By this time Herāt became known as the \"pearl of Khorasan\".\n\nTimur took Herat in 1380 and he brought the Kartid dynasty to an end a few years later. The city reached its greatest glory under the Timurid princes, especially Sultan Husayn Bayqara who ruled Herat from 1469 until May 4, 1506. His chief minister, the poet and author in Persian and Turkish, Mir Ali-Shir Nava'i was a great builder and patron of the arts. Under the Timurids, Herat assumed the role of the main capital of an empire that extended in the West as far as central Persia. As the capital of the Timurid empire, it boasted many fine religious buildings and was famous for its sumptuous court life and musical performance and its tradition of miniature paintings. On the whole, the period was one of relative stability, prosperity, and development of economy and cultural activities. It began with the nomination of Shahrokh, the youngest son of Timur, as governor of Herat in 1397. The reign of Shahrokh in Herat was marked by intense royal patronage, building activities, and promotion of manufacturing and trade, especially through the restoration and enlargement of the Herat's bāzār. The present Musallah Complex, and many buildings such as the madrasa of Goharshad, Ali Shir mahāl, many gardens, and others, date from this time. The village of Gazar Gah, over two km northeast of Herat, contained a shrine which was enlarged and embellished under the Timurids. The tomb of the poet and mystic Khwājah Abdullāh Ansārī (d. 1088), was first rebuilt by Shahrokh about 1425, and other famous men were buried in the shrine area. Herat was shortly captured by Kara Koyunlu between 1458–1459.\n\nIn 1507 Herat was occupied by the Uzbeks but after much fighting the city was taken by Shah Isma'il, the founder of the Safavid dynasty, in 1510 and the Shamlu Qizilbash assumed the governorship of the area. Under the Safavids, Herat was again relegated to the position of a provincial capital, albeit one of a particular importance. At the death of Shah Isma'il the Uzbeks again took Herat and held it until Shah Tahmasp retook it in 1528. The Persian king, Abbas was born in Herat, and in Safavid texts, Herat is referred to as \"a'zam-i bilād-i īrān\", meaning \"the greatest of the cities of Iran\". In the 16th century, all future Safavid rulers, from Tahmasp I to Abbas I, were governors of Herat in their youth.\n\nBy the early 18th century Herat was governed by various Hotaki and Abdali Afghans. After Nader Shah's death in 1747, Ahmad Shah Durrani took possession of the city and became part of the Durrani Empire.\n\nIn 1824, Herat became independent for several years when the Afghan Empire was split between the Durranis and Barakzais. The Persians invaded the city in 1838, but the British helped the Afghans in repelling them. In 1856, they invaded again, and briefly managed to retake the city; it led directly to the Anglo-Persian War. In 1857 hostilities between the Persians and the British ended after the Treaty of Paris was signed, and the Persian troops withdrew from Herat.\n\nOne of the greatest tragedies for the Afghans and Muslims was the British invasion of, and subsequent destruction of the Islamic Musallah complex in Herat in 1885. The officially stated reason was to get a good line of sight for their artillery against Russian invaders who never came. This was but one small sidetrack in the Great Game, a century-long conflict between the British Empire and the Russian Empire in 19th century.\n\nIn the 1960s, engineers from the United States built Herat Airport, which was used by the Soviet forces during the Democratic Republic of Afghanistan in the 1980s. Even before the Soviet invasion at the end of 1979, there was a substantial presence of Soviet advisors in the city with their families.\n\nBetween March 10 and March 20, 1979, the Afghan Army in Herāt under the control of commander Ismail Khan mutinied. Thousands of protesters took to the streets against the Khalq communist regime's oppression led by Nur Mohammad Taraki. The new rebels led by Khan managed to oust the communists and take control of the city for 3 days, with some protesters murdering any Soviet advisers. This shocked the government, who blamed the new administration of Iran following the Iranian Revolution for influencing the uprising. Reprisals by the government followed, and between 3,000 and 24,000 people (according to different sources) were killed, in what is called the 1979 Herat uprising, or in Persian as the \"Qiam-e Herat\". The city itself was recaptured with tanks and airborne forces, but at the cost of thousands of civilians killed. This massacre was the first of its kind since the country's independence in 1919, and was the bloodiest event preceding the Soviet–Afghan War.\n\nHerat received damage during the Soviet–Afghan War in the 1980s, especially its western side. The province as a whole was one of the worst-hit. In April 1983, a series of Soviet bombings damaged half of the city and killed around 3,000 civilians, described as \"extremely heavy, brutal and prolonged\". Ismail Khan was the leading mujahideen commander in Herāt fighting against the Soviet-backed government.\n\nAfter the communist government's collapse in 1992, Khan joined the new government and he became governor of Herat Province. The city was relatively safe and it was recovering and rebuilding from the damage caused in the Soviet–Afghan War. However, on September 5, 1995, the city was captured by the Taliban without much resistance, forcing Khan to flee. Herat became the first Persian-speaking city to be captured by the Taliban. The Taliban's strict enforcement of laws confining women at home and closing girls' schools alienated Heratis who are traditionally more liberal and educated, like the Kabulis, than other urban populations in the country. Two days of anti-Taliban protests occurred in December 1996 which was violently dispersed and led to the imposition of a curfew.\n\nAfter the U.S. invasion of Afghanistan, on November 12, 2001, it was captured from the Taliban by forces loyal to the Northern Alliance and Ismail Khan returned to power (see Battle of Herat). In 2004, Mirwais Sadiq, Aviation Minister of Afghanistan and the son of Ismail Khan, was ambushed and killed in Herāt by a local rival group. More than 200 people were arrested under suspicion of involvement.\n\nIn 2005, the International Security Assistance Force (ISAF) began establishing bases in and around the city. Its main mission was to train the Afghan National Security Forces (ANSF) and help with the rebuilding process of the country. Regional Command West, led by Italy, assisted the Afghan National Army (ANA) 207th Corps. Herat was one of the first seven areas that transitioned security responsibility from NATO to Afghanistan. In July 2011, the Afghan security forces assumed security responsibility from NATO.\n\nDue to their close relations, Iran began investing in the development of Herat's power, economy and education sectors. In the meantime, the United States built a consulate in Herat to help further strengthen its relations with Afghanistan. In addition to the usual services, the consulate works with the local officials on development projects and with security issues in the region.\n\nHerat has a cold semi-arid climate (Köppen climate classification \"BSk\"). Precipitation is very low, and mostly falls in winter. Although Herāt is approximately lower than Kandahar, the summer climate is more temperate, and the climate throughout the year is far from disagreeable, although winter temperatures are comparably lower. From May to September, the wind blows from the northwest with great force. The winter is tolerably mild; snow melts as it falls, and even on the mountains does not lie long. Three years out of four it does not freeze hard enough for the people to store ice. The eastern reaches of the Hari River, including the rapids, are frozen hard in the winter, and people travel on it as on a road.\n\nIndia, Iran and Pakistan operate their consulate here for trade, military and political links.\n\n\nOf the more than dozen minarets that once stood in Herāt, many have been toppled from war and neglect over the past century. Recently, however, everyday traffic threatens many of the remaining unique towers by shaking the very foundations they stand on. Cars and trucks that drive on a road encircling the ancient city rumble the ground every time they pass these historic structures. UNESCO personnel and Afghan authorities have been working to stabilize the Fifth Minaret.\n\n\nThe population of Herat numbers approximately 436,300 as of 2013. It is a multi-ethnic society with Persian-speakers as the majority. There is no current data on the precise ethnic make-over but according to a 2003 map found in the National Geographic Magazine, Persian-speaking Tajiks and Farsiwan form the overwhelming majority of the city, comprising ca. 80%. Pashtuns (10%), Hazaras (2%), Uzbeks (2%), and Turkmens (1%) form sizable minorities.\n\nPersian is the native language of Herat and the local dialect – known by natives as \"Herātī\" – belongs to the \"Khorāsānī\" cluster within Persian. It is akin to the Persian dialects of eastern Iran, notably those of Mashhad and Khorasan Province. It serves as the lingua franca of the city. The second language that is understood by many is Pashto, which is the native language of the Pashtuns. The local Pashto dialect spoken in Herat is a variant of western Pashto, which is also spoken in Kandahar and southern and western Afghanistan. Religiously, Sunni Islam is practiced by the majority while Shias make up the minority.\n\nThe city once had a Jewish community. About 280 families lived in Herat as of 1948 but most moved to Israel that year, and the community disappeared by 1992. There are four former synagogues in the city's old quarter, which were neglected but in the late 2000s renovated by the Aga Khan Trust for Culture, three of them turning into nurseries and schools. The Jewish cemetery is being taken care of by Jalilahmed Abdelaziz.\n\n\nHerat International Airport was built by engineers from the United States in the 1960s and was used by the Soviet Armed Forces during the Soviet–Afghan War in the 1980s. It was bombed in late 2001 during Operation Enduring Freedom but had been rebuilt within the next decade. The runway of the airport has been extended and upgraded and as of August 2014 there were regularly scheduled direct flights to Delhi, Dubai, Mashad, and various airports in Afghanistan. At least five airlines operated regularly scheduled direct flights to Kabul.\n\nRail connections to and from Herat were proposed many times, during \"The Great Game\" of the 19th century and again in the 1970s and 1980s, but nothing came to life. In February 2002, Iran and the Asian Development Bank announced funding for a railway connecting Torbat-e Heydarieh in Iran to Herat. This was later changed to begin in Khaf in Iran, a railway for both cargo and passengers, with work on the Iranian side of the border starting in 2006. Construction is underway in the Afghan side and it was estimated to be completed by March 2018. There is also the prospect of an extension across Afghanistan to Sher Khan Bandar.\n\nThe AH76 highway connects Herat to Maymana and the north. The AH77 connects it east towards Chaghcharan and north towards Mary in Turkmenistan. Highway 1 (part of Asian highway AH1) links it to Mashhad in Iran to the northwest, and south via the Kandahar–Herat Highway to Delaram.\n\n\n\n\n \n"}
{"id": "14130", "url": "https://en.wikipedia.org/wiki?curid=14130", "title": "Hedeby", "text": "Hedeby\n\nHedeby (, Old Norse \"Heiðabýr\", German \"Haithabu\") was an important Danish Viking Age (8th to the 11th centuries) trading settlement near the southern end of the Jutland Peninsula, now in the Schleswig-Flensburg district of Schleswig-Holstein, Germany. It is the most important archaeological site in Schleswig-Holstein.\n\nThe settlement developed as a trading centre at the head of a narrow, navigable inlet known as the Schlei, which connects to the Baltic Sea. The location was favorable because there is a short portage of less than 15 km to the Treene River, which flows into the Eider with its North Sea estuary, making it a convenient place where goods and ships could be pulled on a corduroy road overland for an almost uninterrupted seaway between the Baltic and the North Sea and avoid a dangerous and time-consuming circumnavigation of Jutland, providing Hedeby with a role similar to later Lübeck.\nHedeby was the second largest Nordic town during the Viking Age, after Uppåkra in present-day southern Sweden,\nThe city of Schleswig was later founded on the other side of the Schlei. Hedeby was abandoned after its destruction in 1066.\n\nHedeby was rediscovered in the late 19th century and excavations began in 1900. The Haithabu Museum was opened next to the site in 1985.\n\nThe Old Norse name \"Heiða-býr\" simply translates to \"heath-settlement\" (\"heiðr\" \"heath\" and \"býr\" = \"yard; settlement, village, town\"). The name is recorded in numerous spelling variants.\n\n\nSources from the 9th and 10th century AD also attest to the names \"Sliesthorp\" and \"Sliaswich\" (cf. \"-thorp\" vs. \"-wich\"), and the town of Schleswig still exists 3 km north of Hedeby. However, Æthelweard claimed in his Latin translation of the Anglo-Saxon Chronicle that the Saxons used \"Slesuuic\" and the Danes \"Haithaby\" to refer to the same town.\n\nHedeby is first mentioned in the Frankish chronicles of Einhard (804) who was in the service of Charlemagne,\nbut was probably founded around 770. In 808 the Danish king Godfred (Lat. Godofredus) destroyed a competing Slav trade centre named Reric, and it is recorded in the Frankish chronicles that he moved the merchants from there to Hedeby. This may have provided the initial impetus for the town to develop. The same sources record that Godfred strengthened the Danevirke, an earthen wall that stretched across the south of the Jutland peninsula. The Danevirke joined the defensive walls of Hedeby to form an east-west barrier across the peninsula, from the marshes in the west to the Schlei inlet leading into the Baltic in the east.\n\nThe town itself was surrounded on its three landward sides (north, west, and south) by earthworks. At the end of the 9th century the northern and southern parts of the town were abandoned for the central section. Later a 9-metre (29-ft) high semi-circular wall was erected to guard the western approaches to the town. On the eastern side, the town was bordered by the innermost part of the Schlei inlet and the bay of Haddebyer Noor.\n\nHedeby became a principal marketplace because of its geographical location on the major trade routes between the Frankish Empire and Scandinavia (north-south), and between the Baltic and the North Sea (east-west). Between 800 and 1000 the growing economic power of the Vikings led to its dramatic expansion as a major trading centre.\n\nThe following indicate the importance achieved by the town:\n\nA Swedish dynasty founded by Olof the Brash is said to have ruled Hedeby during the last decades of the 9th century and the first part of the 10th century. This was told to Adam of Bremen by the Danish king Sweyn Estridsson, and it is supported by three runestones found in Denmark. Two of them were raised by the mother of Olof's grandson Sigtrygg Gnupasson. The third runestone, discovered in 1796, is from Hedeby, the \"Stone of Eric\" (). It is inscribed with Norwegian-Swedish runes. It is, however, possible that Danes also occasionally wrote with this version of the younger futhark.\n\nLife was short and crowded in Hedeby. The small houses were clustered tightly together in a grid, with the east-west streets leading down to jetties in the harbour. People rarely lived beyond 30 or 40, and archaeological research shows that their later years were often painful due to crippling diseases such as tuberculosis. Yet make-up for men and rights for women provide surprises to the modern understanding.\n\nAl-Tartushi, a late 10th-century traveller from al-Andalus, provides one of the most colourful and often quoted descriptions of life in Hedeby. Al-Tartushi was from Cordoba in Spain, which had a significantly more wealthy and comfortable lifestyle than Hedeby. While Hedeby may have been significant by Scandinavian standards, Al-Tartushi was unimpressed:\n\nThe town was sacked in 1050 by King Harald Hardrada of Norway during a conflict with King Sweyn II of Denmark. He set the town on fire by sending several burning ships into the harbour, the charred remains of which were found at the bottom of the Schlei during recent excavations. A Norwegian \"skald\", quoted by Snorri Sturluson, describes the sack as follows:\n\nIn 1066 the town was sacked and burned by West Slavs. Following the destruction, Hedeby was slowly abandoned. People moved across the Schlei inlet, which separates the two peninsulas of Angeln and Schwansen, and founded the town of Schleswig.\n\nAfter the settlement was abandoned, rising waters contributed to the complete disappearance of all visible structures on the site. It was even forgotten where the settlement had been. This proved to be fortunate for later archaeological work at the site.\n\nArchaeological work began at the site in 1900 after the rediscovery of the settlement. Excavations were conducted for the next 15 years. Further excavations were carried out between 1930 and 1939. Archaeological work on the site was productive for two main reasons: that the site had never been built on since its destruction some 840 years earlier, and that the permanently waterlogged ground had preserved wood and other perishable materials. After the Second World War, in 1959, archaeological work was started again and has continued intermittently ever since. The embankments surrounding the settlement were excavated, and the harbour was partially dredged, during which the wreck of a Viking ship was discovered. Despite all this work, only 5% of the settlement (and only 1% of the harbour) has as yet been investigated.\n\nThe most important finds resulting from the excavations are now on display in the adjoining Haithabu Museum.\n\nIn 2005 an ambitious archaeological reconstruction program was initiated on the original site. Based on the results of archaeological analyses, exact copies of some of the original Viking houses have been built.\n\n\n\n"}
{"id": "14131", "url": "https://en.wikipedia.org/wiki?curid=14131", "title": "Hazaras", "text": "Hazaras\n\nThe Hazaras (, ) are an ethnic group native to the region of Hazarajat in central Afghanistan, speaking the Hazaragi variant of Dari, itself an eastern variety of Persian and one of the two official languages of Afghanistan.\n\nThey are the third-largest ethnic group in Afghanistan. They also make up a significant minority group in the neighboring Pakistan, with a population of over 650,000–900,000, largely living in the region of Quetta.\n\nBabur, founder of the Mughal Empire in the early 16th century, records the name \"Hazara\" in his autobiography. He referred to the populace of a region called \"Hazaristan\", located west of the Kabulistan region, north of Ghazna, and southwest of Ghor.\n\nThe conventional theory is that the name \"Hazara\" derives from the Persian word for \"thousand\" ( ). It may be the translation of the Mongol word (or ), a military unit of 1,000 soldiers at the time of Genghis Khan. With time, the term \"Hazar\" could have been substituted for the Mongol word and now stands for the group of people, while the Hazaras in their native language always call themselves ( ) and ( ).\n\nThe origins of the Hazara have not been fully reconstructed. Significant inner Asian descent—in historical context, Turkic and Mongol—is impossible to rule out because the Hazara's physical attributes, facial bone structures and parts of their culture and language resemble those of Mongolians and Central Asian Turks. Genetic analysis of the Hazara indicate partial Mongolian ancestry. Invading Mongols and Turco-Mongols mixed with the local Iranian population, forming a distinct group.\nFor example, Nikudari Mongols settled in what is now Afghanistan and mixed with native populations who spoke Dari Persian. A second wave of mostly Chagatai Mongols came from Central Asia and were followed by other Mongolic groups, associated with the Ilkhanate and the Timurids, all of whom settled in Hazarajat and mixed with the local, mostly Dari-speaking population, forming a distinct group.\n\nThe Hazara identity in Afghanistan is believed by many to have originated in the aftermath of the 1221 Siege of Bamyan. The first mention of Hazara are made by Babur in the early 16th century and later by the court historians of Shah Abbas of the Safavid dynasty. It is reported that they embraced Shia Islam between the end of the 16th and the beginning of the 17th century, during the Safavid period.\n\nHazara men along with tribes of other ethnic groups had been recruited and added to the army of Ahmad Shah Durrani in the 18th century. Some claim that in the mid‑18th century Hazara were forced out of Helmand and the Arghandab District of Kandahar Province.\n\nDuring the second reign of Dost Mohammad Khan in the 19th century, Hazara from Hazarajat began to be taxed for the first time. However, for the most part they still managed to keep their regional autonomy until the subjugation of Abdur Rahman Khan began in the late 19th century.\nWhen the Treaty of Gandomak was signed and the Second Anglo-Afghan War ended in 1880, Abdur Rahman Khan set out a goal to bring Hazarajat and Kafiristan under his control. He launched several campaigns in Hazarajat due to resistance from the Hazara in which his forces committed atrocities. The southern part of Hazarajat was spared as they accepted his rule, while the other parts of Hazarajat rejected Abdur Rahman and instead supported his uncle, Sher Ali Khan. In response to this Abdur Rahman waged a war against tribal leaders who rejected his policies and rule. Abdur Rahman arrested Syed Jafar, chief of the Sheikh Ali Hazara tribe, and jailed him in Mazar-i-Sharif.\n\nThe 1888–1893 Uprisings of Hazaras occurred when the Treaty of Gandomak was signed and the Second Anglo-Afghan War ended in 1880, causing Abdur Rahman Khan to set out on a goal to bring Hazarajat and Kafiristan under his control. He launched several campaigns in Hazarajat due to resistance from the Hazara in which his forces committed atrocities. The southern part of Hazarajat was spared as they accepted his rule, while the other parts of Hazarajat rejected Abdur Rahman and instead supported his uncle, Sher Ali Khan. In response to this Abdur Rahman waged a war against tribal leaders who rejected his policies and rule. Abdur Rahman arrested Syed Jafar, chief of the Sheikh Ali Hazara tribe, and jailed him in Mazar-i-Sharif. These campaigns had a catastrophic impact on the demographics of Hazaras causing 60% of them to perish or become displaced.\n\nIn 1901, Habibullah Khan, Abdur Rahman's successor, granted amnesty to all people who were exiled by his predecessor. However, the division between the Afghan government and the Hazara people was already made too deep under Abdur Rahman. Hazara continued to face severe social, economic and political discrimination through most of the 20th century. In 1933 King Mohammed Nadir Khan was assassinated by Abdul Khaliq Hazara. The Afghan government captured and executed him later, along with several of his innocent family members.\n\nMistrust of the central government by the Hazaras and local uprisings continued. In particular, in the 1940s, during Zahir Shah's rule, a revolt took place against new taxes that were exclusively imposed on the Hazara. The Kuchi nomads meanwhile not only were exempted from taxes, but also received allowances from the Afghan government. The angry rebels began capturing and killing government officials. In response, the central government sent a force to subdue the region and later removed the taxes.\nDuring the Soviet–Afghan War, the Hazarajat region did not see as much heavy fighting as other regions of Afghanistan. However, rival Hazara political factions fought. The division was between the \"Tanzáim-i nasl-i naw-i Hazara\", a party based in Quetta, of Hazara nationalists and secular intellectuals, and the pro-Khomeini Islamist parties backed by the new \"Islamic Republic of Iran\". By 1979, the Iran-backed Islamist groups liberated Hazarajat from the central Soviet-backed Afghan government and later took entire control of Hazarajat away from the secularists. By 1984, after severe fighting, the secularist groups lost all their power to the Islamists.\n\nAs the Soviets withdrew in 1989, the Islamist groups felt the need to broaden their political appeal and turned their focus to Hazara ethnic nationalism. This led to establishment of the Hizb-i-Wahdat, an alliance of all the Hazara resistance groups (except the \"Harakat-i Islami\"). In 1992 with the fall of Kabul, the \"Harakat-i Islami\" took sides with Burhanuddin Rabbani's government while the Hizb-i-Wahdat took sides with the opposition. The Hizb-i-Wahdat was eventually forced out of Kabul in 1995 when the Taliban movement captured and killed their leader Abdul Ali Mazari. With the Taliban's capture of Kabul in 1996, all the Hazara groups united with the new Northern Alliance against the common new enemy. However, it was too late and despite the fierce resistance Hazarajat fell to the Taliban by 1998. The Taliban had Hazarajat totally isolated from the rest of the world going as far as not allowing the United Nations to deliver food to the provinces of Bamiyan, Ghor, Wardak, and Daykundi.\n\nThough Hazara played a role in the anti-Soviet movement, other Hazara participated in the new communist government, which actively courted Afghan minorities. Sultan Ali Kishtmand, a Hazara, served as prime minister of Afghanistan from 1981–1990 (with one brief interruption in 1988). The Ismaili Hazara of Baghlan Province likewise supported the communists, and their \"pir\" (religious leader) Jaffar Naderi led a pro-Communist militia in the region.\n\nDuring the years that followed, Hazara suffered severe oppression and many ethnic massacres, genocides and pogroms were carried out by the predominantly ethnic Pashtun Taliban and are documented by such groups the Human Rights Watch. These human rights abuses not only occurred in Hazarajat, but across all districts controlled by the Taliban. Particularly after their capture of Mazar-i-Sharif in 1998, where after a massive killing of some 8,000 civilians, the Taliban openly declared that the Hazara would be targeted.\n\nFollowing the 11 September 2001 attacks in the United States, British and American forces invaded Afghanistan. Many Hazara have become leaders in today's newly emerging Afghanistan. Hazara have also pursued higher education, enrolled in the army, and many have top government positions. For example, Mohammad Mohaqiq, a Hazara from the Hizb-i-Wahdat party, ran in the 2004 presidential election in Afghanistan, and Karim Khalili became the Vice President of Afghanistan. A number of ministers and governors are Hazara, including Sima Samar, Habiba Sarabi, Sarwar Danish, Sayed Hussein Anwari, Abdul Haq Shafaq, Sayed Anwar Rahmati, Qurban Ali Oruzgani and many others. The mayor of Nili in Daykundi Province is Azra Jafari, who became the first female mayor in Afghanistan. The National Assembly of Afghanistan (Parliament) is 25% made up of ethnic Hazara, which represents 61 members.\n\nAlthough Afghanistan has been historically one of the poorest countries in the world, the Hazarajat region has been kept even more poor from development by past governments. Since ousting the Taliban in late 2001, billions of dollars have poured into Afghanistan for reconstruction and several large-scale reconstruction projects took place in Afghanistan from August 2012. For example, there have been more than 5000 kilometers of road pavement completed across Afghanistan, of which little was done in central Afghanistan Hazarajat. On the other hand, the Band-e Amir in the Bamyan Province became the first national park of Afghanistan. The road from Kabul to Bamyan was also built, along with new police stations, government institutions, hospitals, and schools in the Bamyan Province, Daykundi Province, and the others. The first ski resort of Afghanistan was also established in Bamyan Province.\n\nAn indication of discrimination is that Kuchis (Afghan nomads who have historically been migrating from region to region depending on the season) are allowed to use Hazarajat pastures during the summer season. It is believed that allowing the Kuchis to use some of the grazing land in Hazarajat began during the rule of Abdur Rahman Khan. Living in mountainous Hazarajat, where little farm land exists, Hazara people rely on these pasture lands for their livelihood during the long and harsh winters. In 2007 some Kuchi nomads entered into parts of Hazarajat to graze their livestock, and when the local Hazara resisted, a clash took place and several people on both sides died using assault rifles. Such events continue to occur, even after the central government was forced to intervene, including President Hamid Karzai. In late July 2012, a Hazara police commander in Uruzgan province reportedly rounded up and killed 9 Pashtun civilians in revenge for the death of two local Hazara. The matter is being investigated by the Afghan government.\n\nThe drive by President Hamid Karzai after the Peace Jirga to strike a deal with Taliban leaders caused deep unease in Afghanistan's minority communities, who fought the Taliban the longest and suffered the most during their rule. The leaders of the Tajik, Uzbek and Hazara communities, which together make up around 65% of the country's population, vowed to resist any return of the Taliban to power, referring to the large-scale massacres of Hazara civilians during the Taliban period.\n\nGenetically, the Hazara are a mixture of western Eurasian and eastern Eurasian components. While it has been found that \"at least third to half of their chromosomes are of East Asian origin, PCA places them between East Asia and Caucasus/Middle East/Europe clusters\". Genetic research suggests that the Hazaras of Afghanistan cluster closely with the Uzbek population of the country, while both groups are at a notable distance from Afghanistan's Tajik and Pashtun populations. There is evidence of both a patrimonial and maternal relation to Turkic Peoples and Mongols.\n\nMongol male and female ancestry is supported by studies in genetic genealogy as well, which have identified a particular lineage of the Y‑chromosome characteristic of people of Mongolian descent (\"the Y-chromosome of Genghis Khan\"). This chromosome is virtually absent outside the limits of the Mongol Empire except among the Hazara, where it reaches its highest frequency anywhere. These results indicate that the Hazara are also characterized by very high frequencies of eastern Eurasian mtDNAs at 35%, which are virtually absent from bordering populations, suggesting that the male descendants of Genghis Khan, or other Mongols, were accompanied by women of East Asian ancestry. Women of Non-eastern Eurasian mtDNA in Hazaras are at 65% most which are West Eurasians and some South Asian.\n\nThe most frequent paternal Haplogroup type found amongst the Pakistani Hazara was haplogroup C-M217 at 40%(10/25) with Haplogroup O3 (Y-DNA) at 8% (2/25) both which are Eastern Eurasian males ancestry associated with the Mongoloid ethnicity.\n\nIn one mtDNA study of Hazara, mtDNA Haplogroup L which is of African origin was detected at a frequency of 7.5% Also in one Y-DNA study of Hazara people, two haplogroups regarded as extreme outliers geographically have also been identified at low levels among the Hazara:\n\n\nThe vast majority of Hazaras live in central Afghanistan, and significant numbers are also found in major cities and towns. Many Hazara men leave Hazarjat to work in cities, including in neighboring countries or abroad. The latest World Factbook estimates show that Hazara make up nine percent of the total Afghan population but some sources claim that they are about 20 percent. However, they fail to cite a reference. In the 1970s, they were estimated by Louis Dupree at approximately 1,000,000.\n\nAlessandro Monsutti argues, in his recent anthropological book, that migration is the traditional way of life of the Hazara people, referring to the seasonal and historical migrations which have never ceased and do not seem to be dictated only by emergency situations such as war. Due to the decades of war in Afghanistan and the sectarian violence in Pakistan, many Hazaras left their communities and have settled in Australia, New Zealand, Canada, the United States, the United Kingdom and particularly the Northern European countries such as Sweden and Denmark. Some go to these countries as exchange students while others through human smuggling, which sometimes costs them their lives. Since 2001, about 1,000 people have died in the ocean while trying to reach Australia by boats from Indonesia. Many of these were Hazaras, including women and small children who could not swim. The notable case was the Tampa affair in which a shipload of refugees, mostly Hazara, was rescued by the Norwegian freighter MV \"Tampa\" and subsequently sent to Nauru. New Zealand agreed to take some of the refugees and all but one of those were granted stay.\n\nDuring the British expansion in the 19th century, Hazaras worked during the winter months in coal mines, road construction and in other menial labor jobs in some cities of what is now Pakistan. The earliest record of Hazara in the areas of Pakistan is found in Broadfoot's Sappers company from 1835 in Quetta. This company had also participated in the First Anglo-Afghan War. Some Hazara also worked in the agriculture farms in Sindh and construction of Sukkur barrage. Haider Ali Karmal Jaghori was a prominent political thinker of the Hazara people in Pakistan, writing about the political history of Hazara people. His work \"Hazaraha wa Hazarajat Bastan Dar Aiyna-i-Tarikh\" was published in Quetta in 1992, and another work by Aziz Tughyan Hazara \"Tarikh Milli Hazara\" was published in 1984 in Quetta.\n\nMost Pakistani Hazaras today live in the city of Quetta, in Balochistan, Pakistan. Localities in the city of Quetta with prominent Hazara populations include Hazara Town and Mehr Abad and Hazara tribes such as the \"Sardar\" are exclusively Pakistani. Literacy level among the Hazara community in Pakistan is relatively high compare to the Hazaras of Afghanistan, and they have integrated well into the social dynamics of the local society. Saira Batool, a Hazara woman, was one of the first female pilots in Pakistan Air Force. Other notable Hazara include Qazi Mohammad Esa, General Musa Khan Hazara, who served as Commander in Chief of the Pakistani Army from 1958 to 1968, Air Marshal Sharbat Ali Changezi, Hussain Ali Yousafi, the slain chairman of the Hazara Democratic Party, Syed Nasir Ali Shah, MNA from Quetta and his father Haji Sayed Hussain Hazara who was a senator and member of majlis shora during the Zia-ul-Haq era.\n\nDespite all of this, Hazaras are often targeted by militant groups such as the Lashkar-e-Jhangvi and others. \"Activists say at least 800-1,000 Hazaras have been killed since 1999 and the pace is quickening. More than one hundred have been murdered in and around Quetta since January, according to Human Rights Watch.\" The political representation of the community is served by Hazara Democratic Party, a secular liberal democratic party, headed by Abdul Khaliq Hazara.\n\nOver the many years as a result of political unrest in Afghanistan some Hazaras have migrated to Iran. The local Hazara population has been estimated at 500,000 people of which at least one third have spent more than half their life in Iran.\n\nThey have complained of discrimination in Iran. In March 2011, \"Eurasia Daily Monitor\" reported that representatives of Hazara community in Iran have asked Mongolia to intervene in supporting their case with Iranian government and prevent Iranian forced repatriation to Afghanistan.\n\nThe Hazara, outside of Hazarajat, have adopted the cultures of the cities where they dwell, and in many cases they have become Pashtunized and Persianized, resembling customs and traditions of the Afghan Tajiks and Pashtuns. Traditionally the Hazara are highland farmers and although sedentary, in the Hazarajat, they have retained many of their own customs and traditions, some of which are more closely related to those of Central Asia than to those of the Afghan Tajiks. For instance, many Hazara musicians are widely hailed as being skilled in playing the dambura, a regional and native instrument, a lute instrument similarly found in other Central Asian nations such as Tajikistan and Uzbekistan. The Hazara live in houses rather than tents; Aimaq people in tents rather than houses.\n\nHazara people living in Hazarajat (Hazaristan) areas speak the Hazaragi language of Afghanistan, which is infused with a significant number of Altaic loan words including Mongolic and Turkic. The primary differences between Dari and Hazaragi are the accent and Hazaragi's greater array of some Altaic loanwords. Despite these differences, Hazaragi is mutually intelligible with Dari, one of the official languages of Afghanistan.\n\nMany of the urban Hazara in the larger cities such as Kabul and Mazar-i-Sharif no longer speak Hazaragi but speak standard literary Dari (usually the \"Kābolī\" dialect) or other regional varieties of Dari (for example the \"Khorāsānī\" dialect in the western region of Herat).\n\nHazara are predominantly Shi'a Muslims, mostly of the Twelver sect and some Ismaili. Since the majority of Afghans practice Sunni Islam, this may have contributed to the discrimination against the Hazara. Hazara probably converted to Shi'ism during the first part of the 16th century, in the early days of the Safavid Dynasty. Nonetheless, a small number of Hazara are Sunni, such as the Aimaq Hazaras. Sunni Hazara have been attached to non-Hazara tribes (such as Taimuris), while the Ismaili Hazara have always been kept separate from the rest of the Hazara on account of religious beliefs and political purposes.\n\nThe Hazara people have been organized by various tribes. The Daizangi are the largest tribe, representing 57.2% of the Hazara population. However, more recently and since the inclusion of the Hazara into the \"Afghan state\", tribal affiliations have been disappearing and former tribal names Sheikh Ali, Jaghori, Ghaznichi, Muhammad Khwaja, Behsud, Uruzgani, Daikundi, Daizangi and Turkmani are also disappearing. There are smaller tribes such as Sarcheshmae, Kolokheshgi, Dai Mirdadi, Wazirgi that have remained a minority among the Hazara tribes. The different tribes come from regions such as Parwan, Bamiyan, Ghazni and Maydan-Shahr and have spread outwards from Hazarajat (Main City) into Kabul and other parts of Afghanistan.\nMany Hazara are engaged in different sports. Rohullah Nikpai won a bronze medal in Taekwondo in the Beijing Olympics 2008, beating world champion Juan Antonio Ramos of Spain 4–1 in a play-off final. It was Afghanistan's first-ever Olympic medal. He then won a second Olympic medal for Afghanistan in the London 2012 games. Afghanistan's first female Olympic athlete Friba Razayee competed in judo at the 2004 Athens Olympics, but was eliminated in the first round of competition.\n\nOther famous Hazara athletes are Syed Abdul Jalil Waiz (Badminton) and Ali Hazara (Football). Syed Abdul Jalil Waiz was the first ever badminton player representing Afghanistan in Asian Junior Championships in 2005 where he produced the first win for his country against Iraq, with 15–13, 15–1. He participated in several international championships since 2005 and achieved victories against Australia, Philippines and Mongolia. Hamid Rahimi is a new boxer from Afghanistan and lives in Germany. Zohib Islam Amiri the former captain of Afghanistan's national football team is also of Hazara descent.\n\nA Pakistani Hazara named Syed Abrar Hussain Shah, a former Olympic boxer served as deputy director general of the Pakistan Sports Board. Shah represented Pakistan three times at the Olympics and won a gold medal at the 1990 Asian Games in Beijing. Some Hazara from Pakistan have also excelled in sports and have received numerous awards particularly in boxing, football and in field hockey. Qayum Changezi, a legendary Pakistani football player, was a Hazara.\nNew Hazara youngsters are seen to appear in many sports in Pakistan including boys and girls from Hazara Town and Mehr Abad. Rajab Ali Hazara who is leading under 16 Pakistan Football team as captain and many other youngsters representing Hazaras in sports in Pakistan.\n\n\n\n"}
{"id": "14132", "url": "https://en.wikipedia.org/wiki?curid=14132", "title": "Hawala", "text": "Hawala\n\nHawala or hewala ( , meaning \"transfer\" or sometimes \"trust\"), also known as ' in Persian, or hundi ( ) in Hindi or—in Somali, ' or —is a popular and informal value transfer system based not on the movement of cash, or on telegraph or computer network wire transfers between banks, but instead on the performance and honour of a huge network of money brokers (known as \"hawaladars\"). While hawaladars are spread throughout the world, they are primarily located in the Middle East, North Africa, the Horn of Africa, and the Indian subcontinent, operating outside of, or parallel to, traditional banking, financial channels, and remittance systems. Hawala follows Islamic traditions but its use is not limited to Muslims.\n\nThe hawala system has existed since the 8th century between Arabic and Muslim traders alongside the Silk Road and beyond as a protection against theft. It is believed to have arisen in the financing of long-distance trade around the emerging capital trade centers in the early medieval period. In South Asia, it appears to have developed into a fully-fledged money market instrument, which was only gradually replaced by the instruments of the formal banking system in the first half of the 20th century.\n\n\"Hawala\" itself influenced the development of the agency in common law and in civil laws, such as the \"aval\" in French law and the \"avallo\" in Italian law. The words \"aval\" and \"avallo\" were themselves derived from \"hawala\". The transfer of debt, which was \"not permissible under Roman law but became widely practiced in medieval Europe, especially in commercial transactions\", was due to the large extent of the \"trade conducted by the Italian cities with the Muslim world in the Middle Ages\". The agency was also \"an institution unknown to Roman law\" as no \"individual could conclude a binding contract on behalf of another as his agent\". In Roman law, the \"contractor himself was considered the party to the contract and it took a second contract between the person who acted on behalf of a principal and the latter in order to transfer the rights and the obligations deriving from the contract to him\". On the other hand, Islamic law and the later common law \"had no difficulty in accepting agency as one of its institutions in the field of contracts and of obligations in general\".\n\nToday, hawala is probably used mostly for migrant workers' remittances to their countries of origin.\n\nIn the most basic variant of the hawala system, money is transferred via a network of hawala brokers, or \"hawaladars\". It is the transfer of money without actually moving it. In fact, a successful definition of the hawala system that is used is \"money transfer without money movement\". According to author Sam Vaknin, while there are large hawaladar operators with networks of middlemen in cities across many countries, most hawaladars are small businesses who work at hawala as a sideline or moonlighting operation.\nThe figure shows how hawala works: (1) a customer (\"A\", left-hand side) approaches a hawala broker (\"X\") in one city and gives a sum of money (red arrow) that is to be transferred to a recipient (\"B\", right-hand side) in another, usually foreign, city. Along with the money, he usually specifies something like a password that will lead to the money being paid out (blue arrows). (2b) The hawa calls another hawala broker \"M\" in the recipient's city, and informs \"M\" about the agreed password, or gives other disposition of the funds. Then, the intended recipient (\"B\"), who also has been informed by \"A\" about the password (2a), now approaches \"M\" and tells him the agreed password (3a). If the password is correct, then \"M\" releases the transferred sum to \"B\" (3b), usually minus a small commission. \"X\" now basically owes \"M\" the money that \"M\" had paid out to \"B\"; thus \"M\" has to trust \"X\"s promise to settle the debt at a later date.\n\nThe unique feature of the system is that no promissory instruments are exchanged between the hawala brokers; the transaction takes place entirely on the honour system. As the system does not depend on the legal enforceability of claims, it can operate even in the absence of a legal and juridical environment. Trust and extensive use of connections are the components that distinguish it from other remittance systems. Hawaladars networks are often based on membership in the same family, village, clan, or ethnic group, and cheating is punished by effective ex-communication and \"loss of honour\"—leading to severe economic hardship.\n\nInformal records are produced of individual transactions, and a running tally of the amount owed by one broker to another is kept. Settlements of debts between hawala brokers can take a variety of forms (such as goods, services, properties, transfers of employees, etc.), and need not take the form of direct cash transactions.\n\nIn addition to commissions, hawala brokers often earn their profits through bypassing official exchange rates. Generally, the funds enter the system in the source country's currency and leave the system in the recipient country's currency. As settlements often take place without any foreign exchange transactions, they can be made at other than official exchange rates.\n\nHawala is attractive to customers because it provides a fast and convenient transfer of funds, usually with a far lower commission than that charged by banks. Its advantages are most pronounced when the receiving country applies unprofitable exchange rate regulations or when the banking system in the receiving country is less complex (e.g., due to differences in legal environment in places such as Afghanistan, Yemen, Somalia). Moreover, in some parts of the world it is the only option for legitimate fund transfers, and has even been used by aid organizations in areas where it is the best-functioning institution.\n\nDubai has been prominent for decades as a welcoming hub for hawala transactions worldwide.\n\nThe \"hundi\" is a financial instrument that developed on the Indian sub-continent for use in trade and credit transactions. Hundis are used as a form of remittance instrument to transfer money from place to place, as a form of credit instrument or IOU to borrow money and as a bill of exchange in trade transactions. The Reserve Bank of India describes the Hundi as \"an unconditional order in writing made by a person directing another to pay a certain sum of money to a person named in the order.\"\n\nThe word \"angadia\" means courier in Hindi, but also designates those who act as hawaladars within India. These people mostly act as a parallel banking system for businessmen. They charge a commission of around 0.2–0.5% per transaction from transferring money from one city to another.\n\nAccording to the CIA, with the dissolution of Somalia's formal banking system, many informal money transfer operators arose to fill the void. It estimates that such \"hawaladars\", \"xawilaad\" or \"xawala\" brokers are now responsible for the transfer of up to $1.6 billion per year in remittances to the country, most coming from working Somalis outside Somalia. Such funds have in turn had a stimulating effect on local business activity.\n\nThe 2012 Tuareg rebellion left Northern Mali without an official money transfer service for months. The coping mechanisms that appeared were patterned on the hawala system.\n\nSome government officials assert that hawala can be used to facilitate money laundering, avoid taxation, and move wealth anonymously. As a result, it is illegal in some U.S. states, India, Pakistan, and some other countries.\n\nAfter the September 11 terrorist attacks, the American government suspected that some hawala brokers may have helped terrorist organizations transfer money to fund their activities, and the 9/11 Commission Report stated that \"Al Qaeda frequently moved the money it raised by hawala\". As a result of intense pressure from the U.S. authorities to introduce systematic anti-money laundering initiatives on a global scale, a number of hawala networks were closed down and a number of hawaladars were successfully prosecuted for money laundering. However, there is little evidence that these actions brought the authorities any closer to identifying and arresting a significant number of terrorists or drug smugglers. Experts emphasized that the overwhelming majority of those who used these informal networks were doing so for legitimate purposes, and simply chose to use a transaction medium other than state-supported banking systems. Today, the hawala system in Afghanistan is instrumental in providing financial services for the delivery of emergency relief and humanitarian and developmental aid for the majority of international and domestic NGOs, donor organizations, and development aid agencies.\n\nIn November 2001, the Bush administration froze the assets of Al-Barakat, a Somali remittance hawala company used primarily by a large number of Somali immigrants. Many of its agents in several countries were initially arrested, though later freed after no concrete evidence against them was found. In August 2006 the last Al-Barakat representatives were taken off the U.S. terror list, though some assets remain frozen. The mass media has speculated that pirates from Somalia use the hawala system to move funds internationally, for example into neighboring Kenya, where these transactions are neither taxed nor recorded.\n\nIn January 2010, the Kabul office of New Ansari Exchange, Afghanistan's largest hawala money transfer business, was closed following a raid by the Sensitive Investigative Unit, the country's national anti-political corruption unit, allegedly because this company was involved in laundering profits from the illicit opium trade and the moving of cash earned by government allied warlords through extortion and drug trafficking. Thousands of records were seized, from which links were found between money transfers by this company and political and business figures and NGOs in the country, including relatives of President Hamid Karzai. In August 2010, Karzai took control of the task force that staged the raid, and the US-advised anti-corruption group, the Major Crimes Task Force. He ordered a commission to review scores of past and current anti-corruption inquests.\n\n\n"}
{"id": "14133", "url": "https://en.wikipedia.org/wiki?curid=14133", "title": "Hydroponics", "text": "Hydroponics\n\nHydroponics is a subset of hydroculture, which is a method of growing plants without soil by using mineral nutrient solutions in a water solvent. Terrestrial plants may be grown with only their roots exposed to the mineral solution, or the roots may be supported by an inert medium, such as perlite or gravel.\n\nThe nutrients used in hydroponic systems can come from an array of different sources; these can include, but are not limited to, byproduct from fish waste, duck manure, or purchased chemical fertilisers.\n\nThe earliest published work on growing terrestrial plants without soil was the 1627 book \"Sylva Sylvarum\" or 'A Natural History' by Francis Bacon, printed a year after his death. Water culture became a popular research technique after that. In 1699, John Woodward published his water culture experiments with spearmint. He found that plants in less-pure water sources grew better than plants in distilled water. By 1842, a list of nine elements believed to be essential for plant growth had been compiled, and the discoveries of German botanists Julius von Sachs and Wilhelm Knop, in the years 1859–1875, resulted in a development of the technique of soilless cultivation. Growth of terrestrial plants without soil in mineral nutrient solutions was called solution culture. It quickly became a standard research and teaching technique and is still widely used. Solution culture is, now considered, a type of hydroponics where there is no inert medium.\n\nIn 1929, William Frederick Gericke of the University of California at Berkeley began publicly promoting that solution culture be used for agricultural crop production. He first termed it aquaculture but later found that aquaculture was already applied to culture of aquatic organisms. Gericke created a sensation by growing tomato vines high in his back yard in mineral nutrient solutions rather than soil. He introduced the term hydroponics, water culture, in 1937, proposed to him by , a phycologist with an extensive education in the classics. Hydroponics is derived from neologism υδρωπονικά (derived from Greek ύδωρ=water and πονέω=cultivate), constructed in analogy to γεωπονικά (derived from Greek γαία=earth and πονέω=cultivate), geoponica, that which concerns agriculture, replacing, γεω-, earth, with ὑδρο-, water.\n\nReports of Gericke's work and his claims that hydroponics would revolutionize plant agriculture prompted a huge number of requests for further information. Gericke had been denied use of the University's greenhouses for his experiments due to the administration's skepticism, and when the University tried to compel him to release his preliminary nutrient recipes developed at home he requested greenhouse space and time to improve them using appropriate research facilities. While he was eventually provided greenhouse space, the University assigned Hoagland and Arnon to re-develop Gericke's formula and show it held no benefit over soil grown plant yields, a view held by Hoagland. In 1940, Gericke published the book, \"Complete Guide to Soil less Gardening,\" after leaving his academic position in a climate that was politically unfavorable.\n\nTwo other plant nutritionists, Dennis R. Hoagland and Daniel I. Arnon, at the University of California were asked to research Gericke's claims. The two wrote a classic 1938 agricultural bulletin, \"The Water Culture Method for Growing Plants Without Soil,\" which made the claim that hydroponic crop yields were no better than crop yields with good-quality soils. Crop yields were ultimately limited by factors other than mineral nutrients, especially light. This research, however, overlooked the fact that hydroponics has other advantages including the fact that the roots of the plant have constant access to oxygen and that the plants have access to as much or as little water as they need. This is important as one of the most common errors when growing is over- and under- watering; and hydroponics prevents this from occurring as large amounts of water can be made available to the plant and any water not used, drained away, recirculated, or actively aerated, eliminating anoxic conditions, which drown root systems in soil. In soil, a grower needs to be very experienced to know exactly how much water to feed the plant. Too much and the plant will be unable to access oxygen; too little and the plant will lose the ability to transport nutrients, which are typically moved into the roots while in solution. These two researchers developed several formulas for mineral nutrient solutions, known as Hoagland solution. Modified Hoagland solutions are still in use.\n\nOne of the earliest successes of hydroponics occurred on Wake Island, a rocky atoll in the Pacific Ocean used as a refueling stop for Pan American Airlines. Hydroponics was used there in the 1930s to grow vegetables for the passengers. Hydroponics was a necessity on Wake Island because there was no soil, and it was prohibitively expensive to airlift in fresh vegetables.\n\nIn the 1960s, Allen Cooper of England developed the Nutrient film technique. The Land Pavilion at Walt Disney World's EPCOT Center opened in 1982 and prominently features a variety of hydroponic techniques.\n\nIn recent decades, NASA has done extensive hydroponic research for its Controlled Ecological Life Support System (CELSS). Hydroponics research mimicking a Martian environment uses LED lighting to grow in a different color spectrum with much less heat. Ray Wheeler, a plant physiologist at Kennedy Space Center’s Space Life Science Lab, believes that hydroponics will create advances within space travel, as a bioregenerative life support system.\n\nIn 2007, Eurofresh Farms in Willcox, Arizona, sold more than 200 million pounds of hydroponically grown tomatoes. Eurofresh has under glass and represents about a third of the commercial hydroponic greenhouse area in the U.S. Eurofresh tomatoes were pesticide-free, grown in rockwool with top irrigation. Eurofresh declared bankruptcy, and the greenhouses were acquired by NatureSweet Ltd. in 2013.\n\nAs of 2017, Canada had hundreds of acres of large-scale commercial hydroponic greenhouses, producing tomatoes, peppers and cucumbers.\n\nDue to technological advancements within the industry and numerous economic factors, the global hydroponics market is forecast to grow from $226.45 million USD in 2016 to $724.87 million USD by 2023.\n\nThere are two main variations for each medium, sub-irrigation and top irrigation. For all techniques, most hydroponic reservoirs are now built of plastic, but other materials have been used including concrete, glass, metal, vegetable solids, and wood. The containers should exclude light to prevent algae and fungal growth in the nutrient solution.\n\nIn static solution culture, plants are grown in containers of nutrient solution, such as glass Mason jars (typically, in-home applications), plastic buckets, tubs, or tanks. The solution is usually gently aerated but may be un-aerated. If un-aerated, the solution level is kept low enough that enough roots are above the solution so they get adequate oxygen. A hole is cut in the lid of the reservoir for each plant. A single reservoir can be dedicated to a single plant, or to various plants. Reservoir size can be increased as plant size increases. A home made system can be constructed from plastic food containers or glass canning jars with aeration provided by an aquarium pump, aquarium airline tubing and aquarium valves. Clear containers are covered with aluminium foil, butcher paper, black plastic, or other material to exclude light, thus helping to eliminate the formation of algae. The nutrient solution is changed either on a schedule, such as once per week, or when the concentration drops below a certain level as determined with an electrical conductivity meter. Whenever the solution is depleted below a certain level, either water or fresh nutrient solution is added. A Mariotte's bottle, or a float valve, can be used to automatically maintain the solution level. In raft solution culture, plants are placed in a sheet of buoyant plastic that is floated on the surface of the nutrient solution. That way, the solution level never drops below the roots.\n\nIn continuous-flow solution culture, the nutrient solution constantly flows past the roots. It is much easier to automate than the static solution culture because sampling and adjustments to the temperature and nutrient concentrations can be made in a large storage tank that has potential to serve thousands of plants. A popular variation is the nutrient film technique or NFT, whereby a very shallow stream of water containing all the dissolved nutrients required for plant growth is recirculated past the bare roots of plants in a watertight thick root mat, which develops in the bottom of the channel and has an upper surface that, although moist, is in the air. Subsequent to this, an abundant supply of oxygen is provided to the roots of the plants. A properly designed NFT system is based on using the right channel slope, the right flow rate, and the right channel length. The main advantage of the NFT system over other forms of hydroponics is that the plant roots are exposed to adequate supplies of water, oxygen, and nutrients. In all other forms of production, there is a conflict between the supply of these requirements, since excessive or deficient amounts of one results in an imbalance of one or both of the others. NFT, because of its design, provides a system where all three requirements for healthy plant growth can be met at the same time, provided that the simple concept of NFT is always remembered and practised. The result of these advantages is that higher yields of high-quality produce are obtained over an extended period of cropping. A downside of NFT is that it has very little buffering against interruptions in the flow (e.g., power outages). But, overall, it is probably one of the more productive techniques.\n\nThe same design characteristics apply to all conventional NFT systems. While slopes along channels of 1:100 have been recommended, in practice it is difficult to build a base for channels that is sufficiently true to enable nutrient films to flow without ponding in locally depressed areas. As a consequence, it is recommended that slopes of 1:30 to 1:40 are used. This allows for minor irregularities in the surface, but, even with these slopes, ponding and water logging may occur. The slope may be provided by the floor, benches or racks may hold the channels and provide the required slope. Both methods are used and depend on local requirements, often determined by the site and crop requirements.\n\nAs a general guide, flow rates for each gully should be one liter per minute. At planting, rates may be half this and the upper limit of 2 L/min appears about the maximum. Flow rates beyond these extremes are often associated with nutritional problems. Depressed growth rates of many crops have been observed when channels exceed 12 meters in length. On rapidly growing crops, tests have indicated that, while oxygen levels remain adequate, nitrogen may be depleted over the length of the gully. As a consequence, channel length should not exceed 10–15 meters. In situations where this is not possible, the reductions in growth can be eliminated by placing another nutrient feed halfway along the gully and halving the flow rates through each outlet.\n\nAeroponics is a system wherein roots are continuously or discontinuously kept in an environment saturated with fine drops (a mist or aerosol) of nutrient solution. The method requires no substrate and entails growing plants with their roots suspended in a deep air or growth chamber with the roots periodically wetted with a fine mist of atomized nutrients. Excellent aeration is the main advantage of aeroponics.\n\nAeroponic techniques have proven to be commercially successful for propagation, seed germination, seed potato production, tomato production, leaf crops, and micro-greens. Since inventor Richard Stoner commercialized aeroponic technology in 1983, aeroponics has been implemented as an alternative to water intensive hydroponic systems worldwide. The limitation of hydroponics is the fact that of water can only hold of air, no matter whether aerators are utilized or not.\n\nAnother distinct advantage of aeroponics over hydroponics is that any species of plants can be grown in a true aeroponic system because the microenvironment of an aeroponic can be finely controlled. The limitation of hydroponics is that certain species of plants can only survive for so long in water before they become waterlogged. The advantage of aeroponics is that suspended aeroponic plants receive 100% of the available oxygen and carbon dioxide to the roots zone, stems, and leaves, thus accelerating biomass growth and reducing rooting times. NASA research has shown that aeroponically grown plants have an 80% increase in dry weight biomass (essential minerals) compared to hydroponically grown plants. Aeroponics used 65% less water than hydroponics. NASA also concluded that aeroponically grown plants require ¼ the nutrient input compared to hydroponics. Unlike hydroponically grown plants, aeroponically grown plants will not suffer transplant shock when transplanted to soil, and offers growers the ability to reduce the spread of disease and pathogens.\nAeroponics is also widely used in laboratory studies of plant physiology and plant pathology. Aeroponic techniques have been given special attention from NASA since a mist is easier to handle than a liquid in a zero-gravity environment.\n\nFogponics is a derivation of aeroponics wherein the nutrient solution is aerosolized by a diaphragm vibrating at ultrasonic frequencies. Solution droplets produced by this method tend to be 5–10 µm in diameter, smaller than those produced by forcing a nutrient solution through pressurized nozzles, as in aeroponics. The smaller size of the droplets allows them to diffuse through the air more easily, and deliver nutrients to the roots without limiting their access to oxygen.\n\nPassive sub-irrigation, also known as passive hydroponics, semi-hydroponics, or \"hydroculture\", is a method wherein plants are grown in an inert porous medium that transports water and fertilizer to the roots by capillary action from a separate reservoir as necessary, reducing labor and providing a constant supply of water to the roots. In the simplest method, the pot sits in a shallow solution of fertilizer and water or on a capillary mat saturated with nutrient solution. The various hydroponic media available, such as expanded clay and coconut husk, contain more air space than more traditional potting mixes, delivering increased oxygen to the roots, which is important in epiphytic plants such as orchids and bromeliads, whose roots are exposed to the air in nature. Additional advantages of passive hydroponics are the reduction of root rot and the additional ambient humidity provided through evaporations.\n\nHydroculture compared to traditional farming in terms of crops yield per area in a controlled environment was roughly 10 times more efficient than traditional farming, uses 13 times less water in one crop cycle than traditional farming, but on average uses 100 times more kilojoules per kilogram of energy than traditional farming.\n\nIn its simplest form, there is a tray above a reservoir of nutrient solution. Either the tray is filled with growing medium (clay granules being the most common) and then plant directly or place the pot over medium, stand in the tray. At regular intervals, a simple timer causes a pump to fill the upper tray with nutrient solution, after which the solution drains back down into the reservoir. This keeps the medium regularly flushed with nutrients and air. Once the upper tray fills past the drain stop, it begins recirculating the water until the timer turns the pump off, and the water in the upper tray drains back into the reservoirs.\n\nIn a run-to-waste system, nutrient and water solution is periodically applied to the medium surface. The method was invented in Bengal in 1946; for this reason it is sometimes referred to as \"The Bengal System\".\n\nThis method can be set up in various configurations. In its simplest form, a nutrient-and-water solution is manually applied one or more times per day to a container of inert growing media, such as rockwool, perlite, vermiculite, coco fibre, or sand. In a slightly more complex system, it is automated with a delivery pump, a timer and irrigation tubing to deliver nutrient solution with a delivery frequency that is governed by the key parameters of plant size, plant growing stage, climate, substrate, and substrate conductivity, pH, and water content.\n\nIn a commercial setting, watering frequency is multi-factorial and governed by computers or PLCs.\n\nCommercial hydroponics production of large plants like tomatoes, cucumber, and peppers uses one form or another of run-to-waste hydroponics.\n\nIn environmentally responsible uses, the nutrient-rich waste is collected and processed through an on-site filtration system to be used many times, making the system very productive.\n\nSome bonsai are also grown in soil-free substrates (typically consisting of akadama, grit, diatomaceous earth and other inorganic components) and have their water and nutrients provided in a run-to-waste form.\n\nThe hydroponic method of plant production by means of suspending the plant roots in a solution of nutrient-rich, oxygenated water. Traditional methods favor the use of plastic buckets and large containers with the plant contained in a net pot suspended from the centre of the lid and the roots suspended in the nutrient solution.\nThe solution is oxygen saturated by an air pump combined with porous stones. With this method, the plants grow much faster because of the high amount of oxygen that the roots receive.\n\n\"Top-fed\" deep water culture is a technique involving delivering highly oxygenated nutrient solution direct to the root zone of plants. While deep water culture involves the plant roots hanging down into a reservoir of nutrient solution, in top-fed deep water culture the solution is pumped from the reservoir up to the roots (top feeding). The water is released over the plant's roots and then runs back into the reservoir below in a constantly recirculating system. As with deep water culture, there is an airstone in the reservoir that pumps air into the water via a hose from outside the reservoir. The airstone helps add oxygen to the water. Both the airstone and the water pump run 24 hours a day.\n\nThe biggest advantage of top-fed deep water culture over standard deep water culture is increased growth during the first few weeks. With deep water culture, there is a time when the roots have not reached the water yet. With top-fed deep water culture, the roots get easy access to water from the beginning and will grow to the reservoir below much more quickly than with a deep water culture system. Once the roots have reached the reservoir below, there is not a huge advantage with top-fed deep water culture over standard deep water culture. However, due to the quicker growth in the beginning, grow time can be reduced by a few weeks.\n\nA rotary hydroponic garden is a style of commercial hydroponics created within a circular frame which rotates continuously during the entire growth cycle of whatever plant is being grown.\n\nWhile system specifics vary, systems typically rotate once per hour, giving a plant 24 full turns within the circle each 24-hour period. Within the center of each rotary hydroponic garden can be a high intensity grow light, designed to simulate sunlight, often with the assistance of a mechanized timer.\n\nEach day, as the plants rotate, they are periodically watered with a hydroponic growth solution to provide all nutrients necessary for robust growth. Due to the plants continuous fight against gravity, plants typically mature much more quickly than when grown in soil or other traditional hydroponic growing systems. Due to the small foot print a rotary hydroponic system has, it allows for more plant material to be grown per square foot of floor space than other traditional hydroponic systems.\n\nOne of the most obvious decisions hydroponic farmers have to make is which medium they should use. Different media are appropriate for different growing techniques.\n\nBaked clay pellets are suitable for hydroponic systems in which all nutrients are carefully controlled in water solution. The clay pellets are inert, pH neutral and do not contain any nutrient value.\n\nThe clay is formed into round pellets and fired in rotary kilns at . This causes the clay to expand, like popcorn, and become porous. It is light in weight, and does not compact over time. The shape of an individual pellet can be irregular or uniform depending on brand and manufacturing process. The manufacturers consider expanded clay to be an ecologically sustainable and re-usable growing medium because of its ability to be cleaned and sterilized, typically by washing in solutions of white vinegar, chlorine bleach, or hydrogen peroxide (), and rinsing completely.\n\nAnother view is that clay pebbles are best not re-used even when they are cleaned, due to root growth that may enter the medium. Breaking open a clay pebble after a crop has been shown to reveal this growth.\n\nGrowstones, made from glass waste, have both more air and water retention space than perlite and peat. This aggregate holds more water than parboiled rice hulls. Growstones by volume consist of 0.5 to 5% calcium carbonate – for a standard 5.1 kg bag of Growstones that corresponds to 25.8 to 258 grams of calcium carbonate. The remainder is soda-lime glass.\n\nCoco peat, also known as coir or coco, is the leftover material after the fibres have been removed from the outermost shell (bolster) of the coconut. Coir is a 100% natural grow and flowering medium. Coconut coir is colonized with trichoderma fungi, which protects roots and stimulates root growth. It is extremely difficult to over-water coir due to its perfect air-to-water ratio; plant roots thrive in this environment. Coir has a high cation exchange, meaning it can store unused minerals to be released to the plant as and when it requires it. Coir is available in many forms; most common is coco peat, which has the appearance and texture of soil but contains no mineral content.\n\nParboiled rice husks (PBH) are an agricultural byproduct that would otherwise have little use. They decay over time, and allow drainage, and even retain less water than growstones. A study showed that rice husks did not affect the effects of plant growth regulators.\n\nPerlite is a volcanic rock that has been superheated into very lightweight expanded glass pebbles. It is used loose or in plastic sleeves immersed in the water. It is also used in potting soil mixes to decrease soil density. Perlite has similar properties and uses to vermiculite but, in general, holds more air and less water and is buoyant.\n\nLike perlite, vermiculite is a mineral that has been superheated until it has expanded into light pebbles. Vermiculite holds more water than perlite and has a natural \"wicking\" property that can draw water and nutrients in a passive hydroponic system. If too much water and not enough air surrounds the plants roots, it is possible to gradually lower the medium's water-retention capability by mixing in increasing quantities of perlite.\n\nLike perlite, pumice is a lightweight, mined volcanic rock that finds application in hydroponics.\n\nSand is cheap and easily available. However, it is heavy, does not hold water very well, and it must be sterilized between uses.\n\nThe same type that is used in aquariums, though any small gravel can be used, provided it is washed first. Indeed, plants growing in a typical traditional gravel filter bed, with water circulated using electric powerhead pumps, are in effect being grown using gravel hydroponics. Gravel is inexpensive, easy to keep clean, drains well and will not become waterlogged. However, it is also heavy, and, if the system does not provide continuous water, the plant roots may dry out.\n\nWood fibre, produced from steam friction of wood, is a very efficient organic substrate for hydroponics. It has the advantage that it keeps its structure for a very long time. Wood wool (i.e. wood slivers) have been used since the earliest days of the hydroponics research. However, more recent research suggests that wood fibre may have detrimental effects on \"plant growth regulators\".\n\nWool from shearing sheep is a little-used yet promising renewable growing medium. In a study comparing wool with peat slabs, coconut fibre slabs, perlite and rockwool slabs to grow cucumber plants, sheep wool had a greater air capacity of 70%, which decreased with use to a comparable 43%, and water capacity that increased from 23% to 44% with use. Using sheep wool resulted in the greatest yield out of the tested substrates, while application of a biostimulator consisting of humic acid, lactic acid and Bacillus subtilis improved yields in all substrates.\n\nRock wool (mineral wool) is the most widely used medium in hydroponics. Rock wool is an inert substrate suitable for both run-to-waste and recirculating systems. Rock wool is made from molten rock, basalt or 'slag' that is spun into bundles of single filament fibres, and bonded into a medium capable of capillary action, and is, in effect, protected from most common microbiological degradation. Rock wool is typically used only for the seedling stage, or with newly cut clones, but can remain with the plant base for its lifetime. Rock wool has many advantages and some disadvantages. The latter being the possible skin irritancy (mechanical) whilst handling (1:1000). Flushing with cold water usually brings relief. Advantages include its proven efficiency and effectiveness as a commercial hydroponic substrate. Most of the rock wool sold to date is a non-hazardous, non-carcinogenic material, falling under Note Q of the European Union Classification Packaging and Labeling Regulation (CLP).\n\nMineral wool products can be engineered to hold large quantities of water and air that aid root growth and nutrient uptake in hydroponics; their fibrous nature also provides a good mechanical structure to hold the plant stable. The naturally high pH of mineral wool makes them initially unsuitable to plant growth and requires \"conditioning\" to produce a wool with an appropriate, stable pH.\n\nBrick shards have similar properties to gravel. They have the added disadvantages of possibly altering the pH and requiring extra cleaning before reuse.\n\nPolystyrene packing peanuts are inexpensive, readily available, and have excellent drainage. However, they can be too lightweight for some uses. They are used mainly in closed-tube systems. Note that non-biodegradable polystyrene peanuts must be used; biodegradable packing peanuts will decompose into a sludge. Plants may absorb styrene and pass it to their consumers; this is a possible health risk.\n\nThe formulation of hydroponic solutions is an application of plant nutrition, with nutrient deficiency symptoms mirroring those found in traditional soil based agriculture. However, the underlying chemistry of hydroponic solutions can differ from soil chemistry in many significant ways. Important differences include:\n\nAs in conventional agriculture, nutrients should be adjusted to satisfy Liebig's law of the minimum for each specific plant variety. Nevertheless, generally acceptable concentrations for nutrient solutions exist, with minimum and maximum concentration ranges for most plants being somewhat similar. Most nutrient solutions are mixed to have concentrations between 1,000 and 2,500 ppm. Acceptable concentrations for the individual nutrient ions, which comprise that total ppm figure, are summarized in the following table. For essential nutrients, concentrations below these ranges often lead to nutrient deficiencies while exceeding these ranges can lead to nutrient toxicity. Optimum nutrition concentrations for plant varieties are found empirically by experience and/or by plant tissue tests.\n\nOrganic fertilizers can be used to supplement or entirely replace the inorganic compounds used in conventional hydroponic solutions. However, using organic fertilizers introduces a number of challenges that are not easily resolved. Examples include:\n\nNevertheless, if precautions are taken, organic fertilizers can be used successfully in hydroponics.\n\nExamples of suitable materials, with their average nutritional contents tabulated in terms of percent dried mass, are listed in the following table.\nMicronutrients can be sourced from organic fertilizers as well. For example, composted pine bark is high in manganese and is sometimes used to fulfill that mineral requirement in hydroponic solutions. To satisfy requirements for National Organic Programs, pulverized, unrefined minerals (e.g. Gypsum, Calcite, and glauconite) can also be added to satisfy a plant's nutritional needs.\n\nIn addition to chelating agents, humic acids can be added to increase nutrient uptake.\n\nManaging nutrient concentrations and pH values within acceptable ranges is essential for successful hydroponic horticulture. Common tools used to manage hydroponic solutions include:\n\nChemical equipment can also be used to perform accurate chemical analyses of nutrient solutions. Examples include:\n\nUsing chemical equipment for hydroponic solutions can be beneficial to growers of any background because nutrient solutions are often reusable. Because nutrient solutions are virtually never completely depleted, and should never be due to the unacceptably low osmotic pressure that would result, re-fortification of old solutions with new nutrients can save growers money and can control point source pollution, a common source for the eutrophication of nearby lakes and streams.\n\nAlthough pre-mixed concentrated nutrient solutions are generally purchased from commercial nutrient manufacturers by hydroponic hobbyists and small commercial growers, several tools exist to help anyone prepare their own solutions without extensive knowledge about chemistry. The free and open source tools HydroBuddy and HydroCal have been created by professional chemists to help any hydroponics grower prepare their own nutrient solutions. The first program is available for Windows, Mac and Linux while the second one can be used through a simple JavaScript interface. Both programs allow for basic nutrient solution preparation although HydroBuddy provides added functionality to use and save custom substances, save formulations and predict electrical conductivity values. There is also a small robotic helper available called Eddy, created by the Flux Farm. It can send various information such as pH level, temperature, relative humidity, and contamination directly to its designed smartphone app and can even propose some simple solutions to any detected problems.\n\nOften mixing hydroponic solutions using individual salts is impractical for hobbyists and/or small-scale commercial growers because commercial products are available at reasonable prices. However, even when buying commercial products, multi-component fertilizers are popular. Often these products are bought as three part formulas which emphasize certain nutritional roles. For example, solutions for vegetative growth (i.e. high in nitrogen), flowering (i.e. high in potassium and phosphorus), and micronutrient solutions (i.e. with trace minerals) are popular. The timing and application of these multi-part fertilizers should coincide with a plant's growth stage. For example, at the end of an annual plant's life cycle, a plant should be restricted from high nitrogen fertilizers. In most plants, nitrogen restriction inhibits vegetative growth and helps induce flowering.\n\nWith pest problems reduced and nutrients constantly fed to the roots, productivity in hydroponics is high; however, growers can further increase yield by manipulating a plant's environment by constructing sophisticated growrooms.\n\nTo increase yield further, some sealed greenhouses inject CO into their environment to help improve growth and plant fertility.\n\nIn Frank Tayell's zombie apocalypse series, Surviving the Evacuation, Nilda's son Jay leads efforts to grow food via hydroponics at the Tower of London, as does the botany professor at a stadium in Creil, France.\n\nHydroponic farming is depicted in the CW's post-apocalyptic science fiction show The 100. The \"HydroFarm\" as they refer to it grows food for people living in a bunker.\n\n"}
{"id": "14134", "url": "https://en.wikipedia.org/wiki?curid=14134", "title": "Humanist (disambiguation)", "text": "Humanist (disambiguation)\n\nHumanist may refer to:\n\n\n"}
{"id": "14135", "url": "https://en.wikipedia.org/wiki?curid=14135", "title": "Henry Purcell", "text": "Henry Purcell\n\nHenry Purcell ( or ; September 1659 – 21 November 1695) was an English composer. Although incorporating Italian and French stylistic elements into his compositions, Purcell's legacy was a uniquely English form of Baroque music. He is generally considered to be one of the greatest English composers; no later native-born English composer approached his fame until Edward Elgar, Ralph Vaughan Williams, William Walton and Benjamin Britten in the 20th century.\n\nPurcell was born in St Ann's Lane, Old Pye Street, Westminster – the area of London later known as Devil's Acre – in 1659. Henry Purcell Senior, whose older brother, Thomas Purcell, (died 1682) was a musician, was a gentleman of the Chapel Royal and sang at the coronation of King Charles II of England. Henry the elder had three sons: Edward, Henry and Daniel. Daniel Purcell, the youngest of the brothers, was also a prolific composer who wrote the music for much of the final act of \"The Indian Queen\" after Henry Purcell's death. Henry Purcell's family lived just a few hundred yards west of Westminster Abbey from 1659 onwards.\n\nAfter his father's death in 1664, Purcell was placed under the guardianship of his uncle Thomas, who showed him great affection and kindness. Thomas was himself a gentleman of His Majesty's Chapel, and arranged for Henry to be admitted as a chorister. Henry studied first under Captain Henry Cooke, Master of the Children, and afterwards under Pelham Humfrey, Cooke's successor. The composer Matthew Locke was a family friend and, particularly with his semi-operas, probably also had a musical influence on the young Purcell. Henry was a chorister in the Chapel Royal until his voice broke in 1673, when he became assistant to the organ-builder John Hingston, who held the post of keeper of wind instruments to the King.\n\nPurcell is said to have been composing at nine years old, but the earliest work that can be certainly identified as his is an ode for the King's birthday, written in 1670. (The dates for his compositions are often uncertain, despite considerable research.) It is assumed that the three-part song \"Sweet tyranness, I now resign\" was written by him as a child. After Humfrey's death, Purcell continued his studies under Dr John Blow. He attended Westminster School and in 1676 was appointed copyist at Westminster Abbey. Henry Purcell's earliest anthem \"Lord, who can tell\" was composed in 1678. It is a psalm that is prescribed for Christmas Day and also to be read at morning prayer on the fourth day of the month.\n\nIn 1679, he wrote songs for John Playford's \"Choice Ayres, Songs and Dialogues\" and an anthem, the name of which is unknown, for the Chapel Royal. From an extant letter written by Thomas Purcell we learn that this anthem was composed for the exceptionally fine voice of the Rev. John Gostling, then at Canterbury, but afterwards a gentleman of His Majesty's Chapel. Purcell wrote several anthems at different times for Gostling's extraordinary basso profondo voice, which is known to have had a range of at least two full octaves, from D below the bass staff to the D above it. The dates of very few of these sacred compositions are known; perhaps the most notable example is the anthem \"They that go down to the sea in ships.\" In gratitude for the providential escape of King Charles II from shipwreck, Gostling, who had been of the royal party, put together some verses from the Psalms in the form of an anthem and requested Purcell to set them to music. The challenging work opens with a passage which traverses the full extent of Gostling's range, beginning on the upper D and descending two octaves to the lower.\n\nIn 1679, Blow, who had been appointed organist of Westminster Abbey 10 years before, resigned his office in favour of Purcell. Purcell now devoted himself almost entirely to the composition of sacred music, and for six years severed his connection with the theatre. However, during the early part of the year, probably before taking up his new office, he had produced two important works for the stage, the music for Nathaniel Lee's \"Theodosius\", and Thomas d'Urfey's \"Virtuous Wife\". Between 1680 and 1688 Purcell wrote music for seven plays. The composition of his chamber opera \"Dido and Aeneas\", which forms a very important landmark in the history of English dramatic music, has been attributed to this period, and its earliest production may well have predated the documented one of 1689. It was written to a libretto furnished by Nahum Tate, and performed in 1689 in cooperation with Josias Priest, a dancing master and the choreographer for the Dorset Garden Theatre. Priest's wife kept a boarding school for young gentlewomen, first in Leicester Fields and afterwards at Chelsea, where the opera was performed. It is occasionally considered the first genuine English opera, though that title is usually given to Blow's \"Venus and Adonis\": as in Blow's work, the action does not progress in spoken dialogue but in Italian-style recitative. Each work runs to less than one hour. At the time, \"Dido and Aeneas\" never found its way to the theatre, though it appears to have been very popular in private circles. It is believed to have been extensively copied, but only one song was printed by Purcell's widow in \"Orpheus Britannicus\", and the complete work remained in manuscript until 1840, when it was printed by the Musical Antiquarian Society under the editorship of Sir George Macfarren. The composition of \"Dido and Aeneas\" gave Purcell his first chance to write a sustained musical setting of a dramatic text. It was his only opportunity to compose a work in which the music carried the entire drama. The story of \"Dido and Aeneas\" derives from the original source in Virgil's epic the \"Aeneid\".\n\nSoon after Purcell's marriage, in 1682, on the death of Edward Lowe, he was appointed organist of the Chapel Royal, an office which he was able to hold simultaneously with his position at Westminster Abbey. His eldest son was born in this same year, but he was short-lived. His first printed composition, \"Twelve Sonatas\", was published in 1683. For some years after this, he was busy in the production of sacred music, odes addressed to the king and royal family, and other similar works. In 1685, he wrote two of his finest anthems, \"I was glad\" and \"My heart is inditing,\" for the coronation of King James II. In 1690 he composed a setting of the birthday ode for Queen Mary, \"Arise, my muse\" and four years later wrote one of his most elaborate, important and magnificent works – a setting for another birthday ode for the Queen, written by Nahum Tate, entitled \"Come Ye Sons of Art\".\nIn 1687, he resumed his connection with the theatre by furnishing the music for John Dryden's tragedy \"Tyrannick Love\". In this year, Purcell also composed a march and passepied called \"Quick-step\", which became so popular that Lord Wharton adapted the latter to the fatal verses of \"Lillibullero\"; and in or before January 1688, Purcell composed his anthem \"Blessed are they that fear the Lord\" by express command of the King. A few months later, he wrote the music for D'Urfey's play, \"The Fool's Preferment\". In 1690, he composed the music for Betterton's adaptation of Fletcher and Massinger's \"Prophetess\" (afterwards called \"Dioclesian\") and Dryden's \"Amphitryon\". In 1691, he wrote the music for what is sometimes considered his dramatic masterpiece, \"King Arthur\", or \"The British Worthy \". In 1692, he composed \"The Fairy-Queen\" (an adaptation of Shakespeare's \"A Midsummer Night's Dream\"), the score of which (his longest for theatre) was rediscovered in 1901 and published by the Purcell Society. \"The Indian Queen\" followed in 1695, in which year he also wrote songs for Dryden and Davenant's version of Shakespeare's \"The Tempest\" (recently, this has been disputed by music scholars), probably including \"Full fathom five\" and \"Come unto these yellow sands\". \"The Indian Queen\" was adapted from a tragedy by Dryden and Sir Robert Howard. In these semi-operas (another term for which at the time was \"dramatic opera\"), the main characters of the plays do not sing but speak their lines: the action moves in dialogue rather than recitative. The related songs are sung \"for\" them by singers, who have minor dramatic roles.\nPurcell's \"Te Deum\" and \"Jubilate Deo\" were written for Saint Cecilia's Day, 1694, the first English \"Te Deum\" ever composed with orchestral accompaniment. This work was annually performed at St Paul's Cathedral until 1712, after which it was performed alternately with Handel's \"Utrecht Te Deum and Jubilate\" until 1743, when both works were replaced by Handel's \"Dettingen Te Deum\".\n\nHe composed an anthem and two elegies for Queen Mary II's funeral, his \"Funeral Sentences and Music for the Funeral of Queen Mary\". Besides the operas and semi-operas already mentioned, Purcell wrote the music and songs for Thomas d'Urfey's \"The Comical History of Don Quixote\", \"Bonduca\", \"The Indian Queen\" and others, a vast quantity of sacred music, and numerous odes, cantatas, and other miscellaneous pieces. The quantity of his instrumental chamber music is minimal after his early career, and his keyboard music consists of an even more minimal number of harpsichord suites and organ pieces. In 1693, Purcell composed music for two comedies: \"The Old Bachelor\", and \"The Double Dealer\". Purcell also composed for five other plays within the same year. In July 1695, Purcell composed an ode for the Duke of Gloucester for his sixth birthday. The ode is titled \"Who can from joy refrain?\" Purcell's four-part sonatas were issued in 1697. In the final six years of his life, Purcell wrote music for forty-two plays.\n\nPurcell died in 1695 at his home in Marsham Street, at the height of his career. He is believed to have been 35 or 36 years old at the time. The cause of his death is unclear: one theory is that he caught a chill after returning home late from the theatre one night to find that his wife had locked him out. Another is that he succumbed to tuberculosis. The beginning of Purcell's will reads:\n\nPurcell is buried adjacent to the organ in Westminster Abbey. The music that he had earlier composed for Queen Mary's funeral was performed during his funeral as well. Purcell was universally mourned as \"a very great master of music.\"  Following his death, the officials at Westminster honoured him by unanimously voting that he be buried with no expense in the north aisle of the Abbey. His epitaph reads: \"Here lyes Henry Purcell Esq., who left this life and is gone to that Blessed Place where only His harmony can be exceeded.\"\n\nPurcell fathered six children by his wife Frances, four of whom died in infancy. His wife, as well as his son Edward (1689–1740) and daughter Frances, survived him. Frances the elder died in 1706, having published a number of her husband's works, including the now famous collection called \"Orpheus Britannicus\", in two volumes, printed in 1698 and 1702, respectively. Edward was appointed organist of St Clement's, Eastcheap, London, in 1711 and was succeeded by his son Edward Henry Purcell (died 1765). Both men were buried in St Clement's near the organ gallery.\n\nPurcell worked in many genres, both in works closely linked to the court, such as symphony song, to the Chapel Royal, such as the symphony anthem, and the theatre.\n\nAmong Purcell's most notable works are his opera \"Dido and Aeneas\" (1688), his semi-operas \"Dioclesian\" (1690), \"King Arthur\" (1691), \"The Fairy-Queen\" (1692) and \"Timon of Athens\" (1695), as well as the compositions \"Hail! Bright Cecilia\" (1692), \"Come Ye Sons of Art\" (1694) and \"Funeral Sentences and Music for the Funeral of Queen Mary\" (1695).\n\nAfter his death, Purcell was honoured by many of his contemporaries, including his old friend John Blow, who wrote \"An Ode, on the Death of Mr. Henry Purcell (Mark how the lark and linnet sing)\" with text by his old collaborator, John Dryden. William Croft's 1724 setting for the Burial Service, was written in the style of \"the great Master\". Croft preserved Purcell's setting of \"Thou knowest Lord\" (Z 58) in his service, for reasons \"obvious to any artist\"; it has been sung at every British state funeral ever since. More recently, the English poet Gerard Manley Hopkins wrote a famous sonnet entitled simply \"Henry Purcell\", with a headnote reading: \"The poet wishes well to the divine genius of Purcell and praises him that, whereas other musicians have given utterance to the moods of man's mind, he has, beyond that, uttered in notes the very make and species of man as created both in him and in all men generally.\"\n\nPurcell also had a strong influence on the composers of the English musical renaissance of the early 20th century, most notably Benjamin Britten, who created and performed a realisation of \"Dido and Aeneas\" and whose \"The Young Person's Guide to the Orchestra\" is based on a theme from Purcell's \"Abdelazar\". Stylistically, the aria \"I know a bank\" from Britten's opera \"A Midsummer Night's Dream\" is clearly inspired by Purcell's aria \"Sweeter than Roses\", which Purcell originally wrote as part of incidental music to Richard Norton's \"Pausanias, the Betrayer of His Country\".\n\nPurcell is honoured together with Johann Sebastian Bach and George Frideric Handel with a feast day on the liturgical calendar of the Episcopal Church (USA) on 28 July. In a 1940 interview Ignaz Friedman stated that he considered Purcell as great as Bach and Beethoven. In Victoria Street, Westminster, England, there is a bronze monument to Purcell, sculpted by Glynn Williams and erected in 1994.\n\nPurcell's works have been catalogued by Franklin Zimmerman, who gave them a number preceded by Z.\n\nA Purcell Club was founded in London in 1836 for promoting the performance of his music, but was dissolved in 1863. In 1876 a Purcell Society was founded, which published new editions of his works. A modern-day Purcell Club has been created, and provides guided tours and concerts in support of Westminster Abbey.\n\nSo strong was his reputation that a popular wedding processional was incorrectly attributed to Purcell for many years. The so-called \"Purcell's Trumpet Voluntary\" was in fact written around 1700 by a British composer named Jeremiah Clarke as the \"Prince of Denmark's March\".\n\nMusic for the Funeral of Queen Mary was reworked by Wendy Carlos for the title music of the 1971 film by Stanley Kubrick, \"A Clockwork Orange\". The 1973 \"Rolling Stone\" review of Jethro Tull's \"A Passion Play\" compared the musical style of the album with that of Purcell.\nIn 2009 Pete Townshend of The Who, an English rock band that established itself in the 1960s, identified Purcell's harmonies, particularly the use of suspension and resolution that Townshend had learned from producer Kit Lambert, as an influence on the band's music (in songs such as \"Won't Get Fooled Again\" (1971), \"I Can See for Miles\" (1967) and the very Purcellian intro to \"Pinball Wizard\").\n\nPurcell's music was widely featured as background music in the Academy Award winning 1979 film \"Kramer vs. Kramer\" with the soundtrack being released by CBS Masterworks Records\n\nIn the 21st century, the soundtrack of the 2005 film version of \"Pride and Prejudice\" features a dance titled \"A Postcard to Henry Purcell\". This is a version by composer Dario Marianelli of Purcell's \"Abdelazar\" theme. In the German-language 2004 movie, \"Downfall\", the music of Dido's Lament is used repeatedly as the end of the Third Reich culminates. The 2012 film \"Moonrise Kingdom\" contains Benjamin Britten's version of the Rondeau in Purcell's \"Abdelazar\" created for his 1946 \"The Young Person's Guide to the Orchestra\". In 2013, the Pet Shop Boys released their single \"Love Is a Bourgeois Construct\" incorporating one of the same ground basses from \"King Arthur\" used by Nyman in his \"Draughtsman's Contract\" score. Olivia Chaney performs her adaptation of \"There's Not a Swain\" on her CD \"The Longest River.\"\n\nBibliography\n\n"}
{"id": "14136", "url": "https://en.wikipedia.org/wiki?curid=14136", "title": "Hydrophobe", "text": "Hydrophobe\n\nIn chemistry, hydrophobicity is the physical property of a molecule (known as a hydrophobe) that is seemingly repelled from a mass of water. (Strictly speaking, there is no repulsive force involved; it is an absence of attraction.) In contrast, hydrophiles are attracted to water.\n\nHydrophobic molecules tend to be nonpolar and, thus, prefer other neutral molecules and nonpolar solvents. Because water molecules are polar, hydrophobes do not dissolve well among them. Hydrophobic molecules in water often cluster together, forming micelles. Water on hydrophobic surfaces will exhibit a high contact angle.\n\nExamples of hydrophobic molecules include the alkanes, oils, fats, and greasy substances in general. Hydrophobic materials are used for oil removal from water, the management of oil spills, and chemical separation processes to remove non-polar substances from polar compounds.\n\nHydrophobic is often used interchangeably with lipophilic, \"fat-loving\". However, the two terms are not synonymous. While hydrophobic substances are usually lipophilic, there are exceptions, such as the silicones and fluorocarbons.\n\nThe term \"hydrophobe\" comes from the Ancient Greek ὑδρόφοβος, \"having a horror of water\", constructed from ὕδωρ, \"water\", and φόβος, \"fear\".\n\nThe hydrophobic interaction is mostly an entropic effect originating from the disruption of the highly dynamic hydrogen bonds between molecules of liquid water by the nonpolar solute forming a clathrate-like structure around the non-polar molecules. This structure formed is more highly ordered than free water molecules due to the water molecules arranging themselves to interact as much as possible with themselves, and thus results in a higher entropic state which causes non-polar molecules to clump together to reduce the surface area exposed to water and decrease the entropy of the system. Thus, the 2 immiscible phases (hydrophilic vs. hydrophobic) will change so that their corresponding interfacial area will be minimal. This effect can be visualized in the phenomenon called phase separation.\n\nSuperhydrophobic surfaces, such as the leaves of the lotus plant, are those that are extremely difficult to wet. The contact angles of a water droplet exceeds 150°. This is referred to as the lotus effect, and is primarily a physical property related to interfacial tension, rather than a chemical property.\n\nIn 1805, Thomas Young defined the contact angle \"θ\" by analyzing the forces acting on a fluid droplet resting on a solid surface surrounded by a gas.\n\nwhere\n\n\"θ\" can be measured using a contact angle goniometer.\n\nWenzel determined that when the liquid is in intimate contact with a microstructured surface, \"θ\" will change to \"θ\"\n\nwhere \"r\" is the ratio of the actual area to the projected area. Wenzel's equation shows that microstructuring a surface amplifies the natural tendency of the surface. A hydrophobic surface (one that has an original contact angle greater than 90°) becomes more hydrophobic when microstructured – its new contact angle becomes greater than the original. However, a hydrophilic surface (one that has an original contact angle less than 90°) becomes more hydrophilic when microstructured – its new contact angle becomes less than the original.\nCassie and Baxter found that if the liquid is suspended on the tops of microstructures, \"θ\" will change to \"θ\":\n\nwhere φ is the area fraction of the solid that touches the liquid. Liquid in the Cassie–Baxter state is more mobile than in the Wenzel state.\n\nWe can predict whether the Wenzel or Cassie–Baxter state should exist by calculating the new contact angle with both equations. By a minimization of free energy argument, the relation that predicted the smaller new contact angle is the state most likely to exist. Stated in mathematical terms, for the Cassie–Baxter state to exist, the following inequality must be true.\n\nA recent alternative criterion for the Cassie–Baxter state asserts that the Cassie–Baxter state exists when the following 2 criteria are met:1) Contact line forces overcome body forces of unsupported droplet weight and 2) The microstructures are tall enough to prevent the liquid that bridges microstructures from touching the base of the microstructures.\n\nA new criterion for the switch between Wenzel and Cassie-Baxter states has been developed recently based on surface roughness and surface energy. The criterion focuses on the air-trapping capability under liquid droplets on rough surfaces, which could tell whether Wenzel's model or Cassie-Baxter's model should be used for certain combination of surface roughness and energy.\n\nContact angle is a measure of static hydrophobicity, and contact angle hysteresis and slide angle are dynamic measures. Contact angle hysteresis is a phenomenon that characterizes surface heterogeneity. When a pipette injects a liquid onto a solid, the liquid will form some contact angle. As the pipette injects more liquid, the droplet will increase in volume, the contact angle will increase, but its three-phase boundary will remain stationary until it suddenly advances outward. The contact angle the droplet had immediately before advancing outward is termed the advancing contact angle. The receding contact angle is now measured by pumping the liquid back out of the droplet. The droplet will decrease in volume, the contact angle will decrease, but its three-phase boundary will remain stationary until it suddenly recedes inward. The contact angle the droplet had immediately before receding inward is termed the receding contact angle. The difference between advancing and receding contact angles is termed contact angle hysteresis and can be used to characterize surface heterogeneity, roughness, and mobility. Surfaces that are not homogeneous will have domains that impede motion of the contact line. The slide angle is another dynamic measure of hydrophobicity and is measured by depositing a droplet on a surface and tilting the surface until the droplet begins to slide. In general, liquids in the Cassie–Baxter state exhibit lower slide angles and contact angle hysteresis than those in the Wenzel state.\n\nDettre and Johnson discovered in 1964 that the superhydrophobic lotus effect phenomenon was related to rough hydrophobic surfaces, and they developed a theoretical model based on experiments with glass beads coated with paraffin or TFE telomer. The self-cleaning property of superhydrophobic micro-nanostructured surfaces was reported in 1977. Perfluoroalkyl, perfluoropolyether, and RF plasma -formed superhydrophobic materials were developed, used for electrowetting and commercialized for bio-medical applications between 1986 and 1995. Other technology and applications have emerged since the mid 1990s. A durable superhydrophobic hierarchical composition, applied in one or two steps, was disclosed in 2002 comprising nano-sized particles ≤ 100 nanometers overlaying a surface having micrometer-sized features or particles ≤ 100 micrometers. The larger particles were observed to protect the smaller particles from mechanical abrasion.\n\nIn recent research, superhydrophobicity has been reported by allowing alkylketene dimer (AKD) to solidify into a nanostructured fractal surface. Many papers have since presented fabrication methods for producing superhydrophobic surfaces including particle deposition, sol-gel techniques, plasma treatments, vapor deposition, and casting techniques. Current opportunity for research impact lies mainly in fundamental research and practical manufacturing. Debates have recently emerged concerning the applicability of the Wenzel and Cassie–Baxter models. In an experiment designed to challenge the surface energy perspective of the Wenzel and Cassie–Baxter model and promote a contact line perspective, water drops were placed on a smooth hydrophobic spot in a rough hydrophobic field, a rough hydrophobic spot in a smooth hydrophobic field, and a hydrophilic spot in a hydrophobic field. Experiments showed that the surface chemistry and geometry at the contact line affected the contact angle and contact angle hysteresis, but the surface area inside the contact line had no effect. An argument that increased jaggedness in the contact line enhances droplet mobility has also been proposed.\n\nMany hydrophobic materials found in nature rely on Cassie's law and are biphasic on the submicrometer level with one component air. The lotus effect is based on this principle. Inspired by it, many functional superhydrophobic surfaces have been prepared.\n\nAn example of a bionic or biomimetic superhydrophobic material in nanotechnology is nanopin film.\n\nOne study presents a vanadium pentoxide surface that switches reversibly between superhydrophobicity and superhydrophilicity under the influence of UV radiation. According to the study, any surface can be modified to this effect by application of a suspension of rose-like VO particles, for instance with an inkjet printer. Once again hydrophobicity is induced by interlaminar air pockets (separated by 2.1 nm distances). The UV effect is also explained. UV light creates electron-hole pairs, with the holes reacting with lattice oxygen, creating surface oxygen vacancies, while the electrons reduce V to V. The oxygen vacancies are met by water, and it is this water absorbency by the vanadium surface that makes it hydrophilic. By extended storage in the dark, water is replaced by oxygen and hydrophilicity is once again lost.\n\nHydrophobic concrete has been produced since the mid-20th century.\n\nActive recent research on superhydrophobic materials might eventually lead to more industrial applications.\n\nA simple routine of coating cotton fabric with silica or titania particles by sol-gel technique has been reported, which protects the fabric from UV light and makes it superhydrophobic.\n\nAn efficient routine has been reported for making polyethylene superhydrophobic and thus self-cleaning. —99% of dirt absorbed on such surface is easily washed away.\n\nPatterned superhydrophobic surfaces also have promise for lab-on-a-chip microfluidic devices and can drastically improve surface-based bioanalysis.\n\n\n"}
{"id": "14142", "url": "https://en.wikipedia.org/wiki?curid=14142", "title": "Harley-Davidson", "text": "Harley-Davidson\n\nHarley-Davidson, Inc. (H-D), or Harley, is an American motorcycle manufacturer, founded in Milwaukee, Wisconsin in 1903.\n\nOne of two major American motorcycle manufacturers to survive the Great Depression (along with Indian), the company has survived numerous ownership arrangements, subsidiary arrangements (e.g., Aermacchi 1960-1978 and Buell 1987-2009), periods of poor economic health and product quality, as well as intense global competition, to become one of the world's largest motorcycle manufacturers and an iconic brand widely known for its loyal following. There are owner clubs and events worldwide as well as a company-sponsored brand-focused museum.\n\nNoted for a style of customization that gave rise to the chopper motorcycle style, Harley-Davidson traditionally marketed heavyweight, air-cooled cruiser motorcycles with engine displacements greater than 700 cm³ and has broadened its offerings to include its more contemporary VRSC (2002) and middle-weight Street (2015) platforms.\n\nHarley-Davidson manufactures its motorcycles at factories in York, Pennsylvania; Milwaukee, Wisconsin; Kansas City, Missouri (closing); Manaus, Brazil; and Bawal, India. Construction of a new plant in Thailand is scheduled to begin in late 2018. The company markets its products worldwide.\n\nBesides motorcycles, the company licenses and markets merchandise under the Harley-Davidson brand, among them apparel, home decor and ornaments, accessories, toys, and scale figures of its motorcycles, and video games based on its motorcycle line and the community.\n\nIn 1901, -year-old William S. Harley drew up plans for a small engine with a displacement of 7.07 cubic inches (116 cc) and four-inch (102 mm) flywheels. The engine was designed for use in a regular pedal-bicycle frame. Over the next two years, Harley and his childhood friend Arthur Davidson worked on their motor-bicycle using the northside Milwaukee machine shop at the home of their friend, Henry Melk. It was finished in 1903 with the help of Arthur's brother, Walter Davidson. Upon testing their power-cycle, Harley and the Davidson brothers found it unable to climb the hills around Milwaukee without pedal assistance. They quickly wrote off their first motor-bicycle as a valuable learning experiment.\n\nWork immediately began on a new and improved second-generation machine. This first \"real\" Harley-Davidson motorcycle had a bigger engine of 24.74 cubic inches (405 cc) with flywheels weighing . The machine's advanced loop-frame pattern was similar to the 1903 Milwaukee Merkel motorcycle (designed by Joseph Merkel, later of Flying Merkel fame). The bigger engine and loop-frame design took it out of the motorized bicycle category and marked the path to future motorcycle designs. The boys also received help with their bigger engine from outboard motor pioneer Ole Evinrude, who was then building gas engines of his own design for automotive use on Milwaukee's Lake Street.\nThe prototype of the new loop-frame Harley-Davidson was assembled in a shed in the Davidson family backyard. Most of the major parts, however, were made elsewhere, including some probably fabricated at the West Milwaukee railshops where oldest brother William A. Davidson was then toolroom foreman. This prototype machine was functional by September 8, 1904, when it competed in a Milwaukee motorcycle race held at State Fair Park. It was ridden by Edward Hildebrand and placed fourth. This is the first documented appearance of a Harley-Davidson motorcycle in the historical record.\n\nIn January 1905, small advertisements were placed in the \"Automobile and Cycle Trade Journal\" offering bare Harley-Davidson engines to the do-it-yourself trade. By April, complete motorcycles were in production on a very limited basis. That year, the first Harley-Davidson dealer, Carl H. Lang of Chicago, sold three bikes from the five built in the Davidson backyard shed. Years later the original shed was taken to the Juneau Avenue factory where it would stand for many decades as a tribute to the Motor Company's humble origins until it was accidentally destroyed by contractors cleaning the factory yard in the early 1970s.\n\nIn 1906, Harley and the Davidson brothers built their first factory on Chestnut Street (later Juneau Avenue), at the current location of Harley-Davidson's corporate headquarters. The first Juneau Avenue plant was a single-story wooden structure. The company produced about 50 motorcycles that year.\n\nIn 1907, William S. Harley graduated from the University of Wisconsin–Madison with a degree in mechanical engineering. That year additional factory expansion came with a second floor and later with facings and additions of Milwaukee pale yellow (\"cream\") brick. With the new facilities production increased to 150 motorcycles in 1907. The company was officially incorporated that September. They also began selling their motorcycles to police departments around this time, a market that has been important to them ever since.\n\nIn 1907 William A. Davidson, brother to Arthur and Walter Davidson, quit his job as tool foreman for the Milwaukee Road railroad and joined the Motor Company.\n\nProduction in 1905 and 1906 were all single-cylinder models with 26.84 cubic inch (440 cm³) engines. In February 1907 a prototype model with a 45-degree V-Twin engine was displayed at the Chicago Automobile Show. Although shown and advertised, very few V-Twin models were built between 1907 and 1910. These first V-Twins displaced 53.68 cubic inches (880 cm³) and produced about . This gave about double the power of the first singles. Top speed was about . Production jumped from 450 motorcycles in 1908 to 1,149 machines in 1909.\n\nBy 1911, some 150 makes of motorcycles had already been built in the United States – although just a handful would survive the 1910s.\n\nIn 1911, an improved V-Twin model was introduced. The new engine had mechanically operated intake valves, as opposed to the \"automatic\" intake valves used on earlier V-Twins that opened by engine vacuum. With a displacement of 49.48 cubic inches (811 cm³), the 1911 V-Twin was smaller than earlier twins, but gave better performance. After 1913 the majority of bikes produced by Harley-Davidson would be V-Twin models.\n\nIn 1912, Harley-Davidson introduced their patented \"Ful-Floteing Seat\", which was suspended by a coil spring inside the seat tube. The spring tension could be adjusted to suit the rider's weight. More than of travel was available. Harley-Davidson would use seats of this type until 1958.\n\nBy 1913, the yellow brick factory had been demolished and on the site a new 5-story structure had been built. Begun in 1910, the factory with its many additions would take up two blocks along Juneau Avenue and around the corner on 38th Street. Despite the competition, Harley-Davidson was already pulling ahead of Indian and would dominate motorcycle racing after 1914. Production that year swelled to 16,284 machines.\nIn 1917, the United States entered World War I and the military demanded motorcycles for the war effort. Harleys had already been used by the military in the Pancho Villa Expedition but World War I was the first time the motorcycle had been adopted for military issue, first with the British Model H, produced by British Triumph Motorcycles Ltd in 1915. After the U.S. entry into the war, the U.S. military purchased over 20,000 motorcycles from Harley-Davidson.\n\nHarley-Davidson launched a line of bicycles in 1917 in hopes of recruiting customers for its motorcycles. Besides the traditional diamond frame men's bicycle, models included a step-through frame 3-18 \"Ladies Standard\" and a 5-17 \"Boy Scout\" for youth. The effort was discontinued in 1923 because of disappointing sales.\n\nThe bicycles were built for Harley-Davidson in Dayton, Ohio, by the Davis Machine Company from 1917 to 1921, when Davis stopped manufacturing bicycles.\n\nBy 1920, Harley-Davidson was the largest motorcycle manufacturer in the world, with 28,189 machines produced, and dealers in 67 countries.\n\nIn 1921, a Harley-Davidson, ridden by Otto Walker, was the first motorcycle ever to win a race at an average speed greater than .\n\nDuring the 1920s, several improvements were put in place, such as a new 74 cubic inch (1,212.6  cm³) V-Twin, introduced in 1921, and the \"teardrop\" gas tank in 1925. A front brake was added in 1928 although notably only on the J/JD models.\n\nIn the late summer of 1929, Harley-Davidson introduced its 45 cubic inches (737 cm³) flathead V-Twin to compete with the Indian 101 Scout and the Excelsior Super X. This was the \"D\" model, produced from 1929 to 1931. Riders of Indian motorcycles derisively referred to this model as the \"three cylinder Harley\" because the generator was upright and parallel to the front cylinder.\n\nThe Great Depression began a few months after the introduction of their 45 cubic inch (737 cm³) model. Harley-Davidson's sales fell from 21,000 in 1929 to 3,703 in 1933. Despite this, Harley-Davidson unveiled a new lineup for 1934, which included a flathead engine and Art Deco styling.\n\nIn order to survive the remainder of the Depression, the company manufactured industrial powerplants based on their motorcycle engines. They also designed and built a three-wheeled delivery vehicle called the Servi-Car, which remained in production until 1973.\n\nIn the mid-1930s, Alfred Rich Child opened a production line in Japan with the 74-cubic-inch (1,210 cm³) VL. The Japanese license-holder, Sankyo Seiyaku Corporation, severed its business relations with Harley-Davidson in 1936 and continued manufacturing the VL under the Rikuo name.\n\nAn 80-cubic-inch (1,300 cm³) flathead engine was added to the line in 1935, by which time the single-cylinder motorcycles had been discontinued.\n\nIn 1936, the 61E and 61EL models with the \"Knucklehead\" OHV engines were introduced. Valvetrain problems in early Knucklehead engines required a redesign halfway through its first year of production and retrofitting of the new valvetrain on earlier engines.\n\nBy 1937, all Harley-Davidson flathead engines were equipped with dry-sump oil recirculation systems similar to the one introduced in the \"Knucklehead\" OHV engine. The revised 74-cubic-inch (1,212 cc ) V and VL models were renamed U and UL, the 80-cubic-inch (1,300 cc) VH and VLH to be renamed UH and ULH, and the 45-cubic-inch (740 cc) R to be renamed W.\n\nIn 1941, the 74-cubic-inch (1,210 cm³) \"Knucklehead\" was introduced as the F and the FL. The 80-cubic-inch (1,300 cc) flathead UH and ULH models were discontinued after 1941, while the 74 inch (1210 cm³) U & UL flathead models were produced up to 1948.\n\nOne of only two American cycle manufacturers to survive the Great Depression, Harley-Davidson again produced large numbers of motorcycles for the US Army in World War II and resumed civilian production afterwards, producing a range of large V-twin motorcycles that were successful both on racetracks and for private buyers.\n\nHarley-Davidson, on the eve of World War II, was already supplying the Army with a military-specific version of its 45 cubic inches (740 cm³) WL line, called the WLA. The A in this case stood for \"Army\". Upon the outbreak of war, the company, along with most other manufacturing enterprises, shifted to war work. More than 90,000 military motorcycles, mostly WLAs and WLCs (the Canadian version) were produced, many to be provided to allies. Harley-Davidson received two Army-Navy ‘E’ Awards, one in 1943 and the other in 1945, which were awarded for Excellence in Production.\nShipments to the Soviet Union under the Lend-Lease program numbered at least 30,000. The WLAs produced during all four years of war production generally have 1942 serial numbers. Production of the WLA stopped at the end of World War II, but was resumed from 1950 to 1952 for use in the Korean War.\n\nThe U.S. Army also asked Harley-Davidson to produce a new motorcycle with many of the features of BMW's side-valve and shaft-driven R71. Harley largely copied the BMW engine and drive train and produced the shaft-driven 750 cc 1942 Harley-Davidson XA. This shared no dimensions, no parts or no design concepts (except side valves) with any prior Harley-Davidson engine. Due to the superior cooling of the flat-twin engine with the cylinders across the frame, Harley's XA cylinder heads ran 100 °F (56 °C) cooler than its V-twins. The XA never entered full production: the motorcycle by that time had been eclipsed by the Jeep as the Army's general purpose vehicle, and the WLA—already in production—was sufficient for its limited police, escort, and courier roles. Only 1,000 were made and the XA never went into full production. It remains the only shaft-driven Harley-Davidson ever made.\n\nAs part of war reparations, Harley-Davidson acquired the design of a small German motorcycle, the DKW RT 125, which they adapted, manufactured, and sold from 1948 to 1966. Various models were made, including the Hummer from 1955 to 1959, but they are all colloquially referred to as \"Hummers\" at present. BSA in the United Kingdom took the same design as the foundation of their BSA Bantam.\nIn 1960, Harley-Davidson consolidated the Model 165 and Hummer lines into the Super-10, introduced the Topper scooter, and bought fifty percent of Aermacchi's motorcycle division. Importation of Aermacchi's 250 cc horizontal single began the following year. The bike bore Harley-Davidson badges and was marketed as the Harley-Davidson Sprint. The engine of the Sprint was increased to 350 cc in 1969 and would remain that size until 1974, when the four-stroke Sprint was discontinued.\n\nAfter the Pacer and Scat models were discontinued at the end of 1965, the Bobcat became the last of Harley-Davidson's American-made two-stroke motorcycles. The Bobcat was manufactured only in the 1966 model year.\n\nHarley-Davidson replaced their American-made lightweight two-stroke motorcycles with the Italian Aermacchi-built two-stroke powered M-65, M-65S, and Rapido. The M-65 had a semi-step-through frame and tank. The M-65S was a M-65 with a larger tank that eliminated the step-through feature. The Rapido was a larger bike with a 125 cc engine. The Aermacchi-built Harley-Davidsons became entirely two-stroke powered when the 250 cc two-stroke SS-250 replaced the four-stroke 350 cc Sprint in 1974.\n\nHarley-Davidson purchased full control of Aermacchi's motorcycle production in 1974 and continued making two-stroke motorcycles there until 1978, when they sold the facility to Cagiva, owned by the Castiglioni family.\n\nEstablished in 1918, the oldest continuously operating Harley-Davidson dealership outside of the United States is in Australia. Sales in Japan started in 1912 then in 1929, Harley-Davidsons were produced in Japan under license to the company Rikuo (Rikuo Internal Combustion Company) under the name of Harley-Davidson and using the company's tooling, and later under the name Rikuo. Production continued until 1958.\n\nIn 1952, following their application to the U.S. Tariff Commission for a 40 percent tax on imported motorcycles, Harley-Davidson was charged with restrictive practices.\nIn 1969, American Machine and Foundry (AMF) bought the company, streamlined production, and slashed the workforce. This tactic resulted in a labor strike and lower-quality bikes. The bikes were expensive and inferior in performance, handling, and quality to Japanese motorcycles. Sales and quality declined, and the company almost went bankrupt. The \"Harley-Davidson\" name was mocked as \"Hardly Ableson\", \"Hardly Driveable,\" and \"Hogly Ferguson\",\nand the nickname \"Hog\" became pejorative.\n\nIn 1977, following the successful manufacture of the Liberty Edition to commemorate America's bicentennial in 1976, Harley-Davidson produced what has become one of its most controversial models, the Harley-Davidson Confederate Edition. The bike was essentially a stock Harley with Confederate-specific paint and details.\n\nIn 1981, AMF sold the company to a group of 13 investors led by Vaughn Beals and Willie G. Davidson for $80 million. Inventory was strictly controlled using the just-in-time system.\n\nIn the early eighties, Harley-Davidson claimed that Japanese manufacturers were importing motorcycles into the US in such volume as to harm or threaten to harm domestic producers. After an investigation by the U.S. International Trade Commission, President Reagan in 1983 imposed a 45 percent tariff on imported bikes with engine capacities greater than 700 cc. Harley-Davidson subsequently rejected offers of assistance from Japanese motorcycle makers.<ref name=\"7/83 US IMPOSES 45% TARIFF ON IMPORTED MOTORCYCLES\"> – 7/83 US Imposes 45% Tariff on Imported Motorcycles</ref> However, the company did offer to drop the request for the tariff in exchange for loan guarantees from the Japanese.\n\nRather than trying to match the Japanese, the new management deliberately exploited the \"retro\" appeal of the machines, building motorcycles that deliberately adopted the look and feel of their earlier machines and the subsequent customizations of owners of that era. Many components such as brakes, forks, shocks, carburetors, electrics and wheels were outsourced from foreign manufacturers and quality increased, technical improvements were made, and buyers slowly returned.\n\nHarley-Davidson bought the \"Sub Shock\" cantilever-swingarm rear suspension design from Missouri engineer Bill Davis and developed it into its Softail series of motorcycles, introduced in 1984 with the FXST Softail.\n\nIn response to possible motorcycle market loss due to the aging of baby-boomers, Harley-Davidson bought luxury motorhome manufacturer Holiday Rambler in 1986. In 1996, the company sold Holiday Rambler to the Monaco Coach Corporation.\n\nThe \"Sturgis\" model, boasting a dual belt-drive, was introduced initially in 1980 and was made for three years. This bike was then brought back as a commemorative model in 1991.\nBy 1990, with the introduction of the \"Fat Boy\", Harley once again became the sales leader in the heavyweight (over 750 cc) market. At the time of the Fat Boy model introduction, a story rapidly spread that its silver paint job and other features were inspired by the B-29; and Fat Boy was a combination of the names of the atomic bombs Fat Man and Little Boy. However, the Urban Legend Reference Pages lists this story as an urban legend.\n\n1993 and 1994 saw the replacement of FXR models with the Dyna (FXD), which became the sole rubber mount FX Big Twin frame in 1994. The FXR was revived briefly from 1999 to 2000 for special limited editions (FXR, FXR & FXR).\n\nConstruction started on the $75 million, 130,000 square-foot (12,000 m) Harley-Davidson Museum in the Menomonee Valley on June 1, 2006. It opened in 2008 and houses the company's vast collection of historic motorcycles and corporate archives, along with a restaurant, café and meeting space.\n\nHarley-Davidson's association with sportbike manufacturer Buell Motorcycle Company began in 1987 when they supplied Buell with fifty surplus XR1000 engines. Buell continued to buy engines from Harley-Davidson until 1993, when Harley-Davidson bought 49 percent of the Buell Motorcycle Company. Harley-Davidson increased its share in Buell to ninety-eight percent in 1998, and to complete ownership in 2003.\n\nIn an attempt to attract newcomers to motorcycling in general and to Harley-Davidson in particular, Buell developed a low-cost, low-maintenance motorcycle. The resulting single-cylinder Buell Blast was introduced in 2000, and was made through 2009, which, according to Buell, was to be the final year of production. The Buell Blast was the training vehicle for the Harley-Davidson Rider's Edge New Rider Course from 2000 until May 2014, when the company re-branded the training academy and started using the Harley-Davidson Street 500 motorcycles. In those 14 years, more than 350,000 participants in the course learned to ride on the Buell Blast.\n\nOn October 15, 2009, Harley-Davidson Inc. issued an official statement that it would be discontinuing the Buell line and ceasing production immediately. The stated reason was to focus on the Harley-Davidson brand. The company refused to consider selling Buell. Founder Erik Buell subsequently established Erik Buell Racing and continued to manufacture and develop the company's 1125RR racing motorcycle.\n\nIn 1998 the first Harley-Davidson factory outside the US opened in Manaus, Brazil, taking advantage of the free economic zone there. The location was positioned to sell motorcycles in the southern hemisphere market.\n\nDuring its period of peak demand, during the late 1990s and early first decade of the 21st century, Harley-Davidson embarked on a program of expanding the number of dealerships throughout the country. At the same time, its current dealers typically had waiting lists that extended up to a year for some of the most popular models. Harley-Davidson, like the auto manufacturers, records a sale not when a consumer buys their product, but rather when it is delivered to a dealer. Therefore, it is possible for the manufacturer to inflate sales numbers by requiring dealers to accept more inventory than desired in a practice called channel stuffing. When demand softened following the unique 2003 model year, this news led to a dramatic decline in the stock price. In April 2004 alone, the price of HOG shares dropped from more than $60 to less than $40. Immediately prior to this decline, retiring CEO Jeffrey Bleustein profited $42 million on the exercise of employee stock options. Harley-Davidson was named as a defendant in numerous class action suits filed by investors who claimed they were intentionally defrauded by Harley-Davidson's management and directors. By January 2007, the price of Harley-Davidson shares reached $70.\n\nStarting around 2000, several police departments started reporting problems with high speed instability on the Harley-Davidson Touring motorcycles. A Raleigh, North Carolina police officer, Charles Paul, was killed when his 2002 police touring motorcycle crashed after reportedly experiencing a high speed wobble. The California Highway Patrol conducted testing of the Police Touring motorcycles in 2006. The CHP test riders reported experiencing wobble or weave instability while operating the motorcycles on the test track.\n\nOn February 2, 2007, upon the expiration of their union contract, about 2,700 employees at Harley-Davidson Inc.'s largest manufacturing plant in York, Pennsylvania went on strike after failing to agree on wages and health benefits. During the pendency of the strike, the company refused to pay for any portion of the striking employees' health care.\n\nThe day before the strike, after the union voted against the proposed contract and to authorize the strike, the company shut down all production at the plant. The York facility employs more than 3,200 workers, both union and non-union.\n\nHarley-Davidson announced on February 16, 2007, that it had reached a labor agreement with union workers at its largest manufacturing plant, a breakthrough in the two-week-old strike. The strike disrupted Harley-Davidson's national production and was felt in Wisconsin, where 440 employees were laid off, and many Harley suppliers also laid off workers because of the strike.\n\nOn July 11, 2008 Harley-Davidson announced they had signed a definitive agreement to acquire the MV Agusta Group for $109M USD (€70M). MV Agusta Group contains two lines of motorcycles: the high-performance MV Agusta brand and the lightweight Cagiva brand. The acquisition was completed on August 8.\n\nOn October 15, 2009, Harley-Davidson announced that it would divest its interest in MV Agusta. Harley-Davidson Inc. sold Italian motorcycle maker MV Agusta to Claudio Castiglioni - a member of the family that had purchased Aermacchi from H-D in 1978 - for a reported 3 euros, ending the transaction in the first week of August 2010. Castiglioni was MV Agusta's former owner, and had been MV Agusta's chairman since Harley-Davidson bought it in 2008. As part of the deal, Harley-Davidson put $26M into MV Agusta's accounts, essentially giving Castiglioni $26M to take the brand.\n\nIn August 2009, Harley-Davidson announced plans to enter the market in India, and started selling motorcycles there in 2010. The company established a subsidiary, Harley-Davidson India, in Gurgaon, near Delhi, in 2011, and created an Indian dealer network.\n\nAccording to Interbrand, the value of the Harley-Davidson brand fell by 43 percent to $4.34 billion in 2009. The fall in value is believed to be connected to the 66 percent drop in the company profits in two quarters of the previous year. On April 29, 2010, Harley-Davidson stated that they must cut $54 million in manufacturing costs from its production facilities in Wisconsin, and that they would explore alternative U.S. sites to accomplish this. The announcement came in the wake of a massive company-wide restructuring, which began in early 2009 and involved the closing of two factories, one distribution center, and the planned elimination of nearly 25 percent of its total workforce (around 3,500 employees). The company announced on September 14, 2010 that it would remain in Wisconsin.\n\nThe classic Harley-Davidson engines are V-twin engines, with a 45° angle between the cylinders. The crankshaft has a single pin, and both pistons are connected to this pin through their connecting rods.\n\nThis 45° angle is covered under several United States patents and is an engineering tradeoff that allows a large, high-torque engine in a relatively small space. It causes the cylinders to fire at uneven intervals and produces the choppy \"potato-potato\" sound so strongly linked to the Harley-Davidson brand.\n\nTo simplify the engine and reduce costs, the V-twin ignition was designed to operate with a single set of points and no distributor. This is known as a dual fire ignition system, causing both spark plugs to fire regardless of which cylinder was on its compression stroke, with the other spark plug firing on its cylinder's exhaust stroke, effectively \"wasting a spark\". The exhaust note is basically a throaty growling sound with some popping.\nThe 45° design of the engine thus creates a plug firing sequencing as such: The first cylinder fires, the second (rear) cylinder fires 315° later, then there is a 405° gap until the first cylinder fires again, giving the engine its unique sound.\n\nHarley-Davidson has used various ignition systems throughout its history – be it the early points and condenser system, (Big Twin up to 1978 and Sportsters up to 1978), magneto ignition system used on some 1958 to 1969 Sportsters, early electronic with centrifugal mechanical advance weights, (all models 1978 and a half to 1979), or the late electronic with transistorized ignition control module, more familiarly known as the black box or the brain, (all models 1980 to present).\n\nStarting in 1995, the company introduced Electronic Fuel Injection (EFI) as an option for the 30th anniversary edition Electra Glide. EFI became standard on all Harley-Davidson motorcycles, including Sportsters, upon the introduction of the 2007 product line.\n\nIn 1991, Harley-Davidson began to participate in the Sound Quality Working Group, founded by Orfield Labs, Bruel and Kjaer, TEAC, Yamaha, Sennheiser, SMS and Cortex. This was the nation's first group to share research on psychological acoustics. Later that year, Harley-Davidson participated in a series of sound quality studies at Orfield Labs, based on recordings taken at the Talladega Superspeedway, with the objective to lower the sound level for EU standards while analytically capturing the \"Harley Sound\". This research resulted in the bikes that were introduced in compliance with EU standards for 1998.\n\nOn February 1, 1994, the company filed a sound trademark application for the distinctive sound of the Harley-Davidson motorcycle engine: \"The mark consists of the exhaust sound of applicant's motorcycles, produced by V-twin, common crankpin motorcycle engines when the goods are in use\". Nine of Harley-Davidson's competitors filed comments opposing the application, arguing that cruiser-style motorcycles of various brands use a single-crankpin V-twin engine which produce a similar sound. These objections were followed by litigation. In June 2000, the company dropped efforts to federally register its trademark.\n\n\n\nThe Revolution engine is based on the VR-1000 Superbike race program, co-developed by Harley-Davidson's Powertrain Engineering team and Porsche Engineering in Stuttgart, Germany. It is a liquid cooled, dual overhead cam, internally counterbalanced 60 degree V-twin engine with a displacement of 69 cubic inch (1,130 cm³), producing at 8,250 rpm at the crank, with a redline of 9,000 rpm. It was introduced for the new VRSC (V-Rod) line in 2001 for the 2002 model year, starting with the single VRSCA (V-Twin Racing Street Custom) model. The Revolution marks Harley's first collaboration with Porsche since the V4 Nova project, which, like the V-Rod, was a radical departure from Harley's traditional lineup until it was cancelled by AMF in 1981 in favor of the Evolution engine.\n\nA 1,250 cc Screamin' Eagle version of the Revolution engine was made available for 2005 and 2006, and was present thereafter in a single production model from 2005 to 2007. In 2008, the 1,250 cc Revolution Engine became standard for the entire VRSC line. Harley-Davidson claims at the crank for the 2008 VRSCAW model. The VRXSE \"Destroyer\" is equipped with a stroker (75 mm crank) Screamin' Eagle 79 cubic inch (1,300 cm³) Revolution Engine, producing more than .\n\n750 cc and 500 cc versions of the Revolution engine are used in Harley-Davidson's Street line of light cruisers. These motors, named the Revolution X, use a single overhead cam, screw and locknut valve adjustment, a single internal counterbalancer, and vertically split crankcases; all of these changes making it different from the original Revolution design.\n\nAn extreme endurance test of the Revolution engine was performed in a dynometer installation, simulating the German Autobahn (highways without general speed limit) between the Porsche research and development center in Weissach, near Stuttgart to Düsseldorf. Uncounted samples of engines failed, until an engine successfully passed the 500 hour nonstop run. This was the benchmark for the engineers to approve the start of production for the Revolution engine, which was documented in the Discovery channel special Harley-Davidson: Birth of the V-Rod, October 14, 2001.\n\nThe first Harley-Davidson motorcycles were powered by single-cylinder IOE engines with the inlet valve operated by engine vacuum, based on the DeDion-Bouton pattern. Singles of this type continued to be made until 1913, when a pushrod and rocker system was used to operate the overhead inlet valve on the single, a similar system having been used on their V-twins since 1911. Single-cylinder motorcycle engines were discontinued in 1918.\n\nSingle-cylinder engines were reintroduced in 1925 as 1926 models. These singles were available either as flathead engines or as overhead valve engines until 1930, after which they were only available as flatheads. The flathead single-cylinder motorcycles were designated Model A for engines with magneto systems only and Model B for engines with battery and coil systems, while overhead valve versions were designated Model AA and Model BA respectively, and a magneto-only racing version was designated Model S. This line of single-cylinder motorcycles ended production in 1934.\n\nModern Harley-branded motorcycles fall into one of six model families: Touring, Softail, Dyna, Sportster, Vrod and Street. These model families are distinguished by the frame, engine, suspension, and other characteristics.\n\nTouring models use Big-Twin engines and large-diameter telescopic forks. All Touring designations begin with the letters FL, \"e.g.\", FLHR (Road King) and FLTR (Road Glide).\n\nThe touring family, also known as \"dressers\" or \"baggers\", includes Road King, Road Glide, Street Glide and Electra Glide models offered in various trims. The Road Kings have a \"retro cruiser\" appearance and are equipped with a large clear windshield. Road Kings are reminiscent of big-twin models from the 1940s and 1950s. Electra Glides can be identified by their full front fairings. Most Electra Glides sport a fork-mounted fairing referred to as the \"Batwing\" due to its unmistakable shape. The Road Glide and Road Glide Ultra Classic have a frame-mounted fairing, referred to as the \"Sharknose\". The Sharknose includes a unique, dual front headlight.\n\nTouring models are distinguishable by their large saddlebags, rear coil-over air suspension and are the only models to offer full fairings with radios and CBs. All touring models use the same frame, first introduced with a Shovelhead motor in 1980, and carried forward with only modest upgrades until 2009, when it was extensively redesigned. The frame is distinguished by the location of the steering head in front of the forks and was the first H-D frame to rubber mount the drivetrain to isolate the rider from the vibration of the big V-twin.\n\nThe frame was modified for the 1993 model year when the oil tank went under the transmission and the battery was moved inboard from under the right saddlebag to under the seat. In 1997, the frame was again modified to allow for a larger battery under the seat and to lower seat height. In 2007, Harley-Davidson introduced the Twin Cam 96 engine, as well the six-speed transmission to give the rider better speeds on the highway.\n\nIn 2006, Harley introduced the FLHX Street Glide, a bike designed by Willie G. Davidson to be his personal ride, to its touring line.\n\nIn 2008, Harley added anti-lock braking systems and cruise control as a factory installed option on all touring models (standard on CVO and Anniversary models). Also new for 2008 is the fuel tank for all touring models. 2008 also brought throttle-by-wire to all touring models.\n\nFor the 2009 model year, Harley-Davidson redesigned the entire touring range with several changes, including a new frame, new swingarm, a completely revised engine-mounting system, front wheels for all but the FLHRC Road King Classic, and a 2–1–2 exhaust. The changes result in greater load carrying capacity, better handling, a smoother engine, longer range and less exhaust heat transmitted to the rider and passenger.\nAlso released for the 2009 model year is the FLHTCUTG Tri-Glide Ultra Classic, the first three-wheeled Harley since the Servi-Car was discontinued in 1973. The model features a unique frame and a 103-cubic-inch (1,690 cm³) engine exclusive to the trike.\n\nIn 2014, Harley-Davidson released a redesign for specific touring bikes and called it \"Project Rushmore\". Changes include a new 103CI High Output engine, one handed easy open saddlebags and compartments, a new Boom! Box Infotainment system with either 4.3 inch (10 cm) or 6.5 inch (16.5 cm) screens featuring touchscreen functionality [6.5 inch (16.5 cm) models only], Bluetooth (media and phone with approved compatible devices), available GPS and SiriusXM, Text-to-Speech functionality (with approved compatible devices) and USB connectivity with charging. Other features include ABS with Reflex linked brakes, improved styling, Halogen or LED lighting and upgraded passenger comfort.\n\nThese big-twin motorcycles capitalize on Harley's strong value on tradition. With the rear-wheel suspension hidden under the transmission, they are visually similar to the \"hardtail\" choppers popular in the 1960s and 1970s, as well as from their own earlier history. In keeping with that tradition, Harley offers Softail models with \"Heritage\" styling that incorporate design cues from throughout their history and used to offer \"Springer\" front ends on these Softail models from the factory.\n\nSoftail models utilize the big-twin engine (F) and the Softail chassis (ST).\n\nDyna-frame motorcycles were developed in the 1980s and early 1990s and debuted in the 1991 model year with the FXDB Sturgis offered in limited edition quantities. In 1992 the line continued with the limited edition FXDB Daytona and a production model FXD Super Glide. The new DYNA frame featured big-twin engines and traditional styling. They can be distinguished from the Softail by the traditional coil-over suspension that connects the swingarm to the frame, and from the Sportster by their larger engines. On these models, the transmission also houses the engine's oil reservoir.\n\nPrior to 2006, Dyna models typically featured a narrow, XL-style 39mm front fork and front wheel, as well as footpegs which the manufacturer included the letter \"X\" in the model designation to indicate. This lineup traditionally included the Super Glide (FXD), Super Glide Custom (FXDC), Street Bob (FXDB), and Low Rider (FXDL). One exception was the Wide Glide (FXDWG), which featured thicker 41mm forks and a narrow front wheel, but positioned the forks on wider triple-trees that give a beefier appearance. In 2008, the Dyna Fat Bob (FXDF) was introduced to the Dyna lineup, featuring aggressive styling like a new 2–1–2 exhaust, twin headlamps, a 180 mm rear tire, and, for the first time in the Dyna lineup, a 130 mm front tire. For the 2012 model year, the Dyna Switchback (FLD) became the first Dyna to break the tradition of having an FX model designation with floorboards, detachable painted hard saddlebags, touring windshield, headlight nacelle and a wide front tire with full fender. The new front end resembled the big-twin FL models from 1968-1971.\n\nThe Dyna family used the 88-cubic-inch (1,440 cm³) twin cam from 1999 to 2006. In 2007, the displacement was increased to 96 cubic inches (1,570 cm³) as the factory increased the stroke to . For the 2012 model year, the manufacturer began to offer Dyna models with the 103-cubic-inch (1,690 cm³) upgrade. All Dyna models use a rubber-mounted engine to isolate engine vibration. Harley discontinued the Dyna platform in 2017 for the 2018 model year, having been replaced by a completely-redesigned Softail chassis; some of the existing models previously released by the company under the Dyna nameplate have since been carried over to the new Softail line.\nDyna models utilize the big-twin engine (F), footpegs noted as (X) with the exception of the 2012 FLD Switchback, a Dyna model which used floorboards as featured on the Touring (L) models, and the Dyna chassis (D). Therefore, except for the FLD from 2012 to 2016, all Dyna models have designations that begin with FXD, \"e.g.\", FXDWG (Dyna Wide Glide) and FXDL (Dyna Low Rider).\n\nIntroduced in 1957, the Sportster family were conceived as racing motorcycles, and were popular on dirt and flat-track race courses through the 1960s and 1970s. Smaller and lighter than the other Harley models, contemporary Sportsters make use of 883 cc or 1,200 cc Evolution engines and, though often modified, remain similar in appearance to their racing ancestors.\n\nUp until the 2003 model year, the engine on the Sportster was rigidly mounted to the frame. The 2004 Sportster received a new frame accommodating a rubber-mounted engine. This made the bike heavier and reduced the available lean angle, while it reduced the amount of vibration transmitted to the frame and the rider, providing a smoother ride for rider and passenger.\n\nIn the 2007 model year, Harley-Davidson celebrated the 50th anniversary of the Sportster and produced a limited edition called the XL50, of which only 2000 were made for sale worldwide. Each motorcycle was individually numbered and came in one of two colors, Mirage Pearl Orange or Vivid Black. Also in 2007, electronic fuel injection was introduced to the Sportster family, and the Nightster model was introduced in mid-year. In 2009, Harley-Davidson added the Iron 883 to the Sportster line, as part of the Dark Custom series.\nIn the 2008 model year, Harley-Davidson released the XR1200 Sportster in Europe, Africa, and the Middle East. The XR1200 had an Evolution engine tuned to produce , four-piston dual front disc brakes, and an aluminum swing arm. \"Motorcyclist\" featured the XR1200 on the cover of its July 2008 issue and was generally positive about it in their \"First Ride\" story, in which Harley-Davidson was repeatedly asked to sell it in the United States.\nOne possible reason for the delayed availability in the United States was the fact that Harley-Davidson had to obtain the \"XR1200\" naming rights from Storz Performance, a Harley customizing shop in Ventura, Calif. The XR1200 was released in the United States in 2009 in a special color scheme including Mirage Orange highlighting its dirt-tracker heritage. The first 750 XR1200 models in 2009 were pre-ordered and came with a number 1 tag for the front of the bike, autographed by Kenny Coolbeth and Scott Parker and a thank you/welcome letter from the company, signed by Bill Davidson. The XR1200 was discontinued in model year 2013.\nExcept for the street-going XR1000 of the 1980s and the XR1200, most Sportsters made for street use have the prefix XL in their model designation. For the Sportster Evolution engines used since the mid-1980s, there have been two engine sizes. Motorcycles with the smaller engine are designated XL883, while those with the larger engine were initially designated XL1100. When the size of the larger engine was increased from 1,100 cc to 1,200 cc, the designation was changed accordingly from XL1100 to XL1200. Subsequent letters in the designation refer to model variations within the Sportster range, e.g. the XL883C refers to an 883 cc Sportster Custom, while the XL1200S designates the now-discontinued 1200 Sportster Sport.\n\nIntroduced in 2001 and produced until 2017, the VRSC muscle bike family bears little resemblance to Harley's more traditional lineup. Competing against Japanese and American muscle bikes in the upcoming muscle bike/power cruiser segment, the \"V-Rod\" makes use of an engine developed jointly with Porsche that, for the first time in Harley history, incorporates overhead cams and liquid cooling. The V-Rod is visually distinctive, easily identified by the 60-degree V-Twin engine, the radiator and the hydroformed frame members that support the round-topped air cleaner cover. The VRSC platform was also used for factory drag-racing motorcycles.\n\nIn 2008, Harley added the anti-lock braking system as a factory installed option on all VRSC models. Harley also increased the displacement of the stock engine from , which had only previously been available from Screamin' Eagle, and added a slipper clutch as standard equipment.\n\nVRSC models include:\n\n\nVRSC models utilize the Revolution engine (VR), and the street versions are designated Street Custom (SC). After the VRSC prefix common to all street Revolution bikes, the next letter denotes the model, either A (base V-Rod: discontinued), AW (base V-Rod + W for Wide with a 240 mm rear tire), B (discontinued), D (Night Rod: discontinued), R (Street Rod: discontinued), SE and SEII(CVO Special Edition), or X (Special edition). Further differentiation within models are made with an additional letter, \"e.g.\", VRSCDX denotes the Night Rod Special.\n\nThe VRXSE V-Rod Destroyer is Harley-Davidson's production drag racing motorcycle, constructed to run the quarter mile in less than ten seconds. It is based on the same revolution engine that powers the VRSC line, but the VRXSE uses the Screamin' Eagle 1,300 cc \"stroked\" incarnation, featuring a 75 mm crankshaft, 105 mm Pistons, and 58 mm throttle bodies.\n\nThe V-Rod Destroyer is not a street legal motorcycle. As such, it uses \"X\" instead of \"SC\" to denote a non-street bike. \"SE\" denotes a CVO Special Edition.\n\nThe Street, Harley-Davidson's newest platform and their first all new platform in thirteen years, was designed to appeal to younger riders looking for a lighter bike at a cheaper price. The Street 750 model was launched in India at the 2014 Indian Auto Expo, Delhi-NCR on February 5, 2014. The Street 750 weighs 218 kg and has a ground clearance of 144 mm giving it the lowest weight and the highest ground clearance of Harley-Davidson motorcycles currently available.\n\nThe Street 750 uses an all-new, liquid-cooled, 60° V-twin engine called the Revolution X. In the Street 750, the engine displaces and produces 65 Nm at 4,000 rpm. A six speed transmission is used.\n\nThe Street 750 and the smaller-displacement Street 500 has been available since late 2014. Street series motorcycles for the North American market will be built in Harley-Davidson's Kansas City, Missouri plant, while those for other markets around the world will be built completely in their plant in Bawal, India.\n\nCustom Vehicle Operations (CVO) is a team within Harley-Davidson that produces limited-edition customizations of Harley's stock models. Every year since 1999, the team has selected two to five of the company's base models and added higher-displacement engines, performance upgrades, special-edition paint jobs, more chromed or accented components, audio system upgrades, and electronic accessories to create high-dollar, premium-quality customizations for the factory custom market. The models most commonly upgraded in such a fashion are the Ultra Classic Electra Glide, which has been selected for CVO treatment every year from 2006 to the present, and the Road King, which was selected in 2002, 2003, 2007, and 2008. The Dyna, Softail, and VRSC families have also been selected for CVO customization.\n\nThe Environmental Protection Agency conducted emissions-certification and representative emissions test in Ann Arbor, Michigan, in 2005. Subsequently, Harley-Davidson produced an \"environmental warranty\". The warranty ensures each owner that the vehicle is designed and built free of any defects in materials and workmanship that would cause the vehicle to not meet EPA standards. In 2005, the EPA and the Pennsylvania Department of Environmental Protection (PADEP) confirmed Harley-Davidson to be the first corporation to voluntarily enroll in the One Clean-Up Program. This program is designed for the clean-up of the affected soil and groundwater at the former York Naval Ordnance Plant. The program is backed by the state and local government along with participating organizations and corporations.\n\nPaul Gotthold, Director of Operations for the EPA, congratulated the motor company:\n\nHarley-Davidson also purchased most of Castalloy, a South Australian producer of cast motorcycle wheels and hubs. The South Australian government has set forth \"protection to the purchaser (Harley-Davidson) against environmental risks\".\n\nIn August 2016 Harley-Davidson settled with the EPA for $12 million, without admitting wrongdoing, over the sale of after-market \"super tuners\". Super tuners were devices, marketed for competition, which enabled increased performance of Harley-Davidson products. However, the devices also modified the emission control systems, producing increased hydrocarbon and nitrogen oxide. Harley-Davidson is required to buy back and destroy any super tuners which do not meet Clean Air Act requirements and spend $3 million on air pollution mitigation.\n\nHarley Davidson is the main sponsor for Vizag Victors in IPL\n\nThe Milwaukee Bucks of the National Basketball Association sport a Harley Davidson sponsor patch on their jerseys.\n\nAccording to a recent Harley-Davidson study, in 1987 half of all Harley riders were under age 35. Now, only 15 percent of Harley buyers are under 35, and as of 2005, the median age had risen to 46.7. In 2008, Harley-Davidson stopped disclosing the average age of riders; at this point it was 48 years old.\n\nIn 1987, the median household income of a Harley-Davidson rider was $38,000. By 1997, the median household income for those riders had more than doubled, to $83,000.\n\nMany Harley-Davidson Clubs exist nowadays around the world, the oldest one, founded in 1928, is in Prague.\n\nHarley-Davidson attracts a loyal brand community, with licensing of the Harley-Davidson logo accounting for almost 5 percent of the company's net revenue ($41 million in 2004). Harley-Davidson supplies many American police forces with their motorcycle fleets.\n\nFrom its founding, Harley-Davidson had worked to brand its motorcycles as respectable and refined products, with ads that showed what motorcycling writer Fred Rau called \"refined-looking ladies with parasols, and men in conservative suits as the target market\". The 1906 Harley-Davidson's effective, and polite, muffler was emphasized in advertisements with the nickname \"The Silent Gray Fellow\". That began to shift in the 1960s, partially in response to the clean-cut motorcyclist portrayed in Honda's \"You meet the nicest people on a Honda\" campaign, when Harley-Davidson sought to draw a contrast with Honda by underscoring the more working-class, macho, and even a little anti-social attitude associated with motorcycling's dark side. With the 1971 FX Super Glide, the company embraced, rather than distanced, itself from chopper style, and the counterculture custom Harley scene. Their marketing cultivated the \"bad boy\" image of biker and motorcycle clubs, and to a point, even outlaw or one-percenter motorcycle clubs.\n\nBeginning in 1920, a team of farm boys, including Ray Weishaar, who became known as the \"hog boys\", consistently won races. The group had a live hog as their mascot. Following a win, they would put the hog on their Harley and take a victory lap. In 1983, the Motor Company formed a club for owners of its product taking advantage of the long-standing nickname by turning \"hog\" into the acronym HOG., for Harley Owners Group. Harley-Davidson attempted to trademark \"hog\", but lost a case against an independent Harley-Davidson specialist, The Hog Farm of West Seneca, New York, in 1999 when the appellate panel ruled that \"hog\" had become a generic term for large motorcycles and was therefore unprotectable as a trademark.\n\nOn August 15, 2006, Harley-Davidson Inc. had its NYSE ticker symbol changed from HDI to HOG.\n\nHarley-Davidson FL \"big twins\" normally had heavy steel fenders, chrome trim, and other ornate and heavy accessories. After World War II, riders wanting more speed would often shorten the fenders or take them off completely to reduce the weight of the motorcycle. These bikes were called \"bobbers\" or sometimes \"choppers\" because parts considered unnecessary were chopped off. Those who made or rode choppers and bobbers, especially members of motorcycle clubs like the Hells Angels, referred to stock FLs as \"garbage wagons\".\n\nHarley-Davidson established the Harley Owners Group (HOG) in 1983 to build on the loyalty of Harley-Davidson enthusiasts as a means to promote a lifestyle alongside its products. The HOG also opened new revenue streams for the company, with the production of tie-in merchandise offered to club members, numbering more than one million. Other motorcycle brands,\nand other and consumer brands outside motorcycling, have also tried to create factory-sponsored community marketing clubs of their own.\nHOG members typically spend 30 percent more than other Harley owners, on such items as clothing and Harley-Davidson-sponsored events.\n\nIn 1991, HOG went international, with the first official European HOG Rally in Cheltenham, England.\nToday, more than one million members and more than 1400 chapters worldwide make HOG the largest factory-sponsored motorcycle organization in the world.\n\nHOG benefits include organized group rides, exclusive products and product discounts, insurance discounts, and the Hog Tales newsletter. A one-year full membership is included with the purchase of a new, unregistered Harley-Davidson.\n\nIn 2008, HOG celebrated its 25th anniversary in conjunction with the Harley 105th in Milwaukee, Wisconsin.\n\n3rd Southern HOG Rally set to bring together largest gathering of Harley-Davidson owners in South India. More than 600 Harley-Davidson Owners expected to ride to Hyderabad from across 13 HOG Chapters \n\nHarley-Davidson offers factory tours at four of its manufacturing sites, and the Harley-Davidson Museum, which opened in 2008, exhibits Harley-Davidson's history, culture, and vehicles, including the motor company's corporate archives.\n\nDue to the consolidation of operations, the Capitol Drive Tour Center in Wauwatosa, Wisconsin was closed in 2009.\n\nBeginning with Harley-Davidson's 90th anniversary in 1993, Harley-Davidson has had celebratory rides to Milwaukee called the \"Ride Home\". This new tradition has continued every five years, and is referred to unofficially as \"Harleyfest\", in line with Milwaukee's other festivals (Summerfest, German fest, Festa Italiana, etc.). This event brings Harley riders from all around the world. The 105th anniversary celebration was held on August 28–31, 2008, and included events in Milwaukee, Waukesha, Racine, and Kenosha counties, in Southeast Wisconsin. The 110th anniversary celebration was held on August 29–31, 2013. The 115th anniversary was held in Prague, Czech Republic, the home country of the first Harley Davidson Club, on July 5–8, 2018 and attracted more than 100.000 visitors and 60.000 bikes.\n\nWilliam S. Harley, Arthur Davidson, William A. Davidson and Walter Davidson, Sr. were inducted into the Labor Hall of Fame for their accomplishments for the H-D company and its workforce.\n\nThe company's origins were dramatized in a 2016 miniseries entitled \"Harley and the Davidsons\", starring Robert Aramayo as William Harley, Bug Hall as Arthur Davidson and Michiel Huisman as Walter Davidson, and premiered on the Discovery Channel as a \"three-night event series\" on September 5, 2016.\n\n\n"}
{"id": "14144", "url": "https://en.wikipedia.org/wiki?curid=14144", "title": "Hiberno-English", "text": "Hiberno-English\n\nHiberno-English (from Latin \"Hibernia\": \"Ireland\") or Irish English is the set of English dialects natively written and spoken within the island of Ireland (including both the Republic of Ireland and Northern Ireland).\n\nEnglish was brought to Ireland as a result of the Norman invasion of Ireland of the late 12th century. Initially, it was mainly spoken in an area known as the Pale around Dublin, with mostly Irish spoken throughout the rest of the country. By the Tudor period, Irish culture and language had regained most of the territory lost to the invaders: even in the Pale, \"all the common folk… for the most part are of Irish birth, Irish habit, and of Irish language\". Some small pockets remained predominantly English-speaking; because of their sheer isolation their dialects developed into later (now extinct) dialects known as Yola in Wexford and Fingallian in Fingal, Dublin. These were no longer mutually intelligible with other English varieties. However, the Tudor conquest and colonisation of Ireland in the 16th century marked a forced decline in the use of the Irish language. By the mid-19th century, English was the majority language spoken in the country. It has retained this status to the present day, with even those whose first language is Irish being fluent in English as well. Today, there is only a little more than one percent of the population that speaks Irish natively. English is one of two official languages, along with Irish, of the Republic of Ireland, and is the country's working language.\n\nHiberno-English's spelling and pronunciation standards align with British rather than American English. However, Hiberno-English's diverse accents and some of its grammatical structures are unique, with some influence by the Irish language and a tendency to be phonologically conservative, retaining older features no longer common in the accents of England or North America.\n\nPhonologists today often divide Hiberno-English into four or five overarching classes of dialects or accents: Ulster accents, West and South-West Region accents (including, for example, the Cork accent), various Dublin accents, and a supraregional accent developing since only the last quarter of the twentieth century.\nUlster English (or Northern Irish English) here refers collectively to the varieties of the Ulster province, including Northern Ireland and neighbouring counties outside of Northern Ireland, which has been influenced by Ulster Irish as well as the Scots language, brought over by Scottish settlers during the Plantation of Ulster. Its main subdivisions are Mid-Ulster English, South Ulster English and Ulster Scots, the latter of which is more directly and strongly influenced by the Scots language. All Ulster English has more obvious pronunciation similarities with Scottish English than other Irish English dialects do.\n\nUlster varieties distinctly pronounce:\n\n\nWest and South-West Irish English here refers to broad varieties of Ireland's West and South-West Regions. Accents of both regions are known for:\n\nSouth-West Irish English (often known, by specific county, as Cork English, Kerry English, or Limerick English) also features two major defining characteristics of its own. One is the pin–pen merger: the raising of to when before or (as in \"again\" or \"pen\"). The other is the intonation pattern of a slightly higher pitch followed by a significant drop in pitch on stressed long-vowel syllables (across multiple syllables or even within a single one), which is popularly heard in rapid conversation, by speakers of other English dialects, as a noticeable kind of undulating \"sing-song\" pattern.\n\n\nDublin English is highly internally diverse and refers collectively to the Irish English varieties immediately surrounding and within the metropolitan area of Dublin. Modern-day Dublin English largely lies on a phonological continuum, ranging from a more traditional, lower-prestige, local urban accent on the one end to a more recently developing, higher-prestige, non-local (regional and even supraregional) accent on the other end, whose most advanced characteristics only first emerged in the late 1980s and 1990s. The accent that most strongly uses the traditional working-class features has been labelled by linguists as local Dublin English. Most speakers from Dublin and its suburbs, however, have accent features falling variously along the entire middle as well as newer end of the spectrum, which together form what is called non-local Dublin English, spoken by middle- and upper-class natives of Dublin and the greater eastern Irish region surrounding the city. A subset of this variety, whose middle-class speakers mostly range in the middle section of the continuum, is called mainstream Dublin English. Mainstream Dublin English has become the basis of an accent that has otherwise become supraregional (see more below) everywhere except in the north of the country. The majority of Dubliners born since the 1980s (led particularly by females) has shifted towards the most innovative non-local accent, here called new Dublin English, which has gained ground over mainstream Dublin English and which is the most extreme variety in rejecting the local accent's traditional features. The varieties at either extreme of the spectrum, local and new Dublin English, are both discussed in further detail below. In the most general terms, all varieties of Dublin English have the following identifying sounds that are often distinct from the rest of Ireland, pronouncing:\n\nLocal Dublin English (or popular Dublin English) here refers to a traditional, broad, working-class variety spoken in the Republic of Ireland's capital city of Dublin. It is the only Irish English variety that in earlier history was non-rhotic; however, it is today weakly rhotic, and it uniquely pronounces:\n\nThe local Dublin accent is also known for a phenomenon called \"vowel breaking\", in which the vowel sounds , , , and in closed syllables are \"broken\" into two syllables, approximating , , , and , respectively.\n\n\nEvolving as a fashionable outgrowth of the mainstream non-local Dublin English, new Dublin English (also, advanced Dublin English and, formerly, fashionable Dublin English) is a youthful variety that originally began in the early 1990s among the \"avant-garde\" and now those aspiring to a non-local \"urban sophistication\". New Dublin English itself, first associated with affluent and middle-class inhabitants of southside Dublin, is probably now spoken by a majority of Dubliners born since the 1980s. It has replaced (yet was largely influenced by) moribund D4 English (often known as \"Dublin 4\" or \"DART speak\" or, mockingly, \"Dortspeak\"), which originated around the 1970s from Dubliners who rejected traditional notions of Irishness, regarding themselves as more trendy and sophisticated; however, particular aspects of the D4 accent became quickly noticed and ridiculed as sounding affected, causing these features to fall out of fashion by the 1990s.\n\nThis \"new mainstream\" accent of Dublin's youth, rejecting traditional working-class Dublin, pronounces:\n\n\nSupraregional southern Irish English (sometimes, simply, supraregional Irish English or supraregional Hiberno-English) here refers to a variety crossing regional boundaries throughout all of the Republic of Ireland, except the north. As mentioned earlier, mainstream Dublin English of the early- to mid-1900s is the direct influence and catalyst for this variety. Most speakers born in the 1980s or later are showing fewer features of the twentieth-century mainstream supraregional form and more characteristics of an advanced supraregional variety that aligns clearly with the rapidly spreading new Dublin accent (see more above, under \"Non-local Dublin English\").\n\nIreland's supraregional dialect pronounces:\n\nThe following charts list the vowels typical of each Irish English dialect as well as the several distinctive consonants of Irish English. Phonological characteristics of overall Irish English are given as well as categorisations into five major divisions of Hiberno-English: northern Ireland (or Ulster); West & South-West Ireland; local Dublin; new Dublin; and supraregional (southern) Ireland. Features of mainstream non-local Dublin English fall on a range between \"local Dublin\" and \"new Dublin\".\n\nThe defining monophthongs of Irish English:\n\nThe following pure vowel sounds are defining characteristics of Irish English:\n\nAll pure vowels of various Hiberno-English dialects:\n\nFootnotes:\nIn southside Dublin's once-briefly fashionable \"Dublin 4\" (or \"Dortspeak\") accent, the \" and broad \" set becomes rounded as [ɒː].\n\nUnstressed syllable-final or is realised in Ulster accents uniquely as .\n\nOther notes:\n\n\nThe defining diphthongs of Hiberno-English:\n\nThe following gliding vowel (diphthong) sounds are defining characteristics of Irish English:\n\nAll diphthongs of various Hiberno-English dialects:\n\nFootnotes:\nDue to the local Dublin accent's phenomenon of \"vowel breaking\", may be realised in that accent as in a closed syllable, and, in the same environment, may be realised as .\n\nThe defining \"r\"-coloured vowels of Hiberno-English:\n\nThe following \"r\"-coloured vowel features are defining characteristics of Hiberno-English: \n\nAll \"r\"-coloured vowels of various Hiberno-English dialects:\n\nFootnotes:\n\nEvery major accent of Irish English is rhotic (pronounces \"r\" after a vowel sound). The local Dublin accent is the only one that during an earlier time was non-rhotic, though it usually very lightly rhotic today, with a few minor exceptions. The rhotic consonant in this and most other Irish accents is an approximant .\n\nThe \"r\" sound of the mainstream non-local Dublin accent is more precisely a velarised approximant , while the \"r\" sound of the more recently emerging non-local Dublin (or \"new Dublin\") accent is more precisely a retroflex approximant .\n\nIn southside Dublin's once-briefly fashionable \"Dublin 4\" (or \"Dortspeak\") accent, is realised as .\n\nIn non-local Dublin's more recently emerging (or \"new Dublin\") accent, and may both be realised more rounded as .\n\nIn local Dublin, West/South-West, and other very conservative and traditional Irish English varieties ranging from the south to the north, the phoneme is split into two distinct phonemes depending on spelling and preceding consonants, which have sometimes been represented as versus , and often more precisely pronounced as versus . As an example, the words \"earn\" and \"urn\" are not pronounced the same, as they are in most dialects of English around the world. In the local Dublin and West/South-West accents, when after a labial consonant (e.g. \"fern\"), when spelled as \"ur\" or \"or\" (e.g. \"word\"), or when spelled as \"ir\" after an alveolar stop (e.g. \"dirt\") are pronounced as ; in all other situations, is pronounced as . Example words include:\n\n\nIn non-local Dublin, younger, and supraregional Irish accents, this split is seldom preserved, with both of the phonemes typically merged as .\n\nIn rare few local Dublin varieties that are non-rhotic, is either lowered to or backed and raised to .\n\nThe distinction between and is widely preserved in Ireland, so that, for example, \"horse\" and \"hoarse\" are not merged in most Irish English dialects; however, they are usually merged in Belfast and new Dublin.\n\nIn local Dublin, due to the phenomenon of \"vowel breaking\" may in fact be realised as .\n\nThe defining consonants of Hiberno-English:\n\nThe consonants of Hiberno-English mostly align to the typical English consonant sounds. However, a few Irish English consonants have distinctive, varying qualities. The following consonant features are defining characteristics of Hiberno-English: \n\nUnique consonants in various Hiberno-English dialects:\n\nFootnotes:\n\nIn traditional, conservative Ulster English, and are palatalised before a low front vowel.\n\nLocal Dublin also undergoes cluster simplification, so that stop consonant sounds occurring after fricatives or sonorants may be left unpronounced, resulting, for example, in \"poun(d)\" and \"las(t)\".\n\nRhoticity: Every major accent of Irish English is strongly rhotic (pronounces \"r\" after a vowel sound), though to a weaker degree with the local Dublin accent. The accents of local Dublin and some smaller eastern towns like Drogheda were historically non-rhotic and now only very lightly rhotic or variably rhotic, with the rhotic consonant being an alveolar approximant, . In extremely traditional and conservative accents (exemplified, for instance, in the speech of older speakers throughout the country, even in South-West Ireland, such as Mícheál Ó Muircheartaigh and Jackie Healy-Rae), the rhotic consonant, before a vowel sound, can also be an alveolar tap, . The rhotic consonant for the northern Ireland and new Dublin accents is a retroflex approximant, . Dublin's retroflex approximant has no precedent outside of northern Ireland and is a genuine innovation of the past two decades. A guttural/uvular is found in north-east Leinster. Otherwise, the rhotic consonant of virtually all other Irish accents is the postalveolar approximant, .\n\nThe symbol [θ̠] is used here to represent the voiceless alveolar non-sibilant fricative, sometimes known as a \"slit fricative\", whose articulation is described as being apico-alveolar.\n\nOverall, and are being increasingly merged in supraregional Irish English, for example, making \"wine\" and \"whine\" homophones, as in most varieties of English around the world.\n\nOther phonological characteristics of Irish English include that consonant clusters ending in before are distinctive:\n\nThe naming of the letter \"H\" as \"haitch\" is standard.\n\nDue to Gaelic influence, an epenthetic schwa is sometimes inserted, perhaps as a feature of older and less careful speakers, e.g. \"film\" and \"form\" .\n\nA number of Irish-language loan words are used in Hiberno-English, particularly in an official state capacity. For example, the head of government is the Taoiseach, the deputy head is the Tánaiste, the parliament is the Oireachtas and its lower house is Dáil Éireann. Less formally, people also use loan words in day-to-day speech, although this has been on the wane in recent decades and among the young.\n\nAnother group of Hiberno-English words are those \"derived\" from the Irish language. Some are words in English that have entered into general use, while others are unique to Ireland. These words and phrases are often Anglicised versions of words in Irish or direct translations into English. In the latter case, they often give a meaning to a word or phrase that is generally not found in wider English use.\n\nAnother class of vocabulary found in Hiberno-English are words and phrases common in Old and Middle English, but which have since become obscure or obsolete in the modern English language generally. Hiberno-English has also developed particular meanings for words that are still in common use in English generally.\n\nIn addition to the three groups above, there are also additional words and phrases whose origin is disputed or unknown. While this group may not be unique to Ireland, their usage is not widespread, and could be seen as characteristic of the language in Ireland.\n\nThe syntax of the Irish language is quite different from that of English. Various aspects of Irish syntax have influenced Hiberno-English, though many of these idiosyncrasies are disappearing in suburban areas and among the younger population.\n\nThe other major influence on Hiberno-English that sets it apart from modern English in general is the retention of words and phrases from Old- and Middle-English.\n\nReduplication is an alleged trait of Hiberno-English strongly associated with Stage Irish and Hollywood films.\n\n\nIrish lacks words that directly translate as \"yes\" or \"no\", and instead repeats the verb used in the question, negated if necessary, to answer. Hiberno-English uses \"yes\" and \"no\" less frequently than other English dialects as speakers can repeat the verb, positively or negatively, instead of (or in redundant addition to) using \"yes\" or \"no\".\n\n\nThis is not limited only to the verb \"to be\": it is also used with \"to have\" when used as an auxiliary; and, with other verbs, the verb \"to do\" is used. This is most commonly used for intensification, especially in Ulster English.\n\nIrish indicates recency of an action by adding \"after\" to the present continuous (a verb ending in \"-ing\"), a construction known as the \"hot news perfect\" or \"after perfect\". The idiom for \"I had done X when I did Y\" is \"I was after doing X when I did Y\", modelled on the Irish usage of the compound prepositions \"i ndiaidh\", \"tar éis\", and \"in éis\": \"bhí mé tar éis/i ndiaidh/in éis X a dhéanamh, nuair a rinne mé Y\".\nA similar construction is seen where exclamation is used in describing a recent event:\n\nWhen describing less astonishing or significant events, a structure resembling the German perfect can be seen:\n\nThis correlates with an analysis of \"H1 Irish\" proposed by Adger & Mitrovic, in a deliberate parallel to the status of German as a V2 language.\n\nThe reflexive version of pronouns is often used for emphasis or to refer indirectly to a particular person, etc., according to context. \"Herself\", for example, might refer to the speaker's boss or to the woman of the house. Use of \"herself\" or \"himself\" in this way often indicates that the speaker attributes some degree of arrogance or selfishness to the person in question. Note also the indirectness of this construction relative to, for example, \"She's coming now\"\n\nThere are some language forms that stem from the fact that there is no verb \"to have\" in Irish. Instead, possession is indicated in Irish by using the preposition \"at\", (in Irish, \"ag.\"). To be more precise, Irish uses a prepositional pronoun that combines \"ag\" \"at\" and \"mé\" \"me\" to create \"agam\".\nIn English, the verb \"to have\" is used, along with a \"with me\" or \"on me\" that derives from \"Tá … agam.\" This gives rise to the frequent\nSomebody who can speak a language \"has\" a language, in which Hiberno-English has borrowed the grammatical form used in Irish.\n\nWhen describing something, many Hiberno-English speakers use the term \"in it\" where \"there\" would usually be used. This is due to the Irish word \"ann\" (pronounced \"oun\" or \"on\") fulfilling both meanings.\n\nAnother idiom is this thing or that thing described as \"this man here\" or \"that man there\", which also features in Newfoundland English in Canada.\n\nConditionals have a greater presence in Hiberno-English due to the tendency to replace the simple present tense with the conditional (would) and the simple past tense with the conditional perfect (would have).\n\nBring and take: Irish use of these words differs from that of British English because it follows the Irish grammar for \"beir\" and \"tóg\". English usage is determined by direction; person determines Irish usage. So, in English, one takes \"\"from\" here \"to\" there\", and brings it \"\"to\" here \"from\" there\". In Irish, a person takes only when accepting a transfer of possession of the object from someone elseand a person brings at all other times, irrespective of direction (to or from).\n\nThe Irish equivalent of the verb \"to be\" has two present tenses, one (the present tense proper or \"aimsir láithreach\") for cases which are generally true or are true at the time of speaking and the other (the habitual present or \"aimsir ghnáthláithreach\") for repeated actions. Thus, \"you are [now, or generally]\" is \"tá tú\", but \"you are [repeatedly]\" is \"bíonn tú\". Both forms are used with the verbal noun (equivalent to the English present participle) to create compound tenses. This is similar to the distinction between \"ser\" and \"estar\" in Spanish.\n\nThe corresponding usage in English is frequently found in rural areas, especially Mayo/Sligo in the west of Ireland and Wexford in the south-east, Inner-City Dublin along with border areas of the North and Republic. In this form, the verb \"to be\" in English is similar to its use in Irish, with a \"does be/do be\" (or \"bees\", although less frequently) construction to indicate the continuous, or habitual, present:\n\nThis construction also surfaces in African American Vernacular English, as the famous habitual be.\n\nIn old-fashioned usage, \"it is\" can be freely abbreviated \"’tis\", even as a standalone sentence. This also allows the double contraction \"’tisn’t\", for \"it is not\".\n\nIrish has separate forms for the second person singular (\"tú\") and the second person plural (\"sibh\").\nMirroring Irish, and almost every other Indo European language, the plural \"you\" is also distinguished from the singular in Hiberno-English, normally by use of the otherwise archaic English word \"ye\" ; the word \"yous\" (sometimes written as \"youse\") also occurs, but primarily only in Dublin and across Ulster. In addition, in some areas in Leinster, north Connacht and parts of Ulster, the hybrid word \"ye-s\", pronounced \"yiz\", may be used. The pronunciation differs with that of the northwestern being and the Leinster pronunciation being .\n\n\nThe word \"ye\", \"yis\" or \"yous\", otherwise archaic, is still used in place of \"you\" for the second-person plural. \"Ye'r\", \"Yisser\" or \"Yousser\" are the possessive forms, e.g. \"Where are yous going?\"\n\nThe verb \"mitch\" is very common in Ireland, indicating being truant from school. This word appears in Shakespeare (though he wrote in Early Modern English rather than Middle English), but is seldom heard these days in British English, although pockets of usage persist in some areas (notably South Wales, Devon, and Cornwall). In parts of Connacht and Ulster the \"mitch\" is often replaced by the verb \"scheme\", while in Dublin it is often replaced by \"on the hop/bounce\".\n\nAnother usage familiar from Shakespeare is the inclusion of the second person pronoun after the imperative form of a verb, as in \"Wife, go you to her ere you go to bed\" (Romeo and Juliet, Act III, Scene IV). This is still common in Ulster: \"Get youse your homework done or you're no goin' out!\" In Munster, you will still hear children being told, \"Up to bed, let ye\" .\n\nFor influence from Scotland, see Ulster Scots and Ulster English.\n\nNow is often used at the end of sentences or phrases as a semantically empty word, completing an utterance without contributing any apparent meaning. Examples include \"Bye now\" (= \"Goodbye\"), \"There you go now\" (when giving someone something), \"Ah now!\" (expressing dismay), \"Hold on now\" (= \"wait a minute\"), \"Now then\" as a mild attention-getter, etc. This usage is universal among English dialects, but occurs more frequently in Hiberno-English. It is also used in the manner of the Italian 'prego' or German 'bitte', for example a barman might say \"Now, Sir.\" when delivering drinks.\n\nSo is often used for emphasis (\"I can speak Irish, so I can\"), or it may be tacked onto the end of a sentence to indicate agreement, where \"then\" would often be used in Standard English (\"Bye so\", \"Let's go so\", \"That's fine so\", \"We'll do that so\"). The word is also used to contradict a negative statement (\"You're not pushing hard enough\" – \"I am so!\"). (This contradiction of a negative is also seen in American English, though not as often as \"I am too\", or \"Yes, I am\".) The practice of indicating emphasis with \"so\" and including reduplicating the sentence's subject pronoun and auxiliary verb (is, are, have, has, can, etc.) such as in the initial example, is particularly prevalent in more northern dialects such as those of Sligo, Mayo and the counties of Ulster.\n\nSure is often used as a tag word, emphasising the obviousness of the statement, roughly translating as but/and/well. Can be used as \"to be sure\", the famous Irish stereotype phrase. (But note that the other stereotype of \"Sure and …\" is not actually used in Ireland.) Or \"Sure, I can just go on Wednesday\", \"I will not, to be sure.\" The word is also used at the end of sentences (primarily in Munster), for instance \"I was only here five minutes ago, sure!\" and can express emphasis or indignation.\n\nTo is often omitted from sentences where it would exist in British English. For example, \"I'm not let go out tonight\", instead of \"I'm not allowed \"to\" go out tonight\".\n\nWill is often used where British English would use \"shall\" or American English \"should\" (as in \"Will I make us a cup of tea?\"). The distinction between \"shall\" (for first-person simple future, and second- and third-person emphatic future) and \"will\" (second- and third-person simple future, first-person emphatic future), maintained by many in England, does not exist in Hiberno-English, with \"will\" generally used in all cases.\n\nOnce is sometimes used in a different way from how it is used in other dialects; in this usage, it indicates a combination of logical and causal conditionality: \"I have no problem laughing at myself once the joke is funny.\" Other dialects of English would probably use \"if\" in this situation.\n\n\n"}
{"id": "14147", "url": "https://en.wikipedia.org/wiki?curid=14147", "title": "Harmonic analysis", "text": "Harmonic analysis\n\nHarmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of Fourier series and Fourier transforms (i.e. an extended form of Fourier analysis). In the past two centuries, it has become a vast subject with applications in areas as diverse as number theory, representation theory, signal processing, quantum mechanics, tidal analysis and neuroscience.\n\nThe term \"harmonics\" originated as the Ancient Greek word \"harmonikos\", meaning \"skilled in music\". In physical eigenvalue problems, it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes, but the term has been generalized beyond its original meaning.\n\nThe classical Fourier transform on R is still an area of ongoing research, particularly concerning Fourier transformation on more general objects such as tempered distributions. For instance, if we impose some requirements on a distribution \"f\", we can attempt to translate these requirements in terms of the Fourier transform of \"f\". The Paley–Wiener theorem is an example of this. The Paley–Wiener theorem immediately implies that if \"f\" is a nonzero distribution of compact support (these include functions of compact support), then its Fourier transform is never compactly supported. This is a very elementary form of an uncertainty principle in a harmonic-analysis setting. See also: Convergence of Fourier series.\n\nFourier series can be conveniently studied in the context of Hilbert spaces, which provides a connection between harmonic analysis and functional analysis.\n\nMany applications of harmonic analysis in science and engineering begin with the idea or hypothesis that a phenomenon or signal is composed of a sum of individual oscillatory components. Ocean tides and vibrating strings are common and simple examples. The theoretical approach is often to try to describe the system by a differential equation or system of equations to predict the essential features, including the amplitude, frequency, and phases of the oscillatory components. The specific equations depend on the field, but theories generally try to select equations that represent major principles that are applicable.\n\nThe experimental approach is usually to acquire data that accurately quantifies the phenomenon. For example, in a study of tides, the experimentalist would acquire samples of water depth as a function of time at closely enough spaced intervals to see each oscillation and over a long enough duration that multiple oscillatory periods are likely included. In a study on vibrating strings, it is common for the experimentalist to acquire a sound waveform sampled at a rate at least twice that of the highest frequency expected and for a duration many times the period of the lowest frequency expected.\n\nFor example, the top signal at the right is a sound waveform of a bass guitar playing an open string corresponding to an A note with a fundamental frequency of 55 Hz. The waveform appears oscillatory, but it is more complex than a simple sine wave, indicating the presence of additional waves. The different wave components contributing to the sound can be revealed by applying a mathematical analysis technique known as the Fourier transform, the result of which is shown in the lower figure. Note that there is a prominent peak at 55 Hz, but that there are other peaks at 110 Hz, 165 Hz, and at other frequencies corresponding to integer multiples of 55 Hz. In this case, 55 Hz is identified as the fundamental frequency of the string vibration, and the integer multiples are known as harmonics.\n\nOne of the most modern branches of harmonic analysis, having its roots in the mid-20th century, is analysis on topological groups. The core motivating ideas are the various Fourier transforms, which can be generalized to a transform of functions defined on Hausdorff locally compact topological groups.\n\nThe theory for abelian locally compact groups is called Pontryagin duality.\n\nHarmonic analysis studies the properties of that duality and Fourier transform and attempts to extend those features to different settings, for instance, to the case of non-abelian Lie groups.\n\nFor general non-abelian locally compact groups, harmonic analysis is closely related to the theory of unitary group representations. For compact groups, the Peter–Weyl theorem explains how one may get harmonics by choosing one irreducible representation out of each equivalence class of representations. This choice of harmonics enjoys some of the useful properties of the classical Fourier transform in terms of carrying convolutions to pointwise products, or otherwise showing a certain understanding of the underlying group structure. See also: Non-commutative harmonic analysis.\n\nIf the group is neither abelian nor compact, no general satisfactory theory is currently known (\"satisfactory\" means at least as strong as the Plancherel theorem). However, many specific cases have been analyzed, for example SL. In this case, representations in infinite dimensions play a crucial role.\n\n\n\n"}
{"id": "14148", "url": "https://en.wikipedia.org/wiki?curid=14148", "title": "Home run", "text": "Home run\n\nIn baseball, a home run (abbreviated HR) is scored when the ball is hit in such a way that the batter is able to circle the bases and reach home \nsafely in one play without any errors being committed by the defensive team in the process. In modern baseball, the feat is typically achieved by hitting the ball over the outfield fence between the foul poles (or making contact with either foul pole) without first touching the ground, resulting in an automatic home run. There is also the \"inside-the-park\" home run where the batter reaches home safely while the baseball is in play on the field.\n\nWhen a home run is scored, the batter is also credited with a hit and a run scored, and an RBI for each runner that scores, including himself. Likewise, the pitcher is recorded as having given up a hit, and a run for each runner that scores including the batter.\n\nHome runs are among the most popular aspects of baseball and, as a result, prolific home run hitters are usually the most popular among fans and consequently the highest paid by teams—hence the old saying, \"Home run hitters drive Cadillacs, and singles hitters drive Fords (coined, circa 1948, by veteran pitcher Fritz Ostermueller, by way of mentoring his young teammate, Ralph Kiner).\n\nIn modern times a home run is most often scored when the ball is hit over the outfield wall between the foul poles (in fair territory) before it touches the ground (in flight), and without being caught or deflected back onto the field by a fielder. A batted ball is also a home run if it touches either foul pole or its attached screen before touching the ground, as the foul poles are by definition in fair territory. Additionally, many major-league ballparks have ground rules stating that a batted ball in flight that strikes a specified location or fixed object is a home run; this usually applies to objects that are beyond the outfield wall but are located such that it may be difficult for an umpire to judge.\n\nIn professional baseball, a batted ball that goes over the outfield wall \"after\" touching the ground (i.e. a ball that bounces over the outfield wall) becomes an automatic double. This is colloquially referred to as a \"ground rule double\" because the rule is not strictly written into the rules of baseball, but is rather a rule of the field (or \"grounds\") being used.\n\nA fielder is allowed to reach over the wall to attempt to catch the ball as long as his feet are on or over the field during the attempt, and if the fielder successfully catches the ball while it is in flight the batter is out, even if the ball had already passed the vertical plane of the wall. However, since the fielder is not part of the field, a ball that bounces off a fielder (including his glove) and over the wall without touching the ground is still a home run. A fielder may not deliberately throw his glove, cap, or any other equipment or apparel to stop or deflect a fair ball, and an umpire may award a home run to the batter if a fielder does so on a ball that, in the umpire's judgment, would have otherwise been a home run (this is rare in modern professional baseball).\n\nA home run accomplished in any of the above manners is an automatic home run. The ball is dead, even if it rebounds back onto the field (e.g., from striking a foul pole), and the batter and any preceding runners cannot be put out at any time while running the bases. However, if one or more runners fail to touch a base or one runner passes another before reaching home plate, that runner or runners can be called out on appeal, though in the case of not touching a base a runner can go back and touch it if doing so won't cause them to be passed by another preceding runner and they have not yet touched the next base (or home plate in the case of missing third base). This stipulation is in Approved Ruling (2) of Rule 7.10(b).\n\nAn inside-the-park home run occurs when a batter hits the ball into play and is able to circle the bases before the fielders can put him out. Unlike with an outside-the-park home run, the batter-runner and all preceding runners are liable to be put out by the defensive team at any time while running the bases. This can only happen if the ball does not leave the ballfield.\n\nIn the early days of baseball, outfields were relatively much more spacious, reducing the likelihood of an over-the-fence home run, while increasing the likelihood of an inside-the-park home run, as a ball getting past an outfielder had more distance that it could roll before a fielder could track it down.\n\nModern outfields are much less spacious and more uniformly designed than in the game's early days, therefore inside-the-park home runs are now a rarity. They usually occur when a fast runner hits the ball deep into the outfield and the ball bounces in an unexpected direction away from the nearest outfielder (e.g., off a divot in the grass or off the outfield wall), or an outfielder misjudges the flight of the ball in a way that he cannot quickly recover from the mistake (e.g., by diving and missing). The speed of the runner is crucial as even triples are relatively rare in most modern ballparks.\n\nIf any defensive play on an inside-the-park home run is labeled an error by the official scorer, a home run is not scored; instead, it is scored as a single, double, or triple, and the batter-runner and any applicable preceding runners are said to have taken all additional bases on error. All runs scored on such a play, however, still count.\n\nAn example of an unexpected bounce occurred during the 2007 Major League Baseball All-Star Game at AT&T Park in San Francisco on July 10, 2007. Ichiro Suzuki of the American League team hit a fly ball that caromed off the right-center field wall in the opposite direction from where National League right fielder Ken Griffey, Jr. was expecting it to go. By the time the ball was relayed, Ichiro had already crossed the plate standing up. This was the first inside-the-park home run in All-Star Game history, and led to Suzuki being named the game's Most Valuable Player.\n\nHome runs are often characterized by the number of runners on base at the time. A home run hit with the bases empty is seldom called a \"one-run homer\", but rather a solo home run, solo homer, or \"solo shot\". With one runner on base, two runs are scored (the baserunner and the batter) and thus the home run is often called a two-run homer or two-run shot. Similarly, a home runs with two runners on base is a three-run homer or three-run shot.\n\nThe term \"four-run homer\" is seldom used; instead, it is nearly always called a \"grand slam\". Hitting a grand slam is the best possible result for the batter's turn at bat and the worst possible result for the pitcher and his team.\n\nA grand slam occurs when the bases are \"loaded\" (that is, there are base runners standing at first, second, and third base) and the batter hits a home run. According to \"The Dickson Baseball Dictionary\", the term originated in the card game of contract bridge. An inside-the-park grand slam is a grand slam that is also an inside-the-park home run, a home run without the ball leaving the field, and it is very rare, due to the relative rarity of loading the bases along with the significant rarity (nowadays) of inside-the-park home runs.\n\nOn July 25, 1956, Roberto Clemente became the only MLB player to have ever scored a walk-off inside-the-park grand slam in a 9–8 Pittsburgh Pirates win over the Chicago Cubs, at Forbes Field.\n\nOn April 23, 1999, Fernando Tatís made history by hitting two grand slams in one inning, both against Chan Ho Park of the Los Angeles Dodgers. With this feat, Tatís also set a Major League record with 8 RBI in one inning.\n\nOn July 29, 2003 against the Texas Rangers, Bill Mueller of the Boston Red Sox became the only player in major league history to hit two grand slams in one game from opposite sides of the plate. In fact, he hit three home runs in that game, and his two grand slams were in consecutive at-bats.\n\nOn August 25, 2011 the New York Yankees became the first team to hit three grand slams in one game vs the Oakland A's. The Yankees eventually went on to win the game 22–9, after trailing 7–1.\n\nThese types of home runs are characterized by the specific game situation in which they occur, and can theoretically occur on either an outside-the-park or inside-the-park home run.\n\nA walk-off home run is a home run hit by the home team in the bottom of the ninth inning, any extra inning, or other scheduled final inning, which gives the home team the lead and thereby ends the game. The term is attributed to Hall of Fame relief pitcher Dennis Eckersley, so named because after the run is scored, the losing team has to \"walk off\" the field.\n\nTwo World Series have ended via the \"walk-off\" home run. The first was the 1960 World Series when Bill Mazeroski of the Pittsburgh Pirates hit a 9th inning solo home run in the 7th game of the series off New York Yankees pitcher Ralph Terry to give the Pirates the World Championship. The second time was the 1993 World Series when Joe Carter of the Toronto Blue Jays hit a 9th inning 3-run home run off Philadelphia Phillies pitcher Mitch Williams in Game 6 of the series, to help the Toronto Blue Jays capture their second World Series Championship in a row.\n\nSuch a home run can also be called a \"sudden death\" or \"sudden victory\" home run. That usage has lessened as \"walk-off home run\" has gained favor. Along with Mazeroski's 1960 shot, the most famous walk-off or sudden-death homer would probably be the \"Shot Heard 'Round the World\" hit by Bobby Thomson to win the 1951 National League pennant for the New York Giants, along with many other game-ending home runs that famously ended some of the most important and suspenseful baseball games.\n\nA walk-off home run over the fence is an exception to baseball's one-run rule. Normally if the home team is tied or behind in the ninth or extra innings the game ends as soon as the home team scores enough runs to achieve a lead. If the home team has two outs in the inning, and the game is tied, the game will officially end either the moment the batter successfully reaches 1st base or the moment the runner touches home plate—whichever happens last. However, this is superseded by the \"ground rule\", which provides automatic doubles (when a ball-in-play hits the ground first then leaves the playing field) and home runs (when a ball-in-play leaves the playing field without ever touching the ground). In the latter case, all base runners including the batter are allowed to cross the plate.\n\nA leadoff home run is a home run hit by the first batter of a team, the leadoff hitter of the first inning of the game. In MLB, Rickey Henderson holds the career record with 81 lead-off home runs.\n\nIn 1996, Brady Anderson set a Major League record by hitting a lead-off home run in four consecutive games.\n\nWhen two consecutive batters each hit a home run, this is described as back-to-back home runs. It is still considered back-to-back even if both batters hit their home runs off different pitchers. A third batter hitting a home run is commonly referred to as back-to-back-to-back.\n\nFour home runs in a row by consecutive batters has only occurred eight times in the history of Major League Baseball. Following convention, this is called back-to-back-to-back-to-back. The most recent occurrence was on July 27, 2017, when the Washington Nationals hit four in a row against the Milwaukee Brewers in Nationals Park as Brian Goodwin, Wilmer Difo, Bryce Harper and Ryan Zimmerman homered off pitcher Michael Blazek. Blazek became the fourth pitcher to surrender back-to-back-to-back-to-back home runs, following Paul Foytack on July 31, 1963, Chase Wright on April 22, 2007, and Dave Bush.\n\nOn August 14, 2008, the Chicago White Sox defeated the Kansas City Royals 9-2. In this game, Jim Thome, Paul Konerko, Alexei Ramírez, and Juan Uribe hit back-to-back-to-back-to-back home runs in that order. Thome, Konerko, and Ramirez blasted their homers off of Joel Peralta, while Uribe did it off of Rob Tejeda. The next batter, veteran backstop Toby Hall, tried aimlessly to hit the ball as far as possible, but his effort resulted in a strike out.\n\nOn April 22, 2007 the Boston Red Sox were trailing the New York Yankees 3–0 when Manny Ramirez, J. D. Drew, Mike Lowell and Jason Varitek hit back-to-back-to-back-to-back home runs to put them up 4–3. They eventually went on to win the game 7–6 after a three-run home run by Mike Lowell in the bottom of the 7th inning. On September 18, 2006 trailing 9–5 to the San Diego Padres in the 9th inning, Jeff Kent, J. D. Drew, Russell Martin, and Marlon Anderson of the Los Angeles Dodgers hit back-to-back-to-back-to-back home runs to tie the game. After giving up a run in the top of the 10th, the Dodgers won the game in the bottom of the 10th, on a walk-off two run home run by Nomar Garciaparra. J. D. Drew has been part of two different sets of back-to-back-to-back-to-back home runs. In both occurrences, his homer was the second of the four.\n\nOn September 30, 1997, in the sixth inning of Game One of the American League Division Series between the New York Yankees and Cleveland Indians, Tim Raines, Derek Jeter and Paul O'Neill hit back-to-back-to-back home runs for the Yankees. Raines' home run tied the game. New York went on to win 8–6. This was the first occurrence of three home runs in a row ever in postseason play. The Boston Red Sox repeated the feat in Game Four of the 2007 American League Championship Series, also against the Indians. The Indians returned the favor in Game One of the 2016 American League Division Series.\n\nTwice in MLB history have two brothers hit back-to-back home runs. On April 23, 2013, brothers Melvin Upton, Jr. (formerly B.J. Upton) and Justin Upton hit back-to-back home runs. The first time was on September 15, 1938, when Lloyd Waner and Paul Waner performed the feat.\n\nSimple back-to-back home runs are a relatively frequent occurrence. If a pitcher gives up a homer, he might have his concentration broken and might alter his normal approach in an attempt to \"make up for it\" by striking out the next batter with some fastballs. Sometimes the next batter will be expecting that and will capitalize on it. A notable back-to-back home run of that type in World Series play involved \"Babe Ruth's called shot\" in 1932, which was accompanied by various Ruthian theatrics, yet the pitcher, Charlie Root, was allowed to stay in the game. He delivered just one more pitch, which Lou Gehrig drilled out of the park for a back-to-back shot, after which Root was removed from the game.\n\nIn Game 3 of the 1976 NLCS, George Foster and Johnny Bench hit back-to-back homers in the last of the ninth off Ron Reed to tie the game. The Series-winning run was scored later in the inning.\n\nAnother notable pair of back-to-back home runs occurred on September 14, 1990, when Ken Griffey, Sr. and Ken Griffey, Jr. hit back-to-back home runs, off Kirk McCaskill, the only father-and-son duo to do so in Major League history.\n\nOn May 2, 2002, Bret Boone and Mike Cameron of the Seattle Mariners hit back-to-back home runs off of starter Jon Rauch in the first inning of a game against the Chicago White Sox. The Mariners batted around in the inning, and Boone and Cameron came up to bat against reliever Jim Parque with two outs, again hitting back-to-back home runs and becoming the only pair of teammates to hit back-to-back home runs twice in the same inning.\n\nOn June 19, 2012, José Bautista and Colby Rasmus hit back-to-back home runs and back-to-back-to-back home runs with Edwin Encarnación for a lead change in each instance.\n\nOn July 23, 2017 Whit Merrifield, Jorge Bonifacio, and Eric Hosmer of the Kansas City Royals hit back to back to back home runs in the fourth inning against the Chicago White Sox. The Royals went on to win the game 5-4.\n\nOn June 20, 2018 George Springer, Alex Bregman, and José Altuve of the Houston Astros hit back to back to back home runs in the sixth inning against the Tampa Bay Rays. The Astros went on to win the game 5-1.\n\nOn April 3, 2018, the St. Louis Cardinals began the game against the Milwaukee Brewers with back-to-back homers from Dexter Fowler and Tommy Pham. Then in the bottom of the ninth, with two outs and the Cardinals leading 4-3, Christian Yelich homered to tie the game; and Ryan Braun hit the next pitch for a walk-off homer. This is the only major league game to begin and end with back-to-back homers.\n\nThe record for consecutive home runs by a batter under any circumstances is four. Of the sixteen players (through 2012) who have hit four in one game, six have hit them consecutively. Twenty-eight other batters have hit four consecutive across two games.\n\nBases on balls do not count as at-bats, and Ted Williams holds the record for consecutive home runs across the most games, four in four games played, during September 17–22, 1957, for the Red Sox. Williams hit a pinch-hit homer on the 17th; walked as a pinch-hitter on the 18th; there was no game on the 19th; hit another pinch-homer on the 20th; homered and then was lifted for a pinch-runner after at least one walk, on the 21st; and homered after at least one walk on the 22nd. All in all, he had four walks interspersed among his four homers.\n\nIn World Series play, Reggie Jackson hit a record three in one Series game, the final game (Game 6) in 1977. But those three were a part of a much more impressive feat. He walked on four pitches in the second inning of game 6. Then he hit his three home runs on the first pitch of his next three at bats, off of three different pitchers (4th inning- Hooten, 5th inning- Sosa, 8th inning- Hough). He had also hit one in his last at bat of the previous game, giving him four home runs on four consecutive swings. (His home run in game 5 was also hit on the first pitch, although this did not add to any significant streak.) The four in a row set the record for consecutive homers across two Series games.\n\nIn Game 3 of the World Series in 2011, Albert Pujols hit three home runs to tie the record with Babe Ruth and Reggie Jackson. The St. Louis Cardinals went on to win the World Series in Game 7 at Busch Stadium. In Game 1 of the World Series in 2012, Pablo Sandoval of the San Francisco Giants hit three home runs on his first three at-bats of the Series, also tying the record with Pujols, Jackson, and Ruth.\n\nNomar Garciaparra holds the record for consecutive home runs in the shortest time in terms of innings: three homers in two innings, on July 23, 2002, for the Boston Red Sox.\n\nAn offshoot of hitting for the cycle, a \"home run cycle\" is where a player hits a solo, 2-run, 3-run, and grand slam all in one game. This is an extremely rare feat, as it requires the batter to not only hit four home runs in a game (which itself has only occurred 18 times in the Major Leagues), but also to hit those home runs with the specific number of runners already on base. Although it is a rare accomplishment, it is largely dependent on circumstances outside the player's control, such as his preceding teammates' ability to get on base, as well as the order in which he comes to bat in any particular inning.\n\nAnother variant of the home run cycle would be the \"natural home run cycle\", which would require a batter to hit a solo, 2-run, 3-run, and grand slam in that order.\n\nThough multiple home run cycles have been recorded in collegiate baseball, the only home run cycle in a professional baseball game belongs to Tyrone Horne, who stroked four long balls for the minor league, Double-A Arkansas Travelers in a game against the San Antonio Missions on July 27, 1998.\n\nA major league player has come close to hitting for the home run cycle several times. Recent examples include:\n\nIn the early days of the game, when the ball was less lively and the ballparks generally had very large outfields, most home runs were of the inside-the-park variety. The first home run ever hit in the National League was by Ross Barnes of the Chicago White Stockings (now known as the Chicago Cubs), in 1876. The home \"run\" was literally descriptive. Home runs over the fence were rare, and only in ballparks where a fence was fairly close. Hitters were discouraged from trying to hit home runs, with the conventional wisdom being that if they tried to do so they would simply fly out. This was a serious concern in the 19th century, because in baseball's early days a ball caught after one bounce was still an out. The emphasis was on place-hitting and what is now called \"manufacturing runs\" or \"small ball\".\n\nThe home run's place in baseball changed dramatically when the live-ball era began after World War I. First, the materials and manufacturing processes improved significantly, making the now-mass-produced, cork-centered ball somewhat more lively. Batters such as Babe Ruth and Rogers Hornsby took full advantage of rules changes that were instituted during the 1920s, particularly prohibition of the spitball, and the requirement that balls be replaced when worn or dirty. These changes resulted in the baseball being easier to see and hit, and easier to hit out of the park. Meanwhile, as the game's popularity boomed, more outfield seating was built, shrinking the size of the outfield and increasing the chances of a long fly ball resulting in a home run. The teams with the sluggers, typified by the New York Yankees, became the championship teams, and other teams had to change their focus from the \"inside game\" to the \"power game\" in order to keep up.\n\nBefore 1931, a ball that bounced over an outfield fence during a major league game was considered a home run. The rule was changed to require the ball to clear the fence on the fly, and balls that reached the seats on a bounce became ground rule doubles in most parks. A carryover of the old rule is that if a player deflects a ball over the outfield fence without it touching the ground, it is a home run.\n\nAlso, until approximately that time, the ball had to not only go over the fence in fair territory, but to land in the bleachers in fair territory or to still be visibly fair when disappearing behind a wall. The rule stipulated \"fair when last seen\" by the umpires. Photos from that era in ballparks, such as the Polo Grounds and Yankee Stadium, show ropes strung from the foul poles to the back of the bleachers, or a second \"foul pole\" at the back of the bleachers, in a straight line with the foul line, as a visual aid for the umpire. Ballparks still use a visual aid much like the ropes; a net or screen attached to the foul poles on the fair side has replaced ropes. As with American football, where a touchdown once required a literal \"touch down\" of the ball in the end zone but now only requires the \"breaking of the [vertical] plane\" of the goal line, in baseball the ball need only \"break the plane\" of the fence in fair territory (unless the ball is caught by a player who is in play, in which case the batter is called out).\n\nBabe Ruth's 60th home run in 1927 was somewhat controversial, because it landed barely in fair territory in the stands down the right field line. Ruth lost a number of home runs in his career due to the when-last-seen rule. Bill Jenkinson, in \"The Year Babe Ruth Hit 104 Home Runs\", estimates that Ruth lost at least 50 and as many as 78 in his career due to this rule.\n\nFurther, the rules once stipulated that an over-the-fence home run in a sudden-victory situation would only count for as many bases as was necessary to \"force\" the winning run home. For example, if a team trailed by two runs with the bases loaded, and the batter hit a fair ball over the fence, it only counted as a triple, because the runner immediately ahead of him had technically already scored the game-winning run. That rule was changed in the 1920s as home runs became increasingly frequent and popular. Babe Ruth's career total of 714 would have been one higher had that rule not been in effect in the early part of his career.\n\nMajor League Baseball keeps running totals of all-time home runs by team, including teams no longer active (prior to 1900) as well as by individual players. Gary Sheffield hit the 250,000th home run in MLB history with a grand slam on September 8, 2008. Sheffield had hit MLB's 249,999th home run against Gio González in his previous at-bat.\n\nThe all-time, verified professional baseball record for career home runs for one player, excluding the U. S. Negro Leagues during the era of segregation, is held by Sadaharu Oh. Oh spent his entire career playing for the Yomiuri Giants in Japan's Nippon Professional Baseball, later managing the Giants, the Fukuoka SoftBank Hawks and the 2006 World Baseball Classic Japanese team. Oh holds the all-time home run world record, having hit 868 home runs in his career.\n\nIn Major League Baseball, the career record is 762, held by Barry Bonds, who broke Hank Aaron's record on August 7, 2007, when he hit his 756th home run at AT&T Park off pitcher Mike Bacsik. Only eight other major league players have hit as many as 600: Hank Aaron (755), Babe Ruth (714), Alex Rodriguez (696), Willie Mays (660), Albert Pujols (633), Ken Griffey, Jr. (630), Jim Thome (612), and Sammy Sosa (609).\n\nThe single season record is 73, set by Barry Bonds in 2001. Other notable single season records were achieved by Babe Ruth who hit 60 in 1927, Roger Maris, with 61 home runs in 1961, and Mark McGwire, who hit 70 in 1998.\n\nNegro League slugger Josh Gibson's Baseball Hall of Fame plaque says he hit \"almost 800\" home runs in his career. The \"Guinness Book of World Records\" lists Gibson's lifetime home run total at 800. Ken Burns' award-winning series, \"Baseball\", states that his actual total may have been as high as 950. Gibson's true total is not known, in part due to inconsistent record keeping in the Negro Leagues. The 1993 edition of the MacMillan \"Baseball Encyclopedia\" attempted to compile a set of Negro League records, and subsequent work has expanded on that effort. Those records demonstrate that Gibson and Ruth were of comparable power. The 1993 book had Gibson hitting 146 home runs in the 501 \"official\" Negro League games they were able to account for in his 17-year career, about 1 homer every 3.4 games. Babe Ruth, in 22 seasons (several of them in the dead-ball era), hit 714 in 2503 games, or 1 homer every 3.5 games. The large gap in the numbers for Gibson reflect the fact that Negro League clubs played relatively far fewer league games and many more \"barnstorming\" or exhibition games during the course of a season, than did the major league clubs of that era.\n\nOther legendary home run hitters include Jimmie Foxx, Mel Ott, Ted Williams, Mickey Mantle (who on September 10, 1960, mythically hit \"the longest home run ever\" at an estimated distance of , although this was measured after the ball stopped rolling), Reggie Jackson, Harmon Killebrew, Ernie Banks, Mike Schmidt, Dave Kingman, Sammy Sosa (who hit 60 or more home runs in a season 3 times), Ken Griffey, Jr. and Eddie Mathews. In 1987, Joey Meyer of the Denver Zephyrs hit the longest verifiable home run in professional baseball history. The home run was measured at a distance of and was hit inside Denver's Mile High Stadium. Major League Baseball's longest verifiable home run distance is about , by Babe Ruth, to straightaway center field at Tiger Stadium (then called Navin Field and before the double-deck), which landed nearly across the intersection of Trumbull and Cherry.\n\nThe location of where Hank Aaron's record 755th home run landed has been monumented in Milwaukee. The spot sits outside Miller Park, where the Milwaukee Brewers currently play. Similarly, the point where Aaron's 715th homer landed, upon breaking Ruth's career record in 1974, is marked in the Turner Field parking lot. A red-painted seat in Fenway Park marks the landing place of the 502-ft home run Ted Williams hit in 1946, the longest measured homer in Fenway's history; a red stadium seat mounted on the wall of the Mall of America in Bloomington, Minnesota, marks the landing spot of Harmon Killebrew's record 520-foot shot in old Metropolitan Stadium.\n\nReplays \"to get the call right\" have been used extremely sporadically in the past, but the use of instant replay to determine \"boundary calls\"—home runs and foul balls—was not officially allowed until 2008.\n\nIn a game on May 31, 1999, involving the St. Louis Cardinals and Florida Marlins, a hit by Cliff Floyd of the Marlins was initially ruled a double, then a home run, then was changed back to a double when umpire Frank Pulli decided to review video of the play. The Marlins protested that video replay was not allowed, but while the National League office agreed that replay was not to be used in future games, it declined the protest on the grounds it was a judgment call, and the play stood.\n\nIn November 2007, the general managers of Major League Baseball voted in favor of implementing instant replay reviews on boundary home run calls. The proposal limited the use of instant replay to determining whether a boundary/home run call is:\n\nOn August 28, 2008, instant replay review became available in MLB for reviewing calls in accordance with the above proposal. It was first utilized on September 3, 2008 in a game between the New York Yankees and the Tampa Bay Rays at Tropicana Field. Alex Rodriguez of the Yankees hit what appeared to be a home run, but the ball hit a catwalk behind the foul pole. It was at first called a home run, until Tampa Bay manager Joe Maddon argued the call, and the umpires decided to review the play. After 2 minutes and 15 seconds, the umpires came back and ruled it a home run.\n\nAbout two weeks later, on September 19, also at Tropicana Field, a boundary call was overturned for the first time. In this case, Carlos Peña of the Rays was given a ground rule double in a game against the Minnesota Twins after an umpire believed a fan reached into the field of play to catch a fly ball in right field. The umpires reviewed the play, determined the fan did not reach over the fence, and reversed the call, awarding Peña a home run.\n\nAside from the two aforementioned reviews at Tampa Bay, replay was used four more times in the 2008 MLB regular season: twice at Houston, once at Seattle, and once at San Francisco. The San Francisco incident is perhaps the most unusual. Bengie Molina, the Giants' catcher, hit what was first called a single. Molina then was replaced in the game by Emmanuel Burriss, a pinch-runner, before the umpires re-evaluated the call and ruled it a home run. In this instance though, Molina was not allowed to return to the game to complete the run, as he had already been replaced. Molina was credited with the home run, and two RBIs, but not for the run scored which went to Burriss instead.\n\nOn October 31, 2009, in the fourth inning of Game 3 of the World Series, Alex Rodriguez hit a long fly ball that appeared to hit a camera protruding over the wall and into the field of play in deep left field. The ball ricocheted off the camera and re-entered the field, initially ruled a double. However, after the umpires consulted with each other after watching the instant replay, the hit was ruled a home run, marking the first time an instant replay home run was hit in a playoff game.\n\n\nCareer achievements\n\n"}
{"id": "14149", "url": "https://en.wikipedia.org/wiki?curid=14149", "title": "Harappa", "text": "Harappa\n\nHarappa (; Urdu/) is an archaeological site in Punjab, Pakistan, about west of Sahiwal. The site takes its name from a modern village located near the former course of the Ravi River which now runs in north. The current village of Harappa is less than from the ancient site. Although modern Harappa has a legacy railway station from the period of the British Raj, it is a small crossroads town of 15,000 people today.\n\nThe site of the ancient city contains the ruins of a Bronze Age fortified city, which was part of the Indus Valley Civilization centered in Sindh and the Punjab, and then the Cemetery H culture. The city is believed to have had as many as 23,500 residents and occupied about with clay brick houses at its greatest extent during the Mature Harappan phase (2600–1900 BC), which is considered large for its time. Per archaeological convention of naming a previously unknown civilization by its first excavated site, the Indus Valley Civilization is also called the Harappan Civilization.\n\nThe ancient city of Harappa was heavily damaged under British rule, when bricks from the ruins were used as track ballast in the construction of the Lahore–Multan Railway. In 2005, a controversial amusement park scheme at the site was abandoned when builders unearthed many archaeological artifacts during the early stages of building work. A plea from the Pakistani archaeologist Mohit Prem Kumar to the Ministry of Culture resulted in a restoration of the site.\n\nThe Harappan Civilization has its earliest roots in cultures such as that of Mehrgarh, approximately 6000 BC. The two greatest cities, Mohenjo-daro and Harappa, emerged circa 2600 BC along the Indus River valley in Punjab and Sindh. The civilization, with a possible writing system, urban centers, and diversified social and economic system, was rediscovered in the 1920s also after excavations at Mohenjo-daro in Sindh near Larkana, and Harappa, in west Punjab south of Lahore. A number of other sites stretching from the Himalayan foothills in east Punjab, India in the north, to Gujarat in the south and east, and to Pakistani Balochistan in the west have also been discovered and studied. Although the archaeological site at Harappa was damaged in 1857 when engineers constructing the Lahore-Multan railroad (as part of the Sind and Punjab Railway), used brick from the Harappa ruins for track ballast, an abundance of artifacts have nevertheless been found. The bricks discovered were made of red sand, clay, stones and were baked at very high temperature. As early as 1826 Harappa, located in west Punjab, attracted the attention of a British officer in India, who gets credit for preliminary excavations in Harappa.\n\nThe Indus Valley civilization was mainly an urban culture sustained by surplus agricultural production and commerce, the latter including trade with Sumer in southern Mesopotamia. Both Mohenjo-Daro and Harappa are generally characterized as having \"differentiated living quarters, flat-roofed brick houses, and fortified administrative or religious centers.\" Although such similarities have given rise to arguments for the existence of a standardized system of urban layout and planning, the similarities are largely due to the presence of a semi-orthogonal type of civic layout, and a comparison of the layouts of Mohenjo-Daro and Harappa shows that they are in fact, arranged in a quite dissimilar fashion.\n\nThe weights and measures of the Indus Valley Civilization, on the other hand, were highly standardized, and conform to a set scale of gradations. Distinctive seals were used, among other applications, perhaps for identification of property and shipment of goods. Although copper and bronze were in use, iron was not yet employed. \"Cotton was woven and dyed for clothing; wheat, rice, and a variety of vegetables and fruits were cultivated; and a number of animals, including the humped bull, were domesticated,\" as well as \"fowl for fighting\". Wheel-made pottery—some of it adorned with animal and geometric motifs—has been found in profusion at all the major Indus sites. A centralized administration for each city, though not the whole civilization, has been inferred from the revealed cultural uniformity; however, it remains uncertain whether authority lay with a commercial oligarchy. Harappans had many trade routes along the Indus River that went as far as the Persian Gulf, Mesopotamia, and Egypt. Some of the most valuable things traded were carnelian and lapis lazuli.\n\nWhat is clear is that Harappan society was not entirely peaceful, with the human skeletal remains demonstrating some of the highest rates of injury (15.5%) found in South Asian prehistory. Paleopathological analysis demonstrated that leprosy and tuberculosis were present at Harappa, with the highest prevalence of both disease and trauma present in the skeletons from Area G (an ossuary located south-east of the city walls). Furthermore, rates of cranio-facial trauma and infection increased through time demonstrating that the civilization collapsed amid illness and injury. The bioarchaeologists who examined the remains have suggested that the combined evidence for differences in mortuary treatment and epidemiology indicate that some individuals and communities at Harappa were excluded from access to basic resources like health and safety, a basic feature of hierarchical societies worldwide.\n\nThe excavators of the site have proposed the following chronology of Harappa's occupation:\n\nBy far the most exquisite and obscure artifacts unearthed to date are the small, square steatite (soapstone) seals engraved with human or animal motifs. A large number of seals have been found at such sites as Mohenjo-Daro and Harappa. Many bear pictographic inscriptions generally thought to be a form of writing or script. Despite the efforts of philologists from all parts of the world, and despite the use of modern cryptographic analysis, the signs remain undeciphered. It is also unknown if they reflect proto-Dravidian or other non-Vedic language(s). The ascription of Indus Valley Civilization iconography and epigraphy to historically known cultures is extremely problematic, in part due to the rather tenuous archaeological evidence of such claims, as well as the projection of modern South Asian political concerns onto the archaeological record of the area. This is especially evident in the radically varying interpretations of Harappan material culture as seen from both Pakistan- and India-based scholars.\nIn February 2006 a school teacher in the village of Sembian-Kandiyur in Tamil Nadu discovered a stone celt (tool) with an inscription estimated to be up to 3,500 years old.\n\nClay and stone tablets unearthed at Harappa, which were carbon dated 3300–3200 BC., contain trident-shaped and plant-like markings. \"It is a big question as to if we can call what we have found true writing, but we have found symbols that have similarities to what became Indus script\" said Dr. Richard Meadow of Harvard University, Director of the Harappa Archeological Research Project. This primitive writing is placed slightly earlier than primitive writings of the Sumerians of Mesopotamia, dated c.3100 BC. These markings have similarities to what later became Indus Script.\n\n\n\n"}
{"id": "14153", "url": "https://en.wikipedia.org/wiki?curid=14153", "title": "Hendecasyllable", "text": "Hendecasyllable\n\nIn poetry, a hendecasyllable is a line of eleven syllables. The term \"hendecasyllabic\" is used to refer to two different poetic meters, the older of which is quantitative and used chiefly in classical (Ancient Greek and Latin) poetry and the newer of which is accentual and used in medieval and modern poetry.\n\nThe classical hendecasyllable is a quantitative meter used in Ancient Greece in Aeolic verse and in scolia, and later by the Roman poets Catullus and Martial. Each line has eleven syllables; hence the name, which comes from the Greek word for eleven. The heart of the line is the choriamb (- u u -). There are three different versions.\nThe pattern of the Phalaecian (Latin: \"hendecasyllabus phalaecius\") is as follows (using \"-\" for a long syllable, \"u\" for a short and \"x\" for an \"anceps\" or variable syllable):\n\nAnother form of hendecasyllabic verse is the \"Alcaic\" (Latin: \"hendecasyllabus alcaicus\"; used in the Alcaic stanza), which has the pattern:\n\nThe third form of hendecasyllabic verse is the \"Sapphic\" (Latin: \"hendecasyllabus sapphicus\"; so named for its use in the Sapphic stanza), with the pattern:\n\nForty-three of Catullus's poems are hendecasyllabic; for an example, see Catullus 1.\n\nThe metre has been imitated in English, notably by Alfred Tennyson, Swinburne, and Robert Frost, cf. \"For Once Then Something.\" Contemporary American poets Annie Finch (\"Lucid Waking\") and Patricia Smith (\"The Reemergence of the Noose\") have published recent examples. Poets wanting to capture the hendecasyllabic rhythm in English have simply transposed the pattern into its accentual-syllabic equivalent: /u|/u|/uu/u|/u|, or trochee/trochee/dactyl/trochee/trochee, so that the long/short pattern becomes a stress/unstress pattern. Tennyson, however, maintained the quantitative features of the metre:\n\nThe hendecasyllable () is the principal metre in Italian poetry. Its defining feature is a constant stress on the tenth syllable, so that the number of syllables in the verse may vary, equaling eleven in the usual case where the final word is stressed on the penultimate syllable. The verse also has a stress preceding the caesura, on either the fourth or sixth syllable. The first case is called \"endecasillabo a minore\", or lesser hendecasyllable, and has the first hemistich equivalent to a \"quinario\"; the second is called \"endecasillabo a maiore\", or greater hendecasyllable, and has a \"settenario\" as the first hemistich.\nThe most usual stress schemes for the Italian hendecasyllable are stresses on sixth and tenth syllables (for example, \"\"Nel mezzo del cammin di nostra vita\",\" Dante Alighieri, first line of \"The Divine Comedy),\" and on the fourth, seventh and tenth syllables (\"\"Un incalzar di cavalli accorrenti\",\" Ugo Foscolo, \"Dei sepolcri\").\n\nMost classical Italian poems are composed in hendecasyllables, including the major works of Dante, Francesco Petrarca, Ludovico Ariosto, and Torquato Tasso. The rhyme system varies from terza rima to ottava, from sonnet to canzone. From the early 16th century, hendecasyllables are often used without a strict system, with few or no rhymes, both in poetry and in drama. An early example is \"Le Api\" (\"the bees\") by Giovanni di Bernardo Rucellai, written around 1517 and published in 1525, which begins:\n\nLike other early Italian-language tragedies, the \"Sophonisba\" of Gian Giorgio Trissino (1515) is in blank hendecasyllables. Later examples can be found in the \"Canti\" of Giacomo Leopardi, where hendecasyllables are alternated with \"settenari\". The effect of \"endecasillabi sciolti\" (\"untied\" hendecasyllables) may be considered similar to that of English blank verse.\n\nIt has a role in Italian poetry, and a formal structure, comparable to the alexandrine in French.\n\nThe 11-syllable metre was very popular in Polish poetry, especially in the seventeenth and eighteenth centuries, owing to strong Italian literary influence. It was used by Jan Kochanowski, Piotr Kochanowski (who translated \"Jerusalem Delivered\" by Torquato Tasso), Sebastian Grabowiecki, Wespazjan Kochowski and Stanisław Herakliusz Lubomirski. The greatest Polish Romantic poet, Adam Mickiewicz, set his poem Grażyna in this measure. The Polish hendecasyllable is widely used when translating English \nblank verse.\n\nAlmost always, the 11-syllable line is divided by a caesura into 5 + 6 syllables. Only rarely it is fully iambic. \n\nA popular form of Polish literature that employs the hendacasyllable is the Sapphic stanza: 11/11/11/5. \n\nThe Polish hendecasyllable is often combined with an 8-syllable line: 11a/8b/11a/8b. Such a stanza was used by Mickiewicz in his ballads, as in the following example.\n\nIn Polish, the word for hendecasyllable is \"jedenastozgłoskowiec\".\n\nThe hendecasyllable (Portuguese: \"hendecassílabo\") is a common meter in Portuguese poetry. The best-known Portuguese poem composed in hendecasyllables is Luís de Camões' epic \"Os Lusíadas\", which begins as follows:\n\nIn Portuguese, the hendecasyllable meter is often called \"decasyllable\" (\"decassílabo\"), even when the work in question consists overwhelmingly of eleven-syllable lines (as does \"Os Lusíadas\").\n\nThe term \"hendecasyllable\" is sometimes used in English poetry to describe a line of iambic pentameter with a feminine ending, as in the first line of John Keats's \"Endymion:\" \"A thing of beauty is a joy for ever.\"\n\n\n\n"}
{"id": "14155", "url": "https://en.wikipedia.org/wiki?curid=14155", "title": "Hebrides", "text": "Hebrides\n\nThe Hebrides (; , ; ) comprise a widespread and diverse archipelago off the west coast of mainland Scotland. There are two main groups: the Inner and Outer Hebrides. These islands have a long history of occupation dating back to the Mesolithic, and the culture of the residents has been affected by the successive influences of Celtic, Norse, and English-speaking peoples. This diversity is reflected in the names given to the islands, which are derived from the languages that have been spoken there in historic and perhaps prehistoric times.\n\nThe Hebrides are the source of much of Scottish Gaelic literature and Gaelic music. Today the economy of the islands is dependent on crofting, fishing, tourism, the oil industry, and renewable energy. The Hebrides have lower biodiversity than mainland Scotland, but there is a significant presence of seals and seabirds.\n\nThe earliest written references that have survived relating to the islands were made circa 77 AD by Pliny the Elder in his \"Natural History\", where he states that there are 30 ', and makes a separate reference to ', which Watson (1926) concludes is unequivocally the Outer Hebrides. Writing about 80 years later, in 140-150 AD, Ptolemy, drawing on the earlier naval expeditions of , writes that there are five ' (possibly meaning the Inner Hebrides) and '. Later texts in classical Latin, by writers such as , use the forms ' and '.\n\nThe name ' recorded by Ptolemy may be pre-Celtic. Islay is Ptolemy's , the use of the \"p\" hinting at a Brythonic or Pictish tribal name, , although the root is not Gaelic. Woolf (2012) has suggested that ' may be \"an Irish attempt to reproduce the word ' phonetically rather than by translating it\" and that the tribe's name may come from the root ' meaning \"horse\". Watson (1926) also notes the possible relationship between ' and the ancient Irish Ulaid tribal name ' and the personal name of a king recorded in the \"\".\nThe names of other individual islands reflect their complex linguistic history. The majority are Norse or Gaelic but the roots of several other Hebrides may have a pre-Celtic origin. Adomnán, the 7th century abbot of Iona, records Colonsay as \"Colosus\" and Tiree as \"Ethica\", both of which may be pre-Celtic names. The etymology of Skye is complex and may also include a pre-Celtic root. Lewis is \"\" in Old Norse and although various suggestions have been made as to a Norse meaning (such as \"song house\") the name is not of Gaelic origin and the Norse credentials are questionable.\n\nThe earliest comprehensive written list of Hebridean island names was undertaken by Donald Monro in 1549, which in some cases also provides the earliest written form of the island name. The derivations of all of the inhabited islands of the Hebrides and some of the larger uninhabited ones are listed below.\n\nLewis and Harris is the largest island in Scotland and the third largest in the British Isles, after Great Britain and Ireland. It incorporates Lewis in the north and Harris in the south, both of which are frequently referred to as individual islands, although they are joined by a land border. Remarkably, the island does not have a common name in either English or Gaelic and is referred to as \"Lewis and Harris\", \"Lewis with Harris\", \"Harris with Lewis\" etc. For this reason it is treated as two separate islands below. The derivation of Lewis may be pre-Celtic (see above) and the origin of Harris is no less problematic. In the Ravenna Cosmography, \"Erimon\" may refer to Harris (or possibly the Outer Hebrides as a whole). This word may derive from the ( \"desert\". The origin of Uist () is similarly unclear.\n\nThere are various examples of Inner Hebridean island names that were originally Gaelic but have become completely replaced. For example, Adomnán records \"Sainea\", \"Elena\", \"Ommon\" and \"Oideacha\" in the Inner Hebrides, which names must have passed out of usage in the Norse era and whose locations are not clear. One of the complexities is that an island may have had a Celtic name, which was replaced by a similar-sounding Norse name, but then reverted to an essentially Gaelic name with a Norse \"øy\" or \"ey\" ending. See for example Rona below.\n\nThe names of uninhabited islands follow the same general patterns as the inhabited islands. The following are the ten largest in the Hebrides and their outliers.\n\nThe etymology of St Kilda, a small archipelago west of the Outer Hebrides, and its main island Hirta, is very complex. No saint is known by the name of Kilda, and various theories have been proposed for the word's origin, which dates from the late 16th century. Haswell-Smith (2004) notes that the full name \"St Kilda\" first appears on a Dutch map dated 1666, and that it may have been derived from Norse ' (\"sweet wellwater\") or from a mistaken Dutch assumption that the spring ' was dedicated to a saint (' is a tautological placename, consisting of the Gaelic and Norse words for \"well\", i.e. \"well well\"). The origin of the Gaelic for \"Hirta\"—', ', or '—which long pre-dates the use of \"St Kilda\", is similarly open to interpretation. Watson (1926) offers the Old Irish ', a word meaning \"death\", possibly relating to the dangerous seas. Maclean (1977), drawing on an Icelandic saga describing an early 13th-century voyage to Ireland that mentions a visit to the islands of ', speculates that the shape of Hirta resembles a stag, \"\" being \"stags\" in Norse.\n\nThe etymology of small islands may be no less complex. In relation to , R. L. Stevenson believed that \"black and dismal\" was a translation of the name, noting that \"as usual, in Gaelic, it is not the only one.\"\n\nThe Hebrides were settled during the Mesolithic era around 6500 BC or earlier, after the climatic conditions improved enough to sustain human settlement. Occupation at a site on is dated to 8590 ±95 uncorrected radiocarbon years BP, which is amongst the oldest evidence of occupation in Scotland. There are many examples of structures from the Neolithic period, the finest example being the standing stones at Callanish, dating to the 3rd millennium BC. Cladh Hallan, a Bronze Age settlement on South Uist is the only site in the UK where prehistoric mummies have been found.\n\nIn 55 BC, the Greek historian Diodorus Siculus wrote that there was an island called \"Hyperborea\" (which means \"beyond the North Wind\"), where a round temple stood from which the moon appeared only a little distance above the earth every 19 years. This may have been a reference to the stone circle at Callanish.\n\nA traveller called Demetrius of Tarsus related to Plutarch the tale of an expedition to the west coast of Scotland in or shortly before AD 83. He stated it was a gloomy journey amongst uninhabited islands, but he had visited one which was the retreat of holy men. He mentioned neither the druids nor the name of the island.\n\nThe first written records of native life begin in the 6th century AD, when the founding of the kingdom of Dál Riata took place. This encompassed roughly what is now Argyll and Bute and Lochaber in Scotland and County Antrim in Ireland. The figure of Columba looms large in any history of Dál Riata, and his founding of a monastery on Iona ensured that the kingdom would be of great importance in the spread of Christianity in northern Britain. However, Iona was far from unique. Lismore in the territory of the Cenél Loairn, was sufficiently important for the death of its abbots to be recorded with some frequency and many smaller sites, such as on Eigg, Hinba, and Tiree, are known from the annals.\n\nNorth of Dál Riata, the Inner and Outer Hebrides were nominally under Pictish control, although the historical record is sparse. Hunter (2000) states that in relation to King Bridei I of the Picts in the sixth century: \"As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.”\n\nViking raids began on Scottish shores towards the end of the 8th century and the Hebrides came under Norse control and settlement during the ensuing decades, especially following the success of Harald Fairhair at the Battle of in 872. In the Western Isles Ketill Flatnose may have been the dominant figure of the mid 9th century, by which time he had amassed a substantial island realm and made a variety of alliances with other Norse leaders. These princelings nominally owed allegiance to the Norwegian crown, although in practice the latter's control was fairly limited. Norse control of the Hebrides was formalised in 1098 when Edgar of Scotland formally signed the islands over to Magnus III of Norway. The Scottish acceptance of Magnus III as King of the Isles came after the Norwegian king had conquered Orkney, the Hebrides and the Isle of Man in a swift campaign earlier the same year, directed against the local Norwegian leaders of the various island petty kingdoms. By capturing the islands Magnus imposed a more direct royal control, although at a price. His skald Bjorn Cripplehand recorded that in Lewis \"fire played high in the heaven\" as \"flame spouted from the houses\" and that in the Uists \"the king dyed his sword red in blood\".\n\nThe Hebrides were now part of the Kingdom of the Isles, whose rulers were themselves vassals of the Kings of Norway. This situation lasted until the partitioning of the Western Isles in 1156, at which time the Outer Hebrides remained under Norwegian control while the Inner Hebrides broke out under Somerled, the Norse-Gael kinsman of the Manx royal house.\n\nFollowing the ill-fated 1263 expedition of Haakon IV of Norway, the Outer Hebrides and the Isle of Man were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth. Although their contribution to the islands can still be found in personal and place names, the archaeological record of the Norse period is very limited. The best known find is the Lewis chessmen, which date from the mid 12th century.\n\nAs the Norse era drew to a close, the Norse-speaking princes were gradually replaced by Gaelic-speaking clan chiefs including the MacLeods of Lewis and Harris, Clan Donald and MacNeil of Barra. This transition did little to relieve the islands of internecine strife although by the early 14th century the MacDonald Lords of the Isles, based on Islay, were in theory these chiefs' feudal superiors and managed to exert some control.\n\nThe Lords of the Isles ruled the Inner Hebrides as well as part of the Western Highlands as subjects of the King of Scots until John MacDonald, fourth Lord of the Isles, squandered the family's powerful position. A rebellion by his nephew, Alexander of Lochalsh provoked an exasperated James IV to forfeit the family's lands in 1493.\n\nIn 1598, King James VI authorised some \"Gentleman Adventurers\" from Fife to civilise the \"most barbarous Isle of Lewis\". Initially successful, the colonists were driven out by local forces commanded by Murdoch and Neil MacLeod, who based their forces on in . The colonists tried again in 1605 with the same result, but a third attempt in 1607 was more successful and in due course Stornoway became a Burgh of Barony. By this time, Lewis was held by the Mackenzies of Kintail (later the Earls of Seaforth), who pursued a more enlightened approach, investing in fishing in particular. The Seaforths' royalist inclinations led to Lewis becoming garrisoned during the Wars of the Three Kingdoms by Cromwell's troops, who destroyed the old castle in Stornoway.\n\nWith the implementation of the Treaty of Union in 1707, the Hebrides became part of the new Kingdom of Great Britain, but the clans' loyalties to a distant monarch were not strong. A considerable number of islesmen \"came out\" in support of the Jacobite Earl of Mar in the 1715 and again in the 1745 rising including Macleod of Dunvegan and MacLea of Lismore. The aftermath of the decisive Battle of Culloden, which effectively ended Jacobite hopes of a Stuart restoration, was widely felt. The British government's strategy was to estrange the clan chiefs from their kinsmen and turn their descendants into English-speaking landlords whose main concern was the revenues their estates brought rather than the welfare of those who lived on them. This may have brought peace to the islands, but in the following century it came at a terrible price. In the wake of the rebellion, the clan system was broken up and islands of the Hebrides became a series of landed estates.\n\nThe early 19th century was a time of improvement and population growth. Roads and quays were built; the slate industry became a significant employer on Easdale and surrounding islands; and the construction of the Crinan and Caledonian canals and other engineering works such as Clachan Bridge improved transport and access. However, in the mid-19th century, the inhabitants of many parts of the Hebrides were devastated by the Clearances, which destroyed communities throughout the Highlands and Islands as the human populations were evicted and replaced with sheep farms. The position was exacerbated by the failure of the islands' kelp industry that thrived from the 18th century until the end of the Napoleonic Wars in 1815 and large scale emigration became endemic.\n\nAs , a Gaelic poet from South Uist, wrote for his countrymen who were obliged to leave the Hebrides in the late 18th century, emigration was the only alternative to \"sinking into slavery\" as the Gaels had been unfairly dispossessed by rapacious landlords. In the 1880s, the \"Battle of the Braes\" involved a demonstration against unfair land regulation and eviction, stimulating the calling of the Napier Commission. Disturbances continued until the passing of the 1886 Crofters' Act.\n\nThe Hebrides have a diverse geology ranging in age from Precambrian strata that are amongst the oldest rocks in Europe to Paleogene igneous intrusions. Raised shore platforms in the Hebrides are identified as strandflats formed possibly in Pliocene times and later modified by the Quaternary glaciations. \n\nThe Hebrides can be divided into two main groups, separated from one another by the Minch to the north and the Sea of the Hebrides to the south. The Inner Hebrides lie closer to mainland Scotland and include Islay, Jura, Skye, Mull, Raasay, Staffa and the Small Isles. There are 36 inhabited islands in this group. The Outer Hebrides are a chain of more than 100 islands and small skerries located about west of mainland Scotland. There are 15 inhabited islands in this archipelago. The main islands include Barra, Benbecula, Berneray, Harris, Lewis, North Uist, South Uist, and St Kilda. In total, the islands have an area of approximately and a population of 44,759.\n\nA complication is that there are various descriptions of the scope of the Hebrides. The \"Collins Encyclopedia of Scotland\" describes the Inner Hebrides as lying \"east of the Minch\", which would include any and all offshore islands. There are various islands that lie in the sea lochs such as and that might not ordinarily be described as \"Hebridean\", but no formal definitions exist.\n\nIn the past, the Outer Hebrides were often referred to as the \"Long Isle\" (). Today, they are also known as the \"Western Isles\", although this phrase can also be used to refer to the Hebrides in general.\n\nThe Hebrides have a cool temperate climate that is remarkably mild and steady for such a northerly latitude, due to the influence of the Gulf Stream. In the Outer Hebrides the average temperature for the year is 6 °C (44 °F) in January and 14 °C (57 °F) in summer. The average annual rainfall in Lewis is and sunshine hours range from 1,100 – 1,200 \"per annum\" (13%). The summer days are relatively long, and May to August is the driest period.\n\nThe residents of the Hebrides have spoken a variety of different languages during the long period of human occupation.\n\nIt is assumed that Pictish must once have predominated in the northern Inner Hebrides and Outer Hebrides. The Scottish Gaelic language arrived from Ireland due to the growing influence of the kingdom of Dál Riata from the 6th century AD onwards, and became the dominant language of the southern Hebrides at that time. For a few centuries, the military might of the ' meant that Old Norse was prevalent in the Hebrides. North of , the place names that existed prior to the 9th century have been all but obliterated. The Old Norse name for the Hebrides during the Viking occupation was ', which means \"Southern Isles\"; in contrast to the \", or \"Northern Isles\" of Orkney and Shetland.\n\nSouth of , Gaelic place names are more common, and after the 13th century, Gaelic became the main language of the entire Hebridean archipelago. Due to Scots and English being favoured in government and the educational system, the Hebrides have been in a state of diglossia since at least the 17th century. The Highland Clearances of the 19th century accelerated the language shift away from Scottish Gaelic, as did increased migration and the continuing lower status of Gaelic speakers. Nevertheless, as late as the end of the 19th century, there were significant populations of monolingual Gaelic speakers, and the Hebrides still contain the highest percentages of Gaelic speakers in Scotland. This is especially true of the Outer Hebrides, where a slim majority speak the language. The Scottish Gaelic college, , is based on Skye and Islay.\n\nIronically, given the status of the Western Isles as the last Gaelic-speaking stronghold in Scotland, the Gaelic language name for the islands – \" – means \"isles of the foreigners\"; from the time when they were under Norse colonisation.\n\nFor those who remained, new economic opportunities emerged through the export of cattle, commercial fishing and tourism. Nonetheless emigration and military service became the choice of many and the archipelago's populations continued to dwindle throughout the late 19th century and for much of the 20th century. Lengthy periods of continuous occupation notwithstanding, many of the smaller islands were abandoned.\n\nThere were, however, continuing gradual economic improvements, among the most visible of which was the replacement of the traditional thatched blackhouse with accommodation of a more modern design and with the assistance of Highlands and Islands Enterprise many of the islands' populations have begun to increase after decades of decline. The discovery of substantial deposits of North Sea oil in 1965 and the renewables sector have contributed to a degree of economic stability in recent decades. For example, the Arnish yard has had a chequered history but has been a significant employer in both the oil and renewables industries.\n\nThe widespread immigration of mainlanders, particularly non-Gaelic speakers, has been a subject of controversy.\n\nMany contemporary Gaelic musicians have roots in the Hebrides, including Julie Fowlis (North Uist), Catherine-Ann MacPhee (Barra), Kathleen MacInnes (South Uist), and Ishbel MacAskill (Lewis). All of these singers have repertoire based on the Hebridean tradition, such as ' and ' (waulking songs). This tradition includes many songs composed by little-known or anonymous poets before 1800, such as \"\", \"\" and \"\". Several of Runrig's songs are inspired by the archipelago; Calum and were raised on North Uist and Donnie Munro on Skye.\n\nThe Gaelic poet spent much of his life in the Hebrides and often referred to them in his poetry, including in ' and '. The best known Gaelic poet of her era, (Mary MacPherson, 1821–98), embodied the spirit of the land agitation of the 1870s and 1880s. This, and her powerful evocation of the Hebrides—she was from Skye—has made her among the most enduring Gaelic poets. Allan MacDonald (1859–1905), who spent his adult life on Eriskay and South Uist, composed hymns and verse in honour of the Blessed Virgin, the Christ Child, and the Eucharist. In his secular poetry, MacDonald praised the beauty of Eriskay and its people. In his verse drama, \"\" (\"The Old Wives' Parliament\"), he lampooned the gossiping of his female parishioners and local marriage customs.\n\nIn the 20th century, Murdo Macfarlane of Lewis wrote ', a well-known poem about the Gaelic revival in the Outer Hebrides. Sorley MacLean, the most respected 20th-century Gaelic writer, was born and raised on Raasay, where he set his best known poem, ', about the devastating effect of the Highland Clearances. , raised on South Uist and described by MacLean as \"one of the few really significant living poets in Scotland, writing in any language\" (West Highland Free Press, October 1992) wrote the Scottish Gaelic-language novel \"\" which was voted in the Top Ten of the 100 Best-Ever Books from Scotland.\n\n\n\nIn some respects the Hebrides lack biodiversity in comparison to mainland Britain; for example, there are only half as many mammalian species. However, these islands provide breeding grounds for many important seabird species including the world's largest colony of northern gannets. Avian life includes the corncrake, red-throated diver, rock dove, kittiwake, tystie, Atlantic puffin, goldeneye, golden eagle and white-tailed sea eagle. The latter was re-introduced to Rùm in 1975 and has successfully spread to various neighbouring islands, including Mull. There is a small population of red-billed chough concentrated on the islands of Islay and Colonsay.\n\nRed deer are common on the hills and the grey seal and common seal are present around the coasts of Scotland. Colonies of seals are found on Oronsay and the Treshnish Isles. The rich freshwater streams contain brown trout, Atlantic salmon and water shrew. Offshore, minke whales, Killer whales, basking sharks, porpoises and dolphins are among the sealife that can be seen.\n\nHeather moor containing ling, bell heather, cross-leaved heath, bog myrtle and fescues is abundant and there is a diversity of Arctic and alpine plants including Alpine pearlwort and mossy cyphal.\n\nLoch Druidibeg on South Uist is a national nature reserve owned and managed by Scottish Natural Heritage. The reserve covers 1,677 hectares across the whole range of local habitats. Over 200 species of flowering plants have been recorded on the reserve, some of which are nationally scarce. South Uist is considered the best place in the UK for the aquatic plant slender naiad, which is a European Protected Species.\n\nHedgehogs are not native to the Outer Hebrides—they were introduced in the 1970s to reduce garden pests—and their spread poses a threat to the eggs of ground nesting wading birds. In 2003, Scottish Natural Heritage undertook culls of hedgehogs in the area although these were halted in 2007 due to protests. Trapped animals were relocated to the mainland.\n\n\n\n"}
{"id": "14158", "url": "https://en.wikipedia.org/wiki?curid=14158", "title": "HMS Dreadnought", "text": "HMS Dreadnought\n\nSeveral ships and one submarine of the Royal Navy have borne the name HMS \"Dreadnought\" in the expectation that they would \"dread nought\", i.e. \"fear nothing\". The 1906 ship was one of the Royal Navy's most famous vessels; battleships built after her were referred to as 'dreadnoughts', and earlier battleships became known as pre-dreadnoughts.\n\n\nAlso\n\nCitations\n\nReferences\n\n"}
{"id": "14159", "url": "https://en.wikipedia.org/wiki?curid=14159", "title": "Hartmann Schedel", "text": "Hartmann Schedel\n\nHartmann Schedel (13 February 1440 – 28 November 1514) was a German physician, humanist, historian, and one of the first cartographers to use the printing press. He was born and died in Nuremberg. Matheolus Perusinus served as his tutor. \n\nSchedel is best known for his writing the text for the \"Nuremberg Chronicle\", known as \"Schedelsche Weltchronik\" (English: \"Schedel's World Chronicle\"), published in 1493 in Nuremberg. It was commissioned by Sebald Schreyer (1446 – 1520) and Sebastian Kammermeister (1446 – 1503). Maps in the \"Chronicle\" were the first ever illustrations of many cities and countries.\n\nWith the invention of the printing press by Johannes Gutenberg in 1447, it became feasible to print books and maps for a larger customer basis. Because they had to be handwritten, books were previously rare and very expensive.\n\nSchedel was also a notable collector of books, art and old master prints. An album he had bound in 1504, which once contained five engravings by Jacopo de' Barbari, provides important evidence for dating de' Barbari's work.\n\n\n\n"}
{"id": "14160", "url": "https://en.wikipedia.org/wiki?curid=14160", "title": "Hexameter", "text": "Hexameter\n\nHexameter is a metrical line of verses consisting of six feet. It was the standard epic metre in classical Greek and Latin literature, such as in the \"Iliad\", \"Odyssey\" and \"Aeneid\". Its use in other genres of composition include Horace's satires, Ovid's \"Metamorphoses,\" and the Hymns of Orpheus. According to Greek mythology, hexameter was invented by Phemonoe, daughter of Apollo and the first Pythia of Delphi.\nIn classical hexameter, the six feet follow these rules:\n\nA short syllable (υ) is a syllable with a short vowel and no consonant at the end. A long syllable (–) is a syllable that either has a long vowel, one or more consonants at the end (or a long consonant), or both. Spaces between words are not counted in syllabification, so for instance \"cat\" is a long syllable in isolation, but \"cat attack\" would be syllabified as short-short-long: \"ca\", \"ta\", \"tack\" (υ υ –).\n\nVariations of the sequence from line to line, as well as the use of caesura (logical full stops within the line) are essential in avoiding what may otherwise be a monotonous sing-song effect.\n\nAlthough the rules seem simple, it is hard to use classical hexameter in English, because English is a stress-timed language that condenses vowels and consonants between stressed syllables, while hexameter relies on the regular timing of the phonetic sounds. Languages having the latter properties (i.e., languages that are not stress-timed) include Ancient Greek, Latin, Lithuanian and Hungarian.\n\nWhile the above classical hexameter has never enjoyed much popularity in English, where the standard metre is iambic pentameter, English poems have frequently been written in iambic hexameter. There are numerous examples from the 16th century and a few from the 17th; the most prominent of these is Michael Drayton's \"Poly-Olbion\" (1612) in couplets of iambic hexameter. An example from Drayton (marking the feet):\n\nIn the 17th century the iambic hexameter, also called alexandrine, was used as a substitution in the heroic couplet, and as one of the types of permissible lines in lyrical stanzas and the Pindaric odes of Cowley and Dryden.\n\nSeveral attempts were made in the 19th century to naturalise the dactylic hexameter to English, by Henry Wadsworth Longfellow, Arthur Hugh Clough and others, none of them particularly successful. Gerard Manley Hopkins wrote many of his poems in six-foot iambic and sprung rhythm lines. In the 20th century a loose ballad-like six-foot line with a strong medial pause was used by William Butler Yeats. The iambic six-foot line has also been used occasionally, and an accentual six-foot line has been used by translators from the Latin and many poets.\n\nIn the late 18th century the hexameter was adapted to the Lithuanian language by Kristijonas Donelaitis. His poem \"\"Metai\" (The Seasons)\" is considered the most successful hexameter text in Lithuanian as yet.\n\nHungarian is extremely suitable to hexameter (and other forms of poetry based on quantitative metre), insomuch that if a student comes to a halt when reciting a poem s/he was supposed to learn, s/he can say \"I'm stuck here, unfortunately the rest won't come into my mind,\" and it will be an impeccable hexameter in Hungarian: \"Itt elakadtam, sajnos nem jut eszembe a többi.\"\n\nDue to this feature, hexameter has been widely used both in translated (Greek and Roman) and in original poetry up to the twentieth century (e.g. by Miklós Radnóti).\n\n\n\n"}
{"id": "14162", "url": "https://en.wikipedia.org/wiki?curid=14162", "title": "Timeline of Polish history", "text": "Timeline of Polish history\n\nThis is a timeline of Polish history, comprising important legal and territorial changes and political events in Poland and its predecessor states. To read about the background to these events, see History of Poland. See also the list of Polish monarchs and list of Prime Ministers of Poland.\n\n\n\n"}
{"id": "14168", "url": "https://en.wikipedia.org/wiki?curid=14168", "title": "Himalia", "text": "Himalia\n\nHimalia may refer to:\n\n"}
{"id": "14169", "url": "https://en.wikipedia.org/wiki?curid=14169", "title": "Heracleidae", "text": "Heracleidae\n\nIn Greek mythology, the Heracleidae (; ) or Heraclids were the numerous descendants of Heracles (Hercules), especially applied in a narrower sense to the descendants of Hyllus, the eldest of his four sons by Deianira (Hyllus was also sometimes thought of as Heracles' son by Melite). Other Heracleidae included Macaria, Lamos, Manto, Bianor, Tlepolemus, and Telephus. These Heraclids were a group of Dorian kings who conquered the Peloponnesian kingdoms of Mycenae, Sparta and Argos; according to the literary tradition in Greek mythology, they claimed a right to rule through their ancestor. Since Karl Otfried Müller's \"Die Dorier\" (1830, English translation 1839), I. ch. 3, their rise to dominance has been associated with a \"Dorian invasion\".\n\nThough details of genealogy differ from one ancient author to another, the cultural significance of the mythic theme, that the descendants of Heracles, exiled after his death, returned some generations later to reclaim land that their ancestors had held in Mycenaean Greece, was to assert the primal legitimacy of a traditional ruling clan that traced its origin, thus its legitimacy, to Heracles.\n\nHeracles, whom Zeus had originally intended to be ruler of Argos, Lacedaemon and Messenian Pylos, had been supplanted by the cunning of Hera, and his intended possessions had fallen into the hands of Eurystheus, king of Mycenae. After the death of Heracles, his children, after many wanderings, found refuge from Eurystheus at Athens. Eurystheus, on his demand for their surrender being refused, attacked Athens, but was defeated and slain. Hyllus and his brothers then invaded Peloponnesus, but after a year's stay were forced by a pestilence to quit. They withdrew to Thessaly, where Aegimius, the mythical ancestor of the Dorians, whom Heracles had assisted in war against the Lapithae, adopted Hyllus and made over to him a third part of his territory.\n\nAfter the death of Aegimius, his two sons, Pamphylus and Dymas, voluntarily submitted to Hyllus (who was, according to the Dorian tradition in Herodotus V. 72, really an Achaean), who thus became ruler of the Dorians, the three branches of that race being named after these three heroes. Desiring to reconquer his paternal inheritance, Hyllus consulted the Delphic oracle, which told him to wait for \"the third fruit\", (or \"the third crop\") and then enter Peloponnesus by \"a narrow passage by sea\". Accordingly, after three years, Hyllus marched across the isthmus of Corinth to attack Atreus, the successor of Eurystheus, but was slain in single combat by Echemus, king of Tegea. This second attempt was followed by a third under Cleodaeus and a fourth under Aristomachus, both unsuccessful.\n\nAt last, Temenus, Cresphontes and Aristodemus, the sons of Aristomachus, complained to the oracle that its instructions had proved fatal to those who had followed them. They received the answer that by the \"third fruit\" the \"third generation\" was meant, and that the \"narrow passage\" was not the isthmus of Corinth, but the straits of Rhium. They accordingly built a fleet at Naupactus, but before they set sail, Aristodemus was struck by lightning (or shot by Apollo) and the fleet destroyed, because one of the Heracleidae had slain an Acarnanian soothsayer.\n\nThe oracle, being again consulted by Temenus, bade him offer an expiatory sacrifice and banish the murderer for ten years, and look out for a man with three eyes to act as guide. On his way back to Naupactus, Temenus fell in with Oxylus, an Aetolian, who had lost one eye, riding on a horse (thus making up the three eyes) and immediately pressed him into his service. According to another account, a mule on which Oxylus rode had lost an eye. The Heracleidae repaired their ships, sailed from Naupactus to Antirrhium, and thence to Rhium in Peloponnesus. A decisive battle was fought with Tisamenus, son of Orestes, the chief ruler in the peninsula, who was defeated and slain. This conquest was traditionally dated eighty years after the Trojan War.\n\nThe Heracleidae, who thus became practically masters of Peloponnesus, proceeded to distribute its territory among themselves by lot. Argos fell to Temenus, Lacedaemon to Procles and Eurysthenes, the twin sons of Aristodemus; and Messenia to Cresphontes (tradition maintains that Cresphontes cheated in order to obtain Messenia, which had the best land of all.) The fertile district of Elis had been reserved by agreement for Oxylus. The Heracleidae ruled in Lacedaemon until 221 BCE, but disappeared much earlier in the other countries.\n\nThis conquest of Peloponnesus by the Dorians, commonly called the \"Dorian invasion\" or the \"Return of the Heraclidae\", is represented as the recovery by the descendants of Heracles of the rightful inheritance of their hero ancestor and his sons. The Dorians followed the custom of other Greek tribes in claiming as ancestor for their ruling families one of the legendary heroes, but the traditions must not on that account be regarded as entirely mythical. They represent a joint invasion of Peloponnesus by Aetolians and Dorians, the latter having been driven southward from their original northern home under pressure from the Thessalians. It is noticeable that there is no mention of these Heraclidae or their invasion in Homer or Hesiod. Herodotus (vi. 52) speaks of poets who had celebrated their deeds, but these were limited to events immediately succeeding the death of Heracles.\n\nAt Sparta, the Heraclids formed two dynasties ruling jointly: the Agiads and the Eurypontids.\n\nAt Corinth the Heraclids ruled as the Bacchiadae dynasty before the aristocratic revolution, which brought a Bacchiad aristocracy into power. The kings were as follows:\n\nThe Greek tragedians amplified the story, probably drawing inspiration from local legends which glorified the services rendered by Athens to the rulers of Peloponnesus.\n\nThe Heracleidae feature as the main subjects of Euripides' play, \"Heracleidae\". J. A. Spranger found the political subtext of \"Heracleidae\", never far to seek, so particularly apt in Athens towards the end of the peace of Nicias, in 419 BCE, that he suggested the date as that of the play's first performance.\n\nIn the tragedy, Iolaus, Heracles' old comrade, and Heracles' children, Macaria and her brothers and sisters have hidden from Eurystheus in Athens, ruled by King Demophon; as the first scene makes clear, they expect that the blood relationship of the kings with Heracles and their father's past indebtedness to Theseus will finally provide them sanctuary. As Eurystheus prepares to attack, an oracle tells Demophon that only the sacrifice of a noble woman to Persephone can guarantee an Athenian victory. Macaria volunteers for the sacrifice and a spring is named the Macarian spring in her honor.\n\n\n"}
{"id": "14170", "url": "https://en.wikipedia.org/wiki?curid=14170", "title": "HIV", "text": "HIV\n\nThe human immunodeficiency virus (HIV) is a lentivirus (a subgroup of retrovirus) that causes HIV infection and over time acquired immunodeficiency syndrome (AIDS). AIDS is a condition in humans in which progressive failure of the immune system allows life-threatening opportunistic infections and cancers to thrive. Without treatment, average survival time after infection with HIV is estimated to be 9 to 11 years, depending on the HIV subtype. In most cases, HIV is a sexually transmitted infection and occurs by contact with or transfer of blood, pre-ejaculate, semen, and vaginal fluids. Non-sexual transmission can occur from an infected mother to her infant during pregnancy, during childbirth by exposure to her blood or vaginal fluid, and through breast milk. Within these bodily fluids, HIV is present as both free virus particles and virus within infected immune cells.\n\nHIV infects vital cells in the human immune system, such as helper T cells (specifically CD4 T cells), macrophages, and dendritic cells. HIV infection leads to low levels of CD4 T cells through a number of mechanisms, including pyroptosis of abortively infected T cells, apoptosis of uninfected bystander cells, direct viral killing of infected cells, and killing of infected CD4 T cells by CD8 cytotoxic lymphocytes that recognize infected cells. When CD4 T cell numbers decline below a critical level, cell-mediated immunity is lost, and the body becomes progressively more susceptible to opportunistic infections, leading to the development of AIDS.\n\nHIV is a member of the genus \"Lentivirus\", part of the family \"Retroviridae\". Lentiviruses have many morphologies and biological properties in common. Many species are infected by lentiviruses, which are characteristically responsible for long-duration illnesses with a long incubation period. Lentiviruses are transmitted as single-stranded, positive-sense, enveloped RNA viruses. Upon entry into the target cell, the viral RNA genome is converted (reverse transcribed) into double-stranded DNA by a virally encoded enzyme, reverse transcriptase, that is transported along with the viral genome in the virus particle. The resulting viral DNA is then imported into the cell nucleus and integrated into the cellular DNA by a virally encoded enzyme, integrase, and host co-factors. Once integrated, the virus may become latent, allowing the virus and its host cell to avoid detection by the immune system, for an indiscriminate amount of time. The HIV virus can remain dormant in the human body for up to ten years after primary infection; during this period the virus does not cause symptoms. Alternatively, the integrated viral DNA may be transcribed, producing new RNA genomes and viral proteins, using host cell resources, that are packaged and released from the cell as new virus particles that will begin the replication cycle anew.\n\nTwo types of HIV have been characterized: HIV-1 and HIV-2. HIV-1 is the virus that was initially discovered and termed both lymphadenopathy associated virus (LAV) and human T-lymphotropic virus 3 (HTLV-III). HIV-1 is more virulent and more infective than HIV-2, and is the cause of the majority of HIV infections globally. The lower infectivity of HIV-2, compared to HIV-1, implies that fewer of those exposed to HIV-2 will be infected per exposure. Due to its relatively poor capacity for transmission, HIV-2 is largely confined to West Africa.\n\nHIV is different in structure from other retroviruses. It is roughly spherical with a diameter of about 120 nm, around 60 times smaller than a red blood cell. It is composed of two copies of positive-sense single-stranded RNA that codes for the virus's nine genes enclosed by a conical capsid composed of 2,000 copies of the viral protein p24. The single-stranded RNA is tightly bound to nucleocapsid proteins, p7, and enzymes needed for the development of the virion such as reverse transcriptase, proteases, ribonuclease and integrase. A matrix composed of the viral protein p17 surrounds the capsid ensuring the integrity of the virion particle.\n\nThis is, in turn, surrounded by the viral envelope, that is composed of the lipid bilayer taken from the membrane of a human host cell when the newly formed virus particle buds from the cell. The viral envelope contains proteins from the host cell and relatively few copies of the HIV Envelope protein, which consists of a cap made of three molecules known as glycoprotein (gp) 120, and a stem consisting of three gp41 molecules that anchor the structure into the viral envelope. The Envelope protein, encoded by the HIV \"env\" gene, allows the virus to attach to target cells and fuse the viral envelope with the target cell's membrane releasing the viral contents into the cell and initiating the infectious cycle.\n\nAs the sole viral protein on the surface of the virus, the Envelope protein is a major target for HIV vaccine efforts. Over half of the mass of the trimeric envelope spike is N-linked glycans. The density is high as the glycans shield the underlying viral protein from neutralisation by antibodies. This is one of the most densely glycosylated molecules known and the density is sufficiently high to prevent the normal maturation process of glycans during biogenesis in the endoplasmic and Golgi apparatus. The majority of the glycans are therefore stalled as immature 'high-mannose' glycans not normally present on human glycoproteins that are secreted or present on a cell surface. The unusual processing and high density means that almost all broadly neutralising antibodies that have so far been identified (from a subset of patients that have been infected for many months to years) bind to, or are adapted to cope with, these envelope glycans.\n\nThe molecular structure of the viral spike has now been determined by X-ray crystallography and cryogenic electron microscopy. These advances in structural biology were made possible due to the development of stable recombinant forms of the viral spike by the introduction of an intersubunit disulphide bond and an isoleucine to proline mutation (radical replacement of an amino acid) in gp41. The so-called SOSIP trimers not only reproduce the antigenic properties of the native viral spike, but also display the same degree of immature glycans as presented on the native virus. Recombinant trimeric viral spikes are promising vaccine candidates as they display less non-neutralising epitopes than recombinant monomeric gp120, which act to suppress the immune response to target epitopes. \n\nThe RNA genome consists of at least seven structural landmarks (LTR, TAR, RRE, PE, SLIP, CRS, and INS), and nine genes (\"gag\", \"pol\", and \"env\", \"tat\", \"rev\", \"nef\", \"vif\", \"vpr\", \"vpu\", and sometimes a tenth \"tev\", which is a fusion of \"tat\", \"env\" and \"rev\"), encoding 19 proteins. Three of these genes, \"gag\", \"pol\", and \"env\", contain information needed to make the structural proteins for new virus particles. For example, \"env\" codes for a protein called gp160 that is cut in two by a cellular protease to form gp120 and gp41. The six remaining genes, \"tat\", \"rev\", \"nef\", \"vif\", \"vpr\", and \"vpu\" (or \"vpx\" in the case of HIV-2), are regulatory genes for proteins that control the ability of HIV to infect cells, produce new copies of virus (replicate), or cause disease.\n\nThe two \"tat\" proteins (p16 and p14) are transcriptional transactivators for the LTR promoter acting by binding the TAR RNA element. The TAR may also be processed into microRNAs that regulate the apoptosis genes \"ERCC1\" and \"IER3\". The \"rev\" protein (p19) is involved in shuttling RNAs from the nucleus and the cytoplasm by binding to the RRE RNA element. The \"vif\" protein (p23) prevents the action of APOBEC3G (a cellular protein that deaminates cytidine to uridine in the single-stranded viral DNA and/or interferes with reverse transcription). The \"vpr\" protein (p14) arrests cell division at G2/M. The \"nef\" protein (p27) down-regulates CD4 (the major viral receptor), as well as the MHC class I and class II molecules.\n\n\"Nef\" also interacts with SH3 domains. The \"vpu\" protein (p16) influences the release of new virus particles from infected cells. The ends of each strand of HIV RNA contain an RNA sequence called a long terminal repeat (LTR). Regions in the LTR act as switches to control production of new viruses and can be triggered by proteins from either HIV or the host cell. The Psi element is involved in viral genome packaging and recognized by \"gag\" and \"rev\" proteins. The SLIP element () is involved in the frameshift in the \"gag\"-\"pol\" reading frame required to make functional \"pol\".\n\nThe term viral tropism refers to the cell types a virus infects. HIV can infect a variety of immune cells such as CD4 T cells, macrophages, and microglial cells. HIV-1 entry to macrophages and CD4 T cells is mediated through interaction of the virion envelope glycoproteins (gp120) with the CD4 molecule on the target cells' membrane and also with chemokine co-receptors.\n\nMacrophage-tropic (M-tropic) strains of HIV-1, or non-syncytia-inducing strains (NSI; now called R5 viruses) use the \"β\"-chemokine receptor, CCR5, for entry and are thus able to replicate in both macrophages and CD4 T cells. This CCR5 co-receptor is used by almost all primary HIV-1 isolates regardless of viral genetic subtype. Indeed, macrophages play a key role in several critical aspects of HIV infection. They appear to be the first cells infected by HIV and perhaps the source of HIV production when CD4 cells become depleted in the patient. Macrophages and microglial cells are the cells infected by HIV in the central nervous system. In the tonsils and adenoids of HIV-infected patients, macrophages fuse into multinucleated giant cells that produce huge amounts of virus.\n\nT-tropic strains of HIV-1, or syncytia-inducing strains (SI; now called X4 viruses) replicate in primary CD4 T cells as well as in macrophages and use the \"α\"-chemokine receptor, CXCR4, for entry.\n\nDual-tropic HIV-1 strains are thought to be transitional strains of HIV-1 and thus are able to use both CCR5 and CXCR4 as co-receptors for viral entry.\n\nThe \"α\"-chemokine SDF-1, a ligand for CXCR4, suppresses replication of T-tropic HIV-1 isolates. It does this by down-regulating the expression of CXCR4 on the surface of HIV target cells. M-tropic HIV-1 isolates that use only the CCR5 receptor are termed R5; those that use only CXCR4 are termed X4, and those that use both, X4R5. However, the use of co-receptors alone does not explain viral tropism, as not all R5 viruses are able to use CCR5 on macrophages for a productive infection and HIV can also infect a subtype of myeloid dendritic cells, which probably constitute a reservoir that maintains infection when CD4 T cell numbers have declined to extremely low levels.\n\nSome people are resistant to certain strains of HIV. For example, people with the CCR5-Δ32 mutation are resistant to infection by the R5 virus, as the mutation leaves HIV unable to bind to this co-receptor, reducing its ability to infect target cells.\n\nSexual intercourse is the major mode of HIV transmission. Both X4 and R5 HIV are present in the seminal fluid, which enables the virus to be transmitted from a male to his sexual partner. The virions can then infect numerous cellular targets and disseminate into the whole organism. However, a selection process leads to a predominant transmission of the R5 virus through this pathway. In patients infected with subtype B HIV-1, there is often a co-receptor switch in late-stage disease and T-tropic variants that can infect a variety of T cells through CXCR4. These variants then replicate more aggressively with heightened virulence that causes rapid T cell depletion, immune system collapse, and opportunistic infections that mark the advent of AIDS. Thus, during the course of infection, viral adaptation to the use of CXCR4 instead of CCR5 may be a key step in the progression to AIDS. A number of studies with subtype B-infected individuals have determined that between 40 and 50 percent of AIDS patients can harbour viruses of the SI and, it is presumed, the X4 phenotypes.\n\nHIV-2 is much less pathogenic than HIV-1 and is restricted in its worldwide distribution to West Africa. The adoption of \"accessory genes\" by HIV-2 and its more promiscuous pattern of co-receptor usage (including CD4-independence) may assist the virus in its adaptation to avoid innate restriction factors present in host cells. Adaptation to use normal cellular machinery to enable transmission and productive infection has also aided the establishment of HIV-2 replication in humans. A survival strategy for any infectious agent is not to kill its host, but ultimately become a commensal organism. Having achieved a low pathogenicity, over time, variants that are more successful at transmission will be selected.\n\nThe HIV virion enters macrophages and CD4 T cells by the adsorption of glycoproteins on its surface to receptors on the target cell followed by fusion of the viral envelope with the target cell membrane and the release of the HIV capsid into the cell.\n\nEntry to the cell begins through interaction of the trimeric envelope complex (gp160 spike) on the HIV viral envelope and both CD4 and a chemokine co-receptor (generally either CCR5 or CXCR4, but others are known to interact) on the target cell surface. Gp120 binds to integrin αβ activating LFA-1, the central integrin involved in the establishment of virological synapses, which facilitate efficient cell-to-cell spreading of HIV-1. The gp160 spike contains binding domains for both CD4 and chemokine receptors.\n\nThe first step in fusion involves the high-affinity attachment of the CD4 binding domains of gp120 to CD4. Once gp120 is bound with the CD4 protein, the envelope complex undergoes a structural change, exposing the chemokine receptor binding domains of gp120 and allowing them to interact with the target chemokine receptor. This allows for a more stable two-pronged attachment, which allows the N-terminal fusion peptide gp41 to penetrate the cell membrane. Repeat sequences in gp41, HR1, and HR2 then interact, causing the collapse of the extracellular portion of gp41 into a hairpin shape. This loop structure brings the virus and cell membranes close together, allowing fusion of the membranes and subsequent entry of the viral capsid.\n\nAfter HIV has bound to the target cell, the HIV RNA and various enzymes, including reverse transcriptase, integrase, ribonuclease, and protease, are injected into the cell. During the microtubule-based transport to the nucleus, the viral single-strand RNA genome is transcribed into double-strand DNA, which is then integrated into a host chromosome.\n\nHIV can infect dendritic cells (DCs) by this CD4-CCR5 route, but another route using mannose-specific C-type lectin receptors such as DC-SIGN can also be used. DCs are one of the first cells encountered by the virus during sexual transmission. They are currently thought to play an important role by transmitting HIV to T cells when the virus is captured in the mucosa by DCs. The presence of FEZ-1, which occurs naturally in neurons, is believed to prevent the infection of cells by HIV.\nHIV-1 entry, as well as entry of many other retroviruses, has long been believed to occur exclusively at the plasma membrane. More recently, however, productive infection by pH-independent, clathrin-mediated endocytosis of HIV-1 has also been reported and was recently suggested to constitute the only route of productive entry.\n\nShortly after the viral capsid enters the cell, an enzyme called reverse transcriptase liberates the positive-sense single-stranded RNA genome from the attached viral proteins and copies it into a complementary DNA (cDNA) molecule. The process of reverse transcription is extremely error-prone, and the resulting mutations may cause drug resistance or allow the virus to evade the body's immune system. The reverse transcriptase also has ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that creates a sense DNA from the \"antisense\" cDNA. Together, the cDNA and its complement form a double-stranded viral DNA that is then transported into the cell nucleus. The integration of the viral DNA into the host cell's genome is carried out by another viral enzyme called integrase.\n\nThe integrated viral DNA may then lie dormant, in the latent stage of HIV infection. To actively produce the virus, certain cellular transcription factors need to be present, the most important of which is NF-\"κ\"B (nuclear factor kappa B), which is upregulated when T cells become activated. This means that those cells most likely to be targeted, entered and subsequently killed by HIV are those actively fighting infection.\n\nDuring viral replication, the integrated DNA provirus is transcribed into RNA, some of which then undergo RNA splicing to produce mature messenger RNAs (mRNAs). These mRNAs are exported from the nucleus into the cytoplasm, where they are translated into the regulatory proteins Tat (which encourages new virus production) and Rev. As the newly produced Rev protein is produced it moves to the nucleus, where it binds to full-length, unspliced copies of virus RNAs and allows them to leave the nucleus. Some of these full-length RNAs function as new copies of the virus genome, while others function as mRNAs that are translated to produce the structural proteins Gag and Env. Gag proteins bind to copies of the virus RNA genome to package them into new virus particles.\n\nHIV-1 and HIV-2 appear to package their RNA differently. HIV-1 will bind to any appropriate RNA. HIV-2 will preferentially bind to the mRNA that was used to create the Gag protein itself.\n\nTwo RNA genomes are encapsidated in each HIV-1 particle (see Structure and genome of HIV). Upon infection and replication catalyzed by reverse transcriptase, recombination between the two genomes can occur. Recombination occurs as the single-strand, positive-sense RNA genomes are reverse transcribed to form DNA. During reverse transcription, the nascent DNA can switch multiple times between the two copies of the viral RNA. This form of recombination is known as copy-choice. Recombination events may occur throughout the genome. Anywhere from two to 20 recombination events per genome may occur at each replication cycle, and these events can rapidly shuffle the genetic information that is transmitted from parental to progeny genomes.\n\nViral recombination produces genetic variation that likely contributes to the evolution of resistance to anti-retroviral therapy. Recombination may also contribute, in principle, to overcoming the immune defenses of the host. Yet, for the adaptive advantages of genetic variation to be realized, the two viral genomes packaged in individual infecting virus particles need to have arisen from separate progenitor parental viruses of differing genetic constitution. It is unknown how often such mixed packaging occurs under natural conditions.\n\nBonhoeffer \"et al.\" suggested that template switching by reverse transcriptase acts as a repair process to deal with breaks in the single-stranded RNA genome. In addition, Hu and Temin suggested that recombination is an adaptation for repair of damage in the RNA genomes. Strand switching (copy-choice recombination) by reverse transcriptase could generate an undamaged copy of genomic DNA from two damaged single-stranded RNA genome copies. This view of the adaptive benefit of recombination in HIV could explain why each HIV particle contains two complete genomes, rather than one. Furthermore, the view that recombination is a repair process implies that the benefit of repair can occur at each replication cycle, and that this benefit can be realized whether or not the two genomes differ genetically. On the view that recombination in HIV is a repair process, the generation of recombinational variation would be a consequence, but not the cause of, the evolution of template switching.\n\nHIV-1 infection causes chronic inflammation and production of reactive oxygen species. Thus, the HIV genome may be vulnerable to oxidative damages, including breaks in the single-stranded RNA. For HIV, as well as for viruses in general, successful infection depends on overcoming host defensive strategies that often include production of genome-damaging reactive oxygen species. Thus, Michod \"et al.\" suggested that recombination by viruses is an adaptation for repair of genome damages, and that recombinational variation is a byproduct that may provide a separate benefit.\n\nThe final step of the viral cycle, assembly of new HIV-1 virions, begins at the plasma membrane of the host cell. The Env polyprotein (gp160) goes through the endoplasmic reticulum and is transported to the Golgi apparatus where it is cleaved by furin resulting in the two HIV envelope glycoproteins, gp41 and gp120. These are transported to the plasma membrane of the host cell where gp41 anchors gp120 to the membrane of the infected cell. The Gag (p55) and Gag-Pol (p160) polyproteins also associate with the inner surface of the plasma membrane along with the HIV genomic RNA as the forming virion begins to bud from the host cell. The budded virion is still immature as the gag polyproteins still need to be cleaved into the actual matrix, capsid and nucleocapsid proteins. This cleavage is mediated by the packaged viral protease and can be inhibited by antiretroviral drugs of the protease inhibitor class. The various structural components then assemble to produce a mature HIV virion. Only mature virions are then able to infect another cell.\n\nThe classical process of infection of a cell by a virion can be called \"cell-free spread\" to distinguish it from a more recently recognized process called \"cell-to-cell spread\". In cell-free spread (see figure), virus particles bud from an infected T cell, enter the blood or extracellular fluid and then infect another T cell following a chance encounter. HIV can also disseminate by direct transmission from one cell to another by a process of cell-to-cell spread, for which two pathways have been described. Firstly, an infected T cell can transmit virus directly to a target T cell via a virological synapse. Secondly, an antigen-presenting cell (APC), such as a macrophage or dendritic cell, can transmit HIV to T cells by a process that either involves productive infection (in the case of macrophages) or capture and transfer of virions \"in trans\" (in the case of dendritic cells). Whichever pathway is used, infection by cell-to-cell transfer is reported to be much more efficient than cell-free virus spread. A number of factors contribute to this increased efficiency, including polarised virus budding towards the site of cell-to-cell contact, close apposition of cells, which minimizes fluid-phase diffusion of virions, and clustering of HIV entry receptors on the target cell towards the contact zone. Cell-to-cell spread is thought to be particularly important in lymphoid tissues where CD4 T cells are densely packed and likely to interact frequently. Intravital imaging studies have supported the concept of the HIV virological synapse \"in vivo\". The many spreading mechanisms available to HIV contribute to the virus' ongoing replication in spite of anti-retroviral therapies.\n\nHIV differs from many viruses in that it has very high genetic variability. This diversity is a result of its fast replication cycle, with the generation of about 10 virions every day, coupled with a high mutation rate of approximately 3 x 10 per nucleotide base per cycle of replication and recombinogenic properties of reverse transcriptase.\n\nThis complex scenario leads to the generation of many variants of HIV in a single infected patient in the course of one day. This variability is compounded when a single cell is simultaneously infected by two or more different strains of HIV. When simultaneous infection occurs, the genome of progeny virions may be composed of RNA strands from two different strains. This hybrid virion then infects a new cell where it undergoes replication. As this happens, the reverse transcriptase, by jumping back and forth between the two different RNA templates, will generate a newly synthesized retroviral DNA sequence that is a recombinant between the two parental genomes. This recombination is most obvious when it occurs between subtypes.\n\nThe closely related simian immunodeficiency virus (SIV) has evolved into many strains, classified by the natural host species. SIV strains of the African green monkey (SIVagm) and sooty mangabey (SIVsmm) are thought to have a long evolutionary history with their hosts. These hosts have adapted to the presence of the virus, which is present at high levels in the host's blood, but evokes only a mild immune response, does not cause the development of simian AIDS, and does not undergo the extensive mutation and recombination typical of HIV infection in humans.\n\nIn contrast, when these strains infect species that have not adapted to SIV (\"heterologous\" or similar hosts such as rhesus or cynomologus macaques), the animals develop AIDS and the virus generates genetic diversity similar to what is seen in human HIV infection. Chimpanzee SIV (SIVcpz), the closest genetic relative of HIV-1, is associated with increased mortality and AIDS-like symptoms in its natural host. SIVcpz appears to have been transmitted relatively recently to chimpanzee and human populations, so their hosts have not yet adapted to the virus. This virus has also lost a function of the \"nef\" gene that is present in most SIVs. For non-pathogenic SIV variants, \"nef\" suppresses T cell activation through the CD3 marker. \"Nef\"'s function in non-pathogenic forms of SIV is to downregulate expression of inflammatory cytokines, MHC-1, and signals that affect T cell trafficking. In HIV-1 and SIVcpz, \"nef\" does not inhibit T-cell activation and it has lost this function. Without this function, T cell depletion is more likely, leading to immunodeficiency.\n\nThree groups of HIV-1 have been identified on the basis of differences in the envelope (\"env\") region: M, N, and O. Group M is the most prevalent and is subdivided into eight subtypes (or clades), based on the whole genome, which are geographically distinct. The most prevalent are subtypes B (found mainly in North America and Europe), A and D (found mainly in Africa), and C (found mainly in Africa and Asia); these subtypes form branches in the phylogenetic tree representing the lineage of the M group of HIV-1. Co-infection with distinct subtypes gives rise to circulating recombinant forms (CRFs). In 2000, the last year in which an analysis of global subtype prevalence was made, 47.2% of infections worldwide were of subtype C, 26.7% were of subtype A/CRF02_AG, 12.3% were of subtype B, 5.3% were of subtype D, 3.2% were of CRF_AE, and the remaining 5.3% were composed of other subtypes and CRFs. Most HIV-1 research is focused on subtype B; few laboratories focus on the other subtypes. The existence of a fourth group, \"P\", has been hypothesised based on a virus isolated in 2009. The strain is apparently derived from gorilla SIV (SIVgor), first isolated from western lowland gorillas in 2006.\n\nHIV-2's closest relative is SIVsm, a strain of SIV found in sooty mangabees. Since HIV-1 is derived from SIVcpz, and HIV-2 from SIVsm, the genetic sequence of HIV-2 is only partially homologous to HIV-1 and more closely resembles that of SIVsm.\n\nMany HIV-positive people are unaware that they are infected with the virus. For example, in 2001 less than 1% of the sexually active urban population in Africa had been tested, and this proportion is even lower in rural populations. Furthermore, in 2001 only 0.5% of pregnant women attending urban health facilities were counselled, tested or receive their test results. Again, this proportion is even lower in rural health facilities. Since donors may therefore be unaware of their infection, donor blood and blood products used in medicine and medical research are routinely screened for HIV.\n\nHIV-1 testing is initially done using an enzyme-linked immunosorbent assay (ELISA) to detect antibodies to HIV-1. Specimens with a non-reactive result from the initial ELISA are considered HIV-negative, unless new exposure to an infected partner or partner of unknown HIV status has occurred. Specimens with a reactive ELISA result are retested in duplicate. If the result of either duplicate test is reactive, the specimen is reported as repeatedly reactive and undergoes confirmatory testing with a more specific supplemental test (e.g., a polymerase chain reaction (PCR), western blot or, less commonly, an immunofluorescence assay (IFA)). Only specimens that are repeatedly reactive by ELISA and positive by IFA or PCR or reactive by western blot are considered HIV-positive and indicative of HIV infection. Specimens that are repeatedly ELISA-reactive occasionally provide an indeterminate western blot result, which may be either an incomplete antibody response to HIV in an infected person or nonspecific reactions in an uninfected person.\n\nAlthough IFA can be used to confirm infection in these ambiguous cases, this assay is not widely used. In general, a second specimen should be collected more than a month later and retested for persons with indeterminate western blot results. Although much less commonly available, nucleic acid testing (e.g., viral RNA or proviral DNA amplification method) can also help diagnosis in certain situations. In addition, a few tested specimens might provide inconclusive results because of a low quantity specimen. In these situations, a second specimen is collected and tested for HIV infection.\n\nModern HIV testing is extremely accurate, when the window period is taken into consideration. A single screening test is correct more than 99% of the time. The chance of a false-positive result in a standard two-step testing protocol is estimated to be about 1 in 250,000 in a low risk population. Testing post-exposure is recommended immediately and then at six weeks, three months, and six months.\n\nThe latest recommendations of the US Centers for Disease Control and Prevention (CDC) show that HIV testing must start with an immunoassay combination test for HIV-1 and HIV-2 antibodies and p24 antigen. A negative result rules out HIV exposure, while a positive one must be followed by an HIV-1/2 antibody differentiation immunoassay to detect which antibodies are present. This gives rise to four possible scenarios:\n\nHIV/AIDS research includes all medical research that attempts to prevent, treat, or cure HIV/AIDS, as well as fundamental research about the nature of HIV as an infectious agent and AIDS as the disease caused by HIV.\n\nMany governments and research institutions participate in HIV/AIDS research. This research includes behavioral health interventions, such as research into sex education, and drug development, such as research into microbicides for sexually transmitted diseases, HIV vaccines, and anti-retroviral drugs. Other medical research areas include the topics of pre-exposure prophylaxis, post-exposure prophylaxis, circumcision and HIV, and accelerated aging effects.\n\nThe management of HIV/AIDS normally includes the use of multiple antiretroviral drugs. Treatment has been so successful that in many parts of the world, HIV has become a chronic condition in which progression to AIDS is increasingly rare.\n\nHIV latency, and the consequent viral reservoir in CD4 T cells, dendritic cells, as well as macrophages, is the main barrier to eradication of the virus.\n\nIt is important to note that although HIV is highly virulent, transmission is greatly reduced when an HIV-infected person has a suppressed or undetectable viral load (<50 copies/ml) due to prolonged and successful anti-retroviral treatment. Hence, it can be said to be almost impossible (but still non-zero) for an HIV-infected person who has an undetectable viral load to transmit the virus, even during unprotected sexual intercourse, as there would be a negligible amount of HIV present in the seminal fluid, vaginal secretions or blood, for transmission to occur. This does not mean, however, that prolonged anti-retroviral treatment will result in a suppressed viral load. An undetectable viral load, generally agreed as less than 50 copies per milliliter of blood, can only be proven by a polymerase chain reaction (PCR) test.\n\nAt the same time, it is important to recognise that reaching an undetectable viral load is determined by many factors, including treatment adherence, HIV resistance to certain anti-retroviral drugs, stigma, and inadequate health systems.\n\nAIDS was first clinically observed in 1981 in the United States. The initial cases were a cluster of injection drug users and gay men with no known cause of impaired immunity who showed symptoms of \"Pneumocystis jirovecii\" pneumonia (PJP), a rare opportunistic infection that was known to occur in people with very compromised immune systems. Soon thereafter, additional gay men developed a previously rare skin cancer called Kaposi's sarcoma (KS). Many more cases of PJP and KS emerged, alerting U.S. Centers for Disease Control and Prevention (CDC) and a CDC task force was formed to monitor the outbreak. The earliest retrospectively described case of AIDS is believed to have been in Norway beginning in 1966.\n\nIn the beginning, the CDC did not have an official name for the disease, often referring to it by way of the diseases that were associated with it, for example, lymphadenopathy, the disease after which the discoverers of HIV originally named the virus. They also used \"Kaposi's Sarcoma and Opportunistic Infections\", the name by which a task force had been set up in 1981. In the general press, the term \"GRID\", which stood for gay-related immune deficiency, had been coined. The CDC, in search of a name, and looking at the infected communities coined \"the 4H disease\", as it seemed to single out homosexuals, heroin users, hemophiliacs, and Haitians. However, after determining that AIDS was not isolated to the gay community, it was realized that the term GRID was misleading and \"AIDS\" was introduced at a meeting in July 1982. By September 1982 the CDC started using the name AIDS.\nIn 1983, two separate research groups led by American Robert Gallo and French investigators Françoise Barré-Sinoussi and Luc Montagnier independently declared that a novel retrovirus may have been infecting AIDS patients, and published their findings in the same issue of the journal \"Science\". Gallo claimed that a virus his group had isolated from a person with AIDS was strikingly similar in shape to other human T-lymphotropic viruses (HTLVs) his group had been the first to isolate. Gallo's group called their newly isolated virus HTLV-III. At the same time, Montagnier's group isolated a virus from a patient presenting with swelling of the lymph nodes of the neck and physical weakness, two classic symptoms of primary HIV infection. Contradicting the report from Gallo's group, Montagnier and his colleagues showed that core proteins of this virus were immunologically different from those of HTLV-I. Montagnier's group named their isolated virus lymphadenopathy-associated virus (LAV). As these two viruses turned out to be the same, in 1986 LAV and HTLV-III were renamed HIV.\n\nAnother group working contemporaneously with the Montagnier and Gallo groups was that of Dr. Jay Levy at the University of California, San Francisco. He independently discovered the AIDS virus in 1983 and named it the AIDS associated retrovirus (ARV). This virus was very different from the virus reported by the Montagnier and Gallo groups. The ARV strains indicated, for the first time, the heterogeneity of HIV isolates and several of these remain classic examples of the AIDS virus found in the United States.\n\nBoth HIV-1 and HIV-2 are believed to have originated in non-human primates in West-central Africa, and are believed to have transferred to humans (a process known as zoonosis) in the early 20th century.\n\nHIV-1 appears to have originated in southern Cameroon through the evolution of SIVcpz, a simian immunodeficiency virus (SIV) that infects wild chimpanzees (HIV-1 descends from the SIVcpz endemic in the chimpanzee subspecies \"Pan troglodytes troglodytes\"). The closest relative of HIV-2 is SIVsmm, a virus of the sooty mangabey (\"Cercocebus atys atys\"), an Old World monkey living in littoral West Africa (from southern Senegal to western Côte d'Ivoire). New World monkeys such as the owl monkey are resistant to HIV-1 infection, possibly because of a genomic fusion of two viral resistance genes.\n\nHIV-1 is thought to have jumped the species barrier on at least three separate occasions, giving rise to the three groups of the virus, M, N, and O.\nThere is evidence that humans who participate in bushmeat activities, either as hunters or as bushmeat vendors, commonly acquire SIV. However, SIV is a weak virus, and it is typically suppressed by the human immune system within weeks of infection. It is thought that several transmissions of the virus from individual to individual in quick succession are necessary to allow it enough time to mutate into HIV. Furthermore, due to its relatively low person-to-person transmission rate, it can only spread throughout the population in the presence of one or more high-risk transmission channels, which are thought to have been absent in Africa prior to the 20th century.\n\nSpecific proposed high-risk transmission channels, allowing the virus to adapt to humans and spread throughout the society, depend on the proposed timing of the animal-to-human crossing. Genetic studies of the virus suggest that the most recent common ancestor of the HIV-1 M group dates back to circa 1910. Proponents of this dating link the HIV epidemic with the emergence of colonialism and growth of large colonial African cities, leading to social changes, including different patterns of sexual contact (especially multiple, concurrent partnerships), the spread of prostitution, and the concomitant high frequency of genital ulcer diseases (such as syphilis) in nascent colonial cities. While transmission rates of HIV during vaginal intercourse are typically low, they are increased manyfold if one of the partners suffers from a sexually transmitted infection resulting in genital ulcers. Early 1900s colonial cities were notable due to their high prevalence of prostitution and genital ulcers to the degree that as of 1928 as many as 45% of female residents of eastern Leopoldville were thought to have been prostitutes and as of 1933 around 15% of all residents of the same city were infected by one of the forms of syphilis.\n\nAn alternative view—unsupported by evidence—holds that unsafe medical practices in Africa during years following World War II, such as unsterile reuse of single-use syringes during mass vaccination, antibiotic, and anti-malaria treatment campaigns, were the initial vector that allowed the virus to adapt to humans and spread.\n\nThe earliest, well-documented case of HIV in a human dates back to 1959 in the Belgian Congo. The virus may have been present in the United States as early as the mid-to-late 1950s, as a sixteen-year-old male presented with symptoms in 1966 and died in 1969.\n\n\n\n"}
{"id": "14173", "url": "https://en.wikipedia.org/wiki?curid=14173", "title": "HOL", "text": "HOL\n\nHol or HOL may refer to:\n\n\n\n\n\n\n"}
{"id": "14174", "url": "https://en.wikipedia.org/wiki?curid=14174", "title": "Hostile witness", "text": "Hostile witness\n\nA hostile witness, otherwise known as an adverse witness or an unfavorable witness, is a witness at trial whose testimony on direct examination is either openly antagonistic or appears to be contrary to the legal position of the party who called the witness.\n\nDuring direct examination, if the examining attorney who called the witness finds that their testimony is antagonistic or contrary to the legal position of their client, the attorney may request that the judge declare the witness hostile. If the request is granted, the attorney may proceed to ask the witness leading questions. Leading questions either suggest the answer (\"You saw my client sign the contract, correct?\") or challenge (impeach) the witness's testimony. As a rule, leading questions are generally only allowed during cross-examination, but a hostile witness is an exception to this rule.\n\nIn cross-examination conducted by the opposing party's attorney, a witness is presumed to be hostile and the examining attorney is not required to seek the judge's permission before asking leading questions. Attorneys can influence a hostile witness's responses by using Gestalt psychology to influence the way the witness perceives the situation, and utility theory to understand their likely responses. The attorney will integrate a hostile witness's expected responses into the larger case strategy through pretrial planning and through adapting as necessary during the course of the trial.\n\nIn the state of New South Wales, the term 'unfavourable witness' is defined by section 38 of the Evidence Act which permits the prosecution to cross-examine their own witness. For example, if the prosecution calls all material witnesses relevant to a case before the court, and any evidence given is not favourable to, or supports the prosecution case, or a witness has given a prior inconsistent statement, then the prosecution may seek leave of the court, via section 192, to test the witness in relation to their evidence.\n\nIn New Zealand, section 94 of the Evidence Act 2006 permits a party to cross-examine their own witness if the presiding judge determines the witness to be hostile and gives permission.\n\n\n"}
{"id": "14179", "url": "https://en.wikipedia.org/wiki?curid=14179", "title": "Henry I of England", "text": "Henry I of England\n\nHenry I (c. 1068 – 1 December 1135), also known as Henry Beauclerc, was King of England from 1100 to his death in 1135. Henry was the fourth son of William the Conqueror and was educated in Latin and the liberal arts. On William's death in 1087, Henry's elder brothers Robert Curthose and William Rufus inherited Normandy and England, respectively, but Henry was left landless. Henry purchased the County of Cotentin in western Normandy from Robert, but William and Robert deposed him in 1091. Henry gradually rebuilt his power base in the Cotentin and allied himself with William against Robert. Henry was present when William died in a hunting accident in 1100, and he seized the English throne, promising at his coronation to correct many of William's less popular policies. Henry married Matilda of Scotland but continued to have a large number of mistresses by whom he had many illegitimate children.\n\nRobert, who invaded in 1101, disputed Henry's control of England; this military campaign ended in a negotiated settlement that confirmed Henry as king. The peace was short-lived, and Henry invaded the Duchy of Normandy in 1105 and 1106, finally defeating Robert at the Battle of Tinchebray. Henry kept Robert imprisoned for the rest of his life. Henry's control of Normandy was challenged by Louis VI of France, Baldwin VII of Flanders and Fulk V of Anjou, who promoted the rival claims of Robert's son, William Clito, and supported a major rebellion in the Duchy between 1116 and 1119. Following Henry's victory at the Battle of Brémule, a favourable peace settlement was agreed with Louis in 1120.\n\nConsidered by contemporaries to be a harsh but effective ruler, Henry skilfully manipulated the barons in England and Normandy. In England, he drew on the existing Anglo-Saxon system of justice, local government and taxation, but also strengthened it with additional institutions, including the royal exchequer and itinerant justices. Normandy was also governed through a growing system of justices and an exchequer. Many of the officials who ran Henry's system were \"new men\" of obscure backgrounds rather than from families of high status, who rose through the ranks as administrators. Henry encouraged ecclesiastical reform, but became embroiled in a serious dispute in 1101 with Archbishop Anselm of Canterbury, which was resolved through a compromise solution in 1105. He supported the Cluniac order and played a major role in the selection of the senior clergy in England and Normandy.\n\nHenry's only legitimate son and heir, William Adelin, drowned in the \"White Ship\" disaster of 1120, throwing the royal succession into doubt. Henry took a second wife, Adeliza of Louvain, in the hope of having another son, but their marriage was childless. In response to this, Henry declared his daughter, Empress Matilda, his heir and married her to Geoffrey of Anjou. The relationship between Henry and the couple became strained, and fighting broke out along the border with Anjou. Henry died on 1 December 1135 after a week of illness. Despite his plans for Matilda, the King was succeeded by his nephew, Stephen of Blois, resulting in a period of civil war known as the Anarchy.\nHenry was probably born in England in 1068, in either the summer or the last weeks of the year, possibly in the town of Selby in Yorkshire. His father was William the Conqueror, the Duke of Normandy who had invaded England in 1066 to become the King of England, establishing lands stretching into Wales. The invasion had created an Anglo-Norman elite, many with estates spread across both sides of the English Channel. These Anglo-Norman barons typically had close links to the kingdom of France, which was then a loose collection of counties and smaller polities, under only the minimal control of the king. Henry's mother, Matilda of Flanders, was the granddaughter of Robert II of France, and she probably named Henry after her uncle, King Henry I of France.\n\nHenry was the youngest of William and Matilda's four sons. Physically he resembled his older brothers Robert Curthose, Richard and William Rufus, being, as historian David Carpenter describes, \"short, stocky and barrel-chested,\" with black hair. As a result of their age differences and Richard's early death, Henry would have probably seen relatively little of his older brothers. He probably knew his sister Adela well, as the two were close in age. There is little documentary evidence for his early years; historians Warren Hollister and Kathleen Thompson suggest he was brought up predominantly in England, while Judith Green argues he was initially brought up in the Duchy. He was probably educated by the Church, possibly by Bishop Osmund, the King's chancellor, at Salisbury Cathedral; it is uncertain if this indicated an intent by his parents for Henry to become a member of the clergy. It is also uncertain how far Henry's education extended, but he was probably able to read Latin and had some background in the liberal arts. He was given military training by an instructor called Robert Achard, and Henry was knighted by his father on 24 May 1086.\n\nIn 1087, William was fatally injured during a campaign in the Vexin. Henry joined his dying father near Rouen in September, where the King partitioned his possessions among his sons. The rules of succession in western Europe at the time were uncertain; in some parts of France, primogeniture, in which the eldest son would inherit a title, was growing in popularity. In other parts of Europe, including Normandy and England, the tradition was for lands to be divided up, with the eldest son taking patrimonial lands – usually considered to be the most valuable – and younger sons given smaller, or more recently acquired, partitions or estates.\n\nIn dividing his lands, William appears to have followed the Norman tradition, distinguishing between Normandy, which he had inherited, and England, which he had acquired through war. William's second son, Richard, had died in a hunting accident, leaving Henry and his two brothers to inherit William's estate. Robert, the eldest, despite being in armed rebellion against his father at the time of his death, received Normandy. England was given to William Rufus, who was in favour with the dying king. Henry was given a large sum of money, usually reported as £5,000, with the expectation that he would also be given his mother's modest set of lands in Buckinghamshire and Gloucestershire. William's funeral at Caen was marred by angry complaints from a local man, and Henry may have been responsible for resolving the dispute by buying off the protester with silver.\n\nRobert returned to Normandy, expecting to have been given both the Duchy and England, to find that William Rufus had crossed the Channel and been crowned king. The two brothers disagreed fundamentally over the inheritance, and Robert soon began to plan an invasion of England to seize the kingdom, helped by a rebellion by some of the leading nobles against William Rufus. Henry remained in Normandy and took up a role within Robert's court, possibly either because he was unwilling to side openly with William Rufus, or because Robert might have taken the opportunity to confiscate Henry's inherited money if he had tried to leave. William Rufus sequestered Henry's new estates in England, leaving Henry landless.\n\nIn 1088, Robert's plans for the invasion of England began to falter, and he turned to Henry, proposing that his brother lend him some of his inheritance, which Henry refused. Henry and Robert then came to an alternative arrangement, in which Robert would make Henry the count of western Normandy, in exchange for £3,000. Henry's lands were a new countship based around a delegation of the ducal authority in the Cotentin, but it extended across the Avranchin, with control over the bishoprics of both. This also gave Henry influence over two major Norman leaders, Hugh d'Avranches and Richard de Redvers, and the abbey of Mont Saint-Michel, whose lands spread out further across the Duchy. Robert's invasion force failed to leave Normandy, leaving William Rufus secure in England.\n\nHenry quickly established himself as count, building up a network of followers from western Normandy and eastern Brittany, whom historian John Le Patourel has characterised as \"Henry's gang\". His early supporters included Roger of Mandeville, Richard of Redvers, Richard d'Avranches and Robert Fitzhamon, along with the churchman Roger of Salisbury. Robert attempted to go back on his deal with Henry and re-appropriate the county, but Henry's grip was already sufficiently firm to prevent this. Robert's rule of the Duchy was chaotic, and parts of Henry's lands became almost independent of central control from Rouen.\n\nDuring this period, neither William nor Robert seems to have trusted Henry. Waiting until the rebellion against William Rufus was safely over, Henry returned to England in July 1088. He met with the King but was unable to persuade him to grant him their mother's estates, and travelled back to Normandy in the autumn. While he had been away, however, Odo, the Bishop of Bayeux, who regarded Henry as a potential competitor, had convinced Robert that Henry was conspiring against the duke with William Rufus. On landing, Odo seized Henry and imprisoned him in Neuilly-la-Forêt, and Robert took back the county of the Cotentin. Henry was held there over the winter, but in the spring of 1089 the senior elements of the Normandy nobility prevailed upon Robert to release him.\n\nAlthough no longer formally the Count of Cotentin, Henry continued to control the west of Normandy. The struggle between Henry's brothers continued. William Rufus continued to put down resistance to his rule in England, but began to build a number of alliances against Robert with barons in Normandy and neighbouring Ponthieu. Robert allied himself with Philip I of France. In late 1090 William Rufus encouraged Conan Pilatus, a powerful burgher in Rouen, to rebel against Robert; Conan was supported by most of Rouen and made appeals to the neighbouring ducal garrisons to switch allegiance as well.\n\nRobert issued an appeal for help to his barons, and Henry was the first to arrive in Rouen in November. Violence broke out, leading to savage, confused street fighting as both sides attempted to take control of the city. Robert and Henry left the castle to join the battle, but Robert then retreated, leaving Henry to continue the fighting. The battle turned in favour of the ducal forces and Henry took Conan prisoner. Henry was angry that Conan had turned against his feudal lord. He had him taken to the top of Rouen Castle and then, despite Conan's offers to pay a huge ransom, threw him off the top of the castle to his death. Contemporaries considered Henry to have acted appropriately in making an example of Conan, and Henry became famous for his exploits in the battle.\n\nIn the aftermath, Robert forced Henry to leave Rouen, probably because Henry's role in the fighting had been more prominent than his own, and possibly because Henry had asked to be formally reinstated as the count of the Cotentin. In early 1091, William Rufus invaded Normandy with a sufficiently large army to bring Robert to the negotiating table. The two brothers signed a treaty at Rouen, granting William Rufus a range of lands and castles in Normandy. In return, William Rufus promised to support Robert's attempts to regain control of the neighbouring county of Maine, once under Norman control, and help in regaining control over the Duchy, including Henry's lands. They nominated each other as heirs to England and Normandy, excluding Henry from any succession while either one of them lived.\n\nWar now broke out between Henry and his brothers. Henry mobilised a mercenary army in the west of Normandy, but as William Rufus and Robert's forces advanced, his network of baronial support melted away. Henry focused his remaining forces at Mont Saint-Michel, where he was besieged, probably in March 1091. The site was easy to defend, but lacked fresh water. The chronicler William of Malmesbury suggested that when Henry's water ran short, Robert allowed his brother fresh supplies, leading to remonstrations between Robert and William Rufus. The events of the final days of the siege are unclear: the besiegers had begun to argue about the future strategy for the campaign, but Henry then abandoned Mont Saint-Michel, probably as part of a negotiated surrender. He left for Brittany and crossed over into France.\n\nHenry's next steps are not well documented; one chronicler, Orderic Vitalis, suggests that he travelled in the French Vexin, along the Normandy border, for over a year with a small band of followers. By the end of the year, Robert and William Rufus had fallen out once again, and the Treaty of Rouen had been abandoned. In 1092, Henry and his followers seized the Normandy town of Domfront. Domfront had previously been controlled by Robert of Bellême, but the inhabitants disliked his rule and invited Henry to take over the town, which he did in a bloodless coup. Over the next two years, Henry re-established his network of supporters across western Normandy, forming what Judith Green terms a \"court in waiting\". By 1094, he was allocating lands and castles to his followers as if he were the Duke of Normandy. William Rufus began to support Henry with money, encouraging his campaign against Robert, and Henry used some of this to construct a substantial castle at Domfront.\n\nWilliam Rufus crossed into Normandy to take the war to Robert in 1094, and when progress stalled, called upon Henry for assistance. Henry responded, but travelled to London instead of joining the main campaign further east in Normandy, possibly at the request of the King, who in any event abandoned the campaign and returned to England. Over the next few years, Henry appears to have strengthened his power base in western Normandy, visiting England occasionally to attend at William Rufus's court. In 1095 Pope Urban II called the First Crusade, encouraging knights from across Europe to join. Robert joined the Crusade, borrowing money from William Rufus to do so, and granting the King temporary custody of his part of the Duchy in exchange. The King appeared confident of regaining the remainder of Normandy from Robert, and Henry appeared ever closer to William Rufus, the pair campaigning together in the Norman Vexin between 1097 and 1098.\n\nOn the afternoon of 2 August 1100, the King had gone hunting in the New Forest, accompanied by a team of huntsmen and a number of the Norman nobility, including Henry. An arrow, possibly shot by the baron Walter Tirel, hit and killed William Rufus. Numerous conspiracy theories have been put forward suggesting that the King was killed deliberately; most modern historians reject these, as hunting was a risky activity, and such accidents were common. Chaos broke out, and Tirel fled the scene for France, either because he had shot the fatal arrow, or because he had been incorrectly accused and feared that he would be made a scapegoat for the King's death.\n\nHenry rode to Winchester, where an argument ensued as to who now had the best claim to the throne. William of Breteuil championed the rights of Robert, who was still abroad, returning from the Crusade, and to whom Henry and the barons had given homage in previous years. Henry argued that, unlike Robert, he had been born to a reigning king and queen, thereby giving him a claim under the right of porphyrogeniture. Tempers flared, but Henry, supported by Henry de Beaumont and Robert of Meulan, held sway and persuaded the barons to follow him. He occupied Winchester Castle and seized the royal treasury.\n\nHenry was hastily crowned king in Westminster Abbey on 5 August by Maurice, the Bishop of London, as Anselm, the Archbishop of Canterbury, had been exiled by William Rufus, and Thomas, the Archbishop of York, was in the north of England at Ripon. In accordance with English tradition and in a bid to legitimise his rule, Henry issued a coronation charter laying out various commitments. The new king presented himself as having restored order to a trouble-torn country. He announced that he would abandon William Rufus's policies towards the Church, which had been seen as oppressive by the clergy; he promised to prevent royal abuses of the barons' property rights, and assured a return to the gentler customs of Edward the Confessor; he asserted that he would \"establish a firm peace\" across England and ordered \"that this peace shall henceforth be kept\".\n\nIn addition to his existing circle of supporters, many of whom were richly rewarded with new lands, Henry quickly co-opted many of the existing administration into his new royal household. William Giffard, William Rufus's chancellor, was made the Bishop of Winchester, and the prominent sheriffs Urse d'Abetot, Haimo Dapifer and Robert Fitzhamon continued to play a senior role in government. By contrast, the unpopular Ranulf Flambard, the Bishop of Durham and a key member of the previous regime, was imprisoned in the Tower of London and charged with corruption. The late king had left many church positions unfilled, and Henry set about nominating candidates to these, in an effort to build further support for his new government. The appointments needed to be consecrated, and Henry wrote to Anselm, apologising for having been crowned while the Archbishop was still in France and asking him to return at once.\n\nOn 11 November 1100 Henry married Matilda, the daughter of Malcolm III of Scotland. Henry was now around 31 years old, but late marriages for noblemen were not unusual in the 11th century. The pair had probably first met earlier the previous decade, possibly being introduced through Bishop Osmund of Salisbury. Historian Warren Hollister argues that Henry and Matilda were emotionally close, but their union was also certainly politically motivated. Matilda had originally been named Edith, an Anglo-Saxon name, and was a member of the West Saxon royal family, being the niece of Edgar the Ætheling, the great-granddaughter of Edmund Ironside and a descendant of Alfred the Great. For Henry, marrying Matilda gave his reign increased legitimacy, and for Matilda, an ambitious woman, it was an opportunity for high status and power in England.\n\nMatilda had been educated in a sequence of convents, however, and may well have taken the vows to formally become a nun, which formed an obstacle to the marriage progressing. She did not wish to be a nun and appealed to Anselm for permission to marry Henry, and the Archbishop established a council at Lambeth Palace to judge the issue. Despite some dissenting voices, the council concluded that although Matilda had lived in a convent, she had not actually become a nun and was therefore free to marry, a judgement that Anselm then affirmed, allowing the marriage to proceed. Matilda proved an effective queen for Henry, acting as a regent in England on occasion, addressing and presiding over councils, and extensively supporting the arts. The couple soon had two children, Matilda, born in 1102, and William Adelin, born in 1103; it is possible that they also had a second son, Richard, who died young. Following the birth of these children, Matilda preferred to remain based in Westminster while Henry travelled across England and Normandy, either for religious reasons or because she enjoyed being involved in the machinery of royal governance.\n\nHenry had a considerable sexual appetite and enjoyed a substantial number of sexual partners, resulting in a large number of illegitimate children, at least nine sons and 13 daughters, many of whom he appears to have recognised and supported. It was normal for unmarried Anglo-Norman noblemen to have sexual relations with prostitutes and local women, and kings were also expected to have mistresses. Some of these relationships occurred before Henry was married, but many others took place after his marriage to Matilda. Henry had a wide range of mistresses from a range of backgrounds, and the relationships appear to have been conducted relatively openly. He may have chosen some of his noble mistresses for political purposes, but the evidence to support this theory is limited.\n\nBy early 1101, Henry's new regime was established and functioning, but many of the Anglo-Norman elite still supported Robert, or would be prepared to switch sides if Henry's elder brother appeared likely to gain power in England. In February, Flambard escaped from the Tower of London and crossed the Channel to Normandy, where he injected fresh direction and energy to Robert's attempts to mobilise an invasion force. By July, Robert had formed an army and a fleet, ready to move against Henry in England. Raising the stakes in the conflict, Henry seized Flambard's lands and, with the support of Anselm, Flambard was removed from his position as bishop. Henry held court in April and June, where the nobility renewed their oaths of allegiance to him, but their support still appeared partial and shaky.\n\nWith the invasion imminent, Henry mobilised his forces and fleet outside Pevensey, close to Robert's anticipated landing site, training some of them personally in how to counter cavalry charges. Despite English levies and knights owing military service to the Church arriving in considerable numbers, many of his barons did not appear. Anselm intervened with some of the doubters, emphasising the religious importance of their loyalty to Henry. Robert unexpectedly landed further up the coast at Portsmouth on 20 July with a modest force of a few hundred men, but these were quickly joined by many of the barons in England. However, instead of marching into nearby Winchester and seizing Henry's treasury, Robert paused, giving Henry time to march west and intercept the invasion force.\n\nThe two armies met at Alton where peace negotiations began, possibly initiated by either Henry or Robert, and probably supported by Flambard. The brothers then agreed to the Treaty of Alton, under which Robert released Henry from his oath of homage and recognised him as king; Henry renounced his claims on western Normandy, except for Domfront, and agreed to pay Robert £2,000 a year for life; if either brother died without a male heir, the other would inherit his lands; the barons whose lands had been seized by either the King or the Duke for supporting his rival would have them returned, and Flambard would be reinstated as bishop; the two brothers would campaign together to defend their territories in Normandy. Robert remained in England for a few months more with Henry before returning to Normandy.\n\nDespite the treaty, Henry set about inflicting severe penalties on the barons who had stood against him during the invasion. William de Warenne, the Earl of Surrey, was accused of fresh crimes, which were not covered by the Alton amnesty, and was banished from England. In 1102 Henry then turned against Robert of Bellême and his brothers, the most powerful of the barons, accusing him of 45 different offences. Robert escaped and took up arms against Henry. Henry besieged Robert's castles at Arundel, Tickhill and Shrewsbury, pushing down into the south-west to attack Bridgnorth. His power base in England broken, Robert accepted Henry's offer of banishment and left the country for Normandy.\n\nHenry's network of allies in Normandy became stronger during 1103. Henry arranged the marriages of his illegitimate daughters, Juliane and Matilda, to Eustace of Breteuil and Rotrou III, Count of Perche, respectively, the latter union securing the Norman border. Henry attempted to win over other members of the Norman nobility and gave other English estates and lucrative offers to key Norman lords. Duke Robert continued to fight Robert of Bellême, but the Duke's position worsened, until by 1104, he had to ally himself formally with Bellême to survive. Arguing that Duke Robert had broken the terms of their treaty, Henry crossed over the Channel to Domfront, where he met with senior barons from across Normandy, eager to ally themselves with the King. Henry confronted his brother and accused him of siding with his enemies, before returning to England.\n\nNormandy continued to disintegrate into chaos. In 1105, Henry sent his friend Robert Fitzhamon and a force of knights into the Duchy, apparently to provoke a confrontation with Duke Robert. Fitzhamon was captured, and Henry used this as an excuse to invade, promising to restore peace and order. Henry had the support of most of the neighbouring counts around Normandy's borders, and King Philip of France was persuaded to remain neutral. Henry occupied western Normandy, and advanced east on Bayeux, where Fitzhamon was held. The city refused to surrender, and Henry besieged it, burning it to the ground. Terrified of meeting the same fate, the town of Caen switched sides and surrendered, allowing Henry to advance on Falaise, which he took with some casualties. Henry's campaign stalled, and the King instead began peace discussions with Robert. The negotiations were inconclusive and the fighting dragged on until Christmas, when Henry returned to England.\n\nHenry invaded again in July 1106, hoping to provoke a decisive battle. After some initial tactical successes, he turned south-west towards the castle of Tinchebray. He besieged the castle and Duke Robert, supported by Robert of Bellême, advanced from Falaise to relieve it. After attempts at negotiation failed, the Battle of Tinchebray took place, probably on 28 September. The battle lasted around an hour, and began with a charge by Duke Robert's cavalry; the infantry and dismounted knights of both sides then joined the battle. Henry's reserves, led by Elias I, Count of Maine, and Alan IV, Duke of Brittany, attacked the enemy's flanks, routing first Bellême's troops and then the bulk of the ducal forces. Duke Robert was taken prisoner, but Bellême escaped.\n\nHenry mopped up the remaining resistance in Normandy, and Robert ordered his last garrisons to surrender. Reaching Rouen, Henry reaffirmed the laws and customs of Normandy and took homage from the leading barons and citizens. The lesser prisoners taken at Tinchebray were released, but Robert and several other leading nobles were imprisoned indefinitely. Henry's nephew, Robert's son William Clito, was only three years old and was released to the care of Helias of Saint-Saens, a Norman baron. Henry reconciled himself with Robert of Bellême, who gave up the ducal lands he had seized and rejoined the royal court. Henry had no way of legally removing the Duchy from his brother Robert, and initially Henry avoided using the title \"duke\" at all, emphasising that, as the King of England, he was only acting as the guardian of the troubled Duchy.\n\nHenry inherited the kingdom of England from William Rufus, giving him a claim of suzerainty over Wales and Scotland, and acquired the Duchy of Normandy, a complex entity with troubled borders. The borders between England and Scotland were still uncertain during Henry's reign, with Anglo-Norman influence pushing northwards through Cumbria, but Henry's relationship with King David I of Scotland was generally good, partially due to Henry's marriage to his sister. In Wales, Henry used his power to coerce and charm the indigenous Welsh princes, while Norman Marcher Lords pushed across the valleys of South Wales. Normandy was controlled via various interlocking networks of ducal, ecclesiastical and family contacts, backed by a growing string of important ducal castles along the borders. Alliances and relationships with neighbouring counties along the Norman border were particularly important to maintaining the stability of the Duchy.\n\nHenry ruled through the various barons and lords in England and Normandy, whom he manipulated skilfully for political effect. Political friendships, termed \"amicitia\" in Latin, were important during the 12th century, and Henry maintained a wide range of these, mediating between his friends in various factions across his realm when necessary, and rewarding those who were loyal to him. Henry also had a reputation for punishing those barons who stood against him, and he maintained an effective network of informers and spies who reported to him on events. Henry was a harsh, firm ruler, but not excessively so by the standards of the day. Over time, he increased the degree of his control over the barons, removing his enemies and bolstering his friends until the \"reconstructed baronage\", as historian Warren Hollister describes it, was predominantly loyal and dependent on the King.\n\nHenry's itinerant royal court comprised various parts. At the heart was Henry's domestic household, called the \"domus\"; a wider grouping was termed the \"familia regis\", and formal gatherings of the court were termed \"curia\". The \"domus\" was divided into several parts. The chapel, headed by the chancellor, looked after the royal documents, the chamber dealt with financial affairs and the master-marshal was responsible for travel and accommodation. The \"familia regis\" included Henry's mounted household troops, up to several hundred strong, who came from a wider range of social backgrounds, and could be deployed across England and Normandy as required. Initially Henry continued his father's practice of regular crown-wearing ceremonies at his \"curia\", but they became less frequent as the years passed. Henry's court was grand and ostentatious, financing the construction of large new buildings and castles with a range of precious gifts on display, including the King's private menagerie of exotic animals, which he kept at Woodstock Palace. Despite being a lively community, Henry's court was more tightly controlled than those of previous kings. Strict rules controlled personal behaviour and prohibited members of the court from pillaging neighbouring villages, as had been the norm under William Rufus.\n\nHenry was responsible for a substantial expansion of the royal justice system. In England, Henry drew on the existing Anglo-Saxon system of justice, local government and taxes, but strengthened it with additional central governmental institutions. Roger of Salisbury began to develop the royal exchequer after 1110, using it to collect and audit revenues from the King's sheriffs in the shires. Itinerant justices began to emerge under Henry, travelling around the country managing eyre courts, and many more laws were formally recorded. Henry gathered increasing revenue from the expansion of royal justice, both from fines and from fees. The first Pipe Roll that is known to have survived dates from 1130, recording royal expenditures. Henry reformed the coinage in 1107, 1108 and in 1125, inflicting harsh corporal punishments to English coiners who had been found guilty of debasing the currency. In Normandy, Henry restored law and order after 1106, operating through a body of Norman justices and an exchequer system similar to that in England. Norman institutions grew in scale and scope under Henry, although less quickly than in England. Many of the officials that ran Henry's system were termed \"new men\", relatively low-born individuals who rose through the ranks as administrators, managing justice or the royal revenues.\n\nHenry's ability to govern was intimately bound up with the Church, which formed the key to the administration of both England and Normandy, and this relationship changed considerably over the course of his reign. William the Conqueror had reformed the English Church with the support of his Archbishop of Canterbury, Lanfranc, who became a close colleague and advisor to the King. Under William Rufus this arrangement had collapsed, the King and Archbishop Anselm had become estranged and Anselm had gone into exile. Henry also believed in Church reform, but on taking power in England he became embroiled in the investiture controversy.\n\nThe argument concerned who should invest a new bishop with his staff and ring: traditionally, this had been carried out by the king in a symbolic demonstration of royal power, but Pope Urban II had condemned this practice in 1099, arguing that only the papacy could carry out this task, and declaring that the clergy should not give homage to their local temporal rulers. Anselm returned to England from exile in 1100 having heard Urban's pronouncement, and informed Henry that he would be complying with the Pope's wishes. Henry was in a difficult position. On one hand, the symbolism and homage was important to him; on the other hand, he needed Anselm's support in his struggle with his brother Duke Robert.\n\nAnselm stuck firmly to the letter of the papal decree, despite Henry's attempts to persuade him to give way in return for a vague assurance of a future royal compromise. Matters escalated, with Anselm going back into exile and Henry confiscating the revenues of his estates. Anselm threatened excommunication, and in July 1105 the two men finally negotiated a solution. A distinction was drawn between the secular and ecclesiastical powers of the prelates, under which Henry gave up his right to invest his clergy, but retained the custom of requiring them to come and do homage for the temporalities, the landed properties they held in England. Despite this argument, the pair worked closely together, combining to deal with Duke Robert's invasion of 1101, for example, and holding major reforming councils in 1102 and 1108.\n\nA long-running dispute between the Archbishops of Canterbury and York flared up under Anselm's successor, Ralph d'Escures. Canterbury, traditionally the senior of the two establishments, had long argued that the Archbishop of York should formally promise to obey their Archbishop, but York argued that the two episcopates were independent within the English Church and that no such promise was necessary. Henry supported the primacy of Canterbury, to ensure that England remained under a single ecclesiastical administration, but the Pope preferred the case of York. The matter was complicated by Henry's personal friendship with Thurstan, the Archbishop of York, and the King's desire that the case should not end up in a papal court, beyond royal control. Henry badly needed the support of the Papacy in his struggle with Louis of France, however, and therefore allowed Thurstan to attend the Council of Rheims in 1119, where Thurstan was then consecrated by the Pope with no mention of any duty towards Canterbury. Henry believed that this went against assurances Thurstan had previously made and exiled him from England until the King and Archbishop came to a negotiated solution the following year.\n\nEven after the investiture dispute, the King continued to play a major role in the selection of new English and Norman bishops and archbishops. Henry appointed many of his officials to bishoprics and, as historian Martin Brett suggests, \"some of his officers could look forward to a mitre with all but absolute confidence\". Henry's chancellors, and those of his queens, became bishops of Durham, Hereford, London, Lincoln, Winchester and Salisbury. Henry increasingly drew on a wider range of these bishops as advisors – particularly Roger of Salisbury – breaking with the earlier tradition of relying primarily on the Archbishop of Canterbury. The result was a cohesive body of administrators through which Henry could exercise careful influence, holding general councils to discuss key matters of policy. This stability shifted slightly after 1125, when Henry began to inject a wider range of candidates into the senior positions of the Church, often with more reformist views, and the impact of this generation would be felt in the years after Henry's death.\n\nLike other rulers of the period, Henry donated to the Church and patronised various religious communities, but contemporary chroniclers did not consider him an unusually pious king. His personal beliefs and piety may, however, have developed during the course of his life. Henry had always taken an interest in religion, but in his later years he may have become much more concerned about spiritual affairs. If so, the major shifts in his thinking would appear to have occurred after 1120, when his son William Adelin died, and 1129, when his daughter's marriage teetered on the verge of collapse.\n\nAs a proponent of religious reform, Henry gave extensively to reformist groups within the Church. He was a keen supporter of the Cluniac order, probably for intellectual reasons. He donated money to the abbey at Cluny itself, and after 1120 gave generously to Reading Abbey, a Cluniac establishment. Construction on Reading began in 1121, and Henry endowed it with rich lands and extensive privileges, making it a symbol of his dynastic lines. He also focused effort on promoting the conversion of communities of clerks into Augustinian canons, the foundation of leper hospitals, expanding the provision of nunneries, and the charismatic orders of the Savigniacs and Tironensians. He was an avid collector of relics, sending an embassy to Constantinople in 1118 to collect Byzantine items, some of which were donated to Reading Abbey.\n\nNormandy faced an increased threat from France, Anjou and Flanders after 1108. Louis VI succeeded to the French throne in 1108 and began to reassert central royal power. Louis demanded Henry give homage to him and that two disputed castles along the Normandy border be placed into the control of neutral castellans. Henry refused, and Louis responded by mobilising an army. After some arguments, the two kings negotiated a truce and retreated without fighting, leaving the underlying issues unresolved. Fulk V assumed power in Anjou in 1109 and began to rebuild Angevin authority. Fulk also inherited the county of Maine, but refused to recognise Henry as his feudal lord and instead allied himself with Louis. Robert II of Flanders also briefly joined the alliance, before his death in 1111.\n\nIn 1108, Henry betrothed his six-year-old daughter, Matilda, to Henry V, the future Holy Roman Emperor. For King Henry, this was a prestigious match; for Henry V, it was an opportunity to restore his financial situation and fund an expedition to Italy, as he received a dowry of £6,666 from England and Normandy. Raising this money proved challenging, and required the implementation of a special \"aid\", or tax, in England. Matilda was crowned German queen in 1110.\n\nHenry responded to the French and Angevin threat by expanding his own network of supporters beyond the Norman borders. Some Norman barons deemed unreliable were arrested or dispossessed, and Henry used their forfeited estates to bribe his potential allies in the neighbouring territories, in particular Maine. Around 1110, Henry attempted to arrest the young William Clito, but William's mentors moved him to the safety of Flanders before he could be taken. At about this time, Henry probably began to style himself as the duke of Normandy. Robert of Bellême turned against Henry once again, and when he appeared at Henry's court in 1112 in a new role as a French ambassador, he was arrested and imprisoned.\n\nRebellions broke out in France and Anjou between 1111 and 1113, and Henry crossed into Normandy to support his nephew, Count Theobald II of Blois, who had sided against Louis in the uprising. In a bid to diplomatically isolate the French king, Henry betrothed his young son, William Adelin, to Fulk's daughter Matilda, and married his illegitimate daughter Matilda to Duke Conan III of Brittany, creating alliances with Anjou and Brittany respectively. Louis backed down and in March 1113 met with Henry near Gisors to agree a peace settlement, giving Henry the disputed fortresses and confirming Henry's overlordship of Maine, Bellême and Brittany.\n\nMeanwhile, the situation in Wales was deteriorating. Henry had conducted a campaign in South Wales in 1108, pushing out royal power in the region and colonising the area around Pembroke with Flemings. By 1114, some of the resident Norman lords were under attack, while in Mid-Wales, Owain ap Cadwgan blinded one of the political hostages he was holding, and in North Wales Gruffudd ap Cynan threatened the power of the Earl of Chester. Henry sent three armies into Wales that year, with Gilbert Fitz Richard leading a force from the south, Alexander, King of Scotland, pressing from the north and Henry himself advancing into Mid-Wales. Owain and Gruffudd sued for peace, and Henry accepted a political compromise. Henry reinforced the Welsh Marches with his own appointees, strengthening the border territories.\n\nConcerned about the succession, Henry sought to persuade Louis VI to accept his son, William Adelin, as the legitimate future Duke of Normandy, in exchange for his son's homage. Henry crossed into Normandy in 1115 and assembled the Norman barons to swear loyalty; he also almost successfully negotiated a settlement with King Louis, affirming William's right to the Duchy in exchange for a large sum of money, but the deal fell through and Louis, backed by his ally Baldwin of Flanders, instead declared that he considered William Clito the legitimate heir to the Duchy.\n\nWar broke out after Henry returned to Normandy with an army to support Theobald of Blois, who was under attack from Louis. Henry and Louis raided each other's towns along the border, and a wider conflict then broke out, probably in 1116. Henry was pushed onto the defensive as French, Flemish and Angevin forces began to pillage the Normandy countryside. Amaury III of Montfort and many other barons rose up against Henry, and there was an assassination plot from within his own household. Henry's wife, Matilda, died in early 1118, but the situation in Normandy was sufficiently pressing that Henry was unable to return to England for her funeral.\n\nHenry responded by mounting campaigns against the rebel barons and deepening his alliance with Theobald. Baldwin of Flanders was wounded in battle and died in September 1118, easing the pressure on Normandy from the north-east. Henry attempted to crush a revolt in the city of Alençon, but was defeated by Fulk and the Angevin army. Forced to retreat from Alençon, Henry's position deteriorated alarmingly, as his resources became overstretched and more barons abandoned his cause. Early in 1119, Eustace of Breteuil and Henry's daughter, Juliana, threatened to join the baronial revolt. Hostages were exchanged in a bid to avoid conflict, but relations broke down and both sides mutilated their captives. Henry attacked and took the town of Breteuil, despite Juliana's attempt to kill her father with a crossbow. In the aftermath, Henry dispossessed the couple of almost all of their lands in Normandy.\n\nHenry's situation improved in May 1119 when he enticed Fulk to switch sides by finally agreeing to marry William Adelin to Fulk's daughter, Matilda, and paying Fulk a large sum of money. Fulk left for the Levant, leaving the County of Maine in Henry's care, and the King was free to focus on crushing his remaining enemies. During the summer Henry advanced into the Norman Vexin, where he encountered Louis's army, resulting in the Battle of Brémule. Henry appears to have deployed scouts and then organised his troops into several carefully formed lines of dismounted knights. Unlike Henry's forces, the French knights remained mounted; they hastily charged the Anglo-Norman positions, breaking through the first rank of the defences but then becoming entangled in Henry's second line of knights. Surrounded, the French army began to collapse. In the melee, Henry was hit by a sword blow, but his armour protected him. Louis and William Clito escaped from the battle, leaving Henry to return to Rouen in triumph.\n\nThe war slowly petered out after this battle, and Louis took the dispute over Normandy to Pope Callixtus II's council in Reims that October. Henry faced a number of French complaints concerning his acquisition and subsequent management of Normandy, and despite being defended by Geoffrey, the Archbishop of Rouen, Henry's case was shouted down by the pro-French elements of the council. Callixtus declined to support Louis, however, and merely advised the two rulers to seek peace. Amaury de Montfort came to terms with Henry, but Henry and William Clito failed to find a mutually satisfactory compromise. In June 1120, Henry and Louis formally made peace on terms advantageous to the King of England: William Adelin gave homage to Louis, and in return Louis confirmed William's rights to the Duchy.\n\nHenry's succession plans were thrown into chaos by the sinking of the \"White Ship\" on 25 November 1120. Henry had left the port of Barfleur for England in the early evening, leaving William Adelin and many of the younger members of the court to follow on that night in a separate vessel, the \"White Ship\". Both the crew and passengers were drunk and, just outside the harbour, the ship hit a submerged rock. The ship sank, killing as many as 300 people, with only one survivor, a butcher from Rouen. Henry's court was initially too scared to report William's death to the King. When he was finally told, he collapsed with grief.\n\nThe disaster left Henry with no legitimate son, his various nephews now the closest possible male heirs. Henry announced he would take a new wife, Adeliza of Louvain, opening up the prospect of a new royal son, and the two were married at Windsor Castle in January 1121. Henry appears to have chosen her because she was attractive and came from a prestigious noble line. Adeliza seems to have been fond of Henry and joined him in his travels, probably to maximise the chances of her conceiving a child. The \"White Ship\" disaster initiated fresh conflict in Wales, where the drowning of Richard, Earl of Chester, encouraged a rebellion led by Maredudd ap Bleddyn. Henry intervened in North Wales that summer with an army and, although the King was hit by a Welsh arrow, the campaign reaffirmed royal power across the region.\n\nWith William dead, Henry's alliance with Anjou – which had been based on his son marrying Fulk's daughter – began to disintegrate. Fulk returned from the Levant and demanded that Henry return Matilda and her dowry, a range of estates and fortifications in Maine. Matilda left for Anjou, but Henry argued that the dowry had in fact originally belonged to him before it came into the possession of Fulk, and so declined to hand the estates back to Anjou. Fulk married his daughter Sibylla to William Clito, and granted them Maine. Once again, conflict broke out, as Amaury de Montfort allied himself with Fulk and led a revolt along the Norman-Anjou border in 1123. Amaury was joined by several other Norman barons, headed by Waleran de Beaumont, one of the sons of Henry's old ally, Robert of Meulan.\n\nHenry dispatched Robert of Gloucester and Ranulf le Meschin to Normandy and then intervened himself in late 1123. Henry began the process of besieging the rebel castles, before wintering in the Duchy. In the spring of 1124, campaigning began again. In the battle of Bourgthéroulde, Odo Borleng, castellan of Bernay, led the king’s army and received intelligence that the rebels were departing from the rebel base in Beaumont-le-Roger allowing him to ambush them as they traversed through the Brotonne forest. Waleran charged the royal forces, but his knights were cut down by Odo's archers and the rebels were quickly overwhelmed. Waleran was captured, but Amaury escaped. Henry mopped up the remainder of the rebellion, blinding some of the rebel leaders – considered, at the time, a more merciful punishment than execution – and recovering the last rebel castles. Henry paid Pope Callixtus a large amount of money, in exchange for the Papacy annulling the marriage of William Clito and Sibylla on the grounds of consanguinity.\n\nHenry and his new wife did not conceive any children, generating prurient speculation as to the possible explanation, and the future of the dynasty appeared at risk. Henry may have begun to look among his nephews for a possible heir. He may have considered Stephen of Blois as a possible option and, perhaps in preparation for this, he arranged a beneficial marriage for Stephen to a wealthy heiress, Matilda. Theobald of Blois, his close ally, may have also felt that he was in favour with Henry. William Clito, who was King Louis's preferred choice, remained opposed to Henry and was therefore unsuitable. Henry may have also considered his own illegitimate son, Robert of Gloucester, as a possible candidate, but English tradition and custom would have looked unfavourably on this.\n\nHenry's plans shifted when the Empress Matilda's husband, the Emperor Henry, died in 1125. King Henry recalled his daughter to England the next year and declared that, should he die without a male heir, she was to be his rightful successor. The Anglo-Norman barons were gathered together at Westminster at Christmas 1126, where they swore to recognise Matilda and any future legitimate heir she might have. Putting forward a woman as a potential heir in this way was unusual: opposition to Matilda continued to exist within the English court, and Louis was vehemently opposed to her candidacy.\n\nFresh conflict broke out in 1127, when Charles, the childless Count of Flanders, was murdered, creating a local succession crisis. Backed by King Louis, William Clito was chosen by the Flemings to become their new ruler. This development potentially threatened Normandy, and Henry began to finance a proxy war in Flanders, promoting the claims of William's Flemish rivals. In an effort to disrupt the French alliance with William, Henry mounted an attack into France in 1128, forcing Louis to cut his aid to William. William died unexpectedly in July, removing the last major challenger to Henry's rule and bringing the war in Flanders to a halt. Without William, the baronial opposition in Normandy lacked a leader. A fresh peace was made with France, and the King was finally able to release the remaining prisoners from the revolt of 1123, including Waleran of Meulan, who was rehabilitated into the royal court.\n\nMeanwhile, Henry rebuilt his alliance with Fulk of Anjou, this time by marrying Matilda to Fulk's eldest son, Geoffrey. The pair were betrothed in 1127 and married the following year. It is unknown whether Henry intended Geoffrey to have any future claim on England or Normandy, and he was probably keeping his son-in-law's status deliberately uncertain. Similarly, although Matilda was granted a number of Normandy castles as part of her dowry, it was not specified when the couple would actually take possession of them. Fulk left Anjou for Jerusalem in 1129, declaring Geoffrey the Count of Anjou and Maine. The marriage proved difficult, as the couple did not particularly like each other and the disputed castles proved a point of contention, resulting in Matilda returning to Normandy later that year. Henry appears to have blamed Geoffrey for the separation, but in 1131 the couple were reconciled. Much to the pleasure and relief of Henry, Matilda then gave birth to a sequence of two sons, Henry and Geoffrey, in 1133 and 1134.\n\nRelations between Henry, Matilda, and Geoffrey became increasingly strained during the King's final years. Matilda and Geoffrey suspected that they lacked genuine support in England. In 1135 they urged Henry to hand over the royal castles in Normandy to Matilda whilst he was still alive, and insisted that the Norman nobility swear immediate allegiance to her, thereby giving the couple a more powerful position after Henry's death. Henry angrily declined to do so, probably out of concern that Geoffrey would try to seize power in Normandy. A fresh rebellion broke out amongst the barons in southern Normandy, led by William, the Count of Ponthieu, whereupon Geoffrey and Matilda intervened in support of the rebels.\n\nHenry campaigned throughout the autumn, strengthening the southern frontier, and then travelled to Lyons-la-Forêt in November to enjoy some hunting, still apparently healthy. There Henry fell ill – according to the chronicler Henry of Huntingdon, he ate too many (\"a surfeit of\") lampreys against his physician's advice – and his condition worsened over the course of a week. Once the condition appeared terminal, Henry gave confession and summoned Archbishop Hugh of Amiens, who was joined by Robert of Gloucester and other members of the court. In accordance with custom, preparations were made to settle Henry's outstanding debts and to revoke outstanding sentences of forfeiture. The King died on 1 December 1135, and his corpse was taken to Rouen accompanied by the barons, where it was embalmed; his entrails were buried locally at the priory of Notre-Dame du Pré, and the preserved body was taken on to England, where it was interred at Reading Abbey.\n\nDespite Henry's efforts, the succession was disputed. When news began to spread of the King's death, Geoffrey and Matilda were in Anjou supporting the rebels in their campaign against the royal army, which included a number of Matilda's supporters such as Robert of Gloucester. Many of these barons had taken an oath to stay in Normandy until the late king was properly buried, which prevented them from returning to England. The Norman nobility discussed declaring Theobald of Blois king. Theobald's younger brother, Stephen of Blois, quickly crossed from Boulogne to England, however, accompanied by his military household. With the help of his brother, Henry of Blois, he seized power in England and was crowned king on 22 December. The Empress Matilda did not give up her claim to England and Normandy, leading to the prolonged civil war known as the Anarchy between 1135 and 1153.\n\nHistorians have drawn on a range of sources on Henry, including the accounts of chroniclers; other documentary evidence, including early pipe rolls; and surviving buildings and architecture. The three main chroniclers to describe the events of Henry's life were William of Malmesbury, Orderic Vitalis, and Henry of Huntingdon, but each incorporated extensive social and moral commentary into their accounts and borrowed a range of literary devices and stereotypical events from other popular works. Other chroniclers include Eadmer, Hugh the Chanter, Abbot Suger, and the authors of the Welsh \"Brut\". Not all royal documents from the period have survived, but there are a number of royal acts, charters, writs, and letters, along with some early financial records. Some of these have since been discovered to be forgeries, and others had been subsequently amended or tampered with.\n\nLate medieval historians seized on the accounts of selected chroniclers regarding Henry's education and gave him the title of Henry \"Beauclerc\", a theme echoed in the analysis of Victorian and Edwardian historians such as Francis Palgrave and Henry Davis. The historian Charles David dismissed this argument in 1929, showing the more extreme claims for Henry's education to be without foundation. Modern histories of Henry commenced with Richard Southern's work in the early 1960s, followed by extensive research during the rest of the 20th century into a wide number of themes from his reign in England, and a much more limited number of studies of his rule in Normandy. Only two major, modern biographies of Henry have been produced, Warren Hollister's posthumous volume in 2001, and Judith Green's 2006 work.\n\nInterpretation of Henry's personality by historians has altered over time. Earlier historians such as Austin Poole and Richard Southern considered Henry as a cruel, draconian ruler. More recent historians, such as Hollister and Green, view his implementation of justice much more sympathetically, particularly when set against the standards of the day, but even Green has noted that Henry was \"in many respects highly unpleasant\", and Alan Cooper has observed that many contemporary chroniclers were probably too scared of the King to voice much criticism. Historians have also debated the extent to which Henry's administrative reforms genuinely constituted an introduction of what Hollister and John Baldwin have termed systematic, \"administrative kingship\", or whether his outlook remained fundamentally traditional.\n\nHenry's burial at Reading Abbey is marked by a local cross and a plaque, but Reading Abbey was slowly demolished during the Dissolution of the Monasteries in the 16th century. The exact location is uncertain, but the most likely location of the tomb itself is now in a built-up area of central Reading, on the site of the former abbey choir. A plan to locate his remains was announced in March 2015, with support from English Heritage and Philippa Langley, who aided with the successful exhumation of Richard III.\n\nIn addition to Matilda and William, Henry possibly had a short-lived son, Richard, with his first wife, Matilda of Scotland. Henry and his second wife, Adeliza of Louvain, had no children.\n\nHenry had a number of illegitimate children by various mistresses.\n\n\n\n"}
{"id": "14183", "url": "https://en.wikipedia.org/wiki?curid=14183", "title": "Hentai", "text": "Hentai\n\nOutside of Japan, (\"\" ; lit. \"pervert\") is anime and manga pornography. In the Japanese language, however, \"hentai\" is not a genre of media but any type of perverse or bizarre sexual desire or act. For example, outside of Japan a work depicting lesbian sex might be described as \"yuri hentai\", but in Japan it would just be described as \"yuri\".\n\nThe word is short for , a perverse sexual desire. The original meaning of \"hentai\" in the Japanese language is a transformation or metamorphosis. The implication of perversion or paraphilia was derived from there. Both meanings can be distinguished in context easily.\n\n \"\" is a kanji compound of 変 (\"hen\"; \"change\", \"weird\", or \"strange\") and 態 (\"tai\"; \"appearance\" or \"condition\"). It also means \"perversion\" or \"abnormality\", especially when used as an adjective. It is the shortened form of the phrase which means \"sexual perversion\". The character \"hen\" is catch-all for queerness as a peculiarity—it does not carry an explicit sexual reference. While the term has expanded in use to cover a range of publications including homosexual publications, it remains primarily a heterosexual term, as terms indicating homosexuality entered Japan as foreign words. Japanese pornographic works are often simply tagged as , meaning \"prohibited to those not yet 18 years old\", and . Less official terms also in use include , , and the English initialism AV (for \"adult video\"). Usage of the term hentai does not define a genre in Japan.\n\nHentai is defined differently in English. The \"Oxford Dictionary Online\" defines hentai as \"a subgenre of the Japanese genres of manga and anime, characterized by overtly sexualized characters and sexually explicit images and plots.\" The origin of the word in English is unknown, but AnimeNation's John Oppliger points to the early 1990s, when a \"Dirty Pair\" erotic \"doujinshi\" (self-published work) titled \"H-Bomb\" was released, and when many websites sold access to images culled from Japanese erotic visual novels and games. The earliest English use of the term traces back to the rec.arts.anime boards; with a 1990 post concerning Happosai of \"Ranma ½\" and the first discussion of the meaning in 1991. A 1995 Glossary on the rec.arts.anime boards contained reference to the Japanese usage and the evolving definition of hentai as \"pervert\" or \"perverted sex\". \"The Anime Movie Guide\", published in 1997, defines as the initial sound of hentai (i.e., the name of the letter \"H\", as pronounced in Japanese); it included that ecchi was \"milder than hentai\". A year later it was defined as a genre in \"Good Vibrations Guide to Sex\". At the beginning of 2000, \"hentai\" was listed as the 41st most popular search term of the internet, while \"anime\" ranked 99th. The attribution has been applied retroactively to works such as \"Urotsukidōji\", \"La Blue Girl\", and \"Cool Devices\". \"Urotsukidōji\" had previously been described with terms such as \"Japornimation\", and \"erotic grotesque\", prior to being identified as hentai.\n\nThe history of word \"hentai\" has its origins in science and psychology. By the middle of the Meiji era, the term appeared in publications to describe unusual or abnormal traits, including paranormal abilities and psychological disorders. A translation of German sexologist Richard von Krafft-Ebing's text \"Psychopathia Sexualis\" originated the concept of \"hentai seiyoku\", as a \"perverse or abnormal sexual desire\". Though it was popularized outside psychology, as in the case of Mori Ōgai's 1909 novel \"Vita Sexualis\". Continued interest in \"hentai seiyoku\", resulted in numerous journals and publications on sexual advice which circulated in the public, served to establish the sexual connotation of 'hentai' as perverse. Any perverse or abnormal act could be hentai, such as committing \"shinjū\" (love suicide). It was Nakamura Kokyo's journal \"Abnormal Psychology\" which started the popular sexology boom in Japan which would see the rise of other popular journals like \"Sexuality and Human Nature\", \"Sex Research\" and \"Sex\". Originally, Tanaka Kogai wrote articles for \"Abnormal Psychology\", but it would be Tanaka's own journal \"Modern Sexuality\" which would become one of the most popular sources of information about erotic and neurotic expression. \"Modern Sexuality\" was created to promote fetishism, S&M, and necrophilia as a facet of modern life. The ero-guro movement and depiction of perverse, abnormal and often erotic undertones were a response to interest in \"hentai seiyoku\".\n\nFollowing the end of World War II, Japan took a new interest in sexualization and public sexuality. Mark McLelland puts forth the observation that the term \"hentai\" found itself shortened to \"H\" and that the English pronunciation was \"etchi\", referring to lewdness and which did not carry the stronger connotation of abnormality or perversion. By the 1950s, the \"hentai seiyoku\" publications became their own genre and included fetish and homosexual topics. By the 1960s, the homosexual content was dropped in favor of subjects like sadomasochism and stories of lesbianism targeted to male readers. The late 1960s brought a sexual revolution which expanded and solidified the normalizing the terms identity in Japan that continues to exist today through publications such as Bessatsu Takarajima's \"Hentai-san ga iku\" series.\n\nWith the usage of hentai as any erotic depiction, the history of these depictions is split into their media. Japanese artwork and comics serve as the first example of hentai material, coming to represent the iconic style after the publication of Azuma Hideo's \"Cybele\" in 1979. Japanese animation (anime) had its first hentai, in both definitions, with the 1984 release of Wonderkid's \"Lolita Anime\", overlooking the erotic and sexual depictions in 1969's \"One Thousand and One Arabian Nights\" and the bare-breasted Cleopatra in 1970's \"Cleopatra\" film. Erotic games, another area of contention, has its first case of the art style depicting sexual acts in 1985's \"Tenshitachi no Gogo\". The history of each medium itself, complicated based on the broad definition and usage.\n\nDepictions of sex and abnormal sex can be traced back through the ages, predating the term \"hentai\". , a Japanese term for erotic art, is thought to have and existed in some form since Heian period. From the 16th to the 19th century, shunga works were suppressed by \"shōguns\". A well-known example is \"The Dream of the Fisherman's Wife\" which depicts a woman being stimulated by two octopi. Shunga production fell with the rise of pornographic photographs in the late 19th century.\n\nTo define erotic manga, a definition for manga is needed. While the \"Hokusai Manga\" uses the term \"manga\" in its title, it does not depict the story-telling aspect common to modern manga, as the images are unrelated. Due to the influence of pornographic photographs in the 19th and 20th centuries, the manga artwork was depicted by realistic characters. However, Osamu Tezuka has helped define the modern look and form of manga, and was later proclaimed as the \"God of Manga\". His debut work \"New Treasure Island\" was released in 1947 as a comic book through Ikuei Publishing and sold over 400,000 copies, though it was the popularity of Tezuka's \"Astro Boy\", \"Metropolis\", and \"Jungle Emperor\" manga that would come to define the media. This story-driven manga style is distinctly unique from comic strips like \"Sazae-san\", and story-driven works are now dominating shōjo and shōnen magazines.\n\nAdult themes in manga have existed since the 1940s, but some of these depictions were more realistic than the cartoon-cute characters popularized by Tezuka. Early well-known \"ero-gekiga\" releases were \"Ero Mangatropa\" (1973), \"Erogenica\" (1975), and \"Alice\" (1977). The distinct shift in the style of Japanese pornographic comics from realistic to cartoon-cute characters is accredited to Azuma Hideo, \"The Father of Lolicon\". In 1979, he penned \"Cybele\", which offered the first commentary on unrealistic depictions of sexual acts between Tezuka-style characters. This would start a pornographic manga movement. The lolicon boom of the 1980s saw the rise of magazines such as the anthologies \"Lemon People\" and \"Petit Apple Pie\".\n\nThe publication of erotic materials in America can be traced back to at least 1990, when IANVS Publications printed its first \"Anime Shower Special\". In March 1994, Antarctic Press released \"Bondage Fairies\", an English translation of \"Insect Hunter\".\n\nBecause there are fewer animation productions, most erotic works are retroactively tagged as hentai since the coining of the term in English. Hentai is typically defined as consisting of excessive nudity, and graphic sexual intercourse whether or not it is perverse. The term \"ecchi\" is typically related to fanservice, with no sexual intercourse being depicted.\n\nTwo early works escape being defined as hentai, but contain erotic themes. This is likely due to the obscurity and unfamiliarity of the works, arriving in America and fading from public focus a full twenty years before importation and surging interests coined the Americanized term of hentai. The first is the 1969 film \"One Thousand and One Arabian Nights\" which faithfully includes erotic elements of the original story. In 1970, \"\", was the first animated film to carry an X rating, but it was mislabeled as erotica in America.\n\nThe term typically identifies the \"Lolita Anime\" series as the first erotic anime and original video animation (OVA); it was released in 1984 by Wonder Kids. Containing eight episodes, the series focused on underage sex and rape and included one episode containing BDSM bondage. Several sub-series were released in response, including a second \"Lolita Anime\" series released by Nikkatsu. It has not been officially licensed or distributed outside of its original release.\n\nThe \"Cream Lemon\" franchise of works ran from 1984 to 2005, with a number of them entering the American market in various forms. \"The Brothers Grime\" series released by Excalibur Films contained \"Cream Lemon\" works as early as 1986. However, they were not billed as anime and were introduced during the same time that the first underground distribution of erotic works began.\n\nThe American release of licensed erotic anime was first attempted in 1991 by Central Park Media, with \"I Give My All\", but it never occurred. In December 1992, \"Devil Hunter Yohko\" was the first risque (ecchi) title that was released by A.D. Vision. While it contains no sexual intercourse it pushes the limits of the ecchi category with sexual dialogue, nudity and one scene in which the heroine is about to be raped.\n\nIt was Central Park Media's 1993 release of \"Urotsukidoji\" which brought the first hentai film to American viewers. Often cited for creating the hentai and tentacle rape genres, it contains extreme depictions of violence and monster sex. As such, it is acknowledged for being the first to depict tentacle sex on screen. When the movie premiered in America it was described as being \"drenched in graphic scenes of perverse sex and ultra-violence\".\n\nFollowing this release, a wealth of pornographic content began to arrive in America, with companies such as A.D. Vision, Central Park Media and Media Blasters releasing licensed titles under various labels. A.D. Vision's label SoftCel Pictures released 19 titles in 1995 alone. Another label, Critical Mass, was created in 1996 to release an unedited edition of \"Violence Jack\". When A.D. Vision's hentai label SoftCel Pictures shut down in 2005, most of its titles were acquired by Critical Mass. Following the bankruptcy of Central Park Media in 2009, the licenses for all Anime 18-related products and movies were transferred to Critical Mass.\n\nThe term eroge (erotic game) literally defines any erotic game, but has become synonymous with video games depicting the artistic styles of anime and manga. The origins of eroge began in the early 1980s, while the computer industry in Japan was struggling to define a computer standard with makers like NEC, Sharp, and Fujitsu competing against one another. The PC98 series, despite lacking in processing power, CD drives and limited graphics, came to dominate the market, with the popularity of eroge games contributing to their success.\n\nDue to the vague definitions of any erotic game, depending on its classification, citing the first erotic game is a subjective one. If the definition applies to adult themes, the first game was \"Softporn Adventure\". Released in America in 1981 for the Apple II, this was a text-based comedic game from On-Line Systems. If eroge is defined as the first graphical depictions and/or Japanese adult themes, it would be Koei's 1982 release of \"Night Life\". Sexual intercourse is depicted through simple graphic outlines. Notably, \"Night Life\" was not intended to be erotic so much as an instructional guide \"to support married life\". A series of \"undressing\" games appeared as early as 1983, such as \"Strip Mahjong\". The first anime-styled erotic game was Tenshitachi no Gogo, released in 1985 by JAST. In 1988, ASCII released the first erotic role-playing game, \"Chaos Angel\". In 1989, AliceSoft released the turn-based RPG \"Rance\" and ELF released \"Dragon Knight\".\n\nIn the late 1980s, eroge began to stagnate under high prices and the majority of games containing uninteresting plots and mindless sex. ELF's 1992 release of \"Dokyusei\" came as customer frustration with eroge was mounting and spawned a new genre of games called dating sims. \"Dokyusei\" was unique because it had no defined plot and required the player to build a relationship with different girls in order to advance the story. Each girl had her own story, but the prospect of consummating a relationship required the girl growing to love the player; there was no easy sex.\n\nThe term \"visual novel\" is vague, with Japanese and English definitions classifying the genre as a type of interactive fiction game driven by narration and limited player interaction. While the term is often retroactively applied to many games, it was Leaf that coined the term with their \"Leaf Visual Novel Series\" (LVNS) with the 1996 release of \"Shizuku\" and \"Kizuato\". The success of these two dark eroge games would be followed by the third and final installment of the LVNS, the 1997 romantic eroge \"To Heart\". Eroge visual novels took a new emotional turn with Tactics' 1998 release \"\". Key's 1999 release of \"Kanon\" proved to be a major success and would go on to have numerous console ports, two manga series and two anime series.\n\nJapanese laws have impacted depictions of works since the Meiji Restoration, but these predate the common definition of hentai material. Since becoming law in 1907, Article 175 of the Criminal Code of Japan forbids the publication of obscene materials. Specifically, depictions of male-female sexual intercourse and pubic hair are considered obscene, but bare genitalia is not. As censorship is required for published works, the most common representations are the blurring dots on pornographic videos and \"bars\" or \"lights\" on still images. In 1986, Toshio Maeda sought to get past censorship on depictions of sexual intercourse, by creating tentacle sex. This led to the large number of works containing sexual intercourse with monsters, demons, robots, and aliens, whose genitals look different from men's. While western views attribute hentai to any explicit work, it was the products of this censorship which became not only the first titles legally imported to America and Europe, but the first successful ones. While uncut for American release, the United Kingdom's release of \"Urotsukidoji\" removed many scenes of the violence and tentacle rape scenes.\n\nIt was also because of this law that the artists began to depict the characters with a minimum of anatomical details and without pubic hair, by law, prior to 1991. Part of the ban was lifted when Nagisa Oshima prevailed over the obscenity charges at his trial for his film \"In the Realm of the Senses\". Though not enforced, the lifting of this ban did not apply to anime and manga as they were not deemed artistic exceptions.\n\nHowever, alterations of material or censorship and even banning of works are common. The U.S. release of the \"La Blue Girl\" altered the age of the heroine from 16 to 18 and removed sex scenes with a dwarf ninja named Nin-nin, and removed the Japanese censoring blurring dots. \"La Blue Girl\" was outright rejected by UK censors who refused to classify it and prohibited its distribution. In 2011 the Liberal Democratic Party of Japan sought a ban on the subgenre lolicon.\n\nThe most prolific consumers of hentai are men. Eroge games in particular combine three favored media, cartoons, pornography and gaming, into an experience. The hentai genre engages a wide audience that expands yearly, and desires better quality and storylines, or works which push the creative envelope. Nobuhiro Komiya, a manga censorship worker, states that the unusual and extreme depictions in hentai are not about perversion so much as they are an example of the profit-oriented industry. Anime depicting normal sexual situations enjoy less market success than those that break social norms, such as sex at schools or bondage.\n\nAccording to Dr. Megha Hazuria Gorem, a clinical psychologist, \"Because toons are a kind of final fantasy, you can make the person look the way you want him or her to look. Every fetish can be fulfilled.\" Dr. Narayan Reddy, a sexologist, commented on the eroge games, \"Animators make new games because there is a demand for them, and because they depict things that the gamers do not have the courage to do in real life, or that might just be illegal, these games are an outlet for suppressed desire.\"\n\nThe hentai genre can be divided into numerous subgenres, the broadest of which encompasses heterosexual and homosexual acts. Hentai that features mainly heterosexual interactions occur in both male-targeted (\"ero\") and female-targeted (\"ladies' comics\") form. Those that feature mainly homosexual interactions are known as yaoi (male-male) and yuri (female-female). Both yaoi and, to a lesser extent, yuri, are generally aimed at members of the opposite sex from the persons depicted. While yaoi and yuri are not always explicit, their pornographic history and association remain. Yaoi's pornographic usage has remained strong in textual form through fanfiction. The definition of yuri has begun to be replaced by the broader definitions of \"lesbian-themed animation or comics\".\n\nHentai is perceived as \"dwelling\" on sexual fetishes. These include dozens of fetish and paraphilia related subgenres, which can be further classified with additional terms, such as heterosexual or homosexual types.\n\nMany works are focused on depicting the mundane and the impossible across every conceivable act and situation no matter how fantastical. The largest subgenre of hentai is \"futanari\" (hermaphroditism), which most often features a female with a penis or penis-like appendage in place of, or in addition to normal female genitals. Futanari characters are primarily depicted as having sex with other women and will almost always be submissive with a male; exceptions include Yonekura Kengo's work, which features female empowerment and domination over males.\n\n\n"}
{"id": "14186", "url": "https://en.wikipedia.org/wiki?curid=14186", "title": "Henry VII of England", "text": "Henry VII of England\n\nHenry VII (; 28 January 1457 – 21 April 1509) was the King of England and Lord of Ireland from his seizure of the crown on 22 August 1485 to his death on 21 April 1509. He was the first monarch of the House of Tudor.\n\nHenry attained the throne when his forces defeated King Richard III at the Battle of Bosworth Field, the culmination of the Wars of the Roses. He was the last king of England to win his throne on the field of battle. He cemented his claim by marrying Elizabeth of York, daughter of Edward IV and niece of Richard III. Henry was successful in restoring the power and stability of the English monarchy after the civil war.\n\nHis supportive stance of the British Isles' wool industry and his standoff with the Low Countries had long lasting benefits to all of the British economy. However, the capriciousness and lack of due process that indebted many would tarnish his legacy and were soon ended upon Henry VII's death, after a commission revealed widespread abuses. According to the contemporary historian Polydore Vergil, simple \"greed\" underscored the means by which royal control was over-asserted in Henry's final years.\n\nHenry can be credited with a number of administrative, economic and diplomatic initiatives. He paid very close attention to detail, and instead of spending lavishly he concentrated on raising new revenues and after a reign of nearly 24 years, he was peacefully succeeded by his son, Henry VIII. The new taxes were unpopular and two days after his coronation, Henry VIII arrested his father's two most unpopular ministers, Sir Richard Empson and Edmund Dudley. They were charged with high treason and were executed in 1510.\n\nHenry VII was born at Pembroke Castle on 28 January 1457 to Margaret Beaufort, Countess of Richmond. His father, Edmund Tudor, 1st Earl of Richmond, died three months before his birth.\n\nHenry's paternal grandfather, Owen Tudor, originally from the Tudors of Penmynydd, Isle of Anglesey in Wales, had been a page in the court of Henry V. He rose to become one of the \"Squires to the Body to the King\" after military service at the Battle of Agincourt. Owen is said to have secretly married the widow of Henry V, Catherine of Valois. One of their sons was Edmund Tudor, father of Henry VII. Edmund was created Earl of Richmond in 1452, and \"formally declared legitimate by Parliament\".\n\nHenry's main claim to the English throne derived from his mother through the House of Beaufort. Henry's mother, Lady Margaret Beaufort, was a great-granddaughter of John of Gaunt, Duke of Lancaster, fourth son of Edward III, and his third wife Katherine Swynford. Katherine was Gaunt's mistress for about 25 years; when they married in 1396, they already had four children, including Henry's great-grandfather John Beaufort. Thus Henry's claim was somewhat tenuous: it was from a woman, and by illegitimate descent. In theory, the Portuguese and Castilian royal families had a better claim (as far as \"legitimacy\" is concerned) as descendants of Catherine of Lancaster, the daughter of John of Gaunt and his second wife Constance of Castile.\n\nGaunt's nephew Richard II legitimised Gaunt's children by Katherine Swynford by Letters Patent in 1397. In 1407, Henry IV, who was Gaunt's son by his first wife, issued new Letters Patent confirming the legitimacy of his half-siblings, but also declaring them ineligible for the throne. Henry IV's action was of doubtful legality, as the Beauforts were previously legitimised by an Act of Parliament, but it further weakened Henry's claim.\n\nNonetheless, by 1483 Henry was the senior male Lancastrian claimant remaining, after the deaths in battle or by murder or execution of Henry VI, his son Edward of Westminster, Prince of Wales, and the other Beaufort line of descent through Lady Margaret's uncle, the 2nd Duke of Somerset.\n\nHenry also made some political capital out of his Welsh ancestry, for example in attracting military support and safeguarding his army's passage through Wales on its way to the Battle of Bosworth. He came from an old, established Anglesey family that claimed descent from Cadwaladr (in legend, the last ancient British king), and on occasion Henry displayed the red dragon of Cadwaladr. He took it, as well as the standard of St George, on his procession through London after the victory at Bosworth. A contemporary writer and Henry's biographer, Bernard André, also made much of Henry's Welsh descent.\n\nIn reality, however, his hereditary connections to Welsh aristocracy were not strong. He was descended by the paternal line, through several generations, from Ednyfed Fychan, the seneschal (steward) of Gwynedd and through this seneschal's wife from Rhys ap Tewdwr, the King of Deheubarth in South Wales. His more immediate ancestor, Tudur ap Goronwy, had aristocratic land rights, but his sons, who were first cousins to Owain Glyndŵr, sided with Owain in his revolt. One son was executed and the family land was forfeited. Another son, Henry's great-grandfather, became a butler to the Bishop of Bangor. Owen Tudor, the son of the butler, like the children of other rebels, was provided for by Henry V, a circumstance that precipitated his access to Queen Catherine of Valois. Notwithstanding this lineage, to the bards of Wales, Henry was a candidate for Y Mab Darogan – \"The Son of Prophecy\" who would free the Welsh from oppression.\nIn 1456, Henry's father Edmund Tudor was captured while fighting for Henry VI in South Wales against the Yorkists. He died in Carmarthen Castle, three months before Henry was born. Henry's uncle Jasper Tudor, the Earl of Pembroke and Edmund's younger brother, undertook to protect the young widow, who was 13 years old when she gave birth to Henry. When Edward IV became King in 1461, Jasper Tudor went into exile abroad. Pembroke Castle, and later the Earldom of Pembroke, were granted to the Yorkist William Herbert, who also assumed the guardianship of Margaret Beaufort and the young Henry.\n\nHenry lived in the Herbert household until 1469, when Richard Neville, Earl of Warwick (the \"Kingmaker\"), went over to the Lancastrians. Herbert was captured fighting for the Yorkists and executed by Warwick. When Warwick restored Henry VI in 1470, Jasper Tudor returned from exile and brought Henry to court. When the Yorkist Edward IV regained the throne in 1471, Henry fled with other Lancastrians to Brittany, where he spent most of the next 14 years under the protection of Francis II, Duke of Brittany. In November 1476, Henry's protector fell ill and his principal advisers were more amenable to negotiating with the English king. Henry was handed over and escorted to the Breton port of Saint-Malo. While there, he feigned stomach cramps and in the confusion fled into a monastery. As at Tewkesbury Abbey after 1471 battle, Edward IV prepared to order his extraction and probable execution. The townspeople took exception to his behaviour, however, and Francis recovered from his illness. Thus a small band of scouts rescued Henry.\n\nBy 1483, Henry's mother was actively promoting him as an alternative to Richard III, despite her being married to a Yorkist, Lord Stanley. At Rennes Cathedral on Christmas Day 1483, Henry pledged to marry the eldest daughter of Edward IV, Elizabeth of York, who was also Edward's heir since the presumed death of her brothers, the Princes in the Tower (King Edward V and his brother Richard of Shrewsbury, Duke of York). Henry then received the homage of his supporters. With money and supplies borrowed from his host, Francis II, Duke of Brittany, Henry tried to land in England, but his conspiracy unravelled, resulting in the execution of his primary co-conspirator, the Duke of Buckingham. Now supported by Francis II's prime-minister, Pierre Landais, Richard III attempted to extradite Henry from Brittany, but Henry escaped to France. He was welcomed by the French, who readily supplied him with troops and equipment for a second invasion.\n\nHenry gained the support of the Woodvilles, in-laws of the late Edward IV, and sailed with a small French and Scottish force, landing at Mill Bay near Dale, Pembrokeshire. He marched towards England accompanied by his uncle Jasper and the Earl of Oxford. Wales was traditionally a Lancastrian stronghold, and Henry owed the support he gathered to his Welsh birth and ancestry, being directly descended, through his father, from Rhys ap Gruffydd. He amassed an army of around 5,000 soldiers.\n\nHenry was aware that his best chance to seize the throne was to engage Richard quickly and defeat him immediately, as Richard had reinforcements in Nottingham and Leicester. Richard only needed to avoid being killed to keep his throne. Though outnumbered, Henry's Lancastrian forces decisively defeated Richard's Yorkist army at the Battle of Bosworth Field on 22 August 1485. Several of Richard's key allies, such as the Earl of Northumberland and William and Thomas Stanley, crucially switched sides or left the battlefield. Richard III's death at Bosworth Field effectively ended the Wars of the Roses, although it was not the last battle Henry had to fight.\n\nAs king, Henry was styled as His Gracehis full style was: \"Henry, by the Grace of God, King of England and France and Lord of Ireland\". On his succession, Henry became entitled to bear the Royal Arms of England. After his marriage, he used the red-and-white rose as his emblem which became known as the Tudor rose.\n\nHis first concern was to secure his hold on the throne. Henry declared himself king \"by right of conquest\" retroactively from 21 August 1485, the day before Bosworth Field. Thus, anyone who had fought for Richard against him would be guilty of treason, and Henry could legally confiscate his lands and property of Richard III while restoring his own. However, he spared Richard's nephew and designated heir, the Earl of Lincoln, and he made Margaret Plantagenet, a Yorkist heiress, Countess of Salisbury sui juris. He took great care not to address the baronage, or summon Parliament, until after his coronation, which took place in Westminster Abbey on 30 October 1485. Almost immediately afterwards, he issued an edict that any gentleman who swore fealty to him would, notwithstanding any previous attainder, be secure in his property and person.\n\nHenry then honoured his pledge of December 1483 to marry Elizabeth of York. They were third cousins, as both were great-great-grandchildren of John of Gaunt. The marriage took place on 18 January 1486 at Westminster. The marriage unified the warring houses and gave his children a strong claim to the throne. The unification of the houses of York and Lancaster by this marriage is symbolised by the heraldic emblem of the Tudor rose, a combination of the white rose of York and the red rose of Lancaster. It also ended future discussion as to whether the descendants of the fourth son of Edward III, Edmund, Duke of York, through marriage to Philippa, heiress of the second son, Lionel, Duke of Clarence, had a superior or inferior claim to those of the third son John of Gaunt, who had held the throne for three generations.\nIn addition, Henry had Parliament repeal \"Titulus Regius\", the statute that declared Edward IV's marriage invalid and his children illegitimate, thus legitimising his wife. Amateur historians Bertram Fields and Sir Clements Markham have claimed that he may have been involved in the murder of the Princes in the Tower, as the repeal of \"Titulus Regius\" gave the Princes a stronger claim to the throne than his own. Alison Weir, however, points out that the Rennes ceremony, two years earlier, was possible only if Henry and his supporters were certain that the Princes were already dead.\n\nHenry secured his crown principally by dividing and undermining the power of the nobility, especially through the aggressive use of bonds and recognisances to secure loyalty. He also enacted laws against livery and maintenance, the great lords' practice of having large numbers of \"retainers\" who wore their lord's badge or uniform and formed a potential private army.\n\nWhile he was still in Leicester, after the battle of Bosworth Field, Henry was already taking precautions to prevent any rebellions against his reign. Before leaving Leicester to go to London, Henry dispatched Robert Willoughby to Sheriff Hutton in Yorkshire, to have the ten-year-old Edward, Earl of Warwick, arrested and taken to the Tower of London. Edward was the son of George, Duke of Clarence, and as such he presented a threat as a potential rival to the new King Henry VII for the throne of England. However, Henry was threatened by several active rebellions over the next few years. The first was the rebellion of the Stafford brothers and Viscount Lovell of 1486, which collapsed without fighting.\n\nIn 1487, Yorkists led by Lincoln rebelled in support of Lambert Simnel, a boy who was claimed to be the Earl of Warwick, son of Edward IV's brother Clarence (who had last been seen as a prisoner in the Tower). The rebellion began in Ireland, where the traditionally Yorkist nobility, headed by the powerful Gerald FitzGerald, 8th Earl of Kildare, proclaimed Simnel King and provided troops for his invasion of England. The rebellion was defeated and Lincoln killed at the Battle of Stoke. Henry showed remarkable clemency to the surviving rebels: he pardoned Kildare and the other Irish nobles, and he made the boy, Simnel, a servant in the royal kitchen.\n\nIn 1490, a young Fleming, Perkin Warbeck, appeared and claimed to be Richard, the younger of the \"Princes in the Tower\". Warbeck won the support of Edward IV's sister Margaret of Burgundy. He led attempted invasions of Ireland in 1491 and England in 1495, and persuaded James IV of Scotland to invade England in 1496. In 1497 Warbeck landed in Cornwall with a few thousand troops, but was soon captured and executed.\n\nIn 1499, Henry had the Earl of Warwick executed. However, he spared Warwick's elder sister Margaret. She survived until 1541, when she was executed by Henry VIII.\n\nHenry married Elizabeth of York with the hope of uniting the Yorkist and Lancastrian sides of the Plantagenet dynastic disputes, and he was largely successful. However, such a level of paranoia persisted that anyone (John de la Pole, Earl of Richmond, for example) with blood ties to the Plantagenets was suspected of coveting the throne.\n\nFor most of Henry VII's reign Edward Story was Bishop of Chichester. Story's register still exists and, according to the 19th-century historian W.R.W. Stephens, \"affords some illustrations of the avaricious and parsimonious character of the king\". It seems that the king was skillful at extracting money from his subjects on many pretexts, including that of war with France or war with Scotland. The money so extracted added to the king's personal fortune rather than the stated purpose.\n\nUnlike his predecessors, Henry VII came to the throne without personal experience in estate management or financial administration. Yet during his reign he became a fiscally prudent monarch who restored the fortunes of an effectively bankrupt exchequer. Henry VII introduced stability to the financial administration of England by keeping the same financial advisors throughout his reign. For instance, other than the first few months of the reign, Lord Dynham and Thomas Howard, earl of Surrey were the only two office holders in the position of Lord High Treasurer of England throughout his reign.\n\nHenry VII improved tax collection within the realm by introducing ruthlessly efficient mechanisms of taxation. He was supported in this effort by his chancellor, Archbishop John Morton, whose \"Morton's Fork\" was a catch-22 method of ensuring that nobles paid increased taxes: Those nobles who spent little must have saved much and, thus, they could afford the increased taxes; on the other hand, those nobles who spent much obviously had the means to pay the increased taxes. Royal government was also reformed with the introduction of the King's Council that kept the nobility in check. Henry VIII executed Richard Empson and Edmund Dudley, his two most hated tax collectors, on trumped-up charges of treason.\n\nHe established the pound avoirdupois as a standard of weight; it became part of the Imperial and customary systems of units.\n\nHenry VII's policy was both to maintain peace and to create economic prosperity. Up to a point, he succeeded. He was not a military man and had no interest in trying to regain French territories lost during the reigns of his predecessors; he was therefore ready to conclude a treaty with France at Etaples that brought money into the coffers of England, and ensured the French would not support pretenders to the English throne, such as Perkin Warbeck. However, this treaty came at a slight price, as Henry mounted a minor invasion of Brittany in November 1492. Henry decided to keep Brittany out of French hands, signed an alliance with Spain to that end, and sent 6,000 troops to France. The confused, fractious nature of Breton politics undermined his efforts, which finally failed after three sizeable expeditions, at a cost of £24,000. However, as France was becoming more concerned with the Italian Wars, the French were happy to agree to the Treaty of Etaples. Henry had pressured the French by laying siege to Boulogne in October 1492. \nHenry had been under the financial and physical protection of the French throne or its vassals for most of his life, prior to his ascending the throne of England. To strengthen his position, however, he subsidised shipbuilding, so strengthening the navy (he commissioned Europe's first ever – and the world's oldest surviving – dry dock at Portsmouth in 1495) and improving trading opportunities.\n\nHenry VII was one of the first European monarchs to recognise the importance of the newly united Spanish kingdom and concluded the Treaty of Medina del Campo, by which his son, Arthur Tudor, was married to Catherine of Aragon. He also concluded the Treaty of Perpetual Peace with Scotland (the first treaty between England and Scotland for almost two centuries), which betrothed his daughter Margaret to King James IV of Scotland. By means of this marriage, Henry VII hoped to break the Auld Alliance between Scotland and France. Though this was not achieved during his reign, the marriage eventually led to the union of the English and Scottish crowns under Margaret's great-grandson, James VI and I following the death of Henry's granddaughter Elizabeth I.\n\nHe also formed an alliance with Holy Roman Emperor Maximilian I (1493–1519) and persuaded Pope Innocent VIII to issue a papal bull of excommunication against all pretenders to Henry's throne.\n\nHenry VII was much enriched by trading alum, which was used in the wool and cloth trades for use as a chemical dye fixative when dyeing fabrics. Since alum was mined in only one area in Europe (Tolfa, Italy), it was a scarce commodity and therefore especially valuable to its land holder, the pope. With the English economy heavily invested in wool production, Henry VII became involved in the alum trade in 1486. With the assistance of the Italian merchant-banker, Lodovico della Fava and the Italian banker, Girolamo Frescobaldi, Henry VII became deeply involved in the trade by licensing ships, obtaining alum from the Ottoman Empire, and selling it to the Low Countries and in England. This trade made an expensive commodity cheaper, which raised opposition from Pope Julius II since the Tolfa mine was a part of papal territory and had given the Pope monopoly control over alum.\nHenry's most successful diplomatic achievement as regards the economy was the \"Magnus Intercursus\" (\"great agreement\") of 1496. In 1494, Henry embargoed trade (mainly in wool) with the Netherlands as retaliation for Margaret of Burgundy's support of Perkin Warbeck. The Merchant Adventurers, the company which enjoyed the monopoly of the Flemish wool trade, relocated from Antwerp to Calais. At the same time, Flemish merchants were ejected from England. The stand-off eventually paid off for Henry. Both parties realised they were mutually disadvantaged by the reduction in commerce. Its restoration by the \"Magnus Intercursus\" was very much to England's benefit in removing taxation for English merchants and significantly increasing England's wealth. In turn, Antwerp became an extremely important trade entrepôt (transshipment port), through which, for example, goods from the Baltic, spices from the east and Italian silks were exchanged for English cloth.\n\nIn 1506, Henry extorted the Treaty of Windsor from Philip the Handsome of Burgundy. Philip had been shipwrecked on the English coast, and while Henry's guest, was bullied into an agreement so favourable to England at the expense of the Netherlands that it was dubbed the \"Malus Intercursus\" (\"evil agreement\"). France, Burgundy, the Holy Roman Empire, Spain and the Hanseatic League all rejected the treaty, which was never in force. Philip died shortly after the negotiations.\n\nHenry's principal problem was to restore royal authority in a realm recovering from the Wars of the Roses. There were too many powerful noblemen and, as a consequence of the system of so-called bastard feudalism, each had what amounted to private armies of indentured retainers (mercenaries masquerading as servants).\nHe was content to allow the nobles their regional influence if they were loyal to him. For instance, the Stanley family had control of Lancashire and Cheshire, upholding the peace on the condition that they stayed within the law. In other cases, he brought his over-powerful subjects to heel by decree. He passed laws against \"livery\" (the upper classes' flaunting of their adherents by giving them badges and emblems) and \"maintenance\" (the keeping of too many male \"servants\"). These laws were used shrewdly in levying fines upon those that he perceived as threats.\n\nHowever, his principal weapon was the Court of Star Chamber. This revived an earlier practice of using a small (and trusted) group of the Privy Council as a personal or Prerogative Court, able to cut through the cumbersome legal system and act swiftly. Serious disputes involving the use of personal power, or threats to royal authority, were thus dealt with.\n\nHenry VII used Justices of the Peace on a large, nationwide scale. They were appointed for every shire and served for a year at a time. Their chief task was to see that the laws of the country were obeyed in their area. Their powers and numbers steadily increased during the time of the Tudors, never more so than under Henry's reign. Despite this, Henry was keen to constrain their power and influence, applying the same principles to the Justices of the Peace as he did to the nobility: a similar system of bonds and recognisances to that which applied to both the gentry and the nobles who tried to exert their elevated influence over these local officials.\n\nAll Acts of Parliament were overseen by the Justices of the Peace. For example, Justices of the Peace could replace suspect jurors in accordance with the 1495 act preventing the corruption of juries. They were also in charge of various administrative duties, such as the checking of weights and measures.\n\nBy 1509, Justices of the Peace were key enforcers of law and order for Henry VII. They were unpaid, which, in comparison with modern standards, meant a lesser tax bill to pay for a police force. Local gentry saw the office as one of local influence and prestige and were therefore willing to serve. Overall, this was a successful area of policy for Henry, both in terms of efficiency and as a method of reducing the corruption endemic within the nobility of the Middle Ages.\n\nIn 1502, Henry VII's first son and heir apparent, Arthur, Prince of Wales, died suddenly at Ludlow Castle, very likely from a viral respiratory illness known at the time as the \"English sweating sickness\". This made Henry, Duke of York (Henry VIII) heir apparent to the throne. The King, normally a reserved man who rarely showed much emotion in public unless angry, surprised his courtiers by his intense grief and sobbing at his son's death, while his concern for the Queen is evidence that the marriage was a happy one, as is his reaction to the Queen's death the following year, when he shut himself away for several days, refusing to speak to anyone.\n\nHenry VII wanted to maintain the Spanish alliance. He therefore arranged a papal dispensation from Pope Julius II for Prince Henry to marry his brother's widow Catherine, a relationship that would have otherwise precluded marriage in the Roman Catholic Church. In 1503, Queen Elizabeth died in childbirth, so King Henry had the dispensation also permit him to marry Catherine himself. After obtaining the dispensation, Henry had second thoughts about the marriage of his son and Catherine. Catherine's mother Isabella I of Castile had died and Catherine's sister Joanna had succeeded her; Catherine was therefore daughter of only one reigning monarch and so less desirable as a spouse for Henry VII's heir-apparent. The marriage did not take place during his lifetime. Otherwise, at the time of his father's arranging of the marriage to Catherine of Aragon, the future Henry VIII was too young to contract the marriage according to Canon Law, and would be ineligible until age fourteen.\n\nHenry made half-hearted plans to remarry and beget more heirs, but these never came to anything. In 1505 he was sufficiently interested in a potential marriage to Joan, the recently widowed Queen of Naples, that he sent ambassadors to Naples to report on the 27-year-old's physical suitability. The wedding never took place, and the physical description Henry sent with his ambassadors of what he desired in a new wife matched the description of Elizabeth. After 1503, records show the Tower of London was never again used as a royal residence by Henry Tudor, and all royal births under Henry VIII took place in palaces. Henry VII was shattered by the loss of Elizabeth, and her death broke his heart. During his lifetime the nobility often jeered him for re-centralizing power in London, and later the 16th-century historian Francis Bacon was ruthlessly critical of the methods by which he enforced tax law, but it is equally true that Henry Tudor was hellbent on keeping detailed records of his personal finances, down to the last halfpenny; these and one account book detailing the expenses of his queen survive in the British National Archives. Until the death of his wife, the evidence is clear from these accounting books that Henry Tudor was a more doting father and husband than was widely known. Many of the entries show a man who loosened his purse strings generously for his wife and children, and not just on necessities: in spring 1491 he spent a great amount of gold on his daughter Mary for a lute; the following year he spent money on a lion for Elizabeth's menagerie.\n\nWith Elizabeth's death, the possibility for such family indulgences greatly diminished. Immediately afterward, Henry became very sick and nearly died himself, allowing only Margaret Beaufort, his mother, near him: \"privily departed to a solitary place, and would that no man should resort unto him.\"\nHenry VII died at Richmond Palace on 21 April 1509 of tuberculosis and was buried at Westminster Abbey, next to his wife, Elizabeth, in the chapel he commissioned. He was succeeded by his second son, Henry VIII (reign 1509–47). His mother survived him, dying two months later on 29 June 1509.\n\nHenry is the first English king of whose appearance good contemporary visual records in realistic portraits exist that are relatively free of idealization. At 27, he was tall, slender, with small blue eyes, which were said to have a noticeable animation of expression, and noticeably bad teeth in a long, sallow face beneath very fair hair. Amiable and high-spirited, Henry was friendly if dignified in manner, and it was clear to everyone that he was extremely intelligent. His biographer, Professor Chrimes, credits him – even before he had become king – with \"a high degree of personal magnetism, ability to inspire confidence, and a growing reputation for shrewd decisiveness\". On the debit side, he may have looked a little delicate as he suffered from poor health.\n\nHistorians have always compared Henry VII with his continental contemporaries, especially Louis XI of France and Ferdinand II of Aragon. By 1600 historians emphasised Henry's wisdom in drawing lessons in statecraft from other monarchs. In 1622 Francis Bacon published his \"History of the Reign of King Henry VII\". By 1900 the \"New Monarchy\" interpretation stressed the common factors that in each country led to the revival of monarchical power. This approach raised puzzling questions about similarities and differences in the development of national states. In the late 20th century a model of European state formation was prominent in which Henry less resembles Louis and Ferdinand.\n\n\n\n\n"}
{"id": "14187", "url": "https://en.wikipedia.org/wiki?curid=14187", "title": "Henry VIII of England", "text": "Henry VIII of England\n\nHenry VIII (28 June 1491 – 28 January 1547) was King of England from 1509 until his death. Henry was the second Tudor monarch, succeeding his father, Henry VII.\n\nHenry is best known for his six marriages, in particular his efforts to have his first marriage, to Catherine of Aragon, annulled. His disagreement with the Pope on the question of such an annulment led Henry to initiate the English Reformation, separating the Church of England from papal authority. He appointed himself the Supreme Head of the Church of England and dissolved convents and monasteries, for which he was excommunicated. Henry is also known as \"the father of the Royal Navy\"; he invested heavily in the Navy, increasing its size greatly from a few to more than 50 ships.\n\nDomestically, Henry is known for his radical changes to the English Constitution, ushering into England the theory of the divine right of kings. Besides asserting the sovereign's supremacy over the Church of England, he greatly expanded royal power during his reign. Charges of treason and heresy were commonly used to quell dissent, and those accused were often executed without a formal trial, by means of bills of attainder. He achieved many of his political aims through the work of his chief ministers, some of whom were banished or executed when they fell out of his favour. Thomas Wolsey, Thomas More, Thomas Cromwell, Richard Rich, and Thomas Cranmer all figured prominently in Henry's administration. He was an extravagant spender and used the proceeds from the Dissolution of the Monasteries and acts of the Reformation Parliament to convert into royal revenue the money that was formerly paid to Rome. Despite the influx of money from these sources, Henry was continually on the verge of financial ruin due to his personal extravagance as well as his numerous costly and largely unsuccessful continental wars, particularly with Francis I of France and the Holy Roman Emperor Charles V, as he sought to enforce his claim to the Kingdom of France. At home, he oversaw the legal union of England and Wales with the Laws in Wales Acts 1535 and 1542 and following the Crown of Ireland Act 1542 he was the first English monarch to rule as King of Ireland.\n\nHis contemporaries considered Henry in his prime to be an attractive, educated and accomplished king. He has been described as \"one of the most charismatic rulers to sit on the English throne\". He was an author and composer. As he aged, Henry became severely obese and his health suffered, contributing to his death in 1547. He is frequently characterised in his later life as a lustful, egotistical, harsh, and insecure king. He was succeeded by his son Edward VI.\n\nBorn 28 June 1491 at the Palace of Placentia in Greenwich, Kent, Henry Tudor was the third child and second son of Henry VII and Elizabeth of York. Of the young Henry's six siblings, only three – Arthur, Prince of Wales; Margaret; and Mary – survived infancy. He was baptised by Richard Fox, the Bishop of Exeter, at a church of the Observant Franciscans close to the palace. In 1493, at the age of two, Henry was appointed Constable of Dover Castle and Lord Warden of the Cinque Ports. He was subsequently appointed Earl Marshal of England and Lord Lieutenant of Ireland at age three, and was inducted into the Order of the Bath soon after. The day after the ceremony he was created Duke of York and a month or so later made Warden of the Scottish Marches. In May 1495, he was appointed to the Order of the Garter. The reason for all the appointments to a small child was so his father could keep personal control of lucrative positions and not share them with established families. Henry was given a first-rate education from leading tutors, becoming fluent in Latin and French, and learning at least some Italian. Not much is known about his early life – save for his appointments – because he was not expected to become king. In November 1501, Henry also played a considerable part in the ceremonies surrounding his brother's marriage to Catherine of Aragon, the youngest surviving child of King Ferdinand II of Aragon and Queen Isabella I of Castile. As Duke of York, Henry used the arms of his father as king, differenced by a \"label of three points ermine\". He was further honoured, on 9 February 1506, by Holy Roman Emperor Maximilian I who made him a Knight of the Golden Fleece.\n\nIn 1502, Arthur died at the age of 15, possibly of sweating sickness, just 20 weeks after his marriage to Catherine. Arthur's death thrust all his duties upon his younger brother, the 10-year-old Henry. After a little debate, Henry became the new Duke of Cornwall in October 1502, and the new Prince of Wales and Earl of Chester in February 1503. Henry VII gave the boy few tasks. Young Henry was strictly supervised and did not appear in public. As a result, he ascended the throne \"untrained in the exacting art of kingship\".\n\nHenry VII renewed his efforts to seal a marital alliance between England and Spain, by offering his second son in marriage to Arthur's widow Catherine. Both Isabella and Henry VII were keen on the idea, which had arisen very shortly after Arthur's death. On 23 June 1503, a treaty was signed for their marriage, and they were betrothed two days later. A papal dispensation was only needed for the \"impediment of public honesty\" if the marriage had not been consummated as Catherine and her duenna claimed, but Henry VII and the Spanish ambassador set out instead to obtain a dispensation for \"affinity\", which took account of the possibility of consummation. Cohabitation was not possible because Henry was too young. Isabella's death in 1504, and the ensuing problems of succession in Castile, complicated matters. Her father preferred her to stay in England, but Henry VII's relations with Ferdinand had deteriorated. Catherine was therefore left in limbo for some time, culminating in Prince Henry's rejection of the marriage as soon he was able, at the age of 14. Ferdinand's solution was to make his daughter ambassador, allowing her to stay in England indefinitely. Devout, she began to believe that it was God's will that she marry the prince despite his opposition.\n\nHenry VII died on 21 April 1509, and the 17-year-old Henry succeeded him as king. Soon after his father's burial on 10 May, Henry suddenly declared that he would indeed marry Catherine, leaving unresolved several issues concerning the papal dispensation and a missing part of the marriage portion. The new king maintained that it had been his father's dying wish that he marry Catherine. Whether or not this was true, it was certainly convenient. Emperor Maximilian I had been attempting to marry his granddaughter (and Catherine's niece) Eleanor to Henry; she had now been jilted. Henry's wedding to Catherine was kept low-key and was held at the friar's church in Greenwich on 11 June 1509. On 23 June 1509, Henry led the now 23-year-old Catherine from the Tower of London to Westminster Abbey for their coronation, which took place the following day. It was a grand affair: the king's passage was lined with tapestries and laid with fine cloth. Following the ceremony, there was a grand banquet in Westminster Hall. As Catherine wrote to her father, \"our time is spent in continuous festival\".\n\nTwo days after his coronation, Henry arrested his father's two most unpopular ministers, Sir Richard Empson and Edmund Dudley. They were charged with high treason and were executed in 1510. Politically-motivated executions would remain one of Henry's primary tactics for dealing with those who stood in his way. Henry also returned to the public some of the money supposedly extorted by the two ministers. By contrast, Henry's view of the House of York – potential rival claimants for the throne – was more moderate than his father's had been. Several who had been imprisoned by his father, including the Marquess of Dorset, were pardoned. Others (most notably Edmund de la Pole) went unreconciled; de la Pole was eventually beheaded in 1513, an execution prompted by his brother Richard siding against the king.\n\nSoon after, Catherine conceived, but the child, a girl, was stillborn on 31 January 1510. About four months later, Catherine again became pregnant. On New Year's Day 1511, the child – Henry – was born. After the grief of losing their first child, the couple were pleased to have a boy and festivities were held, including a two-day joust known as the Westminster Tournament. However, the child died seven weeks later. Catherine had two stillborn sons in 1514 and 1515, but gave birth in February 1516 to a girl, Mary. Relations between Henry and Catherine had been strained, but they eased slightly after Mary's birth.\n\nAlthough Henry's marriage to Catherine has since been described as \"unusually good\", it is known that Henry took mistresses. It was revealed in 1510 that Henry had been conducting an affair with one of the sisters of Edward Stafford, 3rd Duke of Buckingham, either Elizabeth or Anne Hastings, Countess of Huntingdon. The most significant mistress for about three years, starting in 1516, was Elizabeth Blount. Blount is one of only two completely undisputed mistresses, considered by some to be few for a virile young king. Exactly how many Henry had is disputed: David Loades believes Henry had mistresses \"only to a very limited extent\", whilst Alison Weir believes there were numerous other affairs. There is no evidence that Catherine protested, and in 1518 she fell pregnant again with another girl, who was also stillborn. Blount gave birth in June 1519 to Henry's illegitimate son, Henry FitzRoy. The young boy was made Duke of Richmond in June 1525 in what some thought was one step on the path to his eventual legitimisation. In 1533, FitzRoy married Mary Howard, but died childless three years later. At the time of Richmond's death in June 1536, Parliament was enacting the Second Succession Act, which could have allowed him to become king.\n\nIn 1510, France, with a fragile alliance with the Holy Roman Empire in the League of Cambrai, was winning a war against Venice. Henry renewed his father's friendship with Louis XII of France, an issue that divided his council. Certainly war with the combined might of the two powers would have been exceedingly difficult. Shortly thereafter, however, Henry also signed a pact with Ferdinand. After Pope Julius II created the anti-French Holy League in October 1511, Henry followed Ferdinand's lead and brought England into the new League. An initial joint Anglo-Spanish attack was planned for the spring to recover Aquitaine for England, the start of making Henry's dreams of ruling France a reality. The attack, however, following a formal declaration of war in April 1512, was not led by Henry personally and was a considerable failure; Ferdinand used it simply to further his own ends, and it strained the Anglo-Spanish alliance. Nevertheless, the French were pushed out of Italy soon after, and the alliance survived, with both parties keen to win further victories over the French. Henry then pulled off a diplomatic coup by convincing the Emperor to join the Holy League. Remarkably, Henry had also secured the promised title of \"Most Christian King of France\" from Julius and possibly coronation by the Pope himself in Paris, if only Louis could be defeated.\n\nOn 30 June 1513, Henry invaded France, and his troops defeated a French army at the Battle of the Spurs – a relatively minor result, but one which was seized on by the English for propaganda purposes. Soon after, the English took Thérouanne and handed it over to Maximillian; Tournai, a more significant settlement, followed. Henry had led the army personally, complete with large entourage. His absence from the country, however, had prompted his brother-in-law, James IV of Scotland, to invade England at the behest of Louis. Nevertheless, the English army, overseen by Queen Catherine, decisively defeated the Scots at the Battle of Flodden on 9 September 1513. Among the dead was the Scottish king, thus ending Scotland's brief involvement in the war. These campaigns had given Henry a taste of the military success he so desired. However, despite initial indications, he decided not to pursue a 1514 campaign. He had been supporting Ferdinand and Maximilian financially during the campaign but had received little in return; England's coffers were now empty. With the replacement of Julius by Pope Leo X, who was inclined to negotiate for peace with France, Henry signed his own treaty with Louis: his sister Mary would become Louis' wife, having previously been pledged to the younger Charles, and peace was secured for eight years, a remarkably long time.\n\nCharles V ascended the thrones of both Spain and the Holy Roman Empire following the deaths of his grandfathers, Ferdinand in 1516 and Maximilian in 1519. Francis I likewise became king of France upon the death of Louis in 1515, leaving three relatively young rulers and an opportunity for a clean slate. The careful diplomacy of Cardinal Thomas Wolsey had resulted in the Treaty of London in 1518, aimed at uniting the kingdoms of western Europe in the wake of a new Ottoman threat, and it seemed that peace might be secured. Henry met Francis I on 7 June 1520 at the Field of the Cloth of Gold near Calais for a fortnight of lavish entertainment. Both hoped for friendly relations in place of the wars of the previous decade. The strong air of competition laid to rest any hopes of a renewal of the Treaty of London, however, and conflict was inevitable. Henry had more in common with Charles, whom he met once before and once after Francis. Charles brought the Empire into war with France in 1521; Henry offered to mediate, but little was achieved and by the end of the year Henry had aligned England with Charles. He still clung to his previous aim of restoring English lands in France, but also sought to secure an alliance with Burgundy, then part of Charles' realm, and the continued support of Charles. A small English attack in the north of France made up little ground. Charles defeated and captured Francis at Pavia and could dictate peace; but he believed he owed Henry nothing. Sensing this, Henry decided to take England out of the war before his ally, signing the Treaty of the More on 30 August 1525.\n\nDuring his first marriage to Catherine of Aragon, Henry conducted an affair with Mary Boleyn, Catherine's lady-in-waiting. There has been speculation that Mary's two children, Henry and Catherine Carey, were fathered by Henry, but this has never been proved, and the King never acknowledged them as he did Henry FitzRoy. In 1525, as Henry grew more impatient with Catherine's inability to produce the male heir he desired, he became enamoured of Mary Boleyn's sister, Anne, then a charismatic young woman of 25 in the Queen's entourage. Anne, however, resisted his attempts to seduce her, and refused to become his mistress as her sister Mary Boleyn had. It was in this context that Henry considered his three options for finding a dynastic successor and hence resolving what came to be described at court as the King's \"great matter\". These options were legitimising Henry FitzRoy, which would take the intervention of the pope and would be open to challenge; marrying off Mary as soon as possible and hoping for a grandson to inherit directly, but Mary was considered unlikely to conceive before Henry's death; or somehow rejecting Catherine and marrying someone else of child-bearing age. Probably seeing the possibility of marrying Anne, the third was ultimately the most attractive possibility to the 34-year-old Henry, and it soon became the King's absorbing desire to annul his marriage to the now 40-year-old Catherine. It was a decision that would lead Henry to reject papal authority and initiate the English Reformation.\n\nHenry's precise motivations and intentions over the coming years are not widely agreed on. Henry himself, at least in the early part of his reign, was a devout and well-informed Catholic to the extent that his 1521 publication \"Assertio Septem Sacramentorum\" (\"Defence of the Seven Sacraments\") earned him the title of \"Fidei Defensor\" (Defender of the Faith) from Pope Leo X. The work represented a staunch defence of papal supremacy, albeit one couched in somewhat contingent terms. It is not clear exactly when Henry changed his mind on the issue as he grew more intent on a second marriage. Certainly, by 1527 he had convinced himself that in marrying Catherine, his brother's wife, he had acted contrary to Leviticus 20:21, an impediment Henry now believed that the Pope never had the authority to dispense with. It was this argument Henry took to Pope Clement VII in 1527 in the hope of having his marriage to Catherine annulled, forgoing at least one less openly defiant line of attack. In going public, all hope of tempting Catherine to retire to a nunnery or otherwise stay quiet was lost. Henry sent his secretary, William Knight, to appeal directly to the Holy See by way of a deceptively worded draft papal bull. Knight was unsuccessful; the Pope could not be misled so easily.\n\nOther missions concentrated on arranging an ecclesiastical court to meet in England, with a representative from Clement VII. Though Clement agreed to the creation of such a court, he never had any intention of empowering his legate, Lorenzo Campeggio, to decide in Henry's favour. This bias was perhaps the result of pressure from Charles V, Catherine's nephew, though it is not clear how far this influenced either Campeggio or the Pope. After less than two months of hearing evidence, Clement called the case back to Rome in July 1529, from which it was clear that it would never re-emerge. With the chance for an annulment lost and England's place in Europe forfeit, Cardinal Wolsey bore the blame. He was charged with \"praemunire\" in October 1529 and his fall from grace was \"sudden and total\". Briefly reconciled with Henry (and officially pardoned) in the first half of 1530, he was charged once more in November 1530, this time for treason, but died while awaiting trial. After a short period in which Henry took government upon his own shoulders, Sir Thomas More took on the role of Lord Chancellor and chief minister. Intelligent and able, but also a devout Catholic and opponent of the annulment, More initially cooperated with the king's new policy, denouncing Wolsey in Parliament.\n\nA year later, Catherine was banished from court, and her rooms were given to Anne. Anne was an unusually educated and intellectual woman for her time, and was keenly absorbed and engaged with the ideas of the Protestant Reformers, though the extent to which she herself was a committed Protestant is much debated. When Archbishop of Canterbury William Warham died, Anne's influence and the need to find a trustworthy supporter of the annulment had Thomas Cranmer appointed to the vacant position. This was approved by the Pope, unaware of the King's nascent plans for the Church.\n\nIn the winter of 1532, Henry met with Francis I at Calais and enlisted the support of the French king for his new marriage. Immediately upon returning to Dover in England, Henry, now 41, and Anne went through a secret wedding service. She soon became pregnant, and there was a second wedding service in London on 25 January 1533. On 23 May 1533, Cranmer, sitting in judgment at a special court convened at Dunstable Priory to rule on the validity of the king's marriage to Catherine of Aragon, declared the marriage of Henry and Catherine null and void. Five days later, on 28 May 1533, Cranmer declared the marriage of Henry and Anne to be valid. Catherine was formally stripped of her title as queen, becoming instead \"princess dowager\" as the widow of Arthur. In her place, Anne was crowned queen consort on 1 June 1533. The queen gave birth to a daughter slightly prematurely on 7 September 1533. The child was christened Elizabeth, in honour of Henry's mother, Elizabeth of York.\n\nFollowing the marriage, there was a period of consolidation taking the form of a series of statutes of the Reformation Parliament aimed at finding solutions to any remaining issues, whilst protecting the new reforms from challenge, convincing the public of their legitimacy, and exposing and dealing with opponents. Although the canon law was dealt with at length by Cranmer and others, these acts were advanced by Thomas Cromwell, Thomas Audley and the Duke of Norfolk and indeed by Henry himself. With this process complete, in May 1532 More resigned as Lord Chancellor, leaving Cromwell as Henry's chief minister. With the Act of Succession 1533, Catherine's daughter, Mary, was declared illegitimate; Henry's marriage to Anne was declared legitimate; and Anne's issue was decided to be next in the line of succession. With the Acts of Supremacy in 1534, Parliament also recognised the King's status as head of the church in England and, with the Act in Restraint of Appeals in 1532, abolished the right of appeal to Rome. It was only then that Pope Clement took the step of excommunicating Henry and Thomas Cranmer, although the excommunication was not made official until some time later.\n\nThe king and queen were not pleased with married life. The royal couple enjoyed periods of calm and affection, but Anne refused to play the submissive role expected of her. The vivacity and opinionated intellect that had made her so attractive as an illicit lover made her too independent for the largely ceremonial role of a royal wife and it made her many enemies. For his part, Henry disliked Anne's constant irritability and violent temper. After a false pregnancy or miscarriage in 1534, he saw her failure to give him a son as a betrayal. As early as Christmas 1534, Henry was discussing with Cranmer and Cromwell the chances of leaving Anne without having to return to Catherine. Henry is traditionally believed to have had an affair with Margaret (\"Madge\") Shelton in 1535, although historian Antonia Fraser argues that Henry in fact had an affair with her sister Mary Shelton.\n\nOpposition to Henry's religious policies was quickly suppressed in England. A number of dissenting monks, including the first Carthusian Martyrs, were executed and many more pilloried. The most prominent resisters included John Fisher, Bishop of Rochester, and Sir Thomas More, both of whom refused to take the oath to the King. Neither Henry nor Cromwell sought to have the men executed; rather, they hoped that the two might change their minds and save themselves. Fisher openly rejected Henry as the Supreme Head of the Church, but More was careful to avoid openly breaking the Treason Act, which (unlike later acts) did not forbid mere silence. Both men were subsequently convicted of high treason, however – More on the evidence of a single conversation with Richard Rich, the Solicitor General. Both were duly executed in the summer of 1535.\n\nThese suppressions, as well as the Dissolution of the Lesser Monasteries Act of 1536, in turn contributed to more general resistance to Henry's reforms, most notably in the Pilgrimage of Grace, a large uprising in northern England in October 1536. Some 20,000 to 40,000 rebels were led by Robert Aske, together with parts of the northern nobility. Henry VIII promised the rebels he would pardon them and thanked them for raising the issues. Aske told the rebels they had been successful and they could disperse and go home. Henry saw the rebels as traitors and did not feel obliged to keep his promises with them, so when further violence occurred after Henry's offer of a pardon he was quick to break his promise of clemency. The leaders, including Aske, were arrested and executed for treason. In total, about 200 rebels were executed, and the disturbances ended.\n\nOn 8 January 1536 news reached the king and the queen that Catherine of Aragon had died. Henry called for public displays of joy regarding Catherine's death. The queen was pregnant again, and she was aware of the consequences if she failed to give birth to a son. Later that month, the King was unhorsed in a tournament and was badly injured; it seemed for a time that his life was in danger. When news of this accident reached the queen, she was sent into shock and miscarried a male child that was about 15 weeks old, on the day of Catherine's funeral, 29 January 1536. For most observers, this personal loss was the beginning of the end of the royal marriage.\n\nAlthough the Boleyn family still held important positions on the Privy Council, Anne had many enemies, including the Duke of Suffolk. Even her own uncle, the Duke of Norfolk, had come to resent her attitude to her power. The Boleyns preferred France over the Emperor as a potential ally, but the King's favour had swung towards the latter (partly because of Cromwell), damaging the family's influence. Also opposed to Anne were supporters of reconciliation with Princess Mary (among them the former supporters of Catherine), who had reached maturity. A second annulment was now a real possibility, although it is commonly believed that it was Cromwell's anti-Boleyn influence that led opponents to look for a way of having her executed.\n\nAnne's downfall came shortly after she had recovered from her final miscarriage. Whether it was primarily the result of allegations of conspiracy, adultery, or witchcraft remains a matter of debate among historians. Early signs of a fall from grace included the King's new mistress, the 28-year-old Jane Seymour, being moved into new quarters, and Anne's brother, George Boleyn, being refused the Order of the Garter, which was instead given to Nicholas Carew. Between 30 April and 2 May, five men, including Anne's brother, were arrested on charges of treasonable adultery and accused of having sexual relationships with the queen. Anne was also arrested, accused of treasonous adultery and incest. Although the evidence against them was unconvincing, the accused were found guilty and condemned to death. George Boleyn and the other accused men were executed on 17 May 1536. At 8 am on 19 May 1536, Anne was executed on Tower Green.\n\nThe day after Anne's execution in 1536 the 45-year-old Henry became engaged to Seymour, who had been one of the Queen's ladies-in-waiting. They were married ten days later. On 12 October 1537, Jane gave birth to a son, Prince Edward, the future Edward VI. The birth was difficult, and the queen died on 24 October 1537 from an infection and was buried in Windsor. The euphoria that had accompanied Edward's birth became sorrow, but it was only over time that Henry came to long for his wife. At the time, Henry recovered quickly from the shock. Measures were immediately put in place to find another wife for Henry, which, at the insistence of Cromwell and the court, were focused on the European continent.\n\nWith Charles V distracted by the internal politics of his many kingdoms and external threats, and Henry and Francis on relatively good terms, domestic and not foreign policy issues had been Henry's priority in the first half of the 1530s. In 1536, for example, Henry granted his assent to the Laws in Wales Act 1535, which legally annexed Wales, uniting England and Wales into a single nation. This was followed by the Second Succession Act (the Act of Succession 1536), which declared Henry's children by Jane to be next in the line of succession and declared both Mary and Elizabeth illegitimate, thus excluding them from the throne. The king was also granted the power to further determine the line of succession in his will, should he have no further issue. However, when Charles and Francis made peace in January 1539, Henry became increasingly paranoid, perhaps as a result of receiving a constant list of threats to the kingdom (real or imaginary, minor or serious) supplied by Cromwell in his role as spymaster. Enriched by the dissolution of the monasteries, Henry used some of his financial reserves to build a series of coastal defences and set some aside for use in the event of a Franco-German invasion.\n\nHaving considered the matter, Cromwell, now Earl of Essex, suggested Anne, the 25-year-old sister of the Duke of Cleves, who was seen as an important ally in case of a Roman Catholic attack on England, for the duke fell between Lutheranism and Catholicism. Hans Holbein the Younger was dispatched to Cleves to paint a portrait of Anne for the king. Despite speculation that Holbein painted her in an overly flattering light, it is more likely that the portrait was accurate; Holbein remained in favour at court. After seeing Holbein's portrait, and urged on by the complimentary description of Anne given by his courtiers, the 49-year-old king agreed to wed Anne. However, it was not long before Henry wished to annul the marriage so he could marry another. Anne did not argue, and confirmed that the marriage had never been consummated. Anne's previous betrothal to the Duke of Lorraine's son Francis provided further grounds for the annulment. The marriage was subsequently dissolved, and Anne received the title of \"The King's Sister\", two houses and a generous allowance. It was soon clear that Henry had fallen for the 17-year-old Catherine Howard, the Duke of Norfolk's niece, the politics of which worried Cromwell, for Norfolk was a political opponent.\n\nShortly after, the religious reformers (and protégés of Cromwell) Robert Barnes, William Jerome and Thomas Garret were burned as heretics. Cromwell, meanwhile, fell out of favour although it is unclear exactly why, for there is little evidence of differences of domestic or foreign policy. Despite his role, he was never formally accused of being responsible for Henry's failed marriage. Cromwell was now surrounded by enemies at court, with Norfolk also able to draw on his niece's position. Cromwell was charged with treason, selling export licences, granting passports, and drawing up commissions without permission, and may also have been blamed for the failure of the foreign policy that accompanied the attempted marriage to Anne. He was subsequently attainted and beheaded.\n\nOn 28 July 1540 (the same day Cromwell was executed), Henry married the young Catherine Howard, a first cousin and lady-in-waiting of Anne Boleyn. He was absolutely delighted with his new queen, and awarded her the lands of Cromwell and a vast array of jewellery. Soon after the marriage, however, Queen Catherine had an affair with the courtier Thomas Culpeper. She also employed Francis Dereham, who had previously been informally engaged to her and had an affair with her prior to her marriage, as her secretary. The court was informed of her affair with Dereham whilst Henry was away; they dispatched Thomas Cranmer to investigate, who brought evidence of Queen Catherine's previous affair with Dereham to the king's notice. Though Henry originally refused to believe the allegations, Dereham confessed. It took another meeting of the council, however, before Henry believed the accusations against Dereham and went into a rage, blaming the council before consoling himself in hunting. When questioned, the queen could have admitted a prior contract to marry Dereham, which would have made her subsequent marriage to Henry invalid, but she instead claimed that Dereham had forced her to enter into an adulterous relationship. Dereham, meanwhile, exposed Queen Catherine's relationship with Culpeper. Culpeper and Dereham were both executed, and Catherine too was beheaded on 13 February 1542.\n\nIn 1538, the chief minister Thomas Cromwell pursued an extensive campaign against what was termed \"idolatry\" by the followers of the old religion, culminating in September with the dismantling of the shrine of St. Thomas Becket at Canterbury. As a consequence, the king was excommunicated by Pope Paul III on 17 December of the same year. In 1540, Henry sanctioned the complete destruction of shrines to saints. In 1542, England's remaining monasteries were all dissolved, and their property transferred to the Crown. Abbots and priors lost their seats in the House of Lords; only archbishops and bishops remained. Consequently, the Lords Spiritual—as members of the clergy with seats in the House of Lords were known—were for the first time outnumbered by the Lords Temporal.\n\nThe 1539 alliance between Francis and Charles had soured, eventually degenerating into renewed war. With Catherine of Aragon and Anne Boleyn dead, relations between Charles and Henry improved considerably, and Henry concluded a secret alliance with the Emperor and decided to enter the Italian War in favour of his new ally. An invasion of France was planned for 1543. In preparation for it, Henry moved to eliminate the potential threat of Scotland under the youthful James V. The Scots were defeated at Battle of Solway Moss on 24 November 1542, and James died on 15 December. Henry now hoped to unite the crowns of England and Scotland by marrying his son Edward to James' successor, Mary. The Scottish Regent Lord Arran agreed to the marriage in the Treaty of Greenwich on 1 July 1543, but it was rejected by the Parliament of Scotland on 11 December. The result was eight years of war between England and Scotland, a campaign later dubbed \"the Rough Wooing\". Despite several peace treaties, unrest continued in Scotland until Henry's death.\n\nDespite the early success with Scotland, Henry hesitated to invade France, annoying Charles. Henry finally went to France in June 1544 with a two-pronged attack. One force under Norfolk ineffectively besieged Montreuil. The other, under Suffolk, laid siege to Boulogne. Henry later took personal command, and Boulogne fell on 18 September 1544. However, Henry had refused Charles' request to march against Paris. Charles' own campaign fizzled, and he made peace with France that same day. Henry was left alone against France, unable to make peace. Francis attempted to invade England in the summer of 1545, but reached only the Isle of Wight before being repulsed in the Battle of the Solent. Out of money, France and England signed the Treaty of Camp on 7 June 1546. Henry secured Boulogne for eight years. The city was then to be returned to France for 2 million crowns (£750,000). Henry needed the money; the 1544 campaign had cost £650,000, and England was once again bankrupt.\n\nHenry married his last wife, the wealthy widow Catherine Parr, in July 1543. A reformer at heart, she argued with Henry over religion. Ultimately, Henry remained committed to an idiosyncratic mixture of Catholicism and Protestantism; the reactionary mood which had gained ground following the fall of Cromwell had neither eliminated his Protestant streak nor been overcome by it. Parr helped reconcile Henry with his daughters, Mary and Elizabeth. In 1543, an Act of Parliament put them back in the line of succession after Edward. The same act allowed Henry to determine further succession to the throne in his will.\n\nLate in life, Henry became obese, with a waist measurement of , and had to be moved about with the help of mechanical inventions. He was covered with painful, pus-filled boils and possibly suffered from gout. His obesity and other medical problems can be traced to the jousting accident in 1536 in which he suffered a leg wound. The accident re-opened and aggravated a previous injury he had sustained years earlier, to the extent that his doctors found it difficult to treat. The chronic wound festered for the remainder of his life and became ulcerated, thus preventing him from maintaining the level of physical activity he had previously enjoyed. The jousting accident is also believed to have caused Henry's mood swings, which may have had a dramatic effect on his personality and temperament.\n\nThe theory that Henry suffered from syphilis has been dismissed by most historians. Historian Susan Maclean Kybett ascribes his demise to scurvy, which is caused by a lack of fresh fruits and vegetables. Alternatively, his wives' pattern of pregnancies and his mental deterioration have led some to suggest that the king may have been Kell positive and suffered from McLeod syndrome. According to another study, Henry VIII's history and body morphology may have been the result of traumatic brain injury after his 1536 jousting accident, which in turn led to a neuroendocrine cause of his obesity. This analysis identifies growth hormone deficiency (GHD) as the source for his increased adiposity but also significant behavioural changes noted in his later years, including his multiple marriages.\nHenry's obesity hastened his death at the age of 55, which occurred on 28 January 1547 in the Palace of Whitehall, on what would have been his father's 90th birthday. He was interred in St George's Chapel, Windsor Castle, next to Jane Seymour. Over a hundred years later, King Charles I (1625–1649) was buried in the same vault.\n\nUpon Henry's death, he was succeeded by his son Edward VI. Since Edward was then only nine years old, he could not rule directly. Instead, Henry's will designated 16 executors to serve on a council of regency until Edward reached the age of 18. The executors chose Edward Seymour, 1st Earl of Hertford, Jane Seymour's elder brother, to be Lord Protector of the Realm. If Edward died childless, the throne was to pass to Mary, Henry VIII's daughter by Catherine of Aragon, and her heirs. If Mary's issue failed, the crown was to go to Elizabeth, Henry's daughter by Anne Boleyn, and her heirs. Finally, if Elizabeth's line became extinct, the crown was to be inherited by the descendants of Henry VIII's deceased younger sister, Mary, the Greys. The descendants of Henry's sister Margaret – the Stuarts, rulers of Scotland – were thereby excluded from the succession. This final provision failed when James VI of Scotland became King of England in 1603.\n\nHenry cultivated the image of a Renaissance man, and his court was a centre of scholarly and artistic innovation and glamorous excess, epitomised by the Field of the Cloth of Gold. He scouted the country for choirboys, taking some directly from Wolsey's choir, and introduced Renaissance music into court. Musicians included Benedict de Opitiis, Richard Sampson, Ambrose Lupo, and Venetian organist Dionisio Memo.\n\nHenry himself kept a considerable collection of instruments; he was skilled on the lute, could play the organ, and was a talented player of the virginals. He could also sight read music and sing well. He was an accomplished musician, author, and poet; his best known piece of music is \"Pastime with Good Company\" (\"The Kynges Ballade\"). He is often reputed to have written \"Greensleeves\" but probably did not.\n\nHe was an avid gambler and dice player, and excelled at sports, especially jousting, hunting, and real tennis. He was known for his strong defence of conventional Christian piety. The King was involved in the original construction and improvement of several significant buildings, including Nonsuch Palace, King's College Chapel, Cambridge and Westminster Abbey in London. Many of the existing buildings Henry improved were properties confiscated from Wolsey, such as Christ Church, Oxford; Hampton Court Palace; the Palace of Whitehall; and Trinity College, Cambridge.\n\nHenry was an intellectual. The first English king with a modern humanist education, he read and wrote English, French and Latin, and was thoroughly at home in his well-stocked library. He personally annotated many books and wrote and published one of his own. To promote the public support for the reformation of the church, Henry had numerous pamphlets and lectures prepared. For example, Richard Sampson's \"Oratio\" (1534) was an argument for absolute obedience to the monarchy and claimed that the English church had always been independent from Rome. At the popular level, theatre and minstrel troupes funded by the crown travelled around the land to promote the new religious practices: the pope and Catholic priests and monks were mocked as foreign devils, while the glorious king was hailed as a brave and heroic defender of the true faith. Henry worked hard to present an image of unchallengeable authority and irresistible power.\n\nA large well-built athlete (over tall and strong and broad in proportion), Henry excelled at jousting and hunting. More than pastimes, they were political devices that served multiple goals, from enhancing his athletic royal image to impressing foreign emissaries and rulers, to conveying Henry's ability to suppress any rebellion. Thus he arranged a jousting tournament at Greenwich in 1517, where he wore gilded armour, gilded horse trappings, and outfits of velvet, satin and cloth of gold dripping with pearls and jewels. It suitably impressed foreign ambassadors, one of whom wrote home that, \"The wealth and civilisation of the world are here, and those who call the English barbarians appear to me to render themselves such\". Henry finally retired from jousting in 1536 after a heavy fall from his horse left him unconscious for two hours, but he continued to sponsor two lavish tournaments a year. He then started adding weight and lost the trim, athletic figure that had made him so handsome; Henry's courtiers began dressing in heavily padded clothes to emulate – and flatter – their increasingly stout monarch. Towards the end of his reign his health rapidly declined.\n\nThe power of Tudor monarchs, including Henry, was 'whole' and 'entire', ruling, as they claimed, by the grace of God alone. The crown could also rely on the exclusive use of those functions that constituted the royal prerogative. These included acts of diplomacy (including royal marriages), declarations of war, management of the coinage, the issue of royal pardons and the power to summon and dissolve parliament as and when required. Nevertheless, as evident during Henry's break with Rome, the monarch worked within established limits, whether legal or financial, that forced him to work closely with both the nobility and parliament (representing the gentry).\n\nIn practice, Tudor monarchs used patronage to maintain a royal court that included formal institutions such as the Privy Council as well as more informal advisers and confidants. Both the rise and fall of court nobles could be swift: although the often-quoted figure of 72,000 executions during his reign is inflated, Henry did undoubtedly execute at will, burning or beheading two of his wives, twenty peers, four leading public servants, six close attendants and friends, one cardinal (John Fisher) and numerous abbots. Among those who were in favour at any given point in Henry's reign, one could usually be identified as a chief minister, though one of the enduring debates in the historiography of the period has been the extent to which those chief ministers controlled Henry rather than vice versa. In particular, historian G. R. Elton has argued that one such minister, Thomas Cromwell, led a \"Tudor revolution in government\" quite independent of the king, whom Elton presented as an opportunistic, essentially lazy participant in the nitty-gritty of politics. Where Henry did intervene personally in the running of the country, Elton argued, he mostly did so to its detriment. The prominence and influence of faction in Henry's court is similarly discussed in the context of at least five episodes of Henry's reign, including the downfall of Anne Boleyn.\n\nFrom 1514 to 1529, Thomas Wolsey (1473–1530), a cardinal of the established Church, oversaw domestic and foreign policy for the young king from his position as Lord Chancellor. Wolsey centralised the national government and extended the jurisdiction of the conciliar courts, particularly the Star Chamber. The Star Chamber's overall structure remained unchanged, but Wolsey used it to provide for much-needed reform of the criminal law. The power of the court itself did not outlive Wolsey, however, since no serious administrative reform was undertaken and its role was eventually devolved to the localities. Wolsey helped fill the gap left by Henry's declining participation in government (particularly in comparison to his father) but did so mostly by imposing himself in the King's place. His use of these courts to pursue personal grievances, and particularly to treat delinquents as if mere examples of a whole class worthy of punishment, angered the rich, who were annoyed as well by his enormous wealth and ostentatious living. Following Wolsey's downfall, Henry took full control of his government, although at court numerous complex factions continued to try to ruin and destroy each other.\n\nThomas Cromwell (c. 1485–1540) also came to define Henry's government. Returning to England from the continent in 1514 or 1515, Cromwell soon entered Wolsey's service. He turned to law, also picking up a good knowledge of the Bible, and was admitted to Gray's Inn in 1524. He became Wolsey's \"man of all work\". Cromwell, driven in part by his religious beliefs, attempted to reform the body politic of the English government through discussion and consent, and through the vehicle of continuity and not outward change. He was seen by many people as the man they wanted to bring about their shared aims, including Thomas Audley. By 1531, Cromwell and those associated with him were already responsible for the drafting of much legislation. Cromwell's first office was that of the master of the King's jewels in 1532, from which he began to invigorate the government finances. By this point, Cromwell's power as an efficient administrator in a Council full of politicians exceeded what Wolsey had achieved.\n\nCromwell did much work through his many offices to remove the tasks of government from the Royal Household (and ideologically from the personal body of the King) and into a public state. He did so, however, in a haphazard fashion that left several remnants, not least because he needed to retain Henry's support, his own power, and the possibility of actually achieving the plan he set out. Cromwell made the various income streams put in place by Henry VII more formal and assigned largely autonomous bodies for their administration. The role of the King's Council was transferred to a reformed Privy Council, much smaller and more efficient than its predecessor. A difference emerged between the financial health of the king, and that of the country, although Cromwell's fall undermined much of his bureaucracy, which required his hand to keep order among the many new bodies and prevent profligate spending that strained relations as well as finances. Cromwell's reforms ground to a halt in 1539, the initiative lost, and he failed to secure the passage of an enabling act, the Proclamation by the Crown Act 1539. He too was executed, on 28 July 1540.\n\nHenry inherited a vast fortune and a prosperous economy from his father Henry VII, who had been frugal and careful with money. This fortune was estimated to £1,250,000 (£375 million by today's standards). By comparison, however, the reign of Henry was a near-disaster in financial terms. Although he further augmented his royal treasury through the seizure of church lands, Henry's heavy spending and long periods of mismanagement damaged the economy.\n\nMuch of this wealth was spent by Henry on maintaining his court and household, including many of the building works he undertook on royal palaces. Henry hung 2,000 tapestries in his palaces; by comparison, James V of Scotland hung just 200. Henry took pride in showing off his collection of weapons, which included exotic archery equipment, 2,250 pieces of land ordnance and 6,500 handguns. Tudor monarchs had to fund all the expenses of government out of their own income. This income came from the Crown lands that Henry owned as well as from customs duties like tonnage and poundage, granted by parliament to the king for life. During Henry's reign the revenues of the Crown remained constant (around £100,000), but were eroded by inflation and rising prices brought about by war. Indeed, war and Henry's dynastic ambitions in Europe exhausted the surplus he had inherited from his father by the mid-1520s.\n\nWhereas Henry VII had not involved Parliament in his affairs very much, Henry VIII had to turn to Parliament during his reign for money, in particular for grants of subsidies to fund his wars. The Dissolution of the Monasteries provided a means to replenish the treasury, and as a result the Crown took possession of monastic lands worth £120,000 (£36 million) a year. The Crown had profited a small amount in 1526 when Wolsey had put England onto a gold, rather than silver, standard, and had debased the currency slightly. Cromwell debased the currency more significantly, starting in Ireland in 1540. The English pound halved in value against the Flemish pound between 1540 and 1551 as a result. The nominal profit made was significant, helping to bring income and expenditure together, but it had a catastrophic effect on the overall economy of the country. In part, it helped to bring about a period of very high inflation from 1544 onwards.\n\nHenry is generally credited with initiating the English Reformation – the process of transforming England from a Catholic country to a Protestant one – though his progress at the elite and mass levels is disputed, and the precise narrative not widely agreed. Certainly, in 1527, Henry, until then an observant and well-informed Catholic, appealed to the Pope for an annulment of his marriage to Catherine. No annulment was immediately forthcoming, the result in part of Charles V's control of the Papacy. The traditional narrative gives this refusal as the trigger for Henry's rejection of papal supremacy (which he had previously defended), though as historian A. F. Pollard has argued, even if Henry had not needed an annulment, Henry may have come to reject papal control over the governance of England purely for political reasons.\n\nIn any case, between 1532 and 1537, Henry instituted a number of statutes that dealt with the relationship between king and pope and hence the structure of the nascent Church of England. These included the Statute in Restraint of Appeals (passed 1533), which extended the charge of \"praemunire\" against all who introduced papal bulls into England, potentially exposing them to the death penalty if found guilty. Other acts included the Supplication against the Ordinaries and the Submission of the Clergy, which recognised Royal Supremacy over the church. The Ecclesiastical Appointments Act 1534 required the clergy to elect bishops nominated by the Sovereign. The Act of Supremacy in 1534 declared that the King was \"the only Supreme Head on Earth of the Church of England\" and the Treasons Act 1534 made it high treason, punishable by death, to refuse the Oath of Supremacy acknowledging the King as such. Similarly, following the passage of the Act of Succession 1533, all adults in the Kingdom were required to acknowledge the Act's provisions (declaring Henry's marriage to Anne legitimate and his marriage to Catherine illegitimate) by oath; those who refused were subject to imprisonment for life, and any publisher or printer of any literature alleging that the marriage to Anne was invalid subject to the death penalty. Finally, the Peter's Pence Act was passed, and it reiterated that England had \"no superior under God, but only your Grace\" and that Henry's \"imperial crown\" had been diminished by \"the unreasonable and uncharitable usurpations and exactions\" of the Pope. The King had much support from the Church under Cranmer.\n\nHenry, to Thomas Cromwell's annoyance, insisted on parliamentary time to discuss questions of faith, which he achieved through the Duke of Norfolk. This led to the passing of the Act of Six Articles, whereby six major questions were all answered by asserting the religious orthodoxy, thus restraining the reform movement in England. It was followed by the beginnings of a reformed liturgy and of the Book of Common Prayer, which would take until 1549 to complete. The victory won by religious conservatives did not convert into much change in personnel, however, and Cranmer remained in his position. Overall, the rest of Henry's reign saw a subtle movement away from religious orthodoxy, helped in part by the deaths of prominent figures from before the break with Rome, especially the executions of Thomas More and John Fisher in 1535 for refusing to renounce papal authority. Henry established a new political theology of obedience to the crown that was continued for the next decade. It reflected Martin Luther's new interpretation of the fourth commandment (\"Honour thy father and mother\"), brought to England by William Tyndale. The founding of royal authority on the Ten Commandments was another important shift: reformers within the Church used the Commandments' emphasis on faith and the word of God, while conservatives emphasised the need for dedication to God and doing good. The reformers' efforts lay behind the publication of the \"Great Bible\" in 1539 in English. Protestant Reformers still faced persecution, particularly over objections to Henry's annulment. Many fled abroad, including the influential Tyndale, who was eventually executed and his body burned at Henry's behest.\n\nWhen taxes once payable to Rome were transferred to the Crown, Cromwell saw the need to assess the taxable value of the Church's extensive holdings as they stood in 1535. The result was an extensive compendium, the \"Valor Ecclesiasticus\". In September of the same year, Cromwell commissioned a more general visitation of religious institutions, to be undertaken by four appointee visitors. The visitation focussed almost exclusively on the country's religious houses, with largely negative conclusions. In addition to reporting back to Cromwell, the visitors made the lives of the monks more difficult by enforcing strict behavioural standards. The result was to encourage self-dissolution. In any case, the evidence gathered by Cromwell led swiftly to the beginning of the state-enforced dissolution of the monasteries with all religious houses worth less than £200 vested by statute in the crown in January 1536. After a short pause, surviving religious houses were transferred one by one to the Crown and onto new owners, and the dissolution confirmed by a further statute in 1539. By January 1540 no such houses remained: some 800 had been dissolved. The process had been efficient, with minimal resistance, and brought the crown some £90,000 a year. The extent to which the dissolution of all houses was planned from the start is debated by historians; there is some evidence that major houses were originally intended only to be reformed. Cromwell's actions transferred a fifth of England's landed wealth to new hands. The programme was designed primarily to create a landed gentry beholden to the crown, which would use the lands much more efficiently. Although little opposition to the supremacy could be found in England's religious houses, they had links to the international church and were an obstacle to further religious reform.\n\nResponse to the reforms was mixed. The religious houses had been the only support of the impoverished, and the reforms alienated much of the population outside London, helping to provoke the great northern rising of 1536–1537, known as the Pilgrimage of Grace. Elsewhere the changes were accepted and welcomed, and those who clung to Catholic rites kept quiet or moved in secrecy. They would re-emerge during the reign of Henry's daughter Mary (1553–1558).\n\nApart from permanent garrisons at Berwick, Calais, and Carlisle, England's standing army numbered only a few hundred men. This was increased only slightly by Henry. Henry's invasion force of 1513, some 30,000 men, was composed of billmen and longbowmen, at a time when the other European nations were moving to hand guns and pikemen. The difference in capability was at this stage not significant, however, and Henry's forces had new armour and weaponry. They were also supported by battlefield artillery and the war wagon, relatively new innovations, and several large and expensive siege guns. The invasion force of 1544 was similarly well-equipped and organised, although command on the battlefield was laid with the dukes of Suffolk and Norfolk, which in the case of the latter produced disastrous results at Montreuil.\n\nHenry's break with Rome incurred the threat of a large-scale French or Spanish invasion. To guard against this, in 1538, he began to build a chain of expensive, state-of-the-art defences, along Britain's southern and eastern coasts from Kent to Cornwall, largely built of material gained from the demolition of the monasteries. These were known as Henry VIII's Device Forts. He also strengthened existing coastal defence fortresses such as Dover Castle and, at Dover, Moat Bulwark and Archcliffe Fort, which he personally visited for a few months to supervise. Wolsey had many years before conducted the censuses required for an overhaul of the system of militia, but no reform resulted. In 1538–39, Cromwell overhauled the shire musters, but his work mainly served to demonstrate how inadequate they were in organisation. The building works, including that at Berwick, along with the reform of the militias and musters, were eventually finished under Queen Mary.\n\nHenry is traditionally cited as one of the founders of the Royal Navy. Technologically, Henry invested in large cannon for his warships, an idea that had taken hold in other countries, to replace the smaller serpentines in use. He also flirted with designing ships personally – although his contribution to larger vessels, if any, is not known, it is believed that he influenced the design of rowbarges and similar galleys. Henry was also responsible for the creation of a permanent navy, with the supporting anchorages and dockyards. Tactically, Henry's reign saw the Navy move away from boarding tactics to employ gunnery instead. The Tudor navy was enlarged up to fifty ships (the \"Mary Rose\" was one of them), and Henry was responsible for the establishment of the \"council for marine causes\" to specifically oversee all the maintenance and operation of the Navy, becoming the basis for the later Admiralty.\n\nAt the beginning of Henry's reign, Ireland was effectively divided into three zones: the Pale, where English rule was unchallenged; Leinster and Munster, the so-called \"obedient land\" of Anglo-Irish peers; and the Gaelic Connaught and Ulster, with merely nominal English rule. Until 1513, Henry continued the policy of his father, to allow Irish lords to rule in the king's name and accept steep divisions between the communities. However, upon the death of the 8th Earl of Kildare, governor of Ireland, fractious Irish politics combined with a more ambitious Henry to cause trouble. When Thomas Butler, 7th Earl of Ormond died, Henry recognised one successor for Ormond's English, Welsh and Scottish lands, whilst in Ireland another took control. Kildare's successor, the 9th Earl, was replaced as Lord Lieutenant of Ireland by Thomas Howard, Earl of Surrey in 1520. Surrey's ambitious aims were costly, but ineffective; English rule became trapped between winning the Irish lords over with diplomacy, as favoured by Henry and Wolsey, and a sweeping military occupation as proposed by Surrey. Surrey was recalled in 1521, with Piers Butler – one of claimants to the Earldom of Ormond – appointed in his place. Butler proved unable to control opposition, including that of Kildare. Kildare was appointed chief governor in 1524, resuming his dispute with Butler, which had before been in a lull. Meanwhile, the Earl of Desmond, an Anglo-Irish peer, had turned his support to Richard de la Pole as pretender to the English throne; when in 1528 Kildare failed to take suitable actions against him, Kildare was once again removed from his post.\n\nThe Desmond situation was resolved on his death in 1529, which was followed by a period of uncertainty. This was effectively ended with the appointment of Henry FitzRoy, Duke of Richmond and the king's son, as lord lieutenant. Richmond had never before visited Ireland, his appointment a break with past policy. For a time it looked as if peace might be restored with the return of Kildare to Ireland to manage the tribes, but the effect was limited and the Irish parliament soon rendered ineffective. Ireland began to receive the attention of Cromwell, who had supporters of Ormond and Desmond promoted. Kildare, on the other hand, was summoned to London; after some hesitation, he departed for London in 1534, where he would face charges of treason. His son, Thomas, Lord Offaly was more forthright, denouncing the king and leading a \"Catholic crusade\" against the king, who was by this time mired in marital problems. Offaly had the Archbishop of Dublin murdered, and besieged Dublin. Offaly led a mixture of Pale gentry and Irish tribes, although he failed to secure the support of Lord Darcy, a sympathiser, or Charles V. What was effectively a civil war was ended with the intervention of 2,000 English troops – a large army by Irish standards – and the execution of Offaly (his father was already dead) and his uncles.\n\nAlthough the Offaly revolt was followed by a determination to rule Ireland more closely, Henry was wary of drawn-out conflict with the tribes, and a royal commission recommended that the only relationship with the tribes was to be promises of peace, their land protected from English expansion. The man to lead this effort was Sir Antony St Leger, as Lord Deputy of Ireland, who would remain into the post past Henry's death. Until the break with Rome, it was widely believed that Ireland was a Papal possession granted as a mere fiefdom to the English king, so in 1541 Henry asserted England's claim to the Kingdom of Ireland free from the Papal overlordship. This change did, however, also allow a policy of peaceful reconciliation and expansion: the Lords of Ireland would grant their lands to the King, before being returned as fiefdoms. The incentive to comply with Henry's request was an accompanying barony, and thus a right to sit in the Irish House of Lords, which was to run in parallel with England's. The Irish law of the tribes did not suit such an arrangement, because the chieftain did not have the required rights; this made progress tortuous, and the plan was abandoned in 1543, not to be replaced.\n\nThe complexities and sheer scale of Henry's legacy ensured that, in the words of Betteridge and Freeman, \"throughout the centuries, Henry has been praised and reviled, but he has never been ignored\". Historian J.D. Mackie sums up Henry's personality and its impact on his achievements and popularity:\n\nA particular focus of modern historiography has been the extent to which the events of Henry's life (including his marriages, foreign policy and religious changes) were the result of his own initiative and, if they were, whether they were the result of opportunism or of a principled undertaking by Henry. The traditional interpretation of those events was provided by historian A.F. Pollard, who in 1902 presented his own, largely positive, view of the king, lauding him, \"as the king and statesman who, whatever his personal failings, led England down the road to parliamentary democracy and empire\". Pollard's interpretation remained the dominant interpretation of Henry's life until the publication of the doctoral thesis of G. R. Elton in 1953.\n\nElton's book on \"The Tudor Revolution in Government\", maintained Pollard's positive interpretation of the Henrician period as a whole, but reinterpreted Henry himself as a follower rather than a leader. For Elton, it was Cromwell and not Henry who undertook the changes in government – Henry was shrewd, but lacked the vision to follow a complex plan through. Henry was little more, in other words, than an \"ego-centric monstrosity\" whose reign \"owed its successes and virtues to better and greater men about him; most of its horrors and failures sprang more directly from [the king]\".\n\nAlthough the central tenets of Elton's thesis have since been questioned, it has consistently provided the starting point for much later work, including that of J. J. Scarisbrick, his student. Scarisbrick largely kept Elton's regard for Cromwell's abilities, but returned agency to Henry, who Scarisbrick considered to have ultimately directed and shaped policy. For Scarisbrick, Henry was a formidable, captivating man who \"wore regality with a splendid conviction\". The effect of endowing Henry with this ability, however, was largely negative in Scarisbrick's eyes: to Scarisbrick the Henrician period was one of upheaval and destruction and those in charge worthy of blame more than praise. Even among more recent biographers, including David Loades, David Starkey and John Guy, there has ultimately been little consensus on the extent to which Henry was responsible for the changes he oversaw or the correct assessment of those he did bring about.\n\nThis lack of clarity about Henry's control over events has contributed to the variation in the qualities ascribed to him: religious conservative or dangerous radical; lover of beauty or brutal destroyer of priceless artefacts; friend and patron or betrayer of those around him; chivalry incarnate or ruthless chauvinist. One traditional approach, favoured by Starkey and others, is to divide Henry's reign into two halves, the first Henry being dominated by positive qualities (politically inclusive, pious, athletic but also intellectual) who presided over a period of stability and calm, and the latter a \"hulking tyrant\" who presided over a period of dramatic, sometimes whimsical, change. Other writers have tried to merge Henry's disparate personality into a single whole; Lacey Baldwin Smith, for example, considered him an egotistical borderline neurotic given to great fits of temper and deep and dangerous suspicions, with a mechanical and conventional, but deeply held piety, and having at best a mediocre intellect.\n\nMany changes were made to the royal style during his reign. Henry originally used the style \"Henry the Eighth, by the Grace of God, King of England, France and Lord of Ireland\". In 1521, pursuant to a grant from Pope Leo X rewarding Henry for his \"Defence of the Seven Sacraments\", the royal style became \"Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith and Lord of Ireland\". Following Henry's excommunication, Pope Paul III rescinded the grant of the title \"Defender of the Faith\", but an Act of Parliament (35 Hen 8 c 3) declared that it remained valid; and it continues in royal usage to the present day. Henry's motto was \"Coeur Loyal\" (\"true heart\"), and he had this embroidered on his clothes in the form of a heart symbol and with the word \"loyal\". His emblem was the Tudor rose and the Beaufort portcullis. As king, Henry's arms were the same as those used by his predecessors since Henry IV: \"Quarterly, Azure three fleurs-de-lys Or (for France) and Gules three lions passant guardant in pale Or (for England)\".\n\nIn 1535, Henry added the \"supremacy phrase\" to the royal style, which became \"Henry the Eighth, by the Grace of God, King of England and France, Defender of the Faith, Lord of Ireland and of the Church of England in Earth Supreme Head\". In 1536, the phrase \"of the Church of England\" changed to \"of the Church of England and also of Ireland\". In 1541, Henry had the Irish Parliament change the title \"Lord of Ireland\" to \"King of Ireland\" with the Crown of Ireland Act 1542, after being advised that many Irish people regarded the Pope as the true head of their country, with the Lord acting as a mere representative. The reason the Irish regarded the Pope as their overlord was that Ireland had originally been given to King Henry II of England by Pope Adrian IV in the 12th century as a feudal territory under papal overlordship. The meeting of Irish Parliament that proclaimed Henry VIII as King of Ireland was the first meeting attended by the Gaelic Irish chieftains as well as the Anglo-Irish aristocrats. The style \"Henry the Eighth, by the Grace of God, King of England, France and Ireland, Defender of the Faith and of the Church of England and also of Ireland in Earth Supreme Head\" remained in use until the end of Henry's reign.\n\n\n\n\n\n"}
{"id": "14189", "url": "https://en.wikipedia.org/wiki?curid=14189", "title": "Haryana", "text": "Haryana\n\nHaryana (), carved out of the former state of East Punjab on 1November 1966 on linguistic as well as on cultural basis, is one of the 29 states in India. Situated in North India with less than 1.4% () of India's land area, it is ranked 22nd in terms of area. Chandigarh is the state capital, Faridabad in National Capital Region is the most populous city of the state and Gurugram is a leading financial hub of NCR with major Fortune 500 companies located in it. Haryana has 6 administrative divisions, 22 districts, 72 sub-divisions, 93 revenue tehsils, 50 sub-tehsils, 140 community development blocks, 154 cities and towns, 6,848 villages and 6222 villages panchayats.\n\nAs the largest recipient of investment per capita since 2000 in India, and among one of the wealthiest and most economically developed regions in South Asia, Haryana has the fifth highest per capita income among Indian states and union territories at against the national average of for year 2016–17. Haryana's 2017-18 estimated state GSDP of US$95 billion (52% services, 34% industries and 14% agriculture) is growing at 12.96% 2012-17 CAGR and placed on the 13th position behind only much bigger states, is also boosted by 30 SEZs (mainly along DMIC, ADKIC and DWPE in NCR), 7% national agricultural exports, 65% of national Basmati rice export, 67% cars, 60% motorbikes, 50% tractors and 50% refrigerators produced in India. Faridabad has been described as eighth fastest growing city in the world and third most in India by City Mayors Foundation survey. In services, Gurugram ranks number 1 in India in IT growth rate and existing technology infrastructure, and number 2 in startup ecosystem, innovation and livability (Nov 2016).\n\nAmong the world's oldest and largest ancient civilizations, the Indus Valley Civilization sites at Rakhigarhi village in Hisar district and Bhirrana in Fatehabad district are 9,000 years old. Rich in history, monuments, heritage, flora and fauna, human resources and tourism with well developed economy, national highways and state roads, it is bordered by Himachal Pradesh to the north-east, by river Yamuna along its eastern border with Uttar Pradesh, by Rajasthan to the west and south, and Ghaggar-Hakra River flows along its northern border with Punjab. Since Haryana surrounds the country's capital Delhi on three sides (north, west and south), consequently a large area of Haryana is included in the economically-important National Capital Region for the purposes of planning and development.\n\nThe name Haryana is found in the works of the 12th-century AD Apabhramsha writer Vibudh Shridhar (VS 1189–1230). The name Haryana has been derived from the Sanskrit words \"Hari\" (the Hindu god Vishnu) and \"ayana\" (home), meaning \"the Abode of God\". However, scholars such as Muni Lal, Murli Chand Sharma, HA Phadke and Sukhdev Singh Chib believe that the name comes from a compound of the words \"Hari\" (Sanskrit \"Harit\", \"green\") and \"Aranya\" (forest).\n\nThe Vedic state of Brahmavarta is claimed to be located in south Haryana, where the initial Vedic scriptures were composed after the great floods some 10,000 years ago.\n\nRakhigarhi village in Hisar district and Bhirrana in Fatehabad district are home to the largest and one of the world's oldest ancient Indus Valley Civilization sites, dated at over 9,000 years old. Evidence of paved roads, a drainage system, a large-scale rainwater collection storage system, terracotta brick and statue production, and skilled metal working (in both bronze and precious metals) have been uncovered. According to archaeologists, Rakhigarhi may be the origin of Harappan civilisation, which arose in the Ghaggar basin in Haryana and gradually and slowly moved to the Indus valley.\n\nAncient bronze and stone idols of Jain Tirthankara were found in archaeological expeditions in Badli, Bhiwani (Ranila, Charkhi Dadri and Badhra), Dadri, Gurgaon (Ferozepur Jhirka), Hansi, Hisar (Agroha), Kasan, Nahad, Narnaul, Pehowa, Rewari, Rohad, Rohtak (Asthal Bohar) and Sonepat in Haryana.\n\nAfter the sack of Bhatner fort during the Timurid conquests of India in 1398, Timur attacked and sacked the cities of Sirsa, Fatehabad, Sunam, Kaithal and Panipat. When he reached the town of Sarsuti, the residents, who were mostly non-Muslims, fled and were chased by a detachment of Timur's troops, with thousands of them being killed and looted by the troops. From there he travelled to Fatehabad, whose residents fled and a large number of those remaining in the town were massacred. The Ahirs resisted him at Ahruni but were defeated, with thousands being killed and many being taken prisoners while the town was burnt to ashes. From there he travelled to Tohana, whose Jat inhabitants were stated to be robbers according to Sharaf ad-Din Ali Yazdi. They tried to resist but were defeated and fled. Timur's army pursued and killed 200 Jats, while taking many more as prisoners. He then sent a detachment to chase the fleeing Jats and killed 2,000 of them while their wives and children were enslaved and their property plundered. Timur proceeded to Kaithal whose residents were massacred and plundered, destroying all villages along the way. On the next day, he came to Assandh whose residents were \"fire-worshippers\" according to Yazdi, and had fled to Delhi. Next he travelled to and subdued Tughlaqpur fort and Salwan before reaching Panipat whose residents had already fled. He then marched on to Loni fort.\n\nThe area that is now Haryana has been ruled by some of the major empires of India. Panipat is known for three seminal battles in the history of India. In the First Battle of Panipat (1526), Babur defeated the Lodis. In the Second Battle of Panipat (1556), Akbar defeated the local Haryanvi Hindu Emperor of Delhi, who belonged to Rewari. Hem Chandra Vikramaditya had earlier won 22 battles across India from Punjab to Bengal, defeating Mughals and Afghans. Hemu had defeated Akbar's forces twice at Agra and the Battle of Delhi in 1556 to become the last Hindu Emperor of India with a formal coronation at Purana Quila in Delhi on 7 October 1556. In the Third Battle of Panipat (1761), the Afghan king Ahmad Shah Abdali defeated the Marathas.\n\nHaryana as a state came into existence on 1November 1966 the Punjab Reorganisation Act (1966). The Indian government set up the Shah Commission under the chairmanship of Justice JC Shah on 23 April 1966 to divide the existing state of Punjab and determine the boundaries of the new state of Haryana after consideration of the languages spoken by the people. The commission delivered its report on 31May 1966 whereby the then-districts of Hisar, Mahendragarh, Gurgaon, Rohtak and Karnal were to be a part of the new state of Haryana. Further, the tehsils of Jind and Narwana in the Sangrur district — along with Naraingarh, Ambala and Jagadhri — were to be included.\n\nThe commission recommended that the tehsil of Kharad, which includes Chandigarh, the state capital of Punjab, should be a part of Haryana. However, only a small portion of Kharad was given to Haryana. The city of Chandigarh was made a union territory, serving as the capital of both Punjab and Haryana.\n\nBhagwat Dayal Sharma became the first Chief Minister of Haryana.\n\nAccording to the 2011 census, of total 25,350,000 population of Haryana, Hindus (87.46%) constitute the majority of the state's population with Muslims (7.03%) (mainly Meos) and Sikhs (4.91%) being the largest minorities.\n\nMuslims are mainly found in the Mewat or Nuh district. Haryana has the second largest Sikh population in India after Punjab, and they mostly live in the districts adjoining Punjab, such as Sirsa, Jind, Fatehabad, Kaithal, Kurukshetra, Ambala and Panchkula.\n\nHindi was the sole official language of Haryana until 2010 and it is spoken by the majority of the population (87.31%). Haryana has 70% rural population who primarily speak Haryanvi dialect of Hindi, as well as other related dialects, such as Bagri and Mewati. Significant minority languages spoken in Haryana are Punjabi (10.57%), Urdu (1.23%), Bengali (0.19%) and Nepali (0.10%). Additionally 0.6% of the state population speak other minority languages. Punjabi is the second official language of Haryana for government and administrative purposes.\n\nHaryana has its own unique traditional folk music, folk dances, saang (folk theater), cinema, belief system such as Jathera (ancestral worship), and arts such as Phulkari and Shisha embroidery.\n\nFolk music and dances of Haryana are based on satisfying cultural needs of primarily agrarian and martial natures of Haryanavi tribes.\n\nHaryanvi musical folk theater main types are Saang, Rasa lila and Ragini. The Saang and Ragini form of theater was popularised by Lakhmi Chand.\n\nHaryanvi folk dances and music have fast energetic movements. Three popular categories of dance are: festive-seasonal, devotional, and ceremonial-recreational. The festive-seasonal dances and songs are Gogaji/Gugga, Holi, Phaag, Sawan, Teej. The devotional dances and songs are Chaupaiya, Holi, Manjira, Ras Leela, Raginis). The ceremonial-recreational dances and songs are of following types: legendary bravery (Kissa and Ragini of male warriors and female Satis), love and romance (Been and its variant Nāginī dance, and Ragini), ceremonial (Dhamal Dance, Ghoomar, Jhoomar (male), Khoria, Loor, and Ragini).\n\nHaryanvi folk music is based on day to day themes and injecting earthly humor enlivens the feel of the songs. Haryanvi music takes two main forms: \"Classical folk music\" and \"Desi Folk music\" (Country Music of Haryana), and sung in the form of ballads and love, valor and bravery, harvest, happiness and pangs of parting of lovers.\n\nClassical Haryanvi folk music is based on Indian classical music. Hindustani classical ragas, learnt in gharana parampara of guru–shishya tradition, are used to sing songs of heroic bravery (such as Alha-Khand (1663-1202 CE) about bravery of Alha and Udal, Jaimal Fatta of Maharana Udai Singh II), Brahmas worship and festive seasonal songs (such as Teej, Holi and Phaag songs of Phalgun month near Holi). Bravery songs are sung in high pitch.\n\nDesi Haryanvi folk music (Haryanvi country folk music) The country-side or desi (native) form of Haryanvi music is based on Raag Bhairvi, Raag Bhairav, Raag Kafi, Raag Jaijaivanti, Raag Jhinjhoti and Raag Pahadi and used for celebrating community bonhomie to sing seasonal songs, ballads, ceremonial songs (wedding, etc.) and related religious legendary tales such as Puran Bhagat. Relationship and songs celebrating love and life are sung in medium pitch. Ceremonial and religious songs are sung in low pitch. Young girls and women usually sing entertaining and fast seasonal, love, relationship and friendship related songs such as Phagan (song for eponymous season/month), Katak (songs for the eponymous season/month), Samman (songs for the eponymous season/month), bande-bandi (male-female duet songs), sathne (songs of sharing heartfelt feelings among female friends). Older women usually sing devotional Mangal Geet (auspicious songs) and ceremonial songs such as Bhajan, Bhat (wedding gift to the mother of bride or groom by her brother), Sagai, Ban (Hindu wedding ritual where pre-wedding festivities starts), Kuan-Poojan (a custom that is performed to welcome the birth of a child by worshiping the well or source of drinking water), Sanjhi and Holi festival.\n\nMusic and dance for Haryanvi people is a great way of demolishing societal differences as folk singers are highly esteemed and they are sought after and invited for the events, ceremonies and special occasions regardless of their caste or status. These inter-caste songs are fluid in nature, and never personalized for any specific caste, and they are sung collectively by women from different strata, castes, dialects. These songs do transform fluidly in dialect, style, words, etc. This adoptive style can be seen from the adoption of tunes of Bollywood movie songs into Haryanvi songs. Despite this continuous fluid transforming nature, Haryanvi songs have a distinct style of their own as explained above.\n\n81% people of Haryana are vegetarian, and cuisine of Haryana is based on fresh, earthy and wholesome ethos of its agrarian culture, where staples are roti, saag, vegetarian sabzi and abundance of milk products such as homemade nooni or tindi ghee, ghee (clarified butter), milk, lassi, kheer.\n\nHaryana is a landlocked state in northern India. It is between 27°39' to 30°35' N latitude and between 74°28' and 77°36' E longitude. The total geographical area of the state is 4.42 m ha, which is 1.4% of the geographical area of the country. The altitude of Haryana varies between 700 and 3600 ft (200 metres to 1200 metres) above sea level. Haryana has only 4% (compared to national 21.85%) area under forests. Karoh Peak, a tall mountain peak in the Sivalik Hills range of the greater Himalayas range located near Morni Hills area of Panchkula district, is highest point in Haryana.\n\nHaryana has four main geographical features.\n\nThe Yamuna, tributary of Ganges, flows along the state's eastern boundary.\n\nNorthern Haryana has several north-east to south-west flowing rivers originating from the Sivalik Hills of Himalayas, such as Ghaggar-Hakra (palaeochannel of vedic Sarasvati river), Chautang (paleochannel of vedic Drishadvati river, tributary of Ghagghar), Tangri river (tributary of Ghagghar), Kaushalya river (tributary of Ghagghar), Markanda River (tributary of Ghagghar), Sarsuti, Dangri, Somb river. Haryana's main seasonal river, the Ghaggar-Hakra, known as Ghaggar before the Ottu barrage and as the Hakra downstream of the barrage, rises in the outer Himalayas, between the Yamuna and the Satluj and enters the state near Pinjore in the Panchkula district, passes through Ambala and Sirsa, it reaches Bikaner in Rajasthan and runs for before disappearing into the deserts of Rajasthan. The seasonal Markanda River, known as the \"Aruna\" in ancient times, originates from the lower Shivalik Hills and enters Haryana west of Ambala, and swells into a raging torrent during monsoon is notorious for its devastating power, carries its surplus water on to the \"Sanisa Lake\" where the \"Markanda\" joins the \"Sarasuti\" and later the \"Ghaggar\".\n\nSouthern Haryana has several south-east to north-west flowing seasonal rivulets originating from the Aravalli Range in and around the hills in Mewat region, including Sahibi River (called Najafgarh drain in Delhi), Dohan river (tributary of Sahibi, originates at Mandoli village near Neem Ka Thana in Jhunjhunu district of Rajasthan and then disappears in Mahendragarh district), Krishnavati river (former tributary of Sahibi river, originates near Dariba and disappears in Mahendragarh district much before reaching Sahibi river) and Indori river (longest tributary of Sahibi River, originates in Sikar district of Rajasthan and flows to Rewari district of Haryana), these once were tributaries of the Drishadwati/Saraswati river.\n\nMajor canals are Western Yamuna Canal,\n\nMajor dams are Kaushalya Dam in Panchkula district, Hathnikund Barrage and Tajewala Barrage on Yamuna in Yamunanagar district, Pathrala barrage on Somb river in Yamunanagar district, ancient Anagpur Dam near Surajkund in Faridabad district, and Ottu barrage on Ghaggar-Hakra River in Sirsa district.\n\nMajor lakes are Dighal Wetland, Basai Wetland, Badkhal Lake in Faridabad, holy Brahma Sarovar and Sannihit Sarovar in Kurukshetra, Blue Bird Lake in Hisar, Damdama Lake at Sohna in Gurgram district, Hathni Kund in Yamunanagar district, Karna Lake at Karnal, ancient Surajkund in Faridabad, and Tilyar Lake in Rohtak.\n\nThe \"Haryana State Waterbody Management Board\" is responsible for rejuvenation of 14,000 Johads of Haryana and up to 60 lakes in National Capital Region falling within the Haryana state.\n\nOnly hot spring of Haryana is the Sohna Sulphur Hot Spring at Sohna in Gurugram district. Tosham Hill range has several sacred sulphur pond of religious significance that are revered for the healing impact of sulfur, such as \"Pandu Teerth Kund\", \"Surya Kund\", \"Kukkar Kund\", \"Gyarasia Kund\" or \"Vyas Kund\".\n\nSeasonal waterfalls include Tikkar Taal twin lakes at Morni hiills, Dhosi Hill in Mahendragarh district and Pali village on outskirts of Faridabad.\n\nHaryana is extremely hot in summer at around and mild in winter. The hottest months are May and June and the coldest December and January. The climate is arid to semi-arid with average rainfall of 354.5 mm. Around 29% of rainfall is received during the months from July to September, and the remaining rainfall is received during the period from December to February.\n\nForest Cover in the state in 2013 was 3.59% (1586 km) and the Tree Cover in the state was 2.90% (1282 km), giving a total forest and tree Cover of 6.49%. In 2016-17, 18,412 hectares were brought under tree cover by planting 14.1 million seedlings. Thorny, dry, deciduous forest and thorny shrubs can be found all over the state. During the monsoon, a carpet of grass covers the hills. Mulberry, eucalyptus, pine, kikar, shisham and babul are some of the trees found here. The species of fauna found in the state of Haryana include black buck, nilgai, panther, fox, mongoose, jackal and wild dog. More than 450 species of birds are found here.\n\nHaryana has two national parks, eight wildlife sanctuaries, two wildlife conservation areas, four animal and bird breeding centers, one deer park and three zoos, all of which are managed by the Haryana Forest Department of the Government of Haryana.\n\nHaryana Environment Protection Council is the advisory committee and |Department of Environment, Haryana]] is the department responsible for administration of environment. Areas of Haryana surrounding Delhi NCR are most polluted. During smog of November 2017, Air quality index of Gurugram and Faridabad showed that the density of Fine particulates (2.5 PM diameter) was an average of 400 PM and monthly average of Haryana was 60 PM. Other sources of pollution are exhaust gases from old vehicles, stone crushers and brick kiln. Haryana has 75 lakh (7,500,000) old vehicles, of which 40% are old more polluting vehicles, besides 500,000 new vehicles are added every year. Other majorly polluted cities are Bhiwani, Bahadurgarh, Dharuhera, Hisar and Yamunanagar.\n\nThe state is divided into 6 revenue divisions, 5 Police Ranges and 3 Police Commissionerates (c. January 2017). Six revenue divisions are: Ambala, Rohtak, Gurgaon, Hisar, Karnal and Faridabad. Haryana has 10 municipal corporations (Gurugram, Faridabad, Ambala, Panchkula, Yamunanagar, Rohtak, Hisar, Panipat, Karnal and Sonepat), 18 municipal councils and 52 municipalities (c. Jan 2018).\n\nWithin these there are 22 districts, 72 sub-divisions, 93 tehsils, 50 sub-tehsils, 140 blocks, 154 cities and towns, 6,841 villages, 6212 villages panchayats and numerous smaller dhanis.\n\nHaryana Police force is the law enforcement agency of Haryana. Five Police Ranges are Ambala, Hissar, Karnal, Rewari and Rohtak. Three Police Commissionerates are Faridabad, Gurgaon and Panchkula. Cybercrime investigation cell is based in Gurgaon's Sector 51.\n\nThe highest judicial authority in the state is the Punjab and Haryana High Court, with next higher right of appeal to Supreme Court of India. Haryana uses e-filing facility.\n\nThe Common Service Centres (CSCs) have been upgraded in all districts to offer hundreds of e-services to citizens, including application of new water connection, sewer connection, electricity bill collection, ration card member registration, result of HBSE, admit cards for board examinations, online admission form for government colleges, long route booking of buses, admission forms for Kurukshetra University and HUDA plots status inquiry. Haryana has become the first state to implement Aadhaar-enabled birth registration in all the districts. Thousands of all traditional offline state and central government services are also available 24/7 online through single unified UMANG app and portal as part of Digital India initiative.\n\nHaryana's 14th placed 12.96% 2012-17 CAGR estimated 2017-18 GSDP of US$95 billion is split in to 52% services, 30% industries and 18% agriculture.\n\nServices sector is split across 45% in real estate and financial & professional services, 26% trade and hospitality, 15% state and central govt employees, and 14% transport and logistics & warehousing. In IT services, Gurugram ranks number 1 in India in growth rate and existing technology infrastructure, and number 2 in startup ecosystem, innovation and livability (Nov 2016).\n\nIndustries sector is split across 69% manufacturing, 28% construction, 2% utilities and 1% mining. In industrial manufacturing, Haryana produces India's 67% of passenger cars, 60% of motorcycles, 50% of tractors and 50% of the refrigerators.\n\nServices and industrial sectors are boosted by 7 operational SEZs and additional 23 formally approved SEZs (20 already notified and 3 in-principal approval) that are mostly spread along the Delhi–Mumbai Industrial Corridor, Amritsar Delhi Kolkata Industrial Corridor and Delhi Western Peripheral Expressway in NCR).\n\nAgriculture sector is split across 93% crops and livestock, 4% commercial forestry and logging, and 2% fisheries. Agriculture sector of Haryana, with only less than 1.4% area of India, contributes 15% food grains to the central food security public distribution system, and 7% of total national agricultural exports including 60% of total national Basmati rice export.\n\nHaryana is traditionally an agrarian society of zamindars (owner-cultivator farmers). The Green Revolution in Haryana of 1960s combined with completion of Bhakra Dam in 1963 and Western Yamuna Command Network canal system in 1970s resulted in the significantly increased food grain production.\n\nIn 2015-2016, Haryana produced the following principal crops: 13,352,000 tonne wheat, 4,145,000 tonne rice, 7,169,000 tonne sugarcane, 993,000 tonne cotton and 855,000 tonne oilseeds (mustard seed, sunflower, etc.).\n\nVegetable production was: Potato 853,806 tonnes, Onion 705,795 tonnes, Tomato 675,384 tonnes, Cauliflower 578,953 tonnes, Leafy Vegetables 370,646 tonnes, Brinjal 331,169 tonnes, guard 307,793 tonnes, Peas 111,081 tonnes and others 269,993 tonnes.\n\nFruits production was: Citrus 301,764 tonnes, Guava 152,184 tonnes, Mango 89,965 tonnes, Chikoo 16,022 tonnes, Aonla 12,056 tonnes and other fruits 25,848 tonnes.\n\nSpices production was: Garlic 40,497 tonnes, Fenugreek 9,348 tonnes, Ginger 4,304 tonnes and others 840 tonnes.\n\nCut flowers production was: Marigold 61,830 tonnes, Gladiolus 24,486,200 lakh, Rose 18,611,600 lakh and other 6,913,000 lakh.\n\nMedicinal plants production was: Aloe vera 1403 tonnes and Stevia 13 tonnes.\n\nHaryana is well known for its high-yield Murrah buffalo. Other breeds of cattle native to Haryana are Haryanvi, Mewati, Sahiwal and Nili-Ravi.\n\nTo support its agrarian economy, both central government (Central Institute for Research on Buffaloes, Central Sheep Breeding Farm, National Research Centre on Equines, Central Institute of Fisheries, National Dairy Research Institute, Indian Institute of Wheat and Barley Research and National Bureau of Animal Genetic Resources) and state government (CCS HAU, LUVAS, Government Livestock Farm, Regional Fodder Station and Northern Region Farm Machinery Training and Testing Institute) have opened several institutes for research and education.\n\n\nHaryana State has always given high priority to the expansion of electricity infrastructure, as it is one of the most important inputs for the development of the state. Haryana was the first state in the country to achieve 100% rural electrification in 1970 as well as the first in the country to link all villages with all-weather roads and provide safe drinking water facilities throughout the state.\n\nPower in the state are:\n\nHaryana has a total road length of , including 29 national highways, state highways, Major District Roads (MDR) and Other District Roads (ODR) (c. December 2017). A fleet of 3,864 Haryana Roadways buses covers a distance of 1.15 million km per day, and it was the first state in the country to introduce luxury video coaches.\n\nAncient Delhi Multan Road and Grand Trunk Road, South Asia's oldest and longest major roads, pass through Haryana. GT Road passes through the districts of Sonipat, Panipat, Karnal, Kurukshetra and Ambala in north Haryana where it enters Delhi and subsequently the industrial town of Faridabad on its way. The Kundli-Manesar-Palwal Expressway(KMP) will provide a high-speed link to northern Haryana with its southern districts such as Sonipat, Gurgaon, and Faridabad.\n\nThe Delhi-Agra Expressway (NH-2) that passes through Faridabad is being widened to six lanes from current four lanes. It will further boost Faridabad's connectivity with Delhi.\n\nRail network in Haryana is covered by 5 rail divisions under 3 rail zones. Diamond Quadrilateral High-speed rail network, Eastern Dedicated Freight Corridor (72 km) and Western Dedicated Freight Corridor (177 km) pass through Haryana.\n\nBikaner railway division of North Western Railway zone manages rail network in western and southern Haryana covering Bhatinda-Dabwali-Hanumangarh line, Rewari-Bhiwani-Hisar-Bathinda line, Hisar-Sadulpur line and Rewari-Loharu-Sadulpur line. Jaipur railway division of North Western Railway zone manages rail network in south-west Haryana covering Rewari-Reengas-Jaipur line, Delhi-Alwar-Jaipur line and Loharu-Sikar line.\n\nDelhi railway division of Northern Railway zone manages rail network in north and east and central Haryana covering Delhi-Panipat-Ambala line, Delhi-Rohtak-Tohana line, Rewari–Rohtak line, Jind-Sonepat line and Delhi-Rewari line. Agra railway division of North Central Railway zone manages another very small part of network in south-east Haryana covering Palwal-Mathura line only.\n\nAmbala railway division of Northern Railway zone manages small part of rail network in north-east Haryana covering Ambala-Yamunanagar line, Ambala-Kurukshetra line and UNESCO World Heritage Kalka–Shimla Railway.\n\nDelhi Metro connects the national capital Delhi with NCR cities such as Faridabad, Gurugram and Bahadurgarh. Faridabad has the longest metro network in the NCR Region consisting of 9 stations and track length being 14 km.\n\nThe Haryana and Delhi governments have constructed the international standard Delhi Faridabad Skyway, the first of its kind in North India, to connect Delhi and Faridabad.\n\nHaryana has a statewide network of telecommunication facilities. Haryana Government has its own statewide area network by which all government offices of 22 districts and 126 blocks across the state are connected with each other thus making it the first SWAN of the country. Bharat Sanchar Nigam Limited and most of the leading private sector players (such as Reliance Infocom, Tata Teleservices, Bharti Telecom, Idea Vodafone Essar, Aircel, Uninor and Videocon) have operations in the state. Two biggest cities of Haryana, Faridabad and Gurgaon which are part of National Capital Region come under the local Delhi Mobile Telecommunication System. The rest of the cities of Haryana comes under Haryana Telecommunication System.\n\nElectronic media channels include, MTV, 9XM, Star Group, SET Max, News Time, NDTV 24x7 and Zee Group. The radio stations include All India Radio and other FM stations.\n\nThe major newspapers of Haryana include \"Dainik Bhaskar\", \"Punjab Kesari\", \"Jag Bani\", \"Dainik Jagran\", \"The Tribune\", \"Amar Ujala\", \"Hindustan Times\", \"Dainik Tribune\", \"The Times of India\" and \"Hari-Bhumi\".\n\nThe Total Fertility Rate of Haryana is 2.3. The Infant Mortality Rate is 41 (SRS 2013) and Maternal Mortality Ratio is 146 (SRS 2010–2012).\n\nLiteracy rate in Haryana has seen an upward trend and is 76.64 percent as per 2011 population census. Male literacy stands at 85.38 percent, while female literacy is at 66.67 percent. In 2001, the literacy rate in Haryana stood at 67.91 percent of which male and female were 78.49 percent and 55.73 percent literate respectively. , Gurgaon city had the highest literacy rate in Haryana at 86.30% followed by Panchkula at 81.9 per cent and Ambala at 81.7 percent. In terms of districts, Rewari had the highest literacy rate in Haryana at 74%, higher than the national average of 59.5%: male literacy was 79%, and female 67%.\n\nHaryana Board of School Education, established in September 1969 and shifted to Bhiwani in 1981, conducts public examinations at middle, matriculation, and senior secondary levels twice a year. Over seven lakh candidates attend annual examinations in February and March; 150,000 attend supplementary examinations each November. The Board also conducts examinations for Haryana Open School at senior and senior secondary levels twice a year. The Haryana government provides free education to women up to the bachelor's degree level.\n\nIn 2015-2016, there were nearly 20,000 schools, including 10,100 state government schools (36 Aarohi Schools, 11 Kasturba Gandhi Balika Vidyalayas, 21 Model Sanskriti Schools, 8744 government primary school, 3386 government middle school, 1284 government high school and 1967 government senior secondary schools), 7,635 private schools (200 aided, 6612 recognized unaided, and 821 unrecognied unaided private schools.)and several hundred other central government and private schools such as Kendriya Vidyalaya, Indian Army Public Schools, Jawahar Navodaya Vidyalaya and DAV schools affiliated to central government's CBSE and ICSE school boards.\n\nHaryana has 29 universities and 299 colleges, including 115 government colleges, 88 govt-aided colleges and 96 self-finance colleges (c. Jan 2018). Hisar has three universities: Chaudhary Charan Singh Haryana Agricultural University - Asia's largest agricultural university, Guru Jambheshwar University of Science and Technology, Lala Lajpat Rai University of Veterinary & Animal Sciences); several national agricultural and veterinary research centres (National Research Centre on Equines), Central Sheep Breeding Farm, National Institute on Pig Breeding and Research, Northern Region Farm Machinery Training and Testing Institute and Central Institute for Research on Buffaloes (CIRB); and more than 20 colleges including Maharaja Agrasen Medical College, Agroha.\n\nUnion Minister Ravi Shankar Prasad announced on 27 February 2016 that National Institute of Electronics and Information Technology (NIELIT) would be set up in Kurukshetra to provide computer training to youth and a Software Technology Park of India (STPI) would be set up in Panchkula's existing HSIIDC IT Park in Sector 23. Hindi and English are compulsory languages in schools whereas Punjabi, Sanskrit and Urdu are chosen as optional languages.\n\nIn the 2010 Commonwealth Games at Delhi, 22 out of 38 gold medals that India won came from Haryana. During the 33rd National Games held in Assam in 2007, Haryana stood first in the nation with a medal tally of 80, including 30 gold, 22 silver and 28 bronze medals.\n\nThe 1983 World-Cup-winning captain Kapil Dev is from Haryana. Nahar Singh Stadium was built in Faridabad in the year 1981 for international cricket. This ground has the capacity to hold around 25,000 people as spectators. Tejli Sports Complex is an Ultra-Modern sports complex in Yamuna Nagar. Tau Devi Lal Stadium in Gurgaon is a multi-sport complex.\n\nChief Minister of Haryana Manohar Lal Khattar announced the \"Haryana Sports and Physical Fitness Policy\", a policy to support 26 Olympic sports, on 12 January 2015 with the words \"We will develop Haryana as the sports hub of the country.\"\n\nHaryana is home to Haryana Gold, one of India's eight professional basketball teams which compete in the country's UBA Pro Basketball League.\n\n\n\n"}
{"id": "14190", "url": "https://en.wikipedia.org/wiki?curid=14190", "title": "Himachal Pradesh", "text": "Himachal Pradesh\n\nHimachal Pradesh (; literally \"snow-laden province\") is a state in the northern part of India. Situated in the Western Himalayas, it is bordered by states of Jammu and Kashmir on the north, Punjab on the west, Haryana on the southwest, Uttarakhand on the southeast, and Tibet on the east. At its southernmost point, it also touches the state of Uttar Pradesh. The state's name was coined from the Sanskrit—\"Him\" means 'snow' and \"achal\" means 'land' or 'abode'—by acharya Diwakar Datt Sharma, one of the state's eminent Sanskrit scholars.\n\nThe predominantly mountainous region comprising the present day Himachal Pradesh has been inhabited since pre-historic times having witnessed multiple waves of migration from other areas. Through it's history, the region was mostly ruled by local kingdoms some of which accepted suzerainty of larger empires. Prior to India's independence from British, Himachal comprised the hilly regions of Punjab Province of British India. After independence, many of the hilly territories were organized as the Chief Commisioner's province of Himachal Pradesh which later became a union territory. In 1966, hilly areas of neighboring Punjab state were merged into Himachal and it was ultimately granted full statehood in 1971.\n\nHimachal Pradesh is spread across valleys with many perennial rivers flowing through them. Almost 90% of the state's population lives in rural areas. Agriculture, horticulture, hydropower and tourism are important constituents of the state's economy. The hilly state is almost universally electrified with 99.5% of the households having electricity as of 2016. The state was declared India's second open-defecation free state in 2016. According to a survey of CMS - India Corruption Study 2017, Himachal Pradesh is India's least corrupt state.\n\nTribes such as the Koli, Hali, Dagi, Dhaugri, Dasa, Khasa, Kanaura, and Kirat inhabited the region from the prehistoric era. The foothills of the modern state of Himachal Pradesh were inhabited by people from the Indus valley civilization which flourished between 2250 and 1750 B.C. The Kols or Mundas are believed to be the original migrants to the hills of present day Himachal Pradesh followed by the Bhotas and Kiratas.\n\nDuring the Vedic period, several small republics known as \"Janapada\" existed which were later conquered by the Gupta Empire. After a brief period of supremacy by King Harshavardhana, the region was divided into several local powers headed by chieftains, including some Rajput principalities. These kingdoms enjoyed a large degree of independence and were invaded by Delhi Sultanate a number of times. Mahmud Ghaznavi conquered Kangra at the beginning of the 10th century. Timur and Sikander Lodi also marched through the lower hills of the state and captured a number of forts and fought many battles. Several hill states acknowledged Mughal suzerainty and paid regular tribute to the Mughals.\nThe Kingdom of Gorkha conquered many kingdoms and came to power in Nepal in 1768. They consolidated their military power and began to expand their territory. Gradually, the Kingdom of Nepal annexed Sirmour and Shimla. Under the leadership of Amar Singh Thapa, the Nepali army laid siege to Kangra. They managed to defeat Sansar Chand Katoch, the ruler of Kangra, in 1806 with the help of many provincial chiefs. However, the Nepali army could not capture Kangra fort which came under Maharaja Ranjeet Singh in 1809. After the defeat, they began to expand towards the south of the state. However, Raja Ram Singh, Raja of Siba State, captured the fort of Siba from the remnants of Lahore Darbar in Samvat 1846, during the First Anglo-Sikh War.\n\nThey came into direct conflict with the British along the \"tarai\" belt after which the British expelled them from the provinces of the Satluj. The British gradually emerged as the paramount power in the region. In the revolt of 1857, or first Indian war of independence, arising from a number of grievances against the British, the people of the hill states were not as politically active as were those in other parts of the country. They and their rulers, with the exception of Bushahr, remained more or less inactive. Some, including the rulers of Chamba, Bilaspur, Bhagal and Dhami, rendered help to the British government during the revolt.\nThe British territories came under the British Crown after Queen Victoria's proclamation of 1858. The states of Chamba, Mandi and Bilaspur made good progress in many fields during the British rule. During World War I, virtually all rulers of the hill states remained loyal and contributed to the British war effort, both in the form of men and materials. Among these were the states of Kangra, Jaswan, Datarpur, Guler, Rajgarh, Nurpur, Chamba, Suket, Mandi, and Bilaspur.\n\nAfter independence, the Chief Commissioner's Province of Himachal Pradesh was organized on 15 April 1948 as a result of integration of 28 petty princely states (including feudal princes and \"zaildars\") in the promontories of the western Himalayas. These were known as the Simla Hills States and four Punjab southern hill states under the Himachal Pradesh (Administration) Order, 1948 under Sections 3 and 4 of the Extra-Provincial Jurisdiction Act, 1947 (later renamed as the Foreign Jurisdiction Act, 1947 vide A.O. of 1950). The State of Bilaspur was merged into Himachal Pradesh on 1 July 1954 by the Himachal Pradesh and Bilaspur (New State) Act, 1954.\n\nHimachal became a Part 'C' state on 26 January 1950 with the implementation of the Constitution of India and the Lieutenant Governor was appointed. The Legislative Assembly was elected in 1952. Himachal Pradesh became a union territory on 1 November 1956. Some areas of Punjab State— namely Simla, Kangra, Kullu and Lahul and Spiti Districts, Nalagarh tehsil of Ambala District, Lohara, Amb and Una kanungo circles, some area of Santokhgarh kanungo circle and some other specified area of Una tehsil of Hoshiarpur District, besides some parts of Dhar Kalan Kanungo circle of Pathankot tehsil of Gurdaspur District—were merged with Himachal Pradesh on 1 November 1966 on enactment by Parliament of Punjab Reorganisation Act, 1966. On 18 December 1970, the State of Himachal Pradesh Act was passed by Parliament, and the new state came into being on 25 January 1971. Himachal became the 18th state of the Indian Union with Dr. Yashwant Singh Parmar as its first chief minister.\n\nHimachal is in the western Himalayas. Covering an area of , it is a mountainous state. Most of the state lies on the foothills of the Dhauladhar Range. At 6,816 m Reo Purgyil is the highest mountain peak in the state of Himachal Pradesh.\n\nThe drainage system of Himachal is composed both of rivers and glaciers. Himalayan rivers criss-cross the entire mountain chain.\nHimachal Pradesh provides water to both the Indus and Ganges basins. The drainage systems of the region are the Chandra Bhaga or the Chenab, the Ravi, the Beas, the Sutlej, and the Yamuna. These rivers are perennial and are fed by snow and rainfall. They are protected by an extensive cover of natural vegetation.\n\nDue to extreme variation in elevation, great variation occurs in the climatic conditions of Himachal. The climate varies from hot and subhumid tropical in the southern tracts to, with more elevation, cold, alpine, and glacial in the northern and eastern mountain ranges. The state's winter capital, Dharamsala receives very heavy rainfall, while areas like Lahaul and Spiti are cold and almost rainless. Broadly, Himachal experiences three seasons: summer, winter, and rainy season. Summer lasts from mid-April till the end of June and most parts become very hot (except in the alpine zone which experiences a mild summer) with the average temperature ranging from . Winter lasts from late November till mid March. Snowfall is common in alpine tracts (generally above i.e. in the higher and trans-Himalayan region).\n\nHimachal Pradesh is one of the states that lies in the Indian Himalayan Region (IHR), one of the richest reservoirs of biological diversity in the world. The IHR is currently undergoing large scale irrational extraction of wild, medicinal herbs, thus endangering many of its high-value gene stock. To address this, a workshop on ‘Endangered Medicinal Plant Species in Himachal Pradesh’ was held in 2002 and the conference was attended by forty experts from diverse disciplines.\nAccording to 2003 Forest Survey of India report, legally defined forest areas constitute 66.52% of the area of Himachal Pradesh. Vegetation in the state is dictated by elevation and precipitation. The state endows with a high diversity of medicinal and aromatic plants. Lahaul-Spiti region of the state, being a cold desert, supports unique plants of medicinal value including \"Ferula jaeschkeana\", \"Hyoscyamus niger\", \"Lancea tibetica\", and \"Saussurea bracteata\".\n\nHimachal is also said to be the fruit bowl of the country, with orchards being widespread. Meadows and pastures are also seen clinging to steep slopes. After the winter season, the hillsides and orchards bloom with wild flowers, while gladiolas, carnations, marigolds, roses, chrysanthemums, tulips and lilies are carefully cultivated. Himachal Pradesh Horticultural Produce Marketing and Processing Corporation Ltd. (HPMC) is a state body that markets fresh and processed fruits.\n\nHimachal Pradesh has around 463 bird 77 mammalian, 44 reptile and 80 fish species. Great Himalayan National Park, a UNESCO World Heritage Site and Pin Valley National Park are the national Parks located in the state. The state also has 30 wildlife sanctuaries and 3 conservation reserves.\n\nThe Legislative Assembly of Himachal Pradesh has no pre-Constitution history. The State itself is a post-Independence creation. It came into being as a centrally administered territory on 15 April 1948 from the integration of thirty erstwhile princely states.\n\nHimachal Pradesh is governed through a parliamentary system of representative democracy, a feature the state shares with other Indian states. Universal suffrage is granted to residents. The legislature consists of elected members and special office bearers such as the Speaker and the Deputy Speaker who are elected by the members. Assembly meetings are presided over by the Speaker or the Deputy Speaker in the Speaker's absence. The judiciary is composed of the Himachal Pradesh High Court and a system of lower courts. Executive authority is vested in the Council of Ministers headed by the , although the titular head of government is the Governor. The Governor is the head of state appointed by the President of India. The leader of the party or coalition with a majority in the Legislative Assembly is appointed as the Chief Minister by the Governor, and the Council of Ministers are appointed by the Governor on the advice of the Chief Minister. The Council of Ministers reports to the Legislative Assembly. The Assembly is unicameral with 68 Members of the Legislative Assembly (MLA). Terms of office run for 5 years, unless the Assembly is dissolved prior to the completion of the term. Auxiliary authorities known as \"panchayats\", for which local body elections are regularly held, govern local affairs.\n\nIn the assembly elections held in November 2017, the BJP secured an absolute majority. The BJP won 44 of the 68 seats while the Congress won only 21 of the 68 seats. Jai Ram Thakur was sworn-in as Himachal Pradesh's Chief Minister for the first time in Shimla on 27 December 2017.\n\nThe state of Himachal Pradesh is divided into 12 districts which are grouped into three divisions, Shimla, Kangra and Mandi. The districts are further divided into 69 subdivisions, 78 blocks and 145 Tehsils.\n\nThe era of planning in Himachal Pradesh started in 1951 along with the rest of India with the implementation of the first five-year plan. The First Plan allocated 52.7 million to Himachal Pradesh. More than 50% of this expenditure was incurred on transport and communication; while the power sector got a share of just 4.6%, though it had steadily increased to 7% by the Third Plan. Expenditure on agriculture and allied activities increased from 14.4% in the First Plan to 32% in the Third Plan, showing a progressive decline afterwards from 24% in the Fourth Plan to less than 10% in the Tenth Plan. Expenditure on energy sector was 24.2% of the total in the Tenth Plan. \n\nThe total GDP for 2005-06 was estimated at 254 billion as against 230 billion in the year 2004–05, showing an increase of 10.5%. The GDP for fiscal 2015-16 was estimated at 1.110 trillion, which increased to 1.247 trillion in 2016-17, recording a growth of 6.8%. The per capita income increased from 130,067 in 2015-16 to 147,277 in 2016-17. The state government's advance estimates for fiscal 2017-18 stated the total GDP and per capita income as 1.359 trillion and 158,462 respectively. As of 2018, Himachal is the 22nd-largest state economy in India with in gross domestic product and has the 13th-highest per capita income () among the states and union territories of India.\n\nHimachal Pradesh also ranks as the second best performing state in the country on human development indicators after Kerala. One of the Indian government's key initiatives to tackle unemployment is the National Rural Employment Guarantee Act (NREGA). The participation of women in the NREGA has been observed to vary across different regions of the nation. As of the year 2009-2010, Himachal Pradesh joined the category of high female participation, recording a 46% share of NREGS (National Rural Employment Guarantee Scheme) work days to women. This was a drastic increase from the 13% that was recorded in 2006-2007.\n\nAgriculture contributes about 9.4% to the net state domestic product. It is the main source of income and employment in Himachal. About 90% of the population in Himachal depends directly upon agriculture, which provides direct employment to 62% of total workers of state. The main cereals grown include wheat, maize, rice and barley with major cropping systems being maize-wheat, rice-wheat and maize-potato-wheat. Pulses, fruits, vegetables and oilseeds are among the other crops grown in the state. Land husbandry initiatives such as the Mid-Himalayan Watershed Development Project, which includes the Himachal Pradesh Reforestation Project (HPRP), the world's largest clean development mechanism (CDM) undertaking, have improved agricultural yields and productivity, and raised rural household incomes.\n\nApple is the principal cash crop of the state grown principally in the districts of Shimla, Kinnaur, Kullu, Mandi, Chamba and some parts of Sirmaur and Lahaul-Spiti with an average annual production of 5 lakh tonnes and per hectare production of 8 to 10 tonnes. The apple cultivation constitute 49 per cent of the total area under fruit crops and 85% of total fruit production in the state with an estimated economy of 3500 crore. Apples from Himachal are exported to other Indian states and even other countries. In 2011-12, the total area under apple cultivation was 1.04 lakh hectares, increased from 90,347 hectares in 2000-01. According to the provisional estimates of Ministry of Agriculture & Farmers Welfare, the annual apple production in Himachal for fiscal 2015-16 stood at 7.53 lakh tonnes, making it India's second largest apple producing state after Jammu and Kashmir.\n\nHydropower is one of the major sources of income generation for the state. The state has an abundance of hydropower resources because of the presence of various perennial rivers. Many high capacity hydropower plants have been constructed which produce surplus electricity that is sold to other states, such as Delhi, Punjab and West Bengal. The income generated from exporting the electricity to other states is being provided as subsidy to the consumers in the state. The rich hydropower resources of Himachal have resulted in the state becoming almost universally electrified with around 94.8% houses receiving electricity as of 2001, as compared to the national average of 55.9%. Himachal's hydro-electric power production is however yet to be fully utilized. The identified Hydroelectric Potential for the state is 27,436 MW in five river basins and the annual hydroelectricity production in 2016 was 10,351 MW.\n\nTourism in Himachal Pradesh is a major contributor to the state's economy and growth. The mountainous state with its Himalayan landscapes attracts tourists from all over the world. Hill stations like Shimla, Manali, Dharamshala, Dalhousie, Chamba, Khajjiar, Kullu and Kasauli are popular destinations for both domestic and foreign tourists. The state also has many important Hindu pilgrimage sites with prominent temples like Naina Devi Temple, Bajreshwari Mata Temple, Jwala Ji Temple, Chintpurni, Chamunda Devi Temple, Baijnath Temple, Bhimakali Temple, Bijli Mahadev and Jakhoo Temple. Manimahesh Lake situated in the Bharmour region of Chamba district is the venue of an annual Hindu pilgrimage trek held in the month of August which attracts lakhs of devotees. The state is also referred to as \"\"Dev Bhoomi\"\" (literally meaning \"Abode of Gods\") due to its mention as such in ancient Hindu texts and occurrence of a large number of historical temples in the state. It is also called the Land of the Gods on account of the Hindu belief that deities like Lord Shiva considered the Himalayas their abode, and much of the state is located among the Himalayan mountains. Although modern pop-literature writers online have often also referred to Uttarakhand as the land of the gods because it also contains Himalayan mountains, officially it is Himachal Pradesh that has been considered the land of the gods since before the state of Uttarakhand existed (the UK as it is abbreviated on license plates for automobiles in the state, and the state was founded in the year 2000). A tourism department board on the road when entering Himachal Pradesh from the state of Punjab states \"Welcome to the Land of the Gods.\"\nThe state is also known for its adventure tourism activities like ice skating in Shimla, paragliding in Bir Billing and Solang valley, rafting in Kullu, skiing in Manali, boating in Bilaspur and trekking, horse riding and fishing in different parts in the state. Shimla, the state's capital, is home to Asia's only natural ice skating rink. Spiti Valley in Lahaul & Spiti District situated at an altitude of over 3000 metres with its picturesque landscapes is an important destination for adventure seekers. The region also has some of the oldest Buddhist Monasteries in Asia.\n\nHimachal hosted the first Paragliding World Cup in India from 24 October to 31 October in 2015. Venue for paragliding world cup was Bir Billing, which is 70 km from the tourist town Macleod Ganj, located in the heart of Himachal in Kangra District. Bir Billing is the centre for aero sports in Himachal and considered as best for paragliding. Buddhist monasteries, trekking to tribal villages, mountain biking are other activities to do here.\n\nHimachal has three domestic airports in Kangra, Kullu and Shimla districts. The air routes connect the state with Delhi and Chandigarh.\n\nHimachal is known for its narrow-gauge railways. One is the Kalka-Shimla Railway, a UNESCO World Heritage Site, and another is the Pathankot-Jogindernagar line. The total length of these two tracks is . The Kalka-Shimla Railway passes through many tunnels, while the Pathankot–Jogindernagar meanders through a maze of hills and valleys. The state also has broad-gauge railway track, which connects Amb and Una (district headquarters of Una district) to Delhi. A survey is being conducted to extend this railway line to Hamirpur. The total route length of the operational railway network in the state is . Other proposed railways in the state are Dharamsala-Palampur, Baddi-Chandigarh and Bilaspur-Manali-Leh.\n\nRoads are the major mode of transport in the hilly terrains. The state has road network of , including eight National Highways (NH) that constitute and 19 State Highways with a total length of . Hamirpur district has the highest road density in the country. Some roads get closed during winter and monsoon seasons due to snow and landslides. The state-owned Himachal Road Transport Corporation with a fleet of over 3,100, operates bus services connecting important cities and towns with the villages within the state and also on various interstate routes. In addition, around 3,000 private buses run by various operators also ply in the state.\n\nHimachal Pradesh has a total population of 6,864,602 including 3,481,873 males and 3,382,729 females as per the final results of the Census of India 2011. This is only 0.57 per cent of India's total population, recording a growth of 12.81 per cent. The scheduled castes and scheduled tribes account for 25.19 per cent and 5.71 per cent of the population respectively. The sex ratio stood at 972 females per 1000 males, recording a marginal increase from 968 in 2001. The child sex ratio increased from 896 in 2001 to 909 in 2011. The total fertility rate (TFR) per woman in 2015 stood at 1.7, one of the lowest in India.\n\nIn the census, the state is placed 21st on the population chart, followed by Tripura at 22nd place. Kangra district was top ranked with a population strength of 1,507,223 (21.98%), Mandi district 999,518 (14.58%), Shimla district 813,384 (11.86%), Solan district 576,670 (8.41%), Sirmaur district 530,164 (7.73%), Una district 521,057 (7.60%), Chamba district 518,844 (7.57%), Hamirpur district 454,293 (6.63%), Kullu district 437,474 (6.38%), Bilaspur district 382,056 (5.57%), Kinnaur district 84,298 (1.23%) and Lahaul Spiti 31,528 (0.46%).\n\nThe life expectancy at birth in Himachal Pradesh increased significantly from 52.6 years in the period from 1970-75 (above the national average of 49.7 years) to 72.0 years for the period 2011-15 (above the national average of 68.3 years). The infant mortality rate stood at 40 in 2010, and the crude birth rate has declined from 37.3 in 1971 to 16.9 in 2010, below the national average of 26.5 in 1998. The crude death rate was 6.9 in 2010. Himachal Pradesh's literacy rate has almost doubled between 1981 and 2011 (see table to right). The state is one of the most literate states of India with a literacy rate of 83.78% as of 2011.\n\nHindi is the official language of Himachal Pradesh and is spoken by the majority of the population as a lingua franca. English is given the status of an additional official language. Most of the languages spoken natively belong to the group of the Himachali languages. According to the 2001 Census of India, the languages spoken in the state in descending order of native speakers are Hindi, spoken by 89.01% of the population (including Himachali languages as dialects of Hindi); followed by Punjabi (5.99%), Nepali (1.16%) and Kinnauri (1.06%).\n\nHinduism is the major religion in Himachal Pradesh. More than 95% of the total population adheres to the Hindu faith, the distribution of which is evenly spread throughout the state. Himachal Pradesh has the highest proportion of Hindu population among all the states and union territories in India.\n\nOther religions that form a small percentage are Islam, Sikhism and Buddhism. Muslims are mainly concentrated in Sirmaur, Chamba, Una and Solan districts where they form 2.53-6.27% of the population. Sikhs mostly live in towns and cities and constitute 1.16% of the state population. The Buddhists, who constitute 1.15%, are mainly natives and tribals from Lahaul and Spiti, where they form a majority of 62%, and Kinnaur, where they form 21.5%.\n\nHimachal Pradesh was one of the few states that had remained largely untouched by external customs, largely due to its difficult terrain. With remarkable economic and social advancements, the state has changed very rapidly. Himachal Pradesh is a multireligious, multicultural as well as a multilingual state like other Indian states. Western Pahari languages also known as Himachali languages are widely spoken in the state. Some of the most commonly spoken individual languages are Kangri, Mandeali, Kulvi, Chambeali, Bharmauri and Kinnauri. The Hindu communities residing in Himachal include the \"Brahmins\", \"Rajputs\", \"Kayasthas\", \"Sunars\", \"Kannets\", \"Rathis\" and \"Kolis\". The tribal population of the state consists mainly of \"Gaddis\", \"Gujjars\", \"Kanauras\", \"Pangwalas\", \"Bhots\", \"Swanglas\" and \"Lahaulas\".\n\nHimachal is well known for its handicrafts. The carpets, leather works, Kullu shawls, Kangra paintings, Chamba Rumals, stoles, embroidered grass footwear (\"Pullan chappal\"), silver jewelry, metal-ware, knitted woolen socks, \"Pattoo\", basketry of cane and bamboo (\"Wicker\" and \"Rattan\") and woodwork are among the notable ones. Of late, the demand for these handicrafts has increased within and outside the country. Himachali caps of various colour bands are also well-known art work of the local people, and are often treated as a symbol of the Himachali identity. The colour of the Himachali caps has been an indicator of political loyalties in the hill state for a long period of time with Congress party leaders like Virbhadra Singh always donning caps with green band and the rival BJP leader Prem Kumar Dhumal always wearing a cap with maroon band. The former has served six terms as the Chief Minister of the state while the latter is a two-time Chief Minister. Local music and dance also reflects the cultural identity of the state. Through their dance and music, the Himachali people entreat their gods during local festivals and other special occasions.\n\nApart from the fairs and festivals that are celebrated all over India, there are number of other fairs and festivals, including the temple fairs in nearly every region that are of great significance to Himachal Pradesh. The Kullu Dussehra festival is very famous all over India. The day to day cuisine of \"Himachalis\" is very similar to the rest of the north India with a significant influence of Punjabi and Tibetan cuisines. Lentils (\"Dāl\"), rice (\"Chāwal\" or \"Bhāț\"), vegetables (\"Sabzī\") and chapati (wheat flatbread) form the staple food of the local population. As compared to other states in north India, non-vegetarian food is more preferred and accepted in Himachal Pradesh, partly owing to difficulty in finding a variety of fresh vegetables on the hilly terrain of the state. Some of the local specialities of Himachali cuisine include \"Siddu\", \"Babru\", \"Khatta\", \"Mhanee\", \"Channa Madra\", \"Patrode\", \"Mah Ki Dal\", \"Chamba Style Fried Fish\", \"Kullu Trout\", \"Chha Gosht\", \"Pahadi Chicken\", \"Sepu Badi\", \"Auriya Kaddu\", \"Aloo Palda\", \"Pateer\", \"Makki Ki Roti and Sarson Ka Saag\", \"Chouck\", \"Bhagjery\" and \"Chutney\" of Til.\n\n\nAt the time of Independence, Himachal Pradesh had a literacy rate of 8% - one of the lowest in the country. By 2011, the literacy rate surged to over 82%, making Himachal one of the most literate states in the country. There are over 10,000 primary schools, 1,000 secondary schools and more than 1,300 high schools in the state. In meeting the constitutional obligation to make primary education compulsory, Himachal became the first state in India to make elementary education accessible to every child.\"\" Although gender bias in education levels is a prominent issue all over India, Himachal Pradesh is one of the exceptions. The state has a female literacy rate of around 76%. In addition, school enrollment and participation rates for girls are almost universal at the primary level. While higher levels of education do reflect a gender based disparity, Himachal is still significantly ahead of other states at bridging the gap. The Hamirpur District in particular stands out for high literacy rates across all metrics of measurement.\n\nThe state government has played an instrumental role in the rise of literacy in the state by spending a significant proportion of the state's GDP on education. During the first six five-year plans, most of the development expenditure in education sector was utilized in quantitative expansion, but after the seventh five-year-plan the state government switched emphasis on qualitative improvement and modernisation of education. In an effort to raise the number of teaching staff at primary schools they appointed over 1000 teacher aids through the Vidya Upasak Yojna in 2001. The Sarva Shiksha Abhiyan is another HP government initiative that not only aims for universal elementary education but also encourages communities to engage in the management of schools. The Rashtriya Madhayamic Shiksha Abhiyan launched in 2009, is a similar scheme but focuses on improving access to quality secondary education.\n\nThe standard of education in the state has reached a considerably high level as compared to other states in India with several reputed educational institutes for higher studies. The Indian Institute of Technology Mandi, Indian Institute of Management Sirmaur, Himachal Pradesh University in Shimla, Central University of Himachal Pradesh, Dharamsala, National Institute of Technology, Hamirpur, Indian Institute of Information Technology Una, Alakh Prakash Goyal University and Baddi University of Emerging Sciences and Technologies are some of the notable universities in the state. Indira Gandhi Medical College and Hospital in Shimla, Dr. Rajendra Prasad Government Medical College in Kangra, Rajiv Gandhi Government Post Graduate Ayurvedic College in Paprola and Homoeopathic Medical College & Hospital in Kumarhatti are the prominent medical institutes in the state. Besides these, there is a Government Dental College in Shimla which is the state's first recognised dental institute. The state government has also decided to start three major nursing colleges to develop the healthcare system of the state. CSK Himachal Pradesh Krishi Vishwavidyalya Palampur is one of the most renowned hill agriculture institutes in the world. Dr. Yashwant Singh Parmar University of Horticulture and Forestry has earned a unique distinction in India for imparting teaching, research and extension education in horticulture, forestry and allied disciplines. Further, state-run Jawaharlal Nehru Government Engineering College was inaugurated in 2006 at Sundernagar.\nHimachal Pradesh also hosts a campus of the prestigious fashion college, National Institute of Fashion Technology (NIFT) in Kangra.\n\nSource: \"Department of Information and Public Relations.\"\nCensus 2011-\n\nLargest District (km²)\n(1) Lahul and Spiti 13841\n(2) Chamba 6522\n(3) Kinnaur 6401\n(4) Kangra 5739\n(5) Kullu 5503\n\nHighest Percentage of Child Population\n(1) Chamba 13.55%\n(2) Sirmaur 13.14%\n(3) Solan 11.74%\n(4) Kullu 11.52%\n(5) Una 11.36%\n\nHighest Density\n(1) Hamirpur 407\n(2) Una 338\n(3) Bilaspur 327\n(4) Solan 300\n(5) Kangra 263\n\nTop Population Growth\n(1) Una 16.26%\n(2) Solan 15.93%\n(3) Sirmaur 15.54%\n(4) Kullu 14.76%\n(5) Kangra 12.77%\n\nHighest Literacy\n(1) Hamirpur 100%\n(2) Una 87.23%\n(3) Kangra 86.49%\n(4) Blaspur 85.87%\n(5) Solan 85.02%\n\nHighest Sex Ratio\n(1) Hamirpur 1050\n(2) Kangra 1012\n(3) Mandi 1007\n(4) Chamba 986\n(5) Bilaspur 981\n\n\n"}
{"id": "14192", "url": "https://en.wikipedia.org/wiki?curid=14192", "title": "Helene", "text": "Helene\n\nHelene or Hélène may refer to:\n\n\n"}
{"id": "14193", "url": "https://en.wikipedia.org/wiki?curid=14193", "title": "Hyperion", "text": "Hyperion\n\nHyperion may refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
