{"id": "19589", "url": "https://en.wikipedia.org/wiki?curid=19589", "title": "Minimax", "text": "Minimax\n\nMinimax (sometimes MinMax or MM) is a decision rule used in artificial intelligence, decision theory, game theory, statistics and philosophy for \"mini\"mizing the possible loss for a worst case (\"max\"imum loss) scenario. When dealing with gains, it is referred to as \"maximin\"—to maximize the minimum gain. Originally formulated for two-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.\n\nThe maximin value of a player is the highest value that the player can be sure to get without knowing the actions of the other players; equivalently, it is the lowest value the other players can force the player to receive when they know the player's action. Its formal definition is:\n\nWhere:\n\nCalculating the maximin value of a player is done in a worst-case approach: for each possible action of the player, we check all possible actions of the other players and determine the worst possible combination of actions—the one that gives player the smallest value. Then, we determine which action player can take in order to make sure that this smallest value is the highest possible.\n\nFor example, consider the following game for two players, where the first player (\"row player\") may choose any of three moves, labelled , , or , and the second player (\"column\" player) may choose either of two moves, or . The result of the combination of both moves is expressed in a payoff table:\n(where the first number in each cell is the pay-out of the row player and the second number is the pay-out of the column player).\n\nFor the sake of example, we consider only pure strategies. Check each player in turn:\n\nIf both players play their respective maximin strategies formula_9, the payoff vector is formula_10.\n\nThe minimax value of a player is the smallest value that the other players can force the player to receive, without knowing the player's actions; equivalently, it is the largest value the player can be sure to get when they \"know\" the actions of the other players. Its formal definition is:\n\nThe definition is very similar to that of the maximin value—only the order of the maximum and minimum operators is inverse. In the above example:\n\nFor every player , the maximin is at most the minimax:\nIntuitively, in minmax the maximization comes before the minimization, so player tries to maximize their value before knowing what the others will do; in maxmin the maximization comes after the minimization, so player is in a much better position—they maximize their value knowing what the others did.\n\nAnother way to understand the \"notation\" is by reading from right to left: when we write\nthe initial set of outcomes formula_16 depends on both formula_17 and formula_18. We first \"marginalize away\" formula_17 from formula_16, by maximizing over formula_17 (for every possible value of formula_18) to yield a set of marginal outcomes formula_23, which depends only on formula_18. We then minimize over formula_18 over these outcomes. (Conversely for maximin.)\n\nAlthough it is always the case that formula_26 and formula_27, the payoff vector resulting from both players playing their minimax strategies, formula_28 in the case of formula_29 or formula_30 in the case of formula_31, cannot similarly be ranked against the payoff vector formula_10 resulting from both players playing their maximin strategy.\n\nIn zero-sum games, the minimax solution is the same as the Nash equilibrium.\n\nIn the context of zero-sum games, the minimax theorem is equivalent to:\n\nFor every two-person, zero-sum game with finitely many strategies, there exists a value V and a mixed strategy for each player, such that\n\nEquivalently, Player 1's strategy guarantees them a payoff of V regardless of Player 2's strategy, and similarly Player 2 can guarantee themselves a payoff of −V. The name minimax arises because each player minimizes the maximum payoff possible for the other—since the game is zero-sum, they also minimize their own maximum loss (i.e. maximize their minimum payoff).\nSee also example of a game without a value.\n\nThe following example of a zero-sum game, where A and B make simultaneous moves, illustrates \"minimax\" solutions. Suppose each player has three choices and consider the payoff matrix for A displayed on the right. Assume the payoff matrix for B is the same matrix with the signs reversed (i.e. if the choices are A1 and B1 then B pays 3 to A). Then, the minimax choice for A is A2 since the worst possible result is then having to pay 1, while the simple minimax choice for B is B2 since the worst possible result is then no payment. However, this solution is not stable, since if B believes A will choose A2 then B will choose B1 to gain 1; then if A believes B will choose B1 then A will choose A1 to gain 3; and then B will choose B2; and eventually both players will realize the difficulty of making a choice. So a more stable strategy is needed.\n\nSome choices are \"dominated\" by others and can be eliminated: A will not choose A3 since either A1 or A2 will produce a better result, no matter what B chooses; B will not choose B3 since some mixtures of B1 and B2 will produce a better result, no matter what A chooses.\n\nA can avoid having to make an expected payment of more than 1∕3 by choosing A1 with probability 1∕6 and A2 with probability 5∕6: The expected payoff for A would be 3 × (1∕6) − 1 × (5∕6) = −1∕3 in case B chose B1 and −2 × (1∕6) + 0 × (5∕6) = −1/3 in case B chose B2. Similarly, B can ensure an expected gain of at least 1/3, no matter what A chooses, by using a randomized strategy of choosing B1 with probability 1∕3 and B2 with probability 2∕3. These mixed minimax strategies are now stable and cannot be improved.\n\nFrequently, in game theory, maximin is distinct from minimax. Minimax is used in zero-sum games to denote minimizing the opponent's maximum payoff. In a zero-sum game, this is identical to minimizing one's own maximum loss, and to maximizing one's own minimum gain.\n\n\"Maximin\" is a term commonly used for non-zero-sum games to describe the strategy which maximizes one's own minimum payoff. In non-zero-sum games, this is not generally the same as minimizing the opponent's maximum gain, nor the same as the Nash equilibrium strategy.\n\nThe minimax values are very important in the theory of repeated games. One of the central theorems in this theory, the folk theorem, relies on the minimax values.\n\nIn combinatorial game theory, there is a minimax algorithm for game solutions.\n\nA simple version of the minimax \"algorithm\", stated below, deals with games such as tic-tac-toe, where each player can win, lose, or draw.\nIf player A \"can\" win in one move, their best move is that winning move.\nIf player B knows that one move will lead to the situation where player A \"can\" win in one move, while another move will lead to the situation where player A can, at best, draw, then player B's best move is the one leading to a draw.\nLate in the game, it's easy to see what the \"best\" move is.\nThe Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning).\n\nA minimax algorithm is a recursive algorithm for choosing the next move in an n-player game, usually a two-player game. A value is associated with each position or state of the game. This value is computed by means of a position evaluation function and it indicates how good it would be for a player to reach that position. The player then makes the move that maximizes the minimum value of the position resulting from the opponent's possible following moves. If it is A<nowiki>'s</nowiki> turn to move, A gives a value to each of their legal moves.\n\nA possible allocation method consists in assigning a certain win for A as +1 and for B as −1. This leads to combinatorial game theory as developed by John Horton Conway. An alternative is using a rule that if the result of a move is an immediate win for A it is assigned positive infinity and, if it is an immediate win for B, negative infinity. The value to A of any other move is the minimum of the values resulting from each of B<nowiki>'s</nowiki> possible replies. For this reason, A is called the \"maximizing player\" and B is called the \"minimizing player\", hence the name \"minimax algorithm\". The above algorithm will assign a value of positive or negative infinity to any position since the value of every position will be the value of some final winning or losing position. Often this is generally only possible at the very end of complicated games such as chess or go, since it is not computationally feasible to look ahead as far as the completion of the game, except towards the end, and instead positions are given finite values as estimates of the degree of belief that they will lead to a win for one player or another.\n\nThis can be extended if we can supply a heuristic evaluation function which gives values to non-final game states without considering all possible following complete sequences. We can then limit the minimax algorithm to look only at a certain number of moves ahead. This number is called the \"look-ahead\", measured in \"plies\". For example, the chess computer Deep Blue (the first one to beat a reigning world champion, Garry Kasparov at that time) looked ahead at least 12 plies, then applied a heuristic evaluation function.\n\nThe algorithm can be thought of as exploring the nodes of a \"game tree\". The \"effective branching factor\" of the tree is the average number of children of each node (i.e., the average number of legal moves in a position). The number of nodes to be explored usually increases exponentially with the number of plies (it is less than exponential if evaluating forced moves or repeated positions). The number of nodes to be explored for the analysis of a game is therefore approximately the branching factor raised to the power of the number of plies. It is therefore impractical to completely analyze games such as chess using the minimax algorithm.\n\nThe performance of the naïve minimax algorithm may be improved dramatically, without affecting the result, by the use of alpha-beta pruning.\nOther heuristic pruning methods can also be used, but not all of them are guaranteed to give the same result as the un-pruned search.\n\nA naive minimax algorithm may be trivially modified to additionally return an entire Principal Variation along with a minimax score.\n\nThe pseudocode for the depth limited minimax algorithm is given below.\n\nThe minimax function returns a heuristic value for leaf nodes (terminal nodes and nodes at the maximum search depth).\nNon leaf nodes inherit their value from a descendant leaf node.\nThe heuristic value is a score measuring the favorability of the node for the maximizing player.\nHence nodes resulting in a favorable outcome, such as a win, for the maximizing player have higher scores than nodes more favorable for the minimizing player.\nThe heuristic value for terminal (game ending) leaf nodes are scores corresponding to win, loss, or draw, for the maximizing player.\nFor non terminal leaf nodes at the maximum search depth, an evaluation function estimates a heuristic value for the node.\nThe quality of this estimate and the search depth determine the quality and accuracy of the final minimax result.\n\nMinimax treats the two players (the maximizing player and the minimizing player) separately in its code. Based on the observation that formula_33, minimax may often be simplified into the negamax algorithm.\n\nSuppose the game being played only has a maximum of two possible moves per player each turn. The algorithm generates the tree on the right, where the circles represent the moves of the player running the algorithm (\"maximizing player\"), and squares represent the moves of the opponent (\"minimizing player\"). Because of the limitation of computation resources, as explained above, the tree is limited to a \"look-ahead\" of 4 moves.\n\nThe algorithm evaluates each \"leaf node\" using a heuristic evaluation function, obtaining the values shown. The moves where the \"maximizing player\" wins are assigned with positive infinity, while the moves that lead to a win of the \"minimizing player\" are assigned with negative infinity. At level 3, the algorithm will choose, for each node, the smallest of the \"child node\" values, and assign it to that same node (e.g. the node on the left will choose the minimum between \"10\" and \"+∞\", therefore assigning the value \"10\" to itself). The next step, in level 2, consists of choosing for each node the largest of the \"child node\" values. Once again, the values are assigned to each \"parent node\". The algorithm continues evaluating the maximum and minimum values of the child nodes alternately until it reaches the \"root node\", where it chooses the move with the largest value (represented in the figure with a blue arrow). This is the move that the player should make in order to \"minimize\" the \"maximum\" possible loss.\n\nMinimax theory has been extended to decisions where there is no other player, but where the consequences of decisions depend on unknown facts. For example, deciding to prospect for minerals entails a cost which will be wasted if the minerals are not present, but will bring major rewards if they are. One approach is to treat this as a game against \"nature\" (see move by nature), and using a similar mindset as Murphy's law or resistentialism, take an approach which minimizes the maximum expected loss, using the same techniques as in the two-person zero-sum games.\n\nIn addition, expectiminimax trees have been developed, for two-player games in which chance (for example, dice) is a factor.\n\nIn classical statistical decision theory, we have an estimator formula_34 that is used to estimate a parameter formula_35. We also assume a risk function formula_36, usually specified as the integral of a loss function. In this framework, formula_37 is called minimax if it satisfies\n\nAn alternative criterion in the decision theoretic framework is the Bayes estimator in the presence of a prior distribution formula_39. An estimator is Bayes if it minimizes the \"average\" risk\n\nA key feature of minimax decision making is being non-probabilistic: in contrast to decisions using expected value or expected utility, it makes no assumptions about the probabilities of various outcomes, just scenario analysis of what the possible outcomes are. It is thus robust to changes in the assumptions, as these other decision techniques are not. Various extensions of this non-probabilistic approach exist, notably minimax regret and Info-gap decision theory.\n\nFurther, minimax only requires ordinal measurement (that outcomes be compared and ranked), not \"interval\" measurements (that outcomes include \"how much better or worse\"), and returns ordinal data, using only the modeled outcomes: the conclusion of a minimax analysis is: \"this strategy is minimax, as the worst case is (outcome), which is less bad than any other strategy\". Compare to expected value analysis, whose conclusion is of the form: \"this strategy yields E(\"X\")=\"n.\"\" Minimax thus can be used on ordinal data, and can be more transparent.\n\nIn philosophy, the term \"maximin\" is often used in the context of John Rawls's \"A Theory of Justice,\" where he refers to it (Rawls (1971, p. 152)) in the context of The Difference Principle.\nRawls defined this principle as the rule which states that social and economic inequalities should be arranged so that \"they are to be of the greatest benefit to the least-advantaged members of society\".\n\n"}
{"id": "19590", "url": "https://en.wikipedia.org/wiki?curid=19590", "title": "Minnesota", "text": "Minnesota\n\nMinnesota () is a state in the Upper Midwest and northern regions of the United States. Minnesota was admitted as the 32nd U.S. state on May 11, 1858, created from the eastern half of the Minnesota Territory. The state has a large number of lakes, and is known by the slogan the \"Land of 10,000 Lakes\". Its official motto is \"L'Étoile du Nord\" (French: \"Star of the North\").\n\nMinnesota is the 12th largest in area and the 22nd most populous of the U.S. states; nearly 60% of its residents live in the Minneapolis–Saint Paul metropolitan area (known as the \"Twin Cities\"). This area is the center of transportation, business, industry, education, and government, while being home to an internationally known arts community. The remainder of the state consists of western prairies now given over to intensive agriculture; deciduous forests in the southeast, now partially cleared, farmed, and settled; and the less populated North Woods, used for mining, forestry, and recreation.\n\nMinnesota was inhabited by various indigenous peoples for thousands of years prior to the arrival of Europeans. French explorers, missionaries, and fur traders began exploring the region in the 17th century, encountering the Dakota and Ojibwe/Anishinaabe tribes. Much of what is today Minnesota was part of the vast French holding of Louisiana, which was purchased by the United States in 1803. Following several territorial reorganizations, Minnesota in its current form was admitted as the country's 32nd state on May 11, 1858. Like many Midwestern states, it remained sparsely populated and centered on lumber and agriculture. During the 19th and early 20th centuries, a large number of European immigrants, mainly from Scandinavia and Germany, began to settle the state, which remains a center of Scandinavian American and German American culture. \n\nIn recent decades, immigration from Asia, the Horn of Africa, the Middle East, and Latin America has broadened its demographic and cultural composition. The state's economy has heavily diversified, shifting from traditional activities such as agriculture and resource extraction to services and finance. Minnesota's standard of living index is among the highest in the United States, and the state is also among the best-educated and wealthiest in the nation.\n\nThe word \"Minnesota\" comes from the Dakota name for the Minnesota River: The river got its name from one of two words in the Dakota language, 'Mní sóta' which means \"clear blue water\", or 'Mnißota', which means cloudy water. Native Americans demonstrated the name to early settlers by dropping milk into water and calling it \"mnisota\". Many places in the state have similar names, such as Minnehaha Falls (\"curling water\" or waterfall), Minneiska (\"white water\"), Minneota (\"much water\"), Minnetonka (\"big water\"), Minnetrista (\"crooked water\"), and Minneapolis, a combination of \"mni\" and \"polis\", the Greek word for \"city\".\n\nMinnesota is the second northernmost U.S. state (after Alaska) and northernmost contiguous state. Its isolated Northwest Angle in Lake of the Woods county is the only part of the 48 contiguous states lying north of the 49th parallel. The state is part of the U.S. region known as the Upper Midwest and part of North America's Great Lakes Region. It shares a Lake Superior water border with Michigan and a land and water border with Wisconsin to the east. Iowa is to the south, North Dakota and South Dakota are to the west, and the Canadian provinces of Ontario and Manitoba are to the north. With , or approximately 2.25% of the United States, Minnesota is the 12th-largest state.\n\nMinnesota has some of the earth's oldest rocks, gneisses that are about 3.6 billion years old (80% as old as the planet). About 2.7 billion years ago, basaltic lava poured out of cracks in the floor of the primordial ocean; the remains of this volcanic rock formed the Canadian Shield in northeast Minnesota. The roots of these volcanic mountains and the action of Precambrian seas formed the Iron Range of northern Minnesota. Following a period of volcanism 1.1 billion years ago, Minnesota's geological activity has been more subdued, with no volcanism or mountain formation, but with repeated incursions of the sea, which left behind multiple strata of sedimentary rock.\n\nIn more recent times, massive ice sheets at least one kilometer thick ravaged the state's landscape and sculpted its terrain. The Wisconsin glaciation left 12,000 years ago. These glaciers covered all of Minnesota except the far southeast, an area characterized by steep hills and streams that cut into the bedrock. This area is known as the Driftless Zone for its absence of glacial drift. Much of the remainder of the state outside the northeast has 50 feet (15 m) or more of glacial till left behind as the last glaciers retreated. Gigantic Lake Agassiz formed in the northwest 13,000 years ago. Its bed created the fertile Red River valley, and its outflow, glacial River Warren, carved the valley of the Minnesota River and the Upper Mississippi downstream from Fort Snelling. Minnesota is geologically quiet today; it experiences earthquakes infrequently, and most of them are minor.\nThe state's high point is Eagle Mountain at 2,301 feet (701 m), which is only away from the low of 601 feet (183 m) at the shore of Lake Superior. Notwithstanding dramatic local differences in elevation, much of the state is a gently rolling peneplain.\n\nTwo major drainage divides meet in Minnesota's northeast in rural Hibbing, forming a triple watershed. Precipitation can follow the Mississippi River south to the Gulf of Mexico, the Saint Lawrence Seaway east to the Atlantic Ocean, or the Hudson Bay watershed to the Arctic Ocean.\n\nThe state's nickname, \"Land of 10,000 Lakes\", is apt, as there are 11,842 Minnesota lakes over in size. Minnesota's portion of Lake Superior is the largest at and deepest (at ) body of water in the state. Minnesota has 6,564 natural rivers and streams that cumulatively flow for . The Mississippi River begins its journey from its headwaters at Lake Itasca and crosses the Iowa border downstream. It is joined by the Minnesota River at Fort Snelling, by the St. Croix River near Hastings, by the Chippewa River at Wabasha, and by many smaller streams. The Red River, in the bed of glacial Lake Agassiz, drains the northwest part of the state northward toward Canada's Hudson Bay. Approximately of wetlands are within Minnesota's borders, the most of any state except Alaska.\n\nMinnesota has four ecological provinces: Prairie Parkland, in the southwestern and western parts of the state; the Eastern Broadleaf Forest (Big Woods) in the southeast, extending in a narrowing strip to the state's northwestern part, where it transitions into Tallgrass Aspen Parkland; and the northern Laurentian Mixed Forest, a transitional forest between the northern boreal forest and the broadleaf forests to the south. These northern forests are a vast wilderness of pine and spruce trees mixed with patchy stands of birch and poplar.\n\nMuch of Minnesota's northern forest underwent logging at some time, leaving only a few patches of old growth forest today in areas such as in the Chippewa National Forest and the Superior National Forest, where the Boundary Waters Canoe Area Wilderness has some of unlogged land. Although logging continues, regrowth and replanting keep about one third of the state forested. Nearly all of Minnesota's prairies and oak savannas have been fragmented by farming, grazing, logging, and suburban development.\n\nWhile loss of habitat has affected native animals such as the pine marten, elk, woodland caribou, and bison, others like whitetail deer and bobcat thrive. The state has the nation's largest population of timber wolves outside Alaska, and supports healthy populations of black bears, moose, and gophers. Located on the Mississippi Flyway, Minnesota hosts migratory waterfowl such as geese and ducks, and game birds such as grouse, pheasants, and turkeys. It is home to birds of prey, including the largest number of breeding pairs of bald eagles in the lower 48 states, as of 2007, red-tailed hawks, and snowy owls. The lakes teem with sport fish such as walleye, bass, muskellunge, and northern pike, and streams in the southeast and northeast are populated by brook, brown, and rainbow trout.\n\nMinnesota experiences temperature extremes characteristic of its continental climate, with cold winters and hot summers. The lowest temperature recorded was at Tower on February 2, 1996, whereas the highest was at Moorhead on July 6, 1936. Meteorological events include rain, snow, blizzards, thunderstorms, hail, derechos, tornadoes, and high-velocity straight-line winds. The growing season varies from 90 days per year in the Iron Range to 160 days in southeast Minnesota near the Mississippi River, and average temperatures range from . Average summer dew points range from about in the south to about in the north. Average annual precipitation ranges from , and droughts occur every 10 to 50 years.\n\nMinnesota's first state park, Itasca State Park, was established in 1891, and is the source of the Mississippi River. Today, Minnesota has 72 state parks and recreation areas, 58 state forests covering about four million acres (16,000 km²), and numerous state wildlife preserves, all managed by the Minnesota Department of Natural Resources. There are in the Chippewa and Superior national forests. The Superior National Forest in the northeast contains the Boundary Waters Canoe Area Wilderness, which encompasses over a million acres (4,000 km²) and a thousand lakes. To its west is Voyageurs National Park. The Mississippi National River and Recreation Area (MNRRA), is a corridor along the Mississippi River through the Minneapolis–St. Paul Metropolitan Area connecting a variety of sites of historic, cultural, and geologic interest.\n\nBefore European settlement of North America, Minnesota was populated by a subculture of Sioux called the Dakota people. As Europeans settled the east coast, Native Americans moved away from them causing migration of the Anishinaabe (also known as Ojibwe) and other Native Americans into the Minnesota area. The first Europeans in the area were French voyageur fur traders who arrived in the 17th century and began using the Grand Portage to access trapping and trading areas further inland. Late that century, Anishinaabe migrated westward to Minnesota, causing tensions with the Dakota people. Explorers such as Daniel Greysolon, Sieur du Lhut, Father Louis Hennepin, Jonathan Carver, Henry Schoolcraft, and Joseph Nicollet mapped out the state.\n\nIn 1762 the region became part of Spanish Louisiana until 1802. The portion of the state east of the Mississippi River became part of the United States at the end of the American Revolutionary War, when the Second Treaty of Paris was signed. Land west of the Mississippi River was acquired with the Louisiana Purchase, although a portion of the Red River Valley was disputed until the Treaty of 1818. By the late 1700s, the North West Company had established the post of Fort Charlotte at the Lake Superior end of the Grand Portage, until moving 50 miles northeast to Fort William in 1803. In 1805, Zebulon Pike bargained with Native Americans to acquire land at the confluence of the Minnesota and Mississippi rivers. The construction of Fort Snelling followed between 1819 and 1825. Its soldiers built a grist mill and a sawmill at Saint Anthony Falls, the first of the water-powered industries around which the city of Minneapolis later grew. Meanwhile, squatters, government officials, and tourists had settled near the fort. In 1839, the army forced them to move downriver and they settled in the area that became St. Paul. Minnesota Territory was formed on March 3, 1849. The first territorial legislature (held September 2, 1849) was dominated by men from New England or of New England ancestry. Thousands of people had come to build farms and cut timber, and Minnesota became the 32nd U.S. state on May 11, 1858. The founding population was so overwhelmingly of New England origins that the state was dubbed \"the New England of the West\".\nTreaties between European settlers and the Dakota and Ojibwe gradually forced the natives off their lands and on to smaller reservations. In 1861, residents of Mankato formed the Knights of the Forest, with a goal of eliminating all Indians from Minnesota. As conditions deteriorated for the Dakota, tensions rose, leading to the Dakota War of 1862. The result of the six-week war was the execution of 38 Dakota and the exile of most of the rest of the Dakota to the Crow Creek Reservation in Dakota Territory. As many as 800 white settlers died during the war.\n\nLogging and farming were mainstays of Minnesota's early economy. The sawmills at Saint Anthony Falls, and logging centers like Pine City, Marine on St. Croix, Stillwater, and Winona, processed high volumes of lumber. These cities were situated on rivers that were ideal for transportation. Later, Saint Anthony Falls was tapped to provide power for flour mills. Innovations by Minneapolis millers led to the production of Minnesota \"patent\" flour, which commanded almost double the price of \"bakers'\" or \"clear\" flour, which it replaced. By 1900, Minnesota mills, led by Pillsbury, Northwestern and the Washburn-Crosby Company (a forerunner of General Mills), were grinding 14.1 percent of the nation's grain.\nThe state's iron-mining industry was established with the discovery of iron in the Vermilion Range and the Mesabi Range in the 1880s, and in the Cuyuna Range in the early 20th century. The ore was shipped by rail to Duluth and Two Harbors, then loaded onto ships and transported eastward over the Great Lakes.\n\nIndustrial development and the rise of manufacturing caused the population to shift gradually from rural areas to cities during the early 20th century. Nevertheless, farming remained prevalent. Minnesota's economy was hard-hit by the Great Depression, resulting in lower prices for farmers, layoffs among iron miners, and labor unrest. Compounding the adversity, western Minnesota and the Dakotas were hit by drought from 1931 to 1935. New Deal programs provided some economic turnaround. The Civilian Conservation Corps and other programs around the state established some jobs for Indians on their reservations, and the Indian Reorganization Act of 1934 provided the tribes with a mechanism of self-government. This provided natives a greater voice within the state, and promoted more respect for tribal customs because religious ceremonies and native languages were no longer suppressed.\n\nAfter World War II, industrial development quickened. New technology increased farm productivity through automation of feedlots for hogs and cattle, machine milking at dairy farms, and raising chickens in large buildings. Planting became more specialized with hybridization of corn and wheat, and the use of farm machinery such as tractors and combines became the norm. University of Minnesota professor Norman Borlaug contributed to these developments as part of the Green Revolution. Suburban development accelerated due to increased postwar housing demand and convenient transportation. Increased mobility, in turn, enabled more specialized jobs.\n\nMinnesota became a center of technology after World War II. Engineering Research Associates was formed in 1946 to develop computers for the United States Navy. It later merged with Remington Rand, and then became Sperry Rand. William Norris left Sperry in 1957 to form Control Data Corporation (CDC). Cray Research was formed when Seymour Cray left CDC to form his own company. Medical device maker Medtronic also started business in the Twin Cities in 1949.\n\nSaint Paul, in east-central Minnesota along the banks of the Mississippi River, has been Minnesota's capital city since 1849, first as capital of the Territory of Minnesota, and then as the state capital since 1858.\n\nSaint Paul is adjacent to Minnesota's most populous city, Minneapolis; they and their suburbs are known collectively as the Twin Cities metropolitan area, the 16th-largest metropolitan area in the United States, and home to about 54 percent of the state's population. The remainder of the state is known as \"Greater Minnesota\" or pejoratively \"Outstate Minnesota\".\n\nThe state has 17 cities with populations above 50,000 (as of the 2010 census). In descending order of population, they are Minneapolis, Saint Paul, Rochester, Duluth, Bloomington, Brooklyn Park, Plymouth, Saint Cloud, Woodbury, Eagan, Maple Grove, Coon Rapids, Eden Prairie, Minnetonka, Burnsville, Apple Valley, Blaine, and Lakeville. Of these, only Rochester, Duluth, and Saint Cloud are outside the Twin Cities metropolitan area.\n\nMinnesota's population continues to grow, primarily in the urban centers. The populations of metropolitan Sherburne and Scott counties doubled between 1980 and 2000, while 40 of the state's 87 counties lost residents over the same period.\n\nFrom fewer than 6,120 white settlers in 1850, Minnesota's official population grew to over 1.7 million by 1900. Each of the next six decades saw a 15 percent increase in population, reaching 3.4 million in 1960. Growth then slowed, rising 11 percent to 3.8 million in 1970, and an average of 9 percent over the next three decades to 4.9 million in the 2000 Census.\n\nThe United States Census Bureau estimates the population of Minnesota was 5,489,594 on July 1, 2015, a 3.5 percent increase since the 2010 United States Census. The rate of population change, and age and gender distributions, approximate the national average. Minnesota's center of population is in Hennepin County.\n\nAs of the 2010 Census, the population of Minnesota was 5,303,925. The gender makeup of the state was 49.6% male and 50.4% female. 24.2% of the population were under the age of 18; 9.5% were between the ages of 18 and 24; 26.3% were from 25 to 44; 27.1% were from 45 to 64; and 12.9% were 65 years of age or older.\n\nThe table below shows the racial composition of Minnesota's population as of 2016.\n\nAccording to the 2016 American Community Survey, 5.1% of Minnesota's population were of Hispanic or Latino origin (of any race): Mexican (3.5%), Puerto Rican (0.2%), Cuban (0.1%), and other Hispanic or Latino origin (1.2%). The ancestry groups claimed by more than five percent of the population were: German (33.8%), Norwegian (15.3%), Irish (10.5%), Swedish (8.1%), and English (5.4%).\n\nIn 2011, non-Hispanic whites were involved in 72.3 percent of all the births. Minnesota's growing minority groups, however, still form a smaller percentage of the population than in the nation as a whole.\n\nMinnesota has America's largest Somali population, with an estimated 57,000 people, the largest concentration outside of the Horn of Africa.\n\nThe majority of Minnesotans are Protestants, including a Lutheran contingent, owing to the state's largely Northern European ethnic makeup. Roman Catholics (of largely German, Irish, French and Slavic descent) make up the largest single Christian denomination. A 2010 survey by the Pew Forum on Religion and Public Life showed that 32 percent of Minnesotans were affiliated with Mainline Protestant traditions, 21 percent were Evangelical Protestants, 28 percent were Roman Catholic, 1 percent each were Jewish, Muslim, Buddhist, and Black Protestant, and smaller amounts were of other faiths, with 13 percent unaffiliated. According to the Association of Religion Data Archives, the denominations with the most adherents in 2010 were the Roman Catholic Church with 1,150,367; the Evangelical Lutheran Church in America with 737,537; and the Lutheran Church–Missouri Synod with 182,439. This is broadly consistent with the results of the 2001 American Religious Identification Survey, which also gives detailed percentages for many individual denominations. Although Christianity is dominant, Minnesota has a long history with non-Christian faiths. Ashkenazi Jewish pioneers set up Saint Paul's first synagogue in 1856. Minnesota is home to over 30 mosques, mostly in the Twin Cities metro area. The Temple of ECK, the spiritual home of Eckankar, is based in Minnesota.\n\nOnce primarily a producer of raw materials, Minnesota's economy has transformed to emphasize finished products and services. Perhaps the most significant characteristic of the economy is its diversity; the relative outputs of its business sectors closely match the United States as a whole. The economy of Minnesota had a gross domestic product of $262 billion in 2008. In 2008, thirty-three of the United States' top 1,000 publicly traded companies (by revenue) were headquartered in Minnesota, including Target, UnitedHealth Group, 3M, General Mills, U.S. Bancorp, Ameriprise, Hormel, Land O' Lakes, SuperValu, Best Buy, and Valspar. Private companies based in Minnesota include Cargill, the largest privately owned company in the United States, and Carlson Companies, the parent company of Radisson Hotels.\n\nThe per capita personal income in 2008 was $42,772, the tenth-highest in the nation. The three-year median household income from 2002 to 2004 was $55,914, ranking fifth in the U.S. and first among the 36 states not on the Atlantic coast.\n\nAs of December 2018, the state's unemployment rate was 2.8 percent.\n\nMinnesota's earliest industries were fur trading and agriculture. The city of Minneapolis grew around the flour mills powered by St. Anthony Falls. Although less than one percent of the population is now employed in the agricultural sector, it remains a major part of the state's economy, ranking sixth in the nation in the value of products sold. The state is the U.S.'s largest producer of sugar beets, sweet corn, and green peas for processing, and farm-raised turkeys. Minnesota is also a large producer of corn and soybeans. Minnesota has the most food cooperatives per capita in the United States. Forestry remains strong, including logging, pulpwood processing and paper production, and forest products manufacturing. Minnesota was famous for its soft-ore mines, which produced a significant portion of the world's iron ore for over a century. Although the high-grade ore is now depleted, taconite mining continues, using processes developed locally to save the industry. In 2004, the state produced 75 percent of the country's usable iron ore. The mining boom created the port of Duluth which continues to be important for shipping ore, coal, and agricultural products. The manufacturing sector now includes technology and biomedical firms, in addition to the older food processors and heavy industry. The nation's first indoor shopping mall was Edina's Southdale Center, and its largest is Bloomington's Mall of America.\n\nMinnesota is one of 42 U.S. states with its own lottery; its games include Powerball, Mega Millions, Hot Lotto (all three multi-state), Northstar Cash, and Gopher 5.\n\nMinnesota produces ethanol fuel and is the first to mandate its use, a ten percent mix (E10). In 2005, there were more than 310 service stations supplying E85 fuel, comprising 85 percent ethanol and 15 percent gasoline. A two percent biodiesel blend has been required in diesel fuel since 2005. Minnesota is ranked in the top ten for wind energy produciton. The state gets nearly one fifth of all its energy from wind.\n\nMinnesota has a progressive income tax structure; the four brackets of state income tax rates are 5.35, 7.05, 7.85 and 9.85 percent. As of 2008, Minnesota was ranked 12th in the nation in per capita total state and local taxes. In 2008, Minnesotans paid 10.2 percent of their income in state and local taxes; the U.S. average was 9.7 percent. The state sales tax in Minnesota is 6.875 percent, but there is no sales tax on clothing, prescription drug medications, some services, or food items for home consumption. The state legislature may allow municipalities to institute local sales taxes and special local taxes, such as the 0.5 percent supplemental sales tax in Minneapolis. Excise taxes are levied on alcohol, tobacco, and motor fuel. The state imposes a use tax on items purchased elsewhere but used within Minnesota. Owners of real property in Minnesota pay property tax to their county, municipality, school district, and special taxing districts.\n\n Minnesota's leading fine art museums include the Minneapolis Institute of Art, the Walker Art Center, the Frederick R. Weisman Art Museum, and The Museum of Russian Art (TMORA). All are in Minneapolis. The Minnesota Orchestra and the Saint Paul Chamber Orchestra are prominent full-time professional musical ensembles that perform concerts and offer educational programs to the Twin Cities' community. The world-renowned Guthrie Theater moved into a new Minneapolis facility in 2006, boasting three stages and overlooking the Mississippi River. Attendance at theatrical, musical, and comedy events in the area is strong. In the United States, the Twin Cities' number of theater seats per capita ranks behind only New York City; with some 2.3 million theater tickets sold annually. The Minnesota Fringe Festival is an annual celebration of theatre, dance, improvisation, puppetry, kids' shows, visual art, and musicals. The summer festival consists of over 800 performances over 11 days in Minneapolis, and is the largest non-juried performing arts festival in the United States.\n\nThe rigors and rewards of pioneer life on the prairie are the subject of \"Giants in the Earth\" by Ole Rolvaag and the \"Little House\" series of children's books by Laura Ingalls Wilder. Small-town life is portrayed grimly by Sinclair Lewis in the novel \"Main Street\", and more gently and affectionately by Garrison Keillor in his tales of Lake Wobegon. St. Paul native F. Scott Fitzgerald writes of the social insecurities and aspirations of the young city in stories such as \"Winter Dreams\" and \"The Ice Palace\" (published in \"Flappers and Philosophers\"). Henry Wadsworth Longfellow's epic poem \"The Song of Hiawatha\" was inspired by Minnesota and names many of the state's places and bodies of water. Minnesota native Robert Zimmerman (Bob Dylan) won the 2016 Nobel Prize in Literature.\n\nMinnesota musicians include Bob Dylan, Eddie Cochran, The Andrews Sisters, The Castaways, The Trashmen, Prince, Soul Asylum, David Ellefson, Hüsker Dü, Owl City, and The Replacements. Minnesotans helped shape the history of music through popular American culture: the Andrews Sisters' \"Boogie Woogie Bugle Boy\" was an iconic tune of World War II, while the Trashmen's \"Surfin' Bird\" and Bob Dylan epitomize two sides of the 1960s. In the 1980s, influential hit radio groups and musicians included Prince, The Original 7ven, Jimmy Jam & Terry Lewis, The Jets, Lipps Inc., and Information Society.\n\nMinnesotans have also made significant contributions to comedy, theater, media, and film. The comic strip \"Peanuts\" was created by St. Paul native Charles M. Schulz. A Prairie Home Companion which first aired in 1974, became a long-running comedy radio show on National Public Radio. A cult scifi cable TV show, Mystery Science Theater 3000, was created by Joel Hodgson in Hopkins, and Minneapolis, MN. Another popular comedy staple developed in the 1990s, The Daily Show, was originated through Lizz Winstead and Madeleine Smithberg.\n\nJoel and Ethan Coen, Terry Gilliam, Bill Pohlad, and Mike Todd contributed to the art of filmmaking as writers, directors, and producers. Notable actors from Minnesota include Loni Anderson, Richard Dean Anderson, James Arness, Jessica Biel, Rachael Leigh Cook, Julia Duffy, Mike Farrell, Judy Garland, Peter Graves, Josh Hartnett, Garrett Hedlund, Tippi Hedren, Jessica Lange, Kelly Lynch, E.G. Marshall, Laura Osnes, Melissa Peterman, Chris Pratt, Marion Ross, Jane Russell, Winona Ryder, Seann William Scott, Kevin Sorbo, Lea Thompson, Vince Vaughn, Jesse Ventura, and Steve Zahn.\n\nStereotypical traits of Minnesotans include \"Minnesota nice\", Lutheranism, a strong sense of community and shared culture, and a distinctive brand of North Central American English sprinkled with Scandinavian expressions. Potlucks, usually with a variety of hotdishes, are popular small-town church activities. A small segment of the Scandinavian population attend a traditional lutefisk dinner to celebrate Christmas. Life in Minnesota has also been depicted or used as a backdrop, in movies such as \"Fargo\", \"Grumpy Old Men\", \"Grumpier Old Men\", \"Juno\", \"Drop Dead Gorgeous\", \"Young Adult\", \"A Serious Man\", \"New in Town\", \"Rio\", and in famous television series like \"Little House on the Prairie\", \"The Mary Tyler Moore Show\", \"The Golden Girls\", \"Coach\", \"The Rocky and Bullwinkle Show\", \"How I Met Your Mother\" and \"Fargo\". Major movies that were shot on location in Minnesota include \"That Was Then... This Is Now\", \"Purple Rain\", \"Airport\", \"Beautiful Girls\", \"North Country\", \"Untamed Heart\", \"Feeling Minnesota\", \"Jingle All The Way\", \"A Simple Plan\", and \"The Mighty Ducks films\".\n\nThe Minnesota State Fair, advertised as \"The Great Minnesota Get-Together\", is an icon of state culture. In a state of 5.5 million people, there were over 1.8 million visitors to the fair in 2014, setting a new attendance record. The fair covers the variety of Minnesotan life, including fine art, science, agriculture, food preparation, 4-H displays, music, the midway, and corporate merchandising. It is known for its displays of seed art, butter sculptures of dairy princesses, the birthing barn, and the \"fattest pig\" competition. One can also find dozens of varieties of food on a stick, such as Pronto Pups, cheese curds, and deep-fried candy bars. On a smaller scale, many of these attractions are offered at numerous county fairs.\n\nOther large annual festivals include the Saint Paul Winter Carnival, the Minnesota Renaissance Festival, Minneapolis' Aquatennial and Mill City Music Festival, Moondance Jam in Walker, Sonshine Christian music festival in Willmar, the Judy Garland Festival in Grand Rapids, the Eelpout Festival on Leech Lake, and the WE Fest in Detroit Lakes.\n\nMinnesotans have low rates of premature death, infant mortality, cardiovascular disease, and occupational fatalities. They have long life expectancies, and high rates of health insurance and regular exercise. These and other measures have led two groups to rank Minnesota as the healthiest state in the nation; however, in one of these rankings, Minnesota descended from first to sixth in the nation between 2005 and 2009 because of low levels of public health funding and the prevalence of binge drinking. While overall health indicators are strong, Minnesota does have significant health disparities in minority populations.\n\nOn October 1, 2007, Minnesota became the 17th state to enact the Freedom to Breathe Act, a statewide smoking ban in restaurants and bars.\n\nMedical care in the state is provided by a comprehensive network of hospitals and clinics operated by a number of large providers including Allina Hospitals & Clinics, CentraCare Health System, Essentia Health, Fairview Health Services, HealthPartners, and the Mayo Clinic Health System. There are two teaching hospitals and medical schools in Minnesota. The University of Minnesota Medical School is a high-rated teaching institution that has made a number of breakthroughs in treatment, and its research activities contribute significantly to the state's growing biotechnology industry. The Mayo Clinic, a world-renowned hospital based in Rochester, was founded by William Worrall Mayo, an immigrant from England.\n\n\"U.S. News & World Report\" 2014–2015 survey ranked 4,743 hospitals in the United States in 16 specialized fields of care, and placed the Mayo Clinic in the top four in all fields except psychiatry, where it ranked seventh. The hospital ranked #1 in eight fields and #2 in three others. The Mayo Clinic and the University of Minnesota are partners in the Minnesota Partnership for Biotechnology and Medical Genomics, a state-funded program that conducts research into cancer, Alzheimer's disease, heart health, obesity, and other areas.\n\nOne of the Minnesota Legislature's first acts when it opened in 1858 was the creation of a normal school in Winona. Minnesota's commitment to education has contributed to a literate and well-educated populace. In 2009, according to the U.S. Census Bureau, Minnesota had the second-highest proportion of high school graduates, with 91.5% of people 25 and older holding a diploma, and the tenth-highest proportion of people with bachelor's degrees. In 2015, Minneapolis was named the nation's \"Most Literate City\", while St. Paul placed fourth, according to a major annual survey. In a 2013 study conducted by the National Center for Educational Statistics comparing the performance of eighth-grade students internationally in math and science, Minnesota ranked eighth in the world and third in the United States, behind Massachusetts and Vermont. In 2014, Minnesota students earned the tenth-highest average composite score in the nation on the ACT exam. In 2013, nationwide in per-student public education spending, Minnesota ranked 21st. While Minnesota has chosen not to implement school vouchers, it is home to the first charter school.\n\nThe state supports a network of public universities and colleges, including 37 institutions in the Minnesota State Colleges and Universities System, and five major campuses of the University of Minnesota system. It is also home to more than 20 private colleges and universities, six of which rank among the nation's top 100 liberal arts colleges, according to U.S. News & World Report.\n\nTransportation in Minnesota is overseen by the Minnesota Department of Transportation (MnDOT) at the state level and by regional and local governments at the local level. Principal transportation corridors radiate from the Twin Cities metropolitan area and along interstate corridors in Greater Minnesota. The major Interstate highways are Interstate 35 (I-35), I-90, and I-94, with I-35 and I-94 connecting the Minneapolis–St. Paul area, and I-90 traveling east-west along the southern edge of the state. In 2006, a constitutional amendment was passed that required sales and use taxes on motor vehicles to fund transportation, with at least 40 percent dedicated to public transit. There are nearly two dozen rail corridors in Minnesota, most of which go through Minneapolis–St. Paul or Duluth. There is water transportation along the Mississippi River system and from the ports of Lake Superior.\nMinnesota's principal airport is Minneapolis–St. Paul International Airport (MSP), a major passenger and freight hub for Delta Air Lines and Sun Country Airlines. Most other domestic carriers serve the airport. Large commercial jet service is provided at Duluth and Rochester, with scheduled commuter service to four smaller cities via Delta Connection carriers SkyWest Airlines, Compass Airlines, and Endeavor Air.\n\nPublic transit services are available in the regional urban centers in Minnesota including Metro Transit in the Twin Cities, opt out suburban operators Minnesota Valley Transit Authority, SouthWest Transit, Plymouth Metrolink, Maple Grove Transit and others. In Greater Minnesota transit services are provided by city systems such as Duluth Transit Authority, Mankato Transit System, MATBUS (Fargo-Moorhead), Rochester Public Transit, Saint Cloud Metro Bus, Winona Public Transit and others. Dial-a-Ride service is available for persons with disabilities in a majority of Minnesota Counties. \n\nIn addition to bus services, Amtrak's daily \"Empire Builder\" (Chicago–Seattle/Portland) train runs through Minnesota, calling at the Saint Paul Union Depot and five other stations. Intercity bus providers include Jefferson Lines, Greyhound, and Megabus. Local public transit is provided by bus networks in the larger cities and by two rail services. The Northstar Line commuter rail service runs from Big Lake to the Target Field station in downtown Minneapolis. From there, light rail runs to Saint Paul Union Depot on the Green Line, and to the MSP airport and the Mall of America via the Blue Line.\n\nAs with the federal government of the United States, power in Minnesota is divided into three branches: executive, legislative, and judicial.\n\nThe executive branch is headed by the governor. Governor Tim Walz, DFL (Democratic Farmer Labor), took office on January 7, 2019. The governor has a cabinet consisting of the leaders of various state government agencies, called commissioners. The other elected constitutional offices are secretary of state, attorney general, and state auditor.\n\nConstitutional officeholders:\n\nThe Minnesota Legislature is a bicameral body consisting of the Senate and the House of Representatives. The state has 67 districts, each with about 60,000 people. Each district has one senator and two representatives, each senatorial district being divided into \"A\" and \"B\" sections for members of the House. Senators serve for four years and representatives for two years. In the November 2010 election, the Minnesota Republican Party gained 25 house seats, giving them control of the House of Representatives by a 72–62 margin. The 2010 election also saw Minnesota voters elect a Republican majority in the Senate for the first time since 1972. In 2012, the Democrats regained the House of Representatives by a margin of 73–61, picking up 11 seats; the Democrats also regained the Minnesota Senate. Control of the houses shifted back to Republicans in the 2016 election.\n\nHouse Leadership\n\nSenate Leadership\n\nMinnesota's court system has three levels. Most cases start in the district courts, which are courts of general jurisdiction. There are 279 district court judgeships in ten judicial districts. Appeals from the trial courts and challenges to certain governmental decisions are heard by the Minnesota Court of Appeals, consisting of 19 judges who typically sit in three-judge panels. The seven-justice Minnesota Supreme Court hears all appeals from the tax court, the workers' compensation court of appeals, first-degree murder convictions, and discretionary appeals from the court of appeals; it also has original jurisdiction over election disputes.\n\nTwo specialized courts within administrative agencies have been established: the workers' compensation court of appeals, and the tax court, which deals with non-criminal tax cases.\n\nSupreme Court Justices\nAssociate Justices\n\nIn addition to the city and county levels of government found in the United States, Minnesota has other entities that provide governmental oversight and planning. Regional development commissions (RDCs) provide technical assistance to local governments in broad multi-county area of the state. Along with this Metropolitan Planning Organizations (MPOs), such as the Metropolitan Council, provide planning and oversight of land use actions in metropolitan areas. Many lakes and rivers are overseen by watershed districts and soil and water conservation districts.\n\nMinnesota's United States senators are Democrat Amy Klobuchar and Democrat Tina Smith. The outcome of the 2008 U.S. Senate election in Minnesota was contested until June 30, 2009; when the Minnesota Supreme Court ruled in favor of Franken, Republican Norm Coleman conceded defeat. Franken resigned on January 2, 2018, and Minnesota Governor Mark Dayton appointed his lieutenant governor, Tina Smith, to Franken's seat until a special election in November 2018. The state has eight congressional districts; they are represented by Jim Hagedorn (1st district; R), Angie Craig (2nd; DFL), Dean Phillips (3rd; DFL), Betty McCollum (4th; DFL), Ilhan Omar (5th; DFL), Tom Emmer (6th; R), Collin Peterson (7th; DFL), and Pete Stauber (8th; R).\n\nFederal court cases are heard in the United States District Court for the District of Minnesota, which holds court in Minneapolis, St. Paul, Duluth, and Fergus Falls. Appeals are heard by the Eighth Circuit Court of Appeals, which is based in St. Louis, Missouri and routinely also hears cases in St. Paul.\n\nThe State of Minnesota was created by the United States federal government in the traditional and cultural range of lands occupied by the Dakota and Anishinaabe peoples as well as other Native American groups. After many years of unequal treaties and forced resettlement by the state and federal government, the tribes re-organized into sovereign tribal governments. Today, the tribal governments are divided into 11 semi-autonomous reservations that negotiate with the US and the state on a bilateral basis:\n\nFour Dakota Mdewakanton communities:\n\nSeven Anishinaabe reservations:\n\nThe first six of the Anishinaabe bands compose the Minnesota Chippewa Tribe, the collective federally recognized tribal government of the Bois Forte, Fond du Lac, Grand Portage, Leech Lake, Mille Lacs, and White Earth reservations.\n\nMinnesota is known for a politically active citizenry, and populism has been a long-standing force among the state's political parties. Minnesota has a consistently high voter turnout. In the 2008 U.S. presidential election, 78.2% of eligible Minnesotans voted—the highest percentage of any U.S. state—versus the national average of 61.2%. Voters can register on election day at their polling places with evidence of residency.\n\nHubert Humphrey brought national attention to the state with his address at the 1948 Democratic National Convention. Minnesotans have consistently cast their Electoral College votes for Democratic presidential candidates since 1976, longer than any other state. Minnesota is the only state in the nation that did not vote for Ronald Reagan in either of his presidential runs. Minnesota has gone for the Democratic Party in every presidential election since 1960, with the exception of 1972, when it was carried by Republican Richard Nixon.\n\nBoth the Democratic and Republican parties have major-party status in Minnesota, but its state-level \"Democratic\" party is actually a separate party, officially known as the Minnesota Democratic-Farmer-Labor Party (DFL). It was formed out of a 1944 alliance of the Minnesota Democratic and Farmer-Labor parties.\n\nThe state has had active third-party movements. The Reform Party, now the Independence Party, was able to elect former mayor of Brooklyn Park and professional wrestler Jesse Ventura to the governorship in 1998. The Independence Party has received enough support to keep major-party status. The Green Party, while no longer having major-party status, has a large presence in municipal government, notably in Minneapolis and Duluth, where it competes directly with the DFL party for local offices. Major-party status in Minnesota (which grants state funding for elections) is reserved to parties whose candidates receive five percent or more of the vote in any statewide election (e.g., Governor, Secretary of State, U.S. President).\n\nThe state's U.S. Senate seats have generally been split since the early 1990s, and in the 108th and 109th Congresses, Minnesota's congressional delegation was split, with four representatives and one senator from each party. In the 2006 mid-term election, Democrats were elected to all state offices, except governor and lieutenant governor, where Republicans Tim Pawlenty and Carol Molnau narrowly won re-election. The DFL posted double-digit gains in both houses of the legislature, elected Amy Klobuchar to the U.S. Senate, and increased the party's U.S. House caucus by one. Keith Ellison (DFL) was elected as the first African American U.S. Representative from Minnesota, as well as the first Muslim elected to Congress nationwide. In 2008, DFLer and former comedian and radio talk show host Al Franken beat incumbent Republican Norm Coleman in the United States Senate race by 312 votes out of 3 million cast.\n\nIn the 2010 election, Republicans took control of both chambers of the Minnesota legislature for the first time in 38 years and, with Mark Dayton's election, the DFL party took the governor's office for the first time in 20 years. Two years later, the DFL regained control of both houses, and with Dayton in office, the party had same-party control of both the legislative and executive branches for the first time since 1990. Two years later, the Republicans regained control of the Minnesota House, and in 2016, the GOP also regained control of the State Senate.\n\nIn 2018, the DFL retook control of the Minnesota House, while electing DFLer Tim Walz as Governor.\n\nThe Twin Cities area is the fifteenth-largest media market in the United States, as ranked by Nielsen Media Research. The state's other top markets are Fargo–Moorhead (118th nationally), Duluth–Superior (137th), Rochester–Mason City–Austin (152nd), and Mankato (200th).\n\nBroadcast television in Minnesota and the Upper Midwest started on April 27, 1948, when KSTP-TV began broadcasting. Hubbard Broadcasting, which owns KSTP, is now the only locally owned television company in Minnesota. Twin Cities CBS station WCCO-TV and FOX station KMSP-TV are owned-and-operated by their respective networks. There are 39 analog broadcast stations and 23 digital channels broadcast over Minnesota.\n\nThe four largest daily newspapers are the \"Star Tribune\" in Minneapolis, the \"Pioneer Press\" in Saint Paul, the \"Duluth News Tribune\" in Duluth, and the \"Post-Bulletin\" in Rochester. \"The Minnesota Daily\" is the largest student-run newspaper in the U.S. Sites offering daily news on the Web include \"The UpTake\", \"MinnPost\", the Twin Cities \"Daily Planet\", business news site \"Finance and Commerce\" and Washington D.C.-based \"Minnesota Independent\". Weeklies including \"City Pages\" and monthly publications such as \"Minnesota Monthly\" are available.\n\nTwo of the largest public radio networks, Minnesota Public Radio (MPR) and Public Radio International (PRI), are based in the state. MPR has the largest audience of any regional public radio network in the nation, broadcasting on 37 radio stations. PRI weekly provides more than 400 hours of programming to almost 800 affiliates. The state's oldest radio station, KUOM-AM, was launched in 1922 and is among the 10-oldest radio stations in the United States. The University of Minnesota-owned station is still on the air, and since 1993 broadcasts a college rock format.\n\nMinnesota has an active program of organized amateur and professional sports. Tourism has become an important industry, especially in the Lake region. In the North Country, what had been an industrial area focused on mining and timber has largely been transformed into a vacation destination. Popular interest in the environment and environmentalism, added to traditional interests in hunting and fishing, has attracted a large urban audience within driving range.\n\nMinnesota has professional men's teams in all major sports.\n\nThe Minnesota Vikings have played in the National Football League since their admission as an expansion franchise in 1961. They played in Metropolitan Stadium from 1961 through 1981 and in the Hubert H. Humphrey Metrodome from 1982 until its demolition after the 2013 season for the construction of the team's new home, U.S. Bank Stadium. The Vikings' current stadium hosted Super Bowl LII in February, 2018. Super Bowl XXVI was played in the Metrodome. The Vikings have advanced to the Super Bowl Super Bowl IV, Super Bowl VIII, Super Bowl IX, and Super Bowl XI, losing all four games to their AFC/AFL opponent\n\nThe Minnesota Twins have played in the Major League Baseball in the Twin Cities starting in 1961. The Twins began play as the original Washington Senators, relocating to Minnesota in 1961. The Twins won the 1987 and 1991 World Series in seven game matches where the home team was victorious in all games . The team has played at Target Field since 2010. The Twins also advanced to the 1965 World Series, where they lost to the Los Angeles Dodgers in seven games.\n\nThe Minneapolis Lakers of the National Basketball Association played in the Minneapolis Auditorium from 1947 to 1960, after which they relocated to Los Angeles. The Minnesota Timberwolves joined the NBA in 1989, and play in the Target Center since 1990.\n\nThe National Hockey League's Minnesota Wild play in St. Paul's Xcel Energy Center, and reached 300 consecutive sold-out games on January 16, 2008. Previously, the Minnesota North Stars competed in NHL from 1967 to 1993, which played the 1981 and 1991 Stanley Cup Finals.\n\nMinnesota United FC joined Major League Soccer as an expansion team in 2017, having played in the lower-division North American Soccer League from 2010 to 2016. The team plays at TCF Bank Stadium, but will open Allianz Field in St. Paul in 2019.\n\nMinnesota also has minor-league professional sports teams. The Minnesota Swarm of the National Lacrosse League played at the Xcel Energy Center until the team moved to Georgia in 2015. Minor league baseball is represented by major league-sponsored teams and independent teams such as the St. Paul Saints, who play at CHS Field in St. Paul.\n\nProfessional women's sports include the Minnesota Lynx of the Women's National Basketball Association, winners of the 2011, 2013, 2015, and 2017 WNBA Championships, the Minnesota Lightning of the United Soccer Leagues W-League, the Minnesota Vixen of the Independent Women's Football League, the Minnesota Valkyrie of the Legends Football League, and the Minnesota Whitecaps of the National Women's Hockey League.\n\nThe Twin Cities campus of the University of Minnesota is a National Collegiate Athletic Association (NCAA) Division I school competing in the Big Ten Conference. Four additional schools in the state compete in NCAA Division I ice hockey: the University of Minnesota Duluth; Minnesota State University, Mankato; St. Cloud State University and Bemidji State University. There are nine NCAA Division II colleges in the Northern Sun Intercollegiate Conference, and twenty NCAA Division III colleges in the Minnesota Intercollegiate Athletic Conference and Upper Midwest Athletic Conference.\n\nMinneapolis has hosted the NCAA Men's Division I Basketball Championship in 1951, 1992 and 2001 and will do so again in 2019.\n\nThe Hazeltine National Golf Club has hosted the U.S. Open, U.S. Women's Open, U.S. Senior Open and PGA Championship. The course also hosted the Ryder Cup in the fall of 2016, when it became one of two courses in the U.S. to host all major golf competitions. The Ryder Cup is scheduled to return in 2028.\n\nInterlachen Country Club has hosted the U.S. Open, U.S. Women's Open, and Solheim Cup.\n\nWinter Olympic Games medallists from the state include twelve of the twenty members of the gold medal 1980 ice hockey team (coached by Minnesota native Herb Brooks) and the bronze medallist U.S. men's curling team in the 2006 Winter Olympics. Swimmer Tom Malchow won an Olympic gold medal in the 2000 Summer games and a silver medal in 1996.\n\nGrandma's Marathon is run every summer along the scenic North Shore of Lake Superior, and the Twin Cities Marathon winds around lakes and the Mississippi River during the peak of the fall color season. Farther north, Eveleth is the location of the United States Hockey Hall of Fame.\n\nAs the state's tourism promotion office, Explore Minnesota pursues an entrepreneurial approach, leveraging the state's tourism investment with increased involvement by the private sector. A council of representatives from the state's tourism industry strongly connects Explore Minnesota with tourism businesses and organizations. Explore Minnesota's mission is to inspire and facilitate travel to and within the state of Minnesota.\n\nTourism is a $14.4 billion industry in Minnesota, and a key sector of the state's economy. The leisure and hospitality industry – a major provider of tourism services – employs more than 260,000 workers, representing 11 percent of Minnesota's private sector employment. Leisure and hospitality also generates 17 percent of the state's sales tax revenues. Minnesota welcomes more than 70 million domestic and international travelers annually.\n\nIn 2014, Explore Minnesota launched an all-new travel marketing campaign, themed \"Only in Minnesota\", to increase awareness about Minnesota as a one-of-a-kind Midwest travel destination. The strategic effort, which includes a new and improved website and market reach to audiences in 14 states and provinces, is the largest travel marketing campaign in the state's history. A new series of advertisements and a strong, user-driven #OnlyinMN social media movement has been well-received and has engaged travelers, residents, businesses and visitors bureaus across the state. In the latest evolution of the popular #OnlyinMN campaign, Explore Minnesota generated 3.5 million trips to Minnesota, and more than $388 million in traveler spending. Explore Minnesota engaged with hundreds of thousands of people through social media, surpassing half a million uses of the campaign hashtag as of May 2017. The newly branded slogan represents the diversity of Minnesota, from its bustling downtowns to untouched wilderness, pine forests to bluff country and historic landmarks to modern-day attractions. #OnlyinMN celebrates the inspiring and sometimes unexpected experiences that await travelers on a Minnesota vacation.\n\nMinnesotans participate in high levels of physical activity, and many of these activities are outdoors. The strong interest of Minnesotans in environmentalism has been attributed to the popularity of these pursuits.\n\nIn the warmer months, these activities often involve water. Weekend and longer trips to family cabins on Minnesota's numerous lakes are a way of life for many residents. Activities include water sports such as water skiing, which originated in the state, boating, canoeing, and fishing. More than 36 percent of Minnesotans fish, second only to Alaska.\n\nFishing does not cease when the lakes freeze; ice fishing has been around since the arrival of early Scandinavian immigrants. Minnesotans have learned to embrace their long, harsh winters in ice sports such as skating, hockey, curling, and broomball, and snow sports such as cross-country skiing, alpine skiing, snowshoeing, and snowmobiling. Minnesota is the only U.S. state where bandy is played.\n\nState and national forests and the seventy-two state parks are used year-round for hunting, camping, and hiking. There are almost of snowmobile trails statewide. Minnesota has more miles of bike trails than any other state, and a growing network of hiking trails, including the Superior Hiking Trail in the northeast. Many hiking and bike trails are used for cross-country skiing during the winter.\n\n\n\n\n"}
{"id": "19591", "url": "https://en.wikipedia.org/wiki?curid=19591", "title": "Missouri River", "text": "Missouri River\n\nThe Missouri River is the longest river in North America. Rising in the Rocky Mountains of western Montana, the Missouri flows east and south for before entering the Mississippi River north of St. Louis, Missouri. The river takes drainage from a sparsely populated, semi-arid watershed of more than half a million square miles (1,300,000 km), which includes parts of ten U.S. states and two Canadian provinces. When combined with the lower Mississippi River, it forms the world's fourth longest river system.\n\nFor over 12,000 years, people have depended on the Missouri River and its tributaries as a source of sustenance and transportation. More than ten major groups of Native Americans populated the watershed, most leading a nomadic lifestyle and dependent on enormous bison herds that once roamed through the Great Plains. The first Europeans encountered the river in the late seventeenth century, and the region passed through Spanish and French hands before finally becoming part of the United States through the Louisiana Purchase. The Missouri was long believed to be part of the Northwest Passage – a water route from the Atlantic to the Pacific – but when Lewis and Clark became the first to travel the river's entire length, they confirmed the mythical pathway to be no more than a legend.\n\nThe Missouri River was one of the main routes for the westward expansion of the United States during the 19th century. The growth of the fur trade in the early 19th century laid much of the groundwork as trappers explored the region and blazed trails. Pioneers headed west \"en masse\" beginning in the 1830s, first by covered wagon, then by the growing numbers of steamboats entering service on the river. Former Native American lands in the watershed were taken over by settlers, leading to some of the most longstanding and violent wars against indigenous peoples in American history.\n\nDuring the 20th century, the Missouri River basin was extensively developed for irrigation, flood control and the generation of hydroelectric power. Fifteen dams impound the main stem of the river, with hundreds more on tributaries. Meanders have been cut and the river channelized to improve navigation, reducing its length by almost from pre-development times. Although the lower Missouri valley is now a populous and highly productive agricultural and industrial region, heavy development has taken its toll on wildlife and fish populations as well as water quality.\n\nFrom the Rocky Mountains of Montana and Wyoming, three streams rise to form the headwaters of the Missouri River: \nThe Missouri River officially starts at the confluence of the Jefferson and Madison in Missouri Headwaters State Park near Three Forks, Montana, and is joined by the Gallatin a mile (1.6 km) downstream. The Missouri then passes through Canyon Ferry Lake, a reservoir west of the Big Belt Mountains. Issuing from the mountains near Cascade, the river flows northeast to the city of Great Falls, where it drops over the Great Falls of the Missouri, a series of five substantial waterfalls. It then winds east through a scenic region of canyons and badlands known as the Missouri Breaks, receiving the Marias River from the west then widening into the Fort Peck Lake reservoir a few miles above the confluence with the Musselshell River. Farther on, the river passes through the Fort Peck Dam, and immediately downstream, the Milk River joins from the north.\n\nFlowing eastwards through the plains of eastern Montana, the Missouri receives the Poplar River from the north before crossing into North Dakota where the Yellowstone River, its greatest tributary by volume, joins from the southwest. At the confluence, the Yellowstone is actually the larger river. The Missouri then meanders east past Williston and into Lake Sakakawea, the reservoir formed by Garrison Dam. Below the dam the Missouri receives the Knife River from the west and flows south to Bismarck, the capital of North Dakota, where the Heart River joins from the west. It slows into the Lake Oahe reservoir just before the Cannonball River confluence. While it continues south, eventually reaching Oahe Dam in South Dakota, the Grand, Moreau and Cheyenne Rivers all join the Missouri from the west.\n\nThe Missouri makes a bend to the southeast as it winds through the Great Plains, receiving the Niobrara River and many smaller tributaries from the southwest. It then proceeds to form the boundary of South Dakota and Nebraska, then after being joined by the James River from the north, forms the Iowa–Nebraska boundary. At Sioux City the Big Sioux River comes in from the north. The Missouri flows south to the city of Omaha where it receives its longest tributary, the Platte River, from the west. Downstream, it begins to define the Nebraska–Missouri border, then flows between Missouri and Kansas. The Missouri swings east at Kansas City, where the Kansas River enters from the west, and so on into north-central Missouri. To the east of Kansas City, the Missouri receives, on the left side, the Grand River. It passes south of Columbia and receives the Osage and Gasconade Rivers from the south downstream of Jefferson City. The river then rounds the northern side of St. Louis to join the Mississippi River on the border between Missouri and Illinois.\n\nWith a drainage basin spanning ,\nthe Missouri River's catchment encompasses nearly one-sixth of the area of the United States or just over five percent of the continent of North America. Comparable to the size of the Canadian province of Quebec, the watershed encompasses most of the central Great Plains, stretching from the Rocky Mountains in the west to the Mississippi River Valley in the east and from the southern extreme of western Canada to the border of the Arkansas River watershed. Compared with the Mississippi River above their confluence, the Missouri is twice as long and drains an area three times as large. The Missouri accounts for 45 percent of the annual flow of the Mississippi past St. Louis, and as much as 70 percent in certain droughts.\n\nIn 1990, the Missouri River watershed was home to about 12 million people. This included the entire population of the U.S. state of Nebraska, parts of the U.S. states of Colorado, Iowa, Kansas, Minnesota, Missouri, Montana, North Dakota, South Dakota, and Wyoming, and small southern portions of the Canadian provinces of Alberta and Saskatchewan. The watershed's largest city is Denver, Colorado, with a population of more than six hundred thousand. Denver is the main city of the Front Range Urban Corridor whose cities had a combined population of over four million in 2005, making it the largest metropolitan area in the Missouri River basin. Other major population centers – mostly located in the southeastern portion of the watershed – include Omaha, Nebraska, situated north of the confluence of the Missouri and Platte Rivers; Kansas City, Missouri – Kansas City, Kansas, located at the confluence of the Missouri with the Kansas River; and the St. Louis metropolitan area, situated south of the Missouri River just below the latter's mouth, on the Mississippi. In contrast, the northwestern part of the watershed is sparsely populated. However, many northwestern cities, such as Billings, Montana, are among the fastest growing in the Missouri basin.\n\nWith more than under the plow, the Missouri River watershed includes roughly one-fourth of all the agricultural land in the United States, providing more than a third of the country's wheat, flax, barley and oats. However, only of farmland in the basin is irrigated. A further of the basin is devoted to the raising of livestock, mainly cattle. Forested areas of the watershed, mostly second-growth, total about . Urban areas, on the other hand, comprise less than of land. Most built-up areas are located along the main stem and a few major tributaries, including the Platte and Yellowstone Rivers.\nElevations in the watershed vary widely, ranging from just over at the Missouri's mouth to the summit of Mount Lincoln in central Colorado. The river itself drops a total of from Brower's Spring, the farthest source. Although the plains of the watershed have extremely little local vertical relief, the land rises about 10 feet per mile (1.9 m/km) from east to west. The elevation is less than at the eastern border of the watershed, but is over above sea level in many places at the base of the Rockies.\n\nThe Missouri's drainage basin has highly variable weather and rainfall patterns, Overall, the watershed is defined by a Continental climate with warm, wet summers and harsh, cold winters. Most of the watershed receives an average of of precipitation each year. However, the westernmost portions of the basin in the Rockies as well as southeastern regions in Missouri may receive as much as . The vast majority of precipitation occurs in winter, although the upper basin is known for short-lived but intense summer thunderstorms such as the one which produced the 1972 Black Hills flood through Rapid City, South Dakota. Winter temperatures in Montana, Wyoming and Colorado may drop as low as , while summer highs in Kansas and Missouri have reached at times.\n\nAs one of the continent's most significant river systems, the Missouri's drainage basin borders on many other major watersheds of the United States and Canada. The Continental Divide, running along the spine of the Rocky Mountains, forms most of the western border of the Missouri watershed. The Clark Fork and Snake River, both part of the Columbia River basin, drain the area west of the Rockies in Montana, Idaho and western Wyoming. The Columbia, Missouri and Colorado River watersheds meet at Three Waters Mountain in Wyoming's Wind River Range. South of there, the Missouri basin is bordered on the west by the drainage of the Green River, a tributary of the Colorado, then on the south by the mainstem of the Colorado. Both the Colorado and Columbia Rivers flow to the Pacific Ocean. However, a large endorheic drainage called the Great Divide Basin exists between the Missouri and Green watersheds in western Wyoming. This area is sometimes counted as part of the Missouri River watershed, even though its waters do not flow to either side of the Continental Divide.\n\nTo the north, the much lower Laurentian Divide separates the Missouri River watershed from those of the Oldman River, a tributary of the South Saskatchewan River, as well as the Souris, Sheyenne, and smaller tributaries of the Red River of the North. All of these streams are part of Canada's Nelson River drainage basin, which empties into Hudson Bay. There are also several large endorheic basins between the Missouri and Nelson watersheds in southern Alberta and Saskatchewan. The Minnesota and Des Moines Rivers, tributaries of the upper Mississippi, drain most of the area bordering the eastern side of the Missouri River basin. Finally, on the south, the Ozark Mountains and other low divides through central Missouri, Kansas and Colorado separate the Missouri watershed from those of the White River and Arkansas River, also tributaries of the Mississippi River.\n\nOver 95 significant tributaries and hundreds of smaller ones feed the Missouri River, with most of the larger ones coming in as the river draws close to the mouth. Most rivers and streams in the Missouri River basin flow from west to east, following the incline of the Great Plains; however, some eastern tributaries such as the James, Big Sioux and Grand River systems flow from north to south.\n\nThe Missouri's largest tributaries by runoff are the Yellowstone in Montana and Wyoming, the Platte in Wyoming, Colorado, and Nebraska, and the Kansas–Republican/Smoky Hill and Osage in Kansas and Missouri. Each of these tributaries drains an area greater than , and has an average discharge greater than . The Yellowstone River has the highest discharge, even though the Platte is longer and drains a larger area. In fact, the Yellowstone's flow is about – accounting for sixteen percent of total runoff in the Missouri basin and nearly double that of the Platte. On the other end of the scale is the tiny Roe River in Montana, which at long is one the world's shortest rivers.\n\nThe table on the right lists the ten longest tributaries of the Missouri, along with their respective catchment areas and flows. Length is measured to the hydrologic source, regardless of naming convention. The main stem of the Kansas River, for example, is long. However, including the longest headwaters tributaries, the Republican River and the Arikaree River, brings the total length to . Similar naming issues are encountered with the Platte River, whose longest tributary, the North Platte River, is more than twice as long as its mainstream.\n\nThe Missouri's headwaters above Three Forks extend much farther upstream than the main stem. Measured to the farthest source at Brower's Spring, the Jefferson River is long. Thus measured to its highest headwaters, the Missouri River stretches for . When combined with the lower Mississippi, the Missouri and its headwaters form part of the fourth-longest river system in the world, at .\n\nBy discharge, the Missouri is the ninth largest river of the United States, after the Mississippi, St. Lawrence, Ohio, Columbia, Niagara, Yukon, Detroit, and St. Clair. The latter two, however, are sometimes considered part of a strait between Lake Huron and Lake Erie. Among rivers of North America as a whole, the Missouri is thirteenth largest, after the Mississippi, Mackenzie, St. Lawrence, Ohio, Columbia, Niagara, Yukon, Detroit, St. Clair, Fraser, Slave, and Koksoak.\n\nAs the Missouri drains a predominantly semi-arid region, its discharge is much lower and more variable than other North American rivers of comparable length. Before the construction of dams, the river flooded twice each year – once in the \"April Rise\" or \"Spring Fresh\", with the melting of snow on the plains of the watershed, and in the \"June Rise\", caused by snowmelt and summer rainstorms in the Rocky Mountains. The latter was far more destructive, with the river increasing to over ten times its normal discharge in some years. The Missouri's discharge is affected by over 17,000 reservoirs with an aggregate capacity of some . By providing flood control, the reservoirs dramatically reduce peak flows and increase low flows. Evaporation from reservoirs significantly reduces the river's runoff, causing an annual loss of over from mainstem reservoirs alone.\n\nThe United States Geological Survey operates fifty-one stream gauges along the Missouri River. The river's average discharge at Bismarck, from the mouth, is . This is from a drainage area of , or 35% of the total river basin. At Kansas City, from the mouth, the river's average flow is . The river here drains about , representing about 91% of the entire basin.\n\nThe lowermost gage with a period of record greater than fifty years is at Hermann, Missouri – upstream of the mouth of the Missouri – where the average annual flow was from 1897 to 2010. About , or 98.7% of the watershed, lies above Hermann. The highest annual mean was in 1993, and the lowest was in 2006. Extremes of the flow vary even further. The largest discharge ever recorded was over on July 31, 1993, during a historic flood. The lowest, a mere – caused by the formation of an ice dam – was measured on December 23, 1963.\n\nThe Rocky Mountains of southwestern Montana at the headwaters of the Missouri River first rose in the Laramide Orogeny, a mountain-building episode that occurred from around 70 to 45 million years ago (the end of the Mesozoic through the early Cenozoic). This orogeny uplifted Cretaceous rocks along the western side of the Western Interior Seaway, a vast shallow sea that stretched from the Arctic Ocean to the Gulf of Mexico, and deposited the sediments that now underlie much of the drainage basin of the Missouri River.\nThis Laramide uplift caused the sea to retreat and laid the framework for a vast drainage system of rivers flowing from the Rocky and Appalachian Mountains, the predecessor of the modern-day Mississippi watershed. The Laramide Orogeny is essential to modern Missouri River hydrology, as snow and ice melt from the Rockies provide the majority of the flow in the Missouri and its tributaries.\n\nThe Missouri and many of its tributaries cross the Great Plains, flowing over or cutting into the Ogallala Group and older mid-Cenozoic sedimentary rocks. The lowest major Cenozoic unit, the White River Formation, was deposited between roughly 35 and 29 million years ago and consists of claystone, sandstone, limestone, and conglomerate. Channel sandstones and finer-grained overbank deposits of the fluvial Arikaree Group were deposited between 29 and 19 million years ago. The Miocene-age Ogallala and the slightly younger Pliocene-age Broadwater Formation deposited atop the Arikaree Group, and are formed from material eroded off of the Rocky Mountains during a time of increased generation of topographic relief; these formations stretch from the Rocky Mountains nearly to the Iowa border and give the Great Plains much of their gentle but persistent eastward tilt, and also constitute a major aquifer.\n\nImmediately before the Quaternary Ice Age, the Missouri River was likely split into three segments: an upper portion that drained northwards into Hudson Bay,\nand middle and lower sections that flowed eastward down the regional slope.\nAs the Earth plunged into the Ice Age, a pre-Illinoian (or possibly the Illinoian) glaciation diverted the Missouri River southeastwards towards its present confluence with the Mississippi and caused it to integrate into a single river system that cuts across the regional slope. In western Montana, the Missouri River is thought to have once flowed north then east around the Bear Paw Mountains. Sapphires are found in some spots along the river in western Montana. Advances of the continental ice sheets diverted the river and its tributaries, causing them to pool up into large temporary lakes such as Glacial Lakes Great Falls, Musselshell and others. As the lakes rose, the water in them often spilled across adjacent local drainage divides, creating now-abandoned channels and coulees including the Shonkin Sag, long. When the glaciers retreated, the Missouri flowed in a new course along the south side of the Bearpaws, and the lower part of the Milk River tributary took over the original main channel.\n\nThe Missouri's nickname, the \"Big Muddy\", was inspired by its enormous loads of sediment or silt – some of the largest of any North American river. In its pre-development state, the river transported some per year. The construction of dams and levees has drastically reduced this to in the present day. Much of this sediment is derived from the river's floodplain, also called the meander belt; every time the river changed course, it would erode tons of soil and rocks from its banks. However, damming and channeling the river has kept it from reaching its natural sediment sources along most of its course. Reservoirs along the Missouri trap roughly of sediment each year. Despite this, the river still transports more than half the total silt that empties into the Gulf of Mexico; the Mississippi River Delta, formed by sediment deposits at the mouth of the Mississippi, constitutes a majority of sediments carried by the Missouri.\n\nArchaeological evidence, especially in Missouri, suggests that human beings first inhabited the watershed of the Missouri River between 10,000 and 12,000 years ago at the end of the Pleistocene. During the end of the last glacial period, large migration of humans were taking place, such as those via the Bering land bridge between the Americas and Eurasia. Over centuries, the Missouri River formed one of these main migration paths. Most migratory groups that passed through the area eventually settled in the Ohio Valley and the lower Mississippi River Valley, but many, including the Mound builders, stayed along the Missouri, becoming the ancestors of the later Indigenous peoples of the Great Plains.\nIndigenous peoples of North America who have lived along the Missouri have historically had access to ample food, water, and shelter. Many migratory animals naturally inhabit the plains area. Before they were slaughtered by colonists, these animals, such as the buffalo, provided meat, clothing, and other everyday items; there were also great riparian areas in the river's floodplain that provided habitat for herbs and other staple foods. No written records from the tribes and peoples of the pre-European contact period exist because they did not yet use writing. According to the writings of early colonists, some of the major tribes along the Missouri River included the Otoe, Missouria, Omaha, Ponca, Brulé, Lakota, Arikara, Hidatsa, Mandan, Assiniboine, Gros Ventres and Blackfeet.\n\nIn this pre-colonial and early-colonial era, the Missouri river was used as a path of trade and transport, and the river and its tributaries often formed territorial boundaries. Most of the Indigenous peoples in the region at that time had semi-nomadic cultures, with many tribes maintaining different summer and winter camps. However, the center of Native American wealth and trade lay along the Missouri River in the Dakotas region on its great bend south. A large cluster of walled Mandan, Hidatsa and Arikara villages situated on bluffs and islands of the river was home to thousands, and later served as a market and trading post used by early French and British explorers and fur traders. Following the introduction of horses to Missouri River tribes, possibly from feral European-introduced populations, Natives' way of life changed dramatically. The use of the horse allowed them to travel greater distances, and thus facilitated hunting, communications and trade.\n\nOnce, tens of millions of American bison (commonly called buffalo), one of the keystone species of the Great Plains and the Ohio Valley, roamed the plains of the Missouri River basin. Most Native American nations in the basin relied heavily on the bison as a food source, and their hides and bones served to create other household items. In time, the species came to benefit from the indigenous peoples' periodic controlled burnings of the grasslands surrounding the Missouri to clear out old and dead growth. The large bison population of the region gave rise to the term \"great bison belt\", an area of rich annual grasslands that extended from Alaska to Mexico along the eastern flank of the Continental Divide. However, after the arrival of Europeans in North America, both the bison and the Native Americans saw a rapid decline in population. Massive over-hunting for sport by colonists eliminated bison populations east of the Mississippi River by 1833 and reduced the numbers in the Missouri basin to a mere few hundred. Foreign diseases brought by settlers, such as smallpox, raged across the land, decimating Native American populations. Left without their primary source of sustenance, many of the remaining indigenous people were forced onto resettlement areas and reservations, often at gunpoint.\n\nIn May 1673, the French explorers Louis Jolliet and Jacques Marquette left the settlement of St. Ignace on Lake Huron and traveled down the Wisconsin and Mississippi Rivers, aiming to reach the Pacific Ocean. In late June, Jolliet and Marquette became the first documented European discoverers of the Missouri River, which according to their journals was in full flood. \"I never saw anything more terrific,\" Jolliet wrote, \"a tangle of entire trees from the mouth of the Pekistanoui [Missouri] with such impetuosity that one could not attempt to cross it without great danger. The commotion was such that the water was made muddy by it and could not clear itself.\" They recorded \"Pekitanoui\" or \"Pekistanoui\" as the local name for the Missouri. However, the party never explored the Missouri beyond its mouth, nor did they linger in the area. In addition, they later learned that the Mississippi drained into the Gulf of Mexico and not the Pacific as they had originally presumed; the expedition turned back about short of the Gulf at the confluence of the Arkansas River with the Mississippi.\n\nIn 1682, France expanded its territorial claims in North America to include land on the western side of the Mississippi River, which included the lower portion of the Missouri. However, the Missouri itself remained formally unexplored until Étienne de Veniard, Sieur de Bourgmont commanded an expedition in 1714 that reached at least as far as the mouth of the Platte River. It is unclear exactly how far Bourgmont traveled beyond there; he described the blond-haired Mandans in his journals, so it is likely he reached as far as their villages in present-day North Dakota. Later that year, Bourgmont published \"The Route To Be Taken To Ascend The Missouri River\", the first known document to use the name \"Missouri River\"; many of the names he gave to tributaries, mostly for the native tribes that lived along them, are still in use today. The expedition's discoveries eventually found their way to cartographer Guillaume Delisle, who used the information to create a map of the lower Missouri. In 1718, Jean-Baptiste Le Moyne, Sieur de Bienville requested that the French government bestow upon Bourgmont the Cross of St. Louis because of his \"outstanding service to France\".\n\nBourgmont had in fact been in trouble with the French colonial authorities since 1706, when he deserted his post as commandant of Fort Detroit after poorly handling an attack by the Ottawa that resulted in thirty-one deaths. However, his reputation was enhanced in 1720 when the Pawnee – who had earlier been befriended by Bourgmont – massacred the Spanish Villasur expedition near present-day Columbus, Nebraska on the Missouri River and temporarily ending Spanish encroachment on French Louisiana.\n\nBourgmont established Fort Orleans, the first European settlement of any kind on the Missouri River, near present-day Brunswick, Missouri, in 1723. The following year Bourgmont led an expedition to enlist Comanche support against the Spanish, who continued to show interest in taking over the Missouri. In 1725 Bourgmont brought the chiefs of several Missouri River tribes to visit France. There he was raised to the rank of nobility and did not accompany the chiefs back to North America. Fort Orleans was either abandoned or its small contingent massacred by Native Americans in 1726.\n\nThe French and Indian War erupted when territorial disputes between France and Great Britain in North America reached a head in 1754. By 1763, France was defeated by the much greater strength of the British army and was forced to cede its Canadian possessions to the English and Louisiana to the Spanish in the Treaty of Paris, amounting to most of its colonial holdings in North America. Initially, the Spanish did not extensively explore the Missouri and let French traders continue their activities under license. However, this ended after news of the British Hudson's Bay Company incursions in the upper Missouri River watershed was brought back following an expedition by Jacques D'Eglise in the early 1790s. In 1795 the Spanish chartered the Company of Discoverers and Explorers of the Missouri, popularly referred to as the \"Missouri Company\", and offered a reward for the first person to reach the Pacific Ocean via the Missouri. In 1794 and 1795 expeditions led by Jean Baptiste Truteau and Antoine Simon Lecuyer de la Jonchšre did not even make it as far north as the Mandan villages in central North Dakota.\n\nArguably the most successful of the Missouri Company expeditions was that of James MacKay and John Evans. The two set out along the Missouri, and established Fort Charles about south of present-day Sioux City as a winter camp in 1795. At the Mandan villages in North Dakota, they expelled several British traders, and while talking to the populace they pinpointed the location of the Yellowstone River, which was called \"Roche Jaune\" (\"Yellow Rock\") by the French. Although MacKay and Evans failed to accomplish their original goal of reaching the Pacific, they did create the first accurate map of the upper Missouri River.\n\nIn 1795, the young United States and Spain signed Pinckney's Treaty, which recognized American rights to navigate the Mississippi River and store goods for export in New Orleans. Three years later, Spain revoked the treaty and in 1800 secretly returned Louisiana to Napoleonic France in the Third Treaty of San Ildefonso. This transfer was so secret that the Spanish continued to administer the territory. In 1801, Spain restored rights to use the Mississippi and New Orleans to the United States.\nFearing that the cutoffs could occur again, President Thomas Jefferson proposed to buy the port of New Orleans from France for $10 million. Instead, faced with a debt crisis, Napoleon offered to sell the entirety of Louisiana, including the Missouri River, for $15 million – amounting to less than 3¢ per acre. The deal was signed in 1803, doubling the size of the United States with the acquisition of the Louisiana Territory. In 1803, Jefferson instructed Meriwether Lewis to explore the Missouri and search for a water route to the Pacific Ocean. By then, it had been discovered that the Columbia River system, which drains into the Pacific, had a similar latitude as the headwaters of the Missouri River, and it was widely believed that a connection or short portage existed between the two. However, Spain balked at the takeover, citing that they had never formally returned Louisiana to the French. Spanish authorities warned Lewis not to take the journey and forbade him from seeing the MacKay and Evans map of the Missouri, although Lewis eventually managed to gain access to it.\n\nMeriwether Lewis and William Clark began their famed expedition in 1804 with a party of thirty-three people in three boats. Although they became the first Europeans to travel the entire length of the Missouri and reach the Pacific Ocean via the Columbia, they found no trace of the Northwest Passage. The maps made by Lewis and Clark, especially those of the Pacific Northwest region, provided a foundation for future explorers and emigrants. They also negotiated relations with numerous Native American tribes and wrote extensive reports on the climate, ecology and geology of the region. Many present-day names of geographic features in the upper Missouri basin originated from their expedition.\n\nAs early as the 18th century, fur trappers entered the extreme northern basin of the Missouri River in the hopes of finding populations of beaver and river otter, the sale of whose pelts drove the thriving North American fur trade. They came from many different places – some from the Canadian fur corporations at Hudson Bay, some from the Pacific Northwest (\"see also\": Maritime fur trade), and some from the midwestern United States. Most did not stay in the area for long, as they failed to find significant resources.\n\nThe first glowing reports of country rich with thousands of game animals came in 1806 when Meriwether Lewis and William Clark returned from their two-year expedition. Their journals described lands amply stocked with thousands of buffalo, beaver, and river otter; and also an abundant population of sea otters on the Pacific Northwest coast. In 1807, explorer Manuel Lisa organized an expedition which would lead to the explosive growth of the fur trade in the upper Missouri River country. Lisa and his crew traveled up the Missouri and Yellowstone Rivers, trading manufactured items in return for furs from local Native American tribes, and established a fort at the confluence of the Yellowstone and a tributary, the Bighorn, in southern Montana. Although the business started small, it quickly grew into a thriving trade.\n\nLisa's men started construction of Fort Raymond, which sat on a bluff overlooking the confluence of the Yellowstone and Bighorn, in the fall of 1807. The fort would serve primarily as a trading post for bartering with the Native Americans for furs. This method was unlike that of the Pacific Northwest fur trade, which involved trappers hired by the various fur enterprises, namely Hudson's Bay. Fort Raymond was later replaced by Fort Lisa at the confluence of the Missouri and Yellowstone in North Dakota; a second fort also called Fort Lisa was built downstream on the Missouri River in Nebraska. In 1809 the St. Louis Missouri Fur Company was founded by Lisa in conjunction with William Clark and Pierre Choteau, among others. In 1828, the American Fur Company founded Fort Union at the confluence of the Missouri and Yellowstone Rivers. Fort Union gradually became the main headquarters for the fur trade in the upper Missouri basin.\nFur trapping activities in the early 19th century encompassed nearly all of the Rocky Mountains on both the eastern and western slopes. Trappers of the Hudson's Bay Company, St. Louis Missouri Fur Company, American Fur Company, Rocky Mountain Fur Company, North West Company and other outfits worked thousands of streams in the Missouri watershed as well as the neighboring Columbia, Colorado, Arkansas, and Saskatchewan river systems. During this period, the trappers, also called mountain men, blazed trails through the wilderness that would later form the paths pioneers and settlers would travel by into the West. Transport of the thousands of beaver pelts required ships, providing one of the first large motives for river transport on the Missouri to start.\n\nAs the 1830s drew to a close, the fur industry slowly began to die as silk replaced beaver fur as a desirable clothing item. By this time, also, the beaver population of streams in the Rocky Mountains had been decimated by intense hunting. Furthermore, frequent Native American attacks on trading posts made it dangerous for employees of the fur companies. In some regions, the industry continued well into the 1840s, but in others such as the Platte River valley, declines of the beaver population contributed to an earlier demise. The fur trade finally disappeared in the Great Plains around 1850, with the primary center of industry shifting to the Mississippi Valley and central Canada. Despite the demise of the once-prosperous trade, however, its legacy led to the opening of the American West and a flood of settlers, farmers, ranchers, adventurers, hopefuls, financially bereft, and entrepreneurs took their place.\n\nThe river roughly defined the American frontier in the 19th century, particularly downstream from Kansas City, where it takes a sharp eastern turn into the heart of the state of Missouri, an area known as the Boonslick. As first area settled by Europeans along the river it was largely populated by slave-owning southerners following the Boone's Lick Road. The major trails for the opening of the American West all have their starting points on the river, including the California, Mormon, Oregon, and Santa Fe trails. The first westward leg of the Pony Express was a ferry across the Missouri at St. Joseph, Missouri. Similarly, most emigrants arrived at the eastern terminus of the First Transcontinental Railroad via a ferry ride across the Missouri between Council Bluffs, Iowa, and Omaha. The Hannibal Bridge became the first bridge to cross the Missouri River in 1869, and its location was a major reason why Kansas City became the largest city on the river upstream from its mouth at St. Louis.\n\nTrue to the then-ideal of Manifest Destiny, over 500,000 people set out from the river town of Independence, Missouri to their various destinations in the American West from the 1830s to the 1860s. These people had many reasons to embark on this strenuous year-long journey – economic crisis, and later gold strikes including the California Gold Rush, for example. For most, the route took them up the Missouri to Omaha, Nebraska, where they would set out along the Platte River, which flows from the Rocky Mountains in Wyoming and Colorado eastwards through the Great Plains. An early expedition led by Robert Stuart from 1812 to 1813 proved the Platte impossible to navigate by the dugout canoes they used, let alone the large sidewheelers and sternwheelers that would later ply the Missouri in increasing numbers. One explorer remarked that the Platte was \"too thick to drink, too thin to plow\". Nevertheless, the Platte provided an abundant and reliable source of water for the pioneers as they headed west. Covered wagons, popularly referred to as \"prairie schooners\", provided the primary means of transport until the beginning of regular boat service on the river in the 1850s.\n\nDuring the 1860s, gold strikes in Montana, Colorado, Wyoming, and northern Utah attracted another wave of hopefuls to the region. Although some freight was hauled overland, most transport to and from the gold fields was done through the Missouri and Kansas Rivers, as well as the Snake River in western Wyoming and the Bear River in Utah, Idaho, and Wyoming. It is estimated that of the passengers and freight hauled from the Midwest to Montana, over 80 percent were transported by boat, a journey that took 150 days in the upstream direction. A route more directly west into Colorado lay along the Kansas River and its tributary the Republican River as well as pair of smaller Colorado streams, Big Sandy Creek and the South Platte River, to near Denver. The gold rushes precipitated the decline of the Bozeman Trail as a popular emigration route, as it passed through land held by often-hostile Native Americans. Safer paths were blazed to the Great Salt Lake near Corinne, Utah during the gold rush period, which led to the large-scale settlement of the Rocky Mountains region and eastern Great Basin.\nAs settlers expanded their holdings into the Great Plains, they ran into land conflicts with Native American tribes. This resulted in frequent raids, massacres and armed conflicts, leading to the federal government creating multiple treaties with the Plains tribes, which generally involved establishing borders and reserving lands for the indigenous. As with many other treaties between the U.S. and Native Americans, they were soon broken, leading to huge wars. Over 1,000 battles, big and small, were fought between the U.S. military and Native Americans before the tribes were forced out of their land onto reservations.\n\nConflicts between natives and settlers over the opening of the Bozeman Trail in the Dakotas, Wyoming and Montana led to Red Cloud's War, in which the Lakota and Cheyenne fought against the U.S. Army. The fighting resulted in a complete Native American victory. In 1868, the Treaty of Fort Laramie was signed, which \"guaranteed\" the use of the Black Hills, Powder River Country and other regions surrounding the northern Missouri River to Native Americans without white intervention. The Missouri River was also a significant landmark as it divides northeastern Kansas from western Missouri; pro-slavery forces from Missouri would cross the river into Kansas and spark mayhem during Bleeding Kansas, leading to continued tension and hostility even today between Kansas and Missouri. Another significant military engagement on the Missouri River during this period was the 1861 Battle of Boonville, which did not affect Native Americans but rather was a turning point in the American Civil War that allowed the Union to seize control of transport on the river, discouraging the state of Missouri from joining the Confederacy.\n\nHowever, the peace and freedom of the Native Americans did not last for long. The Great Sioux War of 1876–77 was sparked when American miners discovered gold in the Black Hills of western South Dakota and eastern Wyoming. These lands were originally set aside for Native American use by the Treaty of Fort Laramie. When the settlers intruded onto the lands, they were attacked by Native Americans. U.S. troops were sent to the area to protect the miners, and drive out the natives from the new settlements. During this bloody period, both the Native Americans and the U.S. military won victories in major battles, resulting in the loss of nearly a thousand lives. The war eventually ended in an American victory, and the Black Hills were opened to settlement. Native Americans of that region were relocated to reservations in Wyoming and southeastern Montana.\n\nIn the late 19th and early 20th centuries, a great number of dams were built along the course of the Missouri, transforming 35 percent of the river into a chain of reservoirs. River development was stimulated by a variety of factors, first by growing demand for electricity in the rural northwestern parts of the basin, and also by floods and droughts that plagued rapidly growing agricultural and urban areas along the lower Missouri River. Small, privately owned hydroelectric projects have existed since the 1890s, but the large flood-control and storage dams that characterize the middle reaches of the river today were not constructed until the 1950s.\n\nBetween 1890 and 1940, five dams were built in the vicinity of Great Falls to generate power from the Great Falls of the Missouri, a chain of giant waterfalls formed by the river in its path through western Montana. Black Eagle Dam, built in 1891 on Black Eagle Falls, was the first dam of the Missouri. Replaced in 1926 with a more modern structure, the dam was little more than a small weir atop Black Eagle Falls, diverting part of the Missouri's flow into the Black Eagle power plant. The largest of the five dams, Ryan Dam, was built in 1913. The dam lies directly above the Big Falls, the largest waterfall of the Missouri.\nIn the same period, several private establishments – most notably the Montana Power Company – began to develop the Missouri River above Great Falls and below Helena for power generation. A small run-of-the river structure completed in 1898 near the present site of Canyon Ferry Dam became the second dam to be built on the Missouri. This rock-filled timber crib dam generated seven and a half megawatts of electricity for Helena and the surrounding countryside. The nearby steel Hauser Dam was finished in 1907, but failed in 1908 because of structural deficiencies, causing catastrophic flooding all the way downstream past Craig. At Great Falls, a section of the Black Eagle Dam was dynamited to save nearby factories from inundation. Hauser was rebuilt in 1910 as a concrete gravity structure, and stands to this day.\n\nHolter Dam, about downstream of Helena, was the third hydroelectric dam built on this stretch of the Missouri River. When completed in 1918 by the Montana Power Company and the United Missouri River Power Company, its reservoir flooded the Gates of the Mountains, a limestone canyon which Meriwether Lewis described as \"the most remarkable clifts that we have yet seen ... the tow[er]ing and projecting rocks in many places seem ready to tumble on us.\" In 1949, the U.S. Bureau of Reclamation (USBR) began construction on the modern Canyon Ferry Dam to provide flood control to the Great Falls area. By 1954, the rising waters of Canyon Ferry Lake submerged the old 1898 dam, whose powerhouse still stands underwater about upstream of the present-day dam.\n\n\"[The Missouri's temperament was] uncertain as the actions of a jury or the state of a woman's mind.\" \n\nThe Missouri basin suffered a series of catastrophic floods around the turn of the 20th century, most notably in 1844, 1881, and 1926–1927. In 1940, as part of the Great Depression-era New Deal, the U.S. Army Corps of Engineers (USACE) completed Fort Peck Dam in Montana. Construction of this massive public works project provided jobs for more than 50,000 laborers during the Depression and was a major step in providing flood control to the lower half of the Missouri River. However, Fort Peck only controls runoff from 11 percent of the Missouri River watershed, and had little effect on a severe snowmelt flood that struck the lower basin three years later. This event was particularly destructive as it submerged manufacturing plants in Omaha and Kansas City, greatly delaying shipments of military supplies in World War II.\n\nFlooding damages on the Mississippi–Missouri river system were one of the primary reasons for which Congress passed the Flood Control Act of 1944, opening the way for the USACE to develop the Missouri on a massive scale. The 1944 act authorized the Pick–Sloan Missouri Basin Program (Pick–Sloan Plan), which was a composite of two widely varying proposals. The Pick plan, with an emphasis on flood control and hydroelectric power, called for the construction of large storage dams along the main stem of the Missouri. The Sloan plan, which stressed the development of local irrigation, included provisions for roughly 85 smaller dams on tributaries.\n\nIn the early stages of Pick–Sloan development, tentative plans were made to build a low dam on the Missouri at Riverdale, North Dakota and 27 smaller dams on the Yellowstone River and its tributaries. This was met with controversy from inhabitants of the Yellowstone basin, and eventually the USBR proposed a solution: to greatly increase the size of the proposed dam at Riverdale – today's Garrison Dam, thus replacing the storage that would have been provided by the Yellowstone dams. Because of this decision, the Yellowstone is now the longest free-flowing river in the contiguous United States. In the 1950s, construction commenced on the five mainstem dams – Garrison, Oahe, Big Bend, Fort Randall and Gavins Point – proposed under the Pick-Sloan Plan. Along with Fort Peck, which was integrated as a unit of the Pick-Sloan Plan in the 1940s, these dams now form what is known as the Missouri River Mainstem System.\n\nThe flooding of lands along the Missouri River heavily impacted Native American groups whose reservations included fertile bottomlands and floodplains, especially in the arid Dakotas where it was some of the only good farmland they had. These consequences were pronounced in North Dakota's Fort Berthold Indian Reservation, where of land was taken by the construction of Garrison Dam. The Mandan, Hidatsa and Arikara/Sanish tribes sued the federal government on the basis of the 1851 Treaty of Fort Laramie which provided that reservation land could not be taken without the consent of both the tribes and Congress. After a lengthy legal battle the tribes were coerced in 1947 to accept a $5.1 million ($55 million today) settlement for the land, just $33 per acre. In 1949 this was increased to $12.6 million. The tribes were even denied the right to use the reservoir shore \"for grazing, hunting, fishing, and other purposes.\"\n\nThe six dams of the Mainstem System, chiefly Fort Peck, Garrison and Oahe, are among the largest dams in the world by volume; their sprawling reservoirs also rank within the biggest of the nation. Holding up to in total, the six reservoirs can store more than three years' worth of the river's flow as measured below Gavins Point, the lowermost dam. This capacity makes it the largest reservoir system in the United States and one of the largest in North America. In addition to storing irrigation water, the system also includes an annual flood-control reservation of . Mainstem power plants generate about 9.3 billion KWh annually – equal to a constant output of almost 1,100 megawatts. Along with nearly 100 smaller dams on tributaries, namely the Bighorn, Platte, Kansas, and Osage Rivers, the system provides irrigation water to nearly of land.\n\nThe table at left lists statistics of all fifteen dams on the Missouri River, ordered downstream. Many of the run-of-the-river dams on the Missouri (marked in yellow) form very small impoundments which may or may not have been given names; those unnamed are left blank. All dams are on the upper half of the river above Sioux City; the lower river is uninterrupted due to its longstanding use as a shipping channel.\n\n\"[Missouri River shipping] never achieved its expectations. Even under the very best of circumstances, it was never a huge industry.\"\n\nBoat travel on the Missouri started with the wood-framed canoes and bull boats of the Native Americans, which were used for thousands of years before the introduction of larger craft to the river upon colonization of the Great Plains. The first steamboat on the Missouri was the \"Independence\", which started running between St. Louis and Keytesville, Missouri around 1819. By the 1830s, large mail and freight-carrying vessels were running regularly between Kansas City and St. Louis, and many traveled even farther upstream. A handful, such as the \"Western Engineer\" and the \"Yellowstone\", were able to make it up the river as far as eastern Montana.\n\nDuring the early 19th century, at the height of the fur trade, steamboats and keelboats began traveling nearly the whole length of the Missouri from Montana's rugged Missouri Breaks to the mouth, carrying beaver and buffalo furs to and from the areas that the trappers frequented. This resulted in the development of the Missouri River mackinaw, which specialized in carrying furs. Since these boats could only travel downriver, they were dismantled and sold for lumber upon their arrival at St. Louis.\n\nWater transport increased through the 1850s with multiple craft ferrying pioneers, emigrants and miners; many of these runs were from St. Louis or Independence to near Omaha. There, most of these people would set out overland along the large but shallow and unnavigable Platte River, which was described by pioneers as \"a mile wide and an inch deep\" and \"the most magnificent and useless of rivers\". Steamboat navigation peaked in 1858 with over 130 boats operating full-time on the Missouri, with many more smaller vessels. Many of the earlier vessels were built on the Ohio River before being transferred to the Missouri. Side-wheeler steamboats were preferred over the larger sternwheelers used on the Mississippi and Ohio because of their greater maneuverability.\n\nThe industry's success, however, did not guarantee safety. In the early decades before the river's flow was controlled by man, its sketchy rises and falls and its massive amounts of sediment, which prevented a clear view of the bottom, wrecked some 300 vessels. Because of the dangers of navigating the Missouri River, the average ship's lifespan was short, only about four years. The development of the Transcontinental and Northern Pacific Railroads marked the beginning of the end of steamboat commerce on the Missouri. Outcompeted by trains, the number of boats slowly dwindled, until there was almost nothing left by the 1890s. Transport of agricultural and mining products by barge, however, saw a revival in the early twentieth century.\n\nSince the beginning of the 20th century, the Missouri River has been extensively engineered for water transport purposes, and about 32 percent of the river now flows through artificially straightened channels. In 1912, the USACE was authorized to maintain the Missouri to a depth of from the Port of Kansas City to the mouth, a distance of . This was accomplished by constructing levees and wing dams to direct the river's flow into a straight, narrow channel and prevent sedimentation. In 1925, the USACE began a project to widen the river's navigation channel to ; two years later, they began dredging a deep-water channel from Kansas City to Sioux City. These modifications have reduced the river's length from some in the late 19th century to in the present day.\nConstruction of dams on the Missouri under the Pick-Sloan Plan in the mid-twentieth century was the final step in aiding navigation. The large reservoirs of the Mainstem System help provide a dependable flow to maintain the navigation channel year-round, and are capable of halting most of the river's annual freshets. However, high and low water cycles of the Missouri – notably the protracted early-21st-century drought in the Missouri River basin and historic floods in 1993 and 2011 – are difficult for even the massive Mainstem System reservoirs to control.\n\nIn 1945, the USACE began the Missouri River Bank Stabilization and Navigation Project, which would permanently increase the river's navigation channel to a width of and a depth of . During work that continues to this day, the navigation channel from Sioux City to St. Louis has been controlled by building rock dikes to direct the river's flow and scour out sediments, sealing and cutting off meanders and side channels, and dredging the riverbed. However, the Missouri has often resisted the efforts of the USACE to control its depth. In 2006, several U.S. Coast Guard boats ran aground in the Missouri River because the navigation channel had been severely silted. The USACE was blamed for failing to maintain the channel to the minimum depth.\n\nIn 1929, the Missouri River Navigation Commission estimated the total amount of goods shipped on the river annually at 15 million tons (13.6 million metric tons), providing widespread consensus for the creation of a navigation channel. However, shipping traffic has since been far lower than expected – shipments of commodities including produce, manufactured items, lumber, and oil averaged only 683,000 tons (616,000 t) per year from 1994 to 2006.\n\nBy tonnage of transported material, Missouri is by far the largest user of the river accounting for 83 percent of river traffic, while Kansas has 12 percent, Nebraska three percent and Iowa two percent. Almost all of the barge traffic on the Missouri River ships sand and gravel dredged from the lower of the river; the remaining portion of the shipping channel now sees little to no use by commercial vessels.\n\nFor navigation purposes, the Missouri River is divided into two main sections. The Upper Missouri River is north of Gavins Point Dam, the last hydroelectric dam of fifteen on the river, just upstream from Sioux City, Iowa. The Lower Missouri River is the of river below Gavins Point until it meets the Mississippi just above St. Louis. The Lower Missouri River has no hydroelectric dams or locks but it has a plethora of wing dams that enable barge traffic by directing the flow of the river into a , channel. These wing dams have been put in place by and are maintained by the U.S. Army Corps of Engineers, and there currently are no plans to construct any locks to replace these wing dams on the Missouri River.\n\nTonnage of goods shipped by barges on the Missouri River has seen a serious decline from the 1960s to the present. In the 1960s, the USACE predicted an increase to per year by 2000, but instead the opposite has happened. The amount of goods plunged from in 1977 to just in 2000. One of the largest drops has been in agricultural products, especially wheat. Part of the reason is that irrigated land along the Missouri has only been developed to a fraction of its potential. In 2006, barges on the Missouri hauled only of products which is equal to the amount of \"daily\" freight traffic on the Mississippi.\n\nDrought conditions in the early 21st century and competition from other modes of transport – mainly railroads – are the primary reason for decreasing river traffic on the Missouri. The failure of the USACE to consistently maintain the navigation channel has also hampered the industry. Currently, efforts are being made to revive the shipping industry on the Missouri River, because of the efficiency and cheapness of river transport to haul agricultural products, and the overcrowding of alternative transportation routes. Solutions such as expanding the navigation channel and releasing more water from reservoirs during the peak of the navigation season are being considered. Drought conditions lifted in 2010, in which about were barged on the Missouri, representing the first significant increase in shipments since 2000. However, flooding in 2011 closed record stretches of the river to boat traffic – \"wash[ing] away hopes for a bounce-back year.\"\n\nThere are no lock and dams on the lower Missouri River, but there are plenty of wing dams that jettie out into the river and make it harder for barges to navigate. In contrast, the upper Mississippi has 29 locks and dams and averaged 61.3 million tons of cargo annually from 2008 to 2011, and its locks are closed in the winter.\n\nHistorically, the thousands of square miles that comprised the floodplain of the Missouri River supported a wide range of plant and animal species. Biodiversity generally increased proceeding downstream from the cold, subalpine headwaters in Montana to the temperate, moist climate of Missouri. Today, the river's riparian zone consists primarily of cottonwoods, willows and sycamores, with several other types of trees such as maple and ash. Average tree height generally increases farther from the riverbanks for a limited distance, as land adjacent to the river is vulnerable to soil erosion during floods. Because of its large sediment concentrations, the Missouri does not support many aquatic invertebrates. However, the basin does support about 300 species of birds and 150 species of fish, some of which are endangered such as the pallid sturgeon. The Missouri's aquatic and riparian habitats also support several species of mammals, such as minks, river otters, beavers, muskrats, and raccoons.\n\nThe World Wide Fund For Nature divides the Missouri River watershed into three freshwater ecoregions: the Upper Missouri, Lower Missouri and Central Prairie. The Upper Missouri, roughly encompassing the area within Montana, Wyoming, southern Alberta and Saskatchewan, and North Dakota, comprises mainly semiarid shrub-steppe grasslands with sparse biodiversity because of Ice Age glaciations. There are no known endemic species within the region. Except for the headwaters in the Rockies, there is little precipitation in this part of the watershed. The Middle Missouri ecoregion, extending through Colorado, southwestern Minnesota, northern Kansas, Nebraska, and parts of Wyoming and Iowa, has greater rainfall and is characterized by temperate forests and grasslands. Plant life is more diverse in the Middle Missouri, which is also home to about twice as many animal species. Finally, the Central Prairie ecoregion is situated on the lower part of the Missouri, encompassing all or parts of Missouri, Kansas, Oklahoma and Arkansas. Despite large seasonal temperature fluctuations, this region has the greatest diversity of plants and animals of the three. Thirteen species of crayfish are endemic to the lower Missouri.\n\nSince the beginning of river commerce and industrial development in the 1800s, the Missouri has been severely polluted and its water quality degraded by human activity. Most of the river's floodplain habitat is long gone, replaced by irrigated agricultural land. Development of the floodplain has led to increasing numbers of people and infrastructure within areas at high risk of inundation. Levees have been constructed along more than a third of the river in order to keep floodwater within the channel, but with the consequences of faster stream velocity and a resulting increase of peak flows in downstream areas. Fertilizer runoff, which causes elevated levels of nitrogen and other nutrients, is a major problem along the Missouri River, especially in Iowa and Missouri. This form of pollution also heavily affects the upper Mississippi, Illinois and Ohio Rivers. Low oxygen levels in rivers and the vast Gulf of Mexico dead zone at the end of the Mississippi Delta are both results of high nutrient concentrations in the Missouri and other tributaries of the Mississippi.\nChannelization of the lower Missouri waters has made the river narrower, deeper and less accessible to riparian flora and fauna. Numerous dams and bank stabilization projects have been constructed to facilitate the conversion of of Missouri River floodplain to agricultural land. Channel control has significantly reduced the volume of sediment transported downstream by the river and eliminated critical habitat for fish, birds and amphibians. By the early 21st century, declines in populations of native species prompted the U.S. Fish and Wildlife Service to issue a biological opinion recommending restoration of river habitats for federally endangered bird and fish species.\n\nThe USACE began work on ecosystem restoration projects along the lower Missouri River in the early 21st century. Because of the low use of the shipping channel in the lower Missouri maintained by the USACE, it is now considered feasible to remove some of the levees, dikes, and wing dams that constrict the river's flow, thus allowing it to naturally restore its banks. By 2001, there were of riverside floodplain undergoing active restoration.\n\nRestoration projects have re-mobilized some of the sediments that had been trapped behind bank stabilization structures, prompting concerns of exacerbated nutrient and sediment pollution locally and downstream in the northern Gulf of Mexico. A 2010 National Research Council report assessed the roles of sediment in the Missouri River, evaluating current habitat restoration strategies and alternative ways to manage sediment. The report found that a better understanding of sediment processes in the Missouri River, including the creation of a \"sediment budget\" – an accounting of sediment transport, erosion, and deposition volumes for the length of the Missouri River – would provide a foundation for projects to improve water quality standards and protect endangered species.\n\nSeveral sections of the Missouri River were added to the National Wild and Scenic Rivers System from Fort Benton to Robinson Bridge, Gavins Point Dam to Ponca State Park and Fort Randall Dam to Lewis and Clark Lake. A total of of the river were designated including of wild river and of scenic river in Montana. of the river is listed as recreational under the National Wild and Scenic Rivers System.\n\nWith over of open water, the six reservoirs of the Missouri River Mainstem System provide some of the main recreational areas within the basin. Visitation has increased from 10 million visitor-hours in the mid-1960s to over 60 million visitor-hours in 1990. Development of visitor facilities was spurred by the Federal Water Project Recreation Act of 1965, which required the USACE to build and maintain boat ramps, campgrounds and other public facilities along major reservoirs. Recreational use of Missouri River reservoirs is estimated to contribute $85–100 million to the regional economy each year.\n\nThe Lewis and Clark National Historic Trail, some long, follows nearly the entire Missouri River from its mouth to its source, retracing the route of the Lewis and Clark Expedition. Extending from Wood River, Illinois, in the east, to Astoria, Oregon, in the west, it also follows portions of the Mississippi and Columbia Rivers. The trail, which spans through eleven U.S. states, is maintained by various federal and state government agencies; it passes through some 100 historic sites, notably archaeological locations including the Knife River Indian Villages National Historic Site.\n\nParts of the river itself are designated for recreational or preservational use. The Missouri National Recreational River consists of portions of the Missouri downstream from Fort Randall and Gavins Point Dams that total . These reaches exhibit islands, meanders, sandbars, underwater rocks, riffles, snags, and other once-common features of the lower river that have now disappeared under reservoirs or have been destroyed by channeling. About forty-five steamboat wrecks are scattered along these reaches of the river.\n\nDownstream from Great Falls, Montana, about of the river course through a rugged series of canyons and badlands known as the Missouri Breaks. This part of the river, designated a U.S. National Wild and Scenic River in 1976, flows within the Upper Missouri Breaks National Monument, a preserve comprising steep cliffs, deep gorges, arid plains, badlands, archaeological sites, and whitewater rapids on the Missouri itself. The preserve includes a wide variety of plant and animal life; recreational activities include boating, rafting, hiking and wildlife observation.\n\nIn north-central Montana, some along over of the Missouri River, centering on Fort Peck Lake, comprise the Charles M. Russell National Wildlife Refuge. The wildlife refuge consists of a native northern Great Plains ecosystem that has not been heavily affected by human development, except for the construction of Fort Peck Dam. Although there are few designated trails, the whole preserve is open to hiking and camping.\n\nMany U.S. national parks, such as Glacier National Park, Rocky Mountain National Park, Yellowstone National Park and Badlands National Park are, at least partially, in the watershed. Parts of other rivers in the basin are set aside for preservation and recreational use – notably the Niobrara National Scenic River, which is a protected stretch of the Niobrara River, one of the Missouri's longest tributaries. The Missouri flows through or past many National Historic Landmarks, which include Three Forks of the Missouri, Fort Benton, Montana, Big Hidatsa Village Site, Fort Atkinson, Nebraska and Arrow Rock Historic District.\n\n\n\n\n"}
{"id": "19593", "url": "https://en.wikipedia.org/wiki?curid=19593", "title": "Missouria", "text": "Missouria\n\nThe Missouria or Missouri (in their own language, Niúachi, also spelled Niutachi) are a Native American tribe that originated in the Great Lakes region of United States before European contact. The tribe belongs to the Chiwere division of the Siouan language family, together with the Iowa and Otoe.\n\nHistorically, the tribe lived in bands near the mouth of the Grand River at its confluence with the Missouri River; the mouth of the Missouri at its confluence with the Mississippi River, and in present-day Saline County, Missouri. Since Indian removal, today they live primarily in Oklahoma. They are federally recognized as the Otoe-Missouria Tribe of Indians, based in Red Rock, Oklahoma.\n\nFrench colonists adapted a form of the Illinois language-name for the people: \"Wimihsoorita\". Their name means \"One who has dugout canoes\". In their own Siouan language, the Missouri call themselves \"Niúachi\", also spelled \"Niutachi\", meaning \"People of the River Mouth.\" The Osage called them the \"Waçux¢a,\" and the Quapaw called them the \"Wa-ju'-xd¢ǎ.\"\n\nThe state of Missouri and the Missouri River are named for the tribe.\n\nThe tribe's oral history tells that they once lived north of the Great Lakes. They began migrating south in the 16th century. By 1600, the Missouria lived near the confluence of the Grand and Missouri rivers, where they settled through the 18th century. Their tradition says that they split from the Otoe tribe, which belongs to the same Chiwere branch of the Siouan language, because of a love affair between the children of two tribal chiefs.\n\nThe 17th century brought hardships to the Missouria. The Sauk and Fox frequently attacked them. Their society was even more disrupted by the high fatalities from epidemics of smallpox and other Eurasian infectious diseases that accompanied contact with Europeans. The French explorer Jacques Marquette contacted the tribe in 1673 and paved the way for trade with the French.\n\nThe Missouria migrated west of the Missouri River into Osage territory. During this time, they acquired horses and hunted buffalo. The French explorer Étienne de Veniard, Sieur de Bourgmont visited the people in the early 1720s. He married the daughter of a Missouria chief. They settled nearby, and Veniard created alliances with the people. He built Fort Orleans in 1723 as a trading post near present-day Brunswick, Missouri. It was occupied until 1726.\n\nIn 1730 an attack by the Sauk/Fox tribe nearly destroyed the Missouria, killing hundreds. Most survivors reunited with the Otoe, while some joined the Osage and Kansa. After a smallpox outbreak in 1829, fewer than 100 Missouria survived, and they all joined the Otoe.\n\nThey signed treaties with the US government in 1830 and 1854 to cede their lands in Missouri. They relocated to the Otoe-Missouria reservation, created on the Big Blue River at the Kansas-Nebraska border. The US pressured the two tribes into ceding more lands in 1876 and 1881.\n\nIn 1880 the tribes split into two factions, the Coyote, who were traditionalists, and the Quakers, who were assimilationists. The Coyote settled on the Iowa Reservation in Indian Territory. The Quakers negotiated a small separate reservation in Indian Territory. By 1890 most of the Coyote band rejoined the Quakers on their reservation. Under the Dawes Act, by 1907 members of the tribes were registered and allotted individual plots of land per household. The US declared any excess communal land of the tribe as \"surplus\" and sold it to European-American settlers. The tribe merged with the Otoe tribe.\n\nThe Curtis Act required the disbanding of tribal courts and governments in order to assimilate the people and prepare the territory for statehood, but the tribe created their own court system in 1900. The Missouria were primarily farmers in the early 20th century. After oil was discovered on their lands in 1912, the US government forced many of the tribe off their allotments.\n\nAccording to the enthnographer James Mooney, the population of the tribe was about 200 families in 1702; 1000 people in 1780; 300 in 1805; 80 in 1829, when they were living with the Otoe; and 13 in 1910. Since then, their population numbers are combined with those of the Otoe.\n\n\n"}
{"id": "19594", "url": "https://en.wikipedia.org/wiki?curid=19594", "title": "Missile", "text": "Missile\n\nIn modern language, a missile, also known as a guided missile, is a guided self-propelled system, as opposed to an unguided self-propelled munition, referred to as a rocket (although these too can also be guided). Missiles have four system components: targeting or missile guidance, flight system, engine, and warhead. Missiles come in types adapted for different purposes: surface-to-surface and air-to-surface missiles (ballistic, cruise, anti-ship, anti-tank, etc.), surface-to-air missiles (and anti-ballistic), air-to-air missiles, and anti-satellite weapons. All known existing missiles are designed to be propelled during powered flight by chemical reactions inside a rocket engine, jet engine, or other type of engine. Non-self-propelled airborne explosive devices are generally referred to as shells and usually have a shorter range than missiles.\n\nIn ordinary British-English usage predating guided weapons, a missile is such as objects thrown at players by rowdy spectators at a sporting event.\n\nThe first missiles to be used operationally were a series of missiles developed by Nazi Germany in World War II. Most famous of these are the V-1 flying bomb and V-2 rocket, both of which used a simple mechanical autopilot to keep the missile flying along a pre-chosen route. Less well known were a series of anti-shipping and anti-aircraft missiles, typically based on a simple radio control (command guidance) system directed by the operator. However, these early systems in World War II were only built in small numbers.\n\nGuided missiles have a number of different system components:\n\nThe most common method of guidance is to use some form of radiation, such as infrared, lasers or radio waves, to guide the missile onto its target. This radiation may emanate from the target (such as the heat of an engine or the radio waves from an enemy radar), it may be provided by the missile itself (such as a radar), or it may be provided by a friendly third party (such as the radar of the launch vehicle/platform, or a laser designator operated by friendly infantry). The first two are often known as fire-and-forget as they need no further support or control from the launch vehicle/platform in order to function. Another method is to use a TV guidance, with a visible light or infrared picture produced in order to see the target. The picture may be used either by a human operator who steering the missile onto its target or by a computer doing much the same job. One of the more bizarre guidance methods instead used a pigeon to steer a missile to its target. Some missiles also have a home-on-jam capability to guide itself to a radar-emitting source. Many missiles use a combination of two or more of the methods to improve accuracy and the chances of a successful engagement.\n\nAnother method is to target the missile by knowing the location of the target and using a guidance system such as INS, TERCOM or satellite guidance. This guidance system guides the missile by knowing the missile's current position and the position of the target, and then calculating a course between them. This job can also be performed somewhat crudely by a human operator who can see the target and the missile and guide it using either cable- or radio-based remote control, or by an automatic system that can simultaneously track the target and the missile.\nFurthermore, some missiles use initial targeting, sending them to a target area, where they will switch to primary targeting, using either radar or IR targeting to acquire the target.\n\nWhether a guided missile uses a targeting system, a guidance system or both, it needs a flight system. The flight system uses the data from the targeting or guidance system to maneuver the missile in flight, allowing it to counter inaccuracies in the missile or to follow a moving target. There are two main systems: vectored thrust (for missiles that are powered throughout the guidance phase of their flight) and aerodynamic maneuvering (wings, fins, canard (aeronautics), etc.).\n\nMissiles are powered by an engine, generally either a type of rocket engine or jet engine. Rockets are generally of the solid propellant type for ease of maintenance and fast deployment, although some larger ballistic missiles use liquid-propellant rockets. Jet engines are generally used in cruise missiles, most commonly of the turbojet type, due to its relative simplicity and low frontal area. Turbofans and ramjets are the only other common forms of jet engine propulsion, although any type of engine could theoretically be used. Long-range missiles may have multiple engine stages, particularly in those launched from the surface. These stages may all be of similar types or may include a mix of engine types − for example, surface-launched cruise missiles often have a rocket booster for launching and a jet engine for sustained flight.\n\nSome missiles may have additional propulsion from another source at launch; for example, the V1 was launched by a catapult, and the MGM-51 Shillelagh was fired out of a tank gun (using a smaller charge than would be used for a shell).\n\nMissiles generally have one or more explosive warheads, although other weapon types may also be used. The warheads of a missile provide its primary destructive power (many missiles have extensive secondary destructive power due to the high kinetic energy of the weapon and unburnt fuel that may be on board). Warheads are most commonly of the high explosive type, often employing shaped charges to exploit the accuracy of a guided weapon to destroy hardened targets. Other warhead types include submunitions, incendiaries, nuclear weapons, chemical, biological or radiological weapons or kinetic energy penetrators.\nWarheadless missiles are often used for testing and training purposes.\n\nMissiles are generally categorized by their launch platform and intended target. In broadest terms, these will either be surface (ground or water) or air, and then sub-categorized by range and the exact target type (such as anti-tank or anti-ship). Many weapons are designed to be launched from both surface or the air, and a few are designed to attack either surface or air targets (such as the ADATS missile). Most weapons require some modification in order to be launched from the air or surface, such as adding boosters to the surface-launched version.\n\nAfter the boost stage, ballistic missiles follow a trajectory mainly determined by ballistics. The guidance is for relatively small deviations from that.\n\nBallistic missiles are largely used for land attack missions. Although normally associated with nuclear weapons, some conventionally armed ballistic missiles are in service, such as MGM-140 ATACMS. The V2 had demonstrated that a ballistic missile could deliver a warhead to a target city with no possibility of interception, and the introduction of nuclear weapons meant it could efficiently do damage when it arrived. The accuracy of these systems was fairly poor, but post-war development by most military forces improved the basic Inertial navigation system concept to the point where it could be used as the guidance system on Intercontinental ballistic missiles flying thousands of kilometers. Today, the ballistic missile represents the only strategic deterrent in most military forces; however, some ballistic missiles are being adapted for conventional roles, such as the Russian Iskander or the Chinese DF-21D anti-ship ballistic missile. Ballistic missiles are primarily surface-launched from mobile launchers, silos, ships or submarines, with air launch being theoretically possible with a weapon such as the cancelled Skybolt missile.\n\nThe Russian Topol M (SS-27 Sickle B) is the fastest (7,320 m/s) missile currently in service.\n\nThe V1 had been successfully intercepted during World War II, but this did not make the cruise missile concept entirely useless. After the war, the US deployed a small number of nuclear-armed cruise missiles in Germany, but these were considered to be of limited usefulness. Continued research into much longer-ranged and faster versions led to the US's SM-64 Navaho and its Soviet counterparts, the Burya and Buran cruise missile. However, these were rendered largely obsolete by the ICBM, and none were used operationally. Shorter-range developments have become widely used as highly accurate attack systems, such as the US Tomahawk missile and Russian Kh-55 . Cruise missiles are generally further divided into subsonic or supersonic weapons - supersonic weapons such as BrahMos are difficult to shoot down, whereas subsonic weapons tend to be much lighter and cheaper allowing more to be fired.\n\nCruise missiles are generally associated with land-attack operations, but also have an important role as anti-shipping weapons. They are primarily launched from air, sea or submarine platforms in both roles, although land-based launchers also exist.\n\nAnother major German missile development project was the anti-shipping class (such as the Fritz X and Henschel Hs 293), intended to stop any attempt at a cross-channel invasion. However, the British were able to render their systems useless by jamming their radios, and missiles with wire guidance were not ready by D-Day. After the war, the anti-shipping class slowly developed and became a major class in the 1960s with the introduction of the low-flying jet- or rocket-powered cruise missiles known as \"sea-skimmers\". These became famous during the Falklands War, when an Argentine Exocet missile sank a Royal Navy destroyer.\n\nA number of anti-submarine missiles also exist; these generally use the missile in order to deliver another weapon system such as a torpedo or depth charge to the location of the submarine, at which point the other weapon will conduct the underwater phase of the mission.\n\nBy the end of WWII, all forces had widely introduced unguided rockets using High-explosive anti-tank warheads as their major anti-tank weapon (see Panzerfaust, Bazooka). However, these had a limited useful range of 100 m or so, and the Germans were looking to extend this with the use of a missile using wire guidance, the X-7. After the war, this became a major design class in the later 1950s and, by the 1960s, had developed into practically the only non-tank anti-tank system in general use. During the 1973 Yom Kippur War between Israel and Egypt, the 9M14 Malyutka (aka \"Sagger\") man-portable anti-tank missile proved potent against Israeli tanks. While other guidance systems have been tried, the basic reliability of wire guidance means this will remain the primary means of controlling anti-tank missiles in the near future. Anti-tank missiles may be launched from aircraft, vehicles or by ground troops in the case of smaller weapons.\n\nBy 1944, US and British air forces were sending huge air fleets over occupied Europe, increasing the pressure on the Luftwaffe day and night fighter forces. The Germans were keen to get some sort of useful ground-based anti-aircraft system into operation. Several systems were under development, but none had reached operational status before the war's end. The US Navy also started missile research to deal with the Kamikaze threat. By 1950, systems based on this early research started to reach operational service, including the US Army's MIM-3 Nike Ajax and the Navy's \"3T's\" (Talos, Terrier, Tartar), soon followed by the Soviet S-25 Berkut and S-75 Dvina and French and British systems. Anti-aircraft weapons exist for virtually every possible launch platform, with surface-launched systems ranging from huge, self-propelled or ship-mounted launchers to man-portable systems. Subsurface-to-air missiles are usually launched from below water (usually from submarines).\n\nLike most missiles, the S-300, S-400 (missile), Advanced Air Defence and MIM-104 Patriot are for defense against short-range missiles and carry explosive warheads.\n\nHowever, in the case of a large closing speed, a projectile without explosives is used; just a collision is sufficient to destroy the target. See Missile Defense Agency for the following systems being developed:\n\n\nSoviet RS-82 rockets were successfully tested in combat at the Battle of Khalkhin Gol in 1939.\n\nGerman experience in World War II demonstrated that destroying a large aircraft was quite difficult, and they had invested considerable effort into air-to-air missile systems to do this. Their Messerschmitt Me 262's jets often carried R4M rockets, and other types of \"bomber destroyer\" aircraft had unguided rockets as well. In the post-war period, the R4M served as the pattern for a number of similar systems, used by almost all interceptor aircraft during the 1940s and 1950s. Most rockets (except for the AIR-2 Genie, due to its nuclear warhead with a large blast radius) had to be carefully aimed at relatively close range to hit the target successfully. The United States Navy and U.S. Air Force began deploying guided missiles in the early 1950s, most famous being the US Navy's AIM-9 Sidewinder and the USAF's AIM-4 Falcon. These systems have continued to advance, and modern air warfare consists almost entirely of missile firing. In the Falklands War, less powerful British Harriers were able to defeat faster Argentinian opponents using AIM-9L missiles provided by the United States as the conflict began. The latest heat-seeking designs can lock onto a target from various angles, not just from behind, where the heat signature from the engines is strongest. Other types rely on radar guidance (either on board or \"painted\" by the launching aircraft). Air-to-air missiles also have a wide range of sizes, ranging from helicopter-launched self-defense weapons with a range of a few kilometers, to long-range weapons designed for interceptor aircraft such as the R-37 (missile).\n\nIn the 1950s and 1960s, Soviet designers started work on an anti-satellite weapon, called the Istrebitel Sputnik, which literally means \"interceptor of satellites\" or \"destroyer of satellites\". After a lengthy development process of roughly twenty years, it was finally decided that testing of the Istrebitel Sputnik be canceled. This was when the United States started testing their own systems. The Brilliant Pebbles defense system proposed during the 1980s would have used kinetic energy collisions without explosives. Anti-satellite weapons may be launched either by an aircraft or a surface platform, depending on the design. To date, only a few known tests have occurred.\n\n\n"}
{"id": "19595", "url": "https://en.wikipedia.org/wiki?curid=19595", "title": "Mendelian inheritance", "text": "Mendelian inheritance\n\nMendelian inheritance is a type of biological inheritance that follows the laws originally proposed by Gregor Mendel in 1865 and 1866 and re-discovered in 1900. These laws were initially controversial. When Mendel's theories were integrated with the Boveri–Sutton chromosome theory of inheritance by Thomas Hunt Morgan in 1915, they became the core of classical genetics. Ronald Fisher combined these ideas with the theory of natural selection in his 1930 book \"The Genetical Theory of Natural Selection\", putting evolution onto a mathematical footing and forming the basis for population genetics within the modern evolutionary synthesis.\n\nThe principles of Mendelian inheritance were named for and first derived by Gregor Johann Mendel, a nineteenth-century Moravian monk who formulated his ideas after conducting simple hybridisation experiments with pea plants (\"Pisum sativum\") he had planted in the garden of his monastery. Between 1856 and 1863, Mendel cultivated and tested some 5,000 pea plants. From these experiments, he induced two generalizations which later became known as \"Mendel's Principles of Heredity\" or \"Mendelian inheritance\". He described these principles in a two-part paper, \"Versuche über Pflanzen-Hybriden\" (\"Experiments on Plant Hybridization\"), that he read to the Natural History Society of Brno on 8 February and 8 March 1865, and which was published in 1866.\n\nMendel's conclusions were largely ignored by the vast majority. Although they were not completely unknown to biologists of the time, they were not seen as generally applicable, even by Mendel himself, who thought they only applied to certain categories of species or traits. A major block to understanding their significance was the importance attached by 19th-century biologists to the apparent blending of many inherited traits in the overall appearance of the progeny, now known to be due to multi-gene interactions, in contrast to the organ-specific binary characters studied by Mendel. In 1900, however, his work was \"re-discovered\" by three European scientists, Hugo de Vries, Carl Correns, and Erich von Tschermak. The exact nature of the \"re-discovery\" has been debated: De Vries published first on the subject, mentioning Mendel in a footnote, while Correns pointed out Mendel's priority after having read De Vries' paper and realizing that he himself did not have priority. De Vries may not have acknowledged truthfully how much of his knowledge of the laws came from his own work and how much came only after reading Mendel's paper. Later scholars have accused Von Tschermak of not truly understanding the results at all.\n\nRegardless, the \"re-discovery\" made Mendelism an important but controversial theory. Its most vigorous promoter in Europe was William Bateson, who coined the terms \"genetics\" and \"allele\" to describe many of its tenets. The model of heredity was contested by other biologists because it implied that heredity was discontinuous, in opposition to the apparently continuous variation observable for many traits. Many biologists also dismissed the theory because they were not sure it would apply to all species. However, later work by biologists and statisticians such as Ronald Fisher showed that if multiple Mendelian factors were involved in the expression of an individual trait, they could produce the diverse results observed, and thus showed that Mendelian genetics is compatible with natural selection. Thomas Hunt Morgan and his assistants later integrated Mendel's theoretical model with the chromosome theory of inheritance, in which the chromosomes of cells were thought to hold the actual hereditary material, and created what is now known as classical genetics, a highly successful foundation which eventually cemented Mendel's place in history.\n\nMendel's findings allowed scientists such as Fisher and J.B.S. Haldane to predict the expression of traits on the basis of mathematical probabilities. An important aspect of Mendel's success can be traced to his decision to start his crosses only with plants he demonstrated were true-breeding. He only measured discrete (binary) characteristics, such as color, shape, and position of the seeds, rather than quantitatively variable characteristics. He expressed his results numerically and subjected them to statistical analysis. His method of data analysis and his large sample size gave credibility to his data. He had the foresight to follow several successive generations (F2, F3) of pea plants and record their variations. Finally, he performed \"test crosses\" (backcrossing descendants of the initial hybridization to the initial true-breeding lines) to reveal the presence and proportions of recessive characters.\n\nMendel discovered that, when he crossed purebred white flower and purple flower pea plants (the parental or P generation), the result was not a blend. Rather than being a mix of the two, the offspring (known as the F generation) was purple-flowered. When Mendel self-fertilized the F generation pea plants, he obtained a purple flower to white flower ratio in the F generation of 3 to 1. The results of this cross are tabulated in the Punnett square to the right.\n\nHe then conceived the idea of heredity units, which he called \"factors\". Mendel found that there are alternative forms of factors—now called genes—that account for variations in inherited characteristics. For example, the gene for flower color in pea plants exists in two forms, one for purple and the other for white. The alternative \"forms\" are now called alleles. For each biological trait, an organism inherits two alleles, one from each parent. These alleles may be the same or different. An organism that has two identical alleles for a gene is said to be homozygous for that gene (and is called a homozygote). An organism that has two different alleles for a gene is said be heterozygous for that gene (and is called a heterozygote).\n\nMendel hypothesized that allele pairs separate randomly, or segregate, from each other during the production of gametes: egg and sperm. Because allele pairs separate during gamete production, a sperm or egg carries only one allele for each inherited trait. When sperm and egg unite at fertilization, each contributes its allele, restoring the paired condition in the offspring. This is called the Law of Segregation. Mendel also found that each pair of alleles segregates independently of the other pairs of alleles during gamete formation. This is known as the Law of Independent Assortment.\n\nThe genotype of an individual is made up of the many alleles it possesses. An individual's physical appearance, or phenotype, is determined by its alleles as well as by its environment. The presence of an allele does not mean that the trait will be expressed in the individual that possesses it. If the two alleles of an inherited pair differ (the heterozygous condition), then one determines the organism’s appearance and is called the dominant allele; the other has no noticeable effect on the organism’s appearance and is called the recessive allele. Thus, in the example above the dominant purple flower allele will hide the phenotypic effects of the recessive white flower allele. This is known as the Law of Dominance but it is not a transmission law: it concerns the expression of the genotype. The upper case letters are used to represent dominant alleles whereas the lowercase letters are used to represent recessive alleles.\n\nIn the pea plant example above, the capital \"B\" represents the dominant allele for purple flowers and lowercase \"b\" represents the recessive allele for white flowers. Both parental plants were true-breeding, and one parental variety had two alleles for purple flowers (\"BB\") while the other had two alleles for white flowers (\"bb\"). As a result of fertilization, the F hybrids each inherited one allele for purple flowers and one for white. All the F hybrids (\"Bb\") had purple flowers, because the dominant \"B\" allele has its full effect in the heterozygote, while the recessive \"b\" allele has no effect on flower color. For the F plants, the ratio of plants with purple flowers to those with white flowers (3:1) is called the phenotypic ratio. The genotypic ratio, as seen in the Punnett square, is 1 \"BB\" : 2 \"Bb\" : 1 \"bb\".\n\nThe Law of Segregation states that every individual organism contains two alleles for each trait, and that these alleles segregate (separate) during meiosis such that each gamete contains only one of the alleles. An offspring thus receives a pair of alleles for a trait by inheriting homologous chromosomes from the parent organisms: one allele for each trait from each parent.\n\nMolecular proof of this principle was subsequently found through observation of meiosis by two scientists independently, the German botanist Oscar Hertwig in 1876, and the Belgian zoologist Edouard Van Beneden in 1883. Paternal and maternal chromosomes get separated in meiosis and the alleles with the traits of a character are segregated into two different gametes. Each parent contributes a single gamete, and thus a single, randomly successful allele copy to their offspring and fertilization.\n\nThe Law of Independent Assortment states that alleles for separate traits are passed independently of one another. That is, the biological selection of an allele for one trait has nothing to do with the selection of an allele for any other trait. Mendel found support for this law in his dihybrid cross experiments (Fig. 1). In his monohybrid crosses, an idealized 3:1 ratio between dominant and recessive phenotypes resulted. In dihybrid crosses, however, he found a 9:3:3:1 ratios (Fig. 2). This shows that each of the two alleles is inherited independently from the other, with a 3:1 phenotypic ratio for each.\n\nIndependent assortment occurs in eukaryotic organisms during meiotic metaphase I, and produces a gamete with a mixture of the organism's chromosomes. The physical basis of the independent assortment of chromosomes is the random orientation of each bivalent chromosome along the metaphase plate with respect to the other bivalent chromosomes. Along with crossing over, independent assortment increases genetic diversity by producing novel genetic combinations.\n\nThere are many violations of independent assortment due to genetic linkage.\n\nOf the 46 chromosomes in a normal diploid human cell, half are maternally derived (from the mother's egg) and half are paternally derived (from the father's sperm). This occurs as sexual reproduction involves the fusion of two haploid gametes (the egg and sperm) to produce a new organism having the full complement of chromosomes. During gametogenesis—the production of new gametes by an adult—the normal complement of 46 chromosomes needs to be halved to 23 to ensure that the resulting haploid gamete can join with another gamete to produce a diploid organism. An error in the number of chromosomes, such as those caused by a diploid gamete joining with a haploid gamete, is termed aneuploidy.\n\nIn independent assortment, the chromosomes that result are randomly sorted from all possible maternal and paternal chromosomes. Because zygotes end up with a random mix instead of a pre-defined \"set\" from either parent, chromosomes are therefore considered assorted independently. As such, the zygote can end up with any combination of paternal or maternal chromosomes. Any of the possible variants of a zygote formed from maternal and paternal chromosomes will occur with equal frequency. For human gametes, with 23 pairs of chromosomes, the number of possibilities is 2 or 8,388,608 possible combinations. The zygote will normally end up with 23 chromosomes pairs, but the origin of any particular chromosome will be randomly selected from paternal or maternal chromosomes. This contributes to the genetic variability of progeny.\n\nMendel's Law of Dominance states that recessive alleles will always be masked by dominant alleles. Therefore, a cross between a homozygous dominant and a homozygous recessive will always express the dominant phenotype, while still having a heterozygous genotype.\nThe Law of Dominance can be explained easily with the help of a mono hybrid cross experiment:-\nIn a cross between two organisms pure for any pair (or pairs) of contrasting traits (characters), the character that appears in the F1 generation is called \"dominant\" and the one which is suppressed (not expressed) is called \"recessive.\"\nEach character is controlled by a pair of dissimilar factors. Only one of the characters expresses. The one which expresses in the F1 generation is called Dominant.\nHowever, the law of dominance is not universally applicable.\n\nA Mendelian trait is one that is controlled by a single locus in an inheritance pattern. In such cases, a mutation in a single gene can cause a disease that is inherited according to Mendel's laws. Examples include sickle-cell anemia, Tay–Sachs disease, cystic fibrosis and xeroderma pigmentosa. A disease controlled by a single gene contrasts with a multi-factorial disease, like heart disease, which is affected by several loci (and the environment) as well as those diseases inherited in a non-Mendelian fashion.\n\nMendel explained inheritance in terms of discrete factors—genes—that are passed along from generation to generation according to the rules of probability. Mendel's laws are valid for all sexually reproducing organisms, including garden peas and human beings. However, Mendel's laws stop short of explaining some patterns of genetic inheritance. For most sexually reproducing organisms, cases where Mendel's laws can strictly account for the patterns of inheritance are relatively rare. Often, the inheritance patterns are more complex.\n\nThe F offspring of Mendel's pea crosses always looked like one of the two parental varieties. In this situation of \"complete dominance,\" the dominant allele had the same phenotypic effect whether present in one or two copies. But for some characteristics, the F hybrids have an appearance \"in between\" the phenotypes of the two parental varieties. A cross between two four o'clock (\"Mirabilis jalapa\") plants shows this common exception to Mendel's principles. Some alleles are neither dominant nor recessive. The F generation produced by a cross between red-flowered (RR) and white flowered (WW) \"Mirabilis jalapa\" plants consists of pink-colored flowers (RW). Which allele is dominant in this case? Neither one. This third phenotype results from flowers of the heterzygote having less red pigment than the red homozygotes. Cases in which one allele is not completely dominant over another are called incomplete dominance. In incomplete dominance, the heterozygous phenotype lies somewhere between the two homozygous phenotypes.\n\nA similar situation arises from codominance, in which the phenotypes produced by both alleles are clearly expressed. For example, in certain varieties of chicken, the allele for black feathers is codominant with the allele for white feathers. Heterozygous chickens have a color described as \"erminette\", speckled with black and white feathers. Unlike the blending of red and white colors in heterozygous four o'clocks, black and white colors appear separately in chickens. Many human genes, including one for a protein that controls cholesterol levels in the blood, show codominance, too. People with the heterozygous form of this gene produce two different forms of the protein, each with a different effect on cholesterol levels.\n\nIn Mendelian inheritance, genes have only two alleles, such as \"a\" and \"A\". In nature, such genes exist in several different forms and are therefore said to have multiple alleles. A gene with more than two alleles is said to have multiple alleles. An individual, of course, usually has only two copies of each gene, but many different alleles are often found within a population. One of the best-known examples is coat color in rabbits. A rabbit's coat color is determined by a single gene that has at least four different alleles. The four known alleles display a pattern of simple dominance that can produce four coat colors. Many other genes have multiple alleles, including the human genes for ABO blood type.\n\nFurthermore, many traits are produced by the interaction of several genes. Traits controlled by two or more genes are said to be polygenic traits. \"Polygenic\" means \"many genes.\" For example, at least three genes are involved in making the reddish-brown pigment in the eyes of fruit flies. Polygenic traits often show a wide range of phenotypes. The broad variety of skin color in humans comes about partly because at least four different genes probably control this trait.\n\n\n\n"}
{"id": "19597", "url": "https://en.wikipedia.org/wiki?curid=19597", "title": "Machinima", "text": "Machinima\n\nMachinima () is the use of real-time computer graphics engines to create a cinematic production. Most often, video games are used to generate the computer animation.\n\nMachinima-based artists, sometimes called machinimists or machinimators, are often fan laborers, by virtue of their re-use of copyrighted materials (see below). Machinima offers to provide an archive of gaming performance and access to the look and feel of software and hardware that may already have become unavailable or even obsolete. For game studies, \"Machinima’s gestures grant access to gaming’s historical conditions of possibility and how machinima offers links to a comparative horizon that informs, changes, and fully participates in videogame culture.\"\n\nThe practice of using graphics engines from video games arose from the animated software introductions of the 1980s demoscene, Disney Interactive Studios' 1992 video game \"Stunt Island\", and 1990s recordings of gameplay in first-person shooter (FPS) video games, such as id Software's \"Doom\" and \"Quake\". Originally, these recordings documented speedruns—attempts to complete a level as quickly as possible—and multiplayer matches. The addition of storylines to these films created \"\"Quake\" movies\". The more general term \"machinima\", a portmanteau of \"machine cinema\", arose when the concept spread beyond the \"Quake\" series to other games and software. After this generalization, machinima appeared in mainstream media, including television series and advertisements.\n\nMachinima has advantages and disadvantages when compared to other styles of filmmaking. Its relative simplicity over traditional frame-based animation limits control and range of expression. Its real-time nature favors speed, cost saving, and flexibility over the higher quality of pre-rendered computer animation. Virtual acting is less expensive, dangerous, and physically restricted than live action. Machinima can be filmed by relying on in-game artificial intelligence (AI) or by controlling characters and cameras through digital puppetry. Scenes can be precisely scripted, and can be manipulated during post-production using video editing techniques. Editing, custom software, and creative cinematography may address technical limitations. Game companies have provided software for and have encouraged machinima, but the widespread use of digital assets from copyrighted games has resulted in complex, unresolved legal issues.\n\nMachinima productions can remain close to their gaming roots and feature stunts or other portrayals of gameplay. Popular genres include dance videos, comedy, and drama. Alternatively, some filmmakers attempt to stretch the boundaries of the rendering engines or to mask the original 3-D context. The Academy of Machinima Arts & Sciences (AMAS), a non-profit organization dedicated to promoting machinima, recognizes exemplary productions through Mackie awards given at its annual Machinima Film Festival. Some general film festivals accept machinima, and game companies, such as Epic Games, Blizzard Entertainment and Jagex, have sponsored contests involving it.\n\n1980s software crackers added custom introductory credits sequences (intros) to programs whose copy protection they had removed. Increasing computing power allowed for more complex intros, and the demoscene formed when focus shifted to the intros instead of the cracks. The goal became to create the best 3-D demos in real-time with the least amount of software code. Disk storage was too slow for this, so graphics had to be calculated on the fly and without a pre-existing game engine.\n\nIn Disney Interactive Studios' 1992 computer game \"Stunt Island\", users could stage, record, and play back stunts. As Nitsche stated, the game's goal was \"not ... a high score but a spectacle.\" Released the following year, id Software's \"Doom\" included the ability to record gameplay as sequences of events that the game engine could later replay in real-time. Because events and not video frames were saved, the resulting game demo files were small and easily shared among players. A culture of recording gameplay developed, as Henry Lowood of Stanford University called it, \"a context for spectatorship... The result was nothing less than a metamorphosis of the player into a performer.\" Another important feature of \"Doom\" was that it allowed players to create their own modifications, maps, and software for the game, thus expanding the concept of game authorship. In machinima, there is a dual register of gestures: the trained motions of the player determine the in-game images of expressive motion.\n\nIn parallel of the video game approach, in the media art field, Maurice Benayoun’s Virtual Reality artwork \"The Tunnel under the Atlantic\" (1995), often compared to video games, introduced a virtual film director, fully autonomous intelligent agent, to shoot and edit in real time a full video from the digging performance in the Pompidou Center in Paris and the Museum of Contemporary art in Montreal. The full movie, \"Inside the Tunnel under the Atlantic\", 21h long, was followed in 1997 by \"Inside the Paris New-Delhi Tunnel\" (13h long). Only short excerpts where presented to the public. The complex behavior of the Tunnel’s virtual director makes it a significant precursor of later application to video games based machinimas.\n\n\"Doom\"s 1996 successor, \"Quake\", offered new opportunities for both gameplay and customization, while retaining the ability to record demos. Multiplayer games became popular, and demos of matches between teams of players (clans) were recorded and studied. Paul Marino, executive director of the AMAS, stated that deathmatches, a type of multiplayer game, became more \"cinematic\". At this point, however, they still documented gameplay without a narrative.\n\nOn October 26, 1996, a well-known gaming clan, the Rangers, surprised the \"Quake\" community with \"Diary of a Camper\", the first widely known machinima film. This short, 100-second demo file contained the action and gore of many others, but in the context of a brief story, rather than the usual deathmatch. An example of transformative or emergent gameplay, this shift from competition to theater required both expertise in and subversion of the game's mechanics. The Ranger demo emphasized this transformation by retaining specific gameplay references in its story.\n\n\"Diary of a Camper\" inspired many other \"\"Quake\" movies,\" as these films were then called. A community of game modifiers (modders), artists, expert players, and film fans began to form around them. The works were distributed and reviewed on websites such as The Cineplex, Psyk's Popcorn Jungle, and the Quake Movie Library (QML). Production was supported by dedicated demo-processing software, such as Uwe Girlich's Little Movie Processing Center (LMPC) and David \"crt\" Wright's non-linear editor Keygrip, which later became known as \"Adobe Premiere for Quake demo files\". Among the notable films were Clan Phantasm's \"Devil's Covenant\", the first feature-length \"Quake\" movie; Avatar and Wendigo's \"Blahbalicious\", which the QML awarded seven Quake Movie Oscars; and Clan Undead's \"Operation Bayshield\", which introduced simulated lip synchronization and featured customized digital assets.\n\nReleased in December 1997, id Software's \"Quake II\" improved support for user-created 3-D models. However, without compatible editing software, filmmakers continued to create works based on the original \"Quake\". These included the ILL Clan's \"Apartment Huntin'\" and the Quake done Quick group's \"Scourge Done Slick\". \"Quake II\" demo editors became available in 1998. In particular, Keygrip 2.0 introduced \"recamming\", the ability to adjust camera locations after recording. Paul Marino called the addition of this feature \"a defining moment for [m]achinima\". With \"Quake II\" filming now feasible, Strange Company's 1999 production \"Eschaton: Nightfall\" was the first work to feature entirely custom-made character models.\n\nThe December 1999 release of id's \"Quake III Arena\" posed a problem to the \"Quake\" movie community. The game's demo file included information needed for computer networking; however, to prevent cheating, id warned of legal action for dissemination of the file format. Thus, it was impractical to enhance software to work with \"Quake III\". Concurrently, the novelty of \"Quake\" movies was waning. New productions appeared less frequently, and, according to Marino, the community needed to \"reinvent itself\" to offset this development.\n\n\"Borg War\", a 90-minute animated Star Trek fan film, was produced using Elite Force 2 (a \"Quake III\" variant) and Starfleet Command 3, repurposing the games' voiceover clips to create a new plot. \"Borg War\" was nominated for two \"Mackie\" awards by the Academy of Machinima Arts & Sciences. An August 2007 screening at a \"Star Trek\" convention in Las Vegas was the first time that CBS/Paramount had approved the screening of a non-parody fan film at a licensed convention.\n\nIn January 2000, Hugh Hancock, the founder of Strange Company, launched a new website, machinima.com. A misspelled contraction of \"machine cinema\" (\"machinema\"), the term \"machinima\" was intended to dissociate in-game filming from a specific engine. The misspelling stuck because it also referenced anime. The new site featured tutorials, interviews, articles, and the exclusive release of Tritin Films' \"Quad God\". The first film made with \"Quake III Arena\", \"Quad God\" was also the first to be distributed as recorded video frames, not game-specific instructions. This change was initially controversial among machinima producers who preferred the smaller size of demo files. However, demo files required a copy of the game to view. The more accessible traditional video format broadened \"Quad God\"s viewership, and the work was distributed on CDs bundled with magazines. Thus, id's decision to protect \"Quake III\"s code inadvertently caused machinima creators to use more general solutions and thus widen their audience. Within a few years, machinima films were almost exclusively distributed in common video file formats.\nMachinima began to receive mainstream notice. Roger Ebert discussed it in a June 2000 article and praised Strange Company's machinima setting of Percy Bysshe Shelley's sonnet \"Ozymandias\". At Showtime Network's 2001 Alternative Media Festival, the ILL Clan's 2000 machinima film \"Hardly Workin'\" won the Best Experimental and Best in SHO awards. Steven Spielberg used \"Unreal Tournament\" to test special effects while working on his 2001 film \"\" Eventually, interest spread to game developers. In July 2001, Epic Games announced that its upcoming game \"Unreal Tournament 2003\" would include Matinee, a machinima production software utility. As involvement increased, filmmakers released fewer new productions to focus on quality.\n\nAt the March 2002 Game Developers Conference, five machinima makers—Anthony Bailey, Hugh Hancock, Katherine Anna Kang, Paul Marino, and Matthew Ross—founded the AMAS, a non-profit organization dedicated to promoting machinima. At QuakeCon in August, the new organization held the first Machinima Film Festival, which received mainstream media coverage. \"\", by Jake Hughes and Tom Hall, won three awards, including Best Picture. The next year, \"In the Waiting Line\", directed by Tommy Pallotta and animated by Randy Cole, utilizing Fountainhead Entertainment's Machinimation tools, it became the first machinima music video to air on MTV. As graphics technology improved, machinima filmmakers used other video games and consumer-grade video editing software. Using Bungie's 2001 game \"\", Rooster Teeth Productions created a popular comedy series \"Red vs. Blue: The Blood Gulch Chronicles\". The series' second season premiered at the Lincoln Center for the Performing Arts in 2004.\n\nMachinima has appeared on television, starting with G4's series \"Portal\". In the BBC series \"Time Commanders\", players re-enacted historic battles using Creative Assembly's real-time game \"\". MTV2's \"Video Mods\" re-creates music videos using characters from video games such as \"The Sims 2\", \"BloodRayne\", and \"Tribes\". Blizzard Entertainment helped to set part of \"Make Love, Not Warcraft\", an Emmy Award–winning 2006 episode of the comedy series \"South Park\", in its massively multiplayer online role-playing game (MMORPG) \"World of Warcraft\". By purchasing broadcast rights to Douglas Gayeton's machinima documentary \"Molotov Alva and His Search for the Creator\" in September 2007, HBO became the first television network to buy a work created completely in a virtual world. In December 2008, machinima.com signed fifteen experienced television comedy writers—including Patric Verrone, Bill Oakley, and Mike Rowe—to produce episodes for the site.\n\nCommercial use of machinima has increased.Rooster Teeth sells DVDs of their \"Red vs. Blue\" series and, under sponsorship from Electronic Arts, helped to promote \"The Sims 2\" by using the game to make a machinima series, \"The Strangerhood\". Volvo Cars sponsored the creation of a 2004 advertisement, \"\", the first film to combine machinima and live action. Later, Electronic Arts commissioned Rooster Teeth to promote their \"Madden NFL 07\" video game. Blockhouse TV uses Moviestorm's machinima software to produce its pre-school educational DVD series \"Jack and Holly\"\n\nGame developers have continued to increase support for machinima. Products such as Lionhead Studios' 2005 business simulation game \"The Movies\", Linden Research's virtual world \"Second Life\", and Bungie's 2007 first-person shooter \"Halo 3\" encourage the creation of user content by including machinima software tools. Using \"The Movies\", Alex Chan, a French resident with no previous filmmaking experience, took four days to create \"The French Democracy\", a short political film about the 2005 civil unrest in France. Third-party mods like \"Garry's Mod\" usually offer the ability to manipulate characters and take advantage of custom or migrated content, allowing for the creation of works like \"Counter-Strike For Kids\" that can be filmed using multiple games.\n\nIn a 2010 interview with PC Magazine, Valve Corporation CEO and co-founder Gabe Newell said that they wanted to make a \"Half-Life\" feature film themselves, rather than hand it off to a big-name director like Sam Raimi, and that their recent \"Team Fortress 2\" \"Meet The Team\" machinima shorts were experiments in doing just that. Two years later, Valve released their proprietary non-linear machinima software, Source Filmmaker.\n\nMachinima has also been used for music video clips. \"Second Life\" virtual artist Bryn Oh created a work for Australian performer Megan Bernard's song \"Clean Up Your Life\", released in 2016.\n\nThe AMAS defines machinima as \"animated filmmaking within a real-time virtual 3-D environment\". In other 3-D animation methods, creators can control every frame and nuance of their characters but, in turn, must consider issues such as key frames and inbetweening. Machinima creators leave many rendering details to their host environments, but may thus inherit those environments' limitations. Second Life Machinima film maker Ozymandius King provided a detailed account of the process by which the artists at MAGE Magazine produce their videos. \"Organizing for a photo shoot is similar to organizing for a film production. Once you find the actors / models, you have to scout locations, find clothes and props for the models and type up a shooting script. The more organized you are the less time it takes to shoot the scene.\" Because game animations focus on dramatic rather than casual actions, the range of character emotions is often limited. However, Kelland, Morris, and Lloyd state that a small range of emotions is often sufficient, as in successful Japanese anime television series.\n\nAnother difference is that machinima is created in real time, but other animation is pre-rendered. Real-time engines need to trade quality for speed and use simpler algorithms and models. In the 2001 animated film \"\", every strand of hair on a character's head was independent; real-time needs would likely force them to be treated as a single unit. Kelland, Morris, and Lloyd argue that improvement in consumer-grade graphics technology will allow more realism. Similarly, Paul Marino connects machinima to the increasing computing power predicted by Moore's law. For cut scenes in video games, issues other than visual fidelity arise. Pre-rendered scenes can require more digital storage space, weaken suspension of disbelief through contrast with real-time animation of normal gameplay, and limit interaction.\n\nLike live action, machinima is recorded in real-time, and real people can act and control the camera. Filmmakers are often encouraged to follow traditional cinematic conventions, such as avoiding wide fields of view, the overuse of slow motion, and errors in visual continuity. Unlike live action, machinima involves less expensive, digital special effects and sets, possibly with a science-fiction or historical theme. Explosions and stunts can be tried and repeated without monetary cost and risk of injury, and the host environment may allow unrealistic physical constraints. University of Cambridge experiments in 2002 and 2003 attempted to use machinima to re-create a scene from the 1942 live-action film \"Casablanca\". Machinima filming differed from traditional cinematography in that character expression was limited, but camera movements were more flexible and improvised. Nitsche compared this experiment to an unpredictable Dogme 95 production.\nBerkeley sees machinima as \"a strangely hybrid form, looking forwards and backwards, cutting edge and conservative at the same time\". Machinima is a digital medium based on 3-D computer games, but most works have a linear narrative structure. Some, such as \"Red vs. Blue\" and \"The Strangerhood\", follow narrative conventions of television situational comedy. Nitsche agrees that pre-recorded (\"reel\") machinima tends to be linear and offers limited interactive storytelling while machinima has more opportunities performed live and with audience interaction. In creating their improvisational comedy series \"On the Campaign Trail with Larry & Lenny Lumberjack\" and talk show \"Tra5hTa1k with ILL Will\", the ILL Clan blended real and virtual performance by creating the works on-stage and interacting with a live audience. In another combination of real and virtual worlds, Chris Burke's talk show \"This Spartan Life\" takes place in \"Halo 2\"s open multiplayer environment. There, others playing in earnest may attack the host or his interviewee. Although other virtual theatrical performances have taken place in chat rooms and multi-user dungeons, machinima adds \"cinematic camera work\". Previously, such virtual cinematic performances with live audience interaction were confined to research labs equipped with powerful computers.\n\nMachinima can be less expensive than other forms of filmmaking. Strange Company produced its feature-length machinima film \"BloodSpell\" for less than £10,000. Before using machinima, Burnie Burns and Matt Hullum of Rooster Teeth Productions spent US$9,000 to produce a live-action independent film. In contrast, the four Xbox game consoles used to make \"Red vs. Blue\" in 2005 cost $600. The low cost caused a product manager for Electronic Arts to compare machinima to the low-budget independent film \"The Blair Witch Project\", without the need for cameras and actors. Because these are seen as low barriers to entry, machinima has been called a \"democratization of filmmaking\". Berkeley weighs increased participation and a blurred line between producer and consumer against concerns that game copyrights limit commercialization and growth of machinima.\n\nComparatively, machinimists using pre-made virtual platforms like \"Second Life\" have indicated that their productions can be made quite successfully with no cost at all. Creators like Dutch director Chantal Harvey, producer of the 48 Hour Film Project Machinima sector, have created upwards of 200 films using the platform. Harvey's advocacy of the genre has resulted in the involvement of film director Peter Greenaway who served as a juror for the Machinima category and gave a keynote speech during the event.\n\nKelland, Morris, and Lloyd list four main methods of creating machinima. From simple to advanced, these are: relying on the game's AI to control most actions, digital puppetry, recamming, and precise scripting of actions. Although simple to produce, AI-dependent results are unpredictable, thus complicating the realization of a preconceived film script. For example, when Rooster Teeth produced \"The Strangerhood\" using \"The Sims 2\", a game that encourages the use of its AI, the group had to create multiple instances of each character to accommodate different moods. Individual instances were selected at different times to produce appropriate actions.\n\nIn digital puppetry, machinima creators become virtual actors. Each crew member controls a character in real-time, as in a multiplayer game. The director can use built-in camera controls, if available. Otherwise, video is captured from the perspectives of one or more puppeteers who serve as camera operators. Puppetry allows for improvisation and offers controls familiar to gamers, but requires more personnel than the other methods and is less precise than scripted recordings. However, some games, such as the \"Halo\" series, (except for Halo PC and Custom Edition, which allow AI and custom objects and characters), allow filming only through puppetry. According to Marino, other disadvantages are the possibility of disruption when filming in an open multi-user environment and the temptation for puppeteers to play the game in earnest, littering the set with blood and dead bodies. However, Chris Burke intentionally hosts \"This Spartan Life\" in these unpredictable conditions, which are fundamental to the show. Other works filmed using puppetry are the ILL Clan's improvisational comedy series \"On the Campaign Trail with Larry & Lenny Lumberjack\" and Rooster Teeth Productions' \"Red vs. Blue\". In recamming, which builds on puppetry, actions are first recorded to a game engine's demo file format, not directly as video frames. Without re-enacting scenes, artists can then manipulate the demo files to add cameras, tweak timing and lighting, and change the surroundings. This technique is limited to the few engines and software tools that support it.\n\nA technique common in cut scenes of video games, scripting consists of giving precise directions to the game engine. A filmmaker can work alone this way, as J. Thaddeus \"Mindcrime\" Skubis did in creating the nearly four-hour \"The Seal of Nehahra\" (2000), the longest work of machinima at the time. However, perfecting scripts can be time-consuming. Unless what-you-see-is-what-you-get (WYSIWYG) editing is available, as in \"\", changes may need to be verified in additional runs, and non-linear editing may be difficult. In this respect, Kelland, Morris, and Lloyd compare scripting to stop-motion animation. Another disadvantage is that, depending on the game, scripting capabilities may be limited or unavailable. Matinee, a machinima software tool included with \"Unreal Tournament 2004\", popularized scripting in machinima.\n\nWhen \"Diary of a Camper\" was created, no software tools existed to edit demo files into films. Rangers clan member Eric \"ArchV\" Fowler wrote his own programs to reposition the camera and to splice footage from the \"Quake\" demo file. \"Quake\" movie editing software later appeared, but the use of conventional non-linear video editing software is now common. For example, Phil South inserted single, completely white frames into his work \"No Licence\" to enhance the visual impact of explosions. In the post-production of \"\", Rooster Teeth Productions added letterboxing with Adobe Premiere Pro to hide the camera player's head-up display.\n\nMachinima creators have used different methods to handle limited character expression. The most typical ways that amateur-style machinima gets around limitations of expression include taking advantage of speech bubbles seen above players' heads when speaking, relying on the visual matching between a character's voice and appearance, and finding methods available within the game itself. \"Garry's Mod\" and Source Filmmaker include the ability to manipulate characters and objects in real-time, though the former relies on community addons to take advantage of certain engine features, and the latter renders scenes using non-real-time effects. In the \"Halo\" video game series, helmets completely cover the characters' faces. To prevent confusion, Rooster Teeth's characters move slightly when speaking, a convention shared with anime. Some machinima creators use custom software. For example, Strange Company uses Take Over GL Face Skins to add more facial expressions to their characters filmed in BioWare's 2002 role-playing video game \"Neverwinter Nights\". Similarly, Atussa Simon used a \"library of faces\" for characters in \"The Battle of Xerxes\". Some software, such as Epic Games' Impersonator for \"Unreal Tournament 2004\" and Valve Corporation's Faceposer for Source games, have been provided by the developer. Another solution is to blend in non-machinima elements, as nGame did by inserting painted characters with more expressive faces into its 1999 film \"Berlin Assassins\". It may be possible to point the camera elsewhere or employ other creative cinematography or acting. For example, Tristan Pope combined creative character and camera positioning with video editing to suggest sexual actions in his controversial film \"Not Just Another Love Story\".\n\nNew machinima filmmakers often want to use game-provided digital assets, but doing so raises legal issues. As derivative works, their films could violate copyright or be controlled by the assets' copyright holder, an arrangement that can be complicated by separate publishing and licensing rights. The software license agreement for \"The Movies\" stipulates that Activision, the game's publisher, owns \"any and all content within... Game Movies that was either supplied with the Program or otherwise made available... by Activision or its licensors...\" Some game companies provide software to modify their own games, and machinima makers often cite fair use as a defense, but the issue has never been tested in court. A potential problem with this defense is that many works, such as \"Red vs. Blue\", focus more on satire, which is not as explicitly protected by fair use as parody. Berkeley adds that, even if machinima artists use their own assets, their works could be ruled derivative if filmed in a proprietary engine. The risk inherent in a fair-use defense would cause most machinima artists simply to yield to a cease-and-desist order. The AMAS has attempted to negotiate solutions with video game companies, arguing that an open-source or reasonably priced alternative would emerge from an unfavorable situation. Unlike \"The Movies\", some dedicated machinima software programs, such as Reallusion's iClone, have licenses that avoid claiming ownership of users' films featuring bundled assets.\n\nGenerally, companies want to retain creative control over their intellectual properties and are wary of fan-created works, like fan fiction. However, because machinima provides free marketing, they have avoided a response demanding strict copyright enforcement. In 2003, Linden Lab was praised for changing license terms to allow users to retain ownership of works created in its virtual world \"Second Life\". Rooster Teeth initially tried to release \"Red vs. Blue\" unnoticed by \"Halo\"s owners because they feared that any communication would force them to end the project. However, Microsoft, Bungie's parent company at the time, contacted the group shortly after episode 2, and allowed them to continue without paying licensing fees.\n\nA case in which developer control was asserted involved Blizzard Entertainment's action against Tristan Pope's \"Not Just Another Love Story\". Blizzard's community managers encouraged users to post game movies and screenshots, but viewers complained that Pope's suggestion of sexual actions through creative camera and character positioning was pornographic. Citing the user license agreement, Blizzard closed discussion threads about the film and prohibited links to it. Although Pope accepted Blizzard's right to some control, he remained concerned about censorship of material that already existed in-game in some form. Discussion ensued about boundaries between MMORPG player and developer control. Lowood asserted that this controversy demonstrated that machinima could be a medium of negotiation for players.\n\nIn August 2007, Microsoft issued its Game Content Usage Rules, a license intended to address the legal status of machinima based on its games, including the \"Halo\" series. Microsoft intended the rules to be \"flexible\", and, because it was unilateral, the license was legally unable to reduce rights. However, machinima artists, such as Edgeworks Entertainment, protested the prohibitions on extending Microsoft's fictional universes (a common component of fan fiction) and on selling anything from sites hosting derivative works. Compounding the reaction was the license's statement, \"If you do any of these things, you can expect to hear from Microsoft's lawyers who will tell you that you have to stop distributing your items right away.\"\n\nSurprised by the negative feedback, Microsoft revised and reissued the license after discussion with Hugh Hancock and an attorney for the Electronic Frontier Foundation. The rules allow noncommercial use and distribution of works derived from Microsoft-owned game content, except audio effects and soundtracks. The license prohibits reverse engineering and material that is pornographic or otherwise \"objectionable\". On distribution, derivative works that elaborate on a game's fictional universe or story are automatically licensed to Microsoft and its business partners. This prevents legal problems if a fan and Microsoft independently conceive similar plots.\n\nA few weeks later, Blizzard Entertainment posted on WorldofWarcraft.com their \"Letter to the Machinimators of the World\", a license for noncommercial use of game content. It differs from Microsoft's declaration in that it addresses machinima specifically instead of general game-derived content, allows use of game audio if Blizzard can legally license it, requires derivative material to meet the Entertainment Software Rating Board's Teen content rating guideline, defines noncommercial use differently, and does not address extensions of fictional universes.\n\nHayes states that, although licensees' benefits are limited, the licenses reduce reliance on fair use regarding machinima. In turn, this recognition may reduce film festivals' concerns about copyright clearance. In an earlier analogous situation, festivals were concerned about documentary films until best practices for them were developed. According to Hayes, Microsoft and Blizzard helped themselves through their licenses because fan creations provide free publicity and are unlikely to harm sales. If the companies had instead sued for copyright infringement, defendants could have claimed estoppel or implied license because machinima had been unaddressed for a long time. Thus, these licenses secured their issuers' legal rights. Even though other companies, such as Electronic Arts, have encouraged machinima, they have avoided licensing it. Because of the involved legal complexity, they may prefer to under-enforce copyrights. Hayes believes that this legal uncertainty is a suboptimal solution and that, though limited and \"idiosyncratic\", the Microsoft and Blizzard licenses move towards an ideal video gaming industry standard for handling derivative works.\n\nJust as machinima can be the cause of legal dispute in copyright ownership and illegal use, it makes heavy use of intertextuality and raises the question of authorship. Machinima takes copyrighted property (such as characters in a game engine) and repurposes it to tell a story, but another common practice in machinima-making is to retell an existing story from a different medium in that engine.\n\nThis re-appropriation of established texts, resources, and artistic properties to tell a story or make a statement is an example of a semiotic phenomenon known as intertextuality or resemiosis. A more common term for this phenomenon is “parody”, but not all of these intertextual productions are intended for humor or satire, as demonstrated by the \"Few Good G-Men\" video. Furthermore, the argument of how well-protected machinima is under the guise of parody or satire is still highly debated. A piece of machinima may be reliant upon a protected property, but may not necessarily be making a statement about that property. Therefore, it is more accurate to refer to it simply as resemiosis, because it takes an artistic work and presents it in a new way, form, or medium. This resemiosis can be manifested in a number of ways. The machinima-maker can be considered an author who restructures the story and/or the world that the chosen game engine is built around. In the popular web series \"Red vs. Blue\", most of the storyline takes place within the game engine of \"\" and its subsequent sequels. \"\" has an extensive storyline already, but \"Red vs. Blue\" only ever makes mention of this storyline once in the first episode. Even after over 200 episodes of the show being broadcast onto the Internet since 2003, the only real similarities that can be drawn between \"Red vs. Blue\" and the game-world it takes place in are the character models, props, vehicles, and settings. Yet Burnie Burns and the machinima team at Rooster Teeth created an extensive storyline of their own using these game resources.\n\nThe ability to re-appropriate a game engine to film a video demonstrates intertextuality because it is an obvious example of art being a product of creation-through-manipulation rather than creation per se. The art historian Ernst Gombrich likened art to the \"manipulation of a vocabulary\" and this can be demonstrated in the creation of machinima. When using a game world to create a story, the author is influenced by the engine. For example, since so many video games are built around the concept of war, a significant portion of machinima films also take place in war-like environments.\n\nIntertextuality is further demonstrated in machinima not only in the re-appropriation of content but in artistic and communicatory techniques. Machinima by definition is a form of puppetry, and thus this new form of digital puppetry employs age-old techniques from the traditional artform. It is also, however, a form of filmmaking, and must employ filmmaking techniques such as camera angles and proper lighting. Some machinima takes place in online environments with participants, actors, and \"puppeteers\" working together from thousands of miles apart. This means other techniques born from long-distance communication must also be employed. Thus, techniques and practices that would normally never be used in conjunction with one another in the creation of an artistic work end up being used intertextually in the creation of machinima.\n\nAnother way that machinima demonstrates intertextuality is in its tendency to make frequent references to texts, works, and other media just like TV ads or humorous cartoons such as \"The Simpsons\" might do. For example, the machinima series \"Freeman's Mind\", created by Ross Scott, is filmed by taking a recording of Scott playing through the game \"Half Life\" as a player normally would and combining it with a voiceover (also recorded by Scott) to emulate an inner monologue of the normally voiceless protagonist Gordon Freeman. Scott portrays Freeman as a snarky, sociopathic character who makes frequent references to works and texts including science fiction, horror films, action movies, American history, and renowned novels such as Moby Dick. These references to works outside the game, often triggered by events within the game, are prime examples of the densely intertextual nature of machinima.\n\nNitsche and Lowood describe two methods of approaching machinima: starting from a video game and seeking a medium for expression or for documenting gameplay (\"inside-out\"), and starting outside a game and using it merely as animation tool (\"outside-in\"). Kelland, Morris, and Lloyd similarly distinguish between works that retain noticeable connections to games, and those closer to traditional animation. Belonging to the former category, gameplay and stunt machinima began in 1997 with \"Quake done Quick\". Although not the first speedrunners, its creators used external software to manipulate camera positions after recording, which, according to Lowood, elevated speedrunning \"from cyberathleticism to making movies\". Stunt machinima remains popular. Kelland, Morris, and Lloyd state that \"\" stunt videos offer a new way to look at the game, and compare \"Battlefield 1942\" machinima creators to the Harlem Globetrotters. Built-in features for video editing and post-recording camera positioning in \"Halo 3\" were expected to facilitate gameplay-based machinima. MMORPGs and other virtual worlds have been captured in documentary films, such as \"Miss Galaxies 2004\", a beauty pageant that took place in the virtual world of \"Star Wars Galaxies\". Footage was distributed in the cover disc of the August 2004 issue of \"PC Gamer\". Douglas Gayeton's \"Molotov Alva and His Search for the Creator\" documents the title character's interactions in \"Second Life\".\n\nGaming-related comedy offers another possible entry point for new machinima producers. Presented as five-minute sketches, many machinima comedies are analogous to Internet Flash animations. After Clan Undead's 1997 work \"Operation Bayshield\" built on the earliest \"Quake\" movies by introducing narrative conventions of linear media and sketch comedy reminiscent of the television show \"Saturday Night Live\", the New-York-based ILL Clan further developed the genre in machinima through works including \"Apartment Huntin'\" and \"Hardly Workin'\". \"Red vs. Blue: The Blood Gulch Chronicles\" chronicles a futile civil war over five seasons and 100 episodes. Marino wrote that although the series' humor was rooted in video games, strong writing and characters caused the series to \"transcend the typical gamer\". An example of a comedy film that targets a more general audience is Strange Company's \"Tum Raider\", produced for the BBC in 2004.\n\nMachinima has been used in music videos, of which the first documented example is Ken Thain's 2002 \"Rebel vs. Thug\", made in collaboration with Chuck D. For this, Thain used Quake2Max, a modification of \"Quake II\" that provided cel-shaded animation. The following year, Tommy Pallotta directed \"In the Waiting Line\" for the British group Zero 7. He told \"Computer Graphics World\", \"It probably would have been quicker to do the film in a 3D animated program. But now, we can reuse the assets in an improvisational way.\" Scenes of the game \"Postal 2\" can be seen in the music video of the Black Eyed Peas single \"Where Is the Love?\". In television, MTV features video game characters on its show \"Video Mods\". Among \"World of Warcraft\" players, dance and music videos became popular after dancing animations were discovered in the game.\n\nOthers use machinima in drama. These works may or may not retain signs of their video game provenance. \"Unreal Tournament\" is often used for science fiction and \"Battlefield 1942\" for war, but some artists subvert their chosen game's setting or completely detach their work from it. In 1999, Strange Company used \"Quake II\" in \"Eschaton: Nightfall\", a horror film based on the work of H. P. Lovecraft (although Quake I was also based on the Lovecraft lore). A later example is Damien Valentine's series \"Consanguinity\", made using BioWare's 2002 computer game \"Neverwinter Nights\" and based on the television series \"Buffy the Vampire Slayer\". Another genre consists of experimental works that attempt to push the boundaries of game engines. One example, Fountainhead's \"Anna\", is a short film that focuses on the cycle of life and is reminiscent of \"Fantasia\". Other productions go farther and completely eschew a 3-D appearance. Friedrich Kirschner's \"The Tournament\" and \"The Journey\" deliberately appear hand-drawn, and Dead on Que's \"Fake Science\" resembles two-dimensional Eastern European modernist animation from the 1970s.\n\nAnother derivative genre termed \"machinima verite\", from cinéma vérité, seeks to add a documentary and additional realism to the machinima piece. L.M. Sabo's \"CATACLYSM\" achieves a machinima verite style through displaying and recapturing the machinima video with a low resolution black and white hand-held video camera to produce a shaky camera effect. Other element of cinéma vérité, such as longer takes, sweeping camera transitions, and jump cuts may be included to complete the effect.\n\nSome have used machinima to make political statements, often from left-wing perspectives. Alex Chan's take on the 2005 civil unrest in France, \"The French Democracy\", attained mainstream attention and inspired other machinima commentaries on American and British society. Horwatt deemed Thuyen Nguyen's 2006 \"An Unfair War\", a criticism of the Iraq War, similar in its attempt \"to speak for those who cannot\". Joshua Garrison mimicked Chan's \"political pseudo-documentary style\" in his \"Virginia Tech Massacre\", a controversial \"Halo 3\"–based re-enactment and explanation of the eponymous real-life events. More recently, \"War of Internet Addiction\" addressed internet censorship in China using \"World of Warcraft\".\n\nAfter the QML's Quake Movie Oscars, dedicated machinima awards did not reappear until the AMAS created the Mackies for its first Machinima Film Festival in 2002. The annual festival has become an important one for machinima creators. Ho Chee Yue, a founder of the marketing company AKQA, helped to organize the first festival for the Asia chapter of the AMAS in 2006. In 2007, the AMAS supported the first machinima festival held in Europe. In addition to these smaller ceremonies, Hugh Hancock of Strange Company worked to add an award for machinima to the more general Bitfilm Festival in 2003. Other general festivals that allow machinima include the Sundance Film Festival, the Florida Film Festival, and the New Media Film Festival. The Ottawa International Animation Festival opened a machinima category in 2004, but, citing the need for \"a certain level of excellence\", declined to award anything to the category's four entries that year.\n\nMachinima has been showcased in contests sponsored by game companies. Epic Games' popular Make Something Unreal contest included machinima that impressed event organizer Jeff Morris because of \"the quality of entries that really push the technology, that accomplish things that Epic never envisioned\". In December 2005, Blizzard Entertainment and Xfire, a gaming-focused instant messaging service, jointly sponsored a \"World of Warcraft\" machinima contest.\n\n\n\n"}
{"id": "19599", "url": "https://en.wikipedia.org/wiki?curid=19599", "title": "Mutagenesis", "text": "Mutagenesis\n\nMutagenesis is a process by which the genetic information of an organism is changed, resulting in a mutation. It may occur spontaneously in nature, or as a result of exposure to mutagens. It can also be achieved experimentally using laboratory procedures. In nature mutagenesis can lead to cancer and various heritable diseases, but it is also a driving force of evolution. Mutagenesis as a science was developed based on work done by Hermann Muller, Charlotte Auerbach and J. M. Robson in the first half of the 20th century.\n\nDNA may be modified, either naturally or artificially, by a number of physical, chemical and biological agents, resulting in mutations. Hermann Muller found that \"High temperatures\" have the ability to mutate genes in the early 1920s, and in 1927, demonstrated a causal link to mutation upon experimenting with an x-ray machine and noting phylogenetic changes when irradiating fruit flies with relatively high dose of X-rays. Muller observed a number of chromosome rearrangements in his experiments, and suggested mutation as a cause of cancer. The association of exposure to radiation and cancer had been observed as early as 1902, six years after the discovery of X-ray by Wilhelm Röntgen and radioactivity by Henri Becquerel. Muller's contemporary Lewis Stadler also showed the mutational effect of X-ray on barley in 1928, and ultraviolet (UV) radiation on maize in 1936. In 1940s, Charlotte Auerbach and J. M. Robson, found that mustard gas can also cause mutations in fruit flies.\n\nWhile changes to the chromosome caused by X-ray and mustard gas were readily observable to the early researchers, other changes to the DNA induced by other mutagens were not so easily observable, and the mechanism may be complex and takes longer to unravel. For example, soot was suggested to be a cause of cancer as early as 1775, and coal tar was demonstrated to cause cancer in 1915. The chemicals involved in both were later shown to be polycyclic aromatic hydrocarbons (PAH). PAHs by themselves are not carcinogenic, and it was proposed in 1950 that the carcinogenic forms of PAHs are the oxides produced as metabolites from cellular processes. The metabolic process was identified in 1960s as catalysis by cytochrome P450 which produces reactive species that can interact with the DNA to form adducts,; the mechanism by which the PAH adducts give rise to mutation, however, is still under investigation.\n\nMammalian nuclear DNA may sustain more than 60,000 damage episodes per cell per day, as listed with references in DNA damage (naturally occurring). If left uncorrected, these adducts, after misreplication past the damaged sites, can give rise to mutations. In nature, the mutations that arise may be beneficial or deleterious—this is the driving force of evolution. An organism may acquire new traits through genetic mutation, but mutation may also result in impaired function of the genes, and in severe cases, cause the death of the organism. In the laboratory, however, mutagenesis is a useful technique for generating mutations that allows the functions of genes and gene products to be examined in detail, producing proteins with improved characteristics or novel functions, as well as mutant strains with useful properties. Initially, the ability of radiation and chemical mutagens to cause mutation was exploited to generate random mutations, but later techniques were developed to introduce specific mutations.\n\nHumans on average naturally pass 60 new mutations to their children but fathers pass more mutations depending on their age, transmitting an average of two new mutations with every additional year of their age to the child.\n\nDNA damage is an abnormal alteration in the structure of DNA that cannot, itself, be replicated when DNA replicates. In contrast, a mutation is a change in the nucleic acid sequence that can be replicated; hence, a mutation can be inherited from one generation to the next. Damage can occur from chemical addition (adduct), or structural disruption to a base of DNA (creating an abnormal nucleotide or nucleotide fragment), or a break in one or both DNA strands. When DNA containing damage is replicated, an incorrect base may be inserted in the new complementary strand as it is being synthesized (see ). The incorrect insertion in the new strand will occur opposite the damaged site in the template strand, and this incorrect insertion can become a mutation (i.e. a changed base pair) in the next round of replication. Furthermore, double-strand breaks in DNA may be repaired by an inaccurate repair process, non-homologous end joining, which produces mutations. Mutations can ordinarily be avoided if accurate DNA repair systems recognize DNA damage and repair it prior to completion of the next round of replication. At least 169 enzymes are either directly employed in DNA repair or influence DNA repair processes. Of these, 83 are directly employed in the 5 types of DNA repair processes indicated in the chart shown in the article DNA repair.\n\nMutagenesis may occur endogenously, for example, through spontaneous hydrolysis, or through normal cellular processes that can generate reactive oxygen species and DNA adducts, or through error in replication and repair. Mutagenesis may also arise as a result of the presence of environmental mutagens that induce changes to the DNA. The mechanism by which mutation arises varies according to the causative agent, the mutagen, involved. Most mutagens act either directly, or indirectly via mutagenic metabolites, on the DNA producing lesions. Some, however, may affect the replication or chromosomal partition mechanism, and other cellular processes.\n\nMany chemical mutagens require biological activation to become mutagenic. An important group of enzymes involved in the generation of mutagenic metabolites is cytochrome P450. Other enzymes that may also produce mutagenic metabolites include glutathione S-transferase and microsomal epoxide hydrolase. Mutagens that are not mutagenic by themselves but require biological activation are called promutagens.\n\nMany mutations arise as a result of problems caused by DNA lesions during replication, resulting in errors in replication. In bacteria, extensive damage to DNA due to mutagens results in single-stranded DNA gaps during replication. This induces the SOS response, an emergency repair process that is also error-prone, thereby generating mutations. In mammalian cells, stalling of replication at damaged sites induces a number of rescue mechanisms that help bypass DNA lesions, but which also may result in errors. The Y family of DNA polymerases specializes in DNA lesion bypass in a process termed translesion synthesis (TLS) whereby these lesion-bypass polymerases replace the stalled high-fidelity replicative DNA polymerase, transit the lesion and extend the DNA until the lesion has been passed so that normal replication can resume. These processes may be error-prone or error-free.\n\nDNA is not entirely stable in aqueous solution. Under physiological conditions the glycosidic bond may be hydrolyzed spontaneously and 10,000 purine sites in DNA are estimated to be depurinated each day in a cell. Numerous DNA repair pathways exist for DNA; however, if the apurinic site is not repaired, misincorporation of nucleotides may occur during replication. Adenine is preferentially incorporated by DNA polymerases in an apurinic site.\n\nCytidine may also become deaminated to uridine at one five-hundredth of the rate of depurination and can result in G to A transition. Eukaryotic cells also contain 5-methylcytosine, thought to be involved in the control of gene transcription, which can become deaminated into thymine.\n\nBases may be modified endogenously by normal cellular molecules. For example, DNA may be methylated by S-adenosylmethionine, and glycosylated by reducing sugars.\n\nMany compounds, such as PAHs, aromatic amines, aflatoxin and pyrrolizidine alkaloids, may form reactive oxygen species catalyzed by cytochrome P450. These metabolites form adducts with the DNA, which can cause errors in replication, and the bulky aromatic adducts may form stable intercalation between bases and block replication. The adducts may also induce conformational changes in the DNA. Some adducts may also result in the depurination of the DNA; it is, however, uncertain how significant such depurination as caused by the adducts is in generating mutation. \nAlkylation and arylation of bases can cause errors in replication. Some alkylating agents such as N-Nitrosamines may require the catalytic reaction of cytochrome-P450 for the formation of a reactive alkyl cation. N and O of guanine and the N and N of adenine are most susceptible to attack. N-guanine adducts form the bulk of DNA adducts, but they appear to be non-mutagenic. Alkylation at O of guanine, however, is harmful because excision repair of O-adduct of guanine may be poor in some tissues such as the brain. The O methylation of guanine can result in G to A transition, while O-methylthymine can be mispaired with guanine. The type of the mutation generated, however, may be dependent on the size and type of the adduct as well as the DNA sequence.\n\nIonizing radiation and reactive oxygen species often oxidize guanine to produce 8-oxoguanine.\n\nAs noted above, the number of DNA damage episodes occurring in a mammalian cell per day is high (more than 60,000 per day). Frequent occurrence of DNA damage is likely a problem for all DNA- containing organisms, and the need to cope with DNA damage and minimize their deleterious effects is likely a fundamental problem for life.\n\nMost spontaneous mutations likely arise from error-prone trans-lesion synthesis past a DNA damage site in the template strand during DNA replication. This process can overcome potentially lethal blockages, but at the cost of introducing inaccuracies in daughter DNA. The causal relationship of DNA damage to spontaneous mutation is illustrated by aerobically growing \"E. coli\" bacteria, in which 89% of spontaneously occurring base substitution mutations are caused by reactive oxygen species (ROS)-induced DNA damage. In yeast, more than 60% of spontaneous single-base pair substitutions and deletions are likely caused by trans-lesion synthesis.\n\nAn additional significant source of mutations in eukaryotes is the inaccurate DNA repair process non-homologous end joining, that is often employed in repair of double strand breaks.\n\nIn general, it appears that the main underlying cause of spontaneous mutation is error prone trans-lesion synthesis during DNA replication and that the error-prone non-homologous end joining repair pathway may also be an important contributor in eukaryotes.\n\nSome alkylating agents may produce crosslinking of DNA. Some natural occurring chemicals may also promote crosslinking, such as psoralens after activation by UV radiation, and nitrous acid. Interstrand cross-linking is more damaging as it blocks replication and transcription and can cause chromosomal breakages and rearrangements. Some crosslinkers such as cyclophosphamide, mitomycin C and cisplatin are used as anticancer chemotherapeutic because of their high degree of toxicity to proliferating cells.\n\nUV radiation promotes the formation of a cyclobutyl ring between adjacent thymines, resulting in the formation of pyrimidine dimers. In human skin cells, thousands of dimers may be formed in a day due to normal exposure to sunlight. DNA polymerase η may help bypass these lesions in an error-free manner; however, individuals with defective DNA repair function, such as sufferers of xeroderma pigmentosum, are sensitive to sunlight and may be prone to skin cancer.\n\nThe planar structure of chemicals such as ethidium bromide and proflavine allows them to insert between bases in DNA. This insert causes the DNA's backbone to stretch and makes slippage in DNA during replication more likely to occur since the bonding between the strands is made less stable by the stretching. Forward slippage will result in deletion mutation, while reverse slippage will result in an insertion mutation. Also, the intercalation into DNA of anthracyclines such as daunorubicin and doxorubicin interferes with the functioning of the enzyme topoisomerase II, blocking replication as well as causing mitotic homologous recombination.\nIonizing radiation may produce highly reactive free radicals that can break the bonds in the DNA. Double-stranded breakages are especially damaging and hard to repair, producing translocation and deletion of part of a chromosome. Alkylating agents like mustard gas may also cause breakages in the DNA backbone. Oxidative stress may also generate highly reactive oxygen species that can damage DNA. Incorrect repair of other damage induced by the highly reactive species can also lead to mutations.\n\nTransposons and viruses may insert DNA sequences into coding regions or functional elements of a gene and result in inactivation of the gene.\n\nWhile most mutagens produce effects that ultimately result in errors in replication, for example creating adducts that interfere with replication, some mutagens may directly affect the replication process or reduce its fidelity. Base analog such as 5-bromouracil may substitute for thymine in replication. Metals such as cadmium, chromium, and nickel can increase mutagenesis in a number of ways in addition to direct DNA damage, for example reducing the ability to repair errors, as well as producing epigenetic changes.\n\nMutagenesis in the laboratory is an important technique whereby DNA mutations are deliberately engineered to produce mutant genes, proteins, or strains of organism. Various constituents of a gene, such as its control elements and its gene product, may be mutated so that the functioning of a gene or protein can be examined in detail. The mutation may also produce mutant proteins with interesting properties, or enhanced or novel functions that may be of commercial use. Mutant strains may also be produced that have practical application or allow the molecular basis of particular cell function to be investigated.\n\nEarly methods of mutagenesis produced entirely random mutations; however, later methods of mutagenesis may produce site-specific mutation.\n"}
{"id": "19602", "url": "https://en.wikipedia.org/wiki?curid=19602", "title": "Mackenzie Bowell", "text": "Mackenzie Bowell\n\nSir Mackenzie Bowell (; December 27, 1823 – December 10, 1917) was a Canadian newspaper publisher and politician, who served as the fifth prime minister of Canada, in office from 1894 to 1896.\n\nBowell was born in Rickinghall, Suffolk, England. He and his family moved to Belleville, Ontario, in 1832. His mother died two years after their arrival. When in his early teens, Bowell was apprenticed to the printing shop of the local newspaper, the \"Belleville Intelligencer\", and some 15 years later, became its owner and proprietor.\n\nIn 1867, following Confederation, he was elected to the House of Commons for the Conservative Party. Bowell entered cabinet in 1878, and would serve under three prime ministers: John A. Macdonald, John Abbott, and John Thompson. He served variously as Minister of Customs (1878–1892), Minister of Militia and Defence (1892), and Minister of Trade and Commerce (1892–1894). Bowell kept his Commons seat continuously for 25 years, through a period of Liberal Party rule in the 1870s. In 1892, Bowell was appointed to the Senate. He became Leader of the Government in the Senate the following year.\n\nIn December 1894, Prime Minister Thompson unexpectedly died in office, aged only 49. The Earl of Aberdeen, Canada's governor general, appointed Bowell to replace Thompson as prime minister, due to his status as the most senior cabinet member. The main problem of Bowell's tenure as prime minister was the Manitoba Schools Question. His attempts at compromise alienated members of his own party, and following a Cabinet revolt in early 1896 he was forced to resign in favour of Charles Tupper. Bowell stayed on as a senator until his death at the age of 93, but never again held ministerial office; he served continuously as a Canadian parliamentarian for 50 years.\n\nBowell was born in Rickinghall, England, to John Bowell and Elizabeth Marshall. In 1832 his family emigrated to Belleville, Upper Canada, where he apprenticed with the printer at the town newspaper, \"The Belleville Intelligencer\". He became a successful printer and editor with that newspaper, and later its owner. He was a Freemason but also an Orangeman, becoming Grandmaster of the Orange Order of British North America, 1870–1878. \nIn 1847 he married Harriet Moore, with whom he had five sons and four daughters.\n\nBowell was first elected to the House of Commons in 1867 as a Conservative for the riding of North Hastings, Ontario. He held his seat for the Conservatives when they lost the election of January 1874, in the wake of the Pacific Scandal. Later that year he was instrumental in having Louis Riel expelled from the House.\n\nIn 1878, with the Conservatives again governing, he joined the Cabinet as Minister of Customs. In 1892 he became Minister of Militia and Defence, having held his Commons seat continuously for 25 years. A competent, hardworking administrator, Bowell remained in Cabinet as Minister of Trade and Commerce, a newly created portfolio, after he became a Senator that same year. His visit to Australia in 1893 led to the first leaders' conference of British colonies and territories, held in Ottawa in 1894. He became Leader of the Government in the Senate on October 31, 1893.\n\nIn December 1894, Prime Minister Sir John Sparrow David Thompson died suddenly, and Bowell, as the most senior Cabinet minister, was appointed in Thompson's stead by the Governor General. Bowell thus became the second of just two Canadian Prime Ministers (after John Abbott) to hold that office while serving in the Senate rather than the House of Commons.\n\nAs Prime Minister, Bowell faced the Manitoba Schools Question. In 1890 Manitoba had abolished public funding for denominational schools, both Catholic and Protestant, which many thought was contrary to the provisions made for denominational schools in the Manitoba Act of 1870. However, in a court challenge, the Judicial Committee of the Privy Council held that Manitoba's abolition of public funding for denominational schools was consistent with the Manitoba Act provision. In a second court case, the Judicial Committee held that the federal Parliament had the authority to enact remedial legislation to force Manitoba to re-establish the funding.\n\nBowell and his predecessors struggled to solve this problem, which divided the country, the government, and even Bowell's own Cabinet. He was further hampered in his handling of the issue by his own indecisiveness on it and by his inability, as a Senator, to take part in debates in the House of Commons. Bowell backed legislation, already drafted, that would have forced Manitoba to restore its Catholic schools, but then postponed it due to opposition within his Cabinet. With the ordinary business of government at a standstill, Bowell's Cabinet decided that he was incompetent to lead and so, to force him to step down, seven ministers resigned and then foiled the appointment of successors.\n\nThough Bowell denounced the rebellious ministers as \"a nest of traitors,\" he had to agree to resign. After ten days, following an intervention on Bowell's behalf by the Governor General, the government crisis was resolved and matters seemingly returned to normal when six of the ministers were reinstated, but leadership was then effectively held by Charles Tupper, who had joined Cabinet at the same time, filling the seventh place. Tupper, who had been Canadian High Commissioner to the United Kingdom, had been recalled by the plotters to replace Bowell. Bowell formally resigned in favour of Tupper at the end of the parliamentary session.\n\nBowell stayed in the Senate, serving as his party's leader there until 1906, and afterward as a regular Senator until his death in 1917, having served continuously for more than 50 years as a federal parliamentarian.\n\nHe died of pneumonia in Belleville, seventeen days short of his 94th birthday. He was buried in the Belleville cemetery. His funeral was attended by a full complement of the Orange Order, but not by any currently or formerly elected member of the government.\n\nBowell was designated a National Historic Person in 1945, on the advice of the national Historic Sites and Monuments Board.\n\nThe Post Office Department honored Bowell with a commemorative stamp in 1954, part of a series on prime ministers.\n\nIn their 1998 study of the Canadian prime ministers up through Jean Chrétien, J. L. Granatstein and Norman Hillmer found that a survey of Canadian historians ranked Bowell #19 out of the 20 Prime Ministers up until then.\n\nUntil 2017, Bowell remained the only Canadian prime minister without a full-length biography of his life and career. This shortfall was solved when the Belleville historian Betsy Dewar Boyce's book \"The Accidental Prime Minister\" was published by Bancroft, Ontario publisher Kirby Books. The book was published on the centennial of Bowell's death. Boyce had passed away in 2007, having unsuccessfully sought a publisher for her work for a decade.\n\nThe following jurist was appointed to the Supreme Court of Canada by the Governor General during Bowell's tenure:\n\n\n\"The Accidental Prime Minister\", by Betsy Dewar Boyce, 2017, Kirby Publishing, Bancroft, Ontario, .\n\n"}
{"id": "19603", "url": "https://en.wikipedia.org/wiki?curid=19603", "title": "Manhattan Project", "text": "Manhattan Project\n\nThe Manhattan Project was a research and development undertaking during World War II that produced the first nuclear weapons. It was led by the United States with the support of the United Kingdom and Canada. From 1942 to 1946, the project was under the direction of Major General Leslie Groves of the U.S. Army Corps of Engineers. Nuclear physicist Robert Oppenheimer was the director of the Los Alamos Laboratory that designed the actual bombs. The Army component of the project was designated the Manhattan District; \"Manhattan\" gradually superseded the official codename, Development of Substitute Materials, for the entire project. Along the way, the project absorbed its earlier British counterpart, Tube Alloys. The Manhattan Project began modestly in 1939, but grew to employ more than 130,000 people and cost nearly US$2 billion (about $ in dollars). Over 90% of the cost was for building factories and to produce fissile material, with less than 10% for development and production of the weapons. Research and production took place at more than 30 sites across the United States, the United Kingdom, and Canada.\n\nTwo types of atomic bombs were developed concurrently during the war: a relatively simple gun-type fission weapon and a more complex implosion-type nuclear weapon. The Thin Man gun-type design proved impractical to use with plutonium, and therefore a simpler gun-type called Little Boy was developed that used uranium-235, an isotope that makes up only 0.7 percent of natural uranium. Chemically identical to the most common isotope, uranium-238, and with almost the same mass, it proved difficult to separate the two. Three methods were employed for uranium enrichment: electromagnetic, gaseous and thermal. Most of this work was performed at the Clinton Engineer Works at Oak Ridge, Tennessee.\n\nIn parallel with the work on uranium was an effort to produce plutonium. After the feasibility of the world's first artificial nuclear reactor was demonstrated in Chicago at the Metallurgical Laboratory, it designed the X-10 Graphite Reactor at Oak Ridge and the production reactors in Hanford, Washington, in which uranium was irradiated and transmuted into plutonium. The plutonium was then chemically separated from the uranium, using the bismuth phosphate process. The Fat Man plutonium implosion-type weapon was developed in a concerted design and development effort by the Los Alamos Laboratory.\n\nThe project was also charged with gathering intelligence on the German nuclear weapon project. Through Operation Alsos, Manhattan Project personnel served in Europe, sometimes behind enemy lines, where they gathered nuclear materials and documents, and rounded up German scientists. Despite the Manhattan Project's tight security, Soviet atomic spies successfully penetrated the program.\n\nThe first nuclear device ever detonated was an implosion-type bomb at the Trinity test, conducted at New Mexico's Alamogordo Bombing and Gunnery Range on 16 July 1945. Little Boy and Fat Man bombs were used a month later in the atomic bombings of Hiroshima and Nagasaki, respectively. In the immediate postwar years, the Manhattan Project conducted weapons testing at Bikini Atoll as part of Operation Crossroads, developed new weapons, promoted the development of the network of national laboratories, supported medical research into radiology and laid the foundations for the nuclear navy. It maintained control over American atomic weapons research and production until the formation of the United States Atomic Energy Commission in January 1947.\n\nThe discovery of nuclear fission by German chemists Otto Hahn and Fritz Strassmann in 1938, and its theoretical explanation by Lise Meitner and Otto Frisch, made the development of an atomic bomb a theoretical possibility. There were fears that a German atomic bomb project would develop one first, especially among scientists who were refugees from Nazi Germany and other fascist countries. In August 1939, Hungarian-born physicists Leó Szilárd and Eugene Wigner drafted the Einstein–Szilárd letter, which warned of the potential development of \"extremely powerful bombs of a new type\". It urged the United States to take steps to acquire stockpiles of uranium ore and accelerate the research of Enrico Fermi and others into nuclear chain reactions. They had it signed by Albert Einstein and delivered to President Franklin D. Roosevelt. Roosevelt called on Lyman Briggs of the National Bureau of Standards to head the Advisory Committee on Uranium to investigate the issues raised by the letter. Briggs held a meeting on 21 October 1939, which was attended by Szilárd, Wigner and Edward Teller. The committee reported back to Roosevelt in November that uranium \"would provide a possible source of bombs with a destructiveness vastly greater than anything now known.\"\n\nThe Advisory Committee on Uranium became the National Defense Research Committee (NDRC) Committee on Uranium when that organization was formed on 27 June 1940. Briggs proposed spending $167,000 on research into uranium, particularly the uranium-235 isotope, and the recently discovered plutonium. On 28 June 1941, Roosevelt signed Executive Order 8807, which created the Office of Scientific Research and Development (OSRD), with Vannevar Bush as its director. The office was empowered to engage in large engineering projects in addition to research. The NDRC Committee on Uranium became the S-1 Section of the OSRD; the word \"uranium\" was dropped for security reasons.\n\nIn Britain, Frisch and Rudolf Peierls at the University of Birmingham had made a breakthrough investigating the critical mass of uranium-235 in June 1939. Their calculations indicated that it was within an order of magnitude of , which was small enough to be carried by a bomber of the day. Their March 1940 Frisch–Peierls memorandum initiated the British atomic bomb project and its Maud Committee, which unanimously recommended pursuing the development of an atomic bomb. In July 1940, Britain had offered to give the United States access to its scientific research, and the Tizard Mission's John Cockcroft briefed American scientists on British developments. He discovered that the American project was smaller than the British, and not as far advanced.\n\nAs part of the scientific exchange, the Maud Committee's findings were conveyed to the United States. One of its members, the Australian physicist Mark Oliphant, flew to the United States in late August 1941 and discovered that data provided by the Maud Committee had not reached key American physicists. Oliphant then set out to find out why the committee's findings were apparently being ignored. He met with the Uranium Committee and visited Berkeley, California, where he spoke persuasively to Ernest O. Lawrence. Lawrence was sufficiently impressed to commence his own research into uranium. He in turn spoke to James B. Conant, Arthur H. Compton and George B. Pegram. Oliphant's mission was therefore a success; key American physicists were now aware of the potential power of an atomic bomb.\n\nOn 9 October 1941, President Roosevelt approved the atomic program after he convened a meeting with Vannevar Bush and Vice President Henry A. Wallace. To control the program, he created a Top Policy Group consisting of himself—although he never attended a meeting—Wallace, Bush, Conant, Secretary of War Henry L. Stimson, and the Chief of Staff of the Army, General George C. Marshall. Roosevelt chose the Army to run the project rather than the Navy, because the Army had more experience with management of large-scale construction projects. He also agreed to coordinate the effort with that of the British, and on 11 October he sent a message to Prime Minister Winston Churchill, suggesting that they correspond on atomic matters.\n\nThe S-1 Committee held its meeting on 18 December 1941 \"pervaded by an atmosphere of enthusiasm and urgency\" in the wake of the attack on Pearl Harbor and the subsequent United States declaration of war upon Japan and then on Germany. Work was proceeding on three different techniques for isotope separation to separate uranium-235 from the more abundant uranium-238. Lawrence and his team at the University of California, Berkeley, investigated electromagnetic separation, while Eger Murphree and Jesse Wakefield Beams's team looked into gaseous diffusion at Columbia University, and Philip Abelson directed research into thermal diffusion at the Carnegie Institution of Washington and later the Naval Research Laboratory. Murphree was also the head of an unsuccessful separation project using gas centrifuges.\n\nMeanwhile, there were two lines of research into nuclear reactor technology, with Harold Urey continuing research into heavy water at Columbia, while Arthur Compton brought the scientists working under his supervision from Columbia, California and Princeton University to join his team at the University of Chicago, where he organized the Metallurgical Laboratory in early 1942 to study plutonium and reactors using graphite as a neutron moderator. Briggs, Compton, Lawrence, Murphree, and Urey met on 23 May 1942 to finalize the S-1 Committee recommendations, which called for all five technologies to be pursued. This was approved by Bush, Conant, and Brigadier General Wilhelm D. Styer, the chief of staff of Major General Brehon B. Somervell's Services of Supply, who had been designated the Army's representative on nuclear matters. Bush and Conant then took the recommendation to the Top Policy Group with a budget proposal for $54 million for construction by the United States Army Corps of Engineers, $31 million for research and development by OSRD and $5 million for contingencies in fiscal year 1943. The Top Policy Group in turn sent it to the President on 17 June 1942 and he approved it by writing \"OK FDR\" on the document.\n\nCompton asked theoretical physicist J. Robert Oppenheimer of the University of California, Berkeley, to take over research into fast neutron calculations—the key to calculations of critical mass and weapon detonation—from Gregory Breit, who had quit on 18 May 1942 because of concerns over lax operational security. John H. Manley, a physicist at the Metallurgical Laboratory, was assigned to assist Oppenheimer by contacting and coordinating experimental physics groups scattered across the country. Oppenheimer and Robert Serber of the University of Illinois examined the problems of neutron diffusion—how neutrons moved in a nuclear chain reaction—and hydrodynamics—how the explosion produced by a chain reaction might behave. To review this work and the general theory of fission reactions, Oppenheimer and Fermi convened meetings at the University of Chicago in June and at the University of California, Berkeley, in July 1942 with theoretical physicists Hans Bethe, John Van Vleck, Edward Teller, Emil Konopinski, Robert Serber, Stan Frankel, and Eldred C. Nelson, the latter three former students of Oppenheimer, and experimental physicists Emilio Segrè, Felix Bloch, Franco Rasetti, John Henry Manley, and Edwin McMillan. They tentatively confirmed that a fission bomb was theoretically possible.\n\nThere were still many unknown factors. The properties of pure uranium-235 were relatively unknown, as were those of plutonium, an element that had only been discovered in February 1941 by Glenn Seaborg and his team. The scientists at the Berkeley conference envisioned creating plutonium in nuclear reactors where uranium-238 atoms absorbed neutrons that had been emitted from fissioning uranium-235 atoms. At this point no reactor had been built, and only tiny quantities of plutonium were available from cyclotrons. Even by December 1943, only two milligrams had been produced. There were many ways of arranging the fissile material into a critical mass. The simplest was shooting a \"cylindrical plug\" into a sphere of \"active material\" with a \"tamper\"—dense material that would focus neutrons inward and keep the reacting mass together to increase its efficiency. They also explored designs involving spheroids, a primitive form of \"implosion\" suggested by Richard C. Tolman, and the possibility of autocatalytic methods, which would increase the efficiency of the bomb as it exploded.\n\nConsidering the idea of the fission bomb theoretically settled—at least until more experimental data was available—the Berkeley conference then turned in a different direction. Edward Teller pushed for discussion of a more powerful bomb: the \"super\", now usually referred to as a \"hydrogen bomb\", which would use the explosive force of a detonating fission bomb to ignite a nuclear fusion reaction in deuterium and tritium. Teller proposed scheme after scheme, but Bethe refused each one. The fusion idea was put aside to concentrate on producing fission bombs. Teller also raised the speculative possibility that an atomic bomb might \"ignite\" the atmosphere because of a hypothetical fusion reaction of nitrogen nuclei. Bethe calculated that it could not happen, and a report co-authored by Teller showed that \"no self-propagating chain of nuclear reactions is likely to be started.\" In Serber's account, Oppenheimer mentioned the possibility of this scenario to Arthur Compton, who \"didn't have enough sense to shut up about it. It somehow got into a document that went to Washington\" and was \"never laid to rest\".\n\nThe Chief of Engineers, Major General Eugene Reybold, selected Colonel James C. Marshall to head the Army's part of the project in June 1942. Marshall created a liaison office in Washington, D.C., but established his temporary headquarters on the 18th floor of 270 Broadway in New York, where he could draw on administrative support from the Corps of Engineers' North Atlantic Division. It was close to the Manhattan office of Stone & Webster, the principal project contractor, and to Columbia University. He had permission to draw on his former command, the Syracuse District, for staff, and he started with Lieutenant Colonel Kenneth Nichols, who became his deputy.\nBecause most of his task involved construction, Marshall worked in cooperation with the head of the Corps of Engineers Construction Division, Major General Thomas M. Robbins, and his deputy, Colonel Leslie Groves. Reybold, Somervell, and Styer decided to call the project \"Development of Substitute Materials\", but Groves felt that this would draw attention. Since engineer districts normally carried the name of the city where they were located, Marshall and Groves agreed to name the Army's component of the project the Manhattan District. This became official on 13 August, when Reybold issued the order creating the new district. Informally, it was known as the Manhattan Engineer District, or MED. Unlike other districts, it had no geographic boundaries, and Marshall had the authority of a division engineer. Development of Substitute Materials remained as the official codename of the project as a whole, but was supplanted over time by \"Manhattan\".\n\nMarshall later conceded that, \"I had never heard of atomic fission but I did know that you could not build much of a plant, much less four of them for $90 million.\" A single TNT plant that Nichols had recently built in Pennsylvania had cost $128 million. Nor were they impressed with estimates to the nearest order of magnitude, which Groves compared with telling a caterer to prepare for between ten and a thousand guests. A survey team from Stone & Webster had already scouted a site for the production plants. The War Production Board recommended sites around Knoxville, Tennessee, an isolated area where the Tennessee Valley Authority could supply ample electric power and the rivers could provide cooling water for the reactors. After examining several sites, the survey team selected one near Elza, Tennessee. Conant advised that it be acquired at once and Styer agreed but Marshall temporized, awaiting the results of Conant's reactor experiments before taking action. Of the prospective processes, only Lawrence's electromagnetic separation appeared sufficiently advanced for construction to commence.\n\nMarshall and Nichols began assembling the resources they would need. The first step was to obtain a high priority rating for the project. The top ratings were AA-1 through AA-4 in descending order, although there was also a special AAA rating reserved for emergencies. Ratings AA-1 and AA-2 were for essential weapons and equipment, so Colonel Lucius D. Clay, the deputy chief of staff at Services and Supply for requirements and resources, felt that the highest rating he could assign was AA-3, although he was willing to provide a AAA rating on request for critical materials if the need arose. Nichols and Marshall were disappointed; AA-3 was the same priority as Nichols' TNT plant in Pennsylvania.\n\nVannevar Bush became dissatisfied with Colonel Marshall's failure to get the project moving forward expeditiously, specifically the failure to acquire the Tennessee site, the low priority allocated to the project by the Army and the location of his headquarters in New York City. Bush felt that more aggressive leadership was required, and spoke to Harvey Bundy and Generals Marshall, Somervell, and Styer about his concerns. He wanted the project placed under a senior policy committee, with a prestigious officer, preferably Styer, as overall director.\n\nSomervell and Styer selected Groves for the post, informing him on 17 September of this decision, and that General Marshall ordered that he be promoted to brigadier general, as it was felt that the title \"general\" would hold more sway with the academic scientists working on the Manhattan Project. Groves' orders placed him directly under Somervell rather than Reybold, with Colonel Marshall now answerable to Groves. Groves established his headquarters in Washington, D.C., on the fifth floor of the New War Department Building, where Colonel Marshall had his liaison office. He assumed command of the Manhattan Project on 23 September. Later that day, he attended a meeting called by Stimson, which established a Military Policy Committee, responsible to the Top Policy Group, consisting of Bush (with Conant as an alternate), Styer and Rear Admiral William R. Purnell. Tolman and Conant were later appointed as Groves' scientific advisers.\n\nOn 19 September, Groves went to Donald Nelson, the chairman of the War Production Board, and asked for broad authority to issue a AAA rating whenever it was required. Nelson initially balked but quickly caved in when Groves threatened to go to the President. Groves promised not to use the AAA rating unless it was necessary. It soon transpired that for the routine requirements of the project the AAA rating was too high but the AA-3 rating was too low. After a long campaign, Groves finally received AA-1 authority on 1 July 1944. According to Groves, \"In Washington you became aware of the importance of top priority. Most everything proposed in the Roosevelt administration would have top priority. That would last for about a week or two and then something else would get top priority\".\n\nOne of Groves' early problems was to find a director for Project Y, the group that would design and build the bomb. The obvious choice was one of the three laboratory heads, Urey, Lawrence, or Compton, but they could not be spared. Compton recommended Oppenheimer, who was already intimately familiar with the bomb design concepts. However, Oppenheimer had little administrative experience, and, unlike Urey, Lawrence, and Compton, had not won a Nobel Prize, which many scientists felt that the head of such an important laboratory should have. There were also concerns about Oppenheimer's security status, as many of his associates were Communists, including his brother, Frank Oppenheimer; his wife, Kitty; and his girlfriend, Jean Tatlock. A long conversation on a train in October 1942 convinced Groves and Nichols that Oppenheimer thoroughly understood the issues involved in setting up a laboratory in a remote area and should be appointed as its director. Groves personally waived the security requirements and issued Oppenheimer a clearance on 20 July 1943.\n\nThe British and Americans exchanged nuclear information but did not initially combine their efforts. Britain rebuffed attempts by Bush and Conant in 1941 to strengthen cooperation with its own project, codenamed Tube Alloys, because it was reluctant to share its technological lead and help the United States develop its own atomic bomb. An American scientist who brought a personal letter from Roosevelt to Churchill offering to pay for all research and development in an Anglo-American project was poorly treated, and Churchill did not reply to the letter. The United States as a result decided as early as April 1942 that if its offer was rejected, they should proceed alone. The British, who had made significant contributions early in the war, did not have the resources to carry through such a research program while fighting for their survival. As a result, Tube Alloys soon fell behind its American counterpart. and on 30 July 1942, Sir John Anderson, the minister responsible for Tube Alloys, advised Churchill that: \"We must face the fact that ... [our] pioneering work ... is a dwindling asset and that, unless we capitalise it quickly, we shall be outstripped. We now have a real contribution to make to a 'merger.' Soon we shall have little or none.\" That month Churchill and Roosevelt made an informal, unwritten agreement for atomic collaboration.\nThe opportunity for an equal partnership no longer existed, however, as shown in August 1942 when the British unsuccessfully demanded substantial control over the project while paying none of the costs. By 1943 the roles of the two countries had reversed from late 1941; in January Conant notified the British that they would no longer receive atomic information except in certain areas. While the British were shocked by the abrogation of the Churchill-Roosevelt agreement, head of the Canadian National Research Council C. J. Mackenzie was less surprised, writing \"I can't help feeling that the United Kingdom group [over] emphasizes the importance of their contribution as compared with the Americans.\" As Conant and Bush told the British, the order came \"from the top\".\n\nThe British bargaining position had worsened; the American scientists had decided that the United States no longer needed outside help, and they wanted to prevent Britain exploiting post-war commercial applications of atomic energy. The committee supported, and Roosevelt agreed to, restricting the flow of information to what Britain could use during the war—especially not bomb design—even if doing so slowed down the American project. By early 1943 the British stopped sending research and scientists to America, and as a result the Americans stopped all information sharing. The British considered ending the supply of Canadian uranium and heavy water to force the Americans to again share, but Canada needed American supplies to produce them. They investigated the possibility of an independent nuclear program, but determined that it could not be ready in time to affect the outcome of the war in Europe.\n\nBy March 1943 Conant decided that British help would benefit some areas of the project. James Chadwick and one or two other British scientists were important enough that the bomb design team at Los Alamos needed them, despite the risk of revealing weapon design secrets. In August 1943 Churchill and Roosevelt negotiated the Quebec Agreement, which resulted in a resumption of cooperation between scientists working on the same problem. Britain, however, agreed to restrictions on data on the building of large-scale production plants necessary for the bomb. The subsequent Hyde Park Agreement in September 1944 extended this cooperation to the postwar period. The Quebec Agreement established the Combined Policy Committee to coordinate the efforts of the United States, United Kingdom and Canada. Stimson, Bush and Conant served as the American members of the Combined Policy Committee, Field Marshal Sir John Dill and Colonel J. J. Llewellin were the British members, and C. D. Howe was the Canadian member. Llewellin returned to the United Kingdom at the end of 1943 and was replaced on the committee by Sir Ronald Ian Campbell, who in turn was replaced by the British Ambassador to the United States, Lord Halifax, in early 1945. Sir John Dill died in Washington, D.C., in November 1944 and was replaced both as Chief of the British Joint Staff Mission and as a member of the Combined Policy Committee by Field Marshal Sir Henry Maitland Wilson.\n\nWhen cooperation resumed after the Quebec agreement, the Americans' progress and expenditures amazed the British. The United States had already spent more than $1 billion ($ today), while in 1943, the United Kingdom had spent about £0.5 million. Chadwick thus pressed for British involvement in the Manhattan Project to the fullest extent and abandon any hopes of a British project during the war. With Churchill's backing, he attempted to ensure that every request from Groves for assistance was honored. The British Mission that arrived in the United States in December 1943 included Niels Bohr, Otto Frisch, Klaus Fuchs, Rudolf Peierls, and Ernest Titterton. More scientists arrived in early 1944. While those assigned to gaseous diffusion left by the fall of 1944, the 35 working with Lawrence at Berkeley were assigned to existing laboratory groups and stayed until the end of the war. The 19 sent to Los Alamos also joined existing groups, primarily related to implosion and bomb assembly, but not the plutonium-related ones. Part of the Quebec Agreement specified that nuclear weapons would not be used against another country without mutual consent. In June 1945, Wilson agreed that the use of nuclear weapons against Japan would be recorded as a decision of the Combined Policy Committee.\n\nThe Combined Policy Committee created the Combined Development Trust in June 1944, with Groves as its chairman, to procure uranium and thorium ores on international markets. The Belgian Congo and Canada held much of the world's uranium outside Eastern Europe, and the Belgian government in exile was in London. Britain agreed to give the United States most of the Belgian ore, as it could not use most of the supply without restricted American research. In 1944, the Trust purchased of uranium oxide ore from companies operating mines in the Belgian Congo. In order to avoid briefing US Secretary of the Treasury Henry Morgenthau Jr. on the project, a special account not subject to the usual auditing and controls was used to hold Trust monies. Between 1944 and the time he resigned from the Trust in 1947, Groves deposited a total of $37.5 million into the Trust's account.\n\nGroves appreciated the early British atomic research and the British scientists' contributions to the Manhattan Project, but stated that the United States would have succeeded without them. He also said that Churchill was \"the best friend the atomic bomb project had [as] he kept Roosevelt's interest up ... He just stirred him up all the time by telling him how important he thought the project was.\"\n\nThe British wartime participation was crucial to the success of the United Kingdom's independent nuclear weapons program after the war when the McMahon Act of 1946 temporarily ended American nuclear cooperation.\n\nThe day after he took over the project, Groves took a train to Tennessee with Colonel Marshall to inspect the proposed site there, and Groves was impressed. On 29 September 1942, United States Under Secretary of War Robert P. Patterson authorized the Corps of Engineers to acquire of land by eminent domain at a cost of $3.5 million. An additional was subsequently acquired. About 1,000 families were affected by the condemnation order, which came into effect on 7 October. Protests, legal appeals, and a 1943 Congressional inquiry were to no avail. By mid-November U.S. Marshals were tacking notices to vacate on farmhouse doors, and construction contractors were moving in. Some families were given two weeks' notice to vacate farms that had been their homes for generations; others had settled there after being evicted to make way for the Great Smoky Mountains National Park in the 1920s or the Norris Dam in the 1930s. The ultimate cost of land acquisition in the area, which was not completed until March 1945, was only about $2.6 million, which worked out to around $47 an acre. When presented with Public Proclamation Number Two, which declared Oak Ridge a total exclusion area that no one could enter without military permission, the Governor of Tennessee, Prentice Cooper, angrily tore it up.\n\nInitially known as the Kingston Demolition Range, the site was officially renamed the Clinton Engineer Works (CEW) in early 1943. While Stone & Webster concentrated on the production facilities, the architectural and engineering firm Skidmore, Owings & Merrill designed and built a residential community for 13,000. The community was located on the slopes of Black Oak Ridge, from which the new town of Oak Ridge got its name. The Army presence at Oak Ridge increased in August 1943 when Nichols replaced Marshall as head of the Manhattan Engineer District. One of his first tasks was to move the district headquarters to Oak Ridge although the name of the district did not change. In September 1943 the administration of community facilities was outsourced to Turner Construction Company through a subsidiary, the Roane-Anderson Company (for Roane and Anderson Counties, in which Oak Ridge was located). Chemical engineers, including William J. Wilcox Jr. and Warren Fuchs, were part of \"frantic efforts\" to make 10% to 12% enriched uranium 235, known as the code name \"tuballoy tetroxide\", with tight security and fast approvals for supplies and materials. The population of Oak Ridge soon expanded well beyond the initial plans, and peaked at 75,000 in May 1945, by which time 82,000 people were employed at the Clinton Engineer Works, and 10,000 by Roane-Anderson.\n\nFine-arts photographer, Josephine Herrick, and her colleague, Mary Steers, helped document the work at Oak Ridge.\n\nThe idea of locating Project Y at Oak Ridge was considered, but in the end it was decided that it should be in a remote location. On Oppenheimer's recommendation, the search for a suitable site was narrowed to the vicinity of Albuquerque, New Mexico, where Oppenheimer owned a ranch. In October 1942, Major John H. Dudley of the Manhattan Project was sent to survey the area, and he recommended a site near Jemez Springs, New Mexico. On 16 November, Oppenheimer, Groves, Dudley and others toured the site. Oppenheimer feared that the high cliffs surrounding the site would make his people feel claustrophobic, while the engineers were concerned with the possibility of flooding. The party then moved on to the vicinity of the Los Alamos Ranch School. Oppenheimer was impressed and expressed a strong preference for the site, citing its natural beauty and views of the Sangre de Cristo Mountains, which, it was hoped, would inspire those who would work on the project. The engineers were concerned about the poor access road, and whether the water supply would be adequate, but otherwise felt that it was ideal.\nPatterson approved the acquisition of the site on 25 November 1942, authorizing $440,000 for the purchase of the site of , all but of which were already owned by the Federal Government. Secretary of Agriculture Claude R. Wickard granted use of some of United States Forest Service land to the War Department \"for so long as the military necessity continues\". The need for land, for a new road, and later for a right of way for a power line, eventually brought wartime land purchases to , but only $414,971 was spent. Construction was contracted to the M. M. Sundt Company of Tucson, Arizona, with Willard C. Kruger and Associates of Santa Fe, New Mexico, as architect and engineer. Work commenced in December 1942. Groves initially allocated $300,000 for construction, three times Oppenheimer's estimate, with a planned completion date of 15 March 1943. It soon became clear that the scope of Project Y was greater than expected, and by the time Sundt finished on 30 November 1943, over $7 million had been spent.\nBecause it was secret, Los Alamos was referred to as \"Site Y\" or \"the Hill\". Birth certificates of babies born in Los Alamos during the war listed their place of birth as PO Box 1663 in Santa Fe. Initially Los Alamos was to have been a military laboratory with Oppenheimer and other researchers commissioned into the Army. Oppenheimer went so far as to order himself a lieutenant colonel's uniform, but two key physicists, Robert Bacher and Isidor Rabi, balked at the idea. Conant, Groves and Oppenheimer then devised a compromise whereby the laboratory was operated by the University of California under contract to the War Department.\n\nAn Army-OSRD council on 25 June 1942 decided to build a pilot plant for plutonium production in Red Gate Woods southwest of Chicago. In July, Nichols arranged for a lease of from the Cook County Forest Preserve District, and Captain James F. Grafton was appointed Chicago area engineer. It soon became apparent that the scale of operations was too great for the area, and it was decided to build the plant at Oak Ridge, and keep a research and testing facility in Chicago.\n\nDelays in establishing the plant in Red Gate Woods led Compton to authorize the Metallurgical Laboratory to construct the first nuclear reactor beneath the bleachers of Stagg Field at the University of Chicago. The reactor required an enormous amount of graphite blocks and uranium pellets. At the time, there was a limited source of pure uranium. Frank Spedding of Iowa State University were able to produce only two short tons of pure uranium. Additional three short tons of uranium metal was supplied by Westinghouse Lamp Plant which was produced in a rush with makeshift process. A large square balloon was constructed by Goodyear Tire to encase the reactor. On 2 December 1942, a team led by Enrico Fermi initiated the first artificial self-sustaining nuclear chain reaction in an experimental reactor known as Chicago Pile-1. The point at which a reaction becomes self-sustaining became known as \"going critical\". Compton reported the success to Conant in Washington, D.C., by a coded phone call, saying, \"The Italian navigator [Fermi] has just landed in the new world.\"\n\nIn January 1943, Grafton's successor, Major Arthur V. Peterson, ordered Chicago Pile-1 dismantled and reassembled at Red Gate Woods, as he regarded the operation of a reactor as too hazardous for a densely populated area. At the Argonne site, Chicago Pile-3, the first heavy water reactor, went critical on 15 May 1944. After the war, the operations that remained at Red Gate moved to the new site of the Argonne National Laboratory about away.\n\nBy December 1942 there were concerns that even Oak Ridge was too close to a major population center (Knoxville) in the unlikely event of a major nuclear accident. Groves recruited DuPont in November 1942 to be the prime contractor for the construction of the plutonium production complex. DuPont was offered a standard cost plus fixed-fee contract, but the President of the company, Walter S. Carpenter, Jr., wanted no profit of any kind, and asked for the proposed contract to be amended to explicitly exclude the company from acquiring any patent rights. This was accepted, but for legal reasons a nominal fee of one dollar was agreed upon. After the war, DuPont asked to be released from the contract early, and had to return 33 cents.\nDuPont recommended that the site be located far from the existing uranium production facility at Oak Ridge. In December 1942, Groves dispatched Colonel Franklin Matthias and DuPont engineers to scout potential sites. Matthias reported that Hanford Site near Richland, Washington, was \"ideal in virtually all respects\". It was isolated and near the Columbia River, which could supply sufficient water to cool the reactors that would produce the plutonium. Groves visited the site in January and established the Hanford Engineer Works (HEW), codenamed \"Site W\".\n\nUnder Secretary Patterson gave his approval on 9 February, allocating $5 million for the acquisition of of land in the area. The federal government relocated some 1,500 residents of White Bluffs and Hanford, and nearby settlements, as well as the Wanapum and other tribes using the area. A dispute arose with farmers over compensation for crops, which had already been planted before the land was acquired. Where schedules allowed, the Army allowed the crops to be harvested, but this was not always possible. The land acquisition process dragged on and was not completed before the end of the Manhattan Project in December 1946.\n\nThe dispute did not delay work. Although progress on the reactor design at Metallurgical Laboratory and DuPont was not sufficiently advanced to accurately predict the scope of the project, a start was made in April 1943 on facilities for an estimated 25,000 workers, half of whom were expected to live on-site. By July 1944, some 1,200 buildings had been erected and nearly 51,000 people were living in the construction camp. As area engineer, Matthias exercised overall control of the site. At its peak, the construction camp was the third most populous town in Washington state. Hanford operated a fleet of over 900 buses, more than the city of Chicago. Like Los Alamos and Oak Ridge, Richland was a gated community with restricted access, but it looked more like a typical wartime American boomtown: the military profile was lower, and physical security elements like high fences, towers, and guard dogs were less evident.\n\nCominco had produced electrolytic hydrogen at Trail, British Columbia, since 1930. Urey suggested in 1941 that it could produce heavy water. To the existing $10 million plant consisting of 3,215 cells consuming 75 MW of hydroelectric power, secondary electrolysis cells were added to increase the deuterium concentration in the water from 2.3% to 99.8%. For this process, Hugh Taylor of Princeton developed a platinum-on-carbon catalyst for the first three stages while Urey developed a nickel-chromia one for the fourth stage tower. The final cost was $2.8 million. The Canadian Government did not officially learn of the project until August 1942. Trail's heavy water production started in January 1944 and continued until 1956. Heavy water from Trail was used for Chicago Pile 3, the first reactor using heavy water and natural uranium, which went critical on 15 May 1944.\n\nThe Chalk River, Ontario, site was established to rehouse the Allied effort at the Montreal Laboratory away from an urban area. A new community was built at Deep River, Ontario, to provide residences and facilities for the team members. The site was chosen for its proximity to the industrial manufacturing area of Ontario and Quebec, and proximity to a rail head adjacent to a large military base, Camp Petawawa. Located on the Ottawa River, it had access to abundant water. The first director of the new laboratory was Hans von Halban. He was replaced by John Cockcroft in May 1944, who in turn was succeeded by Bennett Lewis in September 1946. A pilot reactor known as ZEEP (zero-energy experimental pile) became the first Canadian reactor, and the first to be completed outside the United States, when it went critical in September 1945, ZEEP remained in use by researchers until 1970. A larger 10 MW NRX reactor, which was designed during the war, was completed and went critical in July 1947.\n\nThe Eldorado Mine at Port Radium was a source of uranium ore.\n\nAlthough DuPont's preferred designs for the nuclear reactors were helium cooled and used graphite as a moderator, DuPont still expressed an interest in using heavy water as a backup, in case the graphite reactor design proved infeasible for some reason. For this purpose, it was estimated that of heavy water would be required per month. The \"P-9 Project\" was the government's code name for the heavy water production program. As the plant at Trail, which was then under construction, could produce per month, additional capacity was required. Groves therefore authorized DuPont to establish heavy water facilities at the Morgantown Ordnance Works, near Morgantown, West Virginia; at the Wabash River Ordnance Works, near Dana and Newport, Indiana; and at the Alabama Ordnance Works, near Childersburg and Sylacauga, Alabama. Although known as Ordnance Works and paid for under Ordnance Department contracts, they were built and operated by the Army Corps of Engineers. The American plants used a process different from Trail's; heavy water was extracted by distillation, taking advantage of the slightly higher boiling point of heavy water.\n\nThe key raw material for the project was uranium, which was used as fuel for the reactors, as feed that was transformed into plutonium, and, in its enriched form, in the atomic bomb itself. There were four known major deposits of uranium in 1940: in Colorado, in northern Canada, in Joachimsthal in Czechoslovakia, and in the Belgian Congo. All but Joachimstal were in allied hands. A November 1942 survey determined that sufficient quantities of uranium were available to satisfy the project's requirements. Nichols arranged with the State Department for export controls to be placed on uranium oxide and negotiated for the purchase of of uranium ore from the Belgian Congo that was being stored in a warehouse on Staten Island and the remaining stocks of mined ore stored in the Congo. He negotiated with Eldorado Gold Mines for the purchase of ore from its refinery in Port Hope, Ontario, and its shipment in 100-ton lots. The Canadian government subsequently bought up the company's stock until it acquired a controlling interest.\n\nWhile these purchases assured a sufficient supply to meet wartime needs, the American and British leaders concluded that it was in their countries' interest to gain control of as much of the world's uranium deposits as possible. The richest source of ore was the Shinkolobwe mine in the Belgian Congo, but it was flooded and closed. Nichols unsuccessfully attempted to negotiate its reopening and the sale of the entire future output to the United States with Edgar Sengier, the director of the company that owned the mine, Union Minière du Haut Katanga. The matter was then taken up by the Combined Policy Committee. As 30 percent of Union Minière's stock was controlled by British interests, the British took the lead in negotiations. Sir John Anderson and Ambassador John Winant hammered out a deal with Sengier and the Belgian government in May 1944 for the mine to be reopened and of ore to be purchased at $1.45 a pound. To avoid dependence on the British and Canadians for ore, Groves also arranged for the purchase of US Vanadium Corporation's stockpile in Uravan, Colorado. Uranium mining in Colorado yielded about of ore.\n\nMallinckrodt Incorporated in St. Louis, Missouri, took the raw ore and dissolved it in nitric acid to produce uranyl nitrate. Ether was then added in a liquid–liquid extraction process to separate the impurities from the uranyl nitrate. This was then heated to form uranium trioxide, which was reduced to highly pure uranium dioxide. By July 1942, Mallinckrodt was producing a ton of highly pure oxide a day, but turning this into uranium metal initially proved more difficult for contractors Westinghouse and Metal Hydrides. Production was too slow and quality was unacceptably low. A special branch of the Metallurgical Laboratory was established at Iowa State College in Ames, Iowa, under Frank Spedding to investigate alternatives. This became known as the Ames Project, and its Ames process became available in 1943.\n\nNatural uranium consists of 99.3% uranium-238 and 0.7% uranium-235, but only the latter is fissile. The chemically identical uranium-235 has to be physically separated from the more plentiful isotope. Various methods were considered for uranium enrichment, most of which was carried out at Oak Ridge.\n\nThe most obvious technology, the centrifuge, failed, but electromagnetic separation, gaseous diffusion, and thermal diffusion technologies were all successful and contributed to the project. In February 1943, Groves came up with the idea of using the output of some plants as the input for others.\n\nThe centrifuge process was regarded as the only promising separation method in April 1942. Jesse Beams had developed such a process at the University of Virginia during the 1930s, but had encountered technical difficulties. The process required high rotational speeds, but at certain speeds harmonic vibrations developed that threatened to tear the machinery apart. It was therefore necessary to accelerate quickly through these speeds. In 1941 he began working with uranium hexafluoride, the only known gaseous compound of uranium, and was able to separate uranium-235. At Columbia, Urey had Karl Cohen investigate the process, and he produced a body of mathematical theory making it possible to design a centrifugal separation unit, which Westinghouse undertook to construct.\n\nScaling this up to a production plant presented a formidable technical challenge. Urey and Cohen estimated that producing a kilogram (2.2 lb) of uranium-235 per day would require up to 50,000 centrifuges with rotors, or 10,000 centrifuges with rotors, assuming that 4-meter rotors could be built. The prospect of keeping so many rotors operating continuously at high speed appeared daunting, and when Beams ran his experimental apparatus, he obtained only 60% of the predicted yield, indicating that more centrifuges would be required. Beams, Urey and Cohen then began work on a series of improvements which promised to increase the efficiency of the process. However, frequent failures of motors, shafts and bearings at high speeds delayed work on the pilot plant. In November 1942 the centrifuge process was abandoned by the Military Policy Committee following a recommendation by Conant, Nichols and August C. Klein of Stone & Webster.\n\nElectromagnetic isotope separation was developed by Lawrence at the University of California Radiation Laboratory. This method employed devices known as calutrons, a hybrid of the standard laboratory mass spectrometer and cyclotron. The name was derived from the words \"California\", \"university\" and \"cyclotron\". In the electromagnetic process, a magnetic field deflected charged particles according to mass. The process was neither scientifically elegant nor industrially efficient. Compared with a gaseous diffusion plant or a nuclear reactor, an electromagnetic separation plant would consume more scarce materials, require more manpower to operate, and cost more to build. Nonetheless, the process was approved because it was based on proven technology and therefore represented less risk. Moreover, it could be built in stages, and rapidly reach industrial capacity.\nMarshall and Nichols discovered that the electromagnetic isotope separation process would require of copper, which was in desperately short supply. However, silver could be substituted, in an 11:10 ratio. On 3 August 1942, Nichols met with Under Secretary of the Treasury Daniel W. Bell and asked for the transfer of 6,000 tons of silver bullion from the West Point Bullion Depository. \"Young man,\" Bell told him, \"you may think of silver in tons but the Treasury will always think of silver in troy ounces!\" Eventually, were used.\n\nThe silver bars were cast into cylindrical billets and taken to Phelps Dodge in Bayway, New Jersey, where they were extruded into strips thick, wide and long. These were wound onto magnetic coils by Allis-Chalmers in Milwaukee, Wisconsin. After the war, all the machinery was dismantled and cleaned and the floorboards beneath the machinery were ripped up and burned to recover minute amounts of silver. In the end, only 1/3,600,000th was lost. The last silver was returned in May 1970.\n\nResponsibility for the design and construction of the electromagnetic separation plant, which came to be called Y-12, was assigned to Stone & Webster by the S-1 Committee in June 1942. The design called for five first-stage processing units, known as Alpha racetracks, and two units for final processing, known as Beta racetracks. In September 1943 Groves authorized construction of four more racetracks, known as Alpha II. Construction began in February 1943.\n\nWhen the plant was started up for testing on schedule in October, the 14-ton vacuum tanks crept out of alignment because of the power of the magnets, and had to be fastened more securely. A more serious problem arose when the magnetic coils started shorting out. In December Groves ordered a magnet to be broken open, and handfuls of rust were found inside. Groves then ordered the racetracks to be torn down and the magnets sent back to the factory to be cleaned. A pickling plant was established on-site to clean the pipes and fittings. The second Alpha I was not operational until the end of January 1944, the first Beta and first and third Alpha I's came online in March, and the fourth Alpha I was operational in April. The four Alpha II racetracks were completed between July and October 1944.\nTennessee Eastman was contracted to manage Y-12 on the usual cost plus fixed-fee basis, with a fee of $22,500 per month plus $7,500 per racetrack for the first seven racetracks and $4,000 per additional racetrack. The calutrons were initially operated by scientists from Berkeley to remove bugs and achieve a reasonable operating rate. They were then turned over to trained Tennessee Eastman operators who had only a high school education. Nichols compared unit production data, and pointed out to Lawrence that the young \"hillbilly\" girl operators were outperforming his PhDs. They agreed to a production race and Lawrence lost, a morale boost for the Tennessee Eastman workers and supervisors. The girls were \"trained like soldiers not to reason why\", while \"the scientists could not refrain from time-consuming investigation of the cause of even minor fluctuations of the dials.\"\n\nY-12 initially enriched the uranium-235 content to between 13% and 15%, and shipped the first few hundred grams of this to Los Alamos in March 1944. Only 1 part in 5,825 of the uranium feed emerged as final product. Much of the rest was splattered over equipment in the process. Strenuous recovery efforts helped raise production to 10% of the uranium-235 feed by January 1945. In February the Alpha racetracks began receiving slightly enriched (1.4%) feed from the new S-50 thermal diffusion plant. The next month it received enhanced (5%) feed from the K-25 gaseous diffusion plant. By August K-25 was producing uranium sufficiently enriched to feed directly into the Beta tracks.\n\nThe most promising but also the most challenging method of isotope separation was gaseous diffusion. Graham's law states that the rate of effusion of a gas is inversely proportional to the square root of its molecular mass, so in a box containing a semi-permeable membrane and a mixture of two gases, the lighter molecules will pass out of the container more rapidly than the heavier molecules. The gas leaving the container is somewhat enriched in the lighter molecules, while the residual gas is somewhat depleted. The idea was that such boxes could be formed into a cascade of pumps and membranes, with each successive stage containing a slightly more enriched mixture. Research into the process was carried out at Columbia University by a group that included Harold Urey, Karl P. Cohen, and John R. Dunning.\nIn November 1942 the Military Policy Committee approved the construction of a 600-stage gaseous diffusion plant. On 14 December, M. W. Kellogg accepted an offer to construct the plant, which was codenamed K-25. A cost plus fixed-fee contract was negotiated, eventually totaling $2.5 million. A separate corporate entity called Kellex was created for the project, headed by Percival C. Keith, one of Kellogg's vice presidents. The process faced formidable technical difficulties. The highly corrosive gas uranium hexafluoride would have to be used, as no substitute could be found, and the motors and pumps would have to be vacuum tight and enclosed in inert gas. The biggest problem was the design of the barrier, which would have to be strong, porous and resistant to corrosion by uranium hexafluoride. The best choice for this seemed to be nickel. Edward Adler and Edward Norris created a mesh barrier from electroplated nickel. A six-stage pilot plant was built at Columbia to test the process, but the Norris-Adler prototype proved to be too brittle. A rival barrier was developed from powdered nickel by Kellex, the Bell Telephone Laboratories and the Bakelite Corporation. In January 1944, Groves ordered the Kellex barrier into production.\n\nKellex's design for K-25 called for a four-story long U-shaped structure containing 54 contiguous buildings. These were divided into nine sections. Within these were cells of six stages. The cells could be operated independently, or consecutively within a section. Similarly, the sections could be operated separately or as part of a single cascade. A survey party began construction by marking out the site in May 1943. Work on the main building began in October 1943, and the six-stage pilot plant was ready for operation on 17 April 1944. In 1945 Groves canceled the upper stages of the plant, directing Kellex to instead design and build a 540-stage side feed unit, which became known as K-27. Kellex transferred the last unit to the operating contractor, Union Carbide and Carbon, on 11 September 1945. The total cost, including the K-27 plant completed after the war, came to $480 million.\n\nThe production plant commenced operation in February 1945, and as cascade after cascade came online, the quality of the product increased. By April 1945, K-25 had attained a 1.1% enrichment and the output of the S-50 thermal diffusion plant began being used as feed. Some product produced the next month reached nearly 7% enrichment. In August, the last of the 2,892 stages commenced operation. K-25 and K-27 achieved their full potential in the early postwar period, when they eclipsed the other production plants and became the prototypes for a new generation of plants.\n\nThe thermal diffusion process was based on Sydney Chapman and David Enskog's theory, which explained that when a mixed gas passes through a temperature gradient, the heavier one tends to concentrate at the cold end and the lighter one at the warm end. Since hot gases tend to rise and cool ones tend to fall, this can be used as a means of isotope separation. This process was first demonstrated by Klaus Clusius and Gerhard Dickel in Germany in 1938. It was developed by US Navy scientists, but was not one of the enrichment technologies initially selected for use in the Manhattan Project. This was primarily due to doubts about its technical feasibility, but the inter-service rivalry between the Army and Navy also played a part.\nThe Naval Research Laboratory continued the research under Philip Abelson's direction, but there was little contact with the Manhattan Project until April 1944, when Captain William S. Parsons, the naval officer in charge of ordnance development at Los Alamos, brought Oppenheimer news of encouraging progress in the Navy's experiments on thermal diffusion. Oppenheimer wrote to Groves suggesting that the output of a thermal diffusion plant could be fed into Y-12. Groves set up a committee consisting of Warren K. Lewis, Eger Murphree and Richard Tolman to investigate the idea, and they estimated that a thermal diffusion plant costing $3.5 million could enrich of uranium per week to nearly 0.9% uranium-235. Groves approved its construction on 24 June 1944.\n\nGroves contracted with the H. K. Ferguson Company of Cleveland, Ohio, to build the thermal diffusion plant, which was designated S-50. Groves's advisers, Karl Cohen and W. I. Thompson from Standard Oil, estimated that it would take six months to build. Groves gave Ferguson just four. Plans called for the installation of 2,142 diffusion columns arranged in 21 racks. Inside each column were three concentric tubes. Steam, obtained from the nearby K-25 powerhouse at a pressure of and temperature of , flowed downward through the innermost nickel pipe, while water at flowed upward through the outermost iron pipe. Isotope separation occurred in the uranium hexafluoride gas between the nickel and copper pipes.\n\nWork commenced on 9 July 1944, and S-50 began partial operation in September. Ferguson operated the plant through a subsidiary known as Fercleve. The plant produced just of 0.852% uranium-235 in October. Leaks limited production and forced shutdowns over the next few months, but in June 1945 it produced . By March 1945, all 21 production racks were operating. Initially the output of S-50 was fed into Y-12, but starting in March 1945 all three enrichment processes were run in series. S-50 became the first stage, enriching from 0.71% to 0.89%. This material was fed into the gaseous diffusion process in the K-25 plant, which produced a product enriched to about 23%. This was, in turn, fed into Y-12, which boosted it to about 89%, sufficient for nuclear weapons.\n\nAbout of uranium enriched to 89% uranium-235 was delivered to Los Alamos by July 1945. The entire 50 kg, along with some 50%-enriched, averaging out to about 85% enriched, were used in Little Boy.\nThe second line of development pursued by the Manhattan Project used the fissile element plutonium. Although small amounts of plutonium exist in nature, the best way to obtain large quantities of the element is in a nuclear reactor, in which natural uranium is bombarded by neutrons. The uranium-238 is transmuted into uranium-239, which rapidly decays, first into neptunium-239 and then into plutonium-239. Only a small amount of the uranium-238 will be transformed, so the plutonium must be chemically separated from the remaining uranium, from any initial impurities, and from fission products.\n\nIn March 1943, DuPont began construction of a plutonium plant on a site at Oak Ridge. Intended as a pilot plant for the larger production facilities at Hanford, it included the air-cooled X-10 Graphite Reactor, a chemical separation plant, and support facilities. Because of the subsequent decision to construct water-cooled reactors at Hanford, only the chemical separation plant operated as a true pilot. The X-10 Graphite Reactor consisted of a huge block of graphite, long on each side, weighing around , surrounded by of high-density concrete as a radiation shield.\n\nThe greatest difficulty was encountered with the uranium slugs produced by Mallinckrodt and Metal Hydrides. These somehow had to be coated in aluminum to avoid corrosion and the escape of fission products into the cooling system. The Grasselli Chemical Company attempted to develop a hot dipping process without success. Meanwhile, Alcoa tried canning. A new process for flux-less welding was developed, and 97% of the cans passed a standard vacuum test, but high temperature tests indicated a failure rate of more than 50%. Nonetheless, production began in June 1943. The Metallurgical Laboratory eventually developed an improved welding technique with the help of General Electric, which was incorporated into the production process in October 1943.\n\nWatched by Fermi and Compton, the X-10 Graphite Reactor went critical on 4 November 1943 with about of uranium. A week later the load was increased to , raising its power generation to 500 kW, and by the end of the month the first 500 mg of plutonium was created. Modifications over time raised the power to 4,000 kW in July 1944. X-10 operated as a production plant until January 1945, when it was turned over to research activities.\n\nAlthough an air-cooled design was chosen for the reactor at Oak Ridge to facilitate rapid construction, it was recognized that this would be impractical for the much larger production reactors. Initial designs by the Metallurgical Laboratory and DuPont used helium for cooling, before they determined that a water-cooled reactor would be simpler, cheaper and quicker to build. The design did not become available until 4 October 1943; in the meantime, Matthias concentrated on improving the Hanford Site by erecting accommodations, improving the roads, building a railway switch line, and upgrading the electricity, water and telephone lines.\nAs at Oak Ridge, the most difficulty was encountered while canning the uranium slugs, which commenced at Hanford in March 1944. They were pickled to remove dirt and impurities, dipped in molten bronze, tin, and aluminum-silicon alloy, canned using hydraulic presses, and then capped using arc welding under an argon atmosphere. Finally, they were subjected to a series of tests to detect holes or faulty welds. Disappointingly, most canned slugs initially failed the tests, resulting in an output of only a handful of canned slugs per day. But steady progress was made and by June 1944 production increased to the point where it appeared that enough canned slugs would be available to start Reactor B on schedule in August 1944.\n\nWork began on Reactor B, the first of six planned 250 MW reactors, on 10 October 1943. The reactor complexes were given letter designations A through F, with B, D and F sites chosen to be developed first, as this maximised the distance between the reactors. They would be the only ones constructed during the Manhattan Project. Some of steel, of concrete, 50,000 concrete blocks and 71,000 concrete bricks were used to construct the high building.\n\nConstruction of the reactor itself commenced in February 1944. Watched by Compton, Matthias, DuPont's Crawford Greenewalt, Leona Woods and Fermi, who inserted the first slug, the reactor was powered up beginning on 13 September 1944. Over the next few days, 838 tubes were loaded and the reactor went critical. Shortly after midnight on 27 September, the operators began to withdraw the control rods to initiate production. At first all appeared well but around 03:00 the power level started to drop and by 06:30 the reactor had shut down completely. The cooling water was investigated to see if there was a leak or contamination. The next day the reactor started up again, only to shut down once more.\n\nFermi contacted Chien-Shiung Wu, who identified the cause of the problem as neutron poisoning from xenon-135, which has a half-life of 9.2 hours. Fermi, Woods, Donald J. Hughes and John Archibald Wheeler then calculated the nuclear cross section of xenon-135, which turned out to be 30,000 times that of uranium. DuPont engineer George Graves had deviated from the Metallurgical Laboratory's original design in which the reactor had 1,500 tubes arranged in a circle, and had added an additional 504 tubes to fill in the corners. The scientists had originally considered this overengineering a waste of time and money, but Fermi realized that by loading all 2,004 tubes, the reactor could reach the required power level and efficiently produce plutonium. Reactor D was started on 17 December 1944 and Reactor F on 25 February 1945.\n\nMeanwhile, the chemists considered the problem of how plutonium could be separated from uranium when its chemical properties were not known. Working with the minute quantities of plutonium available at the Metallurgical Laboratory in 1942, a team under Charles M. Cooper developed a lanthanum fluoride process for separating uranium and plutonium, which was chosen for the pilot separation plant. A second separation process, the bismuth phosphate process, was subsequently developed by Seaborg and Stanly G. Thomson. This process worked by toggling plutonium between its +4 and +6 oxidation states in solutions of bismuth phosphate. In the former state, the plutonium was precipitated; in the latter, it stayed in solution and the other products were precipitated.\n\nGreenewalt favored the bismuth phosphate process due to the corrosive nature of lanthanum fluoride, and it was selected for the Hanford separation plants. Once X-10 began producing plutonium, the pilot separation plant was put to the test. The first batch was processed at 40% efficiency but over the next few months this was raised to 90%.\n\nAt Hanford, top priority was initially given to the installations in the 300 area. This contained buildings for testing materials, preparing uranium, and assembling and calibrating instrumentation. One of the buildings housed the canning equipment for the uranium slugs, while another contained a small test reactor. Notwithstanding the high priority allocated to it, work on the 300 area fell behind schedule due to the unique and complex nature of the 300 area facilities, and wartime shortages of labor and materials.\n\nEarly plans called for the construction of two separation plants in each of the areas known as 200-West and 200-East. This was subsequently reduced to two, the T and U plants, in 200-West and one, the B plant, at 200-East. Each separation plant consisted of four buildings: a process cell building or \"canyon\" (known as 221), a concentration building (224), a purification building (231) and a magazine store (213). The canyons were each long and wide. Each consisted of forty cells.\n\nWork began on 221-T and 221-U in January 1944, with the former completed in September and the latter in December. The 221-B building followed in March 1945. Because of the high levels of radioactivity involved, all work in the separation plants had to be conducted by remote control using closed-circuit television, something unheard of in 1943. Maintenance was carried out with the aid of an overhead crane and specially designed tools. The 224 buildings were smaller because they had less material to process, and it was less radioactive. The 224-T and 224-U buildings were completed on 8 October 1944, and 224-B followed on 10 February 1945. The purification methods that were eventually used in 231-W were still unknown when construction commenced on 8 April 1944, but the plant was complete and the methods were selected by the end of the year. On 5 February 1945, Matthias hand-delivered the first shipment of 80 g of 95%-pure plutonium nitrate to a Los Alamos courier in Los Angeles.\n\nIn 1943, development efforts were directed to a gun-type fission weapon with plutonium called Thin Man. Initial research on the properties of plutonium was done using cyclotron-generated plutonium-239, which was extremely pure, but could only be created in very small amounts. Los Alamos received the first sample of plutonium from the Clinton X-10 reactor in April 1944 and within days Emilio Segrè discovered a problem: the reactor-bred plutonium had a higher concentration of plutonium-240, resulting in up to five times the spontaneous fission rate of cyclotron plutonium. Seaborg had correctly predicted in March 1943 that some of the plutonium-239 would absorb a neutron and become plutonium-240.\n\nThis made reactor plutonium unsuitable for use in a gun-type weapon. The plutonium-240 would start the chain reaction too quickly, causing a predetonation that would release enough energy to disperse the critical mass with a minimal amount of plutonium reacted (a fizzle). A faster gun was suggested but found to be impractical. The possibility of separating the isotopes was considered and rejected, as plutonium-240 is even harder to separate from plutonium-239 than uranium-235 from uranium-238.\n\nWork on an alternative method of bomb design, known as implosion, had begun earlier under the direction of the physicist Seth Neddermeyer. Implosion used explosives to crush a subcritical sphere of fissile material into a smaller and denser form. When the fissile atoms are packed closer together, the rate of neutron capture increases, and the mass becomes a critical mass. The metal needs to travel only a very short distance, so the critical mass is assembled in much less time than it would take with the gun method. Neddermeyer's 1943 and early 1944 investigations into implosion showed promise, but also made it clear that the problem would be much more difficult from a theoretical and engineering perspective than the gun design. In September 1943, John von Neumann, who had experience with shaped charges used in armor-piercing shells, argued that not only would implosion reduce the danger of predetonation and fizzle, but would make more efficient use of the fissionable material. He proposed using a spherical configuration instead of the cylindrical one that Neddermeyer was working on.\nBy July 1944, Oppenheimer had concluded plutonium could not be used in a gun design, and opted for implosion. The accelerated effort on an implosion design, codenamed Fat Man, began in August 1944 when Oppenheimer implemented a sweeping reorganization of the Los Alamos laboratory to focus on implosion. Two new groups were created at Los Alamos to develop the implosion weapon, X (for explosives) Division headed by explosives expert George Kistiakowsky and G (for gadget) Division under Robert Bacher. The new design that von Neumann and T (for theoretical) Division, most notably Rudolf Peierls, had devised used explosive lenses to focus the explosion onto a spherical shape using a combination of both slow and fast high explosives.\n\nThe design of lenses that detonated with the proper shape and velocity turned out to be slow, difficult and frustrating. Various explosives were tested before settling on composition B as the fast explosive and baratol as the slow explosive. The final design resembled a soccer ball, with 20 hexagonal and 12 pentagonal lenses, each weighing about . Getting the detonation just right required fast, reliable and safe electrical detonators, of which there were two for each lens for reliability. It was therefore decided to use exploding-bridgewire detonators, a new invention developed at Los Alamos by a group led by Luis Alvarez. A contract for their manufacture was given to Raytheon.\n\nTo study the behavior of converging shock waves, Robert Serber devised the RaLa Experiment, which used the short-lived radioisotope lanthanum-140, a potent source of gamma radiation. The gamma ray source was placed in the center of a metal sphere surrounded by the explosive lenses, which in turn were inside in an ionization chamber. This allowed the taking of an X-ray movie of the implosion. The lenses were designed primarily using this series of tests. In his history of the Los Alamos project, David Hawkins wrote: \"RaLa became the most important single experiment affecting the final bomb design\".\n\nWithin the explosives was the thick aluminum pusher, which provided a smooth transition from the relatively low density explosive to the next layer, the thick tamper of natural uranium. Its main job was to hold the critical mass together as long as possible, but it would also reflect neutrons back into the core. Some part of it might fission as well. To prevent predetonation by an external neutron, the tamper was coated in a thin layer of boron. A polonium-beryllium modulated neutron initiator, known as an \"urchin\" because its shape resembled a sea urchin, was developed to start the chain reaction at precisely the right moment. This work with the chemistry and metallurgy of radioactive polonium was directed by Charles Allen Thomas of the Monsanto Company and became known as the Dayton Project. Testing required up to 500 curies per month of polonium, which Monsanto was able to deliver. The whole assembly was encased in a duralumin bomb casing to protect it from bullets and flak.\nThe ultimate task of the metallurgists was to determine how to cast plutonium into a sphere. The difficulties became apparent when attempts to measure the density of plutonium gave inconsistent results. At first contamination was believed to be the cause, but it was soon determined that there were multiple allotropes of plutonium. The brittle α phase that exists at room temperature changes to the plastic β phase at higher temperatures. Attention then shifted to the even more malleable δ phase that normally exists in the 300 °C to 450 °C range. It was found that this was stable at room temperature when alloyed with aluminum, but aluminum emits neutrons when bombarded with alpha particles, which would exacerbate the pre-ignition problem. The metallurgists then hit upon a plutonium-gallium alloy, which stabilized the δ phase and could be hot pressed into the desired spherical shape. As plutonium was found to corrode readily, the sphere was coated with nickel.\n\nThe work proved dangerous. By the end of the war, half the experienced chemists and metallurgists had to be removed from work with plutonium when unacceptably high levels of the element appeared in their urine. A minor fire at Los Alamos in January 1945 led to a fear that a fire in the plutonium laboratory might contaminate the whole town, and Groves authorized the construction of a new facility for plutonium chemistry and metallurgy, which became known as the DP-site. The hemispheres for the first plutonium pit (or core) were produced and delivered on 2 July 1945. Three more hemispheres followed on 23 July and were delivered three days later.\n\nBecause of the complexity of an implosion-style weapon, it was decided that, despite the waste of fissile material, an initial test would be required. Groves approved the test, subject to the active material being recovered. Consideration was therefore given to a controlled fizzle, but Oppenheimer opted instead for a full-scale nuclear test, codenamed \"Trinity\".\nIn March 1944, planning for the test was assigned to Kenneth Bainbridge, a professor of physics at Harvard, working under Kistiakowsky. Bainbridge selected the bombing range near Alamogordo Army Airfield as the site for the test. Bainbridge worked with Captain Samuel P. Davalos on the construction of the Trinity Base Camp and its facilities, which included barracks, warehouses, workshops, an explosive magazine and a commissary.\n\nGroves did not relish the prospect of explaining the loss of a billion dollars worth of plutonium to a Senate committee, so a cylindrical containment vessel codenamed \"Jumbo\" was constructed to recover the active material in the event of a failure. Measuring long and wide, it was fabricated at great expense from of iron and steel by Babcock & Wilcox in Barberton, Ohio. Brought in a special railroad car to a siding in Pope, New Mexico, it was transported the last to the test site on a trailer pulled by two tractors. By the time it arrived, however, confidence in the implosion method was high enough, and the availability of plutonium was sufficient, that Oppenheimer decided not to use it. Instead, it was placed atop a steel tower from the weapon as a rough measure of how powerful the explosion would be. In the end, Jumbo survived, although its tower did not, adding credence to the belief that Jumbo would have successfully contained a fizzled explosion.\n\nA pre-test explosion was conducted on 7 May 1945 to calibrate the instruments. A wooden test platform was erected from Ground Zero and piled with of TNT spiked with nuclear fission products in the form of an irradiated uranium slug from Hanford, which was dissolved and poured into tubing inside the explosive. This explosion was observed by Oppenheimer and Groves's new deputy commander, Brigadier General Thomas Farrell. The pre-test produced data that proved vital for the Trinity test.\n\nFor the actual test, the weapon, nicknamed \"the gadget\", was hoisted to the top of a steel tower, as detonation at that height would give a better indication of how the weapon would behave when dropped from a bomber. Detonation in the air maximized the energy applied directly to the target, and generated less nuclear fallout. The gadget was assembled under the supervision of Norris Bradbury at the nearby McDonald Ranch House on 13 July, and precariously winched up the tower the following day. Observers included Bush, Chadwick, Conant, Farrell, Fermi, Groves, Lawrence, Oppenheimer and Tolman. At 05:30 on 16 July 1945 the gadget exploded with an energy equivalent of around 20 kilotons of TNT, leaving a crater of Trinitite (radioactive glass) in the desert wide. The shock wave was felt over away, and the mushroom cloud reached in height. It was heard as far away as El Paso, Texas, so Groves issued a cover story about an ammunition magazine explosion at Alamogordo Field.\nOppenheimer later recalled that, while witnessing the explosion, he thought of a verse from the Hindu holy book, the \"Bhagavad Gita\" (XI,12):\n\nYears later he would explain that another verse had also entered his head at that time:\nIn June 1944, the Manhattan Project employed some 129,000 workers, of whom 84,500 were construction workers, 40,500 were plant operators and 1,800 were military personnel. As construction activity fell off, the workforce declined to 100,000 a year later, but the number of military personnel increased to 5,600. Procuring the required numbers of workers, especially highly skilled workers, in competition with other vital wartime programs proved very difficult. In 1943, Groves obtained a special temporary priority for labor from the War Manpower Commission. In March 1944, both the War Production Board and the War Manpower Commission gave the project their highest priority.\nTolman and Conant, in their role as the project's scientific advisers, drew up a list of candidate scientists and had them rated by scientists already working on the project. Groves then sent a personal letter to the head of their university or company asking for them to be released for essential war work. At the University of Wisconsin–Madison, Stanislaw Ulam gave one of his students, Joan Hinton, an exam early, so she could leave to do war work. A few weeks later, Ulam received a letter from Hans Bethe, inviting him to join the project. Conant personally persuaded Kistiakowsky to join the project.\n\nOne source of skilled personnel was the Army itself, particularly the Army Specialized Training Program. In 1943, the MED created the Special Engineer Detachment (SED), with an authorized strength of 675. Technicians and skilled workers drafted into the Army were assigned to the SED. Another source was the Women's Army Corps (WAC). Initially intended for clerical tasks handling classified material, the WACs were soon tapped for technical and scientific tasks as well. On 1 February 1945, all military personnel assigned to the MED, including all SED detachments, were assigned to the 9812th Technical Service Unit, except at Los Alamos, where military personnel other than SED, including the WACs and Military Police, were assigned to the 4817th Service Command Unit.\n\nAn Associate Professor of Radiology at the University of Rochester School of Medicine, Stafford L. Warren, was commissioned as a colonel in the United States Army Medical Corps, and appointed as chief of the MED's Medical Section and Groves' medical advisor. Warren's initial task was to staff hospitals at Oak Ridge, Richland and Los Alamos. The Medical Section was responsible for medical research, but also for the MED's health and safety programs. This presented an enormous challenge, because workers were handling a variety of toxic chemicals, using hazardous liquids and gases under high pressures, working with high voltages, and performing experiments involving explosives, not to mention the largely unknown dangers presented by radioactivity and handling fissile materials. Yet in December 1945, the National Safety Council presented the Manhattan Project with the Award of Honor for Distinguished Service to Safety in recognition of its safety record. Between January 1943 and June 1945, there were 62 fatalities and 3,879 disabling injuries, which was about 62 percent below the rate of private industry.\nA 1945 \"Life\" article estimated that before the Hiroshima and Nagasaki bombings \"probably no more than a few dozen men in the entire country knew the full meaning of the Manhattan Project, and perhaps only a thousand others even were aware that work on atoms was involved.\" The magazine wrote that the more than 100,000 others employed with the project \"worked like moles in the dark\". Warned that disclosing the project's secrets was punishable by 10 years in prison or a $10,000 ($ today) fine, they saw enormous quantities of raw materials enter factories with nothing coming out, and monitored \"dials and switches while behind thick concrete walls mysterious reactions took place\" without knowing the purpose of their jobs.\n\nOak Ridge security personnel considered any private party with more than seven people as suspicious, and residents—who believed that US government agents were secretly among them—avoided repeatedly inviting the same guests. Although original residents of the area could be buried in existing cemeteries, every coffin was reportedly opened for inspection. Everyone, including top military officials, and their automobiles were searched when entering and exiting project facilities. One Oak Ridge worker stated that \"if you got inquisitive, you were called on the carpet within two hours by government secret agents. Usually those summoned to explain were then escorted bag and baggage to the gate and ordered to keep going\".\n\nDespite being told that their work would help end the war and perhaps all future wars, not seeing or understanding the results of their often tedious duties—or even typical side effects of factory work such as smoke from smokestacks—and the war in Europe ending without the use of their work, caused serious morale problems among workers and caused many rumors to spread. One manager stated after the war:\n\nAnother worker told of how, working in a laundry, she every day held \"a special instrument\" to uniforms and listened for \"a clicking noise\". She learned only after the war that she had been performing the important task of checking for radiation with a geiger counter. To improve morale among such workers Oak Ridge created an extensive system of intramural sports leagues, including 10 baseball teams, 81 softball teams, and 26 football teams.\n\nVoluntary censorship of atomic information began before the Manhattan Project. After the start of the European war in 1939 American scientists began avoiding publishing military-related research, and in 1940 scientific journals began asking the National Academy of Sciences to clear articles. William L. Laurence of \"The New York Times\", who wrote an article on atomic fission in \"The Saturday Evening Post\" of 7 September 1940, later learned that government officials asked librarians nationwide in 1943 to withdraw the issue. The Soviets noticed the silence, however. In April 1942 nuclear physicist Georgy Flyorov wrote to Josef Stalin on the absence of articles on nuclear fission in American journals; this resulted in the Soviet Union establishing its own atomic bomb project.\n\nThe Manhattan Project operated under tight security lest its discovery induce Axis powers, especially Germany, to accelerate their own nuclear projects or undertake covert operations against the project. The government's Office of Censorship, by contrast, relied on the press to comply with a voluntary code of conduct it published, and the project at first avoided notifying the office. By early 1943 newspapers began publishing reports of large construction in Tennessee and Washington based on public records, and the office began discussing with the project how to maintain secrecy. In June the Office of Censorship asked newspapers and broadcasters to avoid discussing \"atom smashing, atomic energy, atomic fission, atomic splitting, or any of their equivalents. The use for military purposes of radium or radioactive materials, heavy water, high voltage discharge equipment, cyclotrons.\" The office also asked to avoid discussion of \"polonium, uranium, ytterbium, hafnium, protactinium, radium, rhenium, thorium, deuterium\"; only uranium was sensitive, but was listed with other elements to hide its importance.\n\nThe prospect of sabotage was always present, and sometimes suspected when there were equipment failures. While there were some problems believed to be the result of careless or disgruntled employees, there were no confirmed instances of Axis-instigated sabotage. However, on 10 March 1945, a Japanese fire balloon struck a power line, and the resulting power surge caused the three reactors at Hanford to be temporarily shut down. With so many people involved, security was a difficult task. A special Counter Intelligence Corps detachment was formed to handle the project's security issues. By 1943, it was clear that the Soviet Union was attempting to penetrate the project. Lieutenant Colonel Boris T. Pash, the head of the Counter Intelligence Branch of the Western Defense Command, investigated suspected Soviet espionage at the Radiation Laboratory in Berkeley. Oppenheimer informed Pash that he had been approached by a fellow professor at Berkeley, Haakon Chevalier, about passing information to the Soviet Union.\n\nThe most successful Soviet spy was Klaus Fuchs, a member of the British Mission who played an important part at Los Alamos. The 1950 revelation of his espionage activities damaged the United States' nuclear cooperation with Britain and Canada. Subsequently, other instances of espionage were uncovered, leading to the arrest of Harry Gold, David Greenglass, and Ethel and Julius Rosenberg. Other spies like George Koval and Theodore Hall remained unknown for decades. The value of the espionage is difficult to quantify, as the principal constraint on the Soviet atomic bomb project was a shortage of uranium ore. The consensus is that espionage saved the Soviets one or two years of effort.\n\nIn addition to developing the atomic bomb, the Manhattan Project was charged with gathering intelligence on the German nuclear energy project. It was believed that the Japanese nuclear weapons program was not far advanced because Japan had little access to uranium ore, but it was initially feared that Germany was very close to developing its own weapons. At the instigation of the Manhattan Project, a bombing and sabotage campaign was carried out against heavy water plants in German-occupied Norway. A small mission was created, jointly staffed by the Office of Naval Intelligence, OSRD, the Manhattan Project, and Army Intelligence (G-2), to investigate enemy scientific developments. It was not restricted to those involving nuclear weapons. The Chief of Army Intelligence, Major General George V. Strong, appointed Boris Pash to command the unit, which was codenamed \"Alsos\", a Greek word meaning \"grove\".\nThe Alsos Mission to Italy questioned staff of the physics laboratory at the University of Rome following the capture of the city in June 1944. Meanwhile, Pash formed a combined British and American Alsos mission in London under the command of Captain Horace K. Calvert to participate in Operation Overlord. Groves considered the risk that the Germans might attempt to disrupt the Normandy landings with radioactive poisons was sufficient to warn General Dwight D. Eisenhower and send an officer to brief his chief of staff, Lieutenant General Walter Bedell Smith. Under the codename Operation Peppermint, special equipment was prepared and Chemical Warfare Service teams were trained in its use.\n\nFollowing in the wake of the advancing Allied armies, Pash and Calvert interviewed Frédéric Joliot-Curie about the activities of German scientists. They spoke to officials at Union Minière du Haut Katanga about uranium shipments to Germany. They tracked down 68 tons of ore in Belgium and 30 tons in France. The interrogation of German prisoners indicated that uranium and thorium were being processed in Oranienburg, 20 miles north of Berlin, so Groves arranged for it to be bombed on 15 March 1945.\n\nAn Alsos team went to Stassfurt in the Soviet Occupation Zone and retrieved 11 tons of ore from WIFO. In April 1945, Pash, in command of a composite force known as T-Force, conducted Operation Harborage, a sweep behind enemy lines of the cities of Hechingen, Bisingen, and Haigerloch that were the heart of the German nuclear effort. T-Force captured the nuclear laboratories, documents, equipment and supplies, including heavy water and 1.5 tons of metallic uranium.\n\nAlsos teams rounded up German scientists including Kurt Diebner, Otto Hahn, Walther Gerlach, Werner Heisenberg, and Carl Friedrich von Weizsäcker, who were taken to England where they were interned at Farm Hall, a bugged house in Godmanchester. After the bombs were detonated in Japan, the Germans were forced to confront the fact that the Allies had done what they could not.\n\nStarting in November 1943, the Army Air Forces Materiel Command at Wright Field, Ohio, began Silverplate, the codename modification of B-29s to carry the bombs. Test drops were carried out at Muroc Army Air Field, California, and the Naval Ordnance Test Station at Inyokern, California. Groves met with the Chief of United States Army Air Forces (USAAF), General Henry H. Arnold, in March 1944 to discuss the delivery of the finished bombs to their targets. The only Allied aircraft capable of carrying the long Thin Man or the wide Fat Man was the British Avro Lancaster, but using a British aircraft would have caused difficulties with maintenance. Groves hoped that the American Boeing B-29 Superfortress could be modified to carry Thin Man by joining its two bomb bays together. Arnold promised that no effort would be spared to modify B-29s to do the job, and designated Major General Oliver P. Echols as the USAAF liaison to the Manhattan Project. In turn, Echols named Colonel Roscoe C. Wilson as his alternate, and Wilson became Manhattan Project's main USAAF contact. President Roosevelt instructed Groves that if the atomic bombs were ready before the war with Germany ended, he should be ready to drop them on Germany.\nThe 509th Composite Group was activated on 17 December 1944 at Wendover Army Air Field, Utah, under the command of Colonel Paul W. Tibbets. This base, close to the border with Nevada, was codenamed \"Kingman\" or \"W-47\". Training was conducted at Wendover and at Batista Army Airfield, Cuba, where the 393d Bombardment Squadron practiced long-distance flights over water, and dropping dummy pumpkin bombs. A special unit known as Project Alberta was formed at Los Alamos under Navy Captain William S. Parsons from Project Y as part of the Manhattan Project to assist in preparing and delivering the bombs. Commander Frederick L. Ashworth from Alberta met with Fleet Admiral Chester W. Nimitz on Guam in February 1945 to inform him of the project. While he was there, Ashworth selected North Field on the Pacific Island Tinian as a base for the 509th Composite Group, and reserved space for the group and its buildings. The group deployed there in July 1945. Farrell arrived at Tinian on 30 July as the Manhattan Project representative.\n\nMost of the components for Little Boy left San Francisco on the cruiser on 16 July and arrived on Tinian on 26 July. Four days later the ship was sunk by a Japanese submarine. The remaining components, which included six uranium-235 rings, were delivered by three C-54 Skymasters of the 509th Group's 320th Troop Carrier Squadron. Two Fat Man assemblies travelled to Tinian in specially modified 509th Composite Group B-29s. The first plutonium core went in a special C-54. A joint targeting committee of the Manhattan District and USAAF was established to determine which cities in Japan should be targets, and recommended Kokura, Hiroshima, Niigata, and Kyoto. At this point, Secretary of War Henry L. Stimson intervened, announcing that he would be making the targeting decision, and that he would not authorize the bombing of Kyoto on the grounds of its historical and religious significance. Groves therefore asked Arnold to remove Kyoto not just from the list of nuclear targets, but from targets for conventional bombing as well. One of Kyoto's substitutes was Nagasaki.\n\nIn May 1945, the Interim Committee was created to advise on wartime and postwar use of nuclear energy. The committee was chaired by Stimson, with James F. Byrnes, a former US Senator soon to be Secretary of State, as President Harry S. Truman's personal representative; Ralph A. Bard, the Under Secretary of the Navy; William L. Clayton, the Assistant Secretary of State; Vannevar Bush; Karl T. Compton; James B. Conant; and George L. Harrison, an assistant to Stimson and president of New York Life Insurance Company. The Interim Committee in turn established a scientific panel consisting of Arthur Compton, Fermi, Lawrence and Oppenheimer to advise it on scientific issues. In its presentation to the Interim Committee, the scientific panel offered its opinion not just on the likely physical effects of an atomic bomb, but on its probable military and political impact.\n\nAt the Potsdam Conference in Germany, Truman was informed that the Trinity test had been successful. He told Stalin, the leader of the Soviet Union, that the US had a new superweapon, without giving any details. This was the first official communication to the Soviet Union about the bomb, but Stalin already knew about it from spies. With the authorization to use the bomb against Japan already given, no alternatives were considered after the Japanese rejection of the Potsdam Declaration.\nOn 6 August 1945, a Boeing B-29 Superfortress (\"Enola Gay\") of the 393d Bombardment Squadron, piloted by Tibbets, lifted off from North Field, and Little Boy in its bomb bay. Hiroshima, the headquarters of the 2nd General Army and Fifth Division and a port of embarkation, was the primary target of the mission, with Kokura and Nagasaki as alternatives. With Farrell's permission, Parsons, the weaponeer in charge of the mission, completed the bomb assembly in the air to minimize the risks during takeoff. The bomb detonated at an altitude of with a blast that was later estimated to be the equivalent of 13 kilotons of TNT. An area of approximately was destroyed. Japanese officials determined that 69% of Hiroshima's buildings were destroyed and another 6–7% damaged. About 70,000 to 80,000 people, of whom 20,000 were Japanese combatants and 20,000 were Korean slave laborers, or some 30% of the population of Hiroshima, were killed immediately, and another 70,000 injured.\n\nOn the morning of 9 August 1945, a second B-29 (\"Bockscar\"), piloted by the 393d Bombardment Squadron's commander, Major Charles W. Sweeney, lifted off with Fat Man on board. This time, Ashworth served as weaponeer and Kokura was the primary target. Sweeney took off with the weapon already armed but with the electrical safety plugs still engaged. When they reached Kokura, they found cloud cover had obscured the city, prohibiting the visual attack required by orders. After three runs over the city, and with fuel running low, they headed for the secondary target, Nagasaki. Ashworth decided that a radar approach would be used if the target was obscured, but a last-minute break in the clouds over Nagasaki allowed a visual approach as ordered. The Fat Man was dropped over the city's industrial valley midway between the Mitsubishi Steel and Arms Works in the south and the Mitsubishi-Urakami Ordnance Works in the north. The resulting explosion had a blast yield equivalent to 21 kilotons of TNT, roughly the same as the Trinity blast, but was confined to the Urakami Valley, and a major portion of the city was protected by the intervening hills, resulting in the destruction of about 44% of the city. The bombing also crippled the city's industrial production extensively and killed 23,200–28,200 Japanese industrial workers and 150 Japanese soldiers. Overall, an estimated 35,000–40,000 people were killed and 60,000 injured.\n\nGroves expected to have another atomic bomb ready for use on 19 August, with three more in September and a further three in October. Two more Fat Man assemblies were readied, and scheduled to leave Kirtland Field for Tinian on 11 and 14 August. At Los Alamos, technicians worked 24 hours straight to cast another plutonium core. Although cast, it still needed to be pressed and coated, which would take until 16 August. It could therefore have been ready for use on 19 August. On 10 August, Truman secretly requested that additional atomic bombs not be dropped on Japan without his express authority. Groves suspended the third core's shipment on his own authority on 13 August.\n\nOn 11 August, Groves phoned Warren with orders to organize a survey team to report on the damage and radioactivity at Hiroshima and Nagasaki. A party equipped with portable Geiger counters arrived in Hiroshima on 8 September headed by Farrell and Warren, with Japanese Rear Admiral Masao Tsuzuki, who acted as a translator. They remained in Hiroshima until 14 September and then surveyed Nagasaki from 19 September to 8 October. This and other scientific missions to Japan would provide valuable scientific and historical data.\n\nThe necessity of the bombings of Hiroshima and Nagasaki became a subject of controversy among historians. Some questioned whether an \"atomic diplomacy\" would not have attained the same goals and disputed whether the bombings or the Soviet declaration of war on Japan was decisive. The Franck Report was the most notable effort pushing for a demonstration but was turned down by the Interim Committee's scientific panel. The Szilárd petition, drafted in July 1945 and signed by dozens of scientists working on the Manhattan Project, was a late attempt at warning President Harry S. Truman about his responsibility in using such weapons.\n\nSeeing the work they had not understood produce the Hiroshima and Nagasaki bombs amazed the workers of the Manhattan Project as much as the rest of the world; newspapers in Oak Ridge announcing the Hiroshima bomb sold for $1 ($ today). Although the bombs' existence was public, secrecy continued, and many workers remained ignorant of their jobs; one stated in 1946, \"I don't know what the hell I'm doing besides looking into a ——— and turning a ——— alongside a ———. I don't know anything about it, and there's nothing to say\". Many residents continued to avoid discussion of \"the stuff\" in ordinary conversation despite it being the reason for their town's existence.\n\nIn anticipation of the bombings, Groves had Henry DeWolf Smyth prepare a history for public consumption. \"Atomic Energy for Military Purposes\", better known as the \"Smyth Report\", was released to the public on 12 August 1945. Groves and Nichols presented Army–Navy \"E\" Awards to key contractors, whose involvement had hitherto been secret. Over 20 awards of the Presidential Medal for Merit were made to key contractors and scientists, including Bush and Oppenheimer. Military personnel received the Legion of Merit, including the commander of the Women's Army Corps detachment, Captain Arlene G. Scheidenhelm.\n\nAt Hanford, plutonium production fell off as Reactors B, D and F wore out, poisoned by fission products and swelling of the graphite moderator known as the Wigner effect. The swelling damaged the charging tubes where the uranium was irradiated to produce plutonium, rendering them unusable. In order to maintain the supply of polonium for the urchin initiators, production was curtailed and the oldest unit, B pile, was closed down so at least one reactor would be available in the future. Research continued, with DuPont and the Metallurgical Laboratory developing a redox solvent extraction process as an alternative plutonium extraction technique to the bismuth phosphate process, which left unspent uranium in a state from which it could not easily be recovered.\n\nBomb engineering was carried out by the Z Division, named for its director, Dr. Jerrold R. Zacharias from Los Alamos. Z Division was initially located at Wendover Field but moved to Oxnard Field, New Mexico, in September 1945 to be closer to Los Alamos. This marked the beginning of Sandia Base. Nearby Kirtland Field was used as a B-29 base for aircraft compatibility and drop tests. By October, all the staff and facilities at Wendover had been transferred to Sandia. As reservist officers were demobilized, they were replaced by about fifty hand-picked regular officers.\n\nNichols recommended that S-50 and the Alpha tracks at Y-12 be closed down. This was done in September. Although performing better than ever, the Alpha tracks could not compete with K-25 and the new K-27, which had commenced operation in January 1946. In December, the Y-12 plant was closed, thereby cutting the Tennessee Eastman payroll from 8,600 to 1,500 and saving $2 million a month.\nNowhere was demobilization more of a problem than at Los Alamos, where there was an exodus of talent. Much remained to be done. The bombs used on Hiroshima and Nagasaki were like laboratory pieces; work would be required to make them simpler, safer and more reliable. Implosion methods needed to be developed for uranium in place of the wasteful gun method, and composite uranium-plutonium cores were needed now that plutonium was in short supply because of the problems with the reactors. However, uncertainty about the future of the laboratory made it hard to induce people to stay. Oppenheimer returned to his job at the University of California and Groves appointed Norris Bradbury as an interim replacement. In fact, Bradbury would remain in the post for the next 25 years. Groves attempted to combat the dissatisfaction caused by the lack of amenities with a construction program that included an improved water supply, three hundred houses, and recreation facilities.\n\nTwo Fat Man–type detonations were conducted at Bikini Atoll in July 1946 as part of Operation Crossroads to investigate the effect of nuclear weapons on warships. Able was detonated on 1 July 1946. The more spectacular Baker was detonated underwater on 25 July 1946.\n\nAfter the bombings at Hiroshima and Nagasaki, a number of Manhattan Project physicists founded the \"Bulletin of the Atomic Scientists\", which began as an emergency action undertaken by scientists who saw urgent need for an immediate educational program about atomic weapons. In the face of the destructiveness of the new weapons and in anticipation of the nuclear arms race several project members including Bohr, Bush and Conant expressed the view that it was necessary to reach agreement on international control of nuclear research and atomic weapons. The Baruch Plan, unveiled in a speech to the newly formed United Nations Atomic Energy Commission (UNAEC) in June 1946, proposed the establishment of an international atomic development authority, but was not adopted.\n\nFollowing a domestic debate over the permanent management of the nuclear program, the United States Atomic Energy Commission (AEC) was created by the Atomic Energy Act of 1946 to take over the functions and assets of the Manhattan Project. It established civilian control over atomic development, and separated the development, production and control of atomic weapons from the military. Military aspects were taken over by the Armed Forces Special Weapons Project (AFSWP). Although the Manhattan Project ceased to exist on 31 December 1946, the Manhattan District was not abolished until 15 August 1947.\n\nThe project expenditure through 1 October 1945 was $1.845 billion, equivalent to less than nine days of wartime spending, and was $2.191 billion when the AEC assumed control on 1 January 1947. Total allocation was $2.4 billion. Over 90% of the cost was for building plants and producing the fissionable materials, and less than 10% for development and production of the weapons.\n\nA total of four weapons (the Trinity gadget, Little Boy, Fat Man, and an unused bomb) were produced by the end of 1945, making the average cost per bomb around $500 million in 1945 dollars. By comparison, the project's total cost by the end of 1945 was about 90% of the total spent on the production of US small arms (not including ammunition) and 34% of the total spent on US tanks during the same period. Overall, it was the second most expensive weapons project undertaken by the United States in World War II, behind only the design and production of the Boeing B-29 Superfortress.\n\nThe political and cultural impacts of the development of nuclear weapons were profound and far-reaching. William Laurence of \"The New York Times\", the first to use the phrase \"Atomic Age\", became the official correspondent for the Manhattan Project in spring 1945. In 1943 and 1944 he unsuccessfully attempted to persuade the Office of Censorship to permit writing about the explosive potential of uranium, and government officials felt that he had earned the right to report on the biggest secret of the war. Laurence witnessed both the Trinity test and the bombing of Nagasaki and wrote the official press releases prepared for them. He went on to write a series of articles extolling the virtues of the new weapon. His reporting before and after the bombings helped to spur public awareness of the potential of nuclear technology and motivated its development in the United States and the Soviet Union.\n\nThe wartime Manhattan Project left a legacy in the form of the network of national laboratories: the Lawrence Berkeley National Laboratory, Los Alamos National Laboratory, Oak Ridge National Laboratory, Argonne National Laboratory, and Ames Laboratory. Two more were established by Groves soon after the war, the Brookhaven National Laboratory at Upton, New York, and the Sandia National Laboratories at Albuquerque, New Mexico. Groves allocated $72 million to them for research activities in fiscal year 1946–1947. They would be in the vanguard of the kind of large-scale research that Alvin Weinberg, the director of the Oak Ridge National Laboratory, would call Big Science.\n\nThe Naval Research Laboratory had long been interested in the prospect of using nuclear power for warship propulsion, and sought to create its own nuclear project. In May 1946, Nimitz, now Chief of Naval Operations, decided that the Navy should instead work with the Manhattan Project. A group of naval officers were assigned to Oak Ridge, the most senior of whom was Captain Hyman G. Rickover, who became assistant director there. They immersed themselves in the study of nuclear energy, laying the foundations for a nuclear-powered navy. A similar group of Air Force personnel arrived at Oak Ridge in September 1946 with the aim of developing nuclear aircraft. Their Nuclear Energy for the Propulsion of Aircraft (NEPA) project ran into formidable technical difficulties, and was ultimately cancelled.\n\nThe ability of the new reactors to create radioactive isotopes in previously unheard-of quantities sparked a revolution in nuclear medicine in the immediate postwar years. Starting in mid-1946, Oak Ridge began distributing radioisotopes to hospitals and universities. Most of the orders were for iodine-131 and phosphorus-32, which were used in the diagnosis and treatment of cancer. In addition to medicine, isotopes were also used in biological, industrial and agricultural research.\n\nOn handing over control to the Atomic Energy Commission, Groves bid farewell to the people who had worked on the Manhattan Project:\n\nIn 2014, the United States Congress passed a law providing for a national park dedicated to the history of the Manhattan Project. The Manhattan Project National Historical Park was established on 10 November 2015.\n\n\n\n\n"}
{"id": "19605", "url": "https://en.wikipedia.org/wiki?curid=19605", "title": "Main sequence", "text": "Main sequence\n\nIn astronomy, the main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. These color-magnitude plots are known as Hertzsprung–Russell diagrams after their co-developers, Ejnar Hertzsprung and Henry Norris Russell. Stars on this band are known as main-sequence stars or dwarf stars. These are the most numerous true stars in the universe, and include the Earth's Sun.\n\nAfter condensation and ignition of a star, it generates thermal energy in its dense core region through nuclear fusion of hydrogen into helium. During this stage of the star's lifetime, it is located on the main sequence at a position determined primarily by its mass, but also based upon its chemical composition and age. The cores of main-sequence stars are in hydrostatic equilibrium, where outward thermal pressure from the hot core is balanced by the inward pressure of gravitational collapse from the overlying layers. The strong dependence of the rate of energy generation on temperature and pressure helps to sustain this balance. Energy generated at the core makes its way to the surface and is radiated away at the photosphere. The energy is carried by either radiation or convection, with the latter occurring in regions with steeper temperature gradients, higher opacity or both.\n\nThe main sequence is sometimes divided into upper and lower parts, based on the dominant process that a star uses to generate energy. Stars below about 1.5 times the mass of the Sun () primarily fuse hydrogen atoms together in a series of stages to form helium, a sequence called the proton–proton chain. Above this mass, in the upper main sequence, the nuclear fusion process mainly uses atoms of carbon, nitrogen and oxygen as intermediaries in the CNO cycle that produces helium from hydrogen atoms. Main-sequence stars with more than two solar masses undergo convection in their core regions, which acts to stir up the newly created helium and maintain the proportion of fuel needed for fusion to occur. Below this mass, stars have cores that are entirely radiative with convective zones near the surface. With decreasing stellar mass, the proportion of the star forming a convective envelope steadily increases. Main-sequence stars below undergo convection throughout their mass. When core convection does not occur, a helium-rich core develops surrounded by an outer layer of hydrogen.\n\nIn general, the more massive a star is, the shorter its lifespan on the main sequence. After the hydrogen fuel at the core has been consumed, the star evolves away from the main sequence on the HR diagram, into a supergiant, red giant, or directly to a white dwarf.\n\nIn the early part of the 20th century, information about the types and distances of stars became more readily available. The spectra of stars were shown to have distinctive features, which allowed them to be categorized. Annie Jump Cannon and Edward C. Pickering at Harvard College Observatory developed a method of categorization that became known as the Harvard Classification Scheme, published in the \"Harvard Annals\" in 1901.\n\nIn Potsdam in 1906, the Danish astronomer Ejnar Hertzsprung noticed that the reddest stars—classified as K and M in the Harvard scheme—could be divided into two distinct groups. These stars are either much brighter than the Sun, or much fainter. To distinguish these groups, he called them \"giant\" and \"dwarf\" stars. The following year he began studying star clusters; large groupings of stars that are co-located at approximately the same distance. He published the first plots of color versus luminosity for these stars. These plots showed a prominent and continuous sequence of stars, which he named the Main Sequence.\n\nAt Princeton University, Henry Norris Russell was following a similar course of research. He was studying the relationship between the spectral classification of stars and their actual brightness as corrected for distance—their absolute magnitude. For this purpose he used a set of stars that had reliable parallaxes and many of which had been categorized at Harvard. When he plotted the spectral types of these stars against their absolute magnitude, he found that dwarf stars followed a distinct relationship. This allowed the real brightness of a dwarf star to be predicted with reasonable accuracy.\n\nOf the red stars observed by Hertzsprung, the dwarf stars also followed the spectra-luminosity relationship discovered by Russell. However, the giant stars are much brighter than dwarfs and so do not follow the same relationship. Russell proposed that the \"giant stars must have low density or great surface-brightness, and the reverse is true of dwarf stars\". The same curve also showed that there were very few faint white stars.\n\nIn 1933, Bengt Strömgren introduced the term Hertzsprung–Russell diagram to denote a luminosity-spectral class diagram. This name reflected the parallel development of this technique by both Hertzsprung and Russell earlier in the century.\n\nAs evolutionary models of stars were developed during the 1930s, it was shown that, for stars of a uniform chemical composition, a relationship exists between a star's mass and its luminosity and radius. That is, for a given mass and composition, there is a unique solution for determining the star's radius and luminosity. This became known as the Vogt-Russell theorem; named after Heinrich Vogt and Henry Norris Russell. By this theorem, when a star's chemical composition and its position on the main sequence is known, so too is the star's mass and radius. (However, it was subsequently discovered that the theorem breaks down somewhat for stars of non-uniform composition.)\n\nA refined scheme for stellar classification was published in 1943 by William Wilson Morgan and Philip Childs Keenan. The MK classification assigned each star a spectral type—based on the Harvard classification—and a luminosity class. The Harvard classification had been developed by assigning a different letter to each star based on the strength of the hydrogen spectral line, before the relationship between spectra and temperature was known. When ordered by temperature and when duplicate classes were removed, the spectral types of stars followed, in order of decreasing temperature with colors ranging from blue to red, the sequence O, B, A, F, G, K and M. (A popular mnemonic for memorizing this sequence of stellar classes is \"Oh Be A Fine Girl/Guy, Kiss Me\".) The luminosity class ranged from I to V, in order of decreasing luminosity. Stars of luminosity class V belonged to the main sequence.\n\nIn April 2018, astronomers reported the detection of the most distant \"ordinary\" (i.e., main sequence) star, named Icarus (formally, MACS J1149 Lensed Star 1), at 9 billion light-years away from Earth.\n\nWhen a protostar is formed from the collapse of a giant molecular cloud of gas and dust in the local interstellar medium, the initial composition is homogeneous throughout, consisting of about 70% hydrogen, 28% helium and trace amounts of other elements, by mass. The initial mass of the star depends on the local conditions within the cloud. (The mass distribution of newly formed stars is described empirically by the initial mass function.) During the initial collapse, this pre-main-sequence star generates energy through gravitational contraction. Upon reaching a suitable density, energy generation is begun at the core using an exothermic nuclear fusion process that converts hydrogen into helium.\n\nWhen nuclear fusion of hydrogen becomes the dominant energy production process and the excess energy gained from gravitational contraction has been lost, the star lies along a curve on the Hertzsprung–Russell diagram (or HR diagram) called the standard main sequence. Astronomers will sometimes refer to this stage as \"zero age main sequence\", or ZAMS. The ZAMS curve can be calculated using computer models of stellar properties at the point when stars begin hydrogen fusion. From this point, the brightness and surface temperature of stars typically increase with age.\n\nA star remains near its initial position on the main sequence until a significant amount of hydrogen in the core has been consumed, then begins to evolve into a more luminous star. (On the HR diagram, the evolving star moves up and to the right of the main sequence.) Thus the main sequence represents the primary hydrogen-burning stage of a star's lifetime.\n\nThe majority of stars on a typical HR diagram lie along the main-sequence curve. This line is pronounced because both the spectral type and the luminosity depend only on a star's mass, at least to zeroth-order approximation, as long as it is fusing hydrogen at its core—and that is what almost all stars spend most of their \"active\" lives doing.\n\nThe temperature of a star determines its spectral type via its effect on the physical properties of plasma in its photosphere. A star's energy emission as a function of wavelength is influenced by both its temperature and composition. A key indicator of this energy distribution is given by the color index, \"B\" − \"V\", which measures the star's magnitude in blue (\"B\") and green-yellow (\"V\") light by means of filters. This difference in magnitude provides a measure of a star's temperature.\n\nMain-sequence stars are called dwarf stars, but this terminology is partly historical and can be somewhat confusing. For the cooler stars, dwarfs such as red dwarfs, orange dwarfs, and yellow dwarfs are indeed much smaller and dimmer than other stars of those colors. However, for hotter blue and white stars, the size and brightness difference between so-called \"dwarf\" stars that are on the main sequence and the so-called \"giant\" stars that are not becomes smaller; for the hottest stars it is not directly observable. For those stars the terms \"dwarf\" and \"giant\" refer to differences in spectral lines which indicate if a star is on the main sequence or off it. Nevertheless, very hot main-sequence stars are still sometimes called dwarfs, even though they have roughly the same size and brightness as the \"giant\" stars of that temperature.\n\nThe common use of \"dwarf\" to mean main sequence is confusing in another way, because there are dwarf stars which are not main-sequence stars. For example, a white dwarf is the dead core of a star that is left after the star has shed its outer layers, that is much smaller than a main-sequence star, roughly the size of Earth. These represent the final evolutionary stage of many main-sequence stars.\n\nBy treating the star as an idealized energy radiator known as a black body, the luminosity \"L\" and radius \"R\" can be related to the effective temperature \"T\" by the Stefan–Boltzmann law:\n\nwhere \"σ\" is the Stefan–Boltzmann constant. As the position of a star on the HR diagram shows its approximate luminosity, this relation can be used to estimate its radius.\n\nThe mass, radius and luminosity of a star are closely interlinked, and their respective values can be approximated by three relations. First is the Stefan–Boltzmann law, which relates the luminosity \"L\", the radius \"R\" and the surface temperature \"T\". Second is the mass–luminosity relation, which relates the luminosity \"L\" and the mass \"M\". Finally, the relationship between \"M\" and \"R\" is close to linear. The ratio of \"M\" to \"R\" increases by a factor of only three over 2.5 orders of magnitude of \"M\". This relation is roughly proportional to the star's inner temperature \"T\", and its extremely slow increase reflects the fact that the rate of energy generation in the core strongly depends on this temperature, whereas it has to fit the mass–luminosity relation. Thus, a too high or too low temperature will result in stellar instability.\n\nA better approximation is to take \"ε\" = \"L/M\", the energy generation rate per unit mass, as ε is proportional to \"T\", where \"T\" is the core temperature. This is suitable for stars at least as massive as the Sun, exhibiting the CNO cycle, and gives the better fit \"R\" ∝ \"M\".\n\nThe table below shows typical values for stars along the main sequence. The values of luminosity (\"L\"), radius (\"R\") and mass (\"M\") are relative to the Sun—a dwarf star with a spectral classification of G2 V. The actual values for a star may vary by as much as 20–30% from the values listed below.\n\nAll main-sequence stars have a core region where energy is generated by nuclear fusion. The temperature and density of this core are at the levels necessary to sustain the energy production that will support the remainder of the star. A reduction of energy production would cause the overlaying mass to compress the core, resulting in an increase in the fusion rate because of higher temperature and pressure. Likewise an increase in energy production would cause the star to expand, lowering the pressure at the core. Thus the star forms a self-regulating system in hydrostatic equilibrium that is stable over the course of its main sequence lifetime.\n\nMain-sequence stars employ two types of hydrogen fusion processes, and the rate of energy generation from each type depends on the temperature in the core region. Astronomers divide the main sequence into upper and lower parts, based on which of the two is the dominant fusion process. In the lower main sequence, energy is primarily generated as the result of the proton-proton chain, which directly fuses hydrogen together in a series of stages to produce helium. Stars in the upper main sequence have sufficiently high core temperatures to efficiently use the CNO cycle. (See the chart.) This process uses atoms of carbon, nitrogen and oxygen as intermediaries in the process of fusing hydrogen into helium.\n\nAt a stellar core temperature of 18 million Kelvin, the PP process and CNO cycle are equally efficient, and each type generates half of the star's net luminosity. As this is the core temperature of a star with about 1.5 , the upper main sequence consists of stars above this mass. Thus, roughly speaking, stars of spectral class F or cooler belong to the lower main sequence, while A-type stars or hotter are upper main-sequence stars. The transition in primary energy production from one form to the other spans a range difference of less than a single solar mass. In the Sun, a one solar-mass star, only 1.5% of the energy is generated by the CNO cycle. By contrast, stars with 1.8 or above generate almost their entire energy output through the CNO cycle.\n\nThe observed upper limit for a main-sequence star is 120–200 . The theoretical explanation for this limit is that stars above this mass can not radiate energy fast enough to remain stable, so any additional mass will be ejected in a series of pulsations until the star reaches a stable limit. The lower limit for sustained proton–proton nuclear fusion is about 0.08 or 80 times the mass of Jupiter. Below this threshold are sub-stellar objects that can not sustain hydrogen fusion, known as brown dwarfs.\n\nBecause there is a temperature difference between the core and the surface, or photosphere, energy is transported outward. The two modes for transporting this energy are radiation and convection. A radiation zone, where energy is transported by radiation, is stable against convection and there is very little mixing of the plasma. By contrast, in a convection zone the energy is transported by bulk movement of plasma, with hotter material rising and cooler material descending. Convection is a more efficient mode for carrying energy than radiation, but it will only occur under conditions that create a steep temperature gradient.\n\nIn massive stars (above 10 ) the rate of energy generation by the CNO cycle is very sensitive to temperature, so the fusion is highly concentrated at the core. Consequently, there is a high temperature gradient in the core region, which results in a convection zone for more efficient energy transport. This mixing of material around the core removes the helium ash from the hydrogen-burning region, allowing more of the hydrogen in the star to be consumed during the main-sequence lifetime. The outer regions of a massive star transport energy by radiation, with little or no convection.\n\nIntermediate-mass stars such as Sirius may transport energy primarily by radiation, with a small core convection region. Medium-sized, low-mass stars like the Sun have a core region that is stable against convection, with a convection zone near the surface that mixes the outer layers. This results in a steady buildup of a helium-rich core, surrounded by a hydrogen-rich outer region. By contrast, cool, very low-mass stars (below 0.4 ) are convective throughout. Thus the helium produced at the core is distributed across the star, producing a relatively uniform atmosphere and a proportionately longer main sequence lifespan.\n\nAs non-fusing helium ash accumulates in the core of a main-sequence star, the reduction in the abundance of hydrogen per unit mass results in a gradual lowering of the fusion rate within that mass. Since it is the outflow of fusion-supplied energy that supports the higher layers of the star, the core is compressed, producing higher temperatures and pressures. Both factors increase the rate of fusion thus moving the equilibrium towards a smaller, denser, hotter core producing more energy whose increased outflow pushes the higher layers further out. Thus there is a steady increase in the luminosity and radius of the star over time. For example, the luminosity of the early Sun was only about 70% of its current value. As a star ages this luminosity increase changes its position on the HR diagram. This effect results in a broadening of the main sequence band because stars are observed at random stages in their lifetime. That is, the main sequence band develops a thickness on the HR diagram; it is not simply a narrow line.\n\nOther factors that broaden the main sequence band on the HR diagram include uncertainty in the distance to stars and the presence of unresolved binary stars that can alter the observed stellar parameters. However, even perfect observation would show a fuzzy main sequence because mass is not the only parameter that affects a star's color and luminosity. Variations in chemical composition caused by the initial abundances, the star's evolutionary status, interaction with a close companion, rapid rotation, or a magnetic field can all slightly change a main-sequence star's HR diagram position, to name just a few factors. As an example, there are metal-poor stars (with a very low abundance of elements with higher atomic numbers than helium) that lie just below the main sequence and are known as subdwarfs. These stars are fusing hydrogen in their cores and so they mark the lower edge of main sequence fuzziness caused by variance in chemical composition.\n\nA nearly vertical region of the HR diagram, known as the instability strip, is occupied by pulsating variable stars known as Cepheid variables. These stars vary in magnitude at regular intervals, giving them a pulsating appearance. The strip intersects the upper part of the main sequence in the region of class \"A\" and \"F\" stars, which are between one and two solar masses. Pulsating stars in this part of the instability strip that intersects the upper part of the main sequence are called Delta Scuti variables. Main-sequence stars in this region experience only small changes in magnitude and so this variation is difficult to detect. Other classes of unstable main-sequence stars, like Beta Cephei variables, are unrelated to this instability strip.\n\nThe total amount of energy that a star can generate through nuclear fusion of hydrogen is limited by the amount of hydrogen fuel that can be consumed at the core. For a star in equilibrium, the energy generated at the core must be at least equal to the energy radiated at the surface. Since the luminosity gives the amount of energy radiated per unit time, the total life span can be estimated, to first approximation, as the total energy produced divided by the star's luminosity.\n\nFor a star with at least 0.5 , when the hydrogen supply in its core is exhausted and it expands to become a red giant, it can start to fuse helium atoms to form carbon. The energy output of the helium fusion process per unit mass is only about a tenth the energy output of the hydrogen process, and the luminosity of the star increases. This results in a much shorter length of time in this stage compared to the main sequence lifetime. (For example, the Sun is predicted to spend burning helium, compared to about 12 billion years burning hydrogen.) Thus, about 90% of the observed stars above 0.5 will be on the main sequence. On average, main-sequence stars are known to follow an empirical mass-luminosity relationship. The luminosity (\"L\") of the star is roughly proportional to the total mass (\"M\") as the following power law:\n\nThis relationship applies to main-sequence stars in the range 0.1–50 .\n\nThe amount of fuel available for nuclear fusion is proportional to the mass of the star. Thus, the lifetime of a star on the main sequence can be estimated by comparing it to solar evolutionary models. The Sun has been a main-sequence star for about 4.5 billion years and it will become a red giant in 6.5 billion years, for a total main sequence lifetime of roughly 10 years. Hence:\n\nwhere \"M\" and \"L\" are the mass and luminosity of the star, respectively, formula_4 is a solar mass, formula_5 is the solar luminosity and formula_6 is the star's estimated main sequence lifetime.\n\nAlthough more massive stars have more fuel to burn and might intuitively be expected to last longer, they also radiate a proportionately greater amount with increased mass. This is required by the stellar equation of state; for a massive star to maintain equilibrium, the outward pressure of radiated energy generated in the core not only must but \"will\" rise to match the titanic inward gravitational pressure of its envelope. Thus, the most massive stars may remain on the main sequence for only a few million years, while stars with less than a tenth of a solar mass may last for over a trillion years.\n\nThe exact mass-luminosity relationship depends on how efficiently energy can be transported from the core to the surface. A higher opacity has an insulating effect that retains more energy at the core, so the star does not need to produce as much energy to remain in hydrostatic equilibrium. By contrast, a lower opacity means energy escapes more rapidly and the star must burn more fuel to remain in equilibrium. Note, however, that a sufficiently high opacity can result in energy transport via convection, which changes the conditions needed to remain in equilibrium.\n\nIn high-mass main-sequence stars, the opacity is dominated by electron scattering, which is nearly constant with increasing temperature. Thus the luminosity only increases as the cube of the star's mass. For stars below 10 , the opacity becomes dependent on temperature, resulting in the luminosity varying approximately as the fourth power of the star's mass. For very low-mass stars, molecules in the atmosphere also contribute to the opacity. Below about 0.5 , the luminosity of the star varies as the mass to the power of 2.3, producing a flattening of the slope on a graph of mass versus luminosity. Even these refinements are only an approximation, however, and the mass-luminosity relation can vary depending on a star's composition.\n\nWhen a main-sequence star has consumed the hydrogen at its core, the loss of energy generation causes its gravitational collapse to resume and the star evolves off the main sequence. The path which the star follows across the HR diagram is called an evolutionary track.\n\nStars with less than , are predicted to directly become white dwarfs when energy generation by nuclear fusion of hydrogen at their core comes to a halt although no stars are old enough for this to have occurred.\nIn stars more massive than , the hydrogen surrounding the helium core reaches sufficient temperature and pressure to undergo fusion, forming a hydrogen-burning shell and causing the outer layers of the star to expand and cool. The stage as these stars move away from the main sequence is known as the subgiant branch; it is relatively brief and appears as a gap in the evolutionary track since few stars are observed at that point.\n\nWhen the helium core of low-mass stars becomes degenerate, or the outer layers of intermediate-mass stars cool sufficiently to become opaque, their hydrogen shells increase in temperature and the stars start to become more luminous. This is known as the red giant branch; it is a relatively long-lived stage and it appears prominently in H-R diagrams. These stars will eventually end their lives as white dwarfs.\n\nThe most massive stars do not become red giants, instead their cores quickly become hot enough to fuse helium and eventually heavier elements and they are known as supergiants. They follow approximately horizontal evolutionary tracks from the main sequence across the top of the H-R diagram. Supergiants are relatively rare and do not show prominently on most H-R diagrams. Their cores will eventually collapse, usually leading to a supernova and leaving behind either a neutron star or black hole.\n\nWhen a cluster of stars is formed at about the same time, the main sequence lifespan of these stars will depend on their individual masses. The most massive stars will leave the main sequence first, followed in sequence by stars of ever lower masses. The position where stars in the cluster are leaving the main sequence is known as the turnoff point. By knowing the main sequence lifespan of stars at this point, it becomes possible to estimate the age of the cluster.\n\n\n"}
{"id": "19609", "url": "https://en.wikipedia.org/wiki?curid=19609", "title": "Memory leak", "text": "Memory leak\n\nIn computer science, a memory leak is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in such a way that memory which is no longer needed is not released. A memory leak may also happen when an object is stored in memory but cannot be accessed by the running code. A memory leak has symptoms similar to a number of other problems and generally can only be diagnosed by a programmer with access to the program's source code.\n\nA space leak occurs when a computer program uses more memory than necessary. In contrast to memory leaks, where the leaked memory is never released, the memory consumed by a space leak is released, but later than expected. \n\nBecause they can exhaust available system memory as an application runs, memory leaks are often the cause of or a contributing factor to software aging.\n\nA memory leak reduces the performance of the computer by reducing the amount of available memory. Eventually, in the worst case, too much of the available memory may become allocated and all or part of the system or device stops working correctly, the application fails, or the system slows down vastly due to thrashing.\n\nMemory leaks may not be serious or even detectable by normal means. In modern operating systems, normal memory used by an application is released when the application terminates. This means that a memory leak in a program that only runs for a short time may not be noticed and is rarely serious.\n\nMuch more serious leaks include those:\n\n\nThe following example, written in pseudocode, is intended to show how a memory leak can come about, and its effects, without needing any programming knowledge. The program in this case is part of some very simple software designed to control an elevator. This part of the program is run whenever anyone inside the elevator presses the button for a floor.\n\nThe memory leak would occur if the floor number requested is the same floor that the elevator is on; the condition for releasing the memory would be skipped. Each time this case occurs, more memory is leaked.\n\nCases like this wouldn't usually have any immediate effects. People do not often press the button for the floor they are already on, and in any case, the elevator might have enough spare memory that this could happen hundreds or thousands of times. However, the elevator will eventually run out of memory. This could take months or years, so it might not be discovered despite thorough testing.\n\nThe consequences would be unpleasant; at the very least, the elevator would stop responding to requests to move to another floor (such as when an attempt is made to call the elevator or when someone is inside and presses the floor buttons). If other parts of the program need memory (a part assigned to open and close the door, for example), then someone may be trapped inside, or if no one is in, then no one would be able to use the elevator since the software cannot open the door.\n\nThe memory leak lasts until the system is reset. For example: if the elevator's power were turned off or in a power outage, the program would stop running. When power was turned on again, the program would restart and all the memory would be available again, but the slow process of memory leak would restart together with the program, eventually prejudicing the correct running of the system.\n\nThe leak in the above example can be corrected by bringing the 'release' operation outside of the conditional:\n\nMemory leaks are a common error in programming, especially when using languages that have no built in automatic garbage collection, such as C and C++. Typically, a memory leak occurs because dynamically allocated memory has become unreachable. The prevalence of memory leak bugs has led to the development of a number of debugging tools to detect unreachable memory. \"BoundsChecker\", \"Deleaker\", \"IBM Rational Purify\", \"Valgrind\", \"Parasoft Insure++\", \"Dr. Memory\" and \"memwatch\" are some of the more popular memory debuggers for C and C++ programs. \"Conservative\" garbage collection capabilities can be added to any programming language that lacks it as a built-in feature, and libraries for doing this are available for C and C++ programs. A conservative collector finds and reclaims most, but not all, unreachable memory.\n\nAlthough the memory manager can recover unreachable memory, it cannot free memory that is still reachable and therefore potentially still useful. Modern memory managers therefore provide techniques for programmers to semantically mark memory with varying levels of usefulness, which correspond to varying levels of \"reachability\". The memory manager does not free an object that is strongly reachable. An object is strongly reachable if it is reachable either directly by a strong reference or indirectly by a chain of strong references. (A \"strong reference\" is a reference that, unlike a weak reference, prevents an object from being garbage collected.) To prevent this, the developer is responsible for cleaning up references after use, typically by setting the reference to null once it is no longer needed and, if necessary, by deregistering any event listeners that maintain strong references to the object.\n\nIn general, automatic memory management is more robust and convenient for developers, as they don't need to implement freeing routines or worry about the sequence in which cleanup is performed or be concerned about whether or not an object is still referenced. It is easier for a programmer to know when a reference is no longer needed than to know when an object is no longer referenced. However, automatic memory management can impose a performance overhead, and it does not eliminate all of the programming errors that cause memory leaks.\n\nRAII, short for Resource Acquisition Is Initialization, is an approach to the problem commonly taken in C++, D, and Ada. It involves associating scoped objects with the acquired resources, and automatically releasing the resources once the objects are out of scope. Unlike garbage collection, RAII has the advantage of knowing when objects exist and when they do not. Compare the following C and C++ examples:\n\nThe C version, as implemented in the example, requires explicit deallocation; the array is dynamically allocated (from the heap in most C implementations), and continues to exist until explicitly freed.\n\nThe C++ version requires no explicit deallocation; it will always occur automatically as soon as the object codice_1 goes out of scope, including if an exception is thrown. This avoids some of the overhead of garbage collection schemes. And because object destructors can free resources other than memory, RAII helps to prevent the leaking of input and output resources accessed through a handle, which mark-and-sweep garbage collection does not handle gracefully. These include open files, open windows, user notifications, objects in a graphics drawing library, thread synchronisation primitives such as critical sections, network connections, and connections to the Windows Registry or another database.\n\nHowever, using RAII correctly is not always easy and has its own pitfalls. For instance, if one is not careful, it is possible to create dangling pointers (or references) by returning data by reference, only to have that data be deleted when its containing object goes out of scope.\n\nD uses a combination of RAII and garbage collection, employing automatic destruction when it is clear that an object cannot be accessed outside its original scope, and garbage collection otherwise.\n\nMore modern garbage collection schemes are often based on a notion of reachability – if you don't have a usable reference to the memory in question, it can be collected. Other garbage collection schemes can be based on reference counting, where an object is responsible for keeping track of how many references are pointing to it. If the number goes down to zero, the object is expected to release itself and allow its memory to be reclaimed. The flaw with this model is that it doesn't cope with cyclic references, and this is why nowadays most programmers are prepared to accept the burden of the more costly mark and sweep type of systems.\n\nThe following Visual Basic code illustrates the canonical reference-counting memory leak:\n\nIn practice, this trivial example would be spotted straight away and fixed. In most real examples, the cycle of references spans more than two objects, and is more difficult to detect.\n\nA well-known example of this kind of leak came to prominence with the rise of AJAX programming techniques in web browsers in the lapsed listener problem. JavaScript code which associated a DOM element with an event handler, and failed to remove the reference before exiting, would leak memory (AJAX web pages keep a given DOM alive for a lot longer than traditional web pages, so this leak was much more apparent).\n\nIf a program has a memory leak and its memory usage is steadily increasing, there will not usually be an immediate symptom. Every physical system has a finite amount of memory, and if the memory leak is not contained (for example, by restarting the leaking program) it will eventually cause problems.\n\nMost modern consumer desktop operating systems have both main memory which is physically housed in RAM microchips, and secondary storage such as a hard drive. Memory allocation is dynamic – each process gets as much memory as it requests. Active pages are transferred into main memory for fast access; inactive pages are pushed out to secondary storage to make room, as needed. When a single process starts consuming a large amount of memory, it usually occupies more and more of main memory, pushing other programs out to secondary storage – usually significantly slowing performance of the system. Even if the leaking program is terminated, it may take some time for other programs to swap back into main memory, and for performance to return to normal.\n\nWhen all the memory on a system is exhausted (whether there is virtual memory or only main memory, such as on an embedded system) any attempt to allocate more memory will fail. This usually causes the program attempting to allocate the memory to terminate itself, or to generate a segmentation fault. Some programs are designed to recover from this situation (possibly by falling back on pre-reserved memory). The first program to experience the out-of-memory may or may not be the program that has the memory leak.\n\nSome multi-tasking operating systems have special mechanisms to deal with an out-of-memory condition, such as killing processes at random (which may affect \"innocent\" processes), or killing the largest process in memory (which presumably is the one causing the problem). Some operating systems have a per-process memory limit, to prevent any one program from hogging all of the memory on the system. The disadvantage to this arrangement is that the operating system sometimes must be re-configured to allow proper operation of programs that legitimately require large amounts of memory, such as those dealing with graphics, video, or scientific calculations.\n\nIf the memory leak is in the kernel, the operating system itself will likely fail. Computers without sophisticated memory management, such as embedded systems, may also completely fail from a persistent memory leak.\n\nPublicly accessible systems such as web servers or routers are prone to denial-of-service attacks if an attacker discovers a sequence of operations which can trigger a leak. Such a sequence is known as an exploit.\n\nA \"sawtooth\" pattern of memory utilization may be an indicator of a memory leak within an application, particularly if the vertical drops coincide with reboots or restarts of that application. Care should be taken though because garbage collection points could also cause such a pattern and would show a healthy usage of the heap.\n\nNote that constantly increasing memory usage is not necessarily evidence of a memory leak. Some applications will store ever increasing amounts of information in memory (e.g. as a cache). If the cache can grow so large as to cause problems, this may be a programming or design error, but is not a memory leak as the information remains nominally in use. In other cases, programs may require an unreasonably large amount of memory because the programmer has assumed memory is always sufficient for a particular task; for example, a graphics file processor might start by reading the entire contents of an image file and storing it all into memory, something that is not viable where a very large image exceeds available memory.\n\nTo put it another way, a memory leak arises from a particular kind of programming error, and without access to the program code, someone seeing symptoms can only guess that there \"might\" be a memory leak. It would be better to use terms such as \"constantly increasing memory use\" where no such inside knowledge exists.\n\nThe following C function deliberately leaks memory by losing the pointer to the allocated memory. The leak can be said to occur as soon as the pointer 'a' goes out of scope, i.e. when function_which_allocates() returns without freeing 'a'.\n\n\n"}
{"id": "19614", "url": "https://en.wikipedia.org/wiki?curid=19614", "title": "Molecular orbital", "text": "Molecular orbital\n\nIn chemistry, a molecular orbital (MO) is a mathematical function describing the wave-like behavior of an electron in a molecule. This function can be used to calculate chemical and physical properties such as the probability of finding an electron in any specific region. The term \"orbital\" was introduced by Robert S. Mulliken in 1932 as an abbreviation for \"one-electron orbital wave function\". At an elementary level, it is used to describe the \"region\" of space in which the function has a significant amplitude. Molecular orbitals are usually constructed by combining atomic orbitals or hybrid orbitals from each atom of the molecule, or other molecular orbitals from groups of atoms. They can be quantitatively calculated using the Hartree–Fock or self-consistent field (SCF) methods.\n\nA molecular orbital (MO) can be used to represent the regions in a molecule where an electron occupying that orbital is likely to be found. Molecular orbitals are obtained from the combination of atomic orbitals, which predict the location of an electron in an atom. A molecular orbital can specify the electron configuration of a molecule: the spatial distribution and energy of one (or one pair of) electron(s). Most commonly a MO is represented as a linear combination of atomic orbitals (the LCAO-MO method), especially in qualitative or very approximate usage. They are invaluable in providing a simple model of bonding in molecules, understood through molecular orbital theory.\nMost present-day methods in computational chemistry begin by calculating the MOs of the system. A molecular orbital describes the behavior of one electron in the electric field generated by the nuclei and some average distribution of the other electrons. In the case of two electrons occupying the same orbital, the Pauli principle demands that they have opposite spin. Necessarily this is an approximation, and highly accurate descriptions of the molecular electronic wave function do not have orbitals (see configuration interaction).\n\nMolecular orbitals are, in general, delocalized throughout the entire molecule. Moreover, if the molecule has symmetry elements, its nondegenerate molecular orbitals are either symmetric or antisymmetric with respect to any of these symmetries. In other words, application of a symmetry operation S (e.g., a reflection, rotation, or inversion) to molecular orbital ψ results in the molecular orbital being unchanged or reversing its mathematical sign: Sψ = ±ψ. In planar molecules, for example, molecular orbitals are either symmetric (sigma) or antisymmetric (pi) with respect to reflection in the molecular plane. If molecules with degenerate orbital energies are also considered, a more general statement that molecular orbitals form bases for the irreducible representations of the molecule's symmetry group holds. The symmetry properties of molecular orbitals means that delocalization is an inherent feature of molecular orbital theory and makes it fundamentally different from (and complementary to) valence bond theory, in which bonds are viewed as localized electron pairs, with allowance for resonance to account for delocalization.\n\nIn contrast to these symmetry-correct \"canonical\" molecular orbitals, localized molecular orbitals can be formed by applying certain mathematical transformations to the canonical orbitals. The advantage of this approach is that the orbitals will correspond more closely to the \"bonds\" of a molecule as depicted by a Lewis structure. As a disadvantage, the energy levels of these localized orbitals no longer have physical meaning. (The discussion in the rest of this article will focus on canonical molecular orbitals. For further discussions on localized molecular orbitals, see: natural bond orbital and sigma-pi and equivalent-orbital models.)\n\nMolecular orbitals arise from allowed interactions between atomic orbitals, which are allowed if the symmetries (determined from group theory) of the atomic orbitals are compatible with each other. Efficiency of atomic orbital interactions is determined from the overlap (a measure of how well two orbitals constructively interact with one another) between two atomic orbitals, which is significant if the atomic orbitals are close in energy. Finally, the number of molecular orbitals formed must be equal to the number of atomic orbitals in the atoms being combined to form the molecule.\n\nFor an imprecise, but qualitatively useful, discussion of the molecular structure, the molecular orbitals can be obtained from the \"Linear combination of atomic orbitals molecular orbital method\" ansatz. Here, the molecular orbitals are expressed as linear combinations of atomic orbitals.\n\nMolecular orbitals were first introduced by Friedrich Hund and Robert S. Mulliken in 1927 and 1928. The linear combination of atomic orbitals or \"LCAO\" approximation for molecular orbitals was introduced in 1929 by Sir John Lennard-Jones. His ground-breaking paper showed how to derive the electronic structure of the fluorine and oxygen molecules from quantum principles. This qualitative approach to molecular orbital theory is part of the start of modern quantum chemistry.\nLinear combinations of atomic orbitals (LCAO) can be used to estimate the molecular orbitals that are formed upon bonding between the molecule's constituent atoms. Similar to an atomic orbital, a Schrödinger equation, which describes the behavior of an electron, can be constructed for a molecular orbital as well. Linear combinations of atomic orbitals, or the sums and differences of the atomic wavefunctions, provide approximate solutions to the Hartree–Fock equations which correspond to the independent-particle approximation of the molecular Schrödinger equation. For simple diatomic molecules, the wavefunctions obtained are represented mathematically by the equations\n\nwhere formula_3 and formula_4 are the molecular wavefunctions for the bonding and antibonding molecular orbitals, respectively, formula_5 and formula_6 are the atomic wavefunctions from atoms a and b, respectively, and formula_7 and formula_8 are adjustable coefficients. These coefficients can be positive or negative, depending on the energies and symmetries of the individual atomic orbitals. As the two atoms become closer together, their atomic orbitals overlap to produce areas of high electron density, and, as a consequence, molecular orbitals are formed between the two atoms. The atoms are held together by the electrostatic attraction between the positively charged nuclei and the negatively charged electrons occupying bonding molecular orbitals.\n\nWhen atomic orbitals interact, the resulting molecular orbital can be of three types: bonding, antibonding, or nonbonding.\n\nBonding MOs:\n\nAntibonding MOs: \nNonbonding MOs: \n\nThe type of interaction between atomic orbitals can be further categorized by the molecular-orbital symmetry labels σ (sigma), π (pi), δ (delta), φ (phi), γ (gamma) etc. These are the Greek letters corresponding to the atomic orbitals s, p, d, f and g respectively. The number of nodal planes containing the internuclear axis between the atoms concerned is zero for σ MOs, one for π, two for δ, three for φ and four for γ.\n\nA MO with σ symmetry results from the interaction of either two atomic s-orbitals or two atomic p-orbitals. An MO will have σ-symmetry if the orbital is symmetric with respect to the axis joining the two nuclear centers, the internuclear axis. This means that rotation of the MO about the internuclear axis does not result in a phase change. A σ* orbital, sigma antibonding orbital, also maintains the same phase when rotated about the internuclear axis. The σ* orbital has a nodal plane that is between the nuclei and perpendicular to the internuclear axis.\n\nA MO with π symmetry results from the interaction of either two atomic p orbitals or p orbitals. An MO will have π symmetry if the orbital is asymmetric with respect to rotation about the internuclear axis. This means that rotation of the MO about the internuclear axis will result in a phase change. There is one nodal plane containing the internuclear axis, if real orbitals are considered.\n\nA π* orbital, pi antibonding orbital, will also produce a phase change when rotated about the internuclear axis. The π* orbital also has a second nodal plane between the nuclei.\n\nA MO with δ symmetry results from the interaction of two atomic d or d orbitals. Because these molecular orbitals involve low-energy d atomic orbitals, they are seen in transition-metal complexes. A δ bonding orbital has two nodal planes containing the internuclear axis, and a δ* antibonding orbital also has a third nodal plane between the nuclei.\n\nTheoretical chemists have conjectured that higher-order bonds, such as phi bonds corresponding to overlap of f atomic orbitals, are possible. There is as of 2005 only one known example of a molecule purported to contain a phi bond (a U−U bond, in the molecule U).\n\nFor molecules that possess a center of inversion (centrosymmetric molecules) there are additional labels of symmetry that can be applied to molecular orbitals.\nCentrosymmetric molecules include:\n\nNon-centrosymmetric molecules include:\nIf inversion through the center of symmetry in a molecule results in the same phases for the molecular orbital, then the MO is said to have gerade (g) symmetry, from the German word for even.\nIf inversion through the center of symmetry in a molecule results in a phase change for the molecular orbital, then the MO is said to have ungerade (u) symmetry, from the German word for odd.\nFor a bonding MO with σ-symmetry, the orbital is σ (s' + s<nowiki>\"</nowiki> is symmetric), while an antibonding MO with σ-symmetry the orbital is σ, because inversion of s' – s<nowiki>\"</nowiki> is antisymmetric.\nFor a bonding MO with π-symmetry the orbital is π because inversion through the center of symmetry for would produce a sign change (the two p atomic orbitals are in phase with each other but the two lobes have opposite signs), while an antibonding MO with π-symmetry is π because inversion through the center of symmetry for would not produce a sign change (the two p orbitals are antisymmetric by phase).\n\nThe qualitative approach of MO analysis uses a molecular orbital diagram to visualize bonding interactions in a molecule. In this type of diagram, the molecular orbitals are represented by horizontal lines; the higher a line the higher the energy of the orbital, and degenerate orbitals are placed on the same level with a space between them. Then, the electrons to be placed in the molecular orbitals are slotted in one by one, keeping in mind the Pauli exclusion principle and Hund's rule of maximum multiplicity (only 2 electrons, having opposite spins, per orbital; place as many unpaired electrons on one energy level as possible before starting to pair them). For more complicated molecules, the wave mechanics approach loses utility in a qualitative understanding of bonding (although is still necessary for a quantitative approach). \nSome properties:\n\nThe general procedure for constructing a molecular orbital diagram for a reasonably simple molecule can be summarized as follows:\n\n1. Assign a point group to the molecule.\n\n2. Look up the shapes of the SALCs.\n\n3. Arrange the SALCs of each molecular fragment in increasing order of energy, first noting whether they stem from \"s\", \"p\", or \"d\" orbitals \n(and put them in the order \"s\" < \"p\" < \"d\"), and then their number of internuclear nodes.\n\n4. Combine SALCs of the same symmetry type from the two fragments, and from N SALCs form N molecular orbitals.\n\n5. Estimate the relative energies of the molecular orbitals from considerations of overlap and relative energies of the parent orbitals, and draw the levels on a molecular orbital energy level diagram (showing the origin of the orbitals).\n\n6. Confirm, correct, and revise this qualitative order by carrying out a molecular orbital calculation by using commercial software.\n\nMolecular orbitals are said to be degenerate if they have the same energy. For example, in the homonuclear diatomic molecules of the first ten elements, the molecular orbitals derived from the p and the p atomic orbitals result in two degenerate bonding orbitals (of low energy) and two degenerate antibonding orbitals (of high energy).\n\nWhen the energy difference between the atomic orbitals of two atoms is quite large, one atom's orbitals contribute almost entirely to the bonding orbitals, and the other atom's orbitals contribute almost entirely to the antibonding orbitals. Thus, the situation is effectively that one or more electrons have been transferred from one atom to the other. This is called an (mostly) ionic bond.\n\nThe bond order, or number of bonds, of a molecule can be determined by combining the number of electrons in bonding and antibonding molecular orbitals. A pair of electrons in a bonding orbital creates a bond, whereas a pair of electrons in an antibonding orbital negates a bond. For example, N, with eight electrons in bonding orbitals and two electrons in antibonding orbitals, has a bond order of three, which constitutes a triple bond.\n\nBond strength is proportional to bond order—a greater amount of bonding produces a more stable bond—and bond length is inversely proportional to it—a stronger bond is shorter.\n\nThere are rare exceptions to the requirement of molecule having a positive bond order. Although Be has a bond order of 0 according to MO analysis, there is experimental evidence of a highly unstable Be molecule having a bond length of 245 pm and bond energy of 10 kJ/mol.\n\nThe highest occupied molecular orbital and lowest unoccupied molecular orbital are often referred to as the HOMO and LUMO, respectively. The difference of the energies of the HOMO and LUMO, termed the band gap, can sometimes serve as a measure of the excitability of the molecule: The smaller the energy the more easily it will be excited.\n\nHomonuclear diatomic MOs contain equal contributions from each atomic orbital in the basis set. This is shown in the homonuclear diatomic MO diagrams for H, He, and Li, all of which containing symmetric orbitals.\n\nAs a simple MO example, consider the electrons in a hydrogen molecule, H (see molecular orbital diagram), with the two atoms labelled H' and H\". The lowest-energy atomic orbitals, 1s' and 1s\", do not transform according to the symmetries of the molecule. However, the following symmetry adapted atomic orbitals do:\n\nThe symmetric combination (called a bonding orbital) is lower in energy than the basis orbitals, and the antisymmetric combination (called an antibonding orbital) is higher. Because the H molecule has two electrons, they can both go in the bonding orbital, making the system lower in energy (hence more stable) than two free hydrogen atoms. This is called a covalent bond. The bond order is equal to the number of bonding electrons minus the number of antibonding electrons, divided by 2. In this example, there are 2 electrons in the bonding orbital and none in the antibonding orbital; the bond order is 1, and there is a single bond between the two hydrogen atoms.\n\nOn the other hand, consider the hypothetical molecule of He with the atoms labeled He' and He\". As with H, the lowest energy atomic orbitals are the 1s' and 1s\", and do not transform according to the symmetries of the molecule, while the symmetry adapted atomic orbitals do. The symmetric combination—the bonding orbital—is lower in energy than the basis orbitals, and the antisymmetric combination—the antibonding orbital—is higher. Unlike H, with two valence electrons, He has four in its neutral ground state. Two electrons fill the lower-energy bonding orbital, σ(1s), while the remaining two fill the higher-energy antibonding orbital, σ*(1s). Thus, the resulting electron density around the molecule does not support the formation of a bond between the two atoms; without a stable bond holding the atoms together, molecule would not be expected to exist. Another way of looking at it is that there are two bonding electrons and two antibonding electrons; therefore, the bond order is 0 and no bond exists (the molecule has one bound state supported by the Van der Waals potential).\n\nDilithium Li is formed from the overlap of the 1s and 2s atomic orbitals (the basis set) of two Li atoms. Each Li atom contributes three electrons for bonding interactions, and the six electrons fill the three MOs of lowest energy, σ(1s), σ*(1s), and σ(2s). Using the equation for bond order, it is found that dilithium has a bond order of one, a single bond.\n\nConsidering a hypothetical molecule of He, since the basis set of atomic orbitals is the same as in the case of H, we find that both the bonding and antibonding orbitals are filled, so there is no energy advantage to the pair. HeH would have a slight energy advantage, but not as much as H + 2 He, so the molecule is very unstable and exists only briefly before decomposing into hydrogen and helium. In general, we find that atoms such as He that have full energy shells rarely bond with other atoms. Except for short-lived Van der Waals complexes, there are very few noble gas compounds known.\n\nWhile MOs for homonuclear diatomic molecules contain equal contributions from each interacting atomic orbital, MOs for heteronuclear diatomics contain different atomic orbital contributions. Orbital interactions to produce bonding or antibonding orbitals in heteronuclear diatomics occur if there is sufficient overlap between atomic orbitals as determined by their symmetries and similarity in orbital energies.\n\nIn hydrogen fluoride HF overlap between the H 1s and F 2s orbitals is allowed by symmetry but the difference in energy between the two atomic orbitals prevents them from interacting to create a molecular orbital. Overlap between the H 1s and F 2p orbitals is also symmetry allowed, and these two atomic orbitals have a small energy separation. Thus, they interact, leading to creation of σ and σ* MOs and a molecule with a bond order of 1. Since HF is a non-centrosymmetric molecule, the symmetry labels g and u do not apply to its molecular orbitals.\n\nTo obtain quantitative values for the molecular energy levels, one needs to have molecular orbitals that are such that the configuration interaction (CI) expansion converges fast towards the full CI limit. The most common method to obtain such functions is the Hartree–Fock method, which expresses the molecular orbitals as eigenfunctions of the Fock operator. One usually solves this problem by expanding the molecular orbitals as linear combinations of Gaussian functions centered on the atomic nuclei (see linear combination of atomic orbitals and basis set (chemistry)). The equation for the coefficients of these linear combinations is a generalized eigenvalue equation known as the Roothaan equations, which are in fact a particular representation of the Hartree–Fock equation. There are a number of programs in which quantum chemical calculations of MOs can be performed, including Spartan and HyperChem.\n\nSimple accounts often suggest that experimental molecular orbital energies can be obtained by the methods of ultra-violet photoelectron spectroscopy for valence orbitals and X-ray photoelectron spectroscopy for core orbitals. This, however, is incorrect as these experiments measure the ionization energy, the difference in energy between the molecule and one of the ions resulting from the removal of one electron. Ionization energies are linked approximately to orbital energies by Koopmans' theorem. While the agreement between these two values can be close for some molecules, it can be very poor in other cases.\n"}
{"id": "19615", "url": "https://en.wikipedia.org/wiki?curid=19615", "title": "Systems Concepts", "text": "Systems Concepts\n\nSystems Concepts (now the SC Group) is a company co-founded by Stewart Nelson and Mike Levitt focused on making hardware products related to the DEC PDP-10 series of computers. One of its major products was the SA-10, an interface which allowed PDP-10s to be connected to disk and tape drives designed for use with the channel interfaces of IBM mainframes.\n\nLater, Systems Concepts attempted to produce a compatible replacement for the DEC PDP-10 computers. \"Mars\" was the code name for a family of PDP-10-compatible computers built by Systems Concepts, including the initial SC-30M, the smaller SC-25, and the slower SC-20. These machines were marvels of engineering design; although not much slower than the unique Foonly F-1, they were physically smaller and consumed less power than the much slower DEC KS10 or Foonly F-2, F-3, or F-4 machines. They were also completely compatible with the DEC KL10, and ran all KL10 binaries (including the operating system) with no modifications at about 2-3 times faster than a KL10.\n\nWhen DEC cancelled the Jupiter project in 1983, Systems Concepts hoped to sell their machine to customers with a software investment in PDP-10s. Their spring 1984 announcement generated excitement in the PDP-10 world. TOPS-10 was running on the Mars by the summer of 1984, and TOPS-20 by early fall. However, people at Systems Concepts were better at designing machines than at mass-producing or selling them; the company continually improved the design, but lost credibility as delivery dates continued to slip. They also overpriced; believing they were competing with the KL10 and VAX 8600 and not startups such as Sun Microsystems building workstations with comparable power at a fraction of the price. By the time SC shipped the first SC-30M to Stanford University in late 1985, most customers had already abandoned the PDP-10, usually for VMS or Unix systems. Nevertheless, a number were purchased by CompuServe, which depended on PDP-10s to run its online service and was eager to move to newer but fully compatible systems. CompuServe's demand for the computers outpaced Systems Concepts' ability to produce them, so CompuServe licensed the design and built SC-designed computers itself.\nOther companies that purchased the SC-30 machines included Telmar, Reynolds and Reynolds, The Danish National Railway.\n\nPeter Samson was director of marketing and program development.\n\nSC later designed the SC-40, released in 1993, a faster follow-on to the SC-30M and SC-25. It can perform up to 8 times as fast as a DEC KL-10, and it also supports more physical memory, a larger virtual address space, and more modern input/output devices. These systems were also used at CompuServe.\n\nIn 1985, the company contracted to engineer and produce a PC-based cellular automata system for Tommaso Toffoli of MIT, called the CAM-6. The CAM-6 was a 2-card \"sandwich\" that plugged into an IBM PC slot and ran cellular automata rules at a 60 Hz update rate. Toffoli provided Forth-based software to operate the card. The production problems that plagued the company's computer products were demonstrated here as well, and only a few boards were produced.\n\nSystems Concepts remains in business, having changed its name to the SC Group when it moved from California to Nevada.\n"}
{"id": "19616", "url": "https://en.wikipedia.org/wiki?curid=19616", "title": "Messiah", "text": "Messiah\n\nIn Abrahamic religions, a messiah or messias (; , ) is a saviour or liberator of a group of people.\n\nThe concepts of \"moshiach\", messianism, and of a Messianic Age originated in Judaism, and in the Hebrew Bible; a \"moshiach\" (messiah) is a king or High Priest traditionally anointed with holy anointing oil. Messiahs were not exclusively Jewish: the Book of Isaiah refers to Cyrus the Great, king of the Achaemenid Empire, as a messiah for his decree to rebuild the Jerusalem Temple.\n\n\"Ha mashiach\" (המשיח, \"the Messiah\", \"the anointed one\"), often referred to as \"\" (מלך המשיח \"King Messiah\"), is to be a human leader, physically descended from the paternal Davidic line through King David and King Solomon. He is thought to accomplish predetermined things in only one future arrival, including the unification of the tribes of Israel, the gathering of all Jews to \"Eretz Israel\", the rebuilding of the Temple in Jerusalem, the ushering in of a Messianic Age of global universal peace, and the annunciation of the world to come The specific expression, \"HaMashiach\" (המשיח, lit. \"the Messiah\"), does not occur in the Tanakh.\n\nIn Christianity, the Messiah is called the Christ, from , translating the Hebrew word of the same meaning. The concept of the Messiah in Christianity originated from the Messiah in Judaism. However, unlike the concept of the Messiah in Judaism and Islam, the Messiah in Christianity is the Son of God. Christ became the accepted Christian designation and title of Jesus of Nazareth, because Christians believe that the messianic prophecies in the Old Testament were fulfilled in his mission, death, and resurrection. They believe that Christ will fulfill the rest of the messianic prophecies, specifically the prophecy of a future king who would come from the Davidic line and usher in a Messianic Age and the world to come at the Second Coming.\n\nIn Islam, Jesus was a prophet and the \"Masîḥ\" (مسيح), the Messiah sent to the Israelites, and he will return to Earth at the end of times, along with the \"Mahdi\", and defeat \"al-Masih ad-Dajjal\", the false Messiah.\n\nIn Ahmadiyya theology, these prophecies concerning the Mahdi and the second coming of Jesus have been fulfilled in Mirza Ghulam Ahmad (1835–1908), the founder of the Ahmadiyya Movement, and the terms \"Messiah\" and \"Mahdi\" are synonyms for one and the same person.\n\nIn Chabad messianism, Yosef Yitzchak Schneersohn (r. 1920 - 1950), sixth \"Rebbe\" (spiritual leader) of Chabad Lubavitch, and Menachem Mendel Schneerson (1902 - 1994), seventh \"Rebbe\" of Chabad, are Messiah claimants. Resembling early Christianity, the deceased Menachem Mendel Schneerson is believed to be the Messiah among some adherents of the Chabad movement; his second coming is believed to be imminent.\n\nMessiah (; in modern Jewish texts in English spelled \"Mashiach\"; , , , , , , ) literally means \"anointed one\". In Hebrew, the Messiah is often referred to as מלך המשיח (\"\" in the Tiberian vocalization, , literally meaning \"the Anointed King\".)\n\nThe Greek Septuagint version of the Old Testament renders all thirty-nine instances of the Hebrew word for \"anointed\" (\"Mašíaḥ\") as Χριστός (\"Khristós\"). The New Testament records the Greek transliteration Μεσσίας, \"Messias\" twice in John.\n\n\"al-Masīḥ\" (proper name, ) is the Arabic word for messiah. In modern Arabic, it is used as one of the many titles of Jesus. \"Masīḥ\" is used by Arab Christians as well as Muslims, and is written as \"Yasūʿ al-Masih\" (يسوع المسيح) by Arab Christians or \"ʿĪsā al-Masīḥ\" (عيسى المسيح) by Muslims. The word \"al-Masīḥ\" literally means \"the anointed\", \"the traveller\", or the \"one who cures by caressing\".\n\nThe literal translation of the Hebrew word \"mashiach\" (messiah) is \"anointed\", which refers to a ritual of consecrating someone or something by putting holy oil upon it. It is used throughout the Hebrew Bible in reference to a wide variety of individuals and objects; for example, a Jewish king, Jewish priests and prophets, the Jewish Temple and its utensils, unleavened bread, and a non-Jewish king (Cyrus the Great).\n\nIn Jewish eschatology, the term came to refer to a future Jewish king from the Davidic line, who will be \"anointed\" with holy anointing oil, to be king of God's kingdom, and rule the Jewish people during the Messianic Age. In Judaism, the Messiah is not considered to be God or a pre-existent divine Son of God. He is considered to be a great political leader that has descended from King David. That is why he is referred to as Messiah ben David, which means \"Messiah, son of David\". The messiah, in Judaism, is considered to be a great, charismatic leader that is well oriented with the laws that are followed in Judaism. He will be the one who will not \"judge by what his eyes see\" or \"decide by what his ears hear\".\n\nBelief in the eventual coming of a future messiah is a fundamental part of Judaism, and is one of Maimonides' 13 Principles of Faith.\n\nMaimonides describes the identity of the Messiah in the following terms:\nEven though the eventual coming of the messiah is a strongly upheld belief in Judaism, trying to predict the actual time when the messiah will come is an act that is frowned upon. These kinds of actions are thought to weaken the faith the people have in the religion. So in Judaism, there is no specific time when the messiah comes. Rather, it is the acts of the people that determines when the messiah comes. It is said that the messiah would come either when the world needs his coming the most (when the world is so sinful and in desperate need of saving by the messiah) or deserves it the most (when genuine goodness prevails in the world).\n\nA common modern rabbinic interpretation is that there is a \"potential\" messiah in every generation. The Talmud, which often uses stories to make a moral point (\"aggadah\"), tells of a highly respected rabbi who found the Messiah at the gates of Rome and asked him, \"When will you finally come?\" He was quite surprised when he was told, \"Today.\" Overjoyed and full of anticipation, the man waited all day. The next day he returned, disappointed and puzzled, and asked, \"You said messiah would come 'today' but he didn't come! What happened?\" The Messiah replied, \"Scripture says, 'Today, if you will but hearken to his voice.'\"\n\nA Kabbalistic tradition within Judaism is that the commonly discussed messiah who will usher in a period of freedom and peace, Messiah ben David, will be preceded by Messiah ben Joseph, who will gather the children of Israel around him, lead them to Jerusalem. After overcoming the hostile powers in Jerusalem, Messiah ben Joseph, will reestablish the Temple-worship and set up his own dominion. Then Armilus, according to one group of sources, or Gog and Magog, according to the other, will appear with their hosts before Jerusalem, wage war against Messiah ben Joseph, and slay him. His corpse, according to one group, will lie unburied in the streets of Jerusalem; according to the other, it will be hidden by the angels with the bodies of the Patriarchs, until Messiah ben David comes and brings him back to life.\n\nYosef Yitzchak Schneersohn (r. 1920 - 1950), sixth \"Rebbe\" (spiritual leader) of Chabad Lubavitch, and Menachem Mendel Schneerson (1902 - 1994), seventh \"Rebbe\" of Chabad, are messiah claimants.\n\nAs per Chabad-Lubavitch messianism, Menachem Mendel Schneerson openly declared his deceased father-in-law, the former 6th \"Rebbe\" of Chabad Lubavitch, to be the Messiah. He published about Yosef Yitzchak Schneersohn to be \"\"Atzmus u'mehus alein vi er hat zich areingeshtalt in a guf\"\" (Yiddish and English for: \"Essence and Existence [of God] which has placed itself in a body\"). The gravesite of his deceased father-in-law Yosef Yitzchak Schneersohn, known as \"the \"Ohel\"\", became a central point of focus for Menachem Mendel Schneerson's prayers and supplications.\n\nRegarding the deceased Menachem Mendel Schneerson, a later Chabad Halachic ruling claims that it was \"incumbent on every single Jew to heed the Rebbe's words and believe that he is indeed King Moshiach, who will be revealed imminently\". Outside of Chabad messianism, in Judaism, there is no basis to these claims. If anything, this resembles the faith in the resurrection of Jesus and his second coming in early Christianity.\n\nStill today, the deceased rabbi Menachem Mendel Schneerson is believed to be the Messiah among adherents of the Chabad movement, and his second coming is believed to be imminent. He is venerated and invocated to by thousands of visitors and letters each year at two bamot-tombs (\"Ohel\"), especially in a pilgrimage each year on the anniversary of his death.\n\nThe Greek translation of Messiah is \"khristos\" (), anglicized as \"Christ\", and Christians commonly refer to Jesus as either the \"Christ\" or the \"Messiah\". Christians believe that messianic prophecies were fulfilled in the mission, death, and resurrection of Jesus and that he will return to fulfill the rest of messianic prophecies.\n\nThe majority of historical and mainline Christian theologies consider Jesus to be the Son of God and God the Son, a concept of the Messiah fundamentally different from the Jewish and Islamic concepts. In each of the four New Testament Gospels, the only literal anointing of Jesus is conducted by a woman. In the Gospels of Mark, Matthew, and John, this anointing occurs in Bethany, outside Jerusalem. In the Gospel of Luke, the anointing scene takes place at an indeterminate location, but context suggests it to be in Galilee, or even a separate anointing altogether.\n\nWhile the term \"messiah\" does appear in Islam, the meaning is different from that found in Christianity and Judaism. \"Though Islam shares many of the beliefs and characteristics of the two Semitic/Abrahamic/monotheistic religions which preceded it, the idea of messianism, which is of central importance in Judaism and Christianity, is alien to Islam as represented by the Qur'an.\"\n\nThe Quran identifies Jesus (Isa) as the messiah (\"Masih\"), who will one day return to earth. At the time of the second coming, \"according to Islamic tradition, Jesus will come again and exercise his power of healing. He will forever destroy falsehood, as embodied in the Daj-jal, the great falsifier, the anti-Christ. Then God will reign forever.\"\n\nJesus is one of the most important prophets in the Islamic tradition, along with Noah, Abraham, Moses, and Muhammad. Unlike Christians, Muslims see Jesus as merely a prophet, but not as God himself or the son of God. Like all other prophets, Jesus is an ordinary man, who receives revelations from God. According to religious scholar Mona Siddiqui, in Islam, \"Prophecy allows God to remain veiled and there is no suggestion in the Qur'an that God wishes to reveal of himself just yet. Prophets guarantee interpretation of revelation and that God's message will be understood.\" Prophecy in human form does not represent the true powers of God, contrary to the way Jesus is depicted in mainstream Christianity.\n\nThe Quran states that Isa, the Son of Maryam (Arabic: \"Isa ibn Maryam\"), is the messiah and prophet sent to the Children of Israel. The birth of Isa is described in Quran sura 19 verses 1–33, and sura 4 verse 171 explicitly states Isa as the Son of Maryam. Sunni Muslims believe Isa is alive in Heaven and did not die in the crucifixion, as depicted in mainstream Christianity. According to religious scholar Mahmoud Ayoub, \"Jesus' close proximity or nearness (qurb) to God is affirmed in the Qur'anic insistence that Jesus did not die, but was taken up to God and remains with God\" The Quran in sura 4 verse 157-158 states that: \"They did not kill him, nor did they crucify him, but they thought they did\" since the messiah was \"made to resemble him to them.\"\n\nIt is believed that Isa will return to Earth to defeat the Masih ad-Dajjal (false Messiah), a figure similar to the Antichrist in Christianity, who will emerge shortly before \"Yawm al-Qiyāmah\" (\"the Day of Resurrection\"). The \"Mahdi\" will come shortly before the second coming of Jesus. After he has destroyed ad-Dajjal, his final task will be to become leader of the Muslims. Isa will unify the Muslim \"Ummah\" (the followers of Islam) under the common purpose of worshipping Allah alone in pure Islam, thereby ending divisions and deviations by adherents. Mainstream Muslims believe that at that time Isa will dispel Christian and Jewish claims about him.\n\nA \"hadith\" in Abu Dawud () says:\n\nThe Prophet said: There is no prophet between me and him, that is, Isa. He will descend (to the earth). When you see him, recognise him: a man of medium height, reddish fair, wearing two light yellow garments, looking as if drops were falling down from his head though it will not be wet. He will fight the people for the cause of Islam. He will break the cross, kill swine, and abolish jizyah. Allah will perish all religions except Islam. He will destroy the Antichrist and will live on the earth for forty years and then he will die. The Muslims will pray over him.\nBoth Sunni and Shia Muslims agree that al-Mahdi will arrive first, and after him, Isa. Isa will proclaim al-Mahdi as the Islamic community leader. A war will be fought—the Dajjal against al-Mahdi and Isa. This war will mark the approach of the coming of the Last Day. After Isa slays al-Dajjāl at the Gate of Lud, he will bear witness and reveal that Islam is indeed the true and last word from God to humanity as Yusuf Ali's translation reads: \"And there is none of the People of the Book but must believe in him before his death; and on the Day of Judgment he will be a witness against them.\" A \"hadith\" in Sahih Bukhari says:\n\"Allah's Apostle said \"How will you be when the son of Mariam descends among you and your Imam is from among you?\" \"\n\nThe Quran denies the crucifixion of Jesus, claiming that he was neither killed nor crucified. The Quran also emphasizes the difference between Allah (God in Arabic) and the Messiah:\n\"Those who say that Allah is the Messiah, son of Mary, are unbelievers. The Messiah said: \"O Children of Israel, worship Allah, my Lord and your Lord... unbelievers too are those who have said that Allah is the third of three... the Messiah, son of Mary, was only a Messenger before whom other Messengers had gone.\"\n\nShi'i Islam, which significantly values and revolves around the 12 spiritual leaders called Imams, differs significantly from the beliefs of Sunni Islam. Unlike Sunni Islam, \"Messianism is an essential part of religious belief and practice for almost all Shi'a Muslims.\" Shi'i Islam believes that the last Imam will return again, with the return of Jesus. According to religious scholar Mona Sidique, \"Shi'is are acutely aware of the existence everywhere of the twelfth Imam, who disappeared in 874. Shi'i piety teaches that the hidden Imam will return with Jesus Christ to set up the messianic kingdom before the final Judgement Day, when all humanity will stand before God. There is some controversy as to the identity of this imam. There are sources that underscore how the Shia sect agrees with the Jews and Christians that Imam Mehdi (al-Mahdi) is another name for Elijah, whose return prior to the arrival of the Messiah was prophesied in the Old Testament. On the other hand, there is also belief from among Shia and Sunni adherents that the imam will be Muhammad.\n\nThe Imams and Fatima will have a direct impact on the judgements rendered that day. This will represent the ultimate intercession.\" There is debate on whether Shi'i Muslims should accept the death of Jesus. Religious scholar Mahmou Ayoub argues \"Modern Shi'i thinkers have allowed the possibility that Jesus died and only his spirit was taken up to heaven.\" Conversely, religious scholar Mona Siddiqui argues that Shi'i thinkers believe Jesus was \"neither crucified nor slain.\" She also argues that Shi'i Muslims believe that the twelfth imam did not die, but \"was taken to God to return in God's time,\" and \"will return at the end of history to establish the kingdom of God on earth as the expected Mahdi.\"\n\nAccording to Ahmadiyya thought, Messiahship is a phenomenon through which a special emphasis is given on the transformation of a people by way of offering suffering for the sake of God instead of giving suffering (i.e. refraining from revenge). Ahmadis believe that this special emphasis was given through the person of Jesus and Mirza Ghulam Ahmad (1835–1908) among others.\n\nAhmadis hold that the prophesied eschatological figures of Christianity and Islam, the Messiah and Mahdi, were in fact to be fulfilled in one person who was to represent all previous prophets.\n\nNumerous hadith are presented by the Ahmadis in support of their view, such as one from Sunan Ibn Majah, which says, \"There is No Mahdi but Jesus son of Mary.\"\n\nAhmadis believe that the prophecies concerning the Mahdi and the second coming of Jesus have been fulfilled in Mirza Ghulam Ahmad (1835–1908), the founder of the Ahmadiyya Movement. Unlike mainstream Muslims, the Ahmadis do not believe that Jesus is alive in heaven, but that he survived the crucifixion and migrated towards the east where he died a natural death and that Ghulam Ahmad was only the promised spiritual second coming and likeness of Jesus, the promised Messiah and Mahdi. He also claimed to have appeared in the likeness of Krishna and that his advent fulfilled certain prophecies found in Hindu scriptures. He stated that the founder of Sikhism was a Muslim saint, who was a reflection of the religious challenges he perceived to be occurring. Ghulam Ahmad wrote \"Barahin-e-Ahmadiyya\", in 1880, which incorporated Indian, Sufi, Islamic and Western aspects in order to give life to Islam in the face of the British Raj, Protestant Christianity, and rising Hinduism. He later declared himself the Promised Messiah and the Mahdi following Divine revelations in 1891. Ghulam Ahmad argued that Jesus had appeared 1300 years after the formation of the Muslim community and stressed the need for a current Messiah, in turn claiming that he himself embodied both the Mahdi and the Messiah. Ghulam Ahmad was supported by Muslims who especially felt oppressed by Christian and Hindu missionaries.\n\n\nThe following works include the concept of a messiah as a leader of a cause or liberator of a people:\n\n\n\n"}
{"id": "19617", "url": "https://en.wikipedia.org/wiki?curid=19617", "title": "Margaret Mead", "text": "Margaret Mead\n\nMargaret Mead (December 16, 1901 – November 15, 1978) was an American cultural anthropologist who featured frequently as an author and speaker in the mass media during the 1960s and 1970s. She earned her bachelor's degree at Barnard College in New York City and her M.A. and Ph.D. degrees from Columbia University. Mead served as President of the American Association for the Advancement of Science in 1975.\n\nMead was a communicator of anthropology in modern American and Western culture and was often controversial as an academic. Her reports detailing the attitudes towards sex in South Pacific and Southeast Asian traditional cultures influenced the 1960s sexual revolution. She was a proponent of broadening sexual conventions within a context of traditional Western religious life.\n\nMargaret Mead, the first of five children, was born in Philadelphia, but raised in nearby Doylestown, Pennsylvania. Her father, Edward Sherwood Mead, was a professor of finance at the Wharton School of the University of Pennsylvania, and her mother, Emily (née Fogg) Mead, was a sociologist who studied Italian immigrants. Her sister Katharine (1906–1907) died at the age of nine months. This was a traumatic event for Mead, who had named the girl, and thoughts of her lost sister permeated her daydreams for many years. Her family moved frequently, so her early education was directed by her grandmother until, at age 11, she was enrolled by her family at Buckingham Friends School in Lahaska, Pennsylvania. Her family owned the Longland farm from 1912 to 1926. Born into a family of various religious outlooks, she searched for a form of religion that gave an expression of the faith that she had been formally acquainted with, Christianity. In doing so, she found the rituals of the Episcopal Church to fit the expression of religion she was seeking. Mead studied one year, 1919, at DePauw University, then transferred to Barnard College where she found anthropology mired in \"the stupid underbrush of nineteenth century arguments.\"\n\nMead earned her bachelor's degree from Barnard in 1923, then began studying with professor Franz Boas and Dr. Ruth Benedict at Columbia University, earning her master's degree in 1924. Mead set out in 1925 to do fieldwork in Samoa. In 1926, she joined the American Museum of Natural History, New York City, as assistant curator. She received her Ph.D. from Columbia University in 1929.\n\nBefore departing for Samoa, Mead had a short affair with the linguist Edward Sapir, a close friend of her instructor Ruth Benedict. But Sapir's conservative ideas about marriage and the woman's role were anathema to Mead, and as Mead left to do field work in Samoa the two separated permanently. Mead received news of Sapir's remarriage while living in Samoa, where, on a beach, she later burned their correspondence.\n\nMead was married three times. After a six-year engagement, she married her first husband (1923–28) American Luther Cressman, a theology student at the time who eventually became an anthropologist. Between 1925 and 1926 she was in Samoa returning wherefrom on the boat she met Reo Fortune, a New Zealander headed to Cambridge, England, to study psychology. They were married in 1928, after Mead's divorce from Cressman, Mead dismissively characterizing her union with her first husband as \"my student marriage\" in her 1972 autobiography \"Blackberry Winter\", a sobriquet with which Cressman took vigorous issue. Mead's third and longest-lasting marriage (1936–50) was to the British anthropologist Gregory Bateson, with whom she had a daughter, Mary Catherine Bateson, who would also become an anthropologist.\n\nMead's pediatrician was Benjamin Spock, whose subsequent writings on child rearing incorporated some of Mead's own practices and beliefs acquired from her ethnological field observations which she shared with him; in particular, breastfeeding on the baby's demand rather than a schedule. She readily acknowledged that Gregory Bateson was the husband she loved the most. She was devastated when he left her, and she remained his loving friend ever after, keeping his photograph by her bedside wherever she traveled, including beside her hospital deathbed.\n\nMead also had an exceptionally close relationship with Ruth Benedict, one of her instructors. In her memoir about her parents, \"With a Daughter's Eye\", Mary Catherine Bateson implies that the relationship between Benedict and Mead was partly sexual. Mead never openly identified herself as lesbian or bisexual. In her writings she proposed that it is to be expected that an individual's sexual orientation may evolve throughout life.\n\nShe spent her last years in a close personal and professional collaboration with anthropologist Rhoda Metraux, with whom she lived from 1955 until her death in 1978. Letters between the two published in 2006 with the permission of Mead's daughter clearly express a romantic relationship.\n\nMead had two sisters and a brother, Elizabeth, Priscilla, and Richard. Elizabeth Mead (1909–1983), an artist and teacher, married cartoonist William Steig, and Priscilla Mead (1911–1959) married author Leo Rosten. Mead's brother, Richard, was a professor. Mead was also the aunt of Jeremy Steig.\n\nDuring World War II, Mead served as executive secretary of the National Research Council's Committee on Food Habits. She served as curator of ethnology at the American Museum of Natural History from 1946 to 1969. She was elected a Fellow of the American Academy of Arts and Sciences in 1948. She taught at The New School and Columbia University, where she was an adjunct professor from 1954 to 1978 and was a professor of anthropology and chair of the Division of Social Sciences at Fordham University's Lincoln Center campus from 1968 to 1970, founding their anthropology department. In 1970, she joined the faculty of the University of Rhode Island as a Distinguished Professor of Sociology and Anthropology.\n\nFollowing Ruth Benedict's example, Mead focused her research on problems of child rearing, personality, and culture. She served as president of the American Anthropological Association in 1960. In the mid-1960s, Mead joined forces with communications theorist Rudolf Modley, jointly establishing an organization called Glyphs Inc., whose goal was to create a universal graphic symbol language to be understood by any members of culture, no matter how primitive. In the 1960s, Mead served as the Vice President of the New York Academy of Sciences. She held various positions in the American Association for the Advancement of Science, notably president in 1975 and chair of the executive committee of the board of directors in 1976. She was a recognizable figure in academia, usually wearing a distinctive cape and carrying a walking-stick.\n\nMead was featured on two record albums published by Folkways Records. The first, released in 1959, \"An Interview With Margaret Mead,\" explored the topics of morals and anthropology. In 1971, she was included in a compilation of talks by prominent women, \"But the Women Rose, Vol.2: Voices of Women in American History\".\n\nShe is credited with the pluralization of the term \"semiotics.\"\n\nIn later life, Mead was a mentor to many young anthropologists and sociologists, including Jean Houston.\n\nIn 1976, Mead was a key participant at UN Habitat I, the first UN forum on human settlements.\n\nMead died of pancreatic cancer on November 15, 1978 and is buried at Trinity Episcopal Church Cemetery, Buckingham, Pennsylvania.\n\nIn the foreword to \"Coming of Age in Samoa\", Mead's advisor, Franz Boas, wrote of its significance:\n\nCourtesy, modesty, good manners, conformity to definite ethical standards are universal, but what constitutes courtesy, modesty, very good manners, and definite ethical standards is not universal. It is instructive to know that standards differ in the most unexpected ways.\n\nMead's findings suggested that the community ignores both boys and girls until they are about 15 or 16. Before then, children have no social standing within the community. Mead also found that marriage is regarded as a social and economic arrangement where wealth, rank, and job skills of the husband and wife are taken into consideration.\nIn 1983, five years after Mead had died, New Zealand anthropologist Derek Freeman published \"Margaret Mead and Samoa: The Making and Unmaking of an Anthropological Myth\", in which he challenged Mead's major findings about sexuality in Samoan society. Freeman's book was controversial in its turn: later in 1983 the American Anthropological Association declared it to be \"poorly written, unscientific, irresponsible and misleading.\"\n\nIn 1999, Freeman published another book, \"The Fateful Hoaxing of Margaret Mead: A Historical Analysis of Her Samoan Research\", including previously unavailable material. In his obituary in \"The New York Times\", John Shaw stated that his thesis, though upsetting many, had by the time of his death generally gained widespread acceptance. Recent work has nonetheless challenged his critique. A frequent criticism of Freeman is that he regularly misrepresented Mead's research and views. In a 2009 evaluation of the debate, anthropologist Paul Shankman concluded that:\n\nThere is now a large body of criticism of Freeman's work from a number of perspectives in which Mead, Samoa, and anthropology appear in a very different light than they do in Freeman's work. Indeed, the immense significance that Freeman gave his critique looks like 'much ado about nothing' to many of his critics.\n\nWhile nurture-oriented anthropologists are more inclined to agree with Mead's conclusions, there are other non-anthropologists who take a nature-oriented approach following Freeman's lead, among them Harvard psychologist Steven Pinker, biologist Richard Dawkins, evolutionary psychologist David Buss, science writer Matt Ridley and classicist Mary Lefkowitz. The philosopher Peter Singer has also criticized Mead in his book \"A Darwinian Left\", where he states that \"Freeman compiles a convincing case that Mead had misunderstood Samoan customs\".\n\nIn 1996, author Martin Orans examined Mead's notes preserved at the Library of Congress, and credits her for leaving all of her recorded data available to the general public. Orans point out that Freeman's basic criticisms, that Mead was duped by ceremonial virgin Fa'apua'a Fa'amu (who later swore to Freeman that she had played a joke on Mead) were equivocal for several reasons: first, Mead was well aware of the forms and frequency of Samoan joking; second, she provided a careful account of the sexual restrictions on ceremonial virgins that corresponds to Fa'apua'a Fa'auma'a's account to Freeman, and third, that Mead's notes make clear that she had reached her conclusions about Samoan sexuality before meeting Fa'apua'a Fa'amu. Orans points out that Mead's data support several different conclusions, and that Mead's conclusions hinge on an interpretive, rather than positivist, approach to culture. Orans goes on to point out, concerning Mead's work elsewhere, that her own notes do not support her published conclusive claims. However, there are still those who claim Mead was hoaxed, including Peter Singer and zoologist David Attenborough. Evaluating Mead's work in Samoa from a positivist stance, Martin Orans' assessment of the controversy was that Mead did not formulate her research agenda in scientific terms, and that \"her work may properly be damned with the harshest scientific criticism of all, that it is 'not even wrong'.\"\n\nThe Intercollegiate Review , published by the Intercollegiate Studies Institute which promotes conservative thought on college campuses, listed the book as #1 on its \"The Fifty Worst Books of the Century\" list.\n\nAnother influential book by Mead was \"Sex and Temperament in Three Primitive Societies\". This became a major cornerstone of the feminist movement, since it claimed that females are dominant in the Tchambuli (now spelled Chambri) Lake region of the Sepik basin of Papua New Guinea (in the western Pacific) without causing any special problems. The lack of male dominance may have been the result of the Australian administration's outlawing of warfare. According to contemporary research, males are dominant throughout Melanesia (although some believe that female witches have special powers). Others have argued that there is still much cultural variation throughout Melanesia, and especially in the large island of New Guinea. Moreover, anthropologists often overlook the significance of networks of political influence among females. The formal male-dominated institutions typical of some areas of high population density were not, for example, present in the same way in Oksapmin, West Sepik Province, a more sparsely populated area. Cultural patterns there were different from, say, Mt. Hagen. They were closer to those described by Mead.\n\nMead stated that the Arapesh people, also in the Sepik, were pacifists, although she noted that they do on occasion engage in warfare. Her observations about the sharing of garden plots among the Arapesh, the egalitarian emphasis in child rearing, and her documentation of predominantly peaceful relations among relatives are very different from the \"big man\" displays of dominance that were documented in more stratified New Guinea cultures—e.g. by Andrew Strathern. They are a different cultural pattern.\n\nIn brief, her comparative study revealed a full range of contrasting gender roles:\n\n\nDeborah Gewertz (1981) studied the Chambri (called Tchambuli by Mead) in 1974–1975 and found no evidence of such gender roles. Gewertz states that as far back in history as there is evidence (1850s) Chambri men dominated over the women, controlled their produce and made all important political decisions. In later years there has been a diligent search for societies in which women dominate men, or for signs of such past societies, but none have been found (Bamberger, 1974).\n\nDespite its feminist roots, Mead's work on women and men was also criticized by Betty Friedan \non the basis that it contributes to infantilizing women.\n\nIn 1926, there was much debate about race and intelligence. Mead felt the methodologies involved in the experimental psychology research supporting arguments of racial superiority in intelligence were substantially flawed. In \"The Methodology of Racial Testing: Its Significance for Sociology\" Mead proposes that there are three problems with testing for racial differences in intelligence. First, there are concerns with the ability to validly equate one's test score with what Mead refers to as \"racial admixture\" or how much \"Negro or Indian blood\" an individual possesses. She also considers whether this information is relevant when interpreting IQ scores. Mead remarks that a genealogical method could be considered valid if it could be \"subjected to extensive verification\". In addition, the experiment would need a steady control group to establish whether racial admixture was actually affecting intelligence scores. Next, Mead argues that it is difficult to measure the effect that social status has on the results of a person's intelligence test. By this she meant that environment (i.e., family structure, socioeconomic status, exposure to language) has too much influence on an individual to attribute inferior scores solely to a physical characteristic such as race. Lastly, Mead adds that language barriers sometimes create the biggest problem of all. Similarly, Stephen J. Gould finds three main problems with intelligence testing, in his book \"The Mismeasure of Man\" that relate to Mead's view of the problem of determining whether there are indeed racial differences in intelligence.\n\nIn 1929 Mead and Fortune visited Manus, now the northern-most province of Papua New Guinea, travelling there by boat from Rabaul. She amply describes her stay there in her autobiography and it is mentioned in her 1984 biography by Jane Howard. On Manus she studied the Manus people of the south coast village of Peri. \"Over the next five decades Mead would come back oftener to Peri than to any other field site of her career.\n\nMead has been credited with persuading the American Jewish Committee to sponsor a project to study European Jewish villages, \"shtetls\", in which a team of researchers would conduct mass interviews with Jewish immigrants living in New York City. The resulting book, widely cited for decades, allegedly created the Jewish mother stereotype, a mother intensely loving but controlling to the point of smothering, and engendering guilt in her children through the suffering she professed to undertake for their sakes.\n\nMead worked for the RAND Corporation, a U.S. Air Force military funded private research organization, from 1948 to 1950 to study Russian culture and attitudes toward authority.\n\nAs an Anglican Christian, Mead played a considerable part in the drafting of the 1979 American Episcopal Book of Common Prayer.\n\nAfter her death Mead's Samoan research was criticized by anthropologist Derek Freeman, who published a book that argued against many of Mead's conclusions. Freeman argued that Mead had misunderstood Samoan culture when she argued that Samoan culture did not place many restrictions on youths' sexual explorations. Freeman argued instead that Samoan culture prized female chastity and virginity and that Mead had been misled by her female Samoan informants. Freeman's critique was met with a considerable backlash and harsh criticism from the anthropology community, whereas it was received enthusiastically by communities of scientists who believed that sexual mores were more or less universal across cultures. Some anthropologists who studied Samoan culture argued in favor of Freeman's findings and contradicted those of Mead, whereas others argued that Freeman's work did not invalidate Mead's work because Samoan culture had been changed by the integration of Christianity in the decades between Mead's and Freeman's fieldwork periods. While Mead was careful to shield the identity of all her subjects for confidentiality Freeman was able to find and interview one of her original participants, and Freeman reported that she admitted to having wilfully misled Mead. She said that she and her friends were having fun with Mead and telling her stories.\n\nOn the whole, anthropologists have rejected the notion that Mead's conclusions rested on the validity of a single interview with a single person, finding instead that Mead based her conclusions on the sum of her observations and interviews during her time in Samoa, and that the status of the single interview did not falsify her work. Some anthropologists have however maintained that even though Freeman's critique was invalid, Mead's study was not sufficiently scientifically rigorous to support the conclusions she drew.\n\nMead's reputation and significance as an anthropologist have not been diminished by Freeman's criticisms. In her book \"Galileo's Middle Finger\", Alice Dreger argues that Freeman's accusations were unfounded and misleading. A detailed review of the controversy by Paul Shankman, published by the University of Wisconsin Press in 2009, supports the contention that Mead's research was essentially correct, and concludes that Freeman cherry-picked his data and misrepresented both Mead and Samoan culture.\n\nIn 1976, Mead was inducted into the National Women's Hall of Fame.\n\nOn January 19, 1979, President Jimmy Carter announced that he was awarding the Presidential Medal of Freedom posthumously to Mead. UN Ambassador Andrew Young presented the award to Mead's daughter at a special program honoring Mead's contributions, sponsored by the American Museum of Natural History, where she spent many years of her career. The citation read:\n\nIn 1979, the Supersisters trading card set was produced and distributed; one of the cards featured Mead's name and picture.\n\nThe 2014 novel \"Euphoria\" by Lily King is a fictionalized account of Mead's love/marital relationships with fellow anthropologists Reo Fortune and Gregory Bateson in pre-WWII New Guinea.\n\nIn addition, there are several schools named after Mead in the United States: a junior high school in Elk Grove Village, Illinois, an elementary school in Sammamish, Washington and another in Sheepshead Bay, Brooklyn, New York.\n\nThe USPS issued a stamp of face value 32¢ on 28 May 1998, as part of the Celebrate the Century stamp sheet series.\n\n\n\n\n\n\n"}
{"id": "19620", "url": "https://en.wikipedia.org/wiki?curid=19620", "title": "Michael Palin", "text": "Michael Palin\n\nSir Michael Edward Palin, (; born 5 May 1943) is an English comedian, actor, writer and television presenter. He was a member of the comedy group Monty Python. Since 1980 he has made a number of travel documentaries.\n\nPalin wrote most of his comedic material with fellow Python member Terry Jones. Before Monty Python, they had worked on other shows such as the \"Ken Dodd Show\", \"The Frost Report\", and \"Do Not Adjust Your Set\". Palin appeared in some of the most famous Python sketches, including \"Argument Clinic\", \"Dead Parrot sketch\", \"The Lumberjack Song\", \"The Spanish Inquisition\", \"Bicycle Repair Man\" and \"The Fish-Slapping Dance\".\n\nPalin continued to work with Jones after Python, co-writing \"Ripping Yarns\". He has also appeared in several films directed by fellow Python Terry Gilliam and made notable appearances in other films such as \"A Fish Called Wanda\" (1988), for which he won the BAFTA Award for Best Actor in a Supporting Role. In a 2005 poll to find \"The Comedians' Comedian\", he was voted the 30th favourite by fellow comedians and comedy insiders.\n\nAfter Python, he began a new career as a travel writer and travel documentarian. His journeys have taken him across the world, including the North and South Poles, the Sahara Desert, the Himalayas, Eastern Europe, Brazil, and North Korea. Having been awarded a CBE for services to television in the 2000 New Year Honours, Palin received a knighthood in the 2019 New Year Honours for services to travel, culture and geography. From 2009-12 Palin was the president of the Royal Geographical Society.\n\nOn 12 May 2013, Palin was made a BAFTA fellow, the highest honour that is conferred by the organisation.\n\nPalin was born in Ranmoor, Sheffield, the second child and only son of Edward Moreton Palin (1900-1977). and Mary Rachel Lockhart (née Ovey; 1903-1990). His father was a Shrewsbury School and Cambridge University-educated engineer working for a steel firm. His maternal grandfather, Lieutenant-Colonel Richard Lockhart Ovey, DSO, was High Sheriff of Oxfordshire in 1927. He was educated at Birkdale and Shrewsbury School. His sister Angela was nine years older than he was. Despite the age gap the two had a close relationship until her suicide in 1987. He has ancestral roots in Letterkenny, County Donegal.\n\nWhen he was five years old, Palin had his first acting experience at Birkdale playing Martha Cratchit in a school performance of \"A Christmas Carol\". At the age of 10, Palin, still interested in acting, made a comedy monologue and read a Shakespeare play to his mother while playing all the parts. After his school days in 1962 he went on to read modern history at Brasenose College, Oxford. With fellow student Robert Hewison he performed and wrote, for the first time, comedy material at a university Christmas party. Terry Jones, also a student in Oxford, saw that performance and began writing together with Hewison and Palin. In the same year Palin joined the Brightside and Carbrook Co-operative Society Players and first gained fame when he won an acting award at a Co-op drama festival. He also performed and wrote in the Oxford Revue (called the Et ceteras) with Jones.\n\nIn 1966 he married Helen Gibbins, whom he first met in 1959 on holiday in Southwold in Suffolk. This meeting was later fictionalised in Palin's play \"East of Ipswich\". The couple have three children: Thomas (b. 1969), William (b. 1971) and Rachel (b. 1975) and two grandchildren. Rachel is a BBC TV director, whose work includes \"\", shown on BBC Two throughout October and November 2010. A photograph of William as a baby briefly appeared in \"Monty Python and the Holy Grail\" as \"Sir Not-appearing-in-this-film\". His nephew is the theatre designer Jeremy Herbert.\n\nAfter finishing university in 1965 Palin became a presenter on a comedy pop show called \"Now!\" for the television contractor Television Wales and the West. At the same time Palin was contacted by Jones, who had left university a year earlier, for assistance in writing a theatrical documentary about sex through the ages. Although this project was eventually abandoned, it brought Palin and Jones together as a writing duo and led them to write comedy for various BBC programmes, such as \"The Ken Dodd Show\", \"The Billy Cotton Bandshow\", and \"The Illustrated Weekly Hudd\". They collaborated in writing lyrics for an album by Barry Booth called Diversions. They were also in the team of writers working for \"The Frost Report\", whose other members included Frank Muir, Barry Cryer, Marty Feldman, Ronnie Barker, Ronnie Corbett, Dick Vosburgh and future Monty Python members Graham Chapman, John Cleese and Eric Idle. Although the members of Monty Python had already encountered each other over the years, \"The Frost Report\" was the first time all the British members of Monty Python (its sixth member, Terry Gilliam, was at that time an American citizen) worked together. During the run of \"The Frost Report\" the Palin/Jones team contributed material to two shows starring John Bird: \"The Late Show\" and \"A Series of Birds\". For \"A Series of Birds\" the Palin/Jones team had their first experience of writing narrative instead of the short sketches they were accustomed to conceiving.\n\nFollowing \"The Frost Report\" the Palin/Jones team worked both as actors and writers on the show \"Twice a Fortnight\" with Graeme Garden, Bill Oddie and Jonathan Lynn, and the successful children's comedy show \"Do Not Adjust Your Set\" with Idle and David Jason. The show also featured musical numbers by the Bonzo Dog Doo-Dah Band, including future Monty Python musical collaborator Neil Innes. The animations for \"Do Not Adjust Your Set\" were made by Terry Gilliam. Eager to work with Palin sans Jones, Cleese later asked him to perform in \"How to Irritate People\" together with Chapman and Tim Brooke-Taylor. The Palin/Jones team were reunited for \"The Complete and Utter History of Britain\".\n\nOn the strength of their work on \"The Frost Report\" and other programmes, Cleese and Chapman had been offered a show by the BBC, but Cleese was reluctant to do a two-man show for various reasons, among them Chapman's reputedly difficult personality. During this period Cleese contacted Palin about doing the show that would ultimately become \"Monty Python's Flying Circus\". At the same time the success of \"Do Not Adjust Your Set\" had led Palin, Jones, Idle and Gilliam to be offered their own series and, while it was still in production, Palin agreed to Cleese's proposal and brought along Idle, Jones and Gilliam. Thus the formation of the Monty Python troupe has been referred to as a result of Cleese's desire to work with Palin and the chance circumstances that brought the other four members into the fold.\n\nIn \"Monty Python\", Palin played various roles, which ranged from manic enthusiasm (such as the lumberjack of the Lumberjack Song, or Herbert Anchovy, host of the game show \"Blackmail\") to unflappable calmness (such as the Dead parrot vendor or Cheese Shop proprietor). As a straight man he was often a foil to the rising ire of characters portrayed by John Cleese. He also played timid, socially inept characters such as Arthur Putey, the man who sits quietly as a marriage counsellor (Eric Idle) makes love to his wife (Carol Cleveland), and Mr. Anchovy, a chartered accountant who wants to become a lion tamer. He also appeared as the \"It's\" man at the beginning of most episodes.\n\nPalin frequently co-wrote sketches with Terry Jones and also initiated the \"Spanish Inquisition sketch\", which included the catchphrase \"Nobody expects the Spanish Inquisition!\" He also composed songs with Jones including \"The Lumberjack Song\", \"Every Sperm is Sacred\" and \"Spam\". His solo musical compositions included \"Decomposing Composers\" and \"Finland\".\n\nAfter the \"Monty Python\" television series ended in 1974, the Palin/Jones team worked on \"Ripping Yarns\", an intermittent television comedy series broadcast over three years from 1976. They had earlier collaborated on the play \"Secrets\" from the BBC series \"Black and Blue\" in 1973. He starred as Dennis the Peasant in Terry Gilliam's 1977 film \"Jabberwocky\". Palin also appeared in \"All You Need Is Cash\" (1978) as Eric Manchester (based on Derek Taylor), the press agent for the Rutles. In 1980, Palin co-wrote \"Time Bandits\" with Terry Gilliam. He also acted in the film.\n\nIn 1982, Palin wrote and starred in \"The Missionary\", co-starring Maggie Smith. In it, he plays the Reverend Charles Fortescue, who is recalled from Africa to aid prostitutes. He co-starred with Maggie Smith again in the 1984 comedy film \"A Private Function\". In 1984, he reunited with Terry Gilliam to appear in \"Brazil\". He appeared in the comedy film \"A Fish Called Wanda\", for which he won the BAFTA Award for Best Actor in a Supporting Role. Cleese reunited the main cast almost a decade later to make \"Fierce Creatures\". After filming for \"Fierce Creatures\" finished, Palin went on a travel journey for a BBC documentary and, returning a year later, found that the end of \"Fierce Creatures\" had failed at test screenings and had to be reshot.\n\nUntil \"The Death of Stalin\" in 2017, and with the exception of several made-for-TV films and voice work for animations, Palin's last film role was a small part in \"The Wind in the Willows\", a film directed by and starring Terry Jones. Palin also appeared with John Cleese in his documentary, \"The Human Face\". Palin was cast in a supporting role in the Tom Hanks and Meg Ryan romantic comedy \"You've Got Mail\", but his role was eventually cut entirely.\n\nPalin has also appeared in serious drama. In 1991 Palin appeared in a film, \"American Friends\", he wrote based upon a real event in the life of his great-grandfather, a fellow at St John's College, Oxford. In that same year he also played the part of a headmaster in Alan Bleasdale's Channel 4 drama series \"GBH\".\n\nPalin also had a small cameo role in Australian soap opera \"Home and Away\". He played an English surfer with a fear of sharks, who interrupts a conversation between two main characters to ask whether there were any sharks in the sea. This was filmed while he was in Australia for the \"Full Circle\" series, with a segment about the filming of the role featuring in the series. In November 2005, he appeared in the \"John Peel's Record Box\" documentary.\n\nIn 2013 Michael Palin appeared in a First World War drama titled \"The Wipers Times\" written by Ian Hislop and Nick Newman. At the Cannes Film Festival in 2016, it was announced that Palin was set to star alongside Adam Driver in Terry Gilliam's \"The Man Who Killed Don Quixote\". Palin, however, dropped out of the film after it ran into a financial problem.\n\nPalin while speaking at the Edinburgh International Film Festival announced that he was presenting the two-part documentary \"Michael Palin in North Korea\" to be broadcast on the British television network Channel 5 (UK). The documentary was broadcast in September 2018, in two one-hour segments on Channel 5 in Britain and in a single two-hour programme on National Geographic in the United States. It was broadcast again by Channel 5, in a single two-hour programme in December 2018.\n\nPalin assisted Campaign for Better Transport and others with campaigns on sustainable transport, particularly those relating to urban areas, and has been president of the campaign since 1986. On 2 January 2011, he became the first person to sign the UK-based Campaign for Better Transport's Fair Fares Now campaign. In July 2015, he signed an open letter and gave an interview to support \"a strong BBC at the centre of British life\" at a time the government was reviewing the corporation's size and activities.\n\nIn July 2010, Palin sent a message of support for the Dongria Kondh tribe of India, who are resisting mining on their land by the company Vedanta Resources. Palin said, \"I've been to the Nyamgiri Hills in Orissa and seen the forces of money and power that Vedanta Resources have arrayed against a people who have occupied their land for thousands of years, who husband the forest sustainably and make no great demands on the state or the government. The tribe I visited simply want to carry on living in the villages that they and their ancestors have always lived in\".\n\nPalin's first travel documentary was segment 4 of the 1980 BBC Television series \"Great Railway Journeys of the World\", entitled \"Confessions of a Trainspotter\". Throughout the hour long show, Palin humorously reminisces about his childhood hobby of train spotting, while he travels throughout the UK by train, from London to the Kyle of Lochalsh, via Manchester, York, Newcastle upon Tyne, Edinburgh and Inverness. He rides vintage train lines and trains including \"Flying Scotsman\". At the Kyle of Lochalsh, Palin bought the station's long metal platform sign and is seen lugging it back to London with him.\n\nIn 1994, Palin travelled through Ireland for the same series, entitled \"Derry to Kerry\". In a quest for family roots, he attempted to trace his great grandmother – Brita Gallagher – who set sail from Ireland years ago during the Great Famine (1845–1849), bound for a new life in Burlington, New Jersey. The series is a trip along the Palin family line.\n\nStarting in 1989, Palin appeared as presenter in a series of travel programmes made for the BBC. It was after the veteran TV globetrotter Alan Whicker and journalist Miles Kington turned down presenting the first of these, \"Around the World in 80 Days with Michael Palin\", that gave Palin the opportunity to present his first and subsequent travel shows. These programmes have been broadcast worldwide in syndication, and were also sold on VHS tape and later on DVD:\n\n\nFollowing each trip, Palin wrote a book about his travels, providing information and insights not included in the TV programme. Each book is illustrated with photographs by Basil Pao, the stills photographer who was on the team. (Exception: the first book, \"Around the World in 80 Days\", contains some pictures by Pao but most are by other photographers.)\n\nAll seven of these books were also made available as audio books, and all of them are read by Palin himself. \"Around the World in 80 Days\" and \"Hemingway Adventure\" are unabridged, while the other four books were made in both abridged and unabridged versions, although the unabridged versions can be very difficult to find.\n\nFor four of the trips a photography book was made by Pao, each with an introduction written by Palin. These are large coffee-table style books with pictures printed on glossy paper. The majority of the pictures are of various people encountered on the trip, as informal portraits or showing them engaged in some interesting activity. Some of the landscape photos are displayed as two-page spreads.\n\nPalin's travel programmes are responsible for a phenomenon termed the \"Palin effect\": areas of the world that he has visited suddenly become popular tourist attractions – for example, the significant increase in the number of tourists interested in Peru after Palin visited Machu Picchu. In a 2006 survey of \"15 of the world's top travel writers\" by \"The Observer\", Palin named Peru's Pongo de Mainique (canyon below the Machu Picchu) his \"favourite place in the world\".\n\nPalin notes in his book of \"Around the World in 80 Days\" that the final leg of his journey could originally have taken him and his crew on one of the trains involved in the Clapham Junction rail crash, but they arrived ahead of schedule and caught an earlier train.\n\nIn recent years, Palin has written and presented occasional documentary programmes on artists that interest him. The first, on Scottish painter Anne Redpath, was \"Palin on Redpath\" in 1997. In \"The Bright Side of Life\" (2000), Palin continued on a Scottish theme, looking at the work of the Scottish Colourists. Two further programmes followed on European painters; \"Michael Palin and the Ladies Who Loved Matisse\" (2004) and \"Michael Palin and the Mystery of Hammershøi\" (2005), about the French artist Henri Matisse and Danish artist Vilhelm Hammershøi respectively. The DVD \"Michael Palin on Art\" contains all these documentaries except for the Matisse programme.\n\nIn November 2008, Palin presented a First World War documentary about Armistice Day, 11 November 1918, when thousands of soldiers lost their lives in battle after the war had officially ended. Palin filmed on the battlefields of Northern France and Belgium for the programme, called the \"Last Day of World War One\", produced for the BBC's \"Timewatch\" series.\n\nPalin was instrumental in setting up the Michael Palin Centre for Stammering Children in 1993. Also in 1993, each member of Monty Python had an asteroid named after them. Palin's is Asteroid 9621 Michaelpalin. In 2003, inside the Globe a commemorative stone was placed - Palin has his own stone, to mark donors to the theatre, but it is misspelt as \"Michael Pallin\". The story goes that John Cleese paid for the stone, and mischievously insisted on misspelling his name.\n\nIn honour of his achievements as a traveller, especially rail travel, Palin has two British trains named after him. In 2002, Virgin Trains' new £5 million high speed Super Voyager train number 221130 was named \"Michael Palin\" – it carries his name externally and a plaque is located adjacent to the onboard shop with information on Palin and his many journeys. Also, National Express East Anglia named a British Rail Class 153 (unit number 153335) after him. (He is also a model railway enthusiast.)\nIn 2008, he received the James Joyce Award of the Literary and Historical Society in Dublin. In recognition of his services to the promotion of geography, Palin was awarded the Livingstone Medal of the Royal Scottish Geographical Society in March 2009, along with a Fellowship of this Society (FRGS). In June 2013, he was similarly honoured in Canada with a gold medal for achievements in geography by the Royal Canadian Geographical Society. In June 2009, Palin was elected for a three-year term as President of the Royal Geographical Society. Because of his self-described \"amenable, conciliatory character\" Michael Palin has been referred to as unofficially \"Britain's Nicest Man.\"\n\nIn September 2013, Moorlands School, Leeds named one of their school houses \"Palin\" after him. The University of St Andrews awarded Palin an honorary Doctor of Science degree during their June 2017 graduation ceremonies, with the degree recognising his contribution to the public's understanding of contemporary geography. He joins his fellow Pythons John Cleese and Terry Jones in receiving an honorary degree from the Fife institution. In October 2018, the Royal Canadian Geographical Society awarded Palin the first Louie Kamookak Medal for advances in geography, for his book on the history of the polar exploration vessel HMS \"Erebus\".\n\nHe was appointed a Commander of the Order of the British Empire (CBE) for services to television in the 2000 New Year Honours.\n\nOn 28 December 2018, it was announced that Palin had been appointed a Knight Commander of the Order of St Michael and St George (KCMG) for services to travel, culture and geography in the 2019 New Year Honours. Palin is the only Python to receive a knighthood (John Cleese turned down a CBE in 1996).\n\n\nAll his travel books can also be read at no charge, complete and unabridged, on his website.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19622", "url": "https://en.wikipedia.org/wiki?curid=19622", "title": "Materials science", "text": "Materials science\n\nThe interdisciplinary field of materials science, also commonly termed materials science and engineering is the design and discovery of new materials, particularly solids. The intellectual origins of materials science stem from the Enlightenment, when researchers began to use analytical thinking from chemistry, physics, and engineering to understand ancient, phenomenological observations in metallurgy and mineralogy. Materials science still incorporates elements of physics, chemistry, and engineering. As such, the field was long considered by academic institutions as a sub-field of these related fields. Beginning in the 1940s, materials science began to be more widely recognized as a specific and distinct field of science and engineering, and major technical universities around the world created dedicated schools of the study, within either the Science or Engineering schools, hence the naming. \nMany of the most pressing scientific problems humans currently face are due to the limits of the materials that are available and how they are used. Thus, breakthroughs in materials science are likely to affect the future of technology significantly.\n\nMaterials scientists emphasize understanding how the history of a material (its \"processing\") influences its structure, and thus the material's properties and performance. The understanding of processing-structure-properties relationships is called the . This paradigm is used to advance understanding in a variety of research areas, including nanotechnology, biomaterials, and metallurgy. Materials science is also an important part of forensic engineering and failure analysis investigating materials, products, structures or components which fail or do not function as intended, causing personal injury or damage to property. Such investigations are key to understanding, for example, the causes of various aviation accidents and incidents.\n\nThe material of choice of a given era is often a defining point. Phrases such as Stone Age, Bronze Age, Iron Age, and Steel Age are historic, if arbitrary examples. Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering and applied science. Modern materials science evolved directly from metallurgy, which itself evolved from mining and (likely) ceramics and earlier from the use of fire. A major breakthrough in the understanding of materials occurred in the late 19th century, when the American scientist Josiah Willard Gibbs demonstrated that the thermodynamic properties related to atomic structure in various phases are related to the physical properties of a material. Important elements of modern materials science are a product of the space race: the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. Materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials.\n\nBefore the 1960s (and in some cases decades after), many eventual \"materials science\" departments were \"metallurgy\" or \"ceramics engineering\" departments, reflecting the 19th and early 20th century emphasis on metals and ceramics. The growth of materials science in the United States was catalyzed in part by the Advanced Research Projects Agency, which funded a series of university-hosted laboratories in the early 1960s \"to expand the national program of basic research and training in the materials sciences.\" The field has since broadened to include every class of materials, including ceramics, polymers, semiconductors, magnetic materials, biomaterials, and nanomaterials, generally classified into three distinct groups: ceramics, metals, and polymers. The prominent change in materials science during the recent decades is active usage of computer simulations to find new materials, predict properties, and understand phenomena.\n\nA material is defined as a substance (most often a solid, but other condensed phases can be included) that is intended to be used for certain applications. There are a myriad of materials around us—they can be found in anything from buildings to spacecraft. Materials can generally be further divided into two classes: crystalline and non-crystalline. The traditional examples of materials are metals, semiconductors, ceramics and polymers. New and advanced materials that are being developed include nanomaterials, biomaterials, and energy materials to name a few.\n\nThe basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a given application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.\n\nAs mentioned above, structure is one of the most important components of the field of materials science. Materials science examines the structure of materials from the atomic scale, all the way up to the macro scale. Characterization is the way materials scientists examine the structure of a material. This involves methods such as diffraction with X-rays, electrons, or neutrons, and various forms of spectroscopy and chemical analysis such as Raman spectroscopy, energy-dispersive spectroscopy (EDS), chromatography, thermal analysis, electron microscope analysis, etc. Structure is studied at various levels, as detailed below.\n\nThis deals with the atoms of the materials, and how they are arranged to give molecules, crystals, etc. Much of the electrical, magnetic and chemical properties of materials arise from this level of structure. The length scales involved are in angstroms(Å).\nThe way in which the atoms and molecules are bonded and arranged is fundamental to studying the properties and behavior of any material.\n\nNanostructure deals with objects and structures that are in the 1–100 nm range. In many materials, atoms or molecules agglomerate together to form objects at the nanoscale. This causes many interesting electrical, magnetic, optical, and mechanical properties.\n\nIn describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have \"one dimension\" on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have \"two dimensions\" on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have \"three dimensions\" on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometre range. The term 'nanostructure' is often used when referring to magnetic technology. Nanoscale structure in biology is often called ultrastructure.\n\nMaterials which atoms and molecules form constituents in the nanoscale (i.e., they form nanostructure) are called nanomaterials. Nanomaterials are subject of intense research in the materials science community due to the unique properties that they exhibit.\n\nMicrostructure is defined as the structure of a prepared surface or thin foil of material as revealed by a microscope above 25× magnification. It deals with objects from 100 nm to a few cm. The microstructure of a material (which can be broadly classified into metallic, polymeric, ceramic and composite) can strongly influence physical properties such as strength, toughness, ductility, hardness, corrosion resistance, high/low temperature behavior, wear resistance, and so on. Most of the traditional materials (such as metals and ceramics) are microstructured.\n\nThe manufacture of a perfect crystal of a material is physically impossible. For example, any crystalline material will contain defects such as precipitates, grain boundaries (Hall–Petch relationship), vacancies, interstitial atoms or substitutional atoms. The microstructure of materials reveals these larger defects, so that they can be studied, with significant advances in simulation resulting in exponentially increasing understanding of how defects can be used to enhance material properties.\n\nMacro structure is the appearance of a material in the scale millimeters to meters—it is the structure of the material as seen with the naked eye.\n\nCrystallography is the science that examines the arrangement of atoms in crystalline solids. Crystallography is a useful tool for materials scientists. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically, because the natural shapes of crystals reflect the atomic structure. Further, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Mostly, materials do not occur as a single crystal, but in polycrystalline form, i.e., as an aggregate of small crystals with different orientations. Because of this, the powder diffraction method, which uses diffraction patterns of polycrystalline samples with a large number of crystals, plays an important role in structural determination.\nMost materials have a crystalline structure, but some important materials do not exhibit regular crystal structure. Polymers display varying degrees of crystallinity, and many are completely noncrystalline. Glass, some ceramics, and many natural materials are amorphous, not possessing any long-range order in their atomic arrangements. The study of polymers combines elements of chemical and statistical thermodynamics to give thermodynamic and mechanical, descriptions of physical properties.\n\nTo obtain a full understanding of the material structure and how it relates to its properties, the materials scientist must study how the different atoms, ions and molecules are arranged and bonded to each other. This involves the study and use of quantum chemistry or quantum physics. Solid-state physics, solid-state chemistry and physical chemistry are also involved in the study of bonding and structure.\n\nMaterials exhibit myriad properties, including the following.\nThe properties of a material determine its usability and hence its engineering application.\n\nSynthesis and processing involves the creation of a material with the desired micro-nanostructure. From an engineering standpoint, a material cannot be used in industry if no economical production method for it has been developed. Thus, the processing of materials is vital to the field of materials science.\n\nDifferent materials require different processing or synthesis methods. For example, the processing of metals has historically been very important and is studied under the branch of materials science named \"physical metallurgy\". Also, chemical and physical methods are also used to synthesize other materials such as polymers, ceramics, thin films, etc. As of the early 21st century, new methods are being developed to synthesize nanomaterials such as graphene.\n\nThermodynamics is concerned with heat and temperature and their relation to energy and work. It defines macroscopic variables, such as internal energy, entropy, and pressure, that partly describe a body of matter or radiation. It states that the behavior of those variables is subject to general constraints, that are common to all materials, not the peculiar properties of particular materials. These general constraints are expressed in the four laws of thermodynamics. Thermodynamics describes the bulk behavior of the body, not the microscopic behaviors of the very large numbers of its microscopic constituents, such as molecules. The behavior of these microscopic particles is described by, and the laws of thermodynamics are derived from, statistical mechanics.\n\nThe study of thermodynamics is fundamental to materials science. It forms the foundation to treat general phenomena in materials science and engineering, including chemical reactions, magnetism, polarizability, and elasticity. It also helps in the understanding of phase diagrams and phase equilibrium.\n\nChemical kinetics is the study of the rates at which systems that are out of equilibrium change under the influence of various forces. When applied to materials science, it deals with how a material changes with time (moves from non-equilibrium to equilibrium state) due to application of a certain field. It details the rate of various processes evolving in materials including shape, size, composition and structure. Diffusion is important in the study of kinetics as this is the most common mechanism by which materials undergo change.\n\nKinetics is essential in processing of materials because, among other things, it details how the microstructure changes with application of heat.\n\nMaterials science has received much attention from researchers. In most universities, many departments ranging from physics to chemistry to chemical engineering, along with materials science departments, are involved in materials research. Research in materials science is vibrant and consists of many avenues. The following list is in no way exhaustive. It serves only to highlight certain important research areas.\n\nNanomaterials describe, in principle, materials of which a single unit is sized (in at least one dimension) between 1 and 1000 nanometers (10 meter) but is usually 1–100 nm.\n\nNanomaterials research takes a materials science-based approach to nanotechnology, leveraging advances in materials metrology and synthesis which have been developed in support of microfabrication research. Materials with structure at the nanoscale often have unique optical, electronic, or mechanical properties.\n\nThe field of nanomaterials is loosely organized, like the traditional field of chemistry, into organic (carbon-based) nanomaterials such as fullerenes, and inorganic nanomaterials based on other elements, such as silicon. Examples of nanomaterials include fullerenes, carbon nanotubes, nanocrystals, etc.\n\nA biomaterial is any matter, surface, or construct that interacts with biological systems. The study of biomaterials is called \"bio materials science\". It has experienced steady and strong growth over its history, with many companies investing large amounts of money into developing new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering, and materials science.\n\nBiomaterials can be derived either from nature or synthesized in a laboratory using a variety of chemical approaches using metallic components, polymers, bioceramics, or composite materials. They are often used and/or adapted for a medical application, and thus comprises whole or part of a living structure or biomedical device which performs, augments, or replaces a natural function. Such functions may be benign, like being used for a heart valve, or may be bioactive with a more interactive functionality such as hydroxylapatite coated hip implants. Biomaterials are also used every day in dental applications, surgery, and drug delivery. For example, a construct with impregnated pharmaceutical products can be placed into the body, which permits the prolonged release of a drug over an extended period of time. A biomaterial may also be an autograft, allograft or xenograft used as an organ transplant material.\n\nSemiconductors, metals, and ceramics are used today to form highly complex systems, such as integrated electronic circuits, optoelectronic devices, and magnetic and optical mass storage media. These materials form the basis of our modern computing world, and hence research into these materials is of vital importance.\n\nSemiconductors are a traditional example of these types of materials. They are materials that have properties that are intermediate between conductors and insulators. Their electrical conductivities are very sensitive to impurity concentrations, and this allows for the use of doping to achieve desirable electronic properties. Hence, semiconductors form the basis of the traditional computer.\n\nThis field also includes new areas of research such as superconducting materials, spintronics, metamaterials, etc. The study of these materials involves knowledge of materials science and solid-state physics or condensed matter physics.\n\nWith continuing increases in computing power, simulating the behavior of materials has become possible. This enables materials scientists to understand behavior and mechanisms, explain properties formerly poorly understood, and even to design new materials. Efforts surrounding Integrated computational materials engineering are now focusing on combining computational methods with experiments to drastically reduce the time and effort to optimize materials properties for a given application. This involves simulating materials at all length scales, using methods such as density functional theory, molecular dynamics, Monte Carlo algorithm, dislocation dynamics, Phase field models, Finite element method, and many more.\n\nRadical materials advances can drive the creation of new products or even new industries, but stable industries also employ materials scientists to make incremental improvements and troubleshoot issues with currently used materials. Industrial applications of materials science include materials design, cost-benefit tradeoffs in industrial production of materials, processing methods (casting, rolling, welding, ion implantation, crystal growth, thin-film deposition, sintering, glassblowing, etc.), and analytic methods (characterization methods such as electron microscopy, X-ray diffraction, calorimetry, nuclear microscopy (HEFIB), Rutherford backscattering, neutron diffraction, small-angle X-ray scattering (SAXS), etc.).\n\nBesides material characterization, the material scientist or engineer also deals with extracting materials and converting them into useful forms. Thus ingot casting, foundry methods, blast furnace extraction, and electrolytic extraction are all part of the required knowledge of a materials engineer. Often the presence, absence, or variation of minute quantities of secondary elements and compounds in a bulk material will greatly affect the final properties of the materials produced. For example, steels are classified based on 1/10 and 1/100 weight percentages of the carbon and other alloying elements they contain. Thus, the extracting and purifying methods used to extract iron in a blast furnace can affect the quality of steel that is produced.\n\nAnother application of material science is the structures of ceramics and glass typically associated with the most brittle materials. Bonding in ceramics and glasses uses covalent and ionic-covalent types with SiO (silica or sand) as a fundamental building block. Ceramics are as soft as clay or as hard as stone and concrete. Usually, they are crystalline in form. Most glasses contain a metal oxide fused with silica. At high temperatures used to prepare glass, the material is a viscous liquid. The structure of glass forms into an amorphous state upon cooling. Windowpanes and eyeglasses are important examples. Fibers of glass are also available. Scratch resistant Corning Gorilla Glass is a well-known example of the application of materials science to drastically improve the properties of common components. Diamond and carbon in its graphite form are considered to be ceramics.\n\nEngineering ceramics are known for their stiffness and stability under high temperatures, compression and electrical stress. Alumina, silicon carbide, and tungsten carbide are made from a fine powder of their constituents in a process of sintering with a binder. Hot pressing provides higher density material. Chemical vapor deposition can place a film of a ceramic on another material. Cermets are ceramic particles containing some metals. The wear resistance of tools is derived from cemented carbides with the metal phase of cobalt and nickel typically added to modify properties.\n\nFilaments are commonly used for reinforcement in composite materials.\n\nAnother application of materials science in industry is making composite materials. These are structured materials composed of two or more macroscopic phases. Applications range from structural elements such as steel-reinforced concrete, to the thermal insulating tiles which play a key and integral role in NASA's Space Shuttle thermal protection system which is used to protect the surface of the shuttle from the heat of re-entry into the Earth's atmosphere. One example is reinforced Carbon-Carbon (RCC), the light gray material which withstands re-entry temperatures up to and protects the Space Shuttle's wing leading edges and nose cap. RCC is a laminated composite material made from graphite rayon cloth and impregnated with a phenolic resin. After curing at high temperature in an autoclave, the laminate is pyrolized to convert the resin to carbon, impregnated with furfural alcohol in a vacuum chamber, and cured-pyrolized to convert the furfural alcohol to carbon. To provide oxidation resistance for reuse ability, the outer layers of the RCC are converted to silicon carbide.\n\nOther examples can be seen in the \"plastic\" casings of television sets, cell-phones and so on. These plastic casings are usually a composite material made up of a thermoplastic matrix such as acrylonitrile butadiene styrene (ABS) in which calcium carbonate chalk, talc, glass fibers or carbon fibers have been added for added strength, bulk, or electrostatic dispersion. These additions may be termed reinforcing fibers, or dispersants, depending on their purpose.\n\nPolymers are chemical compounds made up of a large number of identical components linked together like chains. They are an important part of materials science. Polymers are the raw materials (the resins) used to make what are commonly called plastics and rubber. Plastics and rubber are really the final product, created after one or more polymers or additives have been added to a resin during processing, which is then shaped into a final form. Plastics which have been around, and which are in current widespread use, include polyethylene, polypropylene, polyvinyl chloride (PVC), polystyrene, nylons, polyesters, acrylics, polyurethanes, and polycarbonates and also rubbers which have been around are natural rubber, styrene-butadiene rubber, chloroprene, and butadiene rubber. Plastics are generally classified as \"commodity\", \"specialty\" and \"engineering\" plastics.\n\nPolyvinyl chloride (PVC) is widely used, inexpensive, and annual production quantities are large. It lends itself to a vast array of applications, from artificial leather to electrical insulation and cabling, packaging, and containers. Its fabrication and processing are simple and well-established. The versatility of PVC is due to the wide range of plasticisers and other additives that it accepts. The term \"additives\" in polymer science refers to the chemicals and compounds added to the polymer base to modify its material properties.\n\nPolycarbonate would be normally considered an engineering plastic (other examples include PEEK, ABS). Such plastics are valued for their superior strengths and other special material properties. They are usually not used for disposable applications, unlike commodity plastics.\n\nSpecialty plastics are materials with unique characteristics, such as ultra-high strength, electrical conductivity, electro-fluorescence, high thermal stability, etc.\n\nThe dividing lines between the various types of plastics is not based on material but rather on their properties and applications. For example, polyethylene (PE) is a cheap, low friction polymer commonly used to make disposable bags for shopping and trash, and is considered a commodity plastic, whereas medium-density polyethylene (MDPE) is used for underground gas and water pipes, and another variety called ultra-high-molecular-weight polyethylene (UHMWPE) is an engineering plastic which is used extensively as the glide rails for industrial equipment and the low-friction socket in implanted hip joints.\n\nThe study of metal alloys is a significant part of materials science. Of all the metallic alloys in use today, the alloys of iron (steel, stainless steel, cast iron, tool steel, alloy steels) make up the largest proportion both by quantity and commercial value. Iron alloyed with various proportions of carbon gives low, mid and high carbon steels. An iron-carbon alloy is only considered steel if the carbon level is between 0.01% and 2.00%. For the steels, the hardness and tensile strength of the steel is related to the amount of carbon present, with increasing carbon levels also leading to lower ductility and toughness. Heat treatment processes such as quenching and tempering can significantly change these properties, however. Cast Iron is defined as an iron–carbon alloy with more than 2.00% but less than 6.67% carbon. Stainless steel is defined as a regular steel alloy with greater than 10% by weight alloying content of Chromium. Nickel and Molybdenum are typically also found in stainless steels.\n\nOther significant metallic alloys are those of aluminium, titanium, copper and magnesium. Copper alloys have been known for a long time (since the Bronze Age), while the alloys of the other three metals have been relatively recently developed. Due to the chemical reactivity of these metals, the electrolytic extraction processes required were only developed relatively recently. The alloys of aluminium, titanium and magnesium are also known and valued for their high strength-to-weight ratios and, in the case of magnesium, their ability to provide electromagnetic shielding. These materials are ideal for situations where high strength-to-weight ratios are more important than bulk cost, such as in the aerospace industry and certain automotive engineering applications.\n\nThe study of semiconductors is a significant part of materials science. A semiconductor is a material that has a resistivity between a metal and insulator. Its electronic properties can be greatly altered through intentionally introducing impurities or doping. From these semiconductor materials, things such as diodes, transistors, light-emitting diodes (LEDs), and analog and digital electric circuits can be built, making them materials of interest in industry. Semiconductor devices have replaced thermionic devices (vacuum tubes) in most applications. Semiconductor devices are manufactured both as single discrete devices and as integrated circuits (ICs), which consist of a number—from a few to millions—of devices manufactured and interconnected on a single semiconductor substrate.\n\nOf all the semiconductors in use today, silicon makes up the largest portion both by quantity and commercial value. Monocrystalline silicon is used to produce wafers used in the semiconductor and electronics industry. Second to silicon, gallium arsenide (GaAs) is the second most popular semiconductor used. Due to its higher electron mobility and saturation velocity compared to silicon, it is a material of choice for high-speed electronics applications. These superior properties are compelling reasons to use GaAs circuitry in mobile phones, satellite communications, microwave point-to-point links and higher frequency radar systems. Other semiconductor materials include germanium, silicon carbide, and gallium nitride and have various applications.\n\nMaterials science evolved—starting from the 1950s—because it was recognized that to create, discover and design new materials, one had to approach it in a unified manner. Thus, materials science and engineering emerged in many ways: renaming and/or combining existing metallurgy and ceramics engineering departments; splitting from existing solid state physics research, itself growing into condensed matter physics); pulling in relatively new polymer engineering and polymer science; recombining from the previous, as well as chemistry, chemical engineering, mechanical engineering, and electrical engineering; and more.\n\nThe field is inherently interdisciplinary, and the materials scientists/engineers must be aware and make use of the methods of the physicist, chemist and engineer. The field thus maintains close relationships with these fields. Also, many physicists, chemists and engineers also find themselves working in materials science.\n\nThe field of materials science and engineering is important both from a scientific perspective, as well as from an engineering one. When discovering new materials, one encounters new phenomena that may not have been observed before. Hence, there is a lot of science to be discovered when working with materials. Materials science also provides a test for theories in condensed matter physics.\n\nMaterials are of the utmost importance for engineers, as the usage of the appropriate materials is crucial when designing systems. As a result, materials science is an increasingly important part of an engineer's education.\n\n\n\n"}
{"id": "19623", "url": "https://en.wikipedia.org/wiki?curid=19623", "title": "Mitsubishi A6M Zero", "text": "Mitsubishi A6M Zero\n\nThe Mitsubishi A6M \"Zero\" is a long-range fighter aircraft formerly manufactured by Mitsubishi Aircraft Company, a part of Mitsubishi Heavy Industries, and operated by the Imperial Japanese Navy from 1940 to 1945. The A6M was designated as the , or the Mitsubishi A6M Rei-sen. The A6M was usually referred to by its pilots as the \"Reisen\" (, zero fighter), \"0\" being the last digit of the imperial year 2600 (1940) when it entered service with the Imperial Navy. The official Allied reporting name was \"Zeke\", although the use of the name \"Zero\" (from Type 0) was used colloquially by the Allies as well.\n\nThe Zero was considered the most capable carrier-based fighter in the world when it was introduced early in World War II, combining excellent maneuverability and very long range. The Imperial Japanese Navy Air Service (IJNAS) also frequently used it as a land-based fighter.\n\nIn early combat operations, the Zero gained a legendary reputation as a dogfighter, achieving an outstanding kill ratio of 12 to 1, but by mid-1942 a combination of new tactics and the introduction of better equipment enabled Allied pilots to engage the Zero on generally equal terms. By 1943, due to inherent design weaknesses, such as a lack of hydraulic flaps and rudder rendering it extremely unmaneuverable at high speeds, and an inability to equip it with a more powerful aircraft engine, the Zero gradually became less effective against newer Allied fighters. By 1944, with opposing Allied fighters approaching its levels of maneuverability and consistently exceeding its firepower, armor, and speed, the A6M had largely become outdated as a fighter aircraft. However, as design delays and production difficulties hampered the introduction of newer Japanese aircraft models, the Zero continued to serve in a front line role until the end of the war in the Pacific. During the final phases, it was also adapted for use in \"kamikaze\" operations. Japan produced more Zeros than any other model of combat aircraft during the war.\n\nThe Mitsubishi A5M fighter was just entering service in early 1937, when the Imperial Japanese Navy (IJN) started looking for its eventual replacement. On October 5, 1937, they issued \"Planning Requirements for the Prototype 12-shi Carrier-based Fighter\", sending it to Nakajima and Mitsubishi. Both firms started preliminary design work while they awaited more definitive requirements to be handed over in a few months.\n\nBased on the experiences of the A5M in China, the IJN sent out updated requirements in October calling for a speed of at and a climb to in 9.5 minutes. With drop tanks, they wanted an endurance of two hours at normal power, or six to eight hours at economical cruising speed. Armament was to consist of two 20 mm cannons, two 7.7 mm (.303 in) machine guns and two bombs. A complete radio set was to be mounted in all aircraft, along with a radio direction finder for long-range navigation. The maneuverability was to be at least equal to that of the A5M, while the wingspan had to be less than to allow for use on aircraft carriers. All this was to be achieved with available engines, a significant design limitation.\n\nNakajima's team considered the new requirements unachievable and pulled out of the competition in January. Mitsubishi's chief designer, Jiro Horikoshi, thought that the requirements could be met, but only if the aircraft were made as light as possible. Every possible weight-saving measure was incorporated into the design. Most of the aircraft was built of a new top-secret aluminium alloy developed by Sumitomo Metal Industries in 1936. Called \"extra super duralumin\" (ESD), it was lighter, stronger and more ductile than other alloys (e.g. 24S alloy) used at the time, but was prone to corrosive attack, which made it brittle. This detrimental effect was countered with an anti-corrosion coating applied after fabrication. No armour protection was provided for the pilot, engine or other critical points of the aircraft, and self-sealing fuel tanks, which were becoming common at the time, were not used. This made the Zero lighter, more maneuverable, and the longest-ranged single-engine fighter of World War II, which made it capable of searching out an enemy hundreds of kilometres away, bringing them to battle, then returning to its base or aircraft carrier. However, that tradeoff in weight and construction also made it prone to catching fire and exploding when struck by enemy rounds.\n\nWith its low-wing cantilever monoplane layout, retractable, wide-set conventional landing gear and enclosed cockpit, the Zero was one of the most modern carrier based aircraft in the world at the time of its introduction. It had a fairly high-lift, low-speed wing with very low wing loading. This, combined with its light weight, resulted in a very low stalling speed of well below . This was the main reason for its phenomenal maneuverability, allowing it to out-turn any Allied fighter of the time. Early models were fitted with servo tabs on the ailerons after pilots complained that control forces became too heavy at speeds above . They were discontinued on later models after it was found that the lightened control forces were causing pilots to overstress the wings during vigorous maneuvers.\n\nIt has been claimed that the Zero's design showed a clear influence from British and American fighter aircraft and components exported to Japan in the 1930s, and in particular on the American side, the Vought V-143 fighter. Chance Vought had sold the prototype for this aircraft and its plans to Japan in 1937. Eugene Wilson, president of Vought, claimed that when shown a captured Zero in 1943, he found that \"There on the floor was the Vought V 142 or just the spitting image of it, Japanese-made\", while the \"power-plant installation was distinctly Chance Vought, the wheel stowage into the wing roots came from Northrop, and the Japanese designers had even copied the Navy inspection stamp from Pratt & Whitney type parts.\" While the sale of the V-143 was fully legal, Wilson later acknowledged the conflicts of interest that can arise whenever military technology is exported. Counterclaims maintain that there was no significant relationship between the V-143 (which was an unsuccessful design that had been rejected by the U.S. Army Air Corps and several export customers) and the Zero, with only a superficial similarity in layout.\n\nThe Zero resembled the 1937 British Gloster F.5/34. Performance of the Gloster F.5/34 was comparable to that of early model Zeros, with its dimensions and appearance remarkably close to the Zero. Gloster had a relationship with the Japanese between the wars, with Nakajima building the carrier-based plane, the Gloster Gambet, under license. However allegations about the Zero being a copy have been discredited by some authors.\n\nThe A6M is usually known as the \"Zero\" from its Japanese Navy type designation, Type 0 carrier fighter (\"Rei shiki Kanjō sentōki\", ), taken from the last digit of the Imperial year 2600 (1940) when it entered service. In Japan, it was unofficially referred to as both \"Rei-sen\" and \"Zero-sen\"; Japanese pilots most commonly called it \"Zero-sen,\" where \"sen\" is the first syllable of \"sentōki,\" Japanese for \"fighter plane\".\n\nIn the official designation \"A6M\", the \"A\" signified a carrier-based fighter, \"6\" meant that it was the sixth such model built for the Imperial Navy, and \"M\" indicated Mitsubishi as the manufacturer.\n\nThe official Allied code name was \"Zeke\", in keeping with the practice of giving male names to Japanese fighters, female names to bombers, bird names to gliders, and tree names to trainers. \"Zeke\" was part of the first batch of \"hillbilly\" code names assigned by Captain Frank T. McCoy of Nashville, Tennessee (assigned to the Allied Technical Air Intelligence Unit (ATAIU) at Eagle Farm Airport in Australia), who wanted quick, distinctive, easy-to-remember names. The Allied code for Japanese aircraft was introduced in 1942, and McCoy chose \"Zeke\" for the \"Zero\". Later, two variants of the fighter received their own code names. The Nakajima A6M2-N floatplane version of the Zero was called \"Rufe\", and the A6M3-32 variant was initially called \"Hap\". General \"Hap\" Arnold, commander of the USAAF, objected to that name, however, so it was changed to \"Hamp\". Captured examples were examined in New Guinea and the Allies realized that it was a variant of the Zero, so it was finally renamed \"Zeke 32\".\n\nThe first Zeros (pre-series of 15 A6M2) went into operation with the 12th Rengo Kōkūtai in July 1940. On 13 September 1940, the Zeros scored their first air-to-air victories when 13 A6M2s led by Lieutenant Saburo Shindo attacked 27 Soviet-built Polikarpov I-15s and I-16s of the Chinese Nationalist Air Force, shooting down all the fighters without loss to themselves. By the time they were redeployed a year later, the Zeros had shot down 99 Chinese aircraft (266 according to other sources).\n\nAt the time of the attack on Pearl Harbor, 521 Zeros were active in the Pacific, 328 in first-line units. The carrier-borne Model 21 was the type encountered by the Americans. Its tremendous range of over allowed it to range farther from its carrier than expected, appearing over distant battlefronts and giving Allied commanders the impression that there were several times as many Zeros as actually existed.\n\nThe Zero quickly gained a fearsome reputation. Thanks to a combination of unsurpassed maneuverability — even when compared to other contemporary Axis fighters — and excellent firepower, it easily disposed the motley collection of Allied aircraft sent against it in the Pacific in 1941. It proved a difficult opponent even for the Supermarine Spitfire. \"The RAF pilots were trained in methods that were excellent against German and Italian equipment but suicide against the acrobatic Japs\", as Lt.Gen. Claire Lee Chennault had to notice. Although not as fast as the British fighter, the Mitsubishi fighter could out-turn the Spitfire with ease, sustain a climb at a very steep angle, and stay in the air for three times as long.\n\nAllied pilots soon developed tactics to cope with the Zero. Due to its extreme agility, engaging a Zero in a traditional, turning dogfight was likely to be fatal. It was better to swoop down from above in a high-speed pass, fire a quick burst, then climb quickly back up to altitude. (A short burst of fire from heavy machine guns or cannon was often enough to bring down the fragile Zero.) Such \"boom-and-zoom\" tactics were used successfully in the China Burma India Theater (CBI) by the \"Flying Tigers\" of the American Volunteer Group (AVG) against similarly maneuverable Japanese Army aircraft such as the Nakajima Ki-27 \"Nate\" and Nakajima Ki-43 \"Oscar\". AVG pilots were trained by their commander Claire Chennault to exploit the advantages of their P-40s, which were very sturdy, heavily armed, generally faster in a dive and level flight at low altitude, with a good rate of roll.\n\nAnother important maneuver was Lieutenant Commander John S. \"Jimmy\" Thach's \"Thach Weave\", in which two fighters would fly about apart. If a Zero latched onto the tail of one of the fighters, the two aircraft would turn toward each other. If the Zero followed his original target through the turn, he would come into a position to be fired on by the target's wingman. This tactic was first used to good effect during the Battle of Midway and later over the Solomon Islands.\n\nMany highly experienced Japanese aviators were lost in combat, resulting in a progressive decline in quality, which became a significant factor in Allied successes. Unexpected heavy losses of pilots at the Battles of the Coral Sea and Midway dealt the Japanese carrier air force a blow from which it never fully recovered.\nThroughout the Battle of Midway Allied pilots expressed a high level of dissatisfaction with the Grumman F4F Wildcat. The Commanding Officer of noted: They were astounded by the Zero's superiority: \n\nIn contrast, Allied fighters were designed with ruggedness and pilot protection in mind. The Japanese ace Saburō Sakai described how the toughness of early Grumman aircraft was a factor in preventing the Zero from attaining total domination: \n\nWhen the powerfully armed Lockheed P-38 Lightning, armed with four \"light barrel\" AN/M2 .50 cal. Browning machine guns and one 20 mm autocannon, and the Grumman F6F Hellcat and Vought F4U Corsair, each with six AN/M2 heavy calibre Browning guns, appeared in the Pacific theater, the A6M, with its low-powered engine and lighter armament, was hard-pressed to remain competitive. In combat with an F6F or F4U, the only positive thing that could be said of the Zero at this stage of the war was that, in the hands of a skillful pilot, it could maneuver as well as most of its opponents. Nonetheless, in competent hands, the Zero could still be deadly.\n\nDue to shortages of high-powered aviation engines and problems with planned successor models, the Zero remained in production until 1945, with over 10,000 of all variants produced.\n\nThe American military discovered many of the A6M's unique attributes when they recovered a largely intact specimen of an A6M2, the Akutan Zero, on Akutan Island in the Aleutians. During an air raid over Dutch Harbor on June 4, 1942, one A6M fighter was hit by ground-based anti-aircraft fire. Losing oil, Flight Petty Officer Tadayoshi Koga attempted an emergency landing on Akutan Island about northeast of Dutch Harbor, but his Zero flipped over on soft ground in a sudden crash-landing. Koga died instantly of head injuries (his neck was broken by the tremendous impact), but the relatively-undamaged fighter was found over a month later by an American salvage team and was shipped to Naval Air Station North Island, where testing flights of the repaired A6M revealed both strengths and deficiencies in design and performance.\n\nThe experts who evaluated the captured Zero found that the plane weighed about fully loaded, some lighter than the F4F Wildcat, the standard United States Navy fighter of the time. The A6M's airframe was \"built like a fine watch\"; the Zero was constructed with flush rivets, and even the guns were flush with the wings. The instrument panel was a \"marvel of simplicity ... with no superfluities to distract [the pilot].\" What most impressed the experts was that the Zero's fuselage and wings were constructed in one piece, unlike the American method that built them separately and joined the two parts together. The Japanese method was much slower, but resulted in a very strong structure and improved close maneuverability.\n\nAmerican test pilots found that the Zero's controls were \"very light\" at , but stiffened at faster speeds (above ) to safeguard against wing failure. The Zero could not keep up with Allied aircraft in high-speed maneuvers, and its low \"never exceed speed\" (V) made it vulnerable in a dive. While stable on the ground despite its light weight, the aircraft was designed purely for the attack role, emphasizing long range, maneuverability, and firepower at the expense of protection of its pilot. Most lacked self-sealing tanks and armor plating.\n\nCaptain Eric Brown, the Chief Naval Test Pilot of the Royal Navy, recalled being impressed by the Zero during tests of captured aircraft. \"I don't think I have ever flown a fighter that could match the rate of turn of the Zero. The Zero had ruled the roost totally and was the finest fighter in the world until mid-1943.\"\n\nThe first two A6M1 prototypes were completed in March 1939, powered by the Mitsubishi Zuisei 13 engine with a two-blade propeller. It first flew on 1 April, and passed testing within a remarkably short period. By September, it had already been accepted for Navy testing as the A6M1 Type 0 Carrier Fighter, with the only notable change being a switch to a three-bladed propeller to cure a vibration problem.\n\nWhile the navy was testing the first two prototypes, they suggested that the third be fitted with the Nakajima Sakae 12 engine instead. Mitsubishi had its own engine of this class in the form of the Kinsei, so they were somewhat reluctant to use the Sakae. Nevertheless, when the first A6M2 was completed in January 1940, the Sakae's extra power pushed the performance of the Zero well past the original specifications.\n\nThe new version was so promising that the Navy had 15 built and shipped to China before they had completed testing. They arrived in Manchuria in July 1940, and first saw combat over Chungking in August. There they proved to be completely untouchable by the Polikarpov I-16s and I-153s that had been such a problem for the A5Ms when in service. In one encounter, 13 Zeros shot down 27 I-15s and I-16s in under three minutes without loss. After hearing of these reports, the navy immediately ordered the A6M2 into production as the Type 0 Carrier Fighter, Model 11.\nReports of the Zero's performance filtered back to the US slowly. There they were dismissed by most military officials, who thought it was impossible for the Japanese to build such an aircraft.\n\nAfter the delivery of the 65th aircraft, a further change was worked into the production lines, which introduced folding wingtips to allow them to fit on aircraft carriers. The resulting Model 21 would become one of the most produced versions early in the war. A feature was the improved range with wing tank and drop tank. When the lines switched to updated models, 740 Model 21s had been completed by Mitsubishi, and another 800 by Nakajima. Two other versions of the Model 21 were built in small numbers, the Nakajima-built A6M2-N \"Rufe\" floatplane (based on the Model 11 with a slightly modified tail), and the A6M2-K two-seat trainer of which a total of 508 were built by Hitachi and the Sasebo Naval Air Arsenal.\n\nIn 1941, Nakajima introduced the Sakae 21 engine, which used a two-speed supercharger for better altitude performance, and increased power to . A prototype Zero with the new engine was first flown on July 15, 1941.\n\nThe new Sakae was slightly heavier and somewhat longer due to the larger supercharger, which moved the center of gravity too far forward on the existing airframe. To correct for this, the engine mountings were cut back by to move the engine toward the cockpit. This had the side effect of reducing the size of the main fuselage fuel tank (located between the engine and the cockpit) from to . The cowling was redesigned to enlarge the cowl flaps, revise the oil cooler air intake, and move the carburetor air intake to the upper half of the cowling.\n\nThe wings were redesigned to reduce span, eliminate the folding tips, and square off the wingtips. The inboard edge of the aileron was moved outboard by one rib, and the wing fuel tanks were enlarged accordingly to . The two 20 mm wing cannon were upgraded from the Type 99 Mark l to the Type 99 Mark II, which required a bulge in the sheet metal of the wing below each cannon. The wings also included larger ammunition boxes and thus allowing 100 rounds per cannon.\n\nThe Sakae 21 engine and other changes increased maximum speed by only compared to the Model 21, but sacrificed nearly of range. Nevertheless, the navy accepted the type and it entered production in April 1942.\n\nThe shorter wing span led to better roll, and the reduced drag allowed the diving speed to be increased to . On the downside, turning and range, which were the strengths of the Model 21, suffered due to smaller ailerons, decreased lift and greater fuel consumption. The shorter range proved a significant limitation during the Solomons Campaign, during which Zeros based at Rabaul had to travel nearly to their maximum range to reach Guadalcanal and return. Consequently, the Model 32 was unsuited to that campaign and was used mainly for shorter range offensive missions and interception.\n\nThe appearance of the redesigned A6M3-32 prompted the US to assign the Model 32 a new code name, \"Hap\". This name was short-lived, as a protest from USAAF Commanding General Henry \"Hap\" Arnold forced a change to \"Hamp\". Soon after, it was realized that it was simply a new model of the \"Zeke\" and was termed \"Zeke 32\".\n\nThis variant was flown by only a small number of units, and only 343 were built.\n\nIn order to correct the deficiencies of the Model 32, a new version with folding wingtips and redesigned wing was introduced. The fuel tanks were moved to the outer wings, fuel lines for a drop tank were installed under each wing and the internal fuel capacity was increased to . More important, it regained back its capabilities for long operating ranges, similar to the previous A6M2 Model 21, which was vastly shortened by the Model 32.\n\nHowever, before the new design type was accepted formally by the Navy, the A6M3 Model 22 already stood ready for service in December 1942. Approximately 560 aircraft of the new type had been produced in the meantime by Mitsubishi Jukogyo K.K.\n\nAccording to a theory, the very late production Model 22 might have had wings similar to the shortened, rounded-tip wing of the Model 52. One plane of such arrangement was photographed at Lakunai Airfield (\"Rabaul East\") in the second half of 1943, and has been published widely in a number of Japanese books. While the engine cowling is the same of previous Model 32 and 22, the theory proposes that the plane is an early production Model 52. \n\nThe Model 32, 22, 22 kou, 52, 52 kou and 52 otsu were all powered by the Nakajima (\"Sakae\") engine. That engine kept its designation in spite of changes in the exhaust system for the Model 52.\n\nMitsubishi is unable to state with certainty that it ever used the designation \"A6M4\" or model numbers for it. However, \"A6M4\" does appear in a translation of a captured Japanese memo from a Naval Air Technical Arsenal, titled Quarterly Report on Research Experiments, dated 1 October 1942. It mentions a \"cross-section of the A6M4 intercooler\" then being designed. Some researchers believe \"A6M4\" was applied to one or two prototype planes fitted with an experimental turbo-supercharged Sakae engine designed for high altitude. Mitsubishi's involvement in the project was probably quite limited or nil; the unmodified Sakae engine was made by Nakajima. The design and testing of the turbo-supercharger was the responsibility of the First Naval Air [Technical] Arsenal (, \"\") at Yokosuka. At least one photo of a prototype plane exists. It shows a turbo unit mounted in the forward left fuselage.\n\nLack of suitable alloys for use in the manufacture of a turbo-supercharger and its related ducting caused numerous ruptures, resulting in fires and poor performance. Consequently, further development of a turbo-supercharged A6M was cancelled. The lack of acceptance by the navy suggests that the navy did not bestow model number 41 or 42 formally, although it appears that the arsenal did use the designation \"A6M4\". The prototype engines nevertheless provided useful experience for future engine designs.\n\nSometimes considered as the most effective variant, the Model 52 was developed to again shorten the wings to increase speed and dispense with the folding wing mechanism. In addition, ailerons, aileron trim tab and flaps were revised. Produced first by Mitsubishi, most Model 52s were made by Nakajima. The prototype was made in June 1943 by modifying an A6M3 and was first flown in August 1943. The first Model 52 is said in the handling manual to have production number 3904, which apparently refers to the prototype.\n\nResearch by Mr. Bunzo Komine published by Mr. Kenji Miyazaki states that aircraft 3904 through 4103 had the same exhaust system and cowl flaps as on the Model 22. This is partially corroborated by two wrecks researched by Mr. Stan Gajda and Mr. L. G. Halls, production number 4007 and 4043, respectively. (The upper cowling was slightly redesigned from that of the Model 22.)\nAn early production A6M5 Zero with non separated exhaust, with an A6M3 Model 22 in the background.\nA new exhaust system provided an increment of thrust by aiming the stacks aft and distributing them around the forward fuselage. The new exhaust system required \"notched\" cowl flaps and heat shields just aft of the stacks. (Note, however, that the handling manual translation states that the new style of exhaust commenced with number 3904. Whether this is correct, indicates retrofitting intentions, refers to the prototype but not to all subsequent planes, or is in error is not clear.) From production number 4274, the wing fuel tanks received carbon dioxide fire extinguishers. From number 4354, the radio became the Model 3, aerial Mark 1, and at that point it is said the antenna mast was shortened slightly. Through production number 4550, the lowest exhaust stacks were approximately the same length as those immediately above them. This caused hot exhaust to burn the forward edge of the landing gear doors and heat the tires. Therefore, from number 4551 Mitsubishi began to install shorter bottom stacks. Nakajima manufactured the Model 52 at its Koizumi plant in Gunma Prefecture. The A6M5 had a maximum speed of ) at and reached that altitude in 7:01 minutes.\n\nSubsequent variants included:\n\nSome Model 21 and 52 aircraft were converted to \"bakusen\" (fighter-bombers) by mounting a bomb rack and bomb in place of the centerline drop tank.\n\nPerhaps seven Model 52 planes were ostensibly converted into A6M5-K two-seat trainers. Mass production was contemplated by Hitachi, but not undertaken.\n\nThis was similar to the A6M5c, but with self-sealing wing tanks and a Nakajima Sakae 31a engine featuring water-methanol engine boost.\n\nSimilar to the A6M6 but intended for attack or Kamikaze role.\n\nSimilar to the A6M6 but with the Sakae (now out of production) replaced by the Mitsubishi Kinsei 62 engine with , 60% more powerful than the engine of the A6M2. This resulted in an extensively modified cowling and nose for the aircraft. The carburetor intake was much larger, a long duct like that on the Nakajima B6N Tenzan was added, and a large spinner—like that on the Yokosuka D4Y Suisei with the Kinsei 62—was mounted. The larger cowling meant of the fuselage-mounted machine gun, armament change to 2x 13.2mm Type 3 machine gun in the fuselage and 4x 20mm Type99 Mark2 cannon (two on each wing). In addition, the Model 64 was modified to carry two drop tanks on either wing in order to permit the mounting of a bomb on the underside of the fuselage. Two prototypes were completed in April 1945 but the chaotic situation of Japanese industry and the end of the war obstructed the start of the ambitious program of production for 6,300 machines, none being completed.\n\nNot included:\n\n\n\nLike many surviving World War II Japanese aircraft, most surviving Zeros are made up of parts from multiple airframes. As a result, some are referred to by conflicting manufacturer serial numbers. In other cases, such as those recovered after decades in a wrecked condition, they have been reconstructed to the point that the majority of their structure is made up of modern parts. All of this means the identities of survivors can be difficult to confirm.\n\nMost flying Zeros have had their engines replaced with similar American units. Only one, the Planes of Fame Museum's A6M5, has the original Sakae engine.\n\nThe rarity of flyable Zeros accounts for the use of single-seat North American T-6 Texans, with heavily modified fuselages and painted in Japanese markings, as substitutes for Zeros in the films \"Tora! Tora! Tora!\", \"The Final Countdown\", and many other television and film depictions of the aircraft, such as \"Baa Baa Black Sheep\" (renamed \"Black Sheep Squadron\"). One Model 52 was used during the production of \"Pearl Harbor\".\n\n\n\n\n\n\n\n\n\n\n\n[[Category:Carrier-based aircraft|Mitsubishi A6M Zero]]\n[[Category:Japanese fighter aircraft 1930–1939]]\n[[Category:Mitsubishi aircraft]]\n[[Category:Attack on Pearl Harbor]]\n[[Category:World War II Japanese fighter aircraft]]\n[[Category:Articles containing video clips]]\n[[Category:Single-engined tractor aircraft]]\n[[Category:Low-wing aircraft]]\n[[Category:Aircraft first flown in 1939]]\n[[Category:Retractable conventional landing gear]]"}
{"id": "19624", "url": "https://en.wikipedia.org/wiki?curid=19624", "title": "May 27", "text": "May 27\n\n\n\n"}
{"id": "19626", "url": "https://en.wikipedia.org/wiki?curid=19626", "title": "Monasticism", "text": "Monasticism\n\nMonasticism (from Greek μοναχός, \"monachos\", derived from μόνος, \"monos\", 'alone') or monkhood is a religious way of life in which one renounces worldly pursuits to devote oneself fully to spiritual work. Monastic life plays an important role in many Christian churches, especially in the Catholic and Orthodox traditions. Similar forms of religious life also exist in other faiths, most notably in Buddhism, but also in Hinduism and Jainism, although the expressions differ considerably. By contrast, in other religions monasticism is criticized and not practiced, as in Islam and Zoroastrianism, or plays a marginal role, as in Judaism.\n\nWomen pursuing a monastic life are generally called \"nuns\", while monastic men are called \"monks\". More recently, both have described themselves simply as “monastics.”\n\nMany monastics live in monasteries to stay away from the secular world. The way of addressing monastics differs between the Christian traditions. As a general rule, in Roman Catholicism, monks and nuns are called brother or sister, while in Eastern Orthodoxy, they are called father or mother.\n\nThe Sangha or community of ordained Buddhist bhikkhus (\"beggar\" or \"one who lives by alms\".) and original bhikkhunis (nuns) was founded by Gautama Buddha during his lifetime over 2500 years ago. This communal monastic lifestyle grew out of the lifestyle of earlier sects of wandering ascetics, some of whom the Buddha had studied under. It was initially fairly eremitic or reclusive in nature. Bhikkhus and bhikkunis were expected to live with a minimum of possessions, which were to be voluntarily provided by the lay community. Lay followers also provided the daily food that bhikkhus required, and provided shelter for bhikkhus when they needed it.\nAfter the Parinibbana (Final Passing) of the Buddha, the Buddhist monastic order developed into a primarily cenobitic or communal movement. The practice of living communally during the rainy vassa season, prescribed by the Buddha, gradually grew to encompass a settled monastic life centered on life in a community of practitioners. Most of the modern disciplinary rules followed by bhikkhus and bhikkhunis — as encoded in the Patimokkha — relate to such an existence, prescribing in great detail proper methods for living and relating in a community of bhikkhus or bhikkhunis. The number of rules observed varies with the order; Theravada bhikkhus follow around 227 rules, the Vinaya. There are a larger number of rules specified for bhikkhunis (nuns).\n\nThe Buddhist monastic order consists of the male bhikkhu assembly and the female bhikkhuni assembly. Initially consisting only of males, it grew to include females after the Buddha's stepmother, Mahaprajapati, asked for and received permission to live as an ordained practitioner.\n\nBhikkhus and bhikkhunis are expected to fulfill a variety of roles in the Buddhist community. First and foremost, they are expected to preserve the doctrine and discipline now known as Buddhism. They are also expected to provide a living example for the laity, and to serve as a \"field of merit\" for lay followers—providing laymen and women with the opportunity to earn merit by giving gifts and support to the bhikkhus. In return for the support of the laity, bhikkhus and bhikkhunis are expected to live an austere life focused on the study of Buddhist doctrine, the practice of meditation, and the observance of good moral character.\n\nA bhikkhu (the term in the Pali language) or Bhikshu (in Sanskrit), first ordains as a \"Samanera\" (novice). Novices often ordain at a young age, but generally no younger than eight. Samaneras live according to the Ten Precepts, but are not responsible for living by the full set of monastic rules. Higher ordination, conferring the status of a full Bhikkhu, is given only to men who are aged 20 or older. Bhikkhunis follow a similar progression, but are required to live as Samaneras for longer periods of time- typically five years.\n\nThe disciplinary regulations for bhikkhus and bhikkhunis are intended to create a life that is simple and focused, rather than one of deprivation or severe asceticism. However, celibacy is a fundamental part of this form of monastic discipline.\n\nMonasticism in Christianity, which provides the origins of the words \"monk\" and \"monastery\", comprises several diverse forms of religious living. It began to develop early in the history of the Church, but is not mentioned in the scriptures. It has come to be regulated by religious rules (e.g. the Rule of St Basil, the Rule of St Benedict) and, in modern times, the Church law of the respective apostolic Christian churches that have forms of monastic living.\n\nThe Christian monk embraces the monastic life as a vocation for God. His goal is to attain eternal life in his presence. The rules of monastic life are codified in the \"counsels of perfection\".\n\nIn the beginning, in Egypt, Christians felt called to a more reclusive or eremitic form of monastic living (in the spirit of the \"Desert Theology\" for the purpose of spiritual renewal and return to God). Saint Anthony the Great is cited by Athanasius as one of these early \"Hermit monks\". Especially in the Middle East, eremitic monasticism continued to be common until the decline of Syriac Christianity in the late Middle Ages.\n\nThe need for some form of organized spiritual guidance was obvious; and around 318 Saint Pachomius started to organize his many followers in what was to become the first Christian cenobitic or communal monastery. Soon, similar institutions were established throughout the Egyptian desert as well as the rest of the eastern half of the Roman Empire. Notable monasteries of the East include:\n\nIn the West, the most significant development occurred when the rules for monastic communities were written, the Rule of St Basil being credited with having been the first. The precise dating of the Rule of the Master is problematic; but it has been argued on internal grounds that it antedates the so-called Rule of Saint Benedict created by Benedict of Nursia for his monastery in Monte Cassino, Italy (c. 529), and the other Benedictine monasteries he himself had founded (cf. Order of St Benedict). It would become the most common rule throughout the Middle Ages and is still in use today. The Augustinian Rule, due to its brevity, has been adopted by various communities, chiefly the Canons Regular. Around the 12th century, the Franciscan, Carmelite, Dominican, Servite Order (see Servants of Mary) and Augustinian mendicant orders chose to live in city convents among the people instead of being secluded in monasteries. St. Augustine's Monastery, founded in 1277 in Erfurt, Germany is regarded by many historians and theologians as the \"cradle of the Reformation\", as it is where Martin Luther lived as a monk from 1505-1511.\n\nToday new expressions of Christian monasticism, many of which are ecumenical, are developing in various places such as the Bose Monastic Community in Italy, the Monastic Fraternities of Jerusalem throughout Europe, the New Skete, the Anglo-Celtic Society of Nativitists, the Taizé Community in France, and the mainly Evangelical Protestant New Monasticism. It is possible that intentional communities such as the Bruderhof could be considered monastic, since they share everything, have a rhythm of life and prayer, and have a degree of separation from the world. Rod Dreher, editor of The American Conservative, said of the Bruderhof, \"It would not be stretching it to call them lay monastics\". \n\nIn their quest to attain the spiritual goal of life, some Hindus choose the path of monasticism (Sannyasa). Monastics commit themselves to a life of simplicity, celibacy, detachment from worldly pursuits, and the contemplation of God. A Hindu monk is called a s\"anyāsī, sādhu\", or \"swāmi\". A nun is called a \"sanyāsini\", \"sādhvi\", or \"swāmini\". Such renunciates are accorded high respect in Hindu society, because their outward renunciation of selfishness and worldliness serves as an inspiration to householders who strive for \"mental\" renunciation. Some monastics live in monasteries, while others wander from place to place, trusting in God alone to provide for their physical needs. It is considered a highly meritorious act for a lay devotee to provide sadhus with food or other necessaries. Sādhus are expected to treat all with respect and compassion, whether a person may be poor or rich, good or wicked. They are also expected to be indifferent to praise, blame, pleasure, and pain. A sādhu can typically be recognized by his ochre-colored clothing. Generally, Vaisnava monks shave their heads except for a small patch of hair on the back of the head, while Saivite monks let their hair and beard grow uncut.\n\nA \"sadhu's\" vow of renunciation typically forbids him from:\n\nIslam forbids the practice of monasticism and is critical of its practice. In Sunni Islam, one example is Uthman bin Maz'oon; one of the companions of Muhammad. He was married to Khawlah bint Hakim, both being two of the earliest converts to Islam. There is a Sunni narration that, out of religious devotion, Uthman bin Maz'oon decided to dedicate himself to night prayers and take a vow of chastity from his wife. His wife got upset and spoke to Muhammad about this. Muhammad reminded Uthman that he himself, as the Prophet, also had a family life, and that Uthman had a responsibility to his family and should not adopt monasticism as a form of religious practice.\n\nMuhammad told his companions to ease their burden and avoid excess. According to some Sunni hadiths, in a message to some companions who wanted to put an end to their sexual life, pray all night long or fast continuously, Muhammad said: “Do not do that! Fast on some days and eat on others. Sleep part of the night, and stand in prayer another part. For your body has rights upon you, your eyes have a right upon you, your wife has a right upon you, your guest has a right upon you.” Muhammad once exclaimed, repeating it three times: “Woe to those who exaggerate [who are too strict]!” And, on another occasion, Muhammad said: “Moderation, moderation! For only with moderation will you succeed.”\n\nMonasticism is also mentioned in four places in the following verses of Qur'an:\n\nThey have taken as lords beside Allah their rabbis and their monks and the Messiah son of Mary, when they were bidden to worship only One God. There is no god save Him. Be He glorified from all that they ascribe as partner (unto Him)!\n\nO ye who believe! Lo! many of the (Jewish) rabbis and the (Christian) monks devour the wealth of mankind wantonly and debar (men) from the way of Allah. They who hoard up gold and silver and spend it not in the way of Allah, unto them give tidings (O Muhammad) of a painful doom\n\nThou wilt find the most vehement of mankind in hostility to those who believe (to be) the Jews and the idolaters. And thou wilt find the nearest of them in affection to those who believe (to be) those who say: Lo! We are Christians. That is because there are among them priests and monks, and because they are not proud.\n\nIn Jainism, monasticism is encouraged and respected. Rules for monasticism are rather strict. A Jain ascetic has neither a permanent home nor any possessions, wandering barefoot from place to place except during the months of Chaturmas. The quality of life they lead is difficult because of the many constraints placed on them. They don't use a vehicle for commuting and always commute barefoot from one place to another, irrespective of the distance. They don't possess any materialistic things and also don't use the basic services like that of a phone, electricity etc. They don't prepare food and live only on what people offer them.\n\nJudaism does not encourage the monastic ideal of celibacy and poverty. To the contrary—all of the Torah's Commandments are a means of sanctifying the physical world. As further disseminated through the teachings of the Yisrael Ba'al Shem Tov, the pursuit of permitted physical pleasures is encouraged as a means to \"serve God with joy\" (Deut. 28:47).\n\nHowever, until the Destruction of the Second Temple, about two thousand years ago, taking Nazirite vows was a common feature of the religion. Nazirite Jews (in Hebrew: נזיר) abstained from grape products, haircuts, and contact with the dead. However, they did not withdraw from general society, and they were permitted to marry and own property; moreover, in most cases a Nazirite vow was for a specified time period and not permanent. In Modern Hebrew, the term \"Nazir\" is most often used to refer to non-Jewish monastics.\n\nUnique among Jewish communities is the monasticism of the Beta Israel of Ethiopia, a practice believed to date to the 15th century.\n\nA form of asceticism was practiced by some individuals in pre–World War II European Jewish communities. Its principal expression was \"prishut\", the practice of a married Talmud student going into self-imposed exile from his home and family to study in the kollel of a different city or town. This practice was associated with, but not exclusive to, the Perushim.\n\nThe Essenes (in Modern but not in Ancient Hebrew: , \"Isiyim\"; Greek: Εσσηνοι, Εσσαιοι, or Οσσαιοι; \"Essēnoi\", \"Essaioi\", or \"Ossaioi\") were a Jewish sect that flourished from the 2nd century BC to AD 100 which some scholars claim seceded from the Zadokite priests. Being much fewer in number than the Pharisees and the Sadducees (the other two major sects at the time), the Essenes lived in various cities but congregated in communal life dedicated to asceticism, voluntary poverty, daily immersion (in mikvah), and abstinence from worldly pleasures, including (for some groups) marriage. Many separate but related religious groups of that era shared similar mystic, eschatological, messianic, and ascetic beliefs. These groups are collectively referred to by various scholars as the \"Essenes\". Josephus records that Essenes existed in large numbers, and thousands lived throughout Roman Judaea.\n\nThe Essenes have gained fame in modern times as a result of the discovery of an extensive group of religious documents known as the Dead Sea Scrolls, which are commonly believed to be the Essenes' library—although there is no proof that the Essenes wrote them. These documents include multiple preserved copies of the Hebrew Bible which were untouched from as early as 300 years before Christ until their discovery in 1946. Some scholars, however, dispute the notion that the Essenes wrote the Dead Sea Scrolls. Rachel Elior, a prominent Israeli scholar, even questions the existence of the Essenes.\n\nTaoism is considered to have originally taken up the idea of monasticism under the influence of Buddhism, but has throughout the centuries developed its own extensive monastic traditions and practices. Particularly well known is the White Cloud Monastery in Beijing, which houses a rare complete copy of the \"Daozang\", a major Taoist Scripture \n\n\n\n\n"}
{"id": "19629", "url": "https://en.wikipedia.org/wiki?curid=19629", "title": "May 10", "text": "May 10\n\n\n\n"}
{"id": "19631", "url": "https://en.wikipedia.org/wiki?curid=19631", "title": "May 17", "text": "May 17\n\n\n\n"}
{"id": "19632", "url": "https://en.wikipedia.org/wiki?curid=19632", "title": "May 19", "text": "May 19\n\n\n"}
{"id": "19633", "url": "https://en.wikipedia.org/wiki?curid=19633", "title": "March 3", "text": "March 3\n\n\n\n"}
{"id": "19635", "url": "https://en.wikipedia.org/wiki?curid=19635", "title": "March 15", "text": "March 15\n\nIn the Roman calendar, March 15 was known as the Ides of March.\n\n\n"}
{"id": "19636", "url": "https://en.wikipedia.org/wiki?curid=19636", "title": "Mathematical logic", "text": "Mathematical logic\n\nMathematical logic is a subfield of mathematics exploring the applications of formal logic to mathematics. It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science. The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.\nMathematical logic is often divided into the fields of set theory, model theory, recursion theory, and proof theory. These areas share basic results on logic, particularly first-order logic, and definability. In computer science (particularly in the ACM Classification) mathematical logic encompasses additional topics not detailed in this article; see Logic in computer science for those.\n\nSince its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.\n\nThe \"Handbook of Mathematical Logic\" makes a rough division of contemporary mathematical logic into four areas:\nEach area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp. Gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb's theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.\n\nThe mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.\n\nMathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics (Ferreirós 2001, p. 443). \"Mathematical logic, also called 'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the last [nineteenth] century with the aid of an artificial notation and a rigorously deductive method.\" Before this emergence, logic was studied with rhetoric, with \"calculationes\", through the syllogism, and with philosophy. The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.\n\nTheories of logic were developed in many cultures in history, including China, India, Greece and the Islamic world. In 18th-century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including Leibniz and Lambert, but their labors remained isolated and little known.\n\nIn the middle of the nineteenth century, George Boole and then Augustus De Morgan presented systematic mathematical treatments of logic. Their work, building on work by algebraists such as George Peacock, extended the traditional Aristotelian doctrine of logic into a sufficient framework for the study of foundations of mathematics (Katz 1998, p. 686).\n\nCharles Sanders Peirce built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.\nGottlob Frege presented an independent development of logic with quantifiers in his \"Begriffsschrift\", published in 1879, a work generally considered as marking a turning point in the history of logic. Frege's work remained obscure, however, until Bertrand Russell began to promote it near the turn of the century. The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.\n\nFrom 1890 to 1905, Ernst Schröder published \"Vorlesungen über die Algebra der Logik\" in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.\n\nConcerns that mathematics had not been built on a proper foundation led to the development of axiomatic systems for fundamental areas of mathematics such as arithmetic, analysis, and geometry.\nIn logic, the term \"arithmetic\" refers to the theory of the natural numbers. Giuseppe Peano (1889) published a set of axioms for arithmetic that came to bear his name (Peano axioms), using a variation of the logical system of Boole and Schröder but adding quantifiers. Peano was unaware of Frege's work at the time. Around the same time Richard Dedekind showed that the natural numbers are uniquely characterized by their induction properties. Dedekind (1888) proposed a different characterization, which lacked the formal logical character of Peano's axioms. Dedekind's work, however, proved theorems inaccessible in Peano's system, including the uniqueness of the set of natural numbers (up to isomorphism) and the recursive definitions of addition and multiplication from the successor function and mathematical induction.\nIn the mid-19th century, flaws in Euclid's axioms for geometry became known (Katz 1998, p. 774). In addition to the independence of the parallel postulate, established by Nikolai Lobachevsky in 1826 (Lobachevsky 1840), mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms. Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect. Hilbert (1899) developed a complete set of axioms for geometry, building on previous work by Pasch (1882). The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the real line. This would prove to be a major area of research in the first half of the 20th century.\n\nThe 19th century saw great advances in the theory of real analysis, including theories of convergence of functions and Fourier series. Mathematicians such as Karl Weierstrass began to construct functions that stretched intuition, such as nowhere-differentiable continuous functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate. Weierstrass began to advocate the arithmetization of analysis, which sought to axiomatize analysis using properties of the natural numbers. The modern (ε, δ)-definition of limit and continuous functions was already developed by Bolzano in 1817 (Felscher 2000), but remained relatively unknown.\nCauchy in 1821 defined continuity in terms of infinitesimals (see Cours d'Analyse, page 34). In 1858, Dedekind proposed a definition of the real numbers in terms of Dedekind cuts of rational numbers (Dedekind 1872), a definition still employed in contemporary texts.\n\nGeorg Cantor developed the fundamental concepts of infinite set theory. His early results developed the theory of cardinality and proved that the reals and the natural numbers have different cardinalities (Cantor 1874). Over the next twenty years, Cantor developed a theory of transfinite numbers in a series of publications. In 1891, he published a new proof of the uncountability of the real numbers that introduced the diagonal argument, and used this method to prove Cantor's theorem that no set can have the same cardinality as its powerset. Cantor believed that every set could be well-ordered, but was unable to produce a proof for this result, leaving it as an open problem in 1895 (Katz 1998, p. 807).\n\nIn the early decades of the 20th century, the main areas of study were set theory and formal logic. The discovery of paradoxes in informal set theory caused some to wonder whether mathematics itself is inconsistent, and to look for proofs of consistency.\n\nIn 1900, Hilbert posed a famous list of 23 problems for the next century. The first two of these were to resolve the continuum hypothesis and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the integers has a solution. Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert's \"Entscheidungsproblem\", posed in 1928. This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.\n\nErnst Zermelo (1904) gave a proof that every set could be well-ordered, a result Georg Cantor had been unable to obtain. To achieve the proof, Zermelo introduced the axiom of choice, which drew heated debate and research among mathematicians and the pioneers of set theory. The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof (Zermelo 1908a). This paper led to the general acceptance of the axiom of choice in the mathematics community.\n\nSkepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory. Cesare Burali-Forti (1897) was the first to state a paradox: the Burali-Forti paradox shows that the collection of all ordinal numbers cannot form a set. Very soon thereafter, Bertrand Russell discovered Russell's paradox in 1901, and Jules Richard (1905) discovered Richard's paradox.\n\nZermelo (1908b) provided the first set of axioms for set theory. These axioms, together with the additional axiom of replacement proposed by Abraham Fraenkel, are now called Zermelo–Fraenkel set theory (ZF). Zermelo's axioms incorporated the principle of limitation of size to avoid Russell's paradox.\n\nIn 1910, the first volume of \"Principia Mathematica\" by Russell and Alfred North Whitehead was published. This seminal work developed the theory of functions and cardinality in a completely formal framework of type theory, which Russell and Whitehead developed in an effort to avoid the paradoxes. \"Principia Mathematica\" is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics (Ferreirós 2001, p. 445).\n\nFraenkel (1922) proved that the axiom of choice cannot be proved from the axioms of Zermelo's set theory with urelements. Later work by Paul Cohen (1966) showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF. Cohen's proof developed the method of forcing, which is now an important tool for establishing independence results in set theory.\n\nLeopold Löwenheim (1915) and Thoralf Skolem (1920) obtained the Löwenheim–Skolem theorem, which says that first-order logic cannot control the cardinalities of infinite structures. Skolem realized that this theorem would apply to first-order formalizations of set theory, and that it implies any such formalization has a countable model. This counterintuitive fact became known as Skolem's paradox.\n\nIn his doctoral thesis, Kurt Gödel (1929) proved the completeness theorem, which establishes a correspondence between syntax and semantics in first-order logic. Gödel used the completeness theorem to prove the compactness theorem, demonstrating the finitary nature of first-order logical consequence. These results helped establish first-order logic as the dominant logic used by mathematicians.\n\nIn 1931, Gödel published \"On Formally Undecidable Propositions of Principia Mathematica and Related Systems\", which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories. This result, known as Gödel's incompleteness theorem, establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert's program. It showed the impossibility of providing a consistency proof of arithmetic within any formal theory of arithmetic. Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.\n\nGödel's theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system. This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider. Gentzen (1936) proved the consistency of arithmetic using a finitistic system together with a principle of transfinite induction. Gentzen's result introduced the ideas of cut elimination and proof-theoretic ordinals, which became key tools in proof theory. Gödel (1958) gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intuitionistic arithmetic in higher types.\n\nAlfred Tarski developed the basics of model theory.\n\nBeginning in 1935, a group of prominent mathematicians collaborated under the pseudonym Nicolas Bourbaki to publish a series of encyclopedic mathematics texts. These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations. Terminology coined by these texts, such as the words \"bijection\", \"injection\", and \"surjection\", and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.\n\nThe study of computability came to be known as recursion theory, because early formalizations by Gödel and Kleene relied on recursive definitions of functions. When these definitions were shown equivalent to Turing's formalization involving Turing machines, it became clear that a new concept – the computable function – had been discovered, and that this definition was robust enough to admit numerous independent characterizations. In his work on the incompleteness theorems in 1931, Gödel lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.\n\nNumerous results in recursion theory were obtained in the 1940s by Stephen Cole Kleene and Emil Leon Post. Kleene (1943) introduced the concepts of relative computability, foreshadowed by Turing (1939), and the arithmetical hierarchy. Kleene later generalized recursion theory to higher-order functionals. Kleene and Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory.\n\nAt its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language. The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties. Stronger classical logics such as second-order logic or infinitary logic are also studied, along with nonclassical logics such as intuitionistic logic.\n\nFirst-order logic is a particular formal system of logic. Its syntax involves only finite expressions as well-formed formulas, while its semantics are characterized by the limitation of all quantifiers to a fixed domain of discourse.\n\nEarly results from formal logic established limitations of first-order logic. The Löwenheim–Skolem theorem (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality. This shows that it is impossible for a set of first-order axioms to characterize the natural numbers, the real numbers, or any other infinite structure up to isomorphism. As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.\n\nGödel's completeness theorem (Gödel 1929) established the equivalence between semantic and syntactic definitions of logical consequence in first-order logic. It shows that if a particular sentence is true in every model that satisfies a particular set of axioms, then there must be a finite deduction of the sentence from the axioms. The compactness theorem first appeared as a lemma in Gödel's proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely. It says that a set of sentences has a model if and only if every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset. The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of model theory, and they are a key reason for the prominence of first-order logic in mathematics.\n\nGödel's incompleteness theorems (Gödel 1931) establish additional limits on first-order axiomatizations. The first incompleteness theorem states that for any consistent, effectively given (defined below) logical system that is capable of interpreting arithmetic, there exists a statement that is true (in the sense that it holds for the natural numbers) but not provable within that logical system (and which indeed may fail in some non-standard models of arithmetic which may be consistent with the logical system). For example, in every logical system capable of expressing the Peano axioms, the Gödel sentence holds for the natural numbers but cannot be proved.\n\nHere a logical system is said to be effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom, and one which can express the Peano axioms is called \"sufficiently strong.\" When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not elementarily equivalent, a stronger limitation than the one established by the Löwenheim–Skolem theorem. The second incompleteness theorem states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert's program cannot be completed.\n\nMany logics besides first-order logic are studied. These include infinitary logics, which allow for formulas to provide an infinite amount of information, and higher-order logics, which include a portion of set theory directly in their semantics.\n\nThe most well studied infinitary logic is formula_1. In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them. Thus, for example, it is possible to say that an object is a whole number using a formula of formula_1 such as\n\nHigher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type. The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type. The logics studied before the development of first-order logic, for example Frege's logic, had similar set-theoretic aspects. Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.\n\nAnother type of logics are s that allow inductive definitions, like one writes for primitive recursive functions.\n\nOne can formally define an extension of first-order logic — a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g. it does not encompass intuitionistic, modal or fuzzy logic. Lindström's theorem implies that the only extension of first-order logic satisfying both the compactness theorem and the Downward Löwenheim–Skolem theorem is first-order logic.\n\nModal logics include additional modal operators, such as an operator which states that a particular formula is not only true, but necessarily true. Although modal logic is not often used to axiomatize mathematics, it has been used to study the properties of first-order provability (Solovay 1976) and set-theoretic forcing (Hamkins and Löwe 2007).\n\nIntuitionistic logic was developed by Heyting to study Brouwer's program of intuitionism, in which Brouwer himself avoided formalization. Intuitionistic logic specifically does not include the law of the excluded middle, which states that each sentence is either true or its negation is true. Kleene's work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs. For example, any provably total function in intuitionistic arithmetic is computable; this is not true in classical theories of arithmetic such as Peano arithmetic.\n\nAlgebraic logic uses the methods of abstract algebra to study the semantics of formal logics. A fundamental example is the use of Boolean algebras to represent truth values in classical propositional logic, and the use of Heyting algebras to represent truth values in intuitionistic propositional logic. Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as cylindric algebras.\n\nSet theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo (1908b), was extended slightly to become Zermelo–Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.\n\nOther formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF). Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of Kripke–Platek set theory is closely related to generalized recursion theory.\n\nTwo famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo (1904), was proved independent of ZF by Fraenkel (1922), but has come to be widely accepted by mathematicians. It states that given a collection of nonempty sets there is a single set \"C\" that contains exactly one element from each set in the collection. The set \"C\" is said to \"choose\" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. Stefan Banach and Alfred Tarski (1924) showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size. This theorem, known as the Banach–Tarski paradox, is one of many counterintuitive results of the axiom of choice.\n\nThe continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold. In 1963, Paul Cohen showed that the continuum hypothesis cannot be proven from the axioms of Zermelo–Fraenkel set theory (Cohen 1966). This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear (Woodin 2001).\n\nContemporary research in set theory includes the study of large cardinals and determinacy. Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC. Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line. \"Determinacy\" refers to the possible existence of winning strategies for certain two-player games (the games are said to be \"determined\"). The existence of these strategies implies structural properties of the real line and other Polish spaces.\n\nModel theory studies the models of various formal theories. Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory. Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.\n\nThe set of all models of a particular theory is called an elementary class; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.\n\nThe method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated. Tarski (1948) established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable. (He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic.) A modern subfield developing from this is concerned with o-minimal structures.\n\nMorley's categoricity theorem, proved by Michael D. Morley (1965), states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.\n\nA trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis. Many special cases of this conjecture have been established.\n\nRecursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability. Recursion theory also includes the study of generalized computability and definability. Recursion theory grew from the work of Rózsa Péter, Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.\n\nClassical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, λ calculus, and other systems. More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.\n\nGeneralized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and α-recursion theory.\n\nContemporary research in recursion theory includes the study of applications such as algorithmic randomness, computable model theory, and reverse mathematics, as well as new results in pure recursion theory.\n\nAn important subfield of recursion theory studies algorithmic unsolvability; a decision problem or function problem is algorithmically unsolvable if there is no possible computable algorithm that returns the correct answer for all legal inputs to the problem. The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the Entscheidungsproblem is algorithmically unsolvable. Turing proved this by establishing the unsolvability of the halting problem, a result with far-ranging implications in both recursion theory and computer science.\n\nThere are many known examples of undecidable problems from ordinary mathematics. The word problem for groups was proved algorithmically unsolvable by Pyotr Novikov in 1955 and independently by W. Boone in 1959. The busy beaver problem, developed by Tibor Radó in 1962, is another well-known example.\n\nHilbert's tenth problem asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers. Partial progress was made by Julia Robinson, Martin Davis and Hilary Putnam. The algorithmic unsolvability of the problem was proved by Yuri Matiyasevich in 1970 (Davis 1973).\n\nProof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques. Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.\n\nThe study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems. An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods (Weyl 1918).\n\nBecause proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability. The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest. Results such as the Gödel–Gentzen negative translation show that it is possible to embed (or \"translate\") classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.\n\nRecent developments in proof theory include the study of proof mining by Ulrich Kohlenbach and the study of proof-theoretic ordinals by Michael Rathjen.\n\n\"Mathematical logic has been successfully applied not only to mathematics and its foundations (G. Frege, B. Russell, D. Hilbert, P. Bernays, H. Scholz, R. Carnap, S. Lesniewski, T. Skolem), but also to physics (R. Carnap, A. Dittrich, B. Russell, C. E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology (J. H. Woodger, A. Tarski), to psychology (F. B. Fitch, C. G. Hempel), to law and morals (K. Menger, U. Klug, P. Oppenheim), to economics (J. Neumann, O. Morgenstern), to practical questions (E. C. Berkeley, E. Stamm), and even to metaphysics (J. [Jan] Salamucha, H. Scholz, J. M. Bochenski). Its applications to the history of logic have proven extremely fruitful (J. Lukasiewicz, H. Scholz, B. Mates, A. Becker, E. Moody, J. Salamucha, K. Duerr, Z. Jordan, P. Boehner, J. M. Bochenski, S. [Stanislaw] T. Schayer, D. Ingalls).\" \"Applications have also been made to theology (F. Drewnowski, J. Salamucha, I. Thomas).\"\n\nThe study of computability theory in computer science is closely related to the study of computability in mathematical logic. There is a difference of emphasis, however. Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.\n\nThe theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry–Howard isomorphism between proofs and programs relates to proof theory, especially intuitionistic logic. Formal calculi such as the lambda calculus and combinatory logic are now studied as idealized programming languages.\n\nComputer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as automated theorem proving and logic programming.\n\nDescriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.\n\nIn the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass' nowhere-differentiable continuous function were discovered.\n\nCantor's study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated \"God made the integers; all else is the work of man,\" endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. David Hilbert argued in favor of the study of the infinite, saying \"No one shall expel us from the Paradise that Cantor has created.\"\n\nMathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs. In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, non-Euclidean geometry can be proved consistent by defining \"point\" to mean a point on a fixed sphere and \"line\" to mean a great circle on the sphere. The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.\n\nWith the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term \"finitary\" to refer to the methods he would allow but not precisely defining them. This project, known as Hilbert's program, was seriously affected by Gödel's incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.\n\nA second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of \"constructive\". At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. \n\nIn the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a philosophy of mathematics. This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to \"intuit\" the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true. Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.\n\n\n\n\n\n\n\n"}
{"id": "19637", "url": "https://en.wikipedia.org/wiki?curid=19637", "title": "Molecular nanotechnology", "text": "Molecular nanotechnology\n\nMolecular nanotechnology (MNT) is a technology based on the ability to build structures to complex, atomic specifications by means of mechanosynthesis. This is distinct from nanoscale materials. Based on Richard Feynman's vision of miniature factories using nanomachines to build complex products (including additional nanomachines), this advanced form of nanotechnology (or \"molecular manufacturing\") would make use of positionally-controlled mechanosynthesis guided by molecular machine systems. MNT would involve combining physical principles demonstrated by biophysics, chemistry, other nanotechnologies, and the molecular machinery of life with the systems engineering principles found in modern macroscale factories.\n\nWhile conventional chemistry uses inexact processes obtaining inexact results, and biology exploits inexact processes to obtain definitive results, molecular nanotechnology would employ original definitive processes to obtain definitive results. The desire in molecular nanotechnology would be to balance molecular reactions in positionally-controlled locations and orientations to obtain desired chemical reactions, and then to build systems by further assembling the products of these reactions.\n\nA roadmap for the development of MNT is an objective of a broadly based technology project led by Battelle (the manager of several U.S. National Laboratories) and the Foresight Institute. The roadmap was originally scheduled for completion by late 2006, but was released in January 2008. The Nanofactory Collaboration is a more focused ongoing effort involving 23 researchers from 10 organizations and 4 countries that is developing a practical research agenda specifically aimed at positionally-controlled diamond mechanosynthesis and diamondoid nanofactory development. In August 2005, a task force consisting of 50+ international experts from various fields was organized by the Center for Responsible Nanotechnology to study the societal implications of molecular nanotechnology.\n\nOne proposed application of MNT is so-called smart materials. This term refers to any sort of material designed and engineered at the nanometer scale for a specific task. It encompasses a wide variety of possible commercial applications. One example would be materials designed to respond differently to various molecules; such a capability could lead, for example, to artificial drugs which would recognize and render inert specific viruses. Another is the idea of self-healing structures, which would repair small tears in a surface naturally in the same way as self-sealing tires or human skin.\n\nA MNT nanosensor would resemble a smart material, involving a small component within a larger machine that would react to its environment and change in some fundamental, intentional way. A very simple example: a photosensor might passively measure the incident light and discharge its absorbed energy as electricity when the light passes above or below a specified threshold, sending a signal to a larger machine. Such a sensor would supposedly cost less and use less power than a conventional sensor, and yet function usefully in all the same applications — for example, turning on parking lot lights when it gets dark.\n\nWhile smart materials and nanosensors both exemplify useful applications of MNT, they pale in comparison with the complexity of the technology most popularly associated with the term: the replicating nanorobot.\n\nMNT nanofacturing is popularly linked with the idea of swarms of coordinated nanoscale robots working together, a popularization of an early proposal by K. Eric Drexler in his 1986 discussions of MNT, but superseded in 1992. In this early proposal, sufficiently capable nanorobots would construct more nanorobots in an artificial environment containing special molecular building blocks.\n\nCritics have doubted both the feasibility of self-replicating nanorobots and the feasibility of control if self-replicating nanorobots could be achieved: they cite the possibility of mutations removing any control and favoring reproduction of mutant pathogenic variations. Advocates address the first doubt by pointing out that the first macroscale autonomous machine replicator, made of Lego blocks, was built and operated experimentally in 2002. While there are sensory advantages present at the macroscale compared to the limited sensorium available at the nanoscale, proposals for positionally controlled nanoscale mechanosynthetic fabrication systems employ dead reckoning of tooltips combined with reliable reaction sequence design to ensure reliable results, hence a limited sensorium is no handicap; similar considerations apply to the positional assembly of small nanoparts. Advocates address the second doubt by arguing that bacteria are (of necessity) evolved to evolve, while nanorobot mutation could be actively prevented by common error-correcting techniques. Similar ideas are advocated in the Foresight Guidelines on Molecular Nanotechnology, and a map of the 137-dimensional replicator design space recently published by Freitas and Merkle provides numerous proposed methods by which replicators could, in principle, be safely controlled by good design.\n\nHowever, the concept of suppressing mutation raises the question: How can design evolution occur at the nanoscale without a process of random mutation and deterministic selection? Critics argue that MNT advocates have not provided a substitute for such a process of evolution in this nanoscale arena where conventional sensory-based selection processes are lacking. The limits of the sensorium available at the nanoscale could make it difficult or impossible to winnow successes from failures. Advocates argue that design evolution should occur deterministically and strictly under human control, using the conventional engineering paradigm of modeling, design, prototyping, testing, analysis, and redesign.\n\nIn any event, since 1992 technical proposals for MNT do not include self-replicating nanorobots, and recent ethical guidelines put forth by MNT advocates prohibit unconstrained self-replication.\n\nOne of the most important applications of MNT would be medical nanorobotics or nanomedicine, an area pioneered by Robert Freitas in numerous books and papers. The ability to design, build, and deploy large numbers of medical nanorobots would, at a minimum, make possible the rapid elimination of disease and the reliable and relatively painless recovery from physical trauma. Medical nanorobots might also make possible the convenient correction of genetic defects, and help to ensure a greatly expanded lifespan. More controversially, medical nanorobots might be used to augment natural human capabilities. One study has reported on the conditions like tumors, arteriosclerosis, blood clots leading to stroke, accumulation of scar tissue and localized pockets of infection can be possibly be addressed by employing medical nanorobots.\n\nAnother proposed application of molecular nanotechnology is \"utility fog\" — in which a cloud of networked microscopic robots (simpler than assemblers) would change its shape and properties to form macroscopic objects and tools in accordance with software commands. Rather than modify the current practices of consuming material goods in different forms, utility fog would simply replace many physical objects.\n\nYet another proposed application of MNT would be phased-array optics (PAO). However, this appears to be a problem addressable by ordinary nanoscale technology. PAO would use the principle of phased-array millimeter technology but at optical wavelengths. This would permit the duplication of any sort of optical effect but virtually. Users could request holograms, sunrises and sunsets, or floating lasers as the mood strikes. PAO systems were described in BC Crandall's \"Nanotechnology: Molecular Speculations on Global Abundance\" in the Brian Wowk article \"Phased-Array Optics.\"\n\nMolecular manufacturing is a potential future subfield of nanotechnology that would make it possible to build complex structures at atomic precision. Molecular manufacturing requires significant advances in nanotechnology, but once achieved could produce highly advanced products at low costs and in large quantities in nanofactories weighing a kilogram or more. When nanofactories gain the ability to produce other nanofactories production may only be limited by relatively abundant factors such as input materials, energy and software.\n\nThe products of molecular manufacturing could range from cheaper, mass-produced versions of known high-tech products to novel products with added capabilities in many areas of application. Some applications that have been suggested are advanced smart materials, nanosensors, medical nanorobots and space travel. Additionally, molecular manufacturing could be used to cheaply produce highly advanced, durable weapons, which is an area of special concern regarding the impact of nanotechnology. Being equipped with compact computers and motors these could be increasingly autonomous and have a large range of capabilities.\n\nAccording to Chris Phoenix and Mike Treder from the Center for Responsible Nanotechnology as well as Anders Sandberg from the Future of Humanity Institute molecular manufacturing is the application of nanotechnology that poses the most significant global catastrophic risk. Several nanotechnology researchers state that the bulk of risk from nanotechnology comes from the potential to lead to war, arms races and destructive global government. Several reasons have been suggested why the availability of nanotech weaponry may with significant likelihood lead to unstable arms races (compared to e.g. nuclear arms races): (1) A large number of players may be tempted to enter the race since the threshold for doing so is low; (2) the ability to make weapons with molecular manufacturing will be cheap and easy to hide; (3) therefore lack of insight into the other parties' capabilities can tempt players to arm out of caution or to launch preemptive strikes; (4) molecular manufacturing may reduce dependency on international trade, a potential peace-promoting factor; (5) wars of aggression may pose a smaller economic threat to the aggressor since manufacturing is cheap and humans may not be needed on the battlefield.\n\nSince self-regulation by all state and non-state actors seems hard to achieve, measures to mitigate war-related risks have mainly been proposed in the area of international cooperation. International infrastructure may be expanded giving more sovereignty to the international level. This could help coordinate efforts for arms control. International institutions dedicated specifically to nanotechnology (perhaps analogously to the International Atomic Energy Agency IAEA) or general arms control may also be designed. One may also jointly make differential technological progress on defensive technologies, a policy that players should usually favour. The Center for Responsible Nanotechnology also suggest some technical restrictions. Improved transparency regarding technological capabilities may be another important facilitator for arms-control.\n\nA grey goo is another catastrophic scenario, which was proposed by Eric Drexler in his 1986 book \"Engines of Creation\", has been analyzed by Freitas in \"Some Limits to Global Ecophagy by Biovorous Nanoreplicators, with Public Policy Recommendations\" and has been a theme in mainstream media and fiction. This scenario involves tiny self-replicating robots that consume the entire biosphere using it as a source of energy and building blocks. Nanotech experts including Drexler now discredit the scenario. According to Chris Phoenix a \"So-called grey goo could only be the product of a deliberate and difficult engineering process, not an accident\". With the advent of nano-biotech, a different scenario called green goo has been forwarded. Here, the malignant substance is not nanobots but rather self-replicating biological organisms engineered through nanotechnology.\n\nNanotechnology (or molecular nanotechnology to refer more specifically to the goals discussed here) will let us continue the historical trends in manufacturing right up to the fundamental limits imposed by physical law. It will let us make remarkably powerful molecular computers. It will let us make materials over fifty times lighter than steel or aluminium alloy but with the same strength. We'll be able to make jets, rockets, cars or even chairs that, by today's standards, would be remarkably light, strong, and inexpensive. Molecular surgical tools, guided by molecular computers and injected into the blood stream could find and destroy cancer cells or invading bacteria, unclog arteries, or provide oxygen when the circulation is impaired.\n\nNanotechnology will replace our entire manufacturing base with a new, radically more precise, radically less expensive, and radically more flexible way of making products. The aim is not simply to replace today's computer chip making plants, but also to replace the assembly lines for cars, televisions, telephones, books, surgical tools, missiles, bookcases, airplanes, tractors, and all the rest. The objective is a pervasive change in manufacturing, a change that will leave virtually no product untouched. Economic progress and military readiness in the 21st Century will depend fundamentally on maintaining a competitive position in nanotechnology.\n\nDespite the current early developmental status of nanotechnology and molecular nanotechnology, much concern surrounds MNT's anticipated impact on economics and on law. Whatever the exact effects, MNT, if achieved, would tend to reduce the scarcity of manufactured goods and make many more goods (such as food and health aids) manufacturable.\n\nMNT should make possible nanomedical capabilities able to cure any medical condition not already cured by advances in other areas. Good health would be common, and poor health of any form would be as rare as smallpox and scurvy are today. Even cryonics would be feasible, as cryopreserved tissue could be fully repaired.\n\nMolecular nanotechnology is one of the technologies that some analysts believe could lead to a technological singularity.\nSome feel that molecular nanotechnology would have daunting risks. It conceivably could enable cheaper and more destructive conventional weapons. Also, molecular nanotechnology might permit weapons of mass destruction that could self-replicate, as viruses and cancer cells do when attacking the human body. Commentators generally agree that, in the event molecular nanotechnology were developed, its self-replication should be permitted only under very controlled or \"inherently safe\" conditions.\n\nA fear exists that nanomechanical robots, if achieved, and if designed to self-replicate using naturally occurring materials (a difficult task), could consume the entire planet in their hunger for raw materials, or simply crowd out natural life, out-competing it for energy (as happened historically when blue-green algae appeared and outcompeted earlier life forms). Some commentators have referred to this situation as the \"grey goo\" or \"ecophagy\" scenario. K. Eric Drexler considers an accidental \"grey goo\" scenario extremely unlikely and says so in later editions of \"Engines of Creation\".\n\nIn light of this perception of potential danger, the Foresight Institute, founded by Drexler, has prepared a set of guidelines for the ethical development of nanotechnology. These include the banning of free-foraging self-replicating pseudo-organisms on the Earth's surface, at least, and possibly in other places.\n\nThe feasibility of the basic technologies analyzed in \"Nanosystems\" has been the subject of a formal scientific review by U.S. National Academy of Sciences, and has also been the focus of extensive debate on the internet and in the popular press.\n\nIn 2006, U.S. National Academy of Sciences released the report of a study of molecular manufacturing as part of a longer report, \"A Matter of Size: Triennial Review of the National Nanotechnology Initiative\" The study committee reviewed the technical content of \"Nanosystems\", and in its conclusion states that no current theoretical analysis can be considered definitive regarding several questions of potential system performance, and that optimal paths for implementing high-performance systems cannot be predicted with confidence. It recommends experimental research to advance knowledge in this area:\n\nA section heading in Drexler's \"Engines of Creation\" reads \"Universal Assemblers\", and the following text speaks of multiple types of assemblers which, collectively, could hypothetically \"build almost anything that the laws of nature allow to exist.\" Drexler's colleague Ralph Merkle has noted that, contrary to widespread legend, Drexler never claimed that assembler systems could build absolutely any molecular structure. The endnotes in Drexler's book explain the qualification \"almost\": \"For example, a delicate structure might be designed that, like a stone arch, would self-destruct unless all its pieces were already in place. If there were no room in the design for the placement and removal of a scaffolding, then the structure might be impossible to build. Few structures of practical interest seem likely to exhibit such a problem, however.\"\n\nIn 1992, Drexler published \"Nanosystems: Molecular Machinery, Manufacturing, and Computation\", a detailed proposal for synthesizing stiff covalent structures using a table-top factory. Diamondoid structures and other stiff covalent structures, if achieved, would have a wide range of possible applications, going far beyond current MEMS technology. An outline of a path was put forward in 1992 for building a table-top factory in the absence of an assembler. Other researchers have begun advancing tentative, alternative proposed paths for this in the years since Nanosystems was published.\n\nIn 2004 Richard Jones wrote Soft Machines (nanotechnology and life), a book for lay audiences published by Oxford University. In this book he describes radical nanotechnology (as advocated by Drexler) as a deterministic/mechanistic idea of nano engineered machines that does not take into account the nanoscale challenges such as wetness, stickiness, Brownian motion, and high viscosity. He also explains what is soft nanotechnology or more appropriatelly biomimetic nanotechnology which is the way forward, if not the best way, to design functional nanodevices that can cope with all the problems at a nanoscale. One can think of soft nanotechnology as the development of nanomachines that uses the lessons learned from biology on how things work, chemistry to precisely engineer such devices and stochastic physics to model the system and its natural processes in detail.\n\nSeveral researchers, including Nobel Prize winner Dr. Richard Smalley (1943–2005), attacked the notion of universal assemblers, leading to a rebuttal from Drexler and colleagues, and eventually to an exchange of letters. Smalley argued that chemistry is extremely complicated, reactions are hard to control, and that a universal assembler is science fiction. Drexler and colleagues, however, noted that Drexler never proposed universal assemblers able to make absolutely anything, but instead proposed more limited assemblers able to make a very wide variety of things. They challenged the relevance of Smalley's arguments to the more specific proposals advanced in \"Nanosystems\". Also, Smalley argued that nearly all of modern chemistry involves reactions that take place in a solvent (usually water), because the small molecules of a solvent contribute many things, such as lowering binding energies for transition states. Since nearly all known chemistry requires a solvent, Smalley felt that Drexler's proposal to use a high vacuum environment was not feasible. However, Drexler addresses this in Nanosystems by showing mathematically that well designed catalysts can provide the effects of a solvent and can fundamentally be made even more efficient than a solvent/enzyme reaction could ever be. It is noteworthy that, contrary to Smalley's opinion that enzymes require water, \"Not only do enzymes work vigorously in anhydrous organic media, but in this unnatural milieu they acquire remarkable properties such as greatly enhanced stability, radically altered substrate and enantiomeric specificities, molecular memory, and the ability to catalyse unusual reactions.\"\n\nFor the future, some means have to be found for MNT design evolution at the nanoscale which mimics the process of biological evolution at the molecular scale. Biological evolution proceeds by random variation in ensemble averages of organisms combined with culling of the less-successful variants and reproduction of the more-successful variants, and macroscale engineering design also proceeds by a process of design evolution from simplicity to complexity as set forth somewhat satirically by John Gall: \"A complex system that works is invariably found to have evolved from a simple system that worked. . . . A complex system designed from scratch never works and can not be patched up to make it work. You have to start over, beginning with a system that works.\" A breakthrough in MNT is needed which proceeds from the simple atomic ensembles which can be built with, e.g., an STM to complex MNT systems via a process of design evolution. A handicap in this process is the difficulty of seeing and manipulation at the nanoscale compared to the macroscale which makes deterministic selection of successful trials difficult; in contrast biological evolution proceeds via action of what Richard Dawkins has called the \"blind watchmaker\"\ncomprising random molecular variation and deterministic reproduction/extinction.\n\nAt present in 2007 the practice of nanotechnology embraces both stochastic approaches (in which, for example, supramolecular chemistry creates waterproof pants) and deterministic approaches wherein single molecules (created by stochastic chemistry) are manipulated on substrate surfaces (created by stochastic deposition methods) by deterministic methods comprising nudging them with STM or AFM probes and causing simple binding or cleavage reactions to occur. The dream of a complex, deterministic molecular nanotechnology remains elusive. Since the mid-1990s, thousands of surface scientists and thin film technocrats have latched on to the nanotechnology bandwagon and redefined their disciplines as nanotechnology. This has caused much confusion in the field and has spawned thousands of \"nano\"-papers on the peer reviewed literature. Most of these reports are extensions of the more ordinary research done in the parent fields.\n\nThe feasibility of Drexler's proposals largely depends, therefore, on whether designs like those in \"Nanosystems\" could be built in the absence of a universal assembler to build them and would work as described. Supporters of molecular nanotechnology frequently claim that no significant errors have been discovered in \"Nanosystems\" since 1992. Even some critics concede that \"Drexler has carefully considered a number of physical principles underlying the 'high level' aspects of the nanosystems he proposes and, indeed, has thought in some detail\" about some issues.\n\nOther critics claim, however, that \"Nanosystems\" omits important chemical details about the low-level 'machine language' of molecular nanotechnology. They also claim that much of the other low-level chemistry in \"Nanosystems\" requires extensive further work, and that Drexler's higher-level designs therefore rest on speculative foundations. Recent such further work by Freitas and Merkle is aimed at strengthening these foundations by filling the existing gaps in the low-level chemistry.\n\nDrexler argues that we may need to wait until our conventional nanotechnology improves before solving these issues: \"Molecular manufacturing will result from a series of advances in molecular machine systems, much as the first Moon landing resulted from a series of advances in liquid-fuel rocket systems. We are now in a position like that of the British Interplanetary Society of the 1930s which described how multistage liquid-fueled rockets could reach the Moon and pointed to early rockets as illustrations of the basic principle.\" However, Freitas and Merkle argue that a focused effort to achieve diamond mechanosynthesis (DMS) can begin now, using existing technology, and might achieve success in less than a decade if their \"direct-to-DMS approach is pursued rather than a more circuitous development approach that seeks to implement less efficacious nondiamondoid molecular manufacturing technologies before progressing to diamondoid\".\n\nTo summarize the arguments against feasibility: First, critics argue that a primary barrier to achieving molecular nanotechnology is the lack of an efficient way to create machines on a molecular/atomic scale, especially in the absence of a well-defined path toward a self-replicating assembler or diamondoid nanofactory. Advocates respond that a preliminary research path leading to a diamondoid nanofactory is being developed.\n\nA second difficulty in reaching molecular nanotechnology is design. Hand design of a gear or bearing at the level of atoms might take a few to several weeks. While Drexler, Merkle and others have created designs of simple parts, no comprehensive design effort for anything approaching the complexity of a Model T Ford has been attempted. Advocates respond that it is difficult to undertake a comprehensive design effort in the absence of significant funding for such efforts, and that despite this handicap much useful design-ahead has nevertheless been accomplished with new software tools that have been developed, e.g., at Nanorex.\n\nIn the latest report \"A Matter of Size: Triennial Review of the National Nanotechnology Initiative\" put out by the National Academies Press in December 2006 (roughly twenty years after Engines of Creation was published), no clear way forward toward molecular nanotechnology could yet be seen, as per the conclusion on page 108 of that report: \"Although theoretical calculations can be made today, the eventually attainable\nrange of chemical reaction cycles, error rates, speed of operation, and thermodynamic\nefficiencies of such bottom-up manufacturing systems cannot be reliably\npredicted at this time. Thus, the eventually attainable perfection and complexity of\nmanufactured products, while they can be calculated in theory, cannot be predicted\nwith confidence. Finally, the optimum research paths that might lead to systems\nwhich greatly exceed the thermodynamic efficiencies and other capabilities of\nbiological systems cannot be reliably predicted at this time. Research funding that\nis based on the ability of investigators to produce experimental demonstrations\nthat link to abstract models and guide long-term vision is most appropriate to\nachieve this goal.\" This call for research leading to demonstrations is welcomed by groups such as the Nanofactory Collaboration who are specifically seeking experimental successes in diamond mechanosynthesis. The \"Technology Roadmap for Productive Nanosystems\" aims to offer additional constructive insights.\n\nIt is perhaps interesting to ask whether or not most structures consistent with physical law can in fact be manufactured. Advocates assert that to achieve most of the vision of molecular manufacturing it is not necessary to be able to build \"any structure that is compatible with natural law.\" Rather, it is necessary to be able to build only a sufficient (possibly modest) subset of such structures—as is true, in fact, of any practical manufacturing process used in the world today, and is true even in biology. In any event, as Richard Feynman once said, \"It is scientific only to say what's more likely or less likely, and not to be proving all the time what's possible or impossible.\"\n\nThere is a growing body of peer-reviewed theoretical work on synthesizing diamond by mechanically removing/adding hydrogen atoms and depositing carbon atoms (a process known as mechanosynthesis). This work is slowly permeating the broader nanoscience community and is being critiqued. For instance, Peng et al. (2006) (in the continuing research effort by Freitas, Merkle and their collaborators) reports that the most-studied mechanosynthesis tooltip motif (DCB6Ge) successfully places a C carbon dimer on a C(110) diamond surface at both 300 K (room temperature) and 80 K (liquid nitrogen temperature), and that the silicon variant (DCB6Si) also works at 80 K but not at 300 K. Over 100,000 CPU hours were invested in this latest study. The DCB6 tooltip motif, initially described by Merkle and Freitas at a Foresight Conference in 2002, was the first complete tooltip ever proposed for diamond mechanosynthesis and remains the only tooltip motif that has been successfully simulated for its intended function on a full 200-atom diamond surface.\n\nThe tooltips modeled in this work are intended to be used only in carefully controlled environments (e. g., vacuum). Maximum acceptable limits for tooltip translational and rotational misplacement errors are reported in Peng et al. (2006) -- tooltips must be positioned with great accuracy to avoid bonding the dimer incorrectly. Peng et al. (2006) reports that increasing the handle thickness from 4 support planes of C atoms above the tooltip to 5 planes decreases the resonance frequency of the entire structure from 2.0 THz to 1.8 THz. More importantly, the vibrational footprints of a DCB6Ge tooltip mounted on a 384-atom handle and of the same tooltip mounted on a similarly constrained but much larger 636-atom \"crossbar\" handle are virtually identical in the non-crossbar directions. Additional computational studies modeling still bigger handle structures are welcome, but the ability to precisely position SPM tips to the requisite atomic accuracy has been repeatedly demonstrated experimentally at low temperature, or even at room temperature constituting a basic existence proof for this capability.\n\nFurther research to consider additional tooltips will require time-consuming computational chemistry and difficult laboratory work.\n\nA working nanofactory would require a variety of well-designed tips for different reactions, and detailed analyses of placing atoms on more complicated surfaces. Although this appears a challenging problem given current resources, many tools will be available to help future researchers: Moore's law predicts further increases in computer power, semiconductor fabrication techniques continue to approach the nanoscale, and researchers grow ever more skilled at using proteins, ribosomes and DNA to perform novel chemistry.\n\n\n\n"}
{"id": "19638", "url": "https://en.wikipedia.org/wiki?curid=19638", "title": "Microelectromechanical systems", "text": "Microelectromechanical systems\n\nMicroelectromechanical systems (MEMS, also written as \"micro-electro-mechanical\", \"MicroElectroMechanical\" or \"microelectronic and microelectromechanical systems\" and the related \"micromechatronics\") is the technology of microscopic devices, particularly those with moving parts. It merges at the nano-scale into nanoelectromechanical systems (NEMS) and nanotechnology. MEMS are also referred to as micromachines in Japan, or \"micro systems technology\" (\"MST\") in Europe.\n\nMEMS are made up of components between 1 and 100 micrometers in size (i.e., 0.001 to 0.1 mm), and MEMS devices generally range in size from 20 micrometres to a millimetre (i.e., 0.02 to 1.0 mm), although components arranged in arrays (e.g., digital micromirror devices) can be more than 1000 mm. \nThey usually consist of a central unit that processes data (the microprocessor) and several components that interact with the surroundings such as microsensors. Because of the large surface area to volume ratio of MEMS, forces produced by ambient electromagnetism (e.g., electrostatic charges and magnetic moments), and fluid dynamics (e.g., surface tension and viscosity) are more important design considerations than with larger scale mechanical devices. MEMS technology is distinguished from molecular nanotechnology or molecular electronics in that the latter must also consider surface chemistry.\n\nThe potential of very small machines was appreciated before the technology existed that could make them (see, for example, Richard Feynman's famous 1959 lecture There's Plenty of Room at the Bottom). MEMS became practical once they could be fabricated using modified semiconductor device fabrication technologies, normally used to make electronics. These include molding and plating, wet etching (KOH, TMAH) and dry etching (RIE and DRIE), electro discharge machining (EDM), and other technologies capable of manufacturing small devices. An early example of a MEMS device is the resonistor, an electromechanical monolithic resonator patented by Raymond J. Wilfinger, and the resonant gate transistor developed by Harvey C. Nathanson.\n\nThe fabrication of MEMS evolved from the process technology in semiconductor device fabrication, i.e. the basic techniques are deposition of material layers, patterning by photolithography and etching to produce the required shapes.\n\nSilicon is the material used to create most integrated circuits used in consumer electronics in the modern industry. The economies of scale, ready availability of inexpensive high-quality materials, and ability to incorporate electronic functionality make silicon attractive for a wide variety of MEMS applications. Silicon also has significant advantages engendered through its material properties. In single crystal form, silicon is an almost perfect Hookean material, meaning that when it is flexed there is virtually no hysteresis and hence almost no energy dissipation. As well as making for highly repeatable motion, this also makes silicon very reliable as it suffers very little fatigue and can have service lifetimes in the range of billions to trillions of cycles without breaking.\n\nEven though the electronics industry provides an economy of scale for the silicon industry, crystalline silicon is still a complex and relatively expensive material to produce. Polymers on the other hand can be produced in huge volumes, with a great variety of material characteristics. MEMS devices can be made from polymers by processes such as injection molding, embossing or stereolithography and are especially well suited to microfluidic applications such as disposable blood testing cartridges.\n\nMetals can also be used to create MEMS elements. While metals do not have some of the advantages displayed by silicon in terms of mechanical properties, when used within their limitations, metals can exhibit very high degrees of reliability. Metals can be deposited by electroplating, evaporation, and sputtering processes. Commonly used metals include gold, nickel, aluminium, copper, chromium, titanium, tungsten, platinum, and silver.\n\nThe nitrides of silicon, aluminium and titanium as well as silicon carbide and other ceramics are increasingly applied in MEMS fabrication due to advantageous combinations of material properties. AlN crystallizes in the wurtzite structure and thus shows pyroelectric and piezoelectric properties enabling sensors, for instance, with sensitivity to normal and shear forces. TiN, on the other hand, exhibits a high electrical conductivity and large elastic modulus, making it possible to implement electrostatic MEMS actuation schemes with ultrathin membranes. Moreover, the high resistance of TiN against biocorrosion qualifies the material for applications in biogenic environments and in biosensors.\n\nOne of the basic building blocks in MEMS processing is the ability to deposit thin films of material with a thickness anywhere between one micrometre, to about 100 micrometres. The NEMS process is the same, although the measurement of film deposition ranges from a few nanometres to one micrometre. There are two types of deposition processes, as follows.\n\nPhysical vapor deposition (\"PVD\") consists of a process in which a material is removed from a target, and deposited on a surface. Techniques to do this include the process of sputtering, in which an ion beam liberates atoms from a target, allowing them to move through the intervening space and deposit on the desired substrate, and evaporation, in which a material is evaporated from a target using either heat (thermal evaporation) or an electron beam (e-beam evaporation) in a vacuum system.\n\nChemical deposition techniques include chemical vapor deposition (\"CVD\"), in which a stream of source gas reacts on the substrate to grow the material desired. This can be further divided into categories depending on the details of the technique, for example, LPCVD (Low Pressure chemical vapor deposition) and PECVD (Plasma-enhanced chemical vapor deposition).\n\nOxide films can also be grown by the technique of thermal oxidation, in which the (typically silicon) wafer is exposed to oxygen and/or steam, to grow a thin surface layer of silicon dioxide.\n\nPatterning in MEMS is the transfer of a pattern into a material.\n\nLithography in MEMS context is typically the transfer of a pattern into a photosensitive material by selective exposure to a radiation source such as light. A photosensitive material is a material that experiences a change in its physical properties when exposed to a radiation source. If a photosensitive material is selectively exposed to radiation (e.g. by masking some of the radiation) the pattern of the radiation on the material is transferred to the material exposed, as the properties of the exposed and unexposed regions differs.\n\nThis exposed region can then be removed or treated providing a mask for the underlying substrate. Photolithography is typically used with metal or other thin film deposition, wet and dry etching. Sometimes, photolithography is used to create structure without any kind of post etching. One example is SU8 based lens where SU8 based square blocks are generated. Then the photoresist is melted to form a semi-sphere which acts as a lens.\n\nElectron beam lithography (often abbreviated as e-beam lithography) is the practice of scanning a beam of electrons in a patterned fashion across a surface covered with a film (called the resist), (\"exposing\" the resist) and of selectively removing either exposed or non-exposed regions of the resist (\"developing\"). The purpose, as with photolithography, is to create very small structures in the resist that can subsequently be transferred to the substrate material, often by etching. It was developed for manufacturing integrated circuits, and is also used for creating nanotechnology architectures.\n\nThe primary advantage of electron beam lithography is that it is one of the ways to beat the diffraction limit of light and make features in the nanometer range. This form of maskless lithography has found wide usage in photomask-making used in photolithography, low-volume production of semiconductor components, and research & development.\n\nThe key limitation of electron beam lithography is throughput, i.e., the very long time it takes to expose an entire silicon wafer or glass substrate. A long exposure time leaves the user vulnerable to beam drift or instability which may occur during the exposure. Also, the turn-around time for reworking or re-design is lengthened unnecessarily if the pattern is not being changed the second time.\n\nIt is known that focused-ion beam lithography has the capability of writing extremely fine lines (less than 50 nm line and space has been achieved) without proximity effect. However, because the writing field in ion-beam lithography is quite small, large area patterns must be created by stitching together the small fields.\n\nIon track technology is a deep cutting tool with a resolution limit around 8 nm applicable to radiation resistant minerals, glasses and polymers. It is capable of generating holes in thin films without any development process. Structural depth can be defined either by ion range or by material thickness. Aspect ratios up to several 10 can be reached. The technique can shape and texture materials at a defined inclination angle. Random pattern, single-ion track structures and aimed pattern consisting of individual single tracks can be generated.\n\nX-ray lithography is a process used in electronic industry to selectively remove parts of a thin film. It uses X-rays to transfer a geometric pattern from a mask to a light-sensitive chemical photoresist, or simply \"resist\", on the substrate. A series of chemical treatments then engraves the produced pattern into the material underneath the photoresist.\n\nA simple way to carve or create patterns on the surface of nanodiamonds without damaging them could lead to a new photonic devices.\n\nDiamond patterning is a method of forming diamond MEMS. It is achieved by the lithographic application of diamond films to a substrate such as silicon. The patterns can be formed by selective deposition through a silicon dioxide mask, or by deposition followed by micromachining or focused ion beam milling.\n\nThere are two basic categories of etching processes: wet etching and dry etching. In the former, the material is dissolved when immersed in a chemical solution. In the latter, the material is sputtered or dissolved using reactive ions or a vapor phase etchant.\n\nWet chemical etching consists in selective removal of material by dipping a substrate into a solution that dissolves it. The chemical nature of this etching process provides a good selectivity, which means the etching rate of the target material is considerably higher than the mask material if selected carefully.\n\nEtching progresses at the same speed in all directions. Long and narrow holes in a mask will produce v-shaped grooves in the silicon. The surface of these grooves can be atomically smooth if the etch is carried out correctly, with dimensions and angles being extremely accurate.\n\nSome single crystal materials, such as silicon, will have different etching rates depending on the crystallographic orientation of the substrate. This is known as anisotropic etching and one of the most common examples is the etching of silicon in KOH (potassium hydroxide), where Si <111> planes etch approximately 100 times slower than other planes (crystallographic orientations). Therefore, etching a rectangular hole in a (100)-Si wafer results in a pyramid shaped etch pit with 54.7° walls, instead of a hole with curved sidewalls as with isotropic etching.\n\nHydrofluoric acid is commonly used as an aqueous etchant for silicon dioxide (, also known as BOX for SOI), usually in 49% concentrated form, 5:1, 10:1 or 20:1 BOE (buffered oxide etchant) or BHF (Buffered HF). They were first used in medieval times for glass etching. It was used in IC fabrication for patterning the gate oxide until the process step was replaced by RIE.\n\nHydrofluoric acid is considered one of the more dangerous acids in the cleanroom. It penetrates the skin upon contact and it diffuses straight to the bone. Therefore, the damage is not felt until it is too late.\n\nElectrochemical etching (ECE) for dopant-selective removal of silicon is a common method to automate and to selectively control etching. An active p-n diode junction is required, and either type of dopant can be the etch-resistant (\"etch-stop\") material. Boron is the most common etch-stop dopant. In combination with wet anisotropic etching as described above, ECE has been used successfully for controlling silicon diaphragm thickness in commercial piezoresistive silicon pressure sensors. Selectively doped regions can be created either by implantation, diffusion, or epitaxial deposition of silicon.\n\nXenon difluoride () is a dry vapor phase isotropic etch for silicon originally applied for MEMS in 1995 at University of California, Los Angeles. Primarily used for releasing metal and dielectric structures by undercutting silicon, has the advantage of a stiction-free release unlike wet etchants. Its etch selectivity to silicon is very high, allowing it to work with photoresist, , silicon nitride, and various metals for masking. Its reaction to silicon is \"plasmaless\", is purely chemical and spontaneous and is often operated in pulsed mode. Models of the etching action are available, and university laboratories and various commercial tools offer solutions using this approach.\n\nModern VLSI processes avoid wet etching, and use plasma etching instead. Plasma etchers can operate in several modes by adjusting the parameters of the plasma. Ordinary plasma etching operates between 0.1 and 5 Torr. (This unit of pressure, commonly used in vacuum engineering, equals approximately 133.3 pascals.) The plasma produces energetic free radicals, neutrally charged, that react at the surface of the wafer. Since neutral particles attack the wafer from all angles, this process is isotropic.\n\nPlasma etching can be isotropic, i.e., exhibiting a lateral undercut rate on a patterned surface approximately the same as its downward etch rate, or can be anisotropic, i.e., exhibiting a smaller lateral undercut rate than its downward etch rate. Such anisotropy is maximized in deep reactive ion etching. The use of the term anisotropy for plasma etching should not be conflated with the use of the same term when referring to orientation-dependent etching.\n\nThe source gas for the plasma usually contains small molecules rich in chlorine or fluorine. For instance, carbon tetrachloride (CCl4) etches silicon and aluminium, and trifluoromethane etches silicon dioxide and silicon nitride. A plasma containing oxygen is used to oxidize (\"ash\") photoresist and facilitate its removal.\n\nIon milling, or sputter etching, uses lower pressures, often as low as 10−4 Torr (10 mPa). It bombards the wafer with energetic ions of noble gases, often Ar+, which knock atoms from the substrate by transferring momentum. Because the etching is performed by ions, which approach the wafer approximately from one direction, this process is highly anisotropic. On the other hand, it tends to display poor selectivity. Reactive-ion etching (RIE) operates under conditions intermediate between sputter and plasma etching (between 10–3 and 10−1 Torr). Deep reactive-ion etching (DRIE) modifies the RIE technique to produce deep, narrow features.\n\nIn reactive-ion etching (RIE), the substrate is placed inside a reactor, and several gases are introduced. A plasma is struck in the gas mixture using an RF power source, which breaks the gas molecules into ions. The ions accelerate towards, and react with, the surface of the material being etched, forming another gaseous material. This is known as the chemical part of reactive ion etching. There is also a physical part, which is similar to the sputtering deposition process. If the ions have high enough energy, they can knock atoms out of the material to be etched without a chemical reaction. It is a very complex task to develop dry etch processes that balance chemical and physical etching, since there are many parameters to adjust. By changing the balance it is possible to influence the anisotropy of the etching, since the chemical part is isotropic and the physical part highly anisotropic the combination can form sidewalls that have shapes from rounded to vertical.\nDeep RIE (DRIE) is a special subclass of RIE that is growing in popularity. In this process, etch depths of hundreds of micrometres are achieved with almost vertical sidewalls. The primary technology is based on the so-called \"Bosch process\", named after the German company Robert Bosch, which filed the original patent, where two different gas compositions alternate in the reactor. Currently there are two variations of the DRIE. The first variation consists of three distinct steps (the original Bosch process) while the second variation only consists of two steps.\n\nIn the first variation, the etch cycle is as follows:\n\n(i) isotropic etch;\n(ii) passivation;\n(iii) anisoptropic etch for floor cleaning.\n\nIn the 2nd variation, steps (i) and (iii) are combined.\n\nBoth variations operate similarly. The creates a polymer on the surface of the substrate, and the second gas composition ( and ) etches the substrate. The polymer is immediately sputtered away by the physical part of the etching, but only on the horizontal surfaces and not the sidewalls. Since the polymer only dissolves very slowly in the chemical part of the etching, it builds up on the sidewalls and protects them from etching. As a result, etching aspect ratios of 50 to 1 can be achieved. The process can easily be used to etch completely through a silicon substrate, and etch rates are 3–6 times higher than wet etching.\n\nAfter preparing a large number of MEMS devices on a silicon wafer, individual dies have to be separated, which is called die preparation in semiconductor technology. For some applications, the separation is preceded by wafer backgrinding in order to reduce the wafer thickness. Wafer dicing may then be performed either by sawing using a cooling liquid or a dry laser process called stealth dicing.\n\nBulk micromachining is the oldest paradigm of silicon based MEMS. The whole thickness of a silicon wafer is used for building the micro-mechanical structures. Silicon is machined using various etching processes. Anodic bonding of glass plates or additional silicon wafers is used for adding features in the third dimension and for hermetic encapsulation. Bulk micromachining has been essential in enabling high performance pressure sensors and accelerometers that changed the sensor industry in the 1980s and 90's.\n\nSurface micromachining uses layers deposited on the surface of a substrate as the structural materials, rather than using the substrate itself. Surface micromachining was created in the late 1980s to render micromachining of silicon more compatible with planar integrated circuit technology, with the goal of combining MEMS and integrated circuits on the same silicon wafer. The original surface micromachining concept was based on thin polycrystalline silicon layers patterned as movable mechanical structures and released by sacrificial etching of the underlying oxide layer. Interdigital comb electrodes were used to produce in-plane forces and to detect in-plane movement capacitively. This MEMS paradigm has enabled the manufacturing of low cost accelerometers for e.g. automotive air-bag systems and other applications where low performance and/or high g-ranges are sufficient. Analog Devices has pioneered the industrialization of surface micromachining and has realized the co-integration of MEMS and integrated circuits.\n\nBoth bulk and surface silicon micromachining are used in the industrial production of sensors, ink-jet nozzles, and other devices. But in many cases the distinction between these two has diminished. A new etching technology, deep reactive-ion etching, has made it possible to combine good performance typical of bulk micromachining with comb structures and in-plane operation typical of surface micromachining. While it is common in surface micromachining to have structural layer thickness in the range of 2 µm, in HAR silicon micromachining the thickness can be from 10 to 100 µm. The materials commonly used in HAR silicon micromachining are thick polycrystalline silicon, known as epi-poly, and bonded silicon-on-insulator (SOI) wafers although processes for bulk silicon wafer also have been created (SCREAM). Bonding a second wafer by glass frit bonding, anodic bonding or alloy bonding is used to protect the MEMS structures. Integrated circuits are typically not combined with HAR silicon micromachining.\n\nSome common commercial applications of MEMS include:\n\n\nThe global market for micro-electromechanical systems, which includes products such as automobile airbag systems, display systems and inkjet cartridges totaled $40 billion in 2006 according to Global MEMS/Microsystems Markets and Opportunities, a research report from SEMI and Yole Developpement and is forecasted to reach $72 billion by 2011.\n\nCompanies with strong MEMS programs come in many sizes. Larger firms specialize in manufacturing high volume inexpensive components or packaged solutions for end markets such as automobiles, biomedical, and electronics. Smaller firms provide value in innovative solutions and absorb the expense of custom fabrication with high sales margins. Both large and small companies typically invest in R&D to explore new MEMS technology.\n\nThe market for materials and equipment used to manufacture MEMS devices topped $1 billion worldwide in 2006. Materials demand is driven by substrates, making up over 70 percent of the market, packaging coatings and increasing use of chemical mechanical planarization (CMP). While MEMS manufacturing continues to be dominated by used semiconductor equipment, there is a migration to 200 mm lines and select new tools, including etch and bonding for certain MEMS applications.\n\n\n"}
{"id": "19639", "url": "https://en.wikipedia.org/wiki?curid=19639", "title": "Marvin Minsky", "text": "Marvin Minsky\n\nMarvin Lee Minsky (August 9, 1927 – January 24, 2016) was an American cognitive scientist concerned largely with research of artificial intelligence (AI), co-founder of the Massachusetts Institute of Technology's AI laboratory, and author of several texts concerning AI and philosophy.\n\nMarvin Lee Minsky was born in New York City, to an eye surgeon father, Henry, and to a mother, Fannie, who was an activist of Zionist affairs. His family was Jewish. He attended the Ethical Culture Fieldston School and the Bronx High School of Science. He later attended Phillips Academy in Andover, Massachusetts. He then served in the US Navy from 1944 to 1945. He received a B.A. in mathematics from Harvard University (1950) and a Ph.D. in mathematics from Princeton University (1954).\n\nHe was on the MIT faculty from 1958 to his death. He joined the staff at MIT Lincoln Laboratory in 1958, and a year later he and John McCarthy initiated what is known now as the MIT Computer Science and Artificial Intelligence Laboratory. He was the Toshiba Professor of Media Arts and Sciences, and professor of electrical engineering and computer science.\n\nMinsky's inventions include the first head-mounted graphical display (1963) and the confocal microscope (1957, a predecessor to today's widely used confocal laser scanning microscope). He developed, with Seymour Papert, the first Logo \"turtle\". Minsky also built, in 1951, the first randomly wired neural network learning machine, SNARC.\n\nIn 1962, Minsky came up with a 7,4 Turing machine that he was able to prove to be universal. At that point in time, it was known to be the simplest universal Turing machine–a record that stood for approximately 40 years until Stephen Wolfram published a 2,5 universal Turing machine in his 2002 book, \"A New Kind of Science\".\n\nMinsky wrote the book \"Perceptrons\" (with Seymour Papert), which became the foundational work in the analysis of artificial neural networks. This book is the center of a controversy in the history of AI, as some claim it to have had great importance in discouraging research of neural networks in the 1970s, and contributing to the so-called \"AI winter\". He also founded several other famous AI models. His book \"A framework for representing knowledge\" created a new paradigm in programming. While his \"Perceptrons\" is now more a historical than practical book, the theory of frames is in wide use. Minsky has also written on the possibility that extraterrestrial life may think like humans, permitting communication.\n\nIn the early 1970s, at the MIT Artificial Intelligence Lab, Minsky and Papert started developing what came to be known as the Society of Mind theory. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks. In 1986, Minsky published \"The Society of Mind\", a comprehensive book on the theory which, unlike most of his previously published work, was written for the general public.\n\nIn November 2006, Minsky published \"The Emotion Machine\", a book that critiques many popular theories of how human minds work and suggests alternative theories, often replacing simple ideas with more complex ones. Recent drafts of the book are freely available from his webpage.\n\nMinsky was an adviser on Stanley Kubrick's movie ; one of the movie's characters, Victor Kaminski, was named in Minsky's honor. Minsky himself is explicitly mentioned in Arthur C. Clarke's derivative novel of the same name, where he is portrayed as achieving a crucial break-through in artificial intelligence in the then-future 1980s, paving the way for HAL 9000 in the early 21st century:\n\nIn 1952, Minsky married pediatrician Gloria Rudisch; together they had three children. Minsky was a talented improvisational pianist who published musings on the relations between music and psychology.\n\nMinsky was an atheist, a signatory to the Scientists' Open Letter on Cryonics.\n\nHe was a critic of the Loebner Prize for conversational robots, and argued that a fundamental difference between humans and machines was that while humans are machines, they are machines in which intelligence emerges from the interplay of the many unintelligent but semi-autonomous agents that comprise the brain. He argued that \"somewhere down the line, some computers will become more intelligent than most people,\" but that it was very hard to predict how fast progress would be. He cautioned that an artificial superintelligence designed to solve an innocuous mathematical problem might decide to assume control of Earth's resources to build supercomputers to help achieve its goal, but believed that such negative scenarios are \"hard to take seriously\" because he felt confident that AI would go through a lot of testing before being deployed.\n\nMinsky died of a cerebral hemorrhage at the age of 88. Minsky was a member of Alcor's Scientific Advisory Board, and is believed to have been cryonically preserved by Alcor, presumably as 'Patient 144', whose cooling procedures began on January 27, 2016.\n\n\nMinsky won the Turing Award (the greatest distinction in computer science) in 1969, the Japan Prize in 1990, the IJCAI Award for Research Excellence for 1991, and the Benjamin Franklin Medal from the Franklin Institute for 2001. In 2006, he was inducted as a Fellow of the Computer History Museum \"for co-founding the field of artificial intelligence, creating early neural networks and robots, and developing theories of human and machine cognition.\" In 2011, Minsky was inducted into IEEE Intelligent Systems' AI Hall of Fame for the \"significant contributions to the field of AI and intelligent systems\". In 2014, Minsky won the Dan David Prize for \"Artificial Intelligence, the Digital Mind\". He was also awarded with the 2013 BBVA Foundation Frontiers of Knowledge Award in the Information and Communication Technologies category.\n\nMinsky was affiliated with the following organizations:\n\n\n\n"}
{"id": "19640", "url": "https://en.wikipedia.org/wiki?curid=19640", "title": "Milton Friedman", "text": "Milton Friedman\n\nMilton Friedman (; July 31, 1912 – November 16, 2006) was an American economist who received the 1976 Nobel Memorial Prize in Economic Sciences for his research on consumption analysis, monetary history and theory and the complexity of stabilization policy. With George Stigler and others, Friedman was among the intellectual leaders of the second generation of Chicago price theory, a methodological movement at the University of Chicago's Department of Economics, Law School and Graduate School of Business from the 1940s onward. Several students and young professors who were recruited or mentored by Friedman at Chicago went on to become leading economists, including Gary Becker, Robert Fogel, Thomas Sowell and Robert Lucas Jr.\n\nFriedman's challenges to what he later called \"naive Keynesian\" theory began with his 1950s reinterpretation of the consumption function. In the 1960s, he became the main advocate opposing Keynesian government policies and described his approach (along with mainstream economics) as using \"Keynesian language and apparatus\" yet rejecting its \"initial\" conclusions. He theorized that there existed a \"natural\" rate of unemployment and argued that unemployment below this rate would cause inflation to accelerate. He argued that the Phillips curve was in the long run vertical at the \"natural rate\" and predicted what would come to be known as stagflation. Friedman promoted an alternative macroeconomic viewpoint known as \"monetarism\" and argued that a steady, small expansion of the money supply was the preferred policy. His ideas concerning monetary policy, taxation, privatization and deregulation influenced government policies, especially during the 1980s. His monetary theory influenced the Federal Reserve's response to the global financial crisis of 2007–2008.\n\nFriedman was an advisor to Republican President Ronald Reagan and Conservative British Prime Minister Margaret Thatcher. His political philosophy extolled the virtues of a free market economic system with minimal intervention. He once stated that his role in eliminating conscription in the United States was his proudest accomplishment. In his 1962 book \"Capitalism and Freedom\", Friedman advocated policies such as a volunteer military, freely floating exchange rates, abolition of medical licenses, a negative income tax and school vouchers and opposed the war on drugs. His support for school choice led him to found the Friedman Foundation for Educational Choice, later renamed EdChoice.\n\nFriedman's works include monographs, books, scholarly articles, papers, magazine columns, television programs and lectures and cover a broad range of economic topics and public policy issues. His books and essays have had global influence, including in former communist states. A survey of economists ranked Friedman as the second-most popular economist of the 20th century following only John Maynard Keynes and \"The Economist\" described him as \"the most influential economist of the second half of the 20th century ... possibly of all of it\".\n\nFriedman was born in Brooklyn, New York on July 31, 1912. His parents, Sára Ethel (née Landau) and Jenő Saul Friedman, were Jewish immigrants from Beregszász in Carpathian Ruthenia, Kingdom of Hungary (now Berehove in Ukraine). They both worked as dry goods merchants. Shortly after his birth, the family relocated to Rahway, New Jersey. In his early teens, Friedman was injured in a car accident, which scarred his upper lip. A talented student, Friedman graduated from Rahway High School in 1928, just before his 16th birthday. He was awarded a competitive scholarship to Rutgers University (then a private university receiving limited support from the State of New Jersey, e.g., for such scholarships).\n\nIn 1932, Friedman graduated from Rutgers University, where he specialized in mathematics and economics and initially intended to become an actuary. During his time at Rutgers, Friedman became influenced by two economics professors, Arthur F. Burns and Homer Jones, who convinced him that modern economics could help end the Great Depression.\n\nAfter graduating from Rutgers, Friedman was offered two scholarships to do graduate work—one in mathematics at Brown University and the other in economics at the University of Chicago. Friedman chose the latter, thus earning a Master of Arts degree in 1933. He was strongly influenced by Jacob Viner, Frank Knight, and Henry Simons. It was at Chicago that Friedman met his future wife, economist Rose Director. During the 1933–1934 academic year he had a fellowship at Columbia University, where he studied statistics with renowned statistician and economist Harold Hotelling. He was back in Chicago for the 1934–1935 academic year, working as a research assistant for Henry Schultz, who was then working on \"Theory and Measurement of Demand\". That year, Friedman formed what would prove to be lifelong friendships with George Stigler and W. Allen Wallis.\n\nFriedman was initially unable to find academic employment, so in 1935 he followed his friend W. Allen Wallis to Washington, D.C., where Franklin D. Roosevelt's New Deal was \"a lifesaver\" for many young economists. At this stage, Friedman said that he and his wife \"regarded the job-creation programs such as the WPA, CCC, and PWA appropriate responses to the critical situation,\" but not \"the price- and wage-fixing measures of the National Recovery Administration and the Agricultural Adjustment Administration.\" Foreshadowing his later ideas, he believed price controls interfered with an essential signaling mechanism to help resources be used where they were most valued. Indeed, Friedman later concluded that all government intervention associated with the New Deal was \"the wrong cure for the wrong disease,\" arguing that the money supply should simply have been expanded, instead of contracted. Later, Friedman and his colleague Anna Schwartz wrote \"A Monetary History of the United States, 1867–1960\", which argued that the Great Depression was caused by a severe monetary contraction due to banking crises and poor policy on the part of the Federal Reserve.\n\nDuring 1935, he began working for the National Resources Planning Board, which was then working on a large consumer budget survey. Ideas from this project later became a part of his \"Theory of the Consumption Function\". Friedman began employment with the National Bureau of Economic Research during autumn 1937 to assist Simon Kuznets in his work on professional income. This work resulted in their jointly authored publication \"Incomes from Independent Professional Practice\", which introduced the concepts of permanent and transitory income, a major component of the Permanent Income Hypothesis that Friedman worked out in greater detail in the 1950s. The book hypothesizes that professional licensing artificially restricts the supply of services and raises prices.\n\nDuring 1940, Friedman was appointed an assistant professor teaching Economics at the University of Wisconsin–Madison, but encountered antisemitism in the Economics department and decided to return to government service. From 1941 to 1943 Friedman worked on wartime tax policy for the federal government, as an advisor to senior officials of the United States Department of the Treasury. As a Treasury spokesman during 1942 he advocated a Keynesian policy of taxation. He helped to invent the payroll withholding tax system, since the federal government badly needed money in order to fight the war. He later said, \"I have no apologies for it, but I really wish we hadn't found it necessary and I wish there were some way of abolishing withholding now.\"\n\nIn 1940, Friedman accepted a position at the University of Wisconsin–Madison, but left because of differences with faculty regarding United States involvement in World War II. Friedman believed the United States should enter the war. In 1943, Friedman joined the Division of War Research at Columbia University (headed by W. Allen Wallis and Harold Hotelling), where he spent the rest of World War II working as a mathematical statistician, focusing on problems of weapons design, military tactics, and metallurgical experiments.\n\nIn 1945, Friedman submitted \"Incomes from Independent Professional Practice\" (co-authored with Kuznets and completed during 1940) to Columbia as his doctoral dissertation. The university awarded him a PhD in 1946. Friedman spent the 1945–1946 academic year teaching at the University of Minnesota (where his friend George Stigler was employed). On February 12, 1945, his son, David D. Friedman was born.\n\nIn 1946, Friedman accepted an offer to teach economic theory at the University of Chicago (a position opened by departure of his former professor Jacob Viner to Princeton University). Friedman would work for the University of Chicago for the next 30 years. There he contributed to the establishment of an intellectual community that produced a number of Nobel Prize winners, known collectively as the Chicago school of economics.\n\nAt that time, Arthur F. Burns, who was then the head of the National Bureau of Economic Research, asked Friedman to rejoin the Bureau's staff. He accepted the invitation, and assumed responsibility for the Bureau's inquiry into the role of money in the business cycle. As a result, he initiated the \"Workshop in Money and Banking\" (the \"Chicago Workshop\"), which promoted a revival of monetary studies. During the latter half of the 1940s, Friedman began a collaboration with Anna Schwartz, an economic historian at the Bureau, that would ultimately result in the 1963 publication of a book co-authored by Friedman and Schwartz, \"A Monetary History of the United States, 1867–1960\".\n\nFriedman spent the 1954–1955 academic year as a Fulbright Visiting Fellow at Gonville and Caius College, Cambridge. At the time, the Cambridge economics faculty was divided into a Keynesian majority (including Joan Robinson and Richard Kahn) and an anti-Keynesian minority (headed by Dennis Robertson). Friedman speculated that he was invited to the fellowship, because his views were unacceptable to both of the Cambridge factions. Later his weekly columns for \"Newsweek\" magazine (1966–84) were well read and increasingly influential among political and business people. From 1968 to 1978, he and Paul Samuelson participated in the Economics Cassette Series, a biweekly subscription series where the economist would discuss the days' issues for about a half-hour at a time.\n\nFriedman was an economic adviser to Republican presidential candidate Barry Goldwater during 1964.\n\nHis \"Capitalism and Freedom\" brought him national and international attention outside academia. It was published in 1962 by the University of Chicago Press and consists of essays that used non-mathematical economic models to explore issues of public policy. It sold over 400,000 copies in the first eighteen years and more than half a million since 1962. It has been translated into eighteen languages. Friedman talks about the need to move to a classically liberal society, that free markets would help nations and individuals in the long-run and fix the efficiency problems currently faced by the United States and other major countries of the 1950s and 1960s. He goes through the chapters specifying a specific issue in each respective chapter from the role of government and money supply to social welfare programs to a special chapter on occupational licensure. Friedman concludes \"Capitalism and Freedom\" with his \"classical liberal\" (more accurately, libertarian) stance, that government should stay out of matters that do not need and should only involve itself when absolutely necessary for the survival of its people and the country. He recounts how the best of a country's abilities come from its free markets while its failures come from government intervention.\n\nIn 1977, at the age of 65, Friedman retired from the University of Chicago after teaching there for 30 years. He and his wife moved to San Francisco, where he became a visiting scholar at the Federal Reserve Bank of San Francisco. From 1977 on, he was affiliated with the Hoover Institution at Stanford University. During the same year, Friedman was approached by the Free To Choose Network and asked to create a television program presenting his economic and social philosophy.\n\nThe Friedmans worked on this project for the next three years, and during 1980, the ten-part series, titled \"Free to Choose\", was broadcast by the Public Broadcasting Service (PBS). The companion book to the series (co-authored by Milton and his wife, Rose Friedman), also titled \"Free To Choose\", was the bestselling nonfiction book of 1980 and has since been translated into 14 languages.\n\nFriedman served as an unofficial adviser to Ronald Reagan during his 1980 presidential campaign, and then served on the President's Economic Policy Advisory Board for the rest of the Reagan Administration. Ebenstein says Friedman was \"the 'guru' of the Reagan administration.\" In 1988 he received the National Medal of Science and Reagan honored him with the Presidential Medal of Freedom.\n\nMilton Friedman is known now as one of the most influential economists of the 20th century. Throughout the 1980s and 1990s, Friedman continued to write editorials and appear on television. He made several visits to Eastern Europe and to China, where he also advised governments. He was also for many years a Trustee of the Philadelphia Society.\n\nAccording to a 2007 article in \"Commentary\" magazine, his \"parents were moderately observant Jews, but Friedman, after an intense burst of childhood piety, rejected religion altogether.\" He described himself as an agnostic. Friedman wrote extensively of his life and experiences, especially in 1998 in his memoirs with his wife, Rose, titled \"Two Lucky People\".\n\nFriedman died of heart failure at the age of 94 years in San Francisco on November 16, 2006. He was still a working economist performing original economic research; his last column was published in \"The Wall Street Journal\" the day after his death. He was survived by his wife (who died on August 18, 2009) and their two children, David, known for the anarcho-capitalist book \"The Machinery of Freedom\", and bridge expert Jan Martel.\n\nFriedman was best known for reviving interest in the money supply as a determinant of the nominal value of output, that is, the quantity theory of money. Monetarism is the set of views associated with modern quantity theory. Its origins can be traced back to the 16th-century School of Salamanca or even further; however, Friedman's contribution is largely responsible for its modern popularization. He co-authored, with Anna Schwartz, \"A Monetary History of the United States, 1867–1960\" (1963), which was an examination of the role of the money supply and economic activity in the U.S. history. A striking conclusion of their research regarded the way in which money supply fluctuations contribute to economic fluctuations. Several regression studies with David Meiselman during the 1960s suggested the primacy of the money supply over investment and government spending in determining consumption and output. These challenged a prevailing, but largely untested, view on their relative importance. Friedman's empirical research and some theory supported the conclusion that the short-run effect of a change of the money supply was primarily on output but that the longer-run effect was primarily on the price level.\n\nFriedman was the main proponent of the monetarist school of economics. He maintained that there is a close and stable association between inflation and the money supply, mainly that inflation could be avoided with proper regulation of the monetary base's growth rate. He famously used the analogy of \"dropping money out of a helicopter\", in order to avoid dealing with money injection mechanisms and other factors that would overcomplicate his models.\n\nFriedman's arguments were designed to counter the popular concept of cost-push inflation, that the increased general price level at the time was the result of increases in the price of oil, or increases in wages; as he wrote,\n\nFriedman rejected the use of fiscal policy as a tool of demand management; and he held that the government's role in the guidance of the economy should be restricted severely. Friedman wrote extensively on the Great Depression, and he termed the 1929–1933 period the Great Contraction. He argued that the Depression had been caused by an ordinary financial shock whose duration and seriousness were greatly increased by the subsequent contraction of the money supply caused by the misguided policies of the directors of the Federal Reserve.\n\nThis theory was put forth in \"A Monetary History of the United States\", and the chapter on the Great Depression was then published as a stand-alone book entitled \"The Great Contraction, 1929–1933\". Both books are still in print from Princeton University Press, and some editions include as an appendix a speech at a University of Chicago event honoring Friedman in which Ben Bernanke made this statement:\n\nLet me end my talk by abusing slightly my status as an official representative of the Federal Reserve. I would like to say to Milton and Anna: Regarding the Great Depression, you're right. We did it. We're very sorry. But thanks to you, we won't do it again.\n\nFriedman also argued for the cessation of government intervention in currency markets, thereby spawning an enormous literature on the subject, as well as promoting the practice of freely floating exchange rates. His close friend George Stigler explained, \"As is customary in science, he did not win a full victory, in part because research was directed along different lines by the theory of rational expectations, a newer approach developed by Robert Lucas, also at the University of Chicago.\" The relationship between Friedman and Lucas, or new classical macroeconomics as a whole, was highly complex. The Friedmanian Phillips curve was an interesting starting point for Lucas, but he soon realized that the solution provided by Friedman was not quite satisfactory. Lucas elaborated a new approach in which rational expectations were presumed instead of the Friedmanian adaptive expectations. Due to this reformulation, the story in which the theory of the new classical Phillips curve was embedded radically changed. This modification, however, had a significant effect on Friedman's own approach, so, as a result, the theory of the Friedmanian Phillips curve also changed. Moreover, new classical Neil Wallace, who was a graduate student at the University of Chicago between 1960 and 1963, regarded Friedman's theoretical courses as a mess. This evaluation clearly indicates the broken relationship between Friedmanian monetarism and new classical macroeconomics.\n\nFriedman was also known for his work on the consumption function, the permanent income hypothesis (1957), which Friedman himself referred to as his best scientific work. This work contended that rational consumers would spend a proportional amount of what they perceived to be their permanent income. Windfall gains would mostly be saved. Tax reductions likewise, as rational consumers would predict that taxes would have to increase later to balance public finances. Other important contributions include his critique of the Phillips curve and the concept of the natural rate of unemployment (1968). This critique associated his name, together with that of Edmund Phelps, with the insight that a government that brings about greater inflation cannot permanently reduce unemployment by doing so. Unemployment may be temporarily lower, if the inflation is a surprise, but in the long run unemployment will be determined by the frictions and imperfections of the labor market.\n\nFriedman's essay \"The Methodology of Positive Economics\" (1953) provided the epistemological pattern for his own subsequent research and to a degree that of the Chicago School. There he argued that economics as \"science\" should be free of value judgments for it to be objective. Moreover, a useful economic theory should be judged not by its descriptive realism but by its simplicity and fruitfulness as an engine of prediction. That is, students should measure the accuracy of its predictions, rather than the 'soundness of its assumptions'. His argument was part of an ongoing debate among such statisticians as Jerzy Neyman, Leonard Savage, and Ronald Fisher.\n\nOne of his most famous contributions to statistics is sequential sampling. Friedman did statistical work at the Division of War Research at Columbia, where he and his colleagues came up with the technique. It became, in the words of \"The New Palgrave Dictionary of Economics\", \"the standard analysis of quality control inspection\". The dictionary adds, \"Like many of Friedman's contributions, in retrospect it seems remarkably simple and obvious to apply basic economic ideas to quality control; that, however, is a measure of his genius.\"\n\nAlthough Friedman concluded the government does have a role in the monetary system he was critical of the Federal Reserve due to its poor performance and felt it should be abolished. He was opposed to Federal Reserve policies, even during the so-called 'Volcker shock' that was labelled 'monetarist'. Friedman believed that the Federal Reserve System should ultimately be replaced with a computer program. He favored a system that would automatically buy and sell securities in response to changes in the money supply.\n\nThe proposal to constantly grow the money supply at a certain predetermined amount every year has become known as Friedman's k-percent rule. There is debate about the effectiveness of a theoretical money supply targeting regime. The Fed's inability to meet its money supply targets from 1978-1982 has led some to conclude it is not a feasible alternative to more conventional inflation and interest rate targeting. Towards the end of his life Milton Friedman expressed doubt about the validity of targeting the quantity of money.\n\nIdeally, Friedman actually favored the principles of the 1930s Chicago plan, which would have ended fractional reserve banking and, thus, private money creation. It would force banks to have 100% reserves backing deposits, and instead place money creation powers solely in the hands of the US Government. This would make targeting money growth more possible, as endogenous money created by fractional reserve lending would no longer be a major issue.\n\nFriedman was a strong advocate for floating exchange rates throughout the entire Bretton-Woods period. He argued that a flexible exchange rate would make external adjustment possible and allow countries to avoid balance of payments crises. He saw fixed exchange rates as an undesirable form of government intervention. The case was articulated in an influential 1953 paper, \"The Case for Flexible Exchange Rates\", at a time, when most commentators regarded the possibility of floating exchange rates as a fantasy.\n\nIn his 1955 article \"The Role of Government in Education\" Friedman proposed supplementing publicly operated schools with privately run but publicly funded schools through a system of school vouchers. Reforms similar to those proposed in the article were implemented in, for example, Chile in 1981 and Sweden in 1992. In 1996, Friedman, together with his wife, founded the Friedman Foundation for Educational Choice to advocate school choice and vouchers. In 2016, the Friedman Foundation changed its name to EdChoice to honor the Friedmans' desire to have the educational choice movement live on without their names attached to it after their deaths.\n\nWhile Walter Oi is credited with establishing the economic basis for a volunteer military, Milton Friedman was a proponent, stating that the draft was \"inconsistent with a free society.\"\nIn \"Capitalism and Freedom\", he argued that conscription is inequitable and arbitrary, preventing young men from shaping their lives as they see fit. During the Nixon administration he headed the committee to research a conversion to paid/volunteer armed force. He would later state that his role in eliminating the conscription in the United States was his proudest accomplishment. Friedman did, however, believe a nation could compel military \"training\" as a reserve in case of war time.\n\nBiographer Lanny Ebenstein noted a drift over time in Friedman's views from an interventionist to a more cautious foreign policy. He supported US involvement in the Second World War and initially supported a hard line against Communism, but moderated over time. However, Friedman did state in a 1995 interview that he is an anti-interventionist. He opposed the Gulf War and the Iraq War. In a spring 2006 interview, Friedman said that the USA's stature in the world had been eroded by the Iraq War, but that it might be improved if Iraq were to become a peaceful and independent country.\n\nFriedman was a supporter of the candidacy of Barry Goldwater against Lyndon Johnson following Kennedy in 1964. Later, he served as a member of President Reagan's Economic Policy Advisory Board starting in 1981. In 1988, he received the Presidential Medal of Freedom and the National Medal of Science. He said that he was a libertarian philosophically, but a member of the U.S. Republican Party for the sake of \"expediency\" (\"I am a libertarian with a small 'l' and a Republican with a capital 'R.' And I am a Republican with a capital 'R' on grounds of expediency, not on principle.\") But, he said, \"I think the term classical liberal is also equally applicable. I don't really care very much what I'm called. I'm much more interested in having people thinking about the ideas, rather than the person.\"\n\nFriedman was supportive of the state provision of some public goods that private businesses are not considered as being able to provide. However, he argued that many of the services performed by government could be performed better by the private sector. Above all, if some public goods are provided by the state, he believed that they should not be a legal monopoly where private competition is prohibited; for example, he wrote:\n\nIn 1962, Friedman criticized Social Security in his book \"Capitalism and Freedom\" arguing that it had created welfare dependency. However, in the penultimate chapter of the same book, Friedman argued that while capitalism had greatly reduced the extent of poverty in absolute terms, \"poverty is in part a relative matter, [and] even in [wealthy Western] countries, there are clearly many people living under conditions that the rest of us label as poverty.\" Friedman noted that while private charity could be one recourse for alleviating poverty (and cited late 19th century Britain and the United States as exemplary periods of extensive private charity and eleemosynary activity), Friedman also noted that:\n\nFriedman argued further that other advantages of the negative income tax were that it could fit directly into the tax system, would be less costly, and would reduce the administrative burden of implementing a social safety net. Friedman reiterated these arguments 18 years later in \"Free to Choose\", with the additional proviso that such a reform would only be satisfactory if it replaced the current system of welfare programs rather than augment it. According to economist Robert H. Frank, writing in \"The New York Times\", Friedman's views in this regard were grounded in a belief that while \"market forces ... accomplish wonderful things\", they \"cannot ensure a distribution of income that enables all citizens to meet basic economic needs\".\n\nFriedman also supported libertarian policies such as legalization of drugs and prostitution. During 2005, Friedman and more than 500 other economists advocated discussions regarding the economic benefits of the legalization of marijuana.\n\nFriedman was also a supporter of gay rights. He never specifically supported same-sex marriage, instead saying \"I do not believe there should be any discrimination against gays.\"\n\nFriedman favored immigration, saying \"legal and illegal immigration has a very positive impact on the U.S. economy.\" Friedman however suggested that immigrants ought not to have access to the welfare system. Friedman stated that immigration from Mexico had been a \"good thing\", in particular illegal immigration. Friedman argued that illegal immigration was a boon because they \"take jobs that most residents of this country are unwilling to take, they provide employers with workers of a kind they cannot get\" and they do not use welfare. In \"Free to Choose\", Friedman wrote, No arbitrary obstacles should prevent people from achieving those positions for which their talents fit them and which their values lead them to seek. Not birth, nationality, color, religion, sex, nor any other irrelevant characteristic should determine the opportunities that are open to a person — only his abilities.\n\nMichael Walker of the Fraser Institute and Friedman hosted a series of conferences from 1986 to 1994. The goal was to create a clear definition of economic freedom and a method for measuring it. Eventually this resulted in the first report on worldwide economic freedom, \"Economic Freedom in the World\". This annual report has since provided data for numerous peer-reviewed studies and has influenced policy in several nations.\n\nAlong with sixteen other distinguished economists he opposed the Copyright Term Extension Act and filed an amicus brief in \"Eldred v. Ashcroft\". He supported the inclusion of the word \"no-brainer\" in the brief.\n\nFriedman argued for stronger basic legal (constitutional) protection of economic rights and freedoms to further promote industrial-commercial growth and prosperity and buttress democracy and freedom and the rule of law generally in society.\n\nGeorge H. Nash, a leading historian of American conservatism, says that by, \"the end of the 1960s he was probably the most highly regarded and influential conservative scholar in the country, and one of the few with an international reputation.\" Friedman allowed the libertarian Cato Institute to use his name for its biannual Milton Friedman Prize for Advancing Liberty beginning in 2001. A Friedman Prize was given to the late British economist Peter Bauer in 2002, Peruvian economist Hernando de Soto in 2004, Mart Laar, former Estonian Prime Minister in 2006 and a young Venezuelan student Yon Goicoechea in 2008. His wife Rose, sister of Aaron Director, with whom he initiated the Friedman Foundation for Educational Choice, served on the international selection committee. Friedman was also a recipient of the Nobel Memorial Prize in Economics.\n\nUpon Friedman's death, Harvard President Lawrence Summers called him \"The Great Liberator\" saying \"... any honest Democrat will admit that we are now all Friedmanites.\" He said Friedman's great popular contribution was \"in convincing people of the importance of allowing free markets to operate.\"\n\nIn 2013 Stephen Moore, a member of the editorial forward of \"The Wall Street Journal\" said, \"Quoting the most-revered champion of free-market economics since Adam Smith has become a little like quoting the Bible.\" He adds, \"There are sometimes multiple and conflicting interpretations.\"\n\nFriedman won the Nobel Memorial Prize in Economic Sciences, the sole recipient for 1976, \"for his achievements in the fields of consumption analysis, monetary history and theory and for his demonstration of the complexity of stabilization policy.\"\n\nFriedman once said, \"If you want to see capitalism in action, go to Hong Kong.\" He wrote in 1990 that the Hong Kong economy was perhaps the best example of a free market economy.\n\nOne month before his death, he wrote the article \"Hong Kong Wrong—What would Cowperthwaite say?\" in \"The Wall Street Journal\", criticizing Donald Tsang, the Chief Executive of Hong Kong, for abandoning \"positive noninterventionism.\"\nTsang later said he was merely changing the slogan to \"big market, small government,\" where small government is defined as less than 20% of GDP. In a debate between Tsang and his rival, Alan Leong, before the 2007 Chief Executive election, Leong introduced the topic and jokingly accused Tsang of angering Friedman to death.\n\nDuring 1975, two years after the military coup that brought military dictator President Augusto Pinochet to power and ended the government of Salvador Allende, the economy of Chile experienced a severe crisis. Friedman and Arnold Harberger accepted an invitation of a private Chilean foundation to visit Chile and speak on principles of economic freedom. He spent seven days in Chile giving a series of lectures at the Universidad Católica de Chile and the (National) University of Chile. One of the lectures was entitled \"The Fragility of Freedom\" and according to Friedman, \"dealt with precisely the threat to freedom from a centralized military government.\"\n\nIn an April 21, 1975, letter to Pinochet, Friedman considered the \"key economic problems of Chile are clearly ... inflation and the promotion of a healthy social market economy\". He stated that \"There is only one way to end inflation: by drastically reducing the rate of increase of the quantity of money ...\" and that \"... cutting government spending is by far and away the most desirable way to reduce the fiscal deficit, because it ... strengthens the private sector thereby laying the foundations for \"healthy\" economic growth\". As to how rapidly inflation should be ended, Friedman felt that \"for Chile where inflation is raging at 10–20% a month ... gradualism is not feasible. It would involve so painful an \"operation\" over so long a period that the \"patient\" would not survive.\" Choosing \"a brief period of higher unemployment ...\" was the lesser evil.. and that \"the experience of Germany, ... of Brazil ..., of the post-war adjustment in the U.S. ... all argue for \"shock treatment\"\". In the letter Friedman recommended to deliver the \"shock approach\" with \"... a package to eliminate the surprise and to relieve acute distress\" and \"... for definiteness let me sketch the contents of a package proposal ... to be taken as illustrative\" although his knowledge of Chile was \"too limited to enable [him] to be precise or comprehensive\". He listed a \"sample proposal\" of 8 monetary and fiscal measures including \"the removal of as many as obstacles as possible that now hinder the private market. For example, suspend ... the present law against discharging employees\". He closed, stating \"Such a \"shock program\" could end inflation in months\". His letter suggested that cutting spending to reduce the fiscal deficit would result in less transitional unemployment than raising taxes.\n\nSergio de Castro, a Chilean Chicago School graduate, became the nation's Minister of Finance in 1975. During his six-year tenure, foreign investment increased, restrictions were placed on striking and labor unions, and GDP rose yearly. A foreign exchange program was created between the Catholic University of Chile and the University of Chicago. Many other Chicago School alumni were appointed government posts during and after the Pinochet years; others taught its economic doctrine at Chilean universities. They became known as the Chicago Boys.\n\nFriedman did not criticize Pinochet's dictatorship at the time, nor the assassinations, illegal imprisonments, torture, or other atrocities that were well known by then.\nIn 1976 Friedman defended his unofficial adviser position with: \"I do not consider it as evil for an economist to render technical economic advice to the Chilean Government, any more than I would regard it as evil for a physician to give technical medical advice to the Chilean Government to help end a medical plague.\"\n\nFriedman defended his activity in Chile on the grounds that, in his opinion, the adoption of free market policies not only improved the economic situation of Chile but also contributed to the amelioration of Pinochet's rule and to the eventual transition to a democratic government during 1990. That idea is included in \"Capitalism and Freedom\", in which he declared that economic freedom is not only desirable in itself but is also a necessary condition for political freedom. In his 1980 documentary \"Free to Choose\", he said the following: \"Chile is not a politically free system, and I do not condone the system. But the people there are freer than the people in Communist societies because government plays a smaller role. ... The conditions of the people in the past few years has been getting better and not worse. They would be still better to get rid of the junta and to be able to have a free democratic system.\" In 1984, Friedman stated that he has \"never refrained from criticizing the political system in Chile.\" In 1991 he said: \"I have nothing good to say about the political regime that Pinochet imposed. It was a terrible political regime. The real miracle of Chile is not how well it has done economically; the real miracle of Chile is that a military junta was willing to go against its principles and support a free market regime designed by principled believers in a free market. ... In Chile, the drive for political freedom, that was generated by economic freedom and the resulting economic success, ultimately resulted in a referendum that introduced political democracy. Now, at long last, Chile has all three things: political freedom, human freedom and economic freedom. Chile will continue to be an interesting experiment to watch to see whether it can keep all three or whether, now that it has political freedom, that political freedom will tend to be used to destroy or reduce economic freedom.\" He stressed that the lectures he gave in Chile were the same lectures he later gave in China and other socialist states.\n\nDuring the 2000 PBS documentary \"The Commanding Heights\" (based on the book), Friedman continued to argue that \"free markets would undermine [Pinochet's] political centralization and political control.\", and that criticism over his role in Chile missed his main contention that freer markets resulted in freer people, and that Chile's unfree economy had caused the military government. Friedman advocated for free markets which undermined \"political centralization and political control\".\n\nFriedman visited Iceland during the autumn of 1984, met with important Icelanders and gave a lecture at the University of Iceland on the \"tyranny of the \"status quo\".\" He participated in a lively television debate on August 31, 1984 with socialist intellectuals, including Ólafur Ragnar Grímsson, who later became the president of Iceland. When they complained that a fee was charged for attending his lecture at the university and that, hitherto, lectures by visiting scholars had been free-of-charge, Friedman replied that previous lectures had not been free-of-charge in a meaningful sense: lectures always have related costs. What mattered was whether attendees or non-attendees covered those costs. Friedman thought that it was fairer that only those who attended paid. In this discussion Friedman also stated that he did not receive any money for delivering that lecture.\n\nAlthough Friedman never visited Estonia, his book \"Free to Choose\" exercised a great influence on that nation's then 32-year-old prime minister, Mart Laar, who has claimed that it was the only book on economics he had read before taking office. Laar's reforms are often credited with responsibility for transforming Estonia from an impoverished Soviet Republic to the \"Baltic Tiger.\" A prime element of Laar's program was introduction of the flat tax. Laar won the 2006 Milton Friedman Prize for Advancing Liberty, awarded by the Cato Institute.\n\nAfter 1950 Friedman was frequently invited to lecture in Britain, and by the 1970s his ideas had gained widespread attention in conservative circles. For example, he was a regular speaker at the Institute of Economic Affairs (IEA), a libertarian think tank. Conservative politician Margaret Thatcher closely followed IEA programs and ideas, and met Friedman there in 1978. He also strongly influenced Keith Joseph, who became Thatcher's senior advisor on economic affairs, as well as Alan Walters and Patrick Minford, two other key advisers. Major newspapers, including the \"Daily Telegraph,\" \"The Times,\" and \"The Financial Times\" all promulgated Friedman's monetarist ideas to British decision-makers. Friedman's ideas strongly influenced Thatcher and her allies when she became Prime Minister in 1979.\n\nAfter his death a number of obituaries and articles were written in Friedman's honor, citing him as one of the most important and influential economists of the post war era. Milton Friedman's somewhat controversial legacy in America remains strong within the conservative moment. However, some journalists and economists like Noah Smith and Scott Sumner have argued Friedman's academic legacy has been buried under his political philosophy and misinterpreted by modern conservatives.\n\nEconometrician David Hendry criticized part of Friedman's and Anna Schwartz's 1982 \"Monetary Trends\". When asked about it during an interview with Icelandic TV in 1984, Friedman said that the criticism referred to a different problem from that which he and Schwartz had tackled, and hence was irrelevant, and pointed out the lack of consequential peer review amongst econometricians on Hendry's work. In 2006, Hendry said that Friedman was guilty of \"serious errors\" of misunderstanding that meant \"the t-ratios he reported for UK money demand were overstated by nearly 100 per cent\", and said that, in a paper published in 1991 with Neil Ericsson, he had refuted \"almost every empirical claim ... made about UK money demand\" by Friedman and Schwartz. A 2004 paper updated and confirmed the validity of the Hendry–Ericsson findings through 2000.\n\nAlthough Keynesian Nobel laureate Paul Krugman praised Friedman as a \"great economist and a great man\" after Friedman's death in 2006, and acknowledged his many, widely accepted contributions to empirical economics, Krugman had been, and remains, a prominent critic of Friedman. Krugman has written that \"he slipped all too easily into claiming both that markets always work and that only markets work. It's extremely hard to find cases in which Friedman acknowledged the possibility that markets could go wrong, or that government intervention could serve a useful purpose.\" Others agree Friedman was not open enough to the possibility of market inefficiencies. Economist Noah Smith argues that while Milton Friedman made many important contributions to economic theory not all of his ideas relating to macroeconomics have entirely held up over the years and that too few people are willing to challenge them.\n\nPolitical scientist C.B. Macpherson disagreed with Friedman's historical assessment of economic freedom leading to political freedom, suggesting that political freedom actually gave way to economic freedom for property owning elites. He also challenged the notion that markets efficiently allocated resources and rejected Friedman's definition of liberty. Friedman's positivist methodological approach to economics has also been critiqued and debated. Finnish economist Uskali Mäki has argued some of his assumptions were unrealistic and vague.\n\nIn her book \"The Shock Doctrine\", author and social activist Naomi Klein criticized Friedman's economic liberalism, identifying it with the principles that guided the economic restructuring that followed the military coups in countries such as Chile and Argentina. Based on their assessments of the extent to which what she describes as neoliberal policies contributed to income disparities and inequality, both Klein and Noam Chomsky have suggested that the primary role of what they describe as neoliberalism was as an ideological cover for capital accumulation by multinational corporations.\n\nBecause of his involvement with the Pinochet government, there were international protests when Friedman was awarded the Nobel Prize in 1976. Friedman was accused of supporting the military dictatorship in Chile because of the relation of economists of the University of Chicago to Pinochet, and a controversial seven-day trip he took to Chile during March 1975 (less than two years after the coup that ended with the suicide of President Salvador Allende). Friedman answered that he never was an adviser to the dictatorship, but only gave some lectures and seminars on inflation, and met with officials, including Augusto Pinochet, while in Chile.\n\nChilean economist Orlando Letelier asserted that Pinochet's dictatorship resorted to oppression because of popular opposition to Chicago School policies in Chile. After a 1991 speech on drug legalisation, Friedman answered a question on his involvement with the Pinochet regime, saying that he was never an advisor to Pinochet (also mentioned in his 1984 Iceland interview), but that a group of his students at the University of Chicago were involved in Chile's economic reforms. Friedman credited these reforms with high levels of economic growth and with the establishment of democracy that has subsequently occurred in Chile. In October 1988, after returning from a lecture tour of China during which he had met with Zhao Ziyang, General Secretary of the Communist Party of China, Friedman wrote to \"The Stanford Daily\" asking if he should anticipate a similar \"avalanche of protests for having been willing to give advice to so evil a government? And if not, why not?\"\n\n\n\n\n"}
{"id": "19641", "url": "https://en.wikipedia.org/wiki?curid=19641", "title": "Mass media", "text": "Mass media\n\nThe mass media is a diversified collection of media technologies that reach a large audience via mass communication. The technologies through which this communication takes place include a variety of outlets.\n\nBroadcast media transmit information electronically via media such as films, radio, recorded music, or television. Digital media comprises both Internet and mobile mass communication. Internet media comprise such services as email, social media sites, websites, and Internet-based radio and television. Many other mass media outlets have an additional presence on the web, by such means as linking to or running TV ads online, or distributing QR Codes in outdoor or print media to direct mobile users to a website. In this way, they can utilise the easy accessibility and outreach capabilities the Internet affords, as thereby easily broadcast information throughout many different regions of the world simultaneously and cost-efficiently. Outdoor media transmit information via such media as AR advertising; billboards; blimps; flying billboards (signs in tow of airplanes); placards or kiosks placed inside and outside buses, commercial buildings, shops, sports stadiums, subway cars, or trains; signs; or skywriting. Print media transmit information via physical objects, such as books, comics, magazines, newspapers, or pamphlets. Event organizing and public speaking can also be considered forms of mass media.\n\nThe organizations that control these technologies, such as movie studios, publishing companies, and radio and television stations, are also known as the mass media.\n\nIn the late 20th century, mass media could be classified into eight mass media industries: books, the Internet, magazines, movies, newspapers, radio, recordings, and television. The explosion of digital communication technology in the late 20th and early 21st centuries made prominent the question: what forms of media should be classified as \"mass media\"? For example, it is controversial whether to include cell phones, computer games (such as MMORPGs), and video games in the definition. In the 2000s, a classification called the \"seven mass media\" became popular. In order of introduction, they are:\n\n\nEach mass medium has its own content types, creative artists, technicians, and business models. For example, the Internet includes blogs, podcasts, web sites, and various other technologies built atop the general distribution network. The sixth and seventh media, Internet and mobile phones, are often referred to collectively as digital media; and the fourth and fifth, radio and TV, as broadcast media. Some argue that video games have developed into a distinct mass form of media.\n\nWhile a telephone is a two-way communication device, mass media communicates to a large group. In addition, the telephone has transformed into a cell phone which is equipped with Internet access. A question arises whether this makes cell phones a mass medium or simply a device used to access a mass medium (the Internet). There is currently a system by which marketers and advertisers are able to tap into satellites, and broadcast commercials and advertisements directly to cell phones, unsolicited by the phone's user. This transmission of mass advertising to millions of people is another form of mass communication.\n\nVideo games may also be evolving into a mass medium. Video games (for example massively multiplayer online role-playing games (MMORPGs), such as \"RuneScape\") provide a common gaming experience to millions of users across the globe and convey the same messages and ideologies to all their users. Users sometimes share the experience with one another by playing online. Excluding the Internet however, it is questionable whether players of video games are sharing a common experience when they play the game individually. It is possible to discuss in great detail the events of a video game with a friend one has never played with, because the experience is identical to each. The question, then, is whether this is a form of mass communication.\n\nFive characteristics of mass communication have been identified by sociologist John Thompson of Cambridge University:\n\nThe term \"mass media\" is sometimes erroneously used as a synonym for \"mainstream media\". Mainstream media are distinguished from alternative media by their content and point of view. Alternative media are also \"mass media\" outlets in the sense that they use technology capable of reaching many people, even if the audience is often smaller than the mainstream.\n\nIn common usage, the term \"mass\" denotes not that a given number of individuals receives the products, but rather that the products are available in principle to a plurality of recipients. \n\nThe sequencing of content in a broadcast is called a schedule. With all technological endeavours a number of technical terms and slang have developed. Please see the list of broadcasting terms for a glossary of terms used.\n\nRadio and television programs are distributed over frequency bands which are highly regulated in the United States. Such regulation includes determination of the width of the bands, range, licensing, types of receivers and transmitters used, and acceptable content.\n\nCable television programs are often broadcast simultaneously with radio and television programs, but have a more limited audience. By coding signals and requiring a cable converter box at individual recipients' locations, cable also enables subscription-based channels and pay-per-view services.\n\nA broadcasting organisation may broadcast several programs simultaneously, through several channels (frequencies), for example BBC One and Two. On the other hand, two or more organisations may share a channel and each use it during a fixed part of the day, such as the Cartoon Network/Adult Swim. Digital radio and digital television may also transmit multiplexed programming, with several channels compressed into one ensemble.\n\nWhen broadcasting is done via the Internet the term webcasting is often used. In 2004, a new phenomenon occurred when a number of technologies combined to produce podcasting. Podcasting is an asynchronous broadcast/narrowcast medium. Adam Curry and his associates, the \"Podshow\", are principal proponents of podcasting.\n\nThe term 'film' encompasses motion pictures as individual projects, as well as the field in general. The name comes from the photographic film (also called filmstock), historically the primary medium for recording and displaying motion pictures. Many other terms for film exist, such as \"motion pictures\" (or just \"pictures\" and \"picture\"), \"the silver screen\", \"photoplays\", \"the cinema\", \"picture shows\", \"flicks\", and most common, \"movies\".\n\nFilms are produced by recording people and objects with cameras, or by creating them using animation techniques or special effects. Films comprise a series of individual frames, but when these images are shown in rapid succession, an illusion of motion is created. Flickering between frames is not seen because of an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after the source has been removed. Also of relevance is what causes the perception of motion: a psychological effect identified as beta movement.\n\nFilm is considered by many to be an important art form; films entertain, educate, enlighten, and inspire audiences. Any film can become a worldwide attraction, especially with the addition of dubbing or subtitles that translate the film message. Films are also artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them.\n\nA video game is a computer-controlled game in which a video display, such as a monitor or television, is the primary feedback device. The term \"computer game\" also includes games which display only text (and which can, therefore, theoretically be played on a teletypewriter) or which use other methods, such as sound or vibration, as their primary feedback device, but there are very few new games in these categories. There always must also be some sort of input device, usually in the form of button/joystick combinations (on arcade games), a keyboard and mouse/trackball combination (computer games), a controller (console games), or a combination of any of the above. Also, more esoteric devices have been used for input, e.g., the player's motion. Usually there are rules and goals, but in more open-ended games the player may be free to do whatever they like within the confines of the virtual universe.\n\nIn common usage, an \"arcade game\" refers to a game designed to be played in an establishment in which patrons pay to play on a per-use basis. A \"computer game\" or \"PC game\" refers to a game that is played on a personal computer. A \"Console game\" refers to one that is played on a device specifically designed for the use of such, while interfacing with a standard television set. A \"video game\" (or \"videogame\") has evolved into a catchall phrase that encompasses the aforementioned along with any game made for any other device, including, but not limited to, advanced calculators, mobile phones, PDAs, etc.\n\nSound recording and reproduction is the electrical or mechanical re-creation or amplification of sound, often as music. This involves the use of audio equipment such as microphones, recording devices, and loudspeakers. From early beginnings with the invention of the phonograph using purely mechanical techniques, the field has advanced with the invention of electrical recording, the mass production of the 78 record, the magnetic wire recorder followed by the tape recorder, the vinyl LP record. The invention of the compact cassette in the 1960s, followed by Sony's Walkman, gave a major boost to the mass distribution of music recordings, and the invention of digital recording and the compact disc in 1983 brought massive improvements in ruggedness and quality. The most recent developments have been in digital audio players.\n\nAn album is a collection of related audio recordings, released together to the public, usually commercially.\n\nThe term record album originated from the fact that 78 RPM Phonograph disc records were kept together in a book resembling a photo album. The first collection of records to be called an \"album\" was Tchaikovsky's \"Nutcracker Suite\", release in April 1909 as a four-disc set by Odeon records. It retailed for 16 shillings – about £15 in modern currency.\n\nA music video (also promo) is a short film or video that accompanies a complete piece of music, most commonly a song. Modern music videos were primarily made and used as a marketing device intended to promote the sale of music recordings. Although the origins of music videos go back much further, they came into their own in the 1980s, when Music Television's format was based on them. In the 1980s, the term \"rock video\" was often used to describe this form of entertainment, although the term has fallen into disuse.\n\nMusic videos can accommodate all styles of filmmaking, including animation, live action films, documentaries, and non-narrative, abstract film.\n\nThe Internet (also known simply as \"the Net\" or less precisely as \"the Web\") is a more interactive medium of mass media, and can be briefly described as \"a network of networks\". Specifically, it is the worldwide, publicly accessible network of interconnected computer networks that transmit data by packet switching using the standard Internet Protocol (IP). It consists of millions of smaller domestic, academic, business, and governmental networks, which together carry various information and services, such as email, online chat, file transfer, and the interlinked web pages and other documents of the World Wide Web.\n\nContrary to some common usage, the Internet and the World Wide Web are not synonymous: the Internet is the system of interconnected \"computer networks\", linked by copper wires, fiber-optic cables, wireless connections etc.; the Web is the contents, or the interconnected \"documents\", linked by hyperlinks and URLs. The World Wide Web is accessible through the Internet, along with many other services including e-mail, file sharing and others described below.\n\nToward the end of the 20th century, the advent of the World Wide Web marked the first era in which most individuals could have a means of exposure on a scale comparable to that of mass media. Anyone with a web site has the potential to address a global audience, although serving to high levels of web traffic is still relatively expensive. It is possible that the rise of peer-to-peer technologies may have begun the process of making the cost of bandwidth manageable. Although a vast amount of information, imagery, and commentary (i.e. \"content\") has been made available, it is often difficult to determine the authenticity and reliability of information contained in web pages (in many cases, self-published). The invention of the Internet has also allowed breaking news stories to reach around the globe within minutes. This rapid growth of instantaneous, decentralized communication is often deemed likely to change mass media and its relationship to society.\n\n\"Cross-media\" means the idea of distributing the same message through different media channels. A similar idea is expressed in the news industry as \"convergence\". Many authors understand cross-media publishing to be the ability to publish in both print and on the web without manual conversion effort. An increasing number of wireless devices with mutually incompatible data and screen formats make it even more difficult to achieve the objective \"create once, publish many\".\n\nThe Internet is quickly becoming the center of mass media. Everything is becoming accessible via the internet. Rather than picking up a newspaper, or watching the 10 o'clock news, people can log onto the internet to get the news they want, when they want it. For example, many workers listen to the radio through the Internet while sitting at their desk.\n\nEven the education system relies on the Internet. Teachers can contact the entire class by sending one e-mail. They may have web pages on which students can get another copy of the class outline or assignments. Some classes have class blogs in which students are required to post weekly, with students graded on their contributions.\n\nBlogging, too, has become a pervasive form of media. A blog is a website, usually maintained by an individual, with regular entries of commentary, descriptions of events, or interactive media such as images or video. Entries are commonly displayed in reverse chronological order, with most recent posts shown on top. Many blogs provide commentary or news on a particular subject; others function as more personal online diaries. A typical blog combines text, images and other graphics, and links to other blogs, web pages, and related media. The ability for readers to leave comments in an interactive format is an important part of many blogs. Most blogs are primarily textual, although some focus on art (artlog), photographs (photoblog), sketchblog, videos (vlog), music (MP3 blog), audio (podcasting) are part of a wider network of social media. Microblogging is another type of blogging which consists of blogs with very short posts.\n\nRSS is a format for syndicating news and the content of news-like sites, including major news sites like Wired, news-oriented community sites like Slashdot, and personal blogs. It is a family of Web feed formats used to publish frequently updated content such as blog entries, news headlines, and podcasts. An RSS document (which is called a \"feed\" or \"web feed\" or \"channel\") contains either a summary of content from an associated web site or the full text. RSS makes it possible for people to keep up with web sites in an automated manner that can be piped into special programs or filtered displays.\n\nA podcast is a series of digital-media files which are distributed over the Internet using syndication feeds for playback on portable media players and computers. The term podcast, like broadcast, can refer either to the series of content itself or to the method by which it is syndicated; the latter is also called podcasting. The host or author of a podcast is often called a podcaster.\n\nMobile phones were introduced in Japan in 1979 but became a mass media only in 1998 when the first downloadable ringing tones were introduced in Finland. Soon most forms of media content were introduced on mobile phones, tablets and other portable devices, and today the total value of media consumed on mobile vastly exceeds that of internet content, and was worth over 31 billion dollars in 2007 (source Informa). The mobile media content includes over 8 billion dollars worth of mobile music (ringing tones, ringback tones, truetones, MP3 files, karaoke, music videos, music streaming services etc.); over 5 billion dollars worth of mobile gaming; and various news, entertainment and advertising services. In Japan mobile phone books are so popular that five of the ten best-selling printed books were originally released as mobile phone books.\n\nSimilar to the internet, mobile is also an interactive media, but has far wider reach, with 3.3 billion mobile phone users at the end of 2007 to 1.3 billion internet users (source ITU). Like email on the internet, the top application on mobile is also a personal messaging service, but SMS text messaging is used by over 2.4 billion people. Practically all internet services and applications exist or have similar cousins on mobile, from search to multiplayer games to virtual worlds to blogs. Mobile has several unique benefits which many mobile media pundits claim make mobile a more powerful media than either TV or the internet, starting with mobile being permanently carried and always connected. Mobile has the best audience accuracy and is the only mass media with a built-in payment channel available to every user without any credit cards or PayPal accounts or even an age limit. Mobile is often called the 7th Mass Medium and either the fourth screen (if counting cinema, TV and PC screens) or the third screen (counting only TV and PC).\n\nA magazine is a periodical publication containing a variety of articles, generally financed by advertising or purchase by readers.\n\nMagazines are typically published weekly, biweekly, monthly, bimonthly or quarterly, with a date on the cover that is in advance of the date it is actually published. They are often printed in color on coated paper, and are bound with a soft cover.\n\nMagazines fall into two broad categories: consumer magazines and business magazines. In practice, magazines are a subset of , distinct from those periodicals produced by scientific, artistic, academic or special interest publishers which are subscription-only, more expensive, narrowly limited in circulation, and often have little or no advertising.\n\nMagazines can be classified as:\n\nA newspaper is a publication containing news and information and advertising, usually printed on low-cost paper called newsprint. It may be general or special interest, most often published daily or weekly. The most important function of newspapers is to inform the public of significant events. Local newspapers inform local communities and include advertisements from local businesses and services, while national newspapers tend to focus on a theme, which can be exampled with \"The Wall Street Journal\" as they offer news on finance and business related-topics. The first printed newspaper was published in 1605, and the form has thrived even in the face of competition from technologies such as radio and television. Recent developments on the Internet are posing major threats to its business model, however. Paid circulation is declining in most countries, and advertising revenue, which makes up the bulk of a newspaper's income, is shifting from print to online; some commentators, nevertheless, point out that historically new media such as radio and television did not entirely supplant existing.\n\nThe internet has challenged the press as an alternative source of information and opinion but has also provided a new platform for newspaper organizations to reach new audiences. According to the World Trends Report, between 2012 and 2016, print newspaper circulation continued to fall in almost all regions, with the exception of Asia and the Pacific, where the dramatic increase in sales in a few select countries has offset falls in historically strong Asian markets such as Japan and the Republic of Korea. Most notably, between 2012 and 2016, India’s print circulation grew by 89 per cent.\n\nOutdoor media is a form of mass media which comprises billboards, signs, placards placed inside and outside commercial buildings/objects like shops/buses, flying billboards (signs in tow of airplanes), blimps, skywriting, AR Advertising. Many commercial advertisers use this form of mass media when advertising in sports stadiums. Tobacco and alcohol manufacturers used billboards and other outdoor media extensively. However, in 1998, the Master Settlement Agreement between the US and the tobacco industries prohibited the billboard advertising of cigarettes. In a 1994 Chicago-based study, Diana Hackbarth and her colleagues revealed how tobacco- and alcohol-based billboards were concentrated in poor neighbourhoods. In other urban centers, alcohol and tobacco billboards were much more concentrated in African-American neighborhoods than in white neighborhoods.\n\nMass media encompasses much more than just news, although it is sometimes misunderstood in this way. It can be used for various purposes:\n\nJournalism is the discipline of collecting, analyzing, verifying and presenting information regarding current events, trends, issues and people. Those who practice journalism are known as journalists.\n\nNews-oriented journalism is sometimes described as the \"first rough draft of history\" (attributed to Phil Graham), because journalists often record important events, producing news articles on short deadlines. While under pressure to be first with their stories, news media organizations usually edit and proofread their reports prior to publication, adhering to each organization's standards of accuracy, quality and style. Many news organizations claim proud traditions of holding government officials and institutions accountable to the public, while media critics have raised questions about holding the press itself accountable to the standards of professional journalism.\n\nPublic relations is the art and science of managing communication between an organization and its key publics to build, manage and sustain its positive image. Examples include:\n\nPublishing is the industry concerned with the production of literature or information – the activity of making information available for public view. In some cases, authors may be their own publishers.\n\nTraditionally, the term refers to the distribution of printed works such as books and newspapers. With the advent of digital information systems and the Internet, the scope of publishing has expanded to include websites, blogs, and the like.\n\nAs a business, publishing includes the development, marketing, production, and distribution of newspapers, magazines, books, literary works, musical works, software, other works dealing with information.\n\nPublication is also important as a legal concept; (1) as the process of giving formal notice to the world of a significant intention, for example, to marry or enter bankruptcy, and; (2) as the essential precondition of being able to claim defamation; that is, the alleged libel must have been published.\n\nA software publisher is a publishing company in the software industry between the developer and the distributor. In some companies, two or all three of these roles may be combined (and indeed, may reside in a single person, especially in the case of shareware).\n\nSoftware publishers often license software from developers with specific limitations, such as a time limit or geographical region. The terms of licensing vary enormously, and are typically secret.\n\nDevelopers may use publishers to reach larger or foreign markets, or to avoid focussing on marketing. Or publishers may use developers to create software to meet a market need that the publisher has identified.\n\nA YouTuber is anyone who has made their fame from creating and promoting videos on the public video-sharing site, YouTube. Many YouTube celebrities have made a profession from their site through sponsorships, advertisements, product placement, and network support.\n\nThe history of mass media can be traced back to the days when dramas were performed in various ancient cultures. This was the first time when a form of media was \"broadcast\" to a wider audience. The first dated printed book known is the \"Diamond Sutra\", printed in China in 868 AD, although it is clear that books were printed earlier. Movable clay type was invented in 1041 in China. However, due to the slow spread of literacy to the masses in China, and the relatively high cost of paper there, the earliest printed mass-medium was probably European popular prints from about 1400. Although these were produced in huge numbers, very few early examples survive, and even most known to be printed before about 1600 have not survived. The term \"mass media\" was coined with the creation of print media, which is notable for being the first example of mass media, as we use the term today. This form of media started in Europe in the Middle Ages.\n\nJohannes Gutenberg's invention of the printing press allowed the mass production of books to sweep the nation. He printed the first book, a Latin Bible, on a printing press with movable type in 1453. The invention of the printing press gave rise to some of the first forms of mass communication, by enabling the publication of books and newspapers on a scale much larger than was previously possible. The invention also transformed the way the world received printed materials, although books remained too expensive really to be called a mass-medium for at least a century after that. Newspapers developed from about 1612, with the first example in English in 1620; but they took until the 19th century to reach a mass-audience directly. The first high-circulation newspapers arose in London in the early 1800s, such as The Times, and were made possible by the invention of high-speed rotary steam printing presses, and railroads which allowed large-scale distribution over wide geographical areas. The increase in circulation, however, led to a decline in feedback and interactivity from the readership, making newspapers a more one-way medium.\n\nThe phrase \"the media\" began to be used in the 1920s. The notion of \"mass media\" was generally restricted to print media up until the post-Second World War, when radio, television and video were introduced. The audio-visual facilities became very popular, because they provided both information and entertainment, because the colour and sound engaged the viewers/listeners and because it was easier for the general public to passively watch TV or listen to the radio than to actively read. In recent times, the Internet become the latest and most popular mass medium. Information has become readily available through websites, and easily accessible through search engines. One can do many activities at the same time, such as playing games, listening to music, and social networking, irrespective of location. Whilst other forms of mass media are restricted in the type of information they can offer, the internet comprises a large percentage of the sum of human knowledge through such things as Google Books. Modern day mass media includes the internet, mobile phones, blogs, podcasts and RSS feeds.\n\nDuring the 20th century, the growth of mass media was driven by technology, including that which allowed much duplication of material. Physical duplication technologies such as printing, record pressing and film duplication allowed the duplication of books, newspapers and movies at low prices to huge audiences. Radio and television allowed the electronic duplication of information for the first time. Mass media had the economics of linear replication: a single work could make money. An example of Riel and Neil's theory. proportional to the number of copies sold, and as volumes went up, unit costs went down, increasing profit margins further. Vast fortunes were to be made in mass media. In a democratic society, the media can serve the electorate about issues regarding government and corporate entities (see Media influence). Some consider the concentration of media ownership to be a threat to democracy.\n\nBetween 1985 and 2018 about 76,720 deals have been announced in the Media industry. This sums up to an overall value of around 5,634 bil USD. There have been three major waves of M&A in the Mass Media Sector (2000, 2007 and 2015), while the most active year in terms of numbers was 2007 with around 3,808 deals. The U.S. is the most prominent country in Media M&A with 41 of the top 50 deals having an acquiror from the United States.\n\nThe largest deal in history was the acquisition of Time Warner by America Online Inc for 164,746.86 mil USD.\n\nLimited-effects theory, originally tested in the 1940s and 1950s, considers that because people usually choose what media to interact with based on what they already believe, media exerts a negligible influence. Class-dominant theory argues that the media reflects and projects the view of a minority elite, which controls it. Culturalist theory, which was developed in the 1980s and 1990s, combines the other two theories and claims that people interact with media to create their own meanings out of the images and messages they receive. This theory states that audience members play an active, rather than passive role in relation to mass media.\nIn an article entitled \"Mass Media Influence on Society\", rayuso argues that the media \"in the US\" is dominated by five major companies (Time Warner, VIACOM, Vivendi Universal, Walt Disney and News Corp) which own 95% of all mass media including theme parks, movie studios, television and radio broadcast networks and programing, video news, sports entertainment, telecommunications, wireless phones, video games software, electronic media and music companies. Whilst historically, there was more diversity in companies, they have recently merged to form an elite which have the power to shape the opinion and beliefs of people. People buy after seeing thousands of advertisements by various companies in TV, newspapers or magazines, which are able to affect their purchasing decisions. The definition of what is acceptable by society is dictated by the media. This power can be used for good, for example encouraging children to play sport. However, it can also be used for bad, for example children being influenced by cigars smoked by film stars, their exposure to sex images, their exposure to images of violence and their exposure to junk food ads. The documentary \"Super Size Me\" describes how companies like McDonald's have been sued in the past, the plaintiffs claiming that it was the fault of their liminal and subliminal advertising that \"forced\" them to purchase the product. The Barbie and Ken dolls of the 1950s are sometimes cited as the main cause for the obsession in modern-day society for women to be skinny and men to be buff. After the attacks of 9/11, the media gave extensive coverage of the event and exposed Osama Bin Laden's guilt for the attack, information they were told by the authorities. This shaped the public opinion to support the war on terrorism, and later, the war on Iraq. A main concern is that due to this immense power of the mass media (being able to drive the public opinion), media receiving inaccurate information could cause the public opinion to support the wrong cause.\n\nIn his book The Commercialization of American Culture, Matthew P. McAllister says that \"a well-developed media system, informing and teaching its citizens, helps democracy move toward its ideal state.\"\n\nIn 1997, J. R. Finnegan Jr. and K. Viswanath identified 3 main effects or functions of mass media:\n\nSince the 1950s, when cinema, radio and TV began to be the primary or the only source of information for a larger and larger percentage of the population, these media began to be considered as central instruments of mass control. Up to the point that it emerged the idea that when a country has reached a high level of industrialization, the country itself \"belongs to the person who controls communications.\"\n\nMass media play a significant role in shaping public perceptions on a variety of important issues, both through the information that is dispensed through them, and through the interpretations they place upon this information. They also play a large role in shaping modern culture, by selecting and portraying a particular set of beliefs, values, and traditions (an entire way of life), as reality. That is, by portraying a certain interpretation of reality, they shape reality to be more in line with that interpretation. Mass media also play a crucial role in the spread of civil unrest activities such as anti-government demonstrations, riots, and general strikes. That is, the use of radio and television receivers has made the unrest influence among cities not only by the geographic location of cities, but also by proximity within the mass media distribution networks.\n\nMass media sources, through theories like framing and agenda-setting, can affect the scope of a story as particular facts and information are highlighted (Media influence). This can directly correlate with how individuals may perceive certain groups of people, as the only media coverage a person receives can be very limited and may not reflect the whole story or situation; stories are often covered to reflect a particular perspective to target a specific demographic.\n\nAccording to Stephen Balkaran, an Instructor of Political Science and African American Studies at Central Connecticut State University, mass media has played a large role in the way white Americans perceive African-Americans. The media focus on African-American in the contexts of crime, drug use, gang violence, and other forms of anti-social behavior has resulted in a distorted and harmful public perception of African-Americans. African-Americans have been subjected to oppression and discrimination for the past few hundred years. According to Stephen Balkaran in his article \"Mass Media and Racism\": \"The media has played a key role in perpetuating the effects of this historical oppression and in contributing to African-Americans' continuing status as second-class citizens\". This has resulted in an uncertainty among white Americans as to what the genuine nature of African-Americans really is. Despite the resulting racial divide, the fact that these people are undeniably American has \"raised doubts about the white man's value system\". This means that there is a somewhat \"troubling suspicion\" among some Americans that their white America is tainted by the black influence. Mass media as well as propaganda tend to reinforce or introduce stereotypes to the general public.\n\nLack of local or specific topical focus is a common criticism of mass media. A mass news media outlet is often forced to cover national and international news due to it having to cater for and be relevant for a wide demographic. As such, it has to skip over many interesting or important local stories because they simply do not interest the large majority of their viewers. An example given by the website WiseGeek is that \"the residents of a community might view their fight against development as critical, but the story would only attract the attention of the mass media if the fight became controversial or if precedents of some form were set\".\n\nThe term \"mass\" suggests that the recipients of media products constitute a vast sea of passive, undifferentiated individuals. This is an image associated with some earlier critiques of \"mass culture\" and mass society which generally assumed that the development of mass communication has had a largely negative impact on modern social life, creating a kind of bland and homogeneous culture which entertains individuals without challenging them. However, interactive digital media have also been seen to challenge the read-only paradigm of earlier broadcast media.\n\nWhilst some refer to the mass media as \"opiate of the masses\", others argue that is a vital aspect of human societies. By understanding mass media, one is then able to analyse and find a deeper understanding of one's population and culture. This valuable and powerful ability is one reason why the field of media studies is popular. As WiseGeek says, \"watching, reading, and interacting with a nation's mass media can provide clues into how people think, especially if a diverse assortment of mass media sources are perused\".\n\nSince the 1950s, in the countries that have reached a high level of industrialization, the mass media of cinema, radio and TV have a key role in political power.\n\nContemporary research demonstrates an increasing level of concentration of media ownership, with many media industries already highly concentrated and dominated by a very small number of firms.\n\nCriticism\n\nWhen the study of mass media began the media was compiled of only mass media which is a very different media system than the social media empire of the 21st-century experiences. With this in mind, there are critiques that mass media no longer exists, or at least that it doesn't exist in the same form as it once did. This original form of mass media put filters on what the general public would be exposed to in regards to \"news\" something that is harder to do in a society of social media.\n\nTheorist Lance Bennett explains that excluding a few major events in recent history, it is uncommon for a group big enough to be labeled a mass, to be watching the same news via the same medium of mass production. Bennett's critique of 21st Century mass media argues that today it is more common for a group of people to be receiving different news stories, from completely different sources, and thus, mass media has been re-invented. As discussed above, filters would have been applied to original mass medias when the journalists decided what would or wouldn't be printed.\n\nSocial Media is a large contributor to the change from mass media to a new paradigm because through social media what is mass communication and what is interpersonal communication is confused. Interpersonal/niche communication is an exchange of information and information in a specific genre. In this form of communication, smaller groups of people are consuming news/information/opinions. In contrast, mass media in its original form is not restricted by genre and it is being consumed by the masses.\n\n\n\n\n"}
{"id": "19643", "url": "https://en.wikipedia.org/wiki?curid=19643", "title": "Mahabharata", "text": "Mahabharata\n\nThe Mahābhārata (, ; , \"\", ) is one of the two major Sanskrit epics of ancient India, the other being the \"Rāmāyaṇa\". It narrates the struggle between two groups of cousins in the Kurukshetra War and the fates of the Kaurava and the Pāṇḍava princes and their succession. Along with the \"Rāmāyaṇa\", it forms the Hindu Itihasa.\n\nThe \"Mahābhārata\" is an epic legendary narrative of the Kurukṣetra War and the fates of the Kaurava and the Pāṇḍava princes. It also contains philosophical and devotional material, such as a discussion of the four \"goals of life\" or \"puruṣārtha\" (12.161). Among the principal works and stories in the \"Mahābhārata\" are the \"Bhagavad Gita\", the story of Damayanti, an abbreviated version of the \"Rāmāyaṇa\", and the story of Ṛṣyasringa, often considered as works in their own right.\n\nTraditionally, the authorship of the \"Mahābhārata\" is attributed to Vyāsa. There have been many attempts to unravel its historical growth and compositional layers. The oldest preserved parts of the text are thought to be not much older than around 400 BCE, though the origins of the epic probably fall between the 8th and 9th centuries BCE. The text probably reached its final form by the early Gupta period (c. 4th century CE). According to the \"Mahābhārata\" itself, the tale is extended from a shorter version of 24,000 verses called simply \"Bhārata\".\n\nThe \"Mahābhārata\" is the longest epic poem known and has been described as \"the longest poem ever written\". Its longest version consists of over 100,000 \"śloka\" or over 200,000 individual verse lines (each shloka is a couplet), and long prose passages. At about 1.8 million words in total, the \"Mahābhārata\" is roughly ten times the length of the \"Iliad\" and the \"Odyssey\" combined, or about four times the length of the \"Rāmāyaṇa\". W. J. Johnson has compared the importance of the \"Mahābhārata\" in the context of world civilization to that of the Bible, the works of William Shakespeare, the works of Homer, Greek drama, or the Quran. Within the Indian tradition it is sometimes called the Fifth Veda.\n\nThe epic is traditionally ascribed to the sage Vyāsa, who is also a major character in the epic. Vyāsa described it as being \"itihāsa\" (history). He also describes the Guru-shishya parampara, which traces all great teachers and their students of the Vedic times.\n\nThe first section of the Mahābhārata states that it was Gaṇeśa who wrote down the text to Vyasa's dictation.\n\nThe epic employs the story within a story structure, otherwise known as frametales, popular in many Indian religious and non-religious works. It is first recited at \"Takshashila\" by the sage Vaiśampāyana, a disciple of Vyāsa, to the King Janamejaya who is the great-grandson of the Pāṇḍava prince Arjuna. The story is then recited again by a professional storyteller named Ugraśrava Sauti, many years later, to an assemblage of sages performing the 12-year sacrifice for the king Saunaka Kulapati in the Naimiśa Forest.\n\nThe text was described by some early 20th-century western Indologists as unstructured and chaotic. Hermann Oldenberg supposed that the original poem must once have carried an immense \"tragic force\" but dismissed the full text as a \"horrible chaos.\" Moritz Winternitz (\"Geschichte der indischen Literatur\" 1909) considered that \"only unpoetical theologists and clumsy scribes\" could have lumped the parts of disparate origin into an unordered whole. \n\nResearch on the Mahābhārata has put an enormous effort into recognizing and dating layers within the text. Some elements of the present Mahābhārata can be traced back to Vedic times. The background to the Mahābhārata suggests the origin of the epic occurs \"after the very early Vedic period\" and before \"the first Indian 'empire' was to rise in the third century B.C.\" That this is \"a date not too far removed from the 8th or 9th century B.C.\" is likely. Mahābhārata started as an orally-transmitted tale of the charioteer bards. It is generally agreed that \"Unlike the Vedas, which have to be preserved letter-perfect, the epic was a popular work whose reciters would inevitably conform to changes in language and style,\" so the earliest 'surviving' components of this dynamic text are believed to be no older than the earliest 'external' references we have to the epic, which may include an allusion in Panini's 4th century BCE grammar Aṣṭādhyāyī 4:2:56. It is estimated that the Sanskrit text probably reached something of a \"final form\" by the early Gupta period (about the 4th century CE). Vishnu Sukthankar, editor of the first great critical edition of the \"Mahābhārata\", commented: \"It is useless to think of reconstructing a fluid text in a literally original shape, on the basis of an archetype and a \"stemma codicum\". What then is possible? Our objective can only be to reconstruct \"the oldest form of the text which it is possible to reach\" on the basis of the manuscript material available.\" That manuscript evidence is somewhat late, given its material composition and the climate of India, but it is very extensive.\n\nThe Mahābhārata itself (1.1.61) distinguishes a core portion of 24,000 verses: the \"Bhārata\" proper, as opposed to additional secondary material, while the \"Aśvalāyana Gṛhyasūtra\" (3.4.4) makes a similar distinction. At least three redactions of the text are commonly recognized: \"Jaya\" (Victory) with 8,800 verses attributed to Vyāsa, \"Bhārata\" with 24,000 verses as recited by Vaiśampāyana, and finally the Mahābhārata as recited by Ugraśrava Sauti with over 100,000 verses. However, some scholars, such as John Brockington, argue that \"Jaya\" and \"Bharata\" refer to the same text, and ascribe the theory of \"Jaya\" with 8,800 verses to a misreading of a verse in Ā\"diparvan\" (1.1.81).\nThe redaction of this large body of text was carried out after formal principles, emphasizing the numbers 18 and 12. The addition of the latest parts may be dated by the absence of the \"Anuśāsana-parva\" and the \"Virāta parva\" from the \"Spitzer manuscript\". The oldest surviving Sanskrit text dates to the Kushan Period (200 CE).\n\nAccording to what one character says at Mbh. 1.1.50, there were three versions of the epic, beginning with \"Manu\" (1.1.27), \"Astika\" (1.3, sub-parva 5) or \"Vasu\" (1.57), respectively. These versions would correspond to the addition of one and then another 'frame' settings of dialogues. The \"Vasu\" version would omit the frame settings and begin with the account of the birth of Vyasa. The \"astika\" version would add the \"sarpasattra\" and \"aśvamedha\" material from Brahmanical literature, introduce the name \"Mahābhārata\", and identify Vyāsa as the work's author. The redactors of these additions were probably Pāñcarātrin scholars who according to Oberlies (1998) likely retained control over the text until its final redaction. Mention of the Huna in the \"Bhīṣma-parva\" however appears to imply that this parva may have been edited around the 4th century.\nThe Ādi-parva includes the snake sacrifice (\"sarpasattra\") of Janamejaya, explaining its motivation, detailing why all snakes in existence were intended to be destroyed, and why in spite of this, there are still snakes in existence. This \"sarpasattra\" material was often considered an independent tale added to a version of the Mahābhārata by \"thematic attraction\" (Minkowski 1991), and considered to have a particularly close connection to Vedic (Brahmana) literature. The Pañcavimśa Brahmana (at 25.15.3) enumerates the officiant priests of a \"sarpasattra\" among whom the names Dhṛtarāṣtra and Janamejaya, two main characters of the \"Mahābhārata\"'s \"sarpasattra\", as well as Takṣaka, the name of a snake in the \"Mahābhārata\", occur.\n\nThe \"Suparṇākhyāna\", a late Vedic period poem considered to be among the \"earliest traces of epic poetry in India,\" is an older, shorter precursor to the expanded legend of Garuda that is included in the \"Āstīka Parva\", within the \"Ādi Parva\" of the \"Mahābhārata\".\n\nThe earliest known references to the Mahābhārata and its core \"Bhārata\" date to the \"Aṣṭādhyāyī\" (sutra 6.2.38) of Pāṇini (\"fl.\" 4th century BCE) and in the \"Aśvalāyana Gṛhyasūtra\" (3.4.4). This may mean the core 24,000 verses, known as the \"Bhārata\", as well as an early version of the extended \"Mahābhārata\", were composed by the 4th century BCE. A report by the Greek writer Dio Chrysostom (c. 40 - c. 120 CE) about Homer's poetry being sung even in India seems to imply that the \"Iliad\" had been translated into Sanskrit. However, Indian scholars have, in general, taken this as evidence for the existence of a Mahābhārata at this date, whose episodes Dio or his sources identify with the story of the \"Iliad\".\n\nSeveral stories within the Mahābhārata took on separate identities of their own in Classical Sanskrit literature. For instance, Abhijñānaśākuntala by the renowned Sanskrit poet Kālidāsa (c. 400 CE), believed to have lived in the era of the Gupta dynasty, is based on a story that is the precursor to the \"Mahābhārata\". Urubhaṅga, a Sanskrit play written by Bhāsa who is believed to have lived before Kālidāsa, is based on the slaying of Duryodhana by the splitting of his thighs by Bhīma.\n\nThe copper-plate inscription of the Maharaja Sharvanatha (533–534 CE) from Khoh (Satna District, Madhya Pradesh) describes the Mahābhārata as a \"collection of 100,000 verses\" (\"śata-sahasri saṃhitā\").\n\nThe division into 18 parvas is as follows:\nThe historicity of the Kurukshetra War is unclear. Many historians estimate the date of the Kurukshetra war to Iron Age India of the 10th century BCE. The setting of the epic has a historical precedent in Iron Age (Vedic) India, where the Kuru kingdom was the center of political power during roughly 1200 to 800 BCE. A dynastic conflict of the period could have been the inspiration for the \"Jaya\", the foundation on which the Mahābhārata corpus was built, with a climactic battle eventually coming to be viewed as an epochal event.\n\nPuranic literature presents genealogical lists associated with the Mahābhārata narrative. The evidence of the Puranas is of two kinds. Of the first kind, there is the direct statement that there were 1015 (or 1050) years between the birth of Parikshit (Arjuna's grandson) and the accession of Mahapadma Nanda (400-329 BCE), which would yield an estimate of about 1400 BCE for the Bharata battle. However, this would imply improbably long reigns on average for the kings listed in the genealogies.\nOf the second kind are analyses of parallel genealogies in the Puranas between the times of Adhisimakrishna (Parikshit's great-grandson) and Mahapadma Nanda. Pargiter accordingly estimated 26 generations by averaging 10 different dynastic lists and, assuming 18 years for the average duration of a reign, arrived at an estimate of 850 BCE for Adhisimakrishna, and thus approximately 950 BCE for the Bharata battle.\nB. B. Lal used the same approach with a more conservative assumption of the average reign to estimate a date of 836 BCE, and correlated this with archaeological evidence from Painted Grey Ware (PGW) sites, the association being strong between PGW artifacts and places mentioned in the epic. John Keay confirm this and also gives 950 BCE for the Bharata battle.\n\nAttempts to date the events using methods of archaeoastronomy have produced, depending on which passages are chosen and how they are interpreted, estimates ranging from the late 4th to the mid-2nd millennium BCE. The late 4th-millennium date has a precedent in the calculation of the Kaliyuga epoch, based on planetary conjunctions, by Aryabhata (6th century). Aryabhata's date of 18 February 3102 BCE for Mahābhārata war has become widespread in Indian tradition. Some sources mark this as the disappearance of Krishna from earth. The Aihole inscription of Pulikeshi II, dated to Saka 556 = 634 CE, claims that 3735 years have elapsed since the Bharata battle, putting the date of Mahābhārata war at 3137 BCE.\nAnother traditional school of astronomers and historians, represented by Vriddha-Garga, Varahamihira (author of the \"Brhatsamhita\") and Kalhana (author of the \"Rajatarangini\"), place the Bharata war 653 years after the Kaliyuga epoch, corresponding to 2449 BCE.\n\nThe core story of the work is that of a dynastic struggle for the throne of Hastinapura, the kingdom ruled by the Kuru clan. The two collateral branches of the family that participate in the struggle are the Kaurava and the Pandava. Although the Kaurava is the senior branch of the family, Duryodhana, the eldest Kaurava, is younger than Yudhishthira, the eldest Pandava. Both Duryodhana and Yudhishthira claim to be first in line to inherit the throne.\n\nThe struggle culminates in the great battle of Kurukshetra, in which the Pandavas are ultimately victorious. The battle produces complex conflicts of kinship and friendship, instances of family loyalty and duty taking precedence over what is right, as well as the converse.\n\nThe Mahābhārata itself ends with the death of Krishna, and the subsequent end of his dynasty and ascent of the Pandava brothers to heaven. It also marks the beginning of the Hindu age of Kali Yuga, the fourth and final age of humankind, in which great values and noble ideas have crumbled, and people are heading towards the complete dissolution of right action, morality and virtue.\n\nKing Janamejaya's ancestor Shantanu, the king of Hastinapura, has a short-lived marriage with the goddess Ganga and has a son, Devavrata (later to be called Bhishma, a great warrior), who becomes the heir apparent. Many years later, when King Shantanu goes hunting, he sees Satyavati, the daughter of the chief of fisherman, and asks her father for her hand. Her father refuses to consent to the marriage unless Shantanu promises to make any future son of Satyavati the king upon his death. To resolve his father's dilemma, Devavrata agrees to relinquish his right to the throne. As the fisherman is not sure about the prince's children honouring the promise, Devavrata also takes a vow of lifelong celibacy to guarantee his father's promise.\n\nShantanu has two sons by Satyavati, Chitrāngada and Vichitravirya. Upon Shantanu's death, Chitrangada becomes king. He lives a very short uneventful life and dies. Vichitravirya, the younger son, rules Hastinapura. Meanwhile, the King of Kāśī arranges a swayamvara for his three daughters, neglecting to invite the royal family of Hastinapur. In order to arrange the marriage of young Vichitravirya, Bhishma attends the swayamvara of the three princesses Amba, Ambika and Ambalika, uninvited, and proceeds to abduct them. Ambika and Ambalika consent to be married to Vichitravirya.\n\nThe oldest princess Amba, however, informs Bhishma that she wishes to marry king of Shalva whom Bhishma defeated at their swayamvara. Bhishma lets her leave to marry king of Shalva, but Shalva refuses to marry her, still smarting at his humiliation at the hands of Bhishma. Amba then returns to marry Bhishma but he refuses due to his vow of celibacy. Amba becomes enraged and becomes Bhishma's bitter enemy, holding him responsible for her plight. Later she is reborn to King Drupada as Shikhandi (or Shikhandini) and causes Bhishma's fall, with the help of Arjuna, in the battle of Kurukshetra.\n\nWhen Vichitravirya dies young without any heirs, Satyavati asks her first son Vyasa to father children with the widows. The eldest, Ambika, shuts her eyes when she sees him, and so her son Dhritarashtra is born blind. Ambalika turns pale and bloodless upon seeing him, and thus her son Pandu is born pale and unhealthy (the term Pandu may also mean 'jaundiced'). Due to the physical challenges of the first two children, Satyavati asks Vyasa to try once again. However, Ambika and Ambalika send their maid instead, to Vyasa's room. Vyasa fathers a third son, Vidura, by the maid. He is born healthy and grows up to be one of the wisest characters in the \"Mahabharata\". He serves as Prime Minister (Mahamantri or Mahatma) to King Pandu and King Dhritarashtra.\n\nWhen the princes grow up, Dhritarashtra is about to be crowned king by Bhishma when Vidura intervenes and uses his knowledge of politics to assert that a blind person cannot be king. This is because a blind man cannot control and protect his subjects. The throne is then given to Pandu because of Dhritarashtra's blindness. Pandu marries twice, to Kunti and Madri. Dhritarashtra marries Gandhari, a princess from Gandhara, who blindfolds herself so that she may feel the pain that her husband feels. Her brother Shakuni is enraged by this and vows to take revenge on the Kuru family. One day, when Pandu is relaxing in the forest, he hears the sound of a wild animal. He shoots an arrow in the direction of the sound. However the arrow hits the sage Kindama, who curses him that if he engages in a sexual act, he will die. Pandu then retires to the forest along with his two wives, and his brother Dhritarashtra rules thereafter, despite his blindness.\n\nPandu's older queen Kunti, however, had been given a boon by Sage Durvasa that she could invoke any god using a special mantra. Kunti uses this boon to ask Dharma the god of justice, Vayu the god of the wind, and Indra the lord of the heavens for sons. She gives birth to three sons, Yudhishthira, Bhima, and Arjuna, through these gods. Kunti shares her mantra with the younger queen Madri, who bears the twins Nakula and Sahadeva through the Ashwini twins. However, Pandu and Madri indulge in sex, and Pandu dies. Madri Commits Sati out of remorse. Kunti raises the five brothers, who are from then on usually referred to as the Pandava brothers.\n\nDhritarashtra has a hundred sons through Gandhari, all born after the birth of Yudhishthira. These are the Kaurava brothers, the eldest being Duryodhana, and the second Dushasana. Other Kaurava brothers were Vikarna and Sukarna. The rivalry and enmity between them and the Pandava brothers, from their youth and into manhood, leads to the Kurukshetra war.\n\nAfter the deaths of their mother (Madri) and father (Pandu), the Pandavas and their mother Kunti return to the palace of Hastinapur. Yudhishthira is made Crown Prince by Dhritarashtra, under considerable pressure from his courtiers. Dhritarashtra wanted his own son Duryodhana to become king and lets his ambition get in the way of preserving justice.\n\nShakuni, Duryodhana and Dusasana plot to get rid of the Pandavas. Shakuni calls the architect Purochana to build a palace out of flammable materials like lac and ghee. He then arranges for the Pandavas and the Queen Mother Kunti to stay there, with the intention of setting it alight. However, the Pandavas are warned by their wise uncle, Vidura, who sends them a miner to dig a tunnel. They are able to escape to safety and go into hiding. During this time Bhima marries a rakshashi Hidimba and has a son Ghatotkachh. Back in Hastinapur, the Pandavas and Kunti are presumed dead.\n\nWhilst they were in hiding the Pandavas learn of a swayamvara which is taking place for the hand of the Pāñcāla princess Draupadī. The Pandavas disguised as Brahmins come to witness the event. Meanwhile Krishna who has already befriended Draupadi, tells her to look out for Arjuna (though now believed to be dead). The task was to string a mighty steel bow and shoot a target on the ceiling, which was the eye of a moving artificial fish, while looking at its reflection in oil below, a feat only Karna, Arjuna and Krishna himself could perform. After all the princes fail, many being unable to lift the bow Karna proceeds to the attempt but is interrupted by Draupadi refusing to marry a sut putra. After this the swayamvara is opened to the brahmins leading Arjuna to win the contest and marry Draupadi. The Pandavas return home and inform their meditating mother that Arjuna has won a competition and to look at what they have brought back. Without looking, Kunti asks them to share whatever Arjuna has won amongst themselves. Thus, Draupadi ends up being the wife of all five brothers.\n\nAfter the wedding, the Pandava brothers are invited back to Hastinapura. The Kuru family elders and relatives negotiate and broker a split of the kingdom, with the Pandavas obtaining and demanding only a wild forest inhabited by Takshaka, the king of snakes and his family. Through hard work the Pandavas are able to build a new glorious capital for the territory at Indraprastha.\n\nShortly after this, Arjuna elopes with and then marries Krishna's sister, Subhadra. Yudhisthra wishes to establish his position as king; he seeks Krishna's advice. Krishna advises him, and after due preparation and the elimination of some opposition, Yudhishthira carries out the \"rājasūya yagna\" ceremony; he is thus recognised as pre-eminent among kings.\n\nThe Pandavas have a new palace built for them, by Maya the Danava. They invite their Kaurava cousins to Indraprastha. Duryodhana walks round the palace, and mistakes a glossy floor for water, and will not step in. After being told of his error, he then sees a pond, and assumes it is not water and falls in. Bhima, Arjun, the twins and the servants laugh at him. In popular adaptations, this insult is wrongly attributed to Draupadi, even though in the Sanskrit epic, it was the Pandavas (except Yudhisthira) who had insulted Duryodhana. Enraged by the insult, and jealous at seeing the wealth of the Pandavas, Duryodhana decides to host a dice-game at Shakuni's suggestion.\n\nShakuni, Duryodhana's uncle, now arranges a dice game, playing against Yudhishthira with loaded dice. In the dice game, Yudhishthira loses all his wealth, then his kingdom. Yudhishthira then gambles his brothers, himself, and finally his wife into servitude. The jubilant Kauravas insult the Pandavas in their helpless state and even try to disrobe Draupadi in front of the entire court, but Draupadi's disrobe is prevented by Krishna, who miraculously make her dress endless, therefore it couldn't be removed.\n\nDhritarashtra, Bhishma, and the other elders are aghast at the situation, but Duryodhana is adamant that there is no place for two crown princes in Hastinapura. Against his wishes Dhritarashtra orders for another dice game. The Pandavas are required to go into exile for 12 years, and in the 13th year, they must remain hidden. If they are discovered by the Kauravas in the 13th year of their exile, then they will be forced into exile for another 12 years.\n\nThe Pandavas spend thirteen years in exile; many adventures occur during this time. They also prepare alliances for a possible future conflict. They spend their final year in disguise in the court of Virata, and they are discovered just after the end of the year.\n\nAt the end of their exile, they try to negotiate a return to Indraprastha with Krishna as their emissary. However, this negotiation fails, because Duryodhana objected that they were discovered in the 13th year of their exile and the return of their kingdom was not agreed. Then the Pandavas fought the Kauravas, claiming their rights over Indraprastha.\n\nThe two sides summon vast armies to their help and line up at Kurukshetra for a war. The kingdoms of Panchala, Dwaraka, Kasi, Kekaya, Magadha, Matsya, Chedi, Pandyas, Telinga, and the Yadus of Mathura and some other clans like the Parama Kambojas were allied with the Pandavas. The allies of the Kauravas included the kings of Pragjyotisha, Anga, Kekaya, Sindhudesa (including Sindhus, Sauviras and Sivis), Mahishmati, Avanti in Madhyadesa, Madra, Gandhara, Bahlika people, Kambojas and many others. Before war being declared, Balarama had expressed his unhappiness at the developing conflict and leaves to go on pilgrimage; thus he does not take part in the battle itself. Krishna takes part in a non-combatant role, as charioteer for Arjuna.\n\nBefore the battle, Arjuna noticing that the opposing army includes his own kith and kin, including his great grandfather Bhishma and his teacher Drona, has grave doubts about the fight and falls into despair.At this time,Krishna reminds him of duty as a Kshatriya to fight for his just cause in the famous Bhagavad Gita section of the epic.\n\nThough initially sticking to chivalrous notions of warfare, both sides soon adopt dishonourable tactics. At the end of the 18-day battle, only the Pandavas, Satyaki, Kripa, Ashwatthama, Kritavarma, Yuyutsu and Krishna survive.\n\nAfter \"seeing\" the carnage, Gandhari, who had lost all her sons, curses Krishna to be a witness to a similar annihilation of his family, for though divine and capable of stopping the war, he had not done so. Krishna accepts the curse, which bears fruit 36 years later.\n\nThe Pandavas, who had ruled their kingdom meanwhile, decide to renounce everything. Clad in skins and rags they retire to the Himalaya and climb towards heaven in their bodily form. A stray dog travels with them. One by one the brothers and Draupadi fall on their way. As each one stumbles, Yudhishthira gives the rest the reason for their fall (Draupadi was partial to Arjuna, Nakula and Sahadeva were vain and proud of their looks, and Bhima and Arjuna were proud of their strength and archery skills, respectively). Only the virtuous Yudhishthira, who had tried everything to prevent the carnage, and the dog remain. The dog reveals himself to be the god Yama (also known as Yama Dharmaraja), and then takes him to the underworld where he sees his siblings and wife. After explaining the nature of the test, Yama takes Yudhishthira back to heaven and explains that it was necessary to expose him to the underworld because (Rajyante narakam dhruvam) any ruler has to visit the underworld at least once. Yama then assures him that his siblings and wife would join him in heaven after they had been exposed to the underworld for measures of time according to their vices.\n\nArjuna's grandson Parikshit rules after them and dies bitten by a snake. His furious son, Janamejaya, decides to perform a snake sacrifice (\"sarpasattra\") in order to destroy the snakes. It is at this sacrifice that the tale of his ancestors is narrated to him.\n\nThe Mahābhārata mentions that Karna, the Pandavas, Draupadi and Dhritarashtra's sons eventually ascended to svarga and \"attained the state of the gods\" and banded together — \"serene and free from anger.\"\n\nThe \"Mahābhārata\" offers one of the first instances of theorizing about \"dharmayuddha\", \"just war\", illustrating many of the standards that would be debated later across the world. In the story, one of five brothers asks if the suffering caused by war can ever be justified. A long discussion ensues between the siblings, establishing criteria like \"proportionality\" (chariots cannot attack cavalry, only other chariots; no attacking people in distress), \"just means\" (no poisoned or barbed arrows), \"just cause\" (no attacking out of rage), and fair treatment of captives and the wounded.\n\nBetween 1919 and 1966, scholars at the Bhandarkar Oriental Research Institute, Pune, compared the various manuscripts of the epic from India and abroad and produced the \"Critical Edition\" of the \"Mahabharata\", on 13,000 pages in 19 volumes, followed by the \"Harivamsha\" in another two volumes and six index volumes. This is the text that is usually used in current Mahābhārata studies for reference. This work is sometimes called the \"Pune\" or \"Poona\" edition of the \"Mahabharata\".\n\nMany regional versions of the work developed over time, mostly differing only in minor details, or with verses or subsidiary stories being added. These include the Tamil street theatre, terukkuttu and kattaikkuttu, the plays of which use themes from the Tamil language versions of \"Mahabharata\", focusing on Draupadi.\n\nOutside the Indian subcontinent, in Indonesia, a version was developed in ancient Java as Kakawin Bhāratayuddha in the 11th century under the patronage of King Dharmawangsa (990–1016) and later it spread to the neighboring island of Bali, which remains a Hindu majority island today. It has become the fertile source for Javanese literature, dance drama (wayang wong), and wayang shadow puppet performances. This Javanese version of the Mahābhārata differs slightly from the original Indian version. For example, Draupadi is only wed to Yudhishthira, not to all the Pandava brothers; this might demonstrate ancient Javanese opposition to polyandry. The author later added some female characters to be wed to the Pandavas, for example, Arjuna is described as having many wives and consorts next to Subhadra. Another difference is that Shikhandini does not change her sex and remains a woman, to be wed to Arjuna, and takes the role of a warrior princess during the war. Another twist is that Gandhari is described as antagonistic character who hates the Pandavas: her hate is out of jealousy because during Gandhari's swayamvara, she was in love with Pandu but was later wed to his blind elder brother instead, whom she did not love, so she blindfolded herself as protest. Another notable difference is the inclusion of the Punakawans, the clown servants of the main characters in the storyline. These characters include Semar, Petruk, Gareng and Bagong, who are much-loved by Indonesian audiences. There are also some spin-off episodes developed in ancient Java, such as Arjunawiwaha composed in 11th century.\n\nA Kawi version of the \"Mahabharata\", of which eight of the eighteen \"parvas\" survive, is found on the Indonesian island of Bali. It has been translated into English by Dr. I. Gusti Putu Phalgunadi.\n\nA Persian translation of \"Mahabharata\", titled \"Razmnameh\", was produced at Akbar's orders, by Faizi and `Abd al-Qadir Bada'uni in the 18th century.\n\nThe first complete English translation was the Victorian prose version by Kisari Mohan Ganguli, published between 1883 and 1896 (Munshiram Manoharlal Publishers) and by M. N. Dutt (Motilal Banarsidass Publishers). Most critics consider the translation by Ganguli to be faithful to the original text. The complete text of Ganguli's translation is in the public domain and is available online.\n\nAnother English prose translation of the full epic, based on the \"Critical Edition\", is in progress, published by University Of Chicago Press. It was initiated by Indologist J. A. B. van Buitenen (books 1–5) and, following a 20-year hiatus caused by the death of van Buitenen, is being continued by D. Gitomer of DePaul University (book 6), J. L. Fitzgerald of Brown University (books 11–13) and Wendy Doniger of the University of Chicago (books 14–18).\n\nAn early poetry translation by Romesh Chunder Dutt and published in 1898 condenses the main themes of the Mahābhārata into English verse. A later poetic \"transcreation\" (author's own description) of the full epic into English, done by the poet P. Lal, is complete, and in 2005 began being published by Writers Workshop, Calcutta. The P. Lal translation is a non-rhyming verse-by-verse rendering, and is the only edition in any language to include all slokas in all recensions of the work (not just those in the \"Critical Edition\"). The completion of the publishing project is scheduled for 2010. Sixteen of the eighteen volumes are now available.\n\nA project to translate the full epic into English prose, translated by various hands, began to appear in 2005 from the Clay Sanskrit Library, published by New York University Press. The translation is based not on the \"Critical Edition\" but on the version known to the commentator Nīlakaṇṭha. Currently available are 15 volumes of the projected 32-volume edition.\n\nIndian economist Bibek Debroy has also begun an unabridged English translation in ten volumes. Volume 1: Adi Parva was published in March 2010.\n\nMany condensed versions, abridgements and novelistic prose retellings of the complete epic have been published in English, including works by Ramesh Menon, William Buck, R. K. Narayan, C. Rajagopalachari, K. M. Munshi, Krishna Dharma, Romesh C. Dutt, Bharadvaja Sarma, John D. Smith and Sharon Maas.\n\nBhasa, the 2nd- or 3rd-century CE Sanskrit playwright, wrote two plays on episodes in the Marabharata, \"Urubhanga\" (Broken Thigh), about the fight between Duryodhana and Bhima, while \"Madhyamavyayoga\" (The Middle One) set around Bhima and his son, Ghatotkacha. The first important play of 20th century was \"Andha Yug\" (\"The Blind Epoch\"), by Dharamvir Bharati, which came in 1955, found in \"Mahabharat\", both an ideal source and expression of modern predicaments and discontent. Starting with Ebrahim Alkazi it was staged by numerous directors. V. S. Khandekar's Marathi novel, \"Yayati\" (1960) and Girish Karnad's debut play \"Yayati\" (1961) are based on the story of King Yayati found in the \"Mahabharat\". Bengali writer and playwright, Buddhadeva Bose wrote three plays set in Mahabharat, \"Anamni Angana\", \"Pratham Partha\" and \"Kalsandhya\". Pratibha Ray wrote an award winning novel entitled Yajnaseni from Draupadi's perspective in 1984. Later, Chitra Banerjee Divakaruni wrote a similar novel entitled \"\" in 2008. Gujarati poet Chinu Modi has written long narrative poetry \"Bahuk\" based on character Bahuka. Krishna Udayasankar, a Singapore-based Indian author has written several novels which are modern-day retellings of the epic, most notably the Aryavarta Chronicles Series. Suman Pokhrel wrote a solo play based on Ray's novel by personalizing and taking Draupadi alone in the scene.\n\nAmar Chitra Katha published a 1,260 page comic book version of the \"Mahabharata\".\n\nIn Indian cinema, several film versions of the epic have been made, dating back to 1920. In Telugu film Daana Veera Soora Karna (1977) directed by and starring N. T. Rama Rao depicts Karna as the lead character. The Mahābhārata was also reinterpreted by Shyam Benegal in Kalyug. Prakash Jha directed 2010 film Raajneeti was partially inspired by the \"Mahabharata\". A 2013 animated adaptation holds the record for India's most expensive animated film.\n\nIn the late 1980s, the \"Mahabharat\" TV series, directed by Ravi Chopra, was televised on India's national television (Doordarshan). The same year as \"Mahabharat\" was being shown on Doordarshan, that same company's other television show, \"Bharat Ek Khoj\", also directed by Shyam Benegal, showed a 2-episode abbreviation of the \"Mahabharata\", drawing from various interpretations of the work, be they sung, danced, or staged. In the Western world, a well-known presentation of the epic is Peter Brook's nine-hour play, which premiered in Avignon in 1985, and its five-hour movie version \"The Mahābhārata\" (1989). In the late 2013 Mahabharat was televised on STAR Plus. It was produced by Swastik Productions Pvt.\n\nUncompleted projects on the Mahābhārata include a ones by Rajkumar Santoshi, and a theaterical adaptation planned by Satyajit Ray.\n\nJain versions of Mahābhārata can be found in the various Jain texts like \"Harivamsapurana\" (the story of Harivamsa) \"Trisastisalakapurusa Caritra\" (Hagiography of 63 Illustrious persons), \"Pandavacaritra\" (lives of Pandavas) and \"Pandavapurana\" (stories of Pandavas). From the earlier canonical literature, \"Antakrddaaśāh\" (8th cannon) and \"Vrisnidasa\" (\"upangagama\" or secondary canon) contain the stories of Neminatha (22nd Tirthankara), Krishna and Balarama. Prof. Padmanabh Jaini notes that, unlike in the Hindu Puranas, the names Baladeva and Vasudeva are not restricted to Balarama and Krishna in Jain puranas. Instead they serve as names of two distinct class of mighty brothers, who appear nine times in each half of time cycles of the Jain cosmology and rule the half the earth as half-chakravartins. Jaini traces the origin of this list of brothers to the Jinacharitra by Bhadrabahu swami (4th–3rd century BCE). According to Jain cosmology Balarama, Krishna and Jarasandha are the ninth and the last set of Baladeva, Vasudeva, and Partivasudeva. The main battle is not the Mahabharata, but the fight between Krishna and Jarasandha (who is killed by Krishna). Ultimately, the Pandavas and Balarama take renunciation as Jain monks and are reborn in heavens, while on the other hand Krishna and Jarasandha are reborn in hell. In keeping with the law of karma, Krishna is reborn in hell for his exploits (sexual and violent) while Jarasandha for his evil ways. Prof. Jaini admits a possibility that perhaps because of his popularity, the Jain authors were keen to rehabilitate Krishna. The Jain texts predict that after his karmic term in hell is over sometime during the next half time-cycle, Krishna will be reborn as a Jain Tirthankara and attain liberation. Krishna and Balrama are shown as contemporaries and cousins of 22nd Tirthankara, Neminatha. According to this story, Krishna arranged young Neminath’s marriage with Rajamati, the daughter of Ugrasena, but Neminatha, empathizing with the animals which were to be slaughtered for the marriage feast, left the procession suddenly and renounced the world.\n\nThis shows the line of royal and family succession, not necessarily the parentage. See the notes below for detail.\n\nKey to Symbols\n\nNotes\n\nThe birth order of siblings is correctly shown in the family tree (from left to right), except for Vyasa and Bhishma whose birth order is not described, and Vichitravirya and Chitrangada who were born after them. The fact that Ambika and Ambalika are sisters is not shown in the family tree. The birth of Duryodhana took place after the birth of Karna, Yudhishthira and Bhima, but before the birth of the remaining Pandava brothers.\n\nSome siblings of the characters shown here have been left out for clarity; these include Chitrāngada, the eldest brother of Vichitravirya. Vidura, half-brother to Dhritarashtra and Pandu.\n\nIn the \"Bhagavad Gita\", Krishna explains to Arjuna his duties as a warrior and prince and elaborates on different Yogic and Vedantic philosophies, with examples and analogies. This has led to the Gita often being described as a concise guide to Hindu philosophy and a practical, self-contained guide to life. In more modern times, Swami Vivekananda, Bal Gangadhar Tilak, Mahatma Gandhi and many others used the text to help inspire the Indian independence movement.\n\nVarious modern day television shows and novels have taken inspiration from the Mahabharata.\n\n\n\n\n"}
{"id": "19644", "url": "https://en.wikipedia.org/wiki?curid=19644", "title": "Mein Kampf", "text": "Mein Kampf\n\nMein Kampf (, \"My Struggle\") is a 1925 autobiographical book by Nazi Party leader Adolf Hitler. The work describes the process by which Hitler became antisemitic and outlines his political ideology and future plans for Germany. Volume 1 of \"Mein Kampf\" was published in 1925 and Volume 2 in 1926. The book was edited by Hitler's deputy Rudolf Hess.\n\nHitler began \"Mein Kampf\" while imprisoned for what he considered to be \"political crimes\" following his failed Putsch in Munich in November 1923. Although Hitler received many visitors initially, he soon devoted himself entirely to the book. As he continued, Hitler realized that it would have to be a two-volume work, with the first volume scheduled for release in early 1925. The governor of Landsberg noted at the time that \"he [Hitler] hopes the book will run into many editions, thus enabling him to fulfill his financial obligations and to defray the expenses incurred at the time of his trial.\" After slow initial sales, the book was a bestseller in Germany after Hitler's rise to power in 1933.\n\nAfter Hitler's death, copyright of \"Mein Kampf\" passed to the state government of Bavaria, which refused to allow any copying or printing of the book in Germany. In 2016, following the expiration of the copyright held by the Bavarian state government, \"Mein Kampf\" was republished in Germany for the first time since 1945, which prompted public debate and divided reactions from Jewish groups.\n\nHitler originally wanted to call his forthcoming book \"Viereinhalb Jahre (des Kampfes) gegen Lüge, Dummheit und Feigheit\", or \"Four and a Half Years (of Struggle) Against Lies, Stupidity and Cowardice\". Max Amann, head of the Franz Eher Verlag and Hitler's publisher, is said to have suggested the much shorter \"\"Mein Kampf\"\" or \"\"My Struggle\"\".\n\nThe arrangement of chapters is as follows: \n\n\nIn \"Mein Kampf\", Hitler used the main thesis of \"the Jewish peril\", which posits a Jewish conspiracy to gain world leadership. The narrative describes the process by which he became increasingly antisemitic and militaristic, especially during his years in Vienna. He speaks of not having met a Jew until he arrived in Vienna, and that at first his attitude was liberal and tolerant. When he first encountered the antisemitic press, he says, he dismissed it as unworthy of serious consideration. Later he accepted the same antisemitic views, which became crucial to his program of national reconstruction of Germany.\n\n\"Mein Kampf\" has also been studied as a work on political theory. For example, Hitler announces his hatred of what he believed to be the world's two evils: Communism and Judaism.\n\nIn the book Hitler blamed Germany's chief woes on the parliament of the Weimar Republic, the Jews, and Social Democrats, as well as Marxists, though he believed that Marxists, Social Democrats, and the parliament were all working for Jewish interests. He announced that he wanted to completely destroy the parliamentary system, believing it to be corrupt in principle, as those who reach power are inherent opportunists.\n\nWhile historians dispute the exact date Hitler decided to force the Jewish people to emigrate to Madagascar, few place the decision before the mid-1930s. First published in 1925, \"Mein Kampf\" shows Hitler's personal grievances and his ambitions for creating a New Order.\n\nThe historian Ian Kershaw points out that several passages in \"Mein Kampf\" are undeniably of a genocidal nature. Hitler wrote \"the nationalization of our masses will succeed only when, aside from all the positive struggle for the soul of our people, their international poisoners are exterminated\", and he suggested that, \"If at the beginning of the war and during the war twelve or fifteen thousand of these Hebrew corrupters of the nation had been subjected to poison gas, such as had to be endured in the field by hundreds of thousands of our very best German workers of all classes and professions, then the sacrifice of millions at the front would not have been in vain.\"\n\nThe racial laws to which Hitler referred resonate directly with his ideas in \"Mein Kampf\". In the first edition of \"Mein Kampf\", Hitler stated that the destruction of the weak and sick is far more humane than their protection. Apart from this allusion to humane treatment, Hitler saw a purpose in destroying \"the weak\" in order to provide the proper space and purity for the \"strong\".\n\nIn the chapter \"Eastern Orientation or Eastern Policy\", Hitler argued that the Germans needed Lebensraum in the East, a \"historic destiny\" that would properly nurture the German people. Hitler believed that \"the organization of a Russian state formation was not the result of the political abilities of the Slavs in Russia, but only a wonderful example of the state-forming efficacity of the German element in an inferior race.\"\n\nIn \"Mein Kampf\" Hitler openly stated the future German expansion in the East, foreshadowing Generalplan Ost:\n\nAlthough Hitler originally wrote \"Mein Kampf\" mostly for the followers of National Socialism, it grew in popularity after he rose to power. (Two other books written by party members, Gottfried Feder's \"Breaking The Interest Slavery\" and Alfred Rosenberg's \"The Myth of the Twentieth Century,\" have since lapsed into comparative literary obscurity, and no translation of Feder's book from the original German is known.) Hitler had made about 1.2 million Reichsmarks from the income of the book by 1933, when the average annual income of a teacher was about 4,800 Marks. He accumulated a tax debt of 405,500 Reichsmark (very roughly in 2015 1.1 million GBP, 1.4 million EUR, 1.5 million USD) from the sale of about 240,000 copies before he became chancellor in 1933 (at which time his debt was waived).\n\nHitler began to distance himself from the book after becoming chancellor of Germany in 1933. He dismissed it as \"fantasies behind bars\" that were little more than a series of articles for the \"Völkischer Beobachter\", and later told Hans Frank that \"If I had had any idea in 1924 that I would have become Reich chancellor, I never would have written the book.\" Nevertheless, \"Mein Kampf\" was a bestseller in Germany during the 1930s. During Hitler's years in power, the book was in high demand in libraries and often reviewed and quoted in other publications. It was given free to every newlywed couple and every soldier fighting at the front. By 1939 it had sold 5.2 million copies in eleven languages. By the end of the war, about 10 million copies of the book had been sold or distributed in Germany.\n\n\"Mein Kampf\", in essence, lays out the ideological program Hitler established for the German revolution, by identifying the Jews and \"Bolsheviks\" as racially and ideologically inferior and threatening, and \"Aryans\" and National Socialists as racially superior and politically progressive. Hitler's revolutionary goals included expulsion of the Jews from Greater Germany and the unification of German peoples into one Greater Germany. Hitler desired to restore German lands to their greatest historical extent, real or imagined.\n\nDue to its racist content and the historical effect of Nazism upon Europe during World War II and the Holocaust, it is considered a highly controversial book. Criticism has not come solely from opponents of Nazism. Italian Fascist dictator and Nazi ally Benito Mussolini was also critical of the book, saying that it was \"a boring tome that I have never been able to read\" and remarking that Hitler's beliefs, as expressed in the book, were \"little more than commonplace clichés\".\n\nThe German journalist Konrad Heiden, an early critic of the Nazi Party, observed that the content of \"Mein Kampf\" is essentially a political argument with other members of the Nazi Party who had appeared to be Hitler's friends, but whom he was actually denouncing in the book's content – sometimes by not even including references to them.\n\nThe American literary theorist and philosopher Kenneth Burke wrote a 1939 rhetorical analysis of the work, \"The Rhetoric of Hitler's \"Battle\"\", which revealed an underlying message of aggressive intent.\n\nAmerican journalist John Gunther said in 1940 that compared to the autobiographies of Leon Trotsky or Henry Adams \"Mein Kampf\" was \"vapid, vain, rhetorical, diffuse, prolix. But it is a powerful and moving book, the product of great passionate feeling\". He suggested that the book exhausted curious German readers, but its \"ceaseless repetition of the argument, left impregnably in their minds, fecund and germinating\".\n\nIn March 1940, British writer George Orwell reviewed a then-recently published uncensored translation of \"Mein Kampf\" for \"The New English Weekly\". Orwell suggested that the force of Hitler's personality shone through the often \"clumsy\" writing, capturing the magnetic allure of Hitler for many Germans. In essence, Orwell notes, Hitler offers only visions of endless struggle and conflict in the creation of \"a horrible brainless empire\" that \"stretch[es] to Afghanistan or thereabouts\". He wrote, \"Whereas Socialism, and even capitalism in a more grudging way, have said to people 'I offer you a good time,' Hitler has said to them, 'I offer you struggle, danger, and death,' and as a result a whole nation flings itself at his feet.\" Orwell's review was written in the aftermath of the 1939 Molotov–Ribbentrop Pact, when Hitler made peace with USSR after more than a decade of vitriolic rhetoric and threats between the two nations; with the pact in place, Orwell believed, England was now facing a risk of Nazi attack and the UK must not underestimate the appeal of Hitler's ideas.\n\nIn his 1943 book \"The Menace of the Herd\", Austrian scholar Erik von Kuehnelt-Leddihn described Hitler's ideas in \"Mein Kampf\" and elsewhere as \"a veritable \"reductio ad absurdum\" of 'progressive' thought\" and betraying \"a curious lack of original thought\" that shows Hitler offered no innovative or original ideas but was merely \"a \"virtuoso\" of commonplaces which he may or may not repeat in the guise of a 'new discovery.'\" Hitler's stated aim, Kuehnelt-Leddihn writes, is to quash individualism in furtherance of political goals:\n\nIn his \"The Second World War\", published in several volumes in the late 1940s and early 1950s, Winston Churchill wrote that he felt that after Hitler's ascension to power, no other book than \"Mein Kampf\" deserved more intensive scrutiny.\n\nThe critic George Steiner has suggested that \"Mein Kampf\" can be seen as one of several books that resulted from the crisis of German culture following Germany's defeat in World War I, comparable in this respect to the philosopher Ernst Bloch's \"The Spirit of Utopia\" (1918), the historian Oswald Spengler's \"The Decline of the West\" (1918), the theologian Franz Rosenzweig's \"The Star of Redemption\" (1921), and the theologian Karl Barth's \"The Epistle to the Romans\" (1922).\n\nWhile Hitler was in power (1933–1945), \"Mein Kampf\" came to be available in three common editions. The first, the \"Volksausgabe\" or People's Edition, featured the original cover on the dust jacket and was navy blue underneath with a gold swastika eagle embossed on the cover. The \"Hochzeitsausgabe\", or Wedding Edition, in a slipcase with the seal of the province embossed in gold onto a parchment-like cover was given free to marrying couples. In 1940, the \"Tornister-Ausgabe\", or Knapsack Edition, was released. This edition was a compact, but unabridged, version in a red cover and was released by the post office, available to be sent to loved ones fighting at the front. These three editions combined both volumes into the same book.\n\nA special edition was published in 1939 in honour of Hitler's 50th birthday. This edition was known as the \"Jubiläumsausgabe\", or Anniversary Issue. It came in both dark blue and bright red boards with a gold sword on the cover. This work contained both volumes one and two. It was considered a deluxe version, relative to the smaller and more common \"Volksausgabe\".\n\nThe book could also be purchased as a two-volume set during Hitler's rule, and was available in soft cover and hardcover. The soft cover edition contained the original cover (as pictured at the top of this article). The hardcover edition had a leather spine with cloth-covered boards. The cover and spine contained an image of three brown oak leaves.\n\nThe first English translation was an abridgement by Edgar Dugdale who started work on it in 1931, at the prompting of his wife, Blanche. When he learned that the London publishing firm of Hurst & Blackett had secured the rights to publish an abridgement in the United Kingdom, he offered it for free in April 1933. However, a local Nazi Party representative insisted that the translation be further abridged before publication, so it was held back until 13 October 1933, although excerpts were allowed to run in \"The Times\" in late July. It was published by Hurst & Blackett as part of \"The Paternoster Library\".\n\nIn America, Houghton Mifflin secured the rights to the Dugdale abridgement on 29 July 1933. The only differences between the American and British versions are that the title was translated \"My Struggle\" in the UK and \"My Battle\" in America; and that Dugdale is credited as translator in the US edition, while the British version withheld his name. Both Dugdale and his wife were active in the Zionist movement; Blanche was the niece of Lord Balfour, and they wished to avoid publicity.\n\nHoughton and Mifflin licensed Reynal & Hitchcock the rights to publish a full unexpurgated translation in 1938. The book was translated from the two volumes of the first German edition (1925 and 1927), with notations appended noting any changes made in later editions, which were deemed \"not as extensive as popularly supposed.\" The translation, made by a committee from the New School for Social Research headed by Alvin Johnson, was said to have been made with a view to readability rather than in an effort to rigidly reproduce Hitler's sometimes idiosyncratic German form.\n\nThe text was heavily annotated for an American audience with biographical and historical details derived largely from German sources. As the translators deemed the book \"a propagandistic essay of a violent partisan\", which \"often warps historical truth and sometimes ignores it completely,\" the tone of many of these annotations reflected a conscious attempt to provide \"factual information that constitutes an extensive critique of the original.\" The book appeared for sale on 28 February 1939.\n\nOne of the earlier complete English translations of \"Mein Kampf\" was by James Murphy in 1939. It was the only English translation approved by Nazi Germany. The version published by Hutchison & Co. in association with Hurst & Blackett, Ltd (London) in 1939 of the combined volumes I and II is profusely illustrated with many full page drawings and photographs. The opening line, \"It has turned out fortunate for me to-day that destiny appointed Braunau-on-the-Inn to be my birthplace,\" is characteristic of Hitler's sense of destiny that began to develop in the early 1920s. Hurst & Blackett ceased publishing the Murphy translation in 1942 when the original plates were destroyed by German bombing, but it is still published and available in facsimile editions and also on the Internet.\n\nThe small Pennsylvania firm of Stackpole and Sons released its own unexpurgated translation by William Soskin on the same day as Houghton Mifflin, amid much legal wrangling. The Second Circuit Court of Appeals ruled in Houghton Mifflin's favour that June and ordered Stackpole to stop selling their version, but litigation followed for a few more years until the case was finally resolved in September 1941.\n\nAmong other things, Stackpole argued that Hitler could not have legally transferred his right to a copyright in the United States to Eher Verlag in 1925, because he was not a citizen of any country. \"Houghton Mifflin v. Stackpole\" was a minor landmark in American copyright law, definitively establishing that stateless persons have the same copyright status in the United States that any other foreigner would. In the three months that Stackpole's version was available it sold 12,000 copies.\n\nHoughton Mifflin's abridged English translation left out some of Hitler's more antisemitic and militaristic statements. This motivated Alan Cranston, an American reporter for United Press International in Germany (and later a U.S. Senator from California), to publish his own abridged and annotated translation. Cranston believed this version more accurately reflected the contents of the book and Hitler's intentions. In 1939, Cranston was sued by Hitler's publisher for copyright infringement, and a Connecticut judge ruled in Hitler's favour. By the time the publication of Cranston's version was stopped, 500,000 copies had already been sold. Today, the profits and proceeds are given to various charities.\n\nHoughton Mifflin published a translation by Ralph Manheim in 1943. They did this to avoid having to share their profits with Reynal & Hitchcock, and to increase sales by offering a more readable translation. The Manheim translation was first published in the United Kingdom by Hurst & Blackett in 1969 amid some controversy.\n\nIn addition to the above translations and abridgments, the following collections of excerpts were available in English before the start of the war:\n\nA previously unknown English translation was released in 2008, which had been prepared by the official Nazi printing office, the Franz Eher Verlag. In 1939, the Nazi propaganda ministry hired James Murphy to create an English version of \"Mein Kampf\", which they hoped to use to promote Nazi goals in English-speaking countries. While Murphy was in Germany, he became less enchanted with Nazi ideology and made some statements that the Propaganda Ministry disliked. As a result, they asked him to leave Germany immediately. He was not able to take any of his notes but later sent his wife back to obtain his partial translation. These notes were later used to create the Murphy translation.\n\nSales of Dugdale abridgment in the United Kingdom.\n\n\nSales of the Houghton Mifflin Dugdale translation in the United States.\n\nThe first printing of the U.S. Dugdale edition, the October 1933 with 7,603 copies, of which 290 were given away as complimentary gifts.\n\nThe royalty on the first printing in the U.S. was 15% or $3,206.45 total. Curtis Brown, literary agent, took 20%, or $641.20 total, and the IRS took $384.75, leaving Eher Verlag $2,180.37 or RM 5668.\n\nThe January 1937 second printing was c. 4,000 copies.\n\nThere were three separate printings from August 1938 to March 1939, totaling 14,000; sales totals by 31 March 1939 were 10,345.\n\nThe Murphy and Houghton Mifflin translations were the only ones published by the authorised publishers while Hitler was still alive, and not at war with the U.K. and the U.S.\n\nThere was some resistance from Eher Verlag to Hurst and Blackett's Murphy translation, as they had not been granted the rights to a full translation. However, they allowed it \"de facto\" permission by not lodging a formal protest, and on 5 May 1939, even inquired about royalties. The British publishers responded on the 12th that the information they requested was \"not yet available\" and the point would be moot within a few months, on 3 September 1939, when all royalties were halted due to the state of war existing between Britain and Germany.\n\nRoyalties were likewise held up in the United States due to the litigation between Houghton Mifflin and Stackpole. Because the matter was only settled in September 1941, only a few months before a state of war existed between Germany and the U.S., all Eher Verlag ever got was a $2,500 advance from Reynal and Hitchcock. It got none from the unauthorised Stackpole edition or the 1943 Manheim edition.\n\nAt the time of his suicide, Hitler's official place of residence was in Munich, which led to his entire estate, including all rights to \"Mein Kampf\", changing to the ownership of the state of Bavaria. The government of Bavaria, in agreement with the federal government of Germany, refused to allow any copying or printing of the book in Germany. It also opposed copying and printing in other countries, but with less success. As per German copyright law, the entire text entered the public domain on 1 January 2016, 70 years after the author's death.\n\nOwning and buying the book in Germany is not an offence. Trading in old copies is lawful as well, unless it is done in such a fashion as to \"promote hatred or war.\" In particular, the unmodified edition is not covered by §86 StGB that forbids dissemination of means of propaganda of unconstitutional organisations, since it is a \"pre-constitutional work\" and as such cannot be opposed to the free and democratic basic order, according to a 1979 decision of the Federal Court of Justice of Germany. Most German libraries carry heavily commented and excerpted versions of \"Mein Kampf.\" In 2008, Stephan Kramer, secretary-general of the Central Council of Jews in Germany, not only recommended lifting the ban, but volunteered the help of his organization in editing and annotating the text, saying that it is time for the book to be made available to all online.\n\nA variety of restrictions or special circumstances apply in other countries.\n\nSince its first publication in India in 1928, \"Mein Kampf\" has gone through hundreds of editions and sold over 100,000 copies.\n\nIn the Netherlands the sale of \"Mein Kampf\" had been forbidden since World War II. In September 2018, however, Dutch publisher Prometheus officially released an academic edition of the 2016 German translation with comprehensive introductions and annotations by Dutch historians. It marks the first time the book is widely available to the general public in the Netherlands since World War II.\n\nIn the Russian Federation, \"Mein Kampf\" has been published at least three times since 1992; the Russian text is also available on websites. In 2006 the Public Chamber of Russia proposed banning the book. In 2009 St. Petersburg's branch of the Russian Ministry of Internal Affairs requested to remove an annotated and hyper-linked Russian translation of the book from a historiography website. On 13 April 2010, it was announced that \"Mein Kampf\" is outlawed on grounds of extremism promotion.\n\n\"Mein Kampf\" has been reprinted several times since 1945; in 1970, 1992, 2002 and 2010. In 1992 the Government of Bavaria tried to stop the publication of the book, and the case went to the Supreme Court of Sweden which ruled in favour of the publisher, stating that the book is protected by copyright, but that the copyright holder is unidentified (and not the State of Bavaria) and that the original Swedish publisher from 1934 had gone out of business. It therefore refused the Government of Bavaria's claim.\nThe only translation changes came in the 1970 edition, but they were only linguistic, based on a new Swedish standard.\n\n\"Mein Kampf\" was widely available and growing in popularity in Turkey, even to the point where it became a bestseller, selling up to 100,000 copies in just two months in 2005. Analysts and commentators believe the popularity of the book to be related to a rise in nationalism and anti-U.S. sentiment. A columnist in Shalom stated this was a result of \"what is happening in the Middle East, the Israeli-Palestinian problem and the war in Iraq.\" Doğu Ergil, a political scientist at Ankara University, said both far-right ultranationalists and extremist Islamists had found common ground - \"not on a common agenda for the future, but on their anxieties, fears and hate\".\n\nIn the United States, \"Mein Kampf\" can be found at many community libraries and can be bought, sold and traded in bookshops. The U.S. government seized the copyright in September 1942 during the Second World War under the Trading with the Enemy Act and in 1979, Houghton Mifflin, the U.S. publisher of the book, bought the rights from the government pursuant to . More than 15,000 copies are sold a year. In 2016, Houghton Mifflin Harcourt reported that it was having difficulty finding a charity that would accept profits from the sales of its version of \"Mein Kampf\", which it had promised to donate.\n\nIn 1999, the Simon Wiesenthal Center documented that major Internet booksellers such as Amazon.com and Barnesandnoble.com sell \"Mein Kampf\" to Germany. After a public outcry, both companies agreed to stop those sales to addresses in Germany. The book is currently available through both companies online. It is also available in various languages, including German, at the Internet Archive. The Murphy translation of the book is freely available on Project Gutenberg Australia. Since the January 2016 republication of the book in Germany, the book can be ordered at Amazon's German website.\n\nOn 3 February 2010, the Institute of Contemporary History (IfZ) in Munich announced plans to republish an annotated version of the text, for educational purposes in schools and universities, in 2015. The book had last been published in Germany in 1945. The IfZ argued that a republication was necessary to get an authoritative annotated edition by the time the copyright ran out, which might open the way for neo-Nazi groups to publish their own versions. The Bavarian Finance Ministry opposed the plan, citing respect for victims of the Holocaust. It stated that permits for reprints would not be issued, at home or abroad. This would also apply to a new annotated edition. There was disagreement about the issue of whether the republished book might be banned as Nazi propaganda. The Bavarian government emphasized that even after expiration of the copyright, \"the dissemination of Nazi ideologies will remain prohibited in Germany and is punishable under the penal code\". However, the Bavarian Science Minister Wolfgang Heubisch supported a critical edition, stating in 2010 that, \"Once Bavaria's copyright expires, there is the danger of charlatans and neo-Nazis appropriating this infamous book for themselves\". \n\nOn 12 December 2013 the Bavarian government cancelled its financial support for an annotated edition. IfZ, which was preparing the translation, announced that it intended to proceed with publication after the copyright expired. The IfZ scheduled an edition of \"Mein Kampf\" for release in 2016.\n\nRichard Verber, vice-president of the Board of Deputies of British Jews, stated in 2015 that the board trusted the academic and educational value of republishing. “We would, of course, be very wary of any attempt to glorify Hitler or to belittle the Holocaust in any way,” Verber declared to \"The Observer\". “But this is not that. I do understand how some Jewish groups could be upset and nervous, but it seems it is being done from a historical point of view and to put it in context.”\n\nAn annotated edition of \"Mein Kampf\" was published in Germany in January 2016 and sold out within hours on Amazon's German site. The book's publication led to public debate in Germany, and divided reactions from Jewish groups, with some supporting, and others opposing, the decision to publish. German officials had previously said they would limit public access to the text amid fears that its republication could stir neo-Nazi sentiment. Some bookstores stated that they would not stock the book. Dussmann, a Berlin bookstore, stated that one copy was available on the shelves in the history section, but that it would not be advertised and more copies would be available only on order. By January 2017, the German annotated edition had sold over 85,000 copies.\n\nAfter the party's poor showing in the 1928 elections, Hitler believed that the reason for his loss was the public's misunderstanding of his ideas. He then retired to Munich to dictate a sequel to \"Mein Kampf\" to expand on its ideas, with more focus on foreign policy.\n\nOnly two copies of the 200-page manuscript were originally made, and only one of these was ever made public. The document was neither edited nor published during the Nazi era and remains known as \"Zweites Buch\", or \"Second Book\". To keep the document strictly secret, in 1935 Hitler ordered that it be placed in a safe in an air raid shelter. It remained there until being discovered by an American officer in 1945.\n\nThe authenticity of the document found in 1945 has been verified by Josef Berg, a former employee of the Nazi publishing house Eher Verlag, and Telford Taylor, a former brigadier general of the United States Army Reserve and Chief Counsel at the Nuremberg war-crimes trials.\n\nIn 1958, the \"Zweites Buch\" was found in the archives of the United States by American historian Gerhard Weinberg. Unable to find an American publisher, Weinberg turned to his mentor – Hans Rothfels at the Institute of Contemporary History in Munich, and his associate Martin Broszat – who published \"Zweites Buch\" in 1961. A pirated edition was published in English in New York in 1962. The first authoritative English edition was not published until 2003 (\"Hitler's Second Book: The Unpublished Sequel to Mein Kampf,\" ).\n\n\n\n\n\n"}
{"id": "19645", "url": "https://en.wikipedia.org/wiki?curid=19645", "title": "Morpheus (disambiguation)", "text": "Morpheus (disambiguation)\n\nMorpheus is a god associated with sleep and dreams. In Ovid's \"Metamorphoses\" he is the son of Sleep, who appears in dreams in human form. Morpheus may also refer to:\n\n\n\n\n"}
{"id": "19648", "url": "https://en.wikipedia.org/wiki?curid=19648", "title": "May 26", "text": "May 26\n\n\n\n\n\n"}
{"id": "19649", "url": "https://en.wikipedia.org/wiki?curid=19649", "title": "MVS", "text": "MVS\n\nMultiple Virtual Storage, more commonly called MVS, was the most commonly used operating system on the System/370 and System/390 IBM mainframe computers. It was developed by IBM, but is unrelated to IBM's other mainframe operating systems, e.g., VSE, VM, TPF.\n\nFirst released in 1974, MVS was extended by program products with new names multiple times:\n\nAt first IBM described MVS as simply a new release of OS/VS2, but it was, in fact a major rewrite. OS/VS2 release 1 was an upgrade of OS/360 MVT that retained most of the original code and, like MVT, was mainly written in assembly language. The MVS core was almost entirely written in Assembler XF, although a few modules were written in PL/S, but not the performance-sensitive ones, in particular not the Input/Output Supervisor (IOS). IBM's use of \"OS/VS2\" emphasized upwards compatibility: application programs that ran under MVT did not even need recompiling to run under MVS. The same Job Control Language files could be used unchanged; utilities and other non-core facilities like TSO ran unchanged. IBM and users almost unanimously called the new system MVS from the start, and IBM continued to use the term \"MVS\" in the naming of later \"major\" versions such as MVS/XA.\n\nOS/360 MFT (Multitasking with a Fixed number of Tasks) provided multitasking: several memory partitions, each of a fixed size, were set up when the operating system was installed and when the operator redefined them. For example, there could be a small partition, two medium partitions, and a large partition. If there were two large programs ready to run, one would have to wait until the other finished and vacated the large partition.\n\nOS/360 MVT (Multitasking with a Variable number of Tasks) was an enhancement that further refined memory use. Instead of using fixed-size memory partitions, MVT allocated memory to regions for job steps as needed, provided enough \"contiguous\" physical memory was available. This was a significant advance over MFT's memory management, but had some weaknesses: if a job allocated memory dynamically (as most sort programs and database management systems do), the programmers had to estimate the job's maximum memory requirement and pre-define it for MVT. A job step that contained a mix of small and large programs wasted memory while the small programs ran. Most seriously, memory could become fragmented, i.e., the memory not used by current jobs could be divided into uselessly small chunks between the areas used by current jobs, and the only remedy was to wait some current jobs finished before starting any new ones.\n\nIn the early 1970s IBM sought to mitigate these difficulties by introducing virtual memory (which IBM called \"virtual storage\"), which allowed programs to request address spaces larger than physical memory. The original implementations had a single virtual address space, shared by all jobs. OS/VS1 was OS/360 MFT within a single virtual address space; OS/VS2 SVS was OS/360 MVT within a single virtual address space. So OS/VS1 and SVS in principle had the same disadvantages as MFT and MVT, but the impacts were less severe because jobs could request much larger address spaces and the requests came out of a 16 MB pool even if physical storage was smaller.\nIn the mid-1970s IBM introduced MVS, which not only supported virtual storage that was larger than the available real storage, as did SVS, but also allowed an indefinite number of applications to run in different address spaces. Two concurrent programs might try to access the same virtual memory address, but the virtual memory system redirected these requests to different areas of physical memory. Each of these address spaces consisted of three areas: an operating system (one instance shared by all jobs), an application area unique for each application, and a shared virtual area used for various purposes, including inter-job communication. IBM promised that application areas would always be at least 8 MB. This made MVS the perfect solution for business problems that resulted from the need to run more applications.\n\nMVS maximized processing potential by providing multiprogramming and multiprocessing capabilities. Like its MVT and OS/VS2 SVS predecessors, MVS supported multiprogramming; program instructions and associated data are scheduled by a control program and given processing cycles. Unlike a single-programming operating system, these systems maximize the use of the processing potential by dividing processing cycles among the instructions associated with several different concurrently running programs. This way, the control program does not have to wait for the I/O operation to complete before proceeding. By executing the instructions for multiple programs, the computer is able to switch back and forth between active and inactive programs.\n\nEarly editions of MVS (mid-1970s) were among the first of the IBM OS series to support multiprocessor configurations, though the M65MP variant of OS/360 running on 360 Models 65 and 67 had provided limited multiprocessor support. The 360 Model 67 had also hosted the multiprocessor capable TSS/360, MTS and CP-67 operating systems. Because multiprocessing systems can execute instructions simultaneously, they offer greater processing power than single-processing system. As a result, MVS was able to address the business problems brought on by the need to process large amounts of data.\n\nMultiprocessing systems are either loosely coupled, which means that each computer has access to a common workload, or tightly coupled, which means that the computers share the same real storage and are controlled by a single copy of the operating system. MVS retained both the loosely coupled multiprocessing of Attached Support Processor (ASP) and the tightly coupled multiprocessing of OS/360 Model 65 Multiprocessing. In tightly coupled systems, two CPUs shared concurrent access to the same memory (and copy of the operating system) and peripherals, providing greater processing power and a degree of graceful degradation if one CPU failed. In loosely coupled configurations each of a group of processors (single and / or tightly coupled) had its own memory and operating system but shared peripherals and the operating system component JES3 allowed managing the whole group from one console. This provided greater resilience and let operators decide which processor should run which jobs from a central job queue. MVS JES3 gave users the opportunity to network together two or more data processing systems via shared disks and Channel-to-Channel Adapters (CTCA's). This capability eventually became available to JES2 users as Multi-Access SPOOL (MAS).\n\nMVS originally supported 24-bit addressing (i.e., up to 16 MB). As the underlying hardware progressed, it supported 31-bit (XA and ESA; up to 2048 MB) and now (as z/OS) 64-bit addressing. The most significant motives for the rapid upgrade to 31-bit addressing were the growth of large transaction-processing networks, mostly controlled by CICS, which ran in a single address space—and the DB2 relational database management system needed more than 8 MB of application address space to run efficiently. (Early versions were configured into two address spaces that communicated via the shared virtual area, but this imposed a significant overhead since all such communications had transmit via the operating system.)\n\nThe main user interfaces to MVS are: Job Control Language (JCL), which was originally designed for batch processing but from the 1970s onwards was also used to start and allocate resources to long-running interactive jobs such CICS; and TSO (Time Sharing Option), the interactive time-sharing interface, which was mainly used to run development tools and a few end-user information systems. ISPF is a TSO application for users on 3270-family terminals (and later, on VM as well), which allows the user to accomplish the same tasks as TSO's command line but in a menu and form oriented manner, and with a full screen editor and file browser. TSO's basic interface is command line, although facilities were added later for form-driven interfaces).\n\nMVS took a major step forward in fault-tolerance, built on the earlier STAE facility, that IBM called \"software recovery\". IBM decided to do this after years of practical real-world experience with MVT in the business world. System failures were now having major impacts on customer businesses, and IBM decided to take a major design jump, to assume that despite the very best software development and testing techniques, that 'problems WILL occur.' This profound assumption was pivotal in adding great percentages of fault-tolerance code to the system and likely contributed to the system's success in tolerating software and hardware failures. Statistical information is hard to come by to prove the value of these design features (how can you measure 'prevented' or 'recovered' problems?), but IBM has, in many dimensions, enhanced these fault-tolerant software recovery and rapid problem resolution features, over time.\n\nThis design specified a hierarchy of error-handling programs, in system (kernel/'privileged') mode, called Functional Recovery Routines, and in user ('task' or 'problem program') mode, called \"ESTAE\" (Extended Specified Task Abnormal Exit routines) that were invoked in case the system detected an error (actually, hardware processor or storage error, or software error). Each recovery routine made the 'mainline' function reinvokable, captured error diagnostic data sufficient to debug the causing problem, and either 'retried' (reinvoke the mainline) or 'percolated' (escalated error processing to the next recovery routine in the hierarchy).\n\nThus, with each error the system captured diagnostic data, and attempted to perform a repair and keep the system up. The worst thing possible was to take down a user address space (a 'job') in the case of unrepaired errors. Though it was an initial design point, it was not until the most recent MVS version (z/OS), that recovery program was not only guaranteed its own recovery routine, but each recovery routine now has its own recovery routine. This recovery structure was embedded in the basic MVS control program, and programming facilities are available and used by application program developers and 3rd party developers.\n\nPractically, the MVS software recovery made problem debugging both easier and more difficult. Software recovery requires that programs leave 'tracks' of where they are and what they are doing, thus facilitating debugging—but the fact that processing progresses despite an error can overwrite the tracks. Early date capture at the time of the error maximizes debugging, and facilities exist for the recovery routines (task and system mode, both) to do this.\n\nIBM included additional criteria for a major software problem that required IBM service. If a mainline component failed to initiate software recovery, that was considered a valid reportable failure. Also, if a recovery routine failed to collect significant diagnostic data such that the original problem was solvable by data collected by that recovery routine, IBM standards dictated that this fault was reportable and required repair. Thus, IBM standards, when rigorously applied, encouraged continuous improvement.\n\nIBM introduced an on-demand hypervisor, a major serviceability tool, called Dynamic Support System (DSS), in the first release of MVS. This facility could be invoked to initiate a session to create diagnostic procedures, or invoke already-stored procedures. The procedures 'trapped' special events, such as the loading of a program, device I/O, system procedure calls, and then triggered the activation of the previously defined procedures. These procedures, which could be invoked recursively, allowed for reading and writing of data, and alteration of instruction flow. Program Event Recording hardware was used. Due to the overhead of this tool, it was removed from customer-available MVS systems. Program-Event Recording (PER) exploitation was performed by the enhancement of the diagnostic \"SLIP\" command with the introduction of the PER support (SLIP/Per) in SU 64/65 (1978).\n\nMultiple copies of MVS (or other IBM operating systems) could share the\nsame machine if that machine was controlled by VM/370. In this case VM/370 was the real operating system, and regarded the \"guest\" operating systems as applications with unusually high privileges. As a result of later hardware enhancements one instance of an operating system (either MVS, or VM with guests, or other) could also occupy a Logical Partition (LPAR) instead of an entire physical system.\n\nMultiple MVS instances can be organized and collectively administered in a structure called a \"systems complex\" or \"sysplex\", introduced in September, 1990. Instances interoperate through a software component called a Cross-system Coupling Facility (XCF) and a hardware component called a \"Hardware Coupling Facility\" (CF or Integrated Coupling Facility, ICF, if co-located on the same mainframe hardware). Multiple sysplexes can be joined via standard network protocols such as IBM's proprietary Systems Network Architecture (SNA) or, more recently, via TCP/IP. The z/OS operating system (MVS' most recent descendant) also has native support to execute POSIX and Single UNIX Specification applications. The support began with MVS/SP V4R3, and IBM has obtained UNIX 95 certification for z/OS V1R2 and later.\n\nThe system is typically used in business and banking, and applications are often written in COBOL. COBOL programs were traditionally used with transaction processing systems like IMS and CICS. For a program running in CICS, special EXEC CICS statements are inserted in the COBOL source code. A preprocessor (translator) replaces those EXEC CICS statements with the appropriate COBOL code to call CICS before the program is compiled — not altogether unlike SQL used to call DB2. Applications can also be written in other languages such as C, C++, Java, assembly language, FORTRAN, BASIC, RPG, and REXX. Language support is packaged as a common component called \"Language Environment\" or \"LE\" to allow uniform debugging, tracing, profiling, and other language independent functions.\n\nMVS systems are traditionally accessed by 3270 terminals or by PCs running 3270 emulators. However, many mainframe applications these days have custom web or GUI interfaces. The z/OS operating system has built-in support for TCP/IP. System management, done in the past with a 3270 terminal, is now done through the Hardware Management Console (HMC) and, increasingly, Web interfaces. Operator consoles are provided through 2074 emulators, so you are unlikely to see any S/390 or zSeries processor with a real 3270 connected to it.\n\nThe native character encoding scheme of MVS and its peripherals is EBCDIC, but the TR instruction made it easy to translate to other 7- and 8-bit codes. Over time IBM added hardware-accelerated services to perform translation to and between larger codes, hardware-specific service for Unicode transforms and software support of, e.g., ASCII, ISO/IEC 8859, UTF-8, UTF-16, and UTF-32. The software translation services take source and destination code pages as inputs.\n\nFiles are properly called data sets in MVS. Names of those files are organized in \"catalogs\" that are VSAM files themselves.\n\nData set names (DSNs, mainframe term for filenames) are organized in a hierarchy whose levels are separated with dots, e.g. \"DEPT01.SYSTEM01.FILE01\". Each level in the hierarchy can be up to eight characters long. The total filename length is a maximum of 44 characters including dots. By convention, the components separated by the dots are used to organize files similarly to directories in other operating systems. For example, there were utility programs that performed similar functions to those of Windows Explorer (but without the GUI and usually in batch processing mode) - adding, renaming or deleting new elements and reporting all the contents of a specified element. However, unlike in many other systems, these levels are not usually actual directories but just a naming convention (like the original Macintosh File System, where folder hierarchy was an illusion maintained by the Finder). TSO supports a default prefix for files (similar to a \"current directory\" concept), and RACF supports setting up access controls based on filename patterns, analogous to access controls on directories on other platforms.\n\nAs with other members of the OS family, MVS' data sets were record-oriented. MVS inherited three main types from its predecessors:\nSequential and ISAM datasets could store either fixed-length or variable length records, and all types could occupy more than one disk volume.\n\nAll of these are based on the VTOC disk structure.\n\nEarly IBM database management systems used various combinations of ISAM and BDAM datasets - usually BDAM for the actual data storage and ISAM for indexes.\n\nIn the early 1970s IBM's virtual memory operating systems introduced a new file management component, VSAM, which provided similar facilities:\n\nThese VSAM formats became the basis of IBM's database management systems, IMS/VS and DB2 - usually ESDS for the actual data storage and KSDS for indexes.\n\nVSAM also included a catalog component used for MVS' master catalog.\n\nPartitioned datasets (PDS) were sequential datasets subdivided into \"members\" that could be processed as sequential files in their own right. The most important use of PDS was for program libraries - system administrators used the main PDS as a way to allocate disk space to a project and the project team then created and edited the members.\n\nGeneration Data Groups (GDGs) were originally designed to support grandfather-father-son backup procedures - if a file was modified, the changed version became the new \"son\", the previous \"son\" became the \"father\", the previous \"father\" became the \"grandfather\" and the previous \"grandfather\" was deleted. But one could set up GDGs with more than 3 generations and some applications used GDGs to collect data from several sources and feed the information to one program - each collecting program created a new generation of the file and the final program read the whole group as a single sequential file (by not specifying a generation in the JCL).\n\nModern versions of MVS (e.g., z/OS) also support POSIX-compatible \"slash\" filesystems along with facilities for integrating the two filesystems. That is, the OS can make an MVS dataset appear as a file to a POSIX program or subsystem. These newer filesystems include Hierarchical File System (HFS) (not to be confused with Apple's Hierarchical File System) and zFS (not to be confused with Sun's ZFS).\n\nPrograms running on network-connected computers (such as the AS/400) can use local data management interfaces to transparently create, manage, and access VSAM record-oriented files by using client-server products implemented according to Distributed Data Management Architecture (DDM). DDM is also the base architecture for the MVS DB2 server that implements Distributed Relational Database Architecture (DRDA).\n\nMVS is now a part of z/OS, older MVS releases are no longer supported by IBM and since 2007 only 64-bit z/OS releases are supported. z/OS supports running older 24-bit and 31-bit MVS applications alongside 64-bit applications.\n\nMVS releases up to 3.8j (24-bit, released in 1981) were freely available and it is now possible to run the MVS 3.8j release in mainframe emulators for free.\n\nMVS/370 is a generic term for all versions of the MVS operating system prior to MVS/XA. The System/370 architecture, at the time MVS was released, supported only 24-bit virtual addresses, so the MVS/370 operating system architecture is based on a 24-bit address. Because of this 24-bit address length, programs running under MVS/370 are each given 16 megabytes of contiguous virtual storage.\n\nMVS/XA, or Multiple Virtual Storage/Extended Architecture, was a version of MVS that supported the 370-XA architecture, which expanded addresses from 24 bits to 31 bits, providing a 2 gigabyte addressable memory area. It also supported a 24-bit legacy addressing mode for older 24-bit applications (i.e. those that stored a 24-bit address in the lower 24 bits of a 32-bit word and utilized the upper 8 bits of that word for other purposes).\n\nMVS/ESA: MVS Enterprise System Architecture. Version of MVS, first introduced as MVS/SP Version 3 in February 1988. Replaced by/renamed as OS/390 late 1995 and subsequently as z/OS.\n\nMVS/ESA OpenEdition: upgrade to Version 4 Release 3 of MVS/ESA announced February 1993 with support for POSIX and other standards. While the initial release only had National Institute of Standards and Technology (NIST) certification for Federal Information Processing Standard (FIPS) 151 compliance, subsequent releases were certified at higher levels and by other organizations, e.g. X/Open and its successor, The Open Group. It included about 1 million new lines of code, which provide an API shell, utilities, and an extended user interface. Works with a hierarchical file system provided by DFSMS (Data Facility System Managed Storage). The shell and utilities are based on Mortice Kerns' InterOpen products. Independent specialists estimate that it was over 80% open systems-compliant—more than most Unix systems. DCE2 support announced February 1994, and many application development tools in March 1995. From mid 1995, as all of the open features became a standard part of vanilla MVS/ESA SP Version 5 Release 1, IBM stopped distinguishing OpenEdition from the operating system. Under OS/390, it became UNIX System Services, and has kept that name under z/OS.\n\nIn late 1995 IBM bundled MVS with several program products and changed the name from MVS/ESA to OS/390.\n\nThe current level of MVS is marketed as z/OS.\n\nJapanese mainframe manufacturers Fujitsu and Hitachi both repeatedly and illegally obtained IBM's MVS source code and internal documentation in one of the 20th century's most famous cases of industrial espionage. Fujitsu relied heavily on IBM's code in its MSP mainframe operating system, and likewise Hitachi did the same for its VOS3 operating system. MSP and VOS3 were heavily marketed in Japan, where they still hold a substantial share of the mainframe installed base, but also to some degree in other countries, notably Australia. Even IBM's bugs and documentation misspellings were faithfully copied. IBM cooperated with the U.S. Federal Bureau of Investigation in a sting operation, reluctantly supplying Fujitsu and Hitachi with proprietary MVS and mainframe hardware technologies during the course of multi-year investigations culminating in the early 1980s—investigations which implicated senior company managers and even some Japanese government officials. Amdahl, however, was not involved in Fujitsu's theft of IBM's intellectual property. Any communications from Amdahl to Fujitsu were through \"Amdahl Only Specifications\" which were scrupulously cleansed of any IBM IP or any references to IBM's IP.\n\nSubsequent to the investigations, IBM reached multimillion-dollar settlements with both Fujitsu and Hitachi, collecting substantial fractions of both companies' profits for many years. Reliable reports indicate that the settlements exceeded US$500,000,000.\n\nThe three companies have long since amicably agreed to many joint business ventures. For example, in 2000 IBM and Hitachi collaborated on developing the IBM z900 mainframe model.\n\nBecause of this historical copying, MSP and VOS3 are properly classified as \"forks\" of MVS, and many third party software vendors with MVS-compatible products were able to produce MSP- and VOS3-compatible versions with little or no modification.\n\nWhen IBM introduced its 64-bit z/Architecture mainframes in the year 2000, IBM also introduced the 64-bit z/OS operating system, the direct successor to OS/390 and MVS. Fujitsu and Hitachi opted not to license IBM's z/Architecture for their quasi-MVS operating systems and hardware systems, and so MSP and VOS3, while still nominally supported by their vendors, maintain most of MVS's 1980s architectural limitations to the present day. Since z/OS still supports MVS-era applications and technologies—indeed, z/OS still contains most of MVS's code, albeit greatly enhanced and improved over decades of evolution—applications (and operational procedures) running on MSP and VOS3 can move to z/OS much more easily than to other operating systems.\n\n\n\n"}
{"id": "19652", "url": "https://en.wikipedia.org/wiki?curid=19652", "title": "Monoid", "text": "Monoid\n\nIn abstract algebra, a branch of mathematics, a monoid is an algebraic structure with a single associative binary operation and an identity element.\n\nMonoids are studied in semigroup theory, because they are semigroups with identity. Monoids occur in several branches of mathematics; for instance, they can be regarded as categories with a single object. Thus, they capture the idea of function composition within a set. In fact, all functions from a set into itself form naturally a monoid with respect to function composition. Monoids are also commonly used in computer science, both in its foundational aspects and in practical programming. The set of strings built from a given set of characters is a free monoid. The transition monoid and syntactic monoid are used in describing finite-state machines, whereas trace monoids and history monoids provide a foundation for process calculi and concurrent computing. Some of the more important results in the study of monoids are the Krohn–Rhodes theorem and the star height problem. The history of monoids, as well as a discussion of additional general properties, are found in the article on semigroups.\n\nSuppose that \"S\" is a set and • is some binary operation , then \"S\" with • is a monoid if it satisfies the following two axioms:\n\n\nIn other words, a monoid is a semigroup with an identity element. It can also be thought of as a magma with associativity and identity. The identity element of a monoid is unique. For this reason the identity is regarded as a constant, i. e. 0-ary (or nullary) operation. The monoid therefore is characterized by specification of the triple (\"S\", • , \"e\").\n\nDepending on the context, the symbol for the binary operation may be omitted, so that the operation is denoted by juxtaposition; for example, the monoid axioms may be written formula_1 and This notation does not imply that it is numbers being multiplied.\n\nA monoid in which each element has an inverse is a group.\n\nA submonoid of a monoid is a subset \"N\" of \"M\" that is closed under the monoid operation and contains the identity element \"e\" of \"M\". Symbolically, \"N\" is a submonoid of \"M\" if , whenever , and . \"N\" is thus a monoid under the binary operation inherited from \"M\".\n\nA subset \"S\" of \"M\" is said to be a generator of \"M\" if \"M\" is the smallest set containing \"S\" that is closed under the monoid operation, or equivalently \"M\" is the result of applying the finitary closure operator to \"S\". If there is a generator of \"M\" that has finite cardinality, then \"M\" is said to be finitely generated. Not every set \"S\" will generate a monoid, as the generated structure may lack an identity element.\n\nA monoid whose operation is commutative is called a commutative monoid (or, less commonly, an abelian monoid). Commutative monoids are often written additively. Any commutative monoid is endowed with its algebraic preordering ≤, defined by if there exists \"z\" such that . An order-unit of a commutative monoid \"M\" is an element \"u\" of \"M\" such that for any element \"x\" of \"M\", there exists a positive integer \"n\" such that . This is often used in case \"M\" is the positive cone of a partially ordered abelian group \"G\", in which case we say that \"u\" is an order-unit of \"G\".\n\nA monoid for which the operation is commutative for some, but not all elements is a trace monoid; trace monoids commonly occur in the theory of concurrent computation.\n\n\nMoreover, \"f\" can be considered as a function on the points formula_6 given by\n\nor, equivalently\n\nMultiplication of elements in formula_2 is then given by function composition.\n\nNote also that when formula_10 then the function \"f\" is a permutation of formula_6\nand gives the unique cyclic group of order \"n\".\n\nIn a monoid, one can define positive integer powers of an element \"x\" : \"x\" = \"x\", and \"x\" = \"x\" • ... • \"x\" (\"n\" times) for \"n\" > 1 . The rule of powers \"x\" = \"x\" • \"x\" is obvious.\n\nFrom the definition of a monoid, one can show that the identity element \"e\" is unique. Then, for any \"x\", one can set \"x\" = \"e\" and the rule of powers is still true with nonnegative exponents.\n\nIt is possible to define invertible elements: an element \"x\" is called invertible if there exists an element \"y\" such that and . The element \"y\" is called the inverse of \"x\". If \"y\" and \"z\" are inverses of \"x\", then by associativity . Thus inverses, if they exist, are unique.\n\nIf \"y\" is the inverse of \"x\", one can define negative powers of \"x\" by setting and (\"n\" times) for . And the rule of exponents is still verified for all integers \"n\", \"p\". This is why the inverse of \"x\" is usually written \"x\". The set of all invertible elements in a monoid \"M\", together with the operation •, forms a group. In that sense, every monoid contains a group (possibly only the trivial group consisting of only the identity).\n\nHowever, not every monoid sits inside a group. For instance, it is perfectly possible to have a monoid in which two elements \"a\" and \"b\" exist such that holds even though \"b\" is not the identity element. Such a monoid cannot be embedded in a group, because in the group we could multiply both sides with the inverse of \"a\" and would get that , which isn't true. A monoid has the cancellation property (or is cancellative) if for all \"a\", \"b\" and \"c\" in \"M\", always implies and always implies . A commutative monoid with the cancellation property can always be embedded in a group via the Grothendieck construction. That is how the additive group of the integers (a group with operation +) is constructed from the additive monoid of natural numbers (a commutative monoid with operation + and cancellation property). However, a non-commutative cancellative monoid need not be embeddable in a group.\n\nIf a monoid has the cancellation property and is \"finite\", then it is in fact a group. Proof: Fix an element \"x\" in the monoid. Since the monoid is finite, for some . But then, by cancellation we have that where \"e\" is the identity. Therefore, , so \"x\" has an inverse.\n\nThe right- and left-cancellative elements of a monoid each in turn form a submonoid (i.e. obviously include the identity and not so obviously are closed under the operation). This means that the cancellative elements of any commutative monoid can be extended to a group.\n\nIt turns out that requiring the cancellative property in a monoid is not required to perform the Grothendieck construction – commutativity is sufficient. However, if the original monoid has an absorbing element then its Grothendieck group is the trivial group. Hence the homomorphism is, in general, not injective.\n\nAn inverse monoid is a monoid where for every \"a\" in \"M\", there exists a unique \"a\" in \"M\" such that and . If an inverse monoid is cancellative, then it is a group.\n\nIn the opposite direction, a zerosumfree monoid is an additively written monoid in which implies that and : equivalently, that no element other than zero has an additive inverse.\n\nLet \"M\" be a monoid, with the binary operation denoted by • and the identity element denoted by \"e\". Then a (left) \"M\"-act (or left act over \"M\") is a set \"X\" together with an operation which is compatible with the monoid structure as follows:\nThis is the analogue in monoid theory of a (left) group action. Right \"M\"-acts are defined in a similar way. A monoid with an act is also known as an operator monoid. Important examples include transition systems of semiautomata. A transformation semigroup can be made into an operator monoid by adjoining the identity transformation.\n\nA homomorphism between two monoids and is a function such that\nwhere \"e\" and \"e\" are the identities on \"M\" and \"N\" respectively. Monoid homomorphisms are sometimes simply called monoid morphisms.\n\nNot every semigroup homomorphism is a monoid homomorphism, since it may not map the identity to the identity of the target monoid, even though the element it maps the identity to will be an identity of the image of the mapping. In contrast, a semigroup homomorphism between groups is always a group homomorphism, as it necessarily preserves the identity. Since for monoids this isn't always true, it is necessary to state this as a separate requirement.\n\nA bijective monoid homomorphism is called a monoid isomorphism. Two monoids are said to be isomorphic if there is a monoid isomorphism between them.\n\nMonoids may be given a presentation, much in the same way that groups can be specified by means of a group presentation. One does this by specifying a set of generators Σ, and a set of relations on the free monoid Σ. One does this by extending (finite) binary relations on Σ to monoid congruences, and then constructing the quotient monoid, as above.\n\nGiven a binary relation , one defines its symmetric closure as . This can be extended to a symmetric relation by defining if and only if and for some strings with . Finally, one takes the reflexive and transitive closure of \"E\", which is then a monoid congruence.\n\nIn the typical situation, the relation \"R\" is simply given as a set of equations, so that formula_12. Thus, for example,\nis the equational presentation for the bicyclic monoid, and\n\nis the plactic monoid of degree 2 (it has infinite order). Elements of this plactic monoid may be written as formula_15 for integers \"i\", \"j\", \"k\", as the relations show that \"ba\" commutes with both \"a\" and \"b\".\n\nMonoids can be viewed as a special class of categories. Indeed, the axioms required of a monoid operation are exactly those required of morphism composition when restricted to the set of all morphisms whose source and target is a given object. That is,\nMore precisely, given a monoid , one can construct a small category with only one object and whose morphisms are the elements of \"M\". The composition of morphisms is given by the monoid operation •.\n\nLikewise, monoid homomorphisms are just functors between single object categories. So this construction gives an equivalence between the category of (small) monoids Mon and a full subcategory of the category of (small) categories Cat. Similarly, the category of groups is equivalent to another full subcategory of Cat.\n\nIn this sense, category theory can be thought of as an extension of the concept of a monoid. Many definitions and theorems about monoids can be generalised to small categories with more than one object. For example, a quotient of a category with one object is just a quotient monoid.\n\nMonoids, just like other algebraic structures, also form their own category, Mon, whose objects are monoids and whose morphisms are monoid homomorphisms.\n\nThere is also a notion of monoid object which is an abstract definition of what is a monoid in a category. A monoid object in Set is just a monoid.\n\nIn computer science, many abstract data types can be endowed with a monoid structure. In a common pattern, a sequence of elements of a monoid is \"folded\" or \"accumulated\" to produce a final value. For instance, many iterative algorithms need to update some kind of \"running total\" at each iteration; this pattern may be elegantly expressed by a monoid operation. Alternatively, the associativity of monoid operations ensures that the operation can be parallelized by employing a prefix sum or similar algorithm, in order to utilize multiple cores or processors efficiently.\n\nGiven a sequence of values of type \"M\" with identity element formula_16 and associative operation formula_17, the \"fold\" operation is defined as follows:\n\nIn addition, any data structure can be 'folded' in a similar way, given a serialization of its elements. For instance, the result of \"folding\" a binary tree might differ depending on pre-order vs. post-order tree traversal.\n\nAn application of monoids in computer science is so-called MapReduce programming model (see Encoding Map-Reduce As A Monoid With Left Folding). MapReduce, in computing, consists of two or three operations. Given a dataset, \"Map\" consists of mapping arbitrary data to elements of a specific monoid. \"Reduce\" consists of folding those elements, so that in the end we produce just one element.\n\nFor example, if we have a multiset, in a program it is represented as a map from elements to their numbers. Elements are called keys in this case. The number of distinct keys may be too big, and in this case the multiset is being sharded. To finalize reduction properly, the \"Shuffling\" stage regroups the data among the nodes. If we do not need this step, the whole Map/Reduce consists of mapping and reducing; both operation are parallelizable, the former due to its element-wise nature, the latter due to associativity of the monoid.\n\nA complete monoid is a commutative monoid equipped with an infinitary sum operation formula_19 for any index set \"I\" such that:\n\nformula_20\n\nand\n\nformula_21\n\nA continuous monoid is an ordered commutative monoid in which every directed set has a least upper bound compatible with the monoid operation:\n\nformula_22\n\nThese two concepts are closely related: a continuous monoid is a complete monoid in which the infinitary sum may be defined as\n\nwhere the supremum on the right runs over all finite subsets \"E\" of \"I\" and each sum on the right is a finite sum in the monoid.\n\n\n\n"}
{"id": "19653", "url": "https://en.wikipedia.org/wiki?curid=19653", "title": "May 31", "text": "May 31\n\n\n\n"}
{"id": "19654", "url": "https://en.wikipedia.org/wiki?curid=19654", "title": "May 30", "text": "May 30\n\n\n\n"}
{"id": "19655", "url": "https://en.wikipedia.org/wiki?curid=19655", "title": "May 23", "text": "May 23\n\n\n\n\n"}
{"id": "19659", "url": "https://en.wikipedia.org/wiki?curid=19659", "title": "May 16", "text": "May 16\n\n\n"}
{"id": "19660", "url": "https://en.wikipedia.org/wiki?curid=19660", "title": "May 22", "text": "May 22\n\n\n\n"}
{"id": "19662", "url": "https://en.wikipedia.org/wiki?curid=19662", "title": "Mean value theorem", "text": "Mean value theorem\n\nIn mathematics, the mean value theorem states, roughly, that for a given planar arc between two endpoints, there is at least one point at which the tangent to the arc is parallel to the secant through its endpoints.\n\nThis theorem is used to prove statements about a function on an interval starting from local hypotheses about derivatives at points of the interval.\n\nMore precisely, if formula_1 is a continuous function on the closed interval formula_2, and differentiable on the open interval formula_3, then there exists a point formula_4 in formula_3 such that:\n\nIt is one of the most important results in real analysis.\n\nA special case of this theorem was first described by Parameshvara (1370–1460), from the Kerala School of Astronomy and Mathematics in India, in his commentaries on Govindasvāmi and Bhāskara II. A restricted form of the theorem was proved by Michel Rolle in 1691; the result was what is now known as Rolle's theorem, and was proved only for polynomials, without the techniques of calculus. The mean value theorem in its modern form was stated and proved by Augustin Louis Cauchy in 1823.\n\nLet formula_7 be a continuous function on the closed interval formula_2 , and differentiable on the open interval formula_3, where <math>a . Then there exists some formula_4 in formula_3 such that\n\nThe mean value theorem is a generalization of Rolle's theorem, which assumes formula_13, so that the right-hand side above is zero.\n\nThe mean value theorem is still valid in a slightly more general setting. One only needs to assume that formula_7 is continuous on formula_2 , and that for every formula_16 in formula_3 the limit\n\nexists as a finite number or equals formula_19 or formula_20 . If finite, that limit equals formula_21 . An example where this version of the theorem applies is given by the real-valued cube root function mapping formula_22 , whose derivative tends to infinity at the origin.\n\nNote that the theorem, as stated, is false if a differentiable function is complex-valued instead of real-valued. For example, define formula_23 for all real formula_16 . Then\nwhile formula_26 for any real formula_16 .\n\nThese formal statements are also known as Lagrange's Mean Value Theorem.\n\nThe expression formula_28 gives the slope of the line joining the points formula_29 and formula_30 , which is a chord of the graph of formula_1 , while formula_21 gives the slope of the tangent to the curve at the point formula_33 . Thus the Mean value theorem says that given any chord of a smooth curve, we can find a point lying between the end-points of the chord such that the tangent at that point is parallel to the chord. The following proof illustrates this idea.\n\nDefine formula_34 , where formula_35 is a constant. Since formula_1 is continuous on formula_2 and differentiable on formula_3 , the same is true for formula_39 . We now want to choose formula_35 so that formula_39 satisfies the conditions of Rolle's theorem. Namely\n\nBy Rolle's theorem, since formula_39 is differentiable and formula_44 , there is some formula_4 in formula_3 for which formula_47 , and it follows from the equality formula_34 that,\n\nAssume that \"f\" is a continuous, real-valued function, defined on an arbitrary interval \"I\" of the real line. If the derivative of \"f\" at every interior point of the interval \"I\" exists and is zero, then \"f\" is constant in the interior.\n\nProof: Assume the derivative of \"f\" at every interior point of the interval \"I\" exists and is zero. Let (\"a\", \"b\") be an arbitrary open interval in \"I\". By the mean value theorem, there exists a point \"c\" in (\"a\",\"b\") such that\n\nThis implies that \"f\"(\"a\") = \"f\"(\"b\"). Thus, \"f\" is constant on the interior of \"I\" and thus is constant on \"I\" by continuity. (See below for a multivariable version of this result.)\n\nRemarks: \n\nCauchy's mean value theorem, also known as the extended mean value theorem, is a generalization of the mean value theorem. It states: If functions \"f\" and \"g\" are both continuous on the closed interval [\"a\", \"b\"], and differentiable on the open interval (\"a\", \"b\"), then there exists some \"c\" ∈ (\"a\", \"b\"), such that\n\nOf course, if and if , this is equivalent to:\n\nGeometrically, this means that there is some tangent to the graph of the curve\n\nwhich is parallel to the line defined by the points (\"f\"(\"a\"), \"g\"(\"a\")) and (\"f\"(\"b\"), \"g\"(\"b\")). However Cauchy's theorem does not claim the existence of such a tangent in all cases where (\"f\"(\"a\"), \"g\"(\"a\")) and (\"f\"(\"b\"), \"g\"(\"b\")) are distinct points, since it might be satisfied only for some value \"c\" with , in other words a value for which the mentioned curve is stationary; in such points no tangent to the curve is likely to be defined at all. An example of this situation is the curve given by\n\nwhich on the interval [−1, 1] goes from the point (−1, 0) to (1, 0), yet never has a horizontal tangent; however it has a stationary point (in fact a cusp) at .\n\nCauchy's mean value theorem can be used to prove l'Hôpital's rule. The mean value theorem is the special case of Cauchy's mean value theorem when .\n\nThe proof of Cauchy's mean value theorem is based on the same idea as the proof of the mean value theorem.\n\n\n\nAssume that formula_58 and formula_59 are differentiable functions on formula_3 that are continuous on formula_2. Define\n\nThere exists formula_63 such that formula_64.\n\nNotice that\nand if we place formula_66, we get Cauchy's mean value theorem. If we place formula_66 and formula_68 we get Lagrange's mean value theorem.\n\nThe proof of the generalization is quite simple: each of formula_69 and formula_70 are determinants with two identical rows, hence formula_71. The Rolle's theorem implies that there exists formula_72 such that formula_73.\n\nThe mean value theorem generalizes to real functions of multiple variables. The trick is to use parametrization to create a real function of one variable, and then apply the one-variable theorem.\n\nLet formula_74 be an open convex subset of formula_75 , and let formula_76 be a differentiable function. Fix points formula_77 , and define formula_78 . Since formula_39 is a differentiable function in one variable, the mean value theorem gives:\n\nfor some formula_4 between 0 and 1. But since formula_82 and formula_83 , computing formula_84 explicitly we have:\n\nwhere formula_86 denotes a gradient and formula_87 a dot product. Note that this is an exact analog of the theorem in one variable (in the case formula_88 this \"is\" the theorem in one variable). By the Cauchy–Schwarz inequality, the equation gives the estimate:\n\nIn particular, when the partial derivatives of formula_1 are bounded, formula_1 is Lipschitz continuous (and therefore uniformly continuous). Note that formula_1 is not assumed to be continuously differentiable or continuous on the closure of formula_74 . However, in order to use the chain rule to compute formula_94, we really do need to know that formula_1 is differentiable; the existence of the formula_96 and formula_97 partial derivatives is not sufficient by itself .\n\nAs an application of the above, we prove that formula_1 is constant if formula_74 is open and connected and every partial derivative of formula_1 is 0. Pick some point formula_101 , and let formula_102 . We want to show formula_103 for every formula_104 . For that, let formula_105 . Then \"E\" is closed and nonempty. It is open too: for every formula_106 ,\n\nfor every formula_108 in some neighborhood of formula_16 . (Here, it is crucial that formula_16 and formula_108 are sufficiently close to each other.) Since formula_74 is connected, we conclude formula_113 .\n\nThe above arguments are made in a coordinate-free manner; hence, they generalize to the case when formula_74 is a subset of a Banach space.\n\nThere is no exact analog of the mean value theorem for vector-valued functions.\n\nIn \"Principles of Mathematical Analysis,\" Rudin gives an inequality which can be applied to many of the same situations to which the mean value theorem is applicable in the one dimensional case:\n\nTheorem. \"For a continuous vector-valued function formula_115 differentiable on formula_3, there exists formula_117 such that formula_118.\"\n\nJean Dieudonné in his classic treatise \"Foundations of Modern Analysis\" discards the mean value theorem and replaces it by mean inequality as the proof is not constructive and one cannot find the mean value and in applications one only needs mean inequality. Serge Lang in \"Analysis I \"uses the mean value theorem, in integral form, as an instant reflex but this use requires the continuity of the derivative. If one uses the Henstock–Kurzweil integral one can have the mean value theorem in integral form without the additional assumption that derivative should be continuous as every derivative is Henstock–Kurzweil integrable. The problem is roughly speaking the following: If \"f\" : \"U\" → R is a differentiable function (where \"U\" ⊂ R is open) and if \"x\" + \"th\", \"x, h\" ∈ R, \"t\" ∈ [0, 1] is the line segment in question (lying inside \"U\"), then one can apply the above parametrization procedure to each of the component functions \"f\" (\"i\" = 1, ..., \"m\") of \"f\" (in the above notation set \"y\" = \"x\" + \"h\"). In doing so one finds points \"x\" + \"th\" on the line segment satisfying\n\nBut generally there will not be a \"single\" point \"x\" + \"t*h\" on the line segment satisfying\n\nfor all \"i\" \"simultaneously\". For example, define:\n\nThen formula_122, but formula_123 and formula_124 are never simultaneously zero as formula_16 ranges over formula_126.\n\nHowever a certain type of generalization of the mean value theorem to vector-valued functions is obtained as follows: Let \"f\" be a continuously differentiable real-valued function defined on an open interval \"I\", and let \"x\" as well as \"x\" + \"h\" be points of \"I\". The mean value theorem in one variable tells us that there exists some \"t*\" between 0 and 1 such that\n\nOn the other hand, we have, by the fundamental theorem of calculus followed by a change of variables,\n\nThus, the value \"f′\"(\"x\" + \"t*h\") at the particular point \"t*\" has been replaced by the mean value\n\nThis last version can be generalized to vector valued functions:\n\nProof. Let \"f\", ..., \"f\" denote the components of \"f\" and define:\n\nThen we have\n\nThe claim follows since \"Df\" is the matrix consisting of the components formula_133\n\nProof. Let \"u\" in R denote the value of the integral\nNow we have (using the Cauchy–Schwarz inequality):\nNow cancelling the norm of \"u\" from both ends gives us the desired inequality.\n\nProof. From Lemma 1 and 2 it follows that\n\nLet \"f\" : [\"a\", \"b\"] → R be a continuous function. Then there exists \"c\" in (\"a\", \"b\") such that\n\nSince the mean value of \"f\" on [\"a\", \"b\"] is defined as\n\nwe can interpret the conclusion as \"f\" achieves its mean value at some \"c\" in (\"a\", \"b\").\n\nIn general, if \"f\" : [\"a\", \"b\"] → R is continuous and \"g\" is an integrable function that does not change sign on [\"a\", \"b\"], then there exists \"c\" in (\"a\", \"b\") such that\n\nSuppose \"f\" : [\"a\", \"b\"] → R is continuous and \"g\" is a nonnegative integrable function on [\"a\", \"b\"]. By the extreme value theorem, there exists \"m\" and \"M\" such that for each \"x\" in [\"a\", \"b\"], formula_142 and formula_143. Since \"g\" is nonnegative,\n\nNow let\n\nIf formula_146, we're done since\n\nmeans\n\nso for any \"c\" in (\"a\", \"b\"),\n\nIf \"I\" ≠ 0, then\n\nBy the intermediate value theorem, \"f\" attains every value of the interval [\"m\", \"M\"], so for some \"c\" in [\"a\", \"b\"]\n\nthat is,\n\nFinally, if \"g\" is negative on [\"a\", \"b\"], then\n\nand we still get the same result as above.\n\nQED\n\nThere are various slightly different theorems called the second mean value theorem for definite integrals. A commonly found version is as follows:\n\nHere formula_155 stands for formula_156, the existence of which follows from the conditions. Note that it is essential that the interval (\"a\", \"b\"] contains \"b\". A variant not having this requirement is:\n\nIf the function formula_74 returns a multi-dimensional vector, then the MVT for integration is not true, even if the domain of formula_74 is also multi-dimensional.\n\nFor example, consider the following 2-dimensional function defined on an formula_160-dimensional cube:\n\nThen, by symmetry it is easy to see that the mean value of formula_74 over its domain is (0,0):\n\nHowever, there is no point in which formula_164, because formula_165 everywhere.\n\nLet \"X\" and \"Y\" be non-negative random variables such that E[\"X\"] < E[\"Y\"] < ∞ and formula_166 (i.e. \"X\" is smaller than \"Y\" in the usual stochastic order). Then there exists an absolutely continuous non-negative random variable \"Z\" having probability density function\n\nLet \"g\" be a measurable and differentiable function such that E[\"g\"(\"X\")], E[\"g\"(\"Y\")] < ∞, and let its derivative \"g′\" be measurable and Riemann-integrable on the interval [\"x\", \"y\"] for all \"y\" ≥ \"x\" ≥ 0. Then, E[\"g′\"(\"Z\")] is finite and\n\nAs noted above, the theorem does not hold for differentiable complex-valued functions. Instead, a generalization of the theorem is stated such:\n\nLet \"f\" : Ω → C be a holomorphic function on the open convex set Ω, and let \"a\" and \"b\" be distinct points in Ω. Then there exist points \"u\", \"v\" on \"L\" (the line segment from \"a\" to \"b\") such that\n\nWhere Re() is the Real part and Im() is the Imaginary part of a complex-valued function.\n\n\n"}
{"id": "19664", "url": "https://en.wikipedia.org/wiki?curid=19664", "title": "Mallow", "text": "Mallow\n\nMallow or mallows may refer to:\n\n\n\n\n\n\n\n"}
{"id": "19665", "url": "https://en.wikipedia.org/wiki?curid=19665", "title": "Marc Bloch", "text": "Marc Bloch\n\nMarc Léopold Benjamin Bloch (; ; 6 July 1886 – 16 June 1944) was a French historian. A founding member of the Annales School of French social history, he specialised in the field of medieval history and published widely on Medieval France over the course of his career. As an academic, he worked at the University of Strasbourg (1920 to 1936), the University of Paris (1936 to 1939), and the University of Montpellier (1941 to 1944).\n\nBorn in Lyon to an Alsatian Jewish family, Bloch was raised in Paris, where his father—the classical historian Gustave Bloch—worked at Sorbonne University. Bloch was educated at various Parisian lycees and the École Normale Supérieure, and from an early age was affected by the anti-semitism of the Dreyfus affair. During the First World War, he served in the French Army and fought at the First Battle of the Marne and the Somme. After the war, he was awarded his doctorate in 1918 and gained employment as a lecturer at the University of Strasbourg. There, he formed an intellectual partnership with modern historian Lucien Febvre. Together they founded the Annales School and began publishing the journal \"Annales d'histoire économique et sociale\" in 1929. Bloch was a modernist in his historiographical approach, and repeatedly emphasised the importance of a multidisciplinary engagement towards history, particularly blending his research with that on geography, sociology and economics, which was his subject when he was offered a post at the University of Paris in 1936.\n\nDuring the Second World War Bloch volunteered for service, becoming responsible for the French Army's fuel supplies during the Phoney War. Involved in the Battle of Dunkirk and spending a brief time in Britain, he unsuccessfully attempted to secure passage to the United States. Back in France, where his ability to work was curtailed by new anti-Semitic regulations, he applied for and received one of the few permits available allowing Jews to continue working in the French university system. He had to leave Paris, and complained that the Nazis looted his apartment and stole his books; he was also forced to relinquish his position on the editorial board of \"Annales\". Bloch worked in Montpellier until November 1942 when Germany invaded Vichy France. He then joined the French Resistance, acting predominantly as a courier and translator. In 1944, he was captured in Lyon and executed by firing squad. Several works—including influential studies like \"The Historian's Craft\" and \"Strange Defeat\"—were published posthumously.\n\nBoth as a result of his historical studies and his death as a member of the Resistance, Bloch was highly regarded by generations of post-war French historians and came to be called \"the greatest historian of all-time\". By the end of the 20th century, historians were making a more sober assessment of Bloch's abilities, influence, and legacy, arguing that there were flaws to his approach.\n\nMarc Bloch was born in Lyon on 6 July 1886, one of two children to Gustave and Sarah Bloch née Ebstein. Bloch's family were Alsatian Jews: secular, liberal and loyal to the French Republic. They \"struck a balance\", says the historian Carole Fink, between both \"fierce Jacobin patriotism and the antinationalism of the left\". His family had lived in Alsace for five generations under French rule. In 1871 France was forced to cede the region to Germany following its defeat in the Franco-Prussian War. The year after Bloch's birth, his father was appointed professor of Roman History at the Sorbonne, and the family moved to Paris—\"the glittering capital of the Third Republic\". Marc had a brother, Louis Constant Alexandre, seven years his senior. The two were close, although Bloch later described Louis as being occasionally somewhat intimidating. The Bloch family lived at 72, Rue d'Alésia, in the 14th arrondissement of Paris. Gustave began teaching Marc history while he was still a boy, with a secular, rather than Jewish education intended to prepare him for a career in professional French society. Bloch's later close collaborator, Lucien Febvre, visited the Bloch family at home in 1902.\n\nBloch's biographer Karen Stirling ascribed significance to the era in which Bloch was born: the middle of the French Third Republic, so \"after those who had founded it and before the generation that would aggressively challenge it\". When Bloch was nine-years old, the Dreyfus affair broke out in France. As the first major display of political antisemitism in Europe, it was probably a formative event of Bloch's youth, along with, more generally, the atmosphere of \"fin de siècle\" Paris. Bloch was 11 when Émile Zola published \"J'Accuse…!\", his indictment of the French establishment's antisemitism and corruption. Bloch was greatly affected by the Dreyfus affair, but even more affected was nineteenth-century France generally, and the ENS particularly, where existing divides in French society were reinforced in every debate. Gustave Bloch was closely involved in the Dreyfusard movement, and his son agreed with the cause.\n\nBloch was educated at the prestigious Lycée Louis-le-Grand for three years, where he was consistently head of his class and won prizes in French, history, Latin and natural history. He passed his \"baccalauréat\", in Letters and Philosophy, in July 1903, being graded \"trés bien\" (very good). The following year, he received a scholarship and undertook postgraduate study there for the École normale supérieure (ENS) (where his father had been appointed \"maitre de conferences\" in 1887). His father had been nicknamed \"le Méga\" by his students at the ENS and the moniker \"Microméga\" was bestowed upon Bloch. Here he was taught history by Christian Pfister and Charles Seignobos, who led a relatively new school of historical thought which saw history as broad themes punctuated by tumultuous events. Another important influence on Bloch from this period was his father's contemporary, the sociologist Émile Durkheim, who pre-empted Bloch's own later emphasis on cross-disciplinary research. The same year, Bloch visited England; he later recalled being struck more by the number of homeless people on the Victoria Embankment than the new Entente Cordiale relationship between the two countries.\n\nThe Dreyfus affair had soured Bloch's views of the French Army, and he considered it laden with \"snobbery, anti-semitism and anti-republicanism\". National service had been made compulsory for all French adult males in 1905, with an enlistment term of two years. Bloch joined the 46th Infantry Regiment based at Pithiviers from 1905 to 1906.\n\nBy this time, changes were taking place in French academia. In Bloch's own speciality of history, attempts were being made at instilling a more scientific methodology. In other, newer departments such a sociology, efforts were made at establishing an independent identity. Bloch graduated in 1908 with degrees in both geography and history (Davies notes, given Bloch's later divergent interests, the significance of the two qualifications). He highly respected the then-French specialty of historical geography as practiced by his teacher Vidal de la Blache whose \"Tableau de la géographie\" Bloch had studied at the ÉNS,and Lucien Gallois. Bloch applied unsuccessfully for a fellowship at the \"Fondation Thiers\". As a result, he travelled to Germany in 1909 where he studied demography under Karl Bücher in Leipzig and religion under Adolf Harnack in Berlin; he did not, however, particularly socialise with fellow students while in Germany. He returned to France the following year and again applied to the \"Fondation\", this time successfully. Bloch researched the medieval Île-de-France in preparation for his thesis. This research was Bloch's first focus on rural history. His parents had moved house and now resided at the Avenue d'Orleans, not far from Bloch's quarters.\n\nBloch's research at the Fondation—especially his research into the Capetian kings—laid the groundwork for his career. He began by creating maps of the Paris area illustrating where serfdom had thrived and where it had not. He also investigated the nature of serfdom, the culture of which, he discovered, was founded almost completely on custom and practice. His studies of this period formed Bloch into a mature scholar and first brought him into contact with other disciplines whose relevance he was to emphasise for most of his career. Serfdom as a topic was so broad that he touched on commerce, currency, popular religion, the nobility, as well as art, architecture and literature. His doctoral thesis—a study of 10th-century French serfdom—was titled \"Rois et Serfs, un Chapitre d'Histoire Capétienne\". Although it helped mould Bloch's ideas for the future, it did not, says Bryce Loyn, give any indication of the originality of thought that Bloch would later be known for, and was not vastly different to what others had written on the subject. Following his graduation, he taught at two lycées, first in Montpelier, a minor university town of 66,000 inhabitants. With Bloch working over 16 hours a week on his classes, there was little time for him to work on his thesis. He also taught at the University of Amiens. While there, he wrote a review of Febvre's first book, \"Histoire de Franche-Comté\". Bloch intended to turn his thesis into a book, but the First World War intervened.\n\nBoth Marc and Louis Bloch volunteered for service in the French Army. Bloch was one of over 800 ENS students who enlisted; 239 were to be killed in action. On 2 August 1914 he was assigned to the 272nd Reserve Regiment. Within eight days he was stationed on the Belgian border where he fought in the Battle of the Meuse later that month. His regiment took part in the general retreat on the 25th, and the following day they were in Barricourt, in the Argonne. The march westward continued towards the Marne—with a temporary recuperative halt in Termes, where Bloch took advantage of the stop to swim in the river—which they reached in early September. During the first battle of the Marne, Bloch's troop was responsible for the assault and capture of Florent, which Bloch viewed as a rustic delight, before advancing on La Gruerie. Bloch led his troop with shouts of \"Forward the 18th!\" They suffered heavy casualties: 89 men were either missing or known to be dead. During this period Bloch grew a beard; this, and the bravery he had shown under fire—at his section of the line, the French and German trenches were sometimes little more than twelve meters apart—earned him the moniker \"The hairy\". Bloch enjoyed the early days of the war; like most of his generation, he had expected a short but glorious conflict. Gustave Bloch remained in France, wishing to be close to his sons at the front.\n\nExcept for two months in hospital followed by another three recuperating, he spent the war in the infantry; he joined as a sergeant and rose to become the head of his section. Bloch kept a war diary from his enlistment. Very detailed in the first few months, it rapidly became more general in its observations. However, says Daniel Hochedez, Bloch was aware of his role as both a \"witness and narrator\" to events and wanted as detailed a basis for his historiographical understanding as possible. Rees Davies notes that although Bloch served in the war with \"considerable distinction\", it had come at the worst possible time both for his intellectual development and his study of medieval society.\n\nFor the first time in his life, Bloch later wrote, he worked and lived alongside people he had never had close contact with before, such as shop workers and labourers, with whom he developed a great camaraderie. It was a completely different world to the one he was used to, being \"a world where differences were settled not by words but by bullets\". His experiences made him rethink his views on history, and, indeed, influenced his subsequent approach to the world in general. He was particularly moved by the collective psychology he witnessed in the trenches. He later declared he knew of no better men than \"the men of the Nord and the Pas de Calais\" with whom he had spent four years in close quarters. His few references to the French Generals were not only sparse but on the rare occasions that he mentioned them, sardonic.\n\nApart from the Marne, Bloch fought at the battles of the Marne, the Somme and Argonne, and the final German assault on Paris. He survived the war, which he later described as having been an \"honour\" to have served through. He had, however, lost many friends and colleagues. Among the closest of them, all killed in action, were: Maxime David (died 1914), Antoine-Jules Bianconi (died 1915) and Ernest Babut (died 1916). Bloch himself was wounded twice and decorated for courage, receiving the Croix de Guerre and the Légion d'Honneur. He had joined as a non-commissioned officer, received an officer's commission after the Marne, and had been promoted to warrant officer and finally a captain in the fuel service, (\"Service des essences)\" before the war ended. He was clearly, says Loyn, both a good and a brave soldier; he later wrote, \"I know only one way to persuade a troop to brave danger: brave it yourself\".\n\nWhile on front-line service, Bloch contracted severe arthritis which required him to retire regularly to the thermal baths of Aix-les-Bains for treatment. He later remembered very little of the historical events he found himself in, writing only that his memories wereBloch later described the war, in a detached style, as having been a \"gigantic social experience, of unbelievable richness\". For example, he had a habit of noting the different coloured smoke that different shells made — percussion bombs had black smoke, timed bombs were brown. He also remembered both the \"friends killed at our side ... of the intoxication which had taken hold of us when we saw the enemy in flight\". He also considered it to have been \"four years of fighting idleness\". Following the Armistice in November 1918, Bloch was demobilized on 13 March 1919.\n\nThe war was fundamental in re-arranging Bloch's approach to history, although he never acknowledged it as a turning point. In the years following the war, a disillusioned Bloch rejected the ideas and the traditions that had formed his scholarly training. He rejected the political and biographical history which up until that point was the norm, along with what George Huppert has described as a \"laborious cult of facts\" that accompanied it. In 1920, with the opening of the University of Strasbourg, Bloch was appointed \"chargé de cours\" of medieval history. Alsace had been returned to France with the Treaty of Versailles and was a contentious political issue in Strasbourg, its capital. Bloch, however, refused to take either side in the debate; indeed, he appears to have avoided politics entirely. Under Wilhelmine Germany, Strasbourg had rivalled Berlin as a centre for intellectual advancement, and the University of Strasbourg possessed the largest academic library in the world. Thus, says Epstein, \"Bloch's unrivalled knowledge of the European Middle Ages was ... built on and around the French University of Strasbourg's inherited German treasures\". Bloch also taught French to the few German students who were still at the Centre d'Études Germaniques at the University of Mainz during the Occupation of the Rhineland. He ignored the 1923 occupation of the Ruhr.\n\nBloch began working energetically, and later said that the most productive years of his life were spent at Strasbourg. In his teaching, his delivery was halting. His approach sometimes appeared cold and distant—caustic enough to be upsetting—but conversely, he could be also both charismatic and forceful. Durkheim died in 1917, but the movement he began against the \"smugness\" that pervaded French intellectual thinking continued. Bloch had been greatly influenced by him, as Durkheim also considered the connections between historians and sociologists to be greater than their differences. Not only did he openly acknowledge Durkheim's influence, but Bloch \"repeatedly seized any opportunity to reiterate\" it, according to R. C. Rhodes.\n\nAt Strasbourg he again met Febvre, who was now a leading historian of the 16th century. Modern and medieval seminars were adjacent to each other at Strasbourg, and attendance often overlapped. Their meeting has been called a \"germinal event for 20th-century historiography\", and they were to work closely together for the rest of Bloch's life. Febvre was some years older than Bloch and was probably a great influence on him. They lived in the same area of Strasbourg and became kindred spirits, often going on walking trips across the Vosges and other excursions.\n\nHis fundamental views on the nature and purpose of the study of history were established by 1920. That same year he defended, and subsequently published, his thesis. It was not as extensive a work as had been intended due to the war. There was a provision in French further education for doctoral candidates for whom the war had interrupted their research to submit only a small portion of the full-length thesis usually required. It sufficed, however, to demonstrate his credentials as a medievalist in the eyes of his contemporaries. He began publishing articles in Henri Berr's \"Revue de Synthèse Historique\". Bloch also published his first major work, \"Les Rois Thaumaturges\", which he later described as \"\"ce gros enfant\"\" (this big child). In 1928, Bloch was invited to lecture at the Institute for the Comparative Study of Civilizations in Oslo. Here he first expounded publicly his theories on total, comparative history:His Oslo lecture, called \"Towards a Comparative History of Europe\", formed the basis of his next book, \"Les Caractères Originaux de l'Histoire Rurale Française\". In the same year he founded the historical journal \"Annales\" with Febvre. One of its aims was to counteract the administrative school of history, which Davies says had \"committed the arch error of emptying history of human element\". As Bloch saw it, it was his duty to correct that tendency. Both Bloch and Febvre were keen to refocus French historical scholarship on social rather than political history and to promote the use of sociological techniques. The journal avoided narrative history almost completely. The inaugural issue of the \"Annales\" stated the editors' basic aims: to counteract the arbitrary and artificial division of history into periods, to re-unite history and social science as a single body of thought, and to promote the acceptance of all other schools of thought into historiography. As a result, the \"Annales\" often contained commentary on contemporary, rather than exclusively historical, events. Editing the journal led to Bloch forming close professional relationships with scholars in different fields across Europe. The \"Annales\" was the only academic journal to have a deliberate perspective. Neither Bloch nor Febvre wanted to present a neutral facade. During the decade it published it maintained a staunchly left-wing position. Henri Pirenne, a Belgian historian who wrote comparative history, closely supported the new journal. Before the war he had acted in an unofficial capacity as a conduit between French and German schools of historiography. Braudel later described the journal's management as being a chief executive officer—Bloch—with a minister of foreign affairs—Lefebvre.\n\nThe comparative method allowed Bloch to discover instances of uniqueness within aspects of society, and he advocated it as a new kind of history. According to Bryce Lyon, Braudel and Bloch, \"promising to perform all the burdensome tasks\" themselves, asked Pirenne to become editor-in-chief of \"Annales\" to no avail. He did remain a strong supporter in the background, however, and had an article published in the first volume in 1929. He became close friends with both Bloch and Febvre. He was particularly influential on Bloch, who later said that Pirenne's approach should be the model for historians and that \"at the time his country was fighting beside mine for justice and civilisation, wrote in captivity a history of Europe\". The three men kept up a regular correspondence until Pirenne's death in 1935. In 1923 Bloch attended the inaugural meeting of the International Congress on Historical Studies (ICHS) in Brussels, which was opened by Pirenne. Bloch was a prolific reviewer for \"Annales\", and during the 1920s and 1930s he contributed over 700 reviews. These were both criticisms of specific works, but more generally, represented his own fluid thinking during this period. The reviews demonstrate the extent to which he shifted his thinking on particular subjects.\n\nIn 1930, both keen to make a move to Paris, Febvre and Bloch applied to the \"École pratique des hautes études\" for a position: both failed. Three years later Febvre was elected to the Collège de France. He moved to Paris, and in doing so says Fink, became all the more aloof. This placed a strain on Bloch's and his relations, although they communicated regularly by letter and much of their correspondence has been preserved. In 1934, Bloch was invited to speak at the London School of Economics. There he met Eileen Power, R. H. Tawney and Michael Postan, among others. While in London, he was asked to write a section of the \"Cambridge Economic History of Europe\"; at the same time, he also attempted to foster interest in the \"Annales\" among British historians. He later told Febvre in some ways he felt he had a closer affinity with academic life in England than that of France. For example, in comparing the \"Bibliothèque Nationale\" with the British Museum, he said that\n\nDuring this period he supported the Popular Front politically. Although he did not believe it would do any good, he signed Alain's—Émile Chartier's pseudonym—petition against Paul Boncour's militarization laws in 1935. And while he was opposed to the growth of European fascism, he also objected to \"demagogic appeals to the masses\" to fight it, as the Communist Party was doing. Braudel and Bloch were both firmly on the left, although with different emphases. Febvre, for example, was more militantly Marxist than Bloch, while the latter criticised both the pacifist left and corporate trade unionism. \n\nIn 1934 Étienne Gilson sponsored Bloch's candidacy for a chair at the Collège de France. The College, says Eugen Weber, was Bloch's \"dream\" appointment, although one never to be realised, as it was one of the few (possibly the only) institutions in France where personal research was central to lecturing. Camille Jullian had died the previous year, and his position was now available. While he had lived, Julian had wished for his chair to go to one of his students, Albert Grenier, and after his death his colleague's generally agreed with him. However, Gilson proposed that not only should Bloch be appointed, but that the position be redesignated the study of comparative history. Bloch, says Weber, enjoyed and welcomed new schools of thought and ideas, but mistakenly believed the College should do so also. The College did not. The contest between Bloch and Grenier was not just the struggle for one post between two historians, but the path that historiography within the College would take for the next generation. To complicate the situation further, the country was in both political and economic crises, and the College had had its budget slashed by 10%. No matter who filled it, this made another new chair financially unviable. By the end of the year, and with further retirements, the College had lost four professors: it could replace only one, and Bloch was not appointed. Bloch personally suspected his failure was due to anti-Semitism and Jewish quotas. At the time, Febvre blamed it on a distrust of Bloch's approach to scholarship by the academic establishment, although this cannot have been an over-riding fear as Bloch's next appointment indicated.Henri Hauser retired from the Sorbonne in 1936, and his chair in economic history was up for appointment. Bloch—\"distancing himself from the encroaching threat of Nazi Germany\"—applied and was approved for his position. This was a more demanding position than the one he had applied for at the College. Eugen Weber has suggested Bloch was appointed because unlike at the College, he had not come into conflict with many faculty members. Weber researched the archives of the College in 1991 and discovered that Bloch had indicated an interest in working there as early as 1928, even though that would have meant him being appointed to the chair in numismatics rather than history. In a letter to the recruitment board written the same year, Bloch indicated that although he was not officially applying, he felt that \"this kind of work (which he claimed to be alone in doing) deserves to have its place one day in our great foundation of free scientific research\". H. Stuart Hughes says of Bloch's Sorbonne appointment: \"In another country, it might have occasioned surprise that a medievalist like Bloch should have been named to such a chair with so little previous preparation. In France it was only to be expected: no one else was better qualified\". His first lecture was on the theme of never-ending history, a process, a never to be finished thing. Davies says his years at the Sorbonne were to be \"the most fruitful\" of Bloch's career, and he was by now the most significant French historian of his age. In 1936 Friedman says he considered using Marx in his teachings, with the intention of bringing \"some fresh air\" into the Sorbonne.\n\nThe same year, Bloch and his family visited Venice, where they were chaperoned by Gino Luzzatto. During this period they were living in the Sèvres – Babylone area of Paris, next to the Hôtel Lutetia.\n\nBy now, \"Annales\" was being published six times a year to keep on top of current affairs, however, its \"outlook was gloomy\". In 1938 the publishers withdrew support and, experiencing financial hardship, the journal moved to cheaper offices, raised its prices and returned to publishing quarterly. Bloch was increasingly opposed to Febvre over the direction he wished to take the journal. Febvre wanted it to be a \"journal of ideas\", whereas Bloch saw it as a vehicle for the exchange of information to different areas of scholarship.\n\nBy early 1939, war was known to be imminent, and Bloch, in spite of his age, which automatically exempted him, had a reserve commission for the army with a captain's rank. He had already been mobilized twice in false alarms. In August 1939 he and Simonne intended to travel to the ICHS in Bucharest. In autumn 1939, just before the outbreak of war, Bloch published the first volume of \"Feudal Society\".\n\nOn 24 August 1939, at the age of 53—making him the eldest reserve officer in the French army —Bloch was mobilized for the third time as a fuel supply officer. He was responsible for the mobilisation of the French Army's massive motorized units. This involved him undertaking such a detailed assessment of the French fuel supply that he later wrote he was able to \"count petrol tins and ration every drop\" of fuel he obtained. During the first few months of the war, called the Phoney War, he was stationed in Alsace. He possessed none of the eager patriotism with which he had approached the first war. Instead, Carole Fink says as a result of the discrimination he believed that he had recently faced, he had \"begun to distance himself intellectually and emotionally from his comrades and leaders\". Back in Strasbourg, his main duty was the evacuation of civilians to behind the Maginot Line. Further transfers occurred, and Bloch was re-stationed to Molsheim, Saverne, and eventually to 1st Army headquarters in Picardy, where he joined the Intelligence Department, in liaison with the British. Much of the period 1939–May 1940 saw Bloch frankly bored in his post, as he often had little to do. To pass the time and occupy himself, he decided to begin writing a history of France. To this end purchased notebooks and began to work out a structure for the work. Although never completed, the pages he managed to write, \"in his cold, poorly lit rooms\", eventually became the kernel of \"The Historian's Craft\". At one point he expected to be invited to neutral Belgium to deliver a series of lectures in Liège. These never took place, however, disappointing Bloch very much; he had planned to speak on Belgian neutrality. He also turned down the opportunity to travel to Oslo as an attaché to the French Military Mission there. He was considered an excellent candidate for the position due to his fluency in Norwegian and knowledge of the country. Bloch considered it and came close to accepting; ultimately, though, it was too far from his family, whom he rarely saw enough of in any case. Some academics had escaped France for The New School in New York City, and the School also invited Bloch. He refused, although this may well have been on account of the difficulties he had in obtaining the necessary visas: the US government would not grant visas to every member of his family.\n\nIn May 1940 the German army outflanked the French and forced them to withdraw further. Facing capture in Rennes, Bloch disguised himself in civilian clothes and lived under German occupation for a fortnight before returning to his family at their country home in Fougères. He fought at the Battle of Dunkirk in May–June 1940 and was evacuated to Britain with the remaining British Expeditionary Force on the steamer \"Royal-Daffodil\", which he later described as taking place \"under golden skies coloured by the black and fawn smoke\". Before the evacuation, Bloch ordered the immediate burning of what remained of the French military's fuel supplies. Although he could have remained in Britain with the Free French, he chose to return to France the day he arrived and re-enlist because his family was still there. He felt, though, that the army, of which he was once again a part, lacked the \"esprit de corps\", a \" fervent fraternity\", of the army of the first war. He saw the French Generals of 1940 as blindly following in the same way as Joseph Joffre had in the first war. He did not, however, believe that the earlier war was an indication of how the next would progress: \"no two successive wars\", he wrote in 1940, \"are ever the same war\".\n\nBloch saw France's collapse as the overrunning of the best qualities mankind possessed—character and intelligence, while pinpointing the cause of that occurrence as France's own sluggish and intractable\" attitude since World War I. He was horrified by the defeat which, Carole Fink has suggested, he saw as being worse, for both France and the world, than her previous defeats at Waterloo and Sedan. Bloch understood the reasons for France's sudden defeat: not in the rumours of British betrayal, communist fifth columns or fascist plots, but in her failure to motorise, and perhaps more importantly, to fail to understand what motorisation meant. He understood that it was the latter that allowed the French army to become bogged down in Belgium, and this had been compounded by the French army's slow retreat. He wrote in \"Strange Defeat\" that a fast, motorised retreat might have saved the army.\n\nTwo-thirds of France was occupied by Germany. Bloch, one of the only elderly academics to volunteer, was demobilised soon after Philippe Pétain's government signed the Armistice of 22 June 1940 forming Vichy France in the remaining southern-third of the country. Bloch moved south, where in January 1941, he applied for and received one of only ten exemptions to the ban on employing Jewish academics the Vichy government ever made. This was probably due to Bloch's pre-eminence in the field which he was known for. He was allowed to work at the \"University of Strasbourg-in-exile\", the Universities of Clermont-Ferrand, and Montpellier. The latter, being farther south was beneficial to his wife's health, which was by then in decline. The dean of faculty at Montpellier was Augustin Fliche, an ecclesiastical historian of the Middle Ages, who, according to Weber, \"made no secret of his antisemitism\". He disliked Bloch further for having once given him a poor review. Fliche not only opposed Bloch's transfer to Montpellier but made his life uncomfortable when he was there. The Vichy government was attempting to promote itself as a return to traditional French values. Bloch condemned this as propaganda; the rural idyll that Vichy said it would return France to was impossible, he said, \"because the idyllic, docile peasant life of the French right had never existed\".Bloch's professional relationship with Febvre was also under strain. The Nazis wanted French editorial boards to be stripped of Jews in accordance with German racial policies; Bloch advocated disobedience, while Lefebvre was passionate about the survival of \"Annales\" at any cost. He believed that it was worth making concessions to keep the journal afloat and more importantly, not allow France's intellectual life to end. Bloch, however, rejected out of hand any suggestion that he should, in his words, \"fall into line\". Febvre also asked Bloch to resign as joint-editor of the journal. Febvre feared that Bloch's involvement, as a Jew in Nazi-occupied France, would hinder the journal's distribution. Bloch, forced to accede, turned the \"Annales\" over to the sole editorship of Febvre, who then changed the journal's name to \"Mélanges d'Histoire Sociale\". Bloch was forced to write for it under the pseudonym Marc Fougéres. The journal's bank account was also in Bloch's name; this too had to go. Henri Hauser supported Febvre's position, and Bloch was offended when Febvre intimated that Hauser had more to lose than both of them. This was because, whereas Bloch had been allowed to retain his research position, Hauser had not. Bloch interpreted Febvre's comment as implying that Bloch was not a victim. Bloch, alluding to his ethnicity, replied that the difference between them was that, whereas he feared for his children because of their Jewishness, Febvre's children were in no more danger than any other man in the country. André Burguière suggests Febvre did not understand fundamentally the position Bloch, or any French Jew, was in. Already damaged by this disagreement, Bloch's and Febvre's relationship declined further when Bloch had been forced to leave his library and papers in his Paris apartment following his move to Vichy. He had attempted to have them transported to his Creuse residence, but the Nazis—who had made their headquarters in the hotel next to Bloch's apartment—looted his rooms and confiscated his library in 1942. Bloch held Febvre responsible for the loss, believing he could have done far more to have prevented it.\n\nBloch's mother had recently died, and his wife was ill; furthermore, although he was permitted to work and live, he faced daily harassment. On 18 March 1941, Bloch made his will in Clermont-Ferrand. Bronisław Geremek suggests in this document Bloch in some way foresaw his death, stating that nobody had the right to avoid fighting for one's country. In March 1942 Bloch and other French academics such as Georges Friedmann and Émile Benveniste, refused to join or condone the establishment of the Union Générele des Israelites des France by the Vichy government, a group intended to include all Jews in France, both of birth and immigration.\n\nIn November 1942, the German Army crossed the demarcation line and removed the Vichy government.\n\nThis was the catalyst for Bloch's decision to join the French Resistance sometime between late 1942 and March 1943. Bloch was careful not to join simply because of his ethnicity or the laws that were passed against it. As Burguière has pointed out, and Bloch would have known, taking such a position would effectively \"indict all Jews who did not join\". Burguière has pinpointed Bloch's motive for joining the Resistance in his characteristic refusal to mince his words or play half a role. He had already written that, in his view, \"there can be no salvation where there is not some sacrifice\". He sent his family away, and returned to Lyon to join the underground. Although he knew some of the \"francs-tireurs\" around Lyon, Bloch still found it difficult to join them because of his age. Although the Resistance recruited heavily among University lecturers—and indeed, Bloch's alma mater, the Ecole Normale Superieur, provided it with many members—he commented in exasperation to Simonne that he \"didn't know it is so difficult to offer one's life\". François Dosse quotes a member of the \"franc-tireurs\" active with Bloch as later describing how \"that eminent professor came to put himself at our command simply and modestly\". Bloch utilised his professional and military skills on their behalf, writing propaganda for them and organising their supplies and materiel, becoming a regional organiser. Bloch also joined the \"Mouvements Unis de la Résistance\" (Unified Resistance Movement, or MUR), section R1, and edited the underground newsletter, \"Cahiers Politique\". He went under various pseudonyms: Arpajon, Chevreuse, Narbonne. Often on the move, Bloch used archival research as his excuse for travelling. Georges Altman later told how Bloch, as Resistance fighter, had been \"a man, made for the creative silence of gentle study, with a cabinet full of books\" but was now \"running from street to street, deciphering secret letters in some Lyonaisse Resistance garret\"; all Bloch's notes were kept in code.\n\nFor the first time, suggests Lyon, Bloch was forced to consider the role of the individual in history, rather than the collective; perhaps by then even realising he should have done so earlier.\n\nBloch was arrested at the \"Place de Pont\", Lyon, during a major roundup by the Vichy \"milice\" on 8 March 1944, and handed over to Klaus Barbie of the Lyon Gestapo. Bloch was using the pseudonym \"Maurice Blanchard\", and in appearance was \"an ageing gentleman, rather short, grey-haired, bespectacled, neatly dressed, holding a briefcase in one hand and a cane in the other\". He was renting a room above a dressmakers on the rue des Quatre Chapeau; the Gestapo raided the place the following day. It is possible Bloch had been denounced by a woman working in the shop. In any case, they found a radio transmitter and many papers. Bloch was imprisoned in Montluc prison, during which his wife died. He was tortured with, for example, ice-cold baths which knocked him out. His ribs and a wrist were broken and led to his being returned to his cell unconscious. He eventually caught bronchopneumonia.and fell seriously ill. It was later claimed that he gave away no information to his interrogators, and while incarcerated taught French history to other inmates.\n\nIn the meantime, the allies had invaded in Normandy on 6 June 1944. As a result, the Nazi regime was keen to evacuate following the invasion and wanted to \"liquidate their holdings\" in France; this meant disposing of as many prisoners as they could. Between May and June 1944 around 700 prisoners were shot in scattered locations to avoid the risk of this becoming common knowledge and inviting Resistance reprisals around southern France. Among those killed was Bloch, one of a group of 26 Resistance prisoners picked out in Montluc and driven along the Saône towards Trévoux on the night of 16 June 1944. Driven to a field near Saint-Didier-de-Formans, they were shot by the Gestapo in groups of four. According to Lyon, Bloch spent his last moments comforting a 16-year-old beside him who was worried that the bullets might hurt. Bloch fell first, reputedly shouting \"\"Vive la France\"\" before being shot. A coup de grâce was delivered. One man managed to crawl away and later provided a detailed report of events; the bodies were discovered on 26 June. For some time Bloch's death was merely a \"dark rumour\" until it was confirmed to Febvre.\n\nAt his burial, his own words were read at the graveside. With them Bloch proudly acknowledged his Jewish ancestry while denying religion in favour of his being foremost a Frenchman. He described himself as being \"a stranger to any formal religious belief as well as any supposed racial solidarity, I have felt myself to be, quite simply French before anything else\". According to his instructions, no orthodox prayers were said over his grave, and on it was to be carved his epitaph \"dilexi veritatem\" (\"I have loved the truth\"). In 1977, his ashes were transferred from St-Didier to Fougeres and the gravestone was inscribed as he requested.\n\nFebvre had not approved of Bloch's decision to join the Resistance believing it to be a waste of his brain and talents, although, as Davies points out, \"such a fate befell many other French intellectuals\". Febvre continued publishing \"Annales\", (\"if in a considerably modified form\" comments Beatrice Gottlieb), dividing his time between his country château in the Franche-Comté and working at the École Normale in Paris. This caused some outrage, and, after liberation, when classes were returning to a degree of normality, he was booed by his students at the Sorbonne.\n\nBloch's first book was \"L'Ile de France\", published in 1913. Although a small book. Lyon notes it was \"light, readable and far from trivial,\" showing the strong influence of H. J. Fleure in its discussion of soil, language and archaeological remains. It was translated into English in 1971. Davies says 1920's \"Rois et Serfs\", (\"Kings and Serfs\"), is a \"long and rather meandering essay\", although it had the potential to be Bloch's definitive monograph upon the single topic that \"might have evoked his genius at his fullest\", the transition from antiquity to the Middle Ages. Loyn also describes it as a \"loose-knit monograph\", and a program to move forward rather than a full-length academic text. Bloch's most important work to date (translated as \"The Royal Touch: Monarchy and Miracles in France and England\" in 1973) and based on his doctoral dissertation, was published in 1924 as \"Rois et Thaumaturges\". Here he examines medieval belief in the royal touch, and the degree to which kings used such a belief for propaganda purposes. It was also the first example of Bloch's inter-disciplinary approach, as he utilised research from the fields of anthropology, medicine, psychology and iconography. It has been described as Bloch's first masterwork. It has a 500-page descriptive analysis of the medieval view of royalty effectively possessing supernatural powers. Verging on the antiquarian in his microscopic approach, and much influenced by the work of Raymond Crawfurd—who saw it as a \"dubious if exotic\" aspect of medicine, rather than history—Bloch makes diverse use of evidence from different disciplines and even different periods, assessing the King's Evil as far forward as the 19th century. The book had originally been inspired by discussions Bloch had with Louis, who acted as a medical consultant while his brother worked on it. Bloch concluded that the royal touch involved a degree of mass delusion among those who witnessed it.\n\n1931 saw the publication of \"Les caractéres originaux de l'histoire rurale francaise\". This, what Bloch called \"mon petit livre\", takes a more general historical approach and was also considered his best work to date. He used both the traditional techniques of historiographical analysis—scrutinising documents, manuscripts, accounts and rolls—and his newer, multi-faceted approach, with a heavy emphasis on maps as evidence. Bloch did not allow the new approach to detract from the former: he knew, says Chirot, that the traditional methods of historical research were \"the bread and butter of historical work. One had to do it well to be a minimally accepted historian\". The first of \"two classic works\", says Hughes,and possibly his finest, studies the relationship between physical geographical location and the development of political institutions. Loyn has called Bloch's assessment of medieval French rural law great, but with the addendum that \"he is not so good at describing ordinary human beings. He is no Eileen Power, and his peasants do not come to life as hers do\". In this study, Chirot says Bloch \"entirely abandoned the concept of linear history, and wrote, instead, from the present or near past into the distant past, and back towards the present\". Febvre wrote the introduction to the book for its publication, and described the technique as \"reading the past from the present\", or what Bloch saw as starting with the known and moving into the unknown.\n\n\"La Société Féodale\" was published in two volumes (\"The Growth of Ties of Dependence\", and \"Social Classes and Political Organisation\") in 1939, and was translated into English as \"Feudal Society\" in 1961. It was described by Bloch as something of a sketch, although a modern biographer has called it his \"most enduring work ... still a cornerstone of medieval curricula\" in 2007 and representative of Bloch at the peak of his career. In \"Feudal Society\" he used research from the broadest range of disciplines to date to examine feudalism in the broadest possible way—most notably including a study of feudal Japan. He also compared areas where feudalism was imposed, rather than organically developed (such as England after the Norman conquest) and where it was never established (such as Scotland and Scandinavia). Bloch defined feudal society as, \"from the peasants' point of view\", politically fragmentary, where they are ruled by an aristocratic upper-class.\n\nThese three works—\"The Royal Touch\", \"French Rural History\" and \"Feudal Society\", which concentrate on the French Middle Ages have been described by Daniel Chirot as Bloch's most significant. The last two works Bloch wrote, \"The Historian's Craft\" and \"Strange Defeat\", have been described as unrepresentative of his historical approach in that they discuss contemporary events in which Bloch was both personally involved and without access to sources to work from. The latter was uncompleted at the time of his death, and both were published posthumously in 1949. Davies has described \"The Historian's Craft as\" \"beautifully sensitive and profound\"; the book was written in response to his son, Étienne, asking his father, \"what is history?\". In his introduction, Bloch wrote to Lebvre.\n\nLikewise, \"Strange Defeat\", in the words of R. R. Davies, is a \"damning and even intolerant analysis\" of the long- and short-term reasons France fell in 1940. Bloch affirmed that the book was more than a personal memoir; rather, he intended it as a deposition and a testament. It contains—\"uncomfortably and honestly\"—Bloch's own self-appraisal:\n\nBloch emphasises failures in the French mindset: in the loss of morale of the soldiery and a failed education of the officers, effectively a failure of both character and intelligence on behalf of both. He condemns the \"mania\" for testing in education which, he felt, treated the testing as being an end in itself, draining generations of Frenchmen and Frenchwomen of originality and initiative or thirst for knowledge, and an \"appreciation only of successful cheating and sheer luck\". \"Strange Defeat\" has been called Bloch's autopsy of the France of the inter-war years.\n\nA collection of essays was published in English in 1961 as \"Land and Work in Medieval Europe\". The long essay was a favoured medium of Bloch's, including, Davies says, \"the famous essay on the water mill and the much-challenged one on the problem of gold in medieval Europe\". In the former, Bloch saw one of the most important technological advances of the era, in the latter, the effective creation of a European currency. Although one of his best essays, according to Davies—\"Liberté et servitude personelles au Moyen Age, particulement en France\"—was not published when it could have been; this, he remarked was \"an unpardonable omission\".\n\nDavies says Bloch was \"no mean disputant\" in historiographical debate, often reducing an opponent's argument to its most basic weaknesses. His approach was a reaction against the prevailing ideas within French historiography of the day which, when he was young, were still very much based on that of the German School, pioneered by Leopold von Ranke. Within French historiography this led to a forensic focus on administrative history as expounded by historians such as Ernest Lavisse. While he acknowledged his and his generation of historians' debt to their predecessors, he considered that they treated historical research as being little more meaningful than detective work. Bloch later wrote how, in his view, \"There is no waste more criminal than that of erudition running ... in neutral gear, nor any pride more vainly misplaced than that in a tool valued as an end in itself\". He believed it was wrong for historians to focus on the evidence rather than the human condition of whatever period they were discussing. Administrative historians, he said, understood every element of a government department without understanding anything of those who worked in it. Bloch was very much influenced by Ferdinand Lot, who had already written comparative history, and by the work of Jules Michelet and Fustel de Coulanges with their emphasis on social history, Durkheim's sociological methodology, François Simiand's social economics, and Henri Bergson's philosophy of collectivism. Bloch's emphasis on using comparative history harked back to the Enlightenment, when writers such as Voltaire and Montesquieu decried the notion that history was a linear narrative of individuals and pushed for a greater use of philosophy in studying the past. Bloch condemned the \"German-dominated\" school of political economy, which he considered \"analytically unsophisticated and riddled with distortions\". Equally condemned were then-fashionable ideas on racial theories of national identity. Bloch believed that political history on its own could not explain deeper socioeconomics trends and influences.\n\nBloch did not see social history as being a separate field within historical research. Rather, he saw all aspects of history to be inherently a part of social history. By definition, all history was social history, an approach he and Febvre termed \"\"histoire totale\"\", not a focus on points of fact such as dates of battles, reigns, and changes of leaders and ministries, and a general confinement by the historian to what he can identify and verify. Boch, however, believed, as he wrote to Pirenne, that the historian's most important quality was the ability to be surprised by what he found—\"I am more and more convinced of this\", he said; \"damn those of us who believe everything is normal!\"\n\nBloch identified two types of historical era: the generational era and the era of civilisation: these were defined by the speed with which they underwent change and development. In the latter type of period, which changed gradually, Bloch included physical, structural and psychological aspects of society, while the generational era could experience fundamental change over a relatively few generations. Bloch founded what modern French historians call the \"regressive method\" of historical scholarship. This method avoids the necessity of relying solely on historical documents as a source, by looking at the issues visible in later historical periods and drawing from them what they may have looked like centuries earlier. Davies says this was particularly useful in Bloch's study of village communities as \"the strength of communal traditions often preserves earlier customs in a more or less fossilized state\". Bloch studied peasant tools in museums, and in action, and discussed their use with the people themselves. He believed that in observing a plough or an annual harvest one was observing history, as more often than not both the technology and the technique were much the same as they had been hundreds of years earlier. However, the individuals themselves were not his focus, which was on \"the collectivity, the community, the society\". He wrote about the peasantry, rather than the individual peasant; says Lyon, \"he roamed the provinces to become familiar with French agriculture over the long term, with the contours of peasant villages, with agrarian routine, its sounds and smells. Bloch claimed that both fighting alongside them in the war and his historical research into their history had shown him \"the vigorous and unwearied quickness\" of the peasant mind.\n\nBloch self-described his area of study as the comparative history of European society and explained why he did not distinguish himself as a medievalist: \"I refuse to do so. I have no interest in changing labels, nor in clever labels themselves, or those that are thought to be so.\" He did not leave a full study of his methodology, although it can be effectively reconstructed piecemeal. He believed that history was the \"science of movement\", but did not accept, for example, the aphorism that one could protect against the future by studying the past. His did not use a revolutionary approach to historiography; rather, he wished to combine the schools of thinking that preceded him into a new broad approach to history and, as he wrote in 1926, to bring to history \"ce murmure qui n'était pas de la mort\", (\"the whisper that was not death'). He criticised what he called the \"idol of the origins\", where historians concentrate overly hard on the formation of something to the detriment of studying the thing itself.\n\nBloch's comparative history led him to tie his researches in with those of many other schools: social sciences, linguistics, philology, comparative literature, folklore, geography and agronomy. Similarly, he did not restrict himself to French history. At various points in his writings Bloch commented on medieval Corsican, Finnish, Japanese, Norwegian and Welsh history. R. R. Davies has compared Bloch's intelligence with what he calls that of \"the Maitland of the 1890s\", regarding his breadth of reading, use of language and multidisciplinary approach. Unlike Maitland, however, Bloch also wished to synthesise scientific history with narrative history. Stirling says, he managed to achieve \"an imperfect and volatile imbalance\" between them. Bloch did not believe that it was possible to understand or recreate the past by the mere act of compiling facts from sources; rather, he saw sources as witnesses, \"and like most witnesses\", he wrote, \"it rarely speaks until one begins to question it\". Likewise, he viewed historians as detectives who gathered evidence and testimony, as \"juges d'instruction\" (examining magistrates) \"charged with a vast enquiry of the past\".\n\nBloch was not only interested in periods or aspects of history but in the importance of history as a subject, regardless of the period, of intellectual exercise. Davies writes, \"he was certainly not afraid of repeating himself; and, unlike most English historians, he felt it his duty to reflect on the aims and purposes of history\". Bloch considered it a mistake for the historian to confine himself overly rigidly to his own discipline. Much of his editorializing in \"Annales\" emphasised the important of parallel evidence to be found in neighbouring fields of study, especially archaeology, ethnography, geography, literature, psychology, sociology, technology, air photography, ecology, pollen analysis and statistics. In Bloch's view, this provided not just for a broader field of study, but a far more comprehensive understanding of the past than would be possible from relying solely on historical sources. Bloch's favourite metaphor for how technology impacts society was the watermill. This can be summed up as illustrating how it was known but little used in the classical period; it became an economic necessity in the early medieval period; and finally, in the later Middle Ages it represented a scarce resource increasingly concentrated in the nobility's hands.\n\nBloch also emphasised the importance of geography in the study of history, and particularly in the study of rural history. He suggested that, fundamentally, they were the same subjects, although he criticised geographers for failing to take historical chronology or human agency into account. A farmer's field, he used as example, was \"fundamentally, a human work, built from generation to generation\". Bloch also condemned the view that rural life was immobile. He believed that the Gallic farmer of the Roman period was inherently different to his 18th-century descendants, cultivating different plants, in a different way. He saw England and France's agricultural history as developing similarly, and, indeed, discovered an Enclosure Movement in France throughout the 15th, 16th and 17th centuries on the basis that it had been occurring in England in similar circumstances. Bloch also took a deep interest in the field of linguistics and their use of the comparative method. He believed that using the method in historical research could prevent the historian from ignoring the broader context in the course of his detailed local researches: \"a simple application of the comparative method exploded the ethnic theories of historical institutions, beloved of so many German historians\". \n\nMarc Bloch was not a tall man, being in height. An elegant dresser, although with \"impossible\" handwriting, he was described as having expressive blue eyes, which could be \"mischievous, inquisitive, ironic and sharp\". Febvre later said that when he first met Bloch in 1902, he found a slender young man with \"a timid face\". Bloch was proud of his family's history of defending France: he later wrote, \"My great-grandfather was a serving soldier in 1793; ... my father was one of the defenders of Strasbourg in 1870 ... I was brought up in the traditions of patriotism which found no more fervent champions than the Jews of the Alsatian exodus\". Bloch was a committed supporter of the Third Republic and politically left-wing. He was not a Marxist, although he was impressed by Marx himself, whom he thought was a great historian if possibly \"an unbearable man\" personally. He viewed contemporary politics as purely moral decisions to be made. He did not, however, let it enter into his work; indeed, he questioned the very idea of a historian studying politics. He believed that society should be governed by the young, and, although politically he was a moderate, he noted that revolutions generally promote the young over the old: \"even the Nazis had done this, while the French had done the reverse, bringing to power a generation of the past\". He was also capable of a \"curious lack of empathy and comprehension for the horrors of modern warfare\". \n\nAlthough Bloch was very reserved —and later acknowledged that he had generally been old-fashioned and \"timid\" with women—he was good friends with Lucien Febvre and Christian Pfister. In July 1919 he married Simonne Vidal, a \"cultivated and discreet, timid and energetic\" woman, at a Jewish wedding. Her father was the \"Inspecteur-Général de Ponts et Chaussées\", and a very prosperous and influential man. Undoubtedly, says Friedman, his wife's family wealth allowed Bloch to focus on his research without having to depend on the income he made from it. Bloch was later to say he had found great happiness with her, and that he believed her to have also found it with him. They had six children together, four sons and two daughters. The eldest two were a daughter Alice and a son, Étienne. As his father had done with him, Bloch took a great interest in his children's education, and regularly helped with their homework. He could, though, be \"caustically critical\" of his children, particularly Étienne. Bloch accused him in one of his wartime letters of having poor manners, being lazy and stubborn, and of being possessed occasionally by \"evil demons\". Regarding the facts of life, Bloch told Etienne to attempt always to avoid what Bloch termed \"contaminated females\".\n\nBloch was certainly agnostic, if not atheist, in matters of religion. His son Étienne later said of his father, \"in his life as well as his writings not even the slightest trace of a supposed Jewish identity\" can be found. \"Marc Bloch was simply French\". Some of his pupils believed him to be an Orthodox Jew, but Loyn says this is incorrect; while his Jewish roots were important to him, this was the result of the political tumult of the Dreyfuss years: that \"it was only anti-semitism that made him want to affirm his Jewishness\". On the other hand, John Lewis Gaddis has found Bloch's failure to condemn Stalinism in the 1930s \"disturbing\", saying that Bloch had ample evidence of Stalin's crimes and yet sought to shroud them in utilitarian calculations about the price of what he called 'progress'\".\n\nBloch's brother Louis became a doctor, and eventually the head of the Diphtheria section of the Hôpital des Enfants-Malades. Louis died prematurely in 1922. Their father died in March the following year. Following these deaths, Bloch took on responsibility for his ageing mother as well as his brother's widow and children. Eugen Weber has suggested that Bloch was probably a monomaniac who, in his own words, \"abhorred falsehood\". He also abhorred, as a result of both the Franco-Prussian war and more recently World War One, German nationalism. This extended to that country's culture and scholarship, and is probably the reason he never debated with German historians. Indeed, in Bloch's later career, he rarely mentioned even those German historians with whom he must, professionally, have felt an affinity, such as Karl Lamprecht. Lyon says Lamprecht had denounced what he saw as the German obsession with political history and had focused on art and comparative history, thus \"infuriat[ing] the \"Rankianer\"\". Bloch once commented, on English historians, that \"en Angleterre, rien qu'en Angleterre\" (\"in England, only England\"); he was not, though, particularly critical of English historiography, and indeed, respected the long tradition of rural history in that country as well as more materially the government funding that went into historical research there.\n\nIt is possible, had Bloch survived the war, that he would have stood to be appointed Minister of Education in a post-war government and reformed the education system he had condemned for losing France the war in 1940. Instead, in 1948, his son Étienne offered the Archives Nationales his father's papers for repository, but they rejected the offer. As a result, the material was placed in the vaults of the École Normale Supérieure, \"where it lay untouched for decades\".\n\nIntellectual historian Peter Burke named Bloch the leader of what he called the \"French Historical Revolution\", and Bloch became an icon for the post-war generation of new historians. Although he has been described as being to some extent a cult in both England and France—\"one of the most influential historians of the twentieth century\" by Stirling, and \"the greatest historian of modern times\" by John H. Plumb—this is a reputation mostly acquired postmortem. Henry Loyn suggests it is also one which would have amused and amazed Bloch. According to Karen Stirling, this posed a particular problem within French historiography when Bloch effectively had martyrdom bestowed upon him after the war, leading to much of his work being overshadowed by the last months of his life. This led to \"indiscriminate heaps of praise under which he is now almost hopelessly buried\". This is partly at least the fault of historians themselves, who have not critically re-examined Bloch's work but rather treat him as a fixed and immutable aspect of the historiographical background. At the turn of the millennium \"there is a woeful lack of critical engagement with Marc Bloch's writing in contemporary academic circles\" according to Stirling. His legacy has been further complicated by the fact that the second generation of Annalists led by Fernand Braudel has \"co-opted his memory\", combining Bloch's academic work and Resistance involvement to create \"a founding myth\". The aspects of his life which made Bloch easy to beatify have been summed up by Henry Loyn as \"Frenchman and Jew, scholar and soldier, staff officer and Resistance worker ... articulate on the present as well as the past\".\n\nThe first critical biography of Bloch did not appear until Carole Fink's \"Marc Bloch: A Life in History\" was published in 1989. This, wrote S. R. Epstein, was the \"professional, extensively researched and documented\" story of Bloch's life, and, he commented, probably had to \"overcome a strong sense of protectiveness among the guardians of Bloch's and the \"Annales\" memory\". Since then, continuing scholarship—such as that by Karen Stirling, who calls Bloch a visionary, although a \"flawed\" one—has been more critically objective of Bloch's recognisable weaknesses. For example, although he was a keen advocate for chronological precision and textual accuracy, his only major work in this area, a discussion of Osbert of Clare's Life of Edward the Confessor, was subsequently \"seriously criticised\" by later experts in the field such as R. W. Southern and Frank Barlow; Epstein later suggested Bloch was \"a mediocre theoretician but an adept artisan of method\". Colleagues who worked with him occasionally complained that Bloch's manner could be \"cold, distant, and both timid and hypocritical\" due to the strong views he had held on the failure of the French education system. Bloch's reduction of the role of individuals, and their personal beliefs, in changing society or making history has been challenged. Even Febvre, reviewing \"Feudal Society\" on its post-war publication, suggested that Bloch had unnecessarily ignored the individual's role in societal development. Bloch has also been accused of ignoring unanswered questions and presenting complete answers when they are perhaps not deserved, and of sometimes ignoring internal inconsistencies. Wallace-Hadrill has also criticised Bloch's division of the feudal period into two distinct times as artificial. He also says Bloch's theory on the transformation of blood-ties into feudal bonds do not match either the chronological evidence or what is known of the nature of the early family unit. Bloch seems to have occasionally ignored, whether accidentally or deliberately, important contemporaries in his field. Richard Lefebvre des Noëttes, for example, who founded the history of technology as a new discipline, built new harnesses from medieval illustrations, and drew histographical conclusions. Bloch, though, does not seem to have acknowledged the similarities between his and Lefebvre's approaches to physical research, even though he cited much earlier historians. Davies argued that there was a sociological aspect to Bloch's work which often neutralised the precision of his historical writing; as a result, he says, those of Bloch's works with a sociological conception, such as \"Feudal Society\", have not always \"stood the test of time\".\n\nComparative history, too, still proved controversial many years after Bloch's death, and Bryce Lyon has posited that, had Bloch survived the war, it is very likely that his views on history—already changing in the early years of the second war, just as they had done in the aftermath of the first—would have re-adjusted themselves against the very school he had founded. Stirling suggests what distinguished Bloch from his predecessors was that he effectively became a new kind of historian, who \"strove primarily for transparency of methodology where his predecessors had striven for transparency of data\" while continuously critiquing himself at the same time. Davies suggests his legacy lies not so much in the body of work he left behind him, which is not always as definitive as it has been made out to be, but the influence he had on \"a whole generation of French historical scholarship\". Bloch's emphasis on how rural and village society has been neglected by historians in favour of the lords and manorial courts that ruled them influenced later historians such as R. H. Hilton in the study of the economics of peasant society. Bloch's combination of economics, history and sociology was \"forty years before it became fashionable\", argues Daniel Chirot, which he says could make Bloch a founding father of post-war sociology scholarship.\n\nThe English-language journal \"Past & Present\", from Oxford, was a direct successor to the \"Annales\", suggests Loyn. Michel Foucault said of the Annales School, \"what Bloch, Lefebvre and Braudel have shown for history, we can show, I believe, for the history of ideas\". Bloch's influence spread beyond historiography after his death. In the 2007 French presidential election, Bloch was quoted many times. For example, candidates Nicolas Sarkozy and Marine Le Pen both cited Bloch's lines from \"Strange Defeat\": \"there are two categories of Frenchmen who will never really grasp the significance of French history: those who refuse to be thrilled by the Consecration of our Kings at Reims, and those who can read unmoved the account of the Festival of Federation\". In 1977, Bloch received a state reburial; streets schools and universities have been named after him, and the centennial of Bloch's birth was celebrated at a conference held in Paris in June 1986. It was attended academics of various disciplines, particularly historians and anthropologists.\n\n"}
{"id": "19667", "url": "https://en.wikipedia.org/wiki?curid=19667", "title": "Michael Ventris", "text": "Michael Ventris\n\nMichael George Francis Ventris, OBE (; 12 July 1922 – 6 September 1956) was an English architect, classicist and philologist who deciphered Linear B, the ancient Mycenaean Greek script. A student of languages, Ventris had pursued the decipherment as a personal vocation since his adolescence. After creating a new field of study, Ventris died in an automobile accident a few weeks before the publication, with John Chadwick, of \"Documents in Mycenaean Greek\".\n\nVentris was born into a traditional army family. His grandfather, Francis Ventris, was a major-general and Commander of British Forces in China. His father, Edward Francis Vereker Ventris, was a lieutenant-colonel in the Indian Army, who retired early due to ill health. Edward Ventris married Anna Dorothea Janasz (Dora), who was from a wealthy Jewish Polish paternal background. Michael Ventris was their only child.\n\nThe family moved to Switzerland for eight years, seeking a healthy environment for Colonel Ventris. Young Michael started school in Gstaad, where classes were taught in French and German. He soon was fluent in both languages and showing proficiency for Swiss German. He was capable of learning a language within a matter of weeks, which allowed him to acquire fluency in a dozen languages. His mother often spoke Polish to him, and he was fluent by the age of eight. At this time, he was reading Adolf Erman's \"Die Hieroglyphen\" in German.\nIn 1931, the Ventris family returned home. From 1931 to 1935 Ventris was sent to Bickley Hill School in Stowe. His parents divorced in 1935. At this time, he secured a scholarship to Stowe School. At Stowe he learned some Latin and Ancient Greek. He did not do outstanding work there - by then he was spending most of his spare time learning as much as he could about Linear B, some of his study time being spent under the covers at night with a flashlight. When he was not boarding at school, Ventris lived with his mother, before 1935 in coastal hotels, and then in the avant garde Berthold Lubetkin's Highpoint modernist apartments in Highgate. His mother's acquaintances, who frequented the house, included many sculptors, painters, and writers of the day. The money for her sophisticated lifestyle came from Polish estates.\n\nVentris's father died in 1938 and his mother Dora became administrator of the estate. With the German invasion of Poland in 1939, Mrs Ventris lost her private income, and in 1940 Dora's father died. Ventris lost his mother to clinical depression and an overdose of barbiturates. He never spoke of her, assuming instead an ebullient and energetic manner in whatever he decided to do, a trait which won him numerous friends. A friend of the family, Russian sculptor Naum Gabo, took Ventris under his wing. Ventris later said that Gabo was the most family he had ever had. It may have been at Gabo's house that he began the study of Russian. He decided on architecture as a career, and enrolled in the Architectural Association School of Architecture. There he met his wife Lois Knox-Niven. Her social background was similar to what Ventris's had been: her family was well-to-do, she had travelled in Europe, and she was interested in architecture, in addition to which she was popular and was considered very beautiful.\nVentris did not complete his architecture studies, being conscripted in 1942. He chose the Royal Air Force (RAF). His preference was for navigator rather than pilot, and he completed the extensive training in the UK and Canada, to qualify early in 1944 and be commissioned. While training, he studied Russian intensively for several weeks, the purpose of which is not clear. He took part in the bombing of Germany, as aircrew on the Handley Page Halifax with No. 76 Squadron RAF, initially at RAF Breighton and then at RAF Holme-on-Spalding Moor. After the conclusion of the war he served out the rest of his term on the ground in Germany, for which he was chosen because of his knowledge of Russian. His duties are unclear. His friends assumed he was on intelligence duties, interpreting his denials as part of a legal gag. No evidence of such assignments has emerged in the decades since. There is also no evidence that he was ever part of any code-breaking unit, as was Chadwick, even though the public has readily believed this explanation of his genius and success with Linear B.\n\nAfter the war he worked briefly in Sweden, learning enough Swedish to communicate with scholars in it. Then he came home to complete his architectural education with honours in 1948 and settled down with Lois working as an architect. He designed schools for the Ministry of Education. He and his wife personally designed their family home, 19 North End, Hampstead. He had two children, a son, Nikki (1942–1984) and a daughter, Tessa (born 1946). Ventris continued with his efforts on Linear B, discovering in 1952 that it was an archaic form of Greek. He was awarded an OBE in 1955 for \"services to Mycenaean paleography.\" In 1959 he was posthumously awarded the British Academy's Kenyon Medal.\n\nIn 1956 Ventris, who lived in Hampstead, died instantly in a late-night collision with a parked truck while driving home, aged 34. The coroner's verdict was accidental death.\n\nAn English Heritage blue plaque commemorates Ventris at his home in Hampstead.\n\nAt the beginning of the 20th century, archaeologist Arthur Evans began excavating an ancient site at Knōssos, on the island of Crete. In doing so he uncovered a great many clay tablets inscribed with two unknown scripts, Linear A and Linear B. Evans attempted to decipher both in the following decades, with little success.\n\nIn 1936, Evans hosted an exhibition on Cretan archaeology at Burlington House in London, home of the Royal Academy. It was the jubilee anniversary (50 years) of the British School of Archaeology in Athens, contemporaneous owners and managers of the Knossos site. Evans had given the site to them some years previously. Villa Ariadne, Evans's home there, was now part of the school. Boys from Stowe school were in attendance at one lecture and tour conducted by Evans himself at age 85. Ventris, 14 years old, was present and remembered Evans walking with a stick. The stick was undoubtedly the cane named Prodger which Evans carried all his life to assist him with his short-sightedness and night blindness. Evans held up tablets of the unknown scripts for the audience to see. During the interview period following the lecture, Ventris immediately confirmed that Linear B was as yet undeciphered, and determined to decipher it. \n\nIn 1940, the 18-year-old Ventris had an article \"Introducing the Minoan Language\" published in the American Journal of Archaeology. Ventris's initial theory was that Etruscan and Linear B were related and that this might provide a key to decipherment. Although this proved incorrect, it was a link he continued to explore until the early 1950s.\n\nShortly after Evans died, Alice Kober noted that certain words in Linear B inscriptions had changing word endings — perhaps declensions in the manner of Latin or Greek. Using this clue, Ventris constructed a series of grids associating the symbols on the tablets with consonants and vowels. While \"which\" consonants and vowels these were remained mysterious, Ventris learned enough about the structure of the underlying language to begin guessing. Alice Kober was a classics professor at Brooklyn College and had done extensive work on Linear B. Ventris acknowledged her work as having made a significant contribution to his own work. \n\nShortly before World War II, American archaeologist Carl Blegen discovered a further 600 or so tablets of Linear B in the Mycenaean palace of Pylos. Photographs of these tablets by archaeologist Alison Frantz facilitated Ventris's later decipherment of the Linear B script.\n\nComparing the Linear B tablets discovered on the Greek mainland, and noting that certain symbol groups appeared only in the Cretan texts, Ventris made the inspired guess that those were place names on the island. This proved to be correct. Armed with the symbols he could decipher from this, Ventris soon unlocked much text and determined that the underlying language of Linear B was in fact Greek. This overturned Evans's theories of Minoan history by establishing that Cretan civilization, at least in the later periods associated with the Linear B tablets, had been part of Mycenean Greece.\n\n\n\n\n"}
{"id": "19668", "url": "https://en.wikipedia.org/wiki?curid=19668", "title": "Maniac Mansion", "text": "Maniac Mansion\n\nManiac Mansion is a 1987 graphic adventure video game developed and published by Lucasfilm Games. It follows teenage protagonist Dave Miller as he attempts to rescue his girlfriend from a mad scientist, whose mind has been enslaved by a sentient meteor. The player uses a point-and-click interface to guide Dave and two of his six playable friends through the scientist's mansion while solving puzzles and avoiding dangers. Gameplay is non-linear, and the game must be completed in different ways based on the player's choice of characters. Initially released for the Commodore 64 and Apple II, \"Maniac Mansion\" was Lucasfilm Games' first self-published product.\n\nThe game was conceived in 1985 by Ron Gilbert and Gary Winnick, who sought to tell a comedic story based on horror film and B-movie clichés. They mapped out the project as a paper-and-pencil game before coding commenced. While earlier adventure titles had relied on command lines, Gilbert disliked such systems, and he developed \"Maniac Mansion\"s simpler point-and-click interface as a replacement. To speed up production, he created a game engine called SCUMM, which was used in many later LucasArts titles. After its release, \"Maniac Mansion\" was ported to several platforms. A port for the Nintendo Entertainment System had to be reworked heavily, in response to complaints by Nintendo of America that the game was inappropriate for children.\n\n\"Maniac Mansion\" was critically acclaimed: reviewers lauded its graphics, cutscenes, animation, and humor. Writer Orson Scott Card praised it as a step toward \"computer games [becoming] a valid storytelling art\". It influenced numerous graphic adventure titles, and its point-and-click interface became a standard feature in the genre. The game's success solidified Lucasfilm as a serious rival to adventure game studios such as Sierra On-Line. In 1990, \"Maniac Mansion\" was adapted into a three-season television series of the same name, written by Eugene Levy and starring Joe Flaherty. A sequel to the game, entitled \"Day of the Tentacle\", was released in 1993.\n\n\"Maniac Mansion\" is a graphic adventure game in which the player uses a point-and-click interface to guide characters through a two-dimensional game world and to solve puzzles. Fifteen action commands, such as \"Walk To\" and \"Unlock\", may be selected by the player from a menu on the screen's lower half. The player starts the game by choosing two out of six characters to accompany protagonist Dave Miller. Each character possesses unique abilities: for example, Syd and Razor can play musical instruments, while Bernard can repair appliances. The game may be completed with any combination of characters; but, since many puzzles are solvable only by certain characters, different paths must be taken based on the group's composition. \"Maniac Mansion\" features cutscenes, a word coined by Ron Gilbert, that interrupt gameplay to advance the story and inform the player about offscreen events.\n\nThe game takes place in the mansion of the fictional Edison family: Dr. Fred, a mad scientist; Nurse Edna, his wife; and their son Weird Ed. Living with the Edisons are two large, disembodied tentacles, one purple and the other green. The intro sequence shows that a sentient meteor crashed near the mansion twenty years earlier; it brainwashed the Edisons and directed Dr. Fred to obtain human brains for use in experiments. The game begins as Dave Miller prepares to enter the mansion to rescue his girlfriend, Sandy Pantz, who had been kidnapped by Dr. Fred. With the exception of the green tentacle, the mansion's inhabitants are hostile, and will throw the player characters into the dungeon—or, in some situations, kill them—if they see them. When a character dies, the player must choose a replacement from the unselected characters; and the game ends if all characters are killed. \"Maniac Mansion\" has five possible endings, based on which characters are chosen, which survive, and what the characters accomplish.\n\n\"Maniac Mansion\" was conceived in 1985 when Lucasfilm Games employees Ron Gilbert and Gary Winnick were assigned to create an original game. Gilbert had been hired the previous year as a programmer for the game \"Koronis Rift\". He befriended Winnick over their similar tastes in humor, film, and television. Company management provided little oversight in the creation of \"Maniac Mansion\", a trend to which Gilbert credited the success of several of his games for Lucasfilm.\n\nGilbert and Winnick co-wrote and co-designed the project, but they worked separately as well: Gilbert on programming and Winnick on visuals. As both of them enjoyed B horror films, they decided to make a comedy-horror game set in a haunted house. They drew inspiration from a film whose name Winnick could not recall. He described it as \"a ridiculous teen horror movie\", in which teenagers inside a building were killed one by one without any thought of leaving. This film, combined with clichés from popular horror movies such as \"Friday the 13th\" and \"A Nightmare on Elm Street\", became the basis for the game's setting. Early work on the game progressed organically: according to Gilbert, \"Very little was written down. Gary and I just talked and laughed a lot, and out it came.\" Lucasfilm Games relocated to the Stable House at Skywalker Ranch during \"Maniac Mansion\"s conception period, and the ranch's Main House was used as a model for the mansion. Several rooms from the Main House received exact reproductions in the game, such as a library with a spiral staircase and a media room with a large-screen TV and grand piano.\n\nStory and characters were a primary concern for Gilbert and Winnick. The pair based the game's cast on friends, family members, acquaintances, and stereotypes. For example, Winnick's girlfriend Ray was the inspiration for Razor, while Dave and Wendy were based, respectively, on Gilbert and a fellow Lucasfilm employee named Wendy. According to Winnick, the Edison family was shaped after characters from EC Comics and Warren Publishing magazines. The sentient meteor that brainwashes Dr. Fred was inspired by a segment from the 1982 anthology film \"Creepshow\". A man-eating plant, similar to that of \"Little Shop of Horrors\", was included as well. The developers sought to strike a balance between tension and humor with the game's story.\n\nInitially, Gilbert and Winnick struggled to choose a gameplay genre for \"Maniac Mansion\". While visiting relatives over Christmas, Gilbert saw his cousin play \"King's Quest: Quest for the Crown\", an adventure game by Sierra On-Line. Although he was a fan of text adventures, this was Gilbert's first experience with a graphic adventure, and he used the holiday to play the game and familiarize himself with the format. As a result, he decided to develop his and Winnick's ideas into a graphic adventure game.\n\n\"Maniac Mansion\"s story and structure were designed before coding commenced. The project's earliest incarnation was a paper-and-pencil board game, in which the mansion's floor plan was used as a game board, and cards represented events and characters. Lines connected the rooms to illustrate pathways by which characters could travel. Strips of cellulose acetate were used to map out the game's puzzles by tracking which items worked together when used by certain characters. Impressed by the map's complexity, Winnick included it in the final game as a poster hung on a wall. Because each character contributes different skills and resources, the pair spent months working on the event combinations that could occur. This extended the game's production time beyond that of previous Lucasfilm Games projects, which almost led to Gilbert's firing. The game's dialogue, written by David Fox, was not created until after programming had begun.\n\nGilbert started programming \"Maniac Mansion\" in 6502 assembly language, but he quickly decided that the project was too large and complex for this method. He decided that a new game engine would have to be created. Its coding language was initially planned to be Lisp-inspired, but Gilbert opted for one similar to C. Lucasfilm employee Chip Morningstar contributed the base code for the engine, which Gilbert then built on. Gilbert hoped to create a \"system that could be used on many adventure games, cutting down the time it took to make them\". \"Maniac Mansion\"s first six-to-nine months of production were dedicated largely to engine development. The game was developed around the Commodore 64 home computer, an 8-bit system with only 64 KB of memory. The team wanted to include scrolling screens, but as it was normally impossible to scroll bitmap graphics on the Commodore 64, they had to use lower-detail tile graphics. Winnick gave each character a large head made of three stacked sprites to make them recognizable.\n\nAlthough Gilbert wrote much of the foundational code for \"Maniac Mansion\", the majority of the game's events were programmed by Lucasfilm employee David Fox. Fox was between projects and planned to work on the game only for a month, but he remained with the team for six months. With Gilbert, he wrote the characters' dialog and choreographed the action. Winnick's concept art inspired him to add new elements to the game: for example, Fox allowed the player to place a hamster inside the kitchen's microwave.\n\nThe team wanted to avoid punishing the player for applying everyday logic in \"Maniac Mansion\". Fox noted that one Sierra game features a scene in which the player, without prior warning, may encounter a game over screen simply by picking up a shard of glass. He characterized such game design as \"sadistic\", and he commented, \"I know that in the real world I can successfully pick up a broken piece of mirror without dying\". Because of the project's nonlinear puzzle design, the team struggled to prevent no-win scenarios, in which the player unexpectedly became unable to complete the game. As a result of this problem, Gilbert later explained, \"We were constantly fighting against the desire just to rip out all the endings and just go with three characters, or even sometimes just one character\". Lucasfilm Games had only one playtester, and many dead-ends went undetected as a result. Further playtesting was provided by Gilbert's uncle, to whom Gilbert mailed a floppy disk of the game's latest version each week.\n\nThe \"Maniac Mansion\" team wanted to retain the structure of a text-based adventure game, but without the standard command-line interface. Gilbert and Winnick were frustrated by the genre's text parsers and frequent game over screens. While in college, Gilbert had enjoyed \"Colossal Cave Adventure\" and the games of Infocom, but he disliked their lack of visuals. He found the inclusion of graphics in Sierra On-Line games, such as \"King's Quest\", to be a step in the right direction, but these games still require the player to type, and to guess which commands must be input. In response, Gilbert programmed a point-and-click graphical user interface that displays every possible command. Fox had made a similar attempt to streamline Lucasfilm's earlier \"\" and he conceived the entirety of \"Maniac Mansion\"s interface, according to Gilbert. Forty input commands were planned at first, but the number was gradually reduced to 12. Gilbert finished the \"Maniac Mansion\" engine—which he later named \"Script Creation Utility for Maniac Mansion\" (SCUMM)—after roughly one year of work. Although the game was designed for the Commodore 64, the SCUMM engine allowed it to be ported easily to other platforms.\n\nAfter 18 to 24 months of development, \"Maniac Mansion\" debuted at the 1987 Consumer Electronics Show in Chicago. The game was released for the Commodore 64 and Apple II in October 1987. While previous Lucasfilm Games products had been published by outside companies, \"Maniac Mansion\" was self-published. This became a trend at Lucasfilm. The company hired Ken Macklin, an acquaintance of Winnick's, to design the game's packaging artwork. Gilbert and Winnick collaborated with the marketing department to design the back cover. The two also created an insert that includes hints, a backstory, and jokes. An MS-DOS port was released in early 1988, developed in part by Lucasfilm employees Aric Wilmunder and Brad Taylor. Ports for the Amiga, Atari ST and Nintendo Entertainment System (NES) followed, with the Amiga and Atari ST ports in 1989 and the NES port in 1990. The 16-bit versions of Maniac Mansion featured a copy protection system requiring the user to enter graphical symbols out of a code book included with the game. This was not present in the Commodore 64 and Apple versions due to lack of disk space, so those instead used an on-disk copy protection.\n\nThere were two separate versions of the game developed for the NES. The first port was handled and published by Jaleco only in Japan. Released on June 23, 1988, it featured characters redrawn in a cute art style and generally shrunken rooms. No scrolling is present, leading to rooms larger than a single screen to be displayed via flip-screens. Many of the background details are missing, and instead of a save feature a password, over 100 characters long, is required to save progress.\nIn September 1990 Jaleco released an American version of \"Maniac Mansion\" as the first NES title developed by Lucasfilm Games in cooperation with Realtime Associates. Generally, this port is regarded as being far closer to the original game than the Japanese effort.\n\nCompany management was occupied with other projects, and so the port received little attention until employee Douglas Crockford volunteered to direct it. The team used a modified version of the SCUMM engine called \"NES SCUMM\" for the port. According to Crockford, \"[One] of the main differences between the NES and PCs is that the NES can do certain things much faster\". The graphics had to be entirely redrawn to match the NES's display resolution. Tim Schafer, who later designed \"Maniac Mansion\"s sequel \"Day of the Tentacle\", received his first professional credit as a playtester for the NES version of \"Maniac Mansion\".\n\nDuring \"Maniac Mansion\"s development for the Commodore 64, Lucasfilm had censored profanity in the script: for instance, the early line of dialogue \"Don't be a shit head\" became \"Don't be a tuna head\". Additional content was removed from the NES version to make it suitable for a younger audience, and to conform with Nintendo's policies. Jaleco USA president Howie Rubin warned Crockford about content to which Nintendo might object, such as the word \"kill\". After reading the NES Game Standards Policy for himself, Crockford suspected that further elements of \"Maniac Mansion\" could be problematic, and he sent a list of questionable content to Jaleco. When the company replied that the content was reasonable, Lucasfilm Games submitted \"Maniac Mansion\" for approval.\n\nOne month later, Nintendo of America contacted Lucasfilm Games to request the removal of offensive text and nude graphics. Crockford censored this content but attempted to leave the game's essence intact. For example, Nintendo wanted graffiti in one room—which provided an important hint to players—removed from the game. Unable to comply without simultaneously removing the hint, the team simply shortened it. Sexually suggestive and otherwise \"graphic\" dialogue was edited, including a remark from Dr. Fred about \"pretty brains [being] sucked out\". The nudity described by Nintendo encompassed a swimsuit calendar, a classical sculpture and a poster of a mummy in a Playmate pose. After a brief fight to keep the sculpture, the team ultimately removed all three. The phrase \"NES SCUMM\" in the credits sequence was censored as well.\n\nLucasfilm Games re-submitted the edited version of \"Maniac Mansion\" to Nintendo, which then manufactured 250,000 cartridges. Each cartridge was fitted with a battery-powered back-up to save data. Nintendo announced the port through its official magazine in early 1990, and it provided further coverage later that year. The ability to microwave a hamster remained in the game, which Crockford cited as an example of the censors' contradictory criteria. Nintendo later noticed it, and after the first batch of cartridges was sold, Jaleco was forced to remove the content from future shipments.\n\nLate in development, Jaleco commissioned Realtime Associates to provide background music, which no previous version of \"Maniac Mansion\" had featured. Realtime Associates' founder and president David Warhol noted that \"video games at that time had to have 'wall to wall' music\". He brought in George \"The Fat Man\" Sanger and his band, along with David Hayes, to compose the score. Their goal was to create songs that suited each character, such as a punk rock theme for Razor, an electronic rock theme for Bernard and a version of Thin Lizzy's \"The Boys Are Back in Town\" for Dave Miller. Warhol translated their work into NES chiptune music.\n\nKeith Farrell of \"Compute!'s Gazette\" was struck by \"Maniac Mansion\"s similarity to film, particularly in its use of cutscenes to impart \"information or urgency\". He lauded the game's graphics, animation and high level of detail. \"Commodore User\"s Bill Scolding and three reviewers from \"Zzap!64\" compared the game to \"The Rocky Horror Picture Show\". Further comparisons were drawn to \"Psycho\", \"Friday the 13th\", \"The Texas Chain Saw Massacre\", \"The Addams Family\" and \"Scooby-Doo\". Russ Ceccola of \"Commodore Magazine\" found the cutscenes to be creative and well made, and he commented that the \"characters are distinctively Lucasfilm's, bringing facial expressions and personality to each individual character\". In \"Compute!\", Orson Scott Card praised the game's humor, cinematic storytelling and lack of violence. He called it \"compellingly good\" and evidence of Lucasfilm's push \"to make computer games a valid storytelling art\".\n\nGerman magazine \"Happy-Computer\" commended the point-and-click interface and likened it to that of \"Uninvited\" by ICOM Simulations. The publication highlighted \"Maniac Mansion\"s graphics, originality, and overall enjoyability: one of the writers called it the best adventure title yet released. \"Happy-Computer\" later reported that \"Maniac Mansion\" was the highest-selling video game in West Germany for three consecutive months. The game's humor received praise from \"Zzap!64\", whose reviewers called the point-and-click controls \"tremendous\" and the total package \"innovative and polished\". Shay Addams of \"Questbusters: The Adventurer's Newsletter\" preferred \"Maniac Mansion\"s interface to that of \"Labyrinth: The Computer Game\". He considered the game to be Lucasfilm's best, and he recommended it to Commodore 64 and Apple II users unable to run titles with better visuals, such as those from Sierra On-Line. A writer for \"ACE\" enjoyed the game's animation and depth, but he noted that fans of text-based adventures would dislike the game's simplicity. \"Entertainment Weekly\" picked the game as the #20 greatest game available in 1991, saying: \"The graphics are merely okay and the music is Nintendo at its tinniest, but Maniac Mansion's plot is enough to overcome these faults. In this command-driven game — adapted from the computer hit — three buddies venture into a sinister haunted mansion and wind up juggling a bunch of wacky story lines.\"\n\nReviewing the MS-DOS and Atari ST ports, a critic from \"The Games Machine\" called \"Maniac Mansion\" \"an enjoyable romp\" that was structurally superior to later LucasArts adventure games. The writer noticed poor pathfinding and disliked the limited audio. Reviewers for \"The Deseret News\" lauded the audiovisuals and considered the product \"wonderful fun\". \"Computer Gaming World\"s Charles Ardai praised the game for attaining \"the necessary and precarious balance between laughs and suspense that so many comic horror films and novels lack\". Although he faulted the control system's limited options, he hailed it as \"one of the most comfortable ever devised\". Writing for \"VideoGames & Computer Entertainment\", Bill Kunkel and Joyce Worley stated that the game's plot and premise were typical of the horror genre; but they praised the interface and execution.\n\nReviewing \"Maniac Mansion\"s Amiga version four years after its release, Simon Byron of \"The One Amiga\" praised the game for retaining \"charm and humour\", but suggested that its art direction had become \"tacky\" compared to more recent titles. Stephen Bradly of \"Amiga Format\" found the game derivative, but he encountered \"loads of visual humour\" in it; and he added, \"Strangely, it's quite compelling after a while.\" Michael Labiner of Germany's \"Amiga Joker\" considered \"Maniac Mansion\" to be one of the best adventure games for the system. He noted minor graphical flaws, such as a limited color palette, but he argued that the gameplay made up for such shortcomings. Writing for \"Datormagazin\" in Sweden, Ingela Palmér commented that the Amiga and Commodore 64 versions of \"Maniac Mansion\" were nearly identical. She criticized the graphics and gameplay of both releases but felt the game to be highly enjoyable regardless.\n\nReviewing the NES release, British magazine \"Mean Machines\" commended the game's presentation, playability, and replay value. The publication also noted undetailed graphics and \"ear-bashing tunes\". The magazine's Julian Rignall compared \"Maniac Mansion\" to the title \"Shadowgate\", but he preferred the former's controls and lack of \"death-without-warning situations\". Writers for Germany's \"Video Games\" referred to the NES version as a \"classic\". Co-reviewer Heinrich Lenhardt stated that \"Maniac Mansion\" was unlike any other NES adventure game, and that it was no less enjoyable than its home computer releases. Co-reviewer Winnie Forster found it to be \"one of the most original representatives of the [adventure game] genre\". In retrospective features, \"Edge\" magazine called the NES version \"somewhat neutered\" and \"GamesTM\" referred to it as \"infamous\" and \"heavily censored\".\n\nLucasfilm conceived the idea for a television adaptation of \"Maniac Mansion\", the rights to which were purchased by The Family Channel in 1990. The two companies collaborated with Atlantis Films to produce a sitcom named after the game, which debuted in September of that year. It aired on YTV in Canada and The Family Channel in the United States. Based in part on the video game, the series focuses on the Edison family's life and stars Joe Flaherty as Dr. Fred. Its writing staff was led by Eugene Levy. Gilbert later said that the premise of the series changed during production until it differed heavily from the game's original plot. Upon its debut, the adaptation received positive reviews from \"Variety\", \"Entertainment Weekly\" and the \"Los Angeles Times\". \"Time\" named it one of the year's best new series. Ken Tucker of \"Entertainment Weekly\" questioned the decision to air the series on The Family Channel, given Flaherty's subversive humor. Discussing the series in retrospect, Richard Cobbett of \"PC Gamer\" criticized its generic storylines and lack of relevance to the game. The series lasted for three seasons; sixty-six episodes were filmed.\n\nIn the early 1990s, LucasArts tasked Dave Grossman and Tim Schafer, both of whom had worked on the \"Monkey Island\" series, with designing a sequel to \"Maniac Mansion\". Gilbert and Winnick initially assisted with the project's writing. The team included voice acting and more detailed graphics, which Gilbert had originally envisioned for \"Maniac Mansion\". The first game's nonlinear design was discarded, and the team implemented a Chuck Jones-inspired visual style, alongside numerous puzzles based on time travel. Bernard and the Edison family were retained. The sequel, entitled \"Day of the Tentacle\", was released in 1993, and came with a fully playable copy of \"Maniac Mansion\" hidden as an easter egg.\n\nIn 2010, the staff of \"GamesTM\" dubbed \"Maniac Mansion\" a \"seminal\" title that overhauled the gameplay of the graphic adventure genre. Removing the need to guess syntax allowed players to concentrate on the story and puzzles, which created a smoother and more enjoyable experience, according to the magazine. Eurogamer's Kristan Reed agreed: he believed that the design was \"infinitely more elegant and intuitive\" than its predecessors and that it freed players from \"guessing-game frustration\". Designer Dave Grossman, who worked on Lucasfilm Games' later \"Day of the Tentacle\" and \"The Secret of Monkey Island\", felt that \"Maniac Mansion\" had revolutionized the adventure game genre. Although 1985's \"Uninvited\" had featured a point-and-click interface, it was not influential. \"Maniac Mansion\"s implementation of the concept was widely imitated in other adventure titles. Writing in the game studies journal \"Kinephanos\", Jonathan Lessard argued that \"Maniac Mansion\" led a \"Casual Revolution\" in the late 1980s, which opened the adventure genre to a wider audience. Similarly, Christopher Buecheler of GameSpy called the game a contributor to its genre's subsequent critical adoration and commercial success.\n\nReed highlighted the \"wonderfully ambitious\" design of \"Maniac Mansion\", in reference to its writing, interface, and cast of characters. Game designer Sheri Graner Ray believed the game to challenge \"damsel in distress\" stereotypes through its inclusion of female protagonists. Conversely, writer Mark Dery argued that the goal of rescuing a kidnapped cheerleader reinforced negative gender roles. The Lucasfilm team built on their experiences from \"Maniac Mansion\" and became increasingly ambitious in subsequent titles. Gilbert admitted to making mistakes—such as the inclusion of no-win situations—in \"Maniac Mansion\", and he applied these lessons to future projects. For example, the game relies on timers rather than events to trigger cutscenes, which occasionally results in awkward transitions: Gilbert worked to avoid this flaw with the \"Monkey Island\" series. Because of \"Maniac Mansion\"s imperfections, Gilbert considers it his favorite of his games.\n\nAccording to writers Mike and Sandie Morrison, Lucasfilm Games became \"serious competition\" in the adventure genre after the release of \"Maniac Mansion\". The game's success solidified Lucasfilm as one of the leading producers of adventure games: authors Rusel DeMaria and Johnny Wilson described it as a \"landmark title\" for the company. In their view, \"Maniac Mansion\"—along with \"Space Quest: The Sarien Encounter\" and \"Leisure Suit Larry in the Land of the Lounge Lizards\"—inaugurated a \"new era of humor-based adventure games\". This belief was shared by Reed, who wrote that \"Maniac Mansion\" \"set in motion a captivating chapter in the history of gaming\" that encompassed wit, invention, and style. The SCUMM engine was reused by Lucasfilm in eleven later titles; improvements were made to its code with each game. Over time, rival adventure game developers adopted this paradigm in their own software. \"GamesTM\" attributed the change to a desire to streamline production and create enjoyable games. Following his 1992 departure from LucasArts—a conglomeration of Lucasfilm Games, ILM and Skywalker Sound formed in 1990—Gilbert used SCUMM to create adventure games and \"Backyard Sports\" titles for Humongous Entertainment.\n\nIn 2011, Richard Cobbett summarized \"Maniac Mansion\" as \"one of the most intricate and important adventure games ever made\". \"Retro Gamer\" ranked it as one of the ten best Commodore 64 games in 2006, and IGN later named it one of the ten best LucasArts adventure games. Seven years after the NES version's debut, \"Nintendo Power\" named it the 61st best game ever. The publication dubbed it the 16th best NES title in 2008. The game's uniqueness and clever writing were praised by \"Nintendo Power\": in 2010, the magazine's Chris Hoffman stated that the game is \"unlike anything else out there — a point-and-click adventure with an awesome sense of humor and multiple solutions to almost every puzzle.\" In its retrospective coverage, \"Nintendo Power\" several times noted the ability to microwave a hamster, which the staff considered to be an iconic scene. In March 2012, \"Retro Gamer\" listed the hamster incident as one of the \"100 Classic Gaming Moments\".\n\n\"Maniac Mansion\" enthusiasts have drawn fan art of its characters, participated in tentacle-themed cosplay and produced a trailer for a fictitious film adaptation of the game. German fan Sascha Borisow created a fan game remake, entitled \"Maniac Mansion Deluxe\", with enhanced audio and visuals. He used the Adventure Game Studio engine to develop the project, which he distributed free of charge on the Internet. By the end of 2004, the remake had over 200,000 downloads. A remake with three-dimensional graphics called \"Meteor Mess\" was created by the German developer Vampyr Games, and, as of 2011, another group in Germany is producing one with art direction similar to that of \"Day of the Tentacle\". Fans have created an episodic series of games based on \"Maniac Mansion\" as well. Gilbert has said that he would like to see an official remake, similar in its graphics and gameplay to \"The Secret of Monkey Island: Special Edition\" and \"Monkey Island 2 Special Edition: LeChuck's Revenge\". He also expressed doubts about its potential quality, in light of George Lucas' enhanced remakes of the original \"Star Wars\" trilogy. In December 2017, Disney, which gained rights to the LucasArts games following its acquisition of Lucasfilms, published \"Maniac Mansion\" running atop the ScummVM virtual machine to various digital storefronts.\n\n"}
{"id": "19669", "url": "https://en.wikipedia.org/wiki?curid=19669", "title": "Marx Brothers", "text": "Marx Brothers\n\nThe Marx Brothers were an American family comedy act that was successful in vaudeville, on Broadway, and in motion pictures from 1905 to 1949. Five of the Marx Brothers' thirteen feature films were selected by the American Film Institute (AFI) as among the top 100 comedy films, with two of them (\"Duck Soup\" and \"A Night at the Opera\") in the top twelve. They are widely considered by critics, scholars, and fans to be among the greatest and most influential comedians of the 20th century. The brothers were included in AFI's 100 Years... 100 Stars list of the 25 greatest male stars of Classic Hollywood cinema, the only performers to be inducted collectively.\n\nThe group is almost universally known today by their stage names: Chico, Harpo, Groucho, Gummo, and Zeppo. There was a 6th brother, the first born, named Manfred (Mannie), who died aged 7 months; Zeppo was given the middle name Manfred in his memory.\n\nThe core of the act was the three elder brothers: Chico, Harpo, and Groucho, each of whom developed a highly distinctive stage persona. After the group essentially disbanded in 1950, Groucho went on to begin a successful second career in television, while Harpo and Chico appeared less prominently. The two younger brothers, Gummo and Zeppo, never developed their stage characters to the same extent as the elder three. They each left the act to pursue business careers at which they were successful, and for a time ran a large theatrical agency through which they represented their brothers and others. Gummo was not in any of the movies; Zeppo appeared in the first five films in relatively straight (non-comedic) roles. The performing lives of the brothers owed much to their mother Minnie Marx, who acted as their manager until her death in 1929.\n\nThe Marx Brothers were born in New York City, the sons of Jewish immigrants from Germany and France. Their mother Miene \"Minnie\" Schoenberg (professionally known as Minnie Palmer, later the brothers' manager) was from Dornum in East Frisia, and their father Samuel (\"Sam\"; born Simon) Marx was a native of Alsace and worked as a tailor. (His name was changed to Samuel Marx, and he was nicknamed \"Frenchy\".) The family lived in the poor Yorkville section of New York City's Upper East Side, centered in the Irish, German and Italian quarters. The brothers are best known by their stage names:\n\nAnother brother, Manfred (\"Mannie\"), the first-born son of Sam and Minnie, was born in 1886 and died in infancy:\n\nThe Marx Brothers also had an older sister, actually a cousin, born in January 1885 who had been adopted by Minnie and Frenchie. Her name was Pauline, or \"Polly\". Groucho talked about her in his 1972 Carnegie Hall concert.\n\nMinnie Marx came from a family of performers. Her mother was a yodeling harpist and her father a ventriloquist; both were funfair entertainers. Around 1880, the family emigrated to New York City, where Minnie married Sam in 1884. During the early 20th century, Minnie helped her younger brother Abraham Elieser Adolf Schönberg (stage name Al Shean) to enter show business; he became highly successful on vaudeville and Broadway as half of the musical comedy double act Gallagher and Shean, and this gave the brothers an entree to musical comedy, vaudeville and Broadway at Minnie's instigation. Minnie also acted as the brothers' manager, using the name Minnie Palmer so that agents did not realize that she was also their mother. All the brothers confirmed that Minnie Marx had been the head of the family and the driving force in getting the troupe launched, the only person who could keep them in order; she was said to be a hard bargainer with theatre management.\n\nGummo and Zeppo both became successful businessmen: Gummo gained success through his agency activities and a raincoat business, and Zeppo became a multi-millionaire through his engineering business.\n\nThe brothers were from a family of artists, and their musical talent was encouraged from an early age. Harpo was particularly talented, learning to play an estimated six different instruments throughout his career. He became a dedicated harpist, which gave him his nickname. Chico was an excellent pianist, Groucho a guitarist and singer, and Zeppo a vocalist.\n\nThey got their start in vaudeville, where their uncle Albert Schönberg performed as Al Shean of Gallagher and Shean. Groucho's debut was in 1905, mainly as a singer. By 1907, he and Gummo were singing together as \"The Three Nightingales\" with Mabel O'Donnell. The next year, Harpo became the fourth Nightingale and by 1910, the group briefly expanded to include their mother Minnie and their Aunt Hannah. The troupe was renamed \"The Six Mascots\".\n\nOne evening in 1912, a performance at the Opera House in Nacogdoches, Texas, was interrupted by shouts from outside about a runaway mule. The audience hurried out to see what was happening. Groucho was angered by the interruption and, when the audience returned, he made snide comments at their expense, including \"Nacogdoches is full of roaches\" and \"the jackass is the flower of Tex-ass\". Instead of becoming angry, the audience laughed. The family then realized that it had potential as a comic troupe. (However, in his autobiography \"Harpo Speaks\", Harpo Marx stated that the runaway mule incident occurred in Ada, Oklahoma. A 1930 article in the \"San Antonio Express\" newspaper stated that the incident took place in Marshall, Texas.)\n\nThe act slowly evolved from singing with comedy to comedy with music. The brothers' sketch \"Fun in Hi Skule\" featured Groucho as a German-accented teacher presiding over a classroom that included students Harpo, Gummo, and Chico. The last version of the school act was titled \"Home Again\" and was written by their uncle Al Shean. The \"Home Again\" tour reached Flint, Michigan in 1915, where 14-year-old Zeppo joined his four brothers for what is believed to be the only time that all five Marx Brothers appeared together on stage. Gummo then left to serve in World War I, reasoning that \"anything is better than being an actor!\" Zeppo replaced him in their final vaudeville years and in the jump to Broadway, and then to Paramount films.\nDuring World War I, anti-German sentiments were common, and the family tried to conceal its German origin. Mother Minnie learned that farmers were excluded from the draft rolls, so she purchased a poultry farm near Countryside, Illinois — but the brothers soon found that chicken ranching was not in their blood. During this time, Groucho discontinued his \"German\" stage personality.\n\nBy this time, \"The Four Marx Brothers\" had begun to incorporate their unique style of comedy into their act and to develop their characters. Both Groucho's and Harpo's memoirs say that their now-famous on-stage personae were created by Al Shean. Groucho began to wear his trademark greasepaint mustache and to use a stooped walk. Harpo stopped speaking onstage and began to wear a red fright wig and carry a taxi-cab horn. Chico spoke with a fake Italian accent, developed off-stage to deal with neighborhood toughs, while Zeppo adopted the role of the romantic (and \"peerlessly cheesy\", according to James Agee) straight man.\n\nThe on-stage personalities of Groucho, Chico, and Harpo were said to have been based on their actual traits. Zeppo, on the other hand, was considered the funniest brother offstage, despite his straight stage roles. He was the youngest and had grown up watching his brothers, so he could fill in for and imitate any of the others when illness kept them from performing. \"He was so good as Captain Spaulding [in \"Animal Crackers\"] that I would have let him play the part indefinitely, if they had allowed me to smoke in the audience,\" Groucho recalled. (Zeppo stood in for Groucho in the film version of \"Animal Crackers\". Groucho was unavailable to film the scene in which the Beaugard painting is stolen, so the script was contrived to include a power failure, which allowed Zeppo to play the Spaulding part in near-darkness.) In December 1917 the Marx brothers were noted in an advertisement playing in a musical comedy act \"Home Again\".\n\nBy the 1920s, the Marx Brothers had become one of America's favorite theatrical acts, with their sharp and bizarre sense of humor. They satirized high society and human hypocrisy, and they became famous for their improvisational comedy in free-form scenarios. A famous early instance was when Harpo arranged to chase a fleeing chorus girl across the stage during the middle of a Groucho monologue to see if Groucho would be thrown off. However, to the audience's delight, Groucho merely reacted by commenting, \"First time I ever saw a taxi hail a passenger\". When Harpo chased the girl back in the other direction, Groucho calmly checked his watch and ad-libbed, \"The 9:20's right on time. You can set your watch by the Lehigh Valley.\"\n\nThe brothers' vaudeville act had made them stars on Broadway under Chico's management and with Groucho's creative direction—first with the musical revue \"I'll Say She Is\" (1924–1925) and then with two musical comedies: \"The Cocoanuts\" (1925–1926) and \"Animal Crackers\" (1928–1929). Playwright George S. Kaufman worked on the last two and helped sharpen the brothers' characterizations.\n\nOut of their distinctive costumes, the brothers looked alike, even down to their receding hairlines. Zeppo could pass for a younger Groucho, and played the role of his son in \"Horse Feathers\". A scene in \"Duck Soup\" finds Groucho, Harpo, and Chico all appearing in the famous greasepaint eyebrows, mustache, and round glasses while wearing nightcaps. The three are indistinguishable, enabling them to carry off the \"mirror scene\" perfectly.\n\nThe stage names of the brothers (except Zeppo) were coined by monologist Art Fisher during a poker game in Galesburg, Illinois, based both on the brothers' personalities and Gus Mager's \"Sherlocko the Monk\", a popular comic strip of the day that included a supporting character named \"Groucho\". As Fisher dealt each brother a card, he addressed him, for the very first time, by the names they kept for the rest of their lives.\n\nThe reasons behind Chico's and Harpo's stage names are undisputed, and Gummo's is fairly well established. Groucho's and Zeppo's are far less clear. Arthur was named Harpo because he played the harp, and Leonard became Chico (pronounced \"Chick-o\") because he was, in the slang of the period, a \"chicken chaser\". (\"Chickens\"—later \"chicks\"—was period slang for women. \"In England now,\" said Groucho, \"they were called 'birds'.\")\n\nIn his autobiography, Harpo explained that Milton became Gummo because he crept about the theater like a gumshoe detective. Other sources reported that Gummo was the family's hypochondriac, having been the sickliest of the brothers in childhood, and therefore wore rubber overshoes, called gumshoes, in all kinds of weather. Still others reported that Milton was the troupe's best dancer, and dance shoes tended to have rubber soles. Groucho stated that the source of the name was Gummo wearing galoshes. Whatever the details, the name relates to rubber-soled shoes.\n\nThe reason that Julius was named Groucho is perhaps the most disputed. There are three explanations:\n\n\nI kept my money in a 'grouch bag'. This was a small chamois bag that actors used to wear around their neck to keep other hungry actors from pinching their dough. Naturally, you're going to think that's where I got my name from. But that's not so. Grouch bags were worn on manly chests long before there was a Groucho.\n\n\nHerbert was not nicknamed by Art Fisher, since he did not join the act until Gummo had departed. As with Groucho, three explanations exist for Herbert's name \"Zeppo\":\n\n\nMaxine Marx reported in \"The Unknown Marx Brothers\" that the brothers listed their \"real\" names (Julius, Leonard, Adolph, Milton, and Herbert) on playbills and in programs, and only used the nicknames behind the scenes, until Alexander Woollcott overheard them calling one another by the nicknames. He asked them why they used their real names publicly when they had such wonderful nicknames, and they replied, \"That wouldn't be dignified.\" Woollcott answered with a belly laugh. Woollcott did not meet the Marx Brothers until the premiere of \"I'll Say She Is\", which was their first Broadway show, so this would mean that they used their real names throughout their vaudeville days, and that the name \"Gummo\" never appeared in print during his time in the act. Other sources reported that the Marx Brothers went by their nicknames during their vaudeville era, but briefly listed themselves by their given names when \"I'll Say She Is\" opened because they were worried that a Broadway audience would reject a vaudeville act if they were perceived as low class.\n\nThe Marx Brothers' stage shows became popular just as motion pictures were evolving to \"talkies\". They signed a contract with Paramount Pictures and embarked on their film career at Paramount's studios in New York City's Astoria section. Their first two released films (after an unreleased short silent film titled \"Humor Risk\") were adaptations of the Broadway shows \"The Cocoanuts\" (1929) and \"Animal Crackers\" (1930). Both were written by George S. Kaufman and Morrie Ryskind. Production then shifted to Hollywood, beginning with a short film that was included in Paramount's twentieth anniversary documentary, \"The House That Shadows Built\" (1931), in which they adapted a scene from \"I'll Say She Is\". Their third feature-length film, \"Monkey Business\" (1931), was their first movie not based on a stage production.\n\n\"Horse Feathers\" (1932), in which the brothers satirized the American college system and Prohibition, was their most popular film yet, and won them the cover of \"Time\" magazine. It included a running gag from their stage work, in which Harpo produces a ludicrous array of props from inside his coat, including a wooden mallet, a fish, a coiled rope, a tie, a poster of a woman in her underwear, a cup of hot coffee, a sword; and, just after Groucho warns him that he \"can't burn the candle at both ends,\" a candle burning at both ends.\n\nDuring this period Chico and Groucho starred in a radio comedy series, \"Flywheel, Shyster and Flywheel\". Though the series was short lived, much of the material developed for it was used in subsequent films. The show's scripts and recordings were believed lost until copies of the scripts were found in the Library of Congress in the 1980s. After publication in a book they were performed with Marx Brothers impersonators for BBC Radio.\n\nTheir last Paramount film, \"Duck Soup\" (1933), directed by the highly regarded Leo McCarey, is the highest rated of the five Marx Brothers films on the American Film Institute's \"100 years ... 100 Movies\" list. It did not do as well financially as \"Horse Feathers\", but was the sixth-highest grosser of 1933. The film sparked a dispute between the Marxes and the village of Fredonia, New York. \"Freedonia\" was the name of a fictional country in the script, and the city fathers wrote to Paramount and asked the studio to remove all references to Freedonia because \"it is hurting our town's image\". Groucho fired back a sarcastic retort asking them to change the name of their town, because \"it's hurting our picture.\"\n\nAfter expiration of the Paramount contract Zeppo left the act to become an agent. He and brother Gummo went on to build one of the biggest talent agencies in Hollywood, helping the likes of Jack Benny and Lana Turner get their starts. Groucho and Chico did radio, and there was talk of returning to Broadway. At a bridge game with Chico, Irving Thalberg began discussing the possibility of the Marxes joining Metro-Goldwyn-Mayer. They signed, now billed as \"Groucho, Chico, Harpo, Marx Bros.\"\n\nUnlike the free-for-all scripts at Paramount, Thalberg insisted on a strong story structure that made the brothers more sympathetic characters, interweaving their comedy with romantic plots and non-comic musical numbers, and targeting their mischief-making at obvious villains. Thalberg was adamant that scripts include a \"low point\", where all seems lost for both the Marxes and the romantic leads. He instituted the innovation of testing the film's script before live audiences before filming began, to perfect the comic timing, and to retain jokes that earned laughs and replace those that did not. Thalberg restored Harpo's harp solos and Chico's piano solos, which had been omitted from \"Duck Soup\".\n\nThe first Marx Brothers/Thalberg film was \"A Night at the Opera\" (1935), a satire on the world of opera, where the brothers help two young singers in love by throwing a production of \"Il Trovatore\" into chaos. The film—including its famous scene where an absurd number of people crowd into a tiny stateroom on a ship—was a great success, and was followed two years later by an even bigger hit, \"A Day at the Races\" (1937), in which the brothers cause mayhem in a sanitarium and at a horse race. The film features Groucho and Chico's famous \"Tootsie Frootsie Ice Cream\" sketch. In a 1969 interview with Dick Cavett, Groucho said that the two movies made with Thalberg were the best that they ever produced. Despite the Thalberg films' success, the brothers left MGM in 1937; Thalberg had died suddenly on September 14, 1936, two weeks after filming began on \"A Day at the Races\", leaving the Marxes without an advocate at the studio.\n\nAfter a short experience at RKO (\"Room Service\", 1938), the Marx Brothers returned to MGM and made three more films: \"At the Circus\" (1939), \"Go West\" (1940) and \"The Big Store\" (1941). Prior to the release of \"The Big Store\" the team announced they were retiring from the screen. Four years later, however, Chico persuaded his brothers to make two additional films, \"A Night in Casablanca\" (1946) and \"Love Happy\" (1949), to alleviate his severe gambling debts. Both pictures were released by United Artists.\n\nFrom the 1940s onward Chico and Harpo appeared separately and together in nightclubs and casinos. Chico fronted a big band, the Chico Marx Orchestra (with 17-year-old Mel Tormé as a vocalist). Groucho made several radio appearances during the 1940s and starred in \"You Bet Your Life\", which ran from 1947 to 1961 on NBC radio and television. He authored several books, including \"Groucho and Me\" (1959), \"Memoirs of a Mangy Lover\" (1964) and \"The Groucho Letters\" (1967).\n\nGroucho and Chico briefly appeared in a 1957 color short film promoting \"The Saturday Evening Post\" entitled \"Showdown at Ulcer Gulch,\" directed by animator Shamus Culhane, Chico's son-in-law. Groucho, Chico, and Harpo worked together (in separate scenes) in \"The Story of Mankind\" (1957). In 1959, the three began production of \"Deputy Seraph\", a TV series starring Harpo and Chico as blundering angels, and Groucho (in every third episode) as their boss, the \"Deputy Seraph.\" The project was abandoned when Chico was found to be uninsurable (and incapable of memorizing his lines) due to severe arteriosclerosis. On March 8 of that year, Chico and Harpo starred as bumbling thieves in \"The Incredible Jewel Robbery\", a half-hour pantomimed episode of the \"General Electric Theater\" on CBS. Groucho made a cameo appearance—uncredited, because of constraints in his NBC contract—in the last scene, and delivered the only line of dialogue (\"We won't talk until we see our lawyer!\").\nAccording to a September 1947 article in \"Newsweek\", Groucho, Harpo, Chico and Zeppo all signed to appear as themselves in a biopic entitled \"The Life and Times of the Marx Brothers\". In addition to being a non-fiction biography of the Marxes, the film would have featured the brothers reenacting much of their previously unfilmed material from both their vaudeville and Broadway eras. The film, had it been made, would have been the first performance by the Brothers as a quartet since 1933.\n\nThe five brothers made only one television appearance together, in 1957, on an early incarnation of \"The Tonight Show\" called \"Tonight! America After Dark\", hosted by Jack Lescoulie. Five years later (October 1, 1962) after Jack Paar's tenure, Groucho made a guest appearance to introduce the \"Tonight Show's\" new host, Johnny Carson.\n\nAround 1960, the acclaimed director Billy Wilder considered writing and directing a new Marx Brothers film. Tentatively titled \"A Day at the U.N.\", it was to be a comedy of international intrigue set around the United Nations building in New York. Wilder had discussions with Groucho and Gummo, but the project was put on hold because of Harpo's ill-health and abandoned when Chico died in 1961. He was 74. Three years later, on September 28, 1964, Harpo died at the age of 75 of a heart attack one day after heart surgery.\n\nIn 1966 Filmation produced a pilot for a Marx Brothers cartoon. Groucho's voice was supplied by Pat Harrington Jr. and other voices were done by Ted Knight and Joe Besser.\n\nIn 1970, the four Marx Brothers had a brief reunion of sorts in the animated ABC television special \"The Mad, Mad, Mad Comedians\", produced by Rankin-Bass animation (of \"Rudolph the Red-Nosed Reindeer\" fame). The special featured animated reworkings of various famous comedians' acts, including W. C. Fields, Jack Benny, George Burns, Henny Youngman, the Smothers Brothers, Flip Wilson, Phyllis Diller, Jack E. Leonard, George Jessel and the Marx Brothers. Most of the comedians provided their own voices for their animated counterparts, except for Fields and Chico Marx (both had died), and Zeppo Marx (who had left show business in 1933). Voice actor Paul Frees filled in for all three (no voice was needed for Harpo). The Marx Brothers' segment was a reworking of a scene from their Broadway play \"I'll Say She Is\", a parody of Napoleon that Groucho considered among the brothers' funniest routines. The sketch featured animated representations, if not the voices, of all four brothers. Romeo Muller is credited as having written special material for the show, but the script for the classic \"Napoleon Scene\" was probably supplied by Groucho.\n\nOn January 16, 1977, the Marx Brothers were inducted into the Motion Picture Hall of Fame. With the deaths of Gummo in April 1977, Groucho in August 1977, and Zeppo in November 1979, the brothers were gone. But their impact on the entertainment community continues well into the 21st century.\n\nMany television shows and movies have used Marx Brothers references. \"Animaniacs\" and \"Tiny Toons\", for example, have featured Marx Brothers jokes and skits. Hawkeye Pierce (Alan Alda) on \"M*A*S*H\" occasionally put on a fake nose and glasses, and, holding a cigar, did a Groucho impersonation to amuse patients recovering from surgery. Early episodes also featured a singing and off-scene character named Captain Spaulding as a tribute.\n\nBugs Bunny impersonated Groucho Marx in the 1947 cartoon \"Slick Hare\" (with Elmer Fudd dressing up as Harpo and chasing him with a cleaver) and in a later cartoon he again impersonated Groucho hosting a TV show called \"You Beat Your Wife,\" asking Elmer Fudd if he had stopped beating his wife. Tex Avery's cartoon \"Hollywood Steps Out\" (1941) featured appearances by Harpo and Groucho. They appeared, sometimes with Chico and Zeppo caricatured, in cartoons starring Mickey Mouse, Flip the Frog and others. In the \"Airwolf\" episode 'Condemned', four anti-virus formulae for a deadly plague were named after the four Marx Brothers.\n\nIn \"All in the Family\", Rob Reiner often did imitations of Groucho, and Sally Struthers dressed as Harpo in one episode in which she (as Gloria Stivic) and Rob (as Mike Stivic) were going to a Marx Brothers film festival, with Reiner dressing as Groucho. Gabe Kaplan did many Groucho imitations on his sit-com \"Welcome Back, Kotter\" and Robert Hegyes sometimes imitated both Chico and Harpo on the show. In Woody Allen's film \"Hannah and Her Sisters\" (1986), Woody's character, after a suicide attempt, is inspired to go on living after seeing a revival showing of \"Duck Soup\". In \"Manhattan\" (1979), he names the Marx Brothers as something that makes life worth living. In an episode of \"The Mary Tyler Moore Show\" Murray calls the new station owner at home late at night to complain when the song \"Hooray for Captain Spaulding\" is cut from a showing of \"Animal Crackers\" because of the new owners' policy to cut more and more from shows to sell more ad time, putting his job on the line.\n\nIn \"Everyone Says I Love You\" (1996), Woody Allen and Goldie Hawn dress as Groucho for a Marx Brothers celebration in France, and the song \"Hooray for Captain Spaulding\", from \"Animal Crackers\", is performed, with various actors dressed as the brothers, striking poses famous to Marx fans. (The film itself is named after a song from \"Horse Feathers\", a version of which plays over the opening credits.)\n\nHarpo Marx appeared as himself in a sketch on \"I Love Lucy\" in which he and Lucille Ball reprised the mirror routine from \"Duck Soup\", with Lucy dressed up as Harpo. Lucy had worked with the Marxes when she appeared in a supporting role in an earlier Marx Brothers film, \"Room Service\". Chico once appeared on \"I've Got a Secret\" dressed up as Harpo; his secret was shown in a caption reading, \"I'm pretending to be Harpo Marx (I'm Chico)\". The Marx Brothers were spoofed in the second act of the Broadway Review \"A Day in Hollywood/A Night in the Ukraine\".\n\nIn the 1989 film \"Indiana Jones and the Last Crusade\", Professor Henry Jones (Sean Connery) mails his diary to his son Indiana Jones (Harrison Ford) to keep it out of Nazi hands. When Indy misconstrues the purpose of being sent it and returns it to his father instead, his father berates him by saying \"I should have mailed it to the Marx Brothers!\".\n\nIn the 1996 musical, By Jeeves, based on the Jeeves stories by P.G. Wodehouse, during \"The Hallo Song\", Gussie Fink-Nottle suggests \"You're either Pablo Picasso\", to which Cyrus Budge III replies \"or maybe Harpo Marx!\"\n\nIn Rob Zombie's 2003 film House of 1000 Corpses, the clown Captain Spaulding is named after the Marx brothers character, and this is mentioned in the movie.\n\nFilms with the four Marx Brothers:\n\nFilms with the three Marx Brothers (post-Zeppo):\n\nSolo endeavors:\n\nIn the 1974 Academy Awards telecast, Jack Lemmon presented Groucho with an honorary Academy Award to a standing ovation. The award was also on behalf of Harpo, Chico, and Zeppo, whom Lemmon mentioned by name. It was one of Groucho's final major public appearances. \"I wish that Harpo and Chico could be here to share with me this great honor,\" he said, naming the two deceased brothers (Zeppo was still alive at the time). Groucho also praised the late Margaret Dumont as a great straight woman who never understood any of his jokes.\n\nThe Marx Brothers were collectively named #20 on AFI's list of the Top 25 American male screen legends of Classic Hollywood. They are the only group to be so honored.\n\nThe \"Sweathogs\" of the ABC-TV series \"Welcome Back Kotter\" (John Travolta, Robert Hegyes, Lawrence Hilton-Jacobs, and Ron Palillo) patterned much of their on-camera banter in that series after the Marx Brothers. Series star Gabe Kaplan was reputedly a big Marx Brothers fan.\n\n\n\n"}
{"id": "19672", "url": "https://en.wikipedia.org/wiki?curid=19672", "title": "May 28", "text": "May 28\n\n\n\n"}
{"id": "19673", "url": "https://en.wikipedia.org/wiki?curid=19673", "title": "MP3", "text": "MP3\n\nMP3 (formally MPEG-1 Audio Layer III or MPEG-2 Audio Layer III) is a coding format for digital audio. Originally defined as the third audio format of the MPEG-1 standard, it was retained and further extended—defining additional bit-rates and support for more audio channels—as the third audio format of the subsequent MPEG-2 standard. A third version, known as MPEG 2.5—extended to better support lower bit rates—is commonly implemented, but is not a recognized standard.\n\nMP3 (or mp3) as a file format commonly designates files containing an elementary stream of MPEG-1 audio and video encoded data, without other complexities of the MP3 standard.\n\nIn the aspects of MP3 pertaining to audio compression—the aspect of the standard most apparent to end-users (and for which is it best known)—MP3 uses lossy data-compression to encode data using inexact approximations and the partial discarding of data. This allows a large reduction in file sizes when compared to uncompressed audio. The combination of small size and acceptable fidelity led to a boom in the distribution of music over the Internet in the mid- to late-1990s, with MP3 serving as an enabling technology at a time when bandwidth and storage were still at a premium. The MP3 format soon became associated with controversies surrounding copyright infringement, music piracy, and the file ripping/sharing services MP3.com and Napster, among others. With the advent of portable media players, a product category also including smartphones, MP3 support remains near-universal.\n\nMP3 compression works by reducing (or approximating) the accuracy of certain components of sound that are considered to be beyond the hearing capabilities of most humans. This method is commonly referred to as perceptual coding or as psychoacoustic modeling. The remaining audio information is then recorded in a space-efficient manner. Compared to CD-quality digital audio, MP3 compression can commonly achieve a 75 to 95% reduction in size. For example, an MP3 encoded at a constant bitrate of 128 kbit/s would result in a file approximately 9% of the size of the original CD audio.\n\nAlso designed as a streamable format, segments of a transmission can be lost without affecting the ability to decode later segments.\n\nThe Moving Picture Experts Group (MPEG) designed MP3 as part of its MPEG-1, and later MPEG-2, standards. The first subgroup for audio was formed by several teams of engineers at CCETT, Matsushita, Philips, Sony, AT&T-Bell Labs, Thomson-Brandt, and others. MPEG-1 Audio (MPEG-1 Part 3), which included MPEG-1 Audio Layer I, II and III, was approved as a committee draft for an ISO/IEC standard in 1991, finalised in 1992, and published in 1993 as ISO/IEC 11172-3:1993. A backwards-compatible MPEG-2 Audio (MPEG-2 Part 3) extension with lower sample- and bit-rates was published in 1995 as ISO/IEC 13818-3:1995.\n\nThe MP3 lossy audio data compression algorithm takes advantage of a perceptual limitation of human hearing called auditory masking. In 1894, the American physicist Alfred M. Mayer reported that a tone could be rendered inaudible by another tone of lower frequency. In 1959, Richard Ehmer described a complete set of auditory curves regarding this phenomenon. Ernst Terhardt \"et al.\" created an algorithm describing auditory masking with high accuracy. This work added to a variety of reports from authors dating back to Fletcher, and to the work that initially determined critical ratios and critical bandwidths.\n\nThe psychoacoustic masking codec was first proposed in 1979, apparently independently, by Manfred R. Schroeder, et al. from Bell Telephone Laboratories, Inc. in Murray Hill, New Jersey, and M. A. Krasner both in the United States. Krasner was the first to publish and to produce hardware for speech (not usable as music bit compression), but the publication of his results as a relatively obscure Lincoln Laboratory Technical Report, did not immediately influence the mainstream of psychoacoustic codec development. Manfred Schroeder was already a well-known and revered figure in the worldwide community of acoustical and electrical engineers, but his paper was not much noticed, since it described negative results due to the particular nature of speech and the linear predictive coding (LPC) gain present in speech.\n\nBoth Krasner and Schroeder built upon the work performed by Eberhard F. Zwicker in the areas of tuning and masking of critical frequency bands, that in turn built on the fundamental research in the area from Bell Labs of Harvey Fletcher and his collaborators. A wide variety of (mostly perceptual) audio compression algorithms were reported in IEEE's refereed Journal on Selected Areas in Communications. That journal reported in February 1988 on a wide range of established, working audio bit compression technologies, some of them using auditory masking as part of their fundamental design, and several showing real-time hardware implementations.\n\nThe Moving Picture Experts Group (MPEG) was established in 1988 by the initiative of Hiroshi Yasuda (Nippon Telegraph and Telephone) and Leonardo Chiariglione. Yasuda was leading an initiative in Japan, called the Digital Audio and Picture Architecture (DAPA), while Chiariglione was leading an initiative in Europe, called the Coding of Moving Images for Storage (COMIS). Both eventually met in May 1988 to work on a global standard.\n\nThe genesis of the MP3 technology is fully described in a paper from Professor Hans Musmann, who chaired the ISO MPEG Audio group for several years. In December 1988, MPEG called for an audio coding standard. In June 1989, 14 audio coding algorithms were submitted. Because of certain similarities between these coding proposals, they were clustered into four development groups. The first group was MUSICAM, by Matsushita, CCETT, ITT and Philips. The second group was ASPEC, by AT&T, France Telecom, Fraunhofer Gesellschaft, Deutsche and Thomson-Brandt. The third group was ATAC, by Fujitsu, JVC, NEC and Sony. And the fourth group was SB-ADPCM, by NTT and BTRL.\n\nThe immediate predecessors of MP3 were \"Optimum Coding in the Frequency Domain\" (OCF), and Perceptual Transform Coding (PXFM). These two codecs, along with block-switching contributions from Thomson-Brandt, were merged into a codec called ASPEC, which was submitted to MPEG, and which won the quality competition, but that was mistakenly rejected as too complex to implement. The first practical implementation of an audio perceptual coder (OCF) in hardware (Krasner's hardware was too cumbersome and slow for practical use), was an implementation of a psychoacoustic transform coder based on Motorola 56000 DSP chips.\n\nAnother predecessor of the MP3 format and technology is to be found in the perceptual codec MUSICAM based on an integer arithmetics 32 sub-bands filterbank, driven by a psychoacoustic model. It was primarily designed for Digital Audio Broadcasting (digital radio) and digital TV, and its basic principles disclosed to the scientific community by CCETT (France) and IRT (Germany) in Atlanta during an IEEE-ICASSP conference in 1991, after having worked on MUSICAM with Matsushita and Philips since 1989.\n\nThis codec incorporated into a broadcasting system using COFDM modulation was demonstrated on air and on the field together with Radio Canada and CRC Canada during the NAB show (Las Vegas) in 1991. The implementation of the audio part of this broadcasting system was based on a two chips encoder (one for the subband transform, one for the psychoacoustic model designed by the team of G. Stoll (IRT Germany), later known as psychoacoustic model I) and a real time decoder using one Motorola 56001 DSP chip running an integer arithmetics software designed by Y.F. Dehery's team (CCETT, France). The simplicity of the corresponding decoder together with the high audio quality of this codec using for the first time a 48 kHz sampling frequency, a 20 bits/sample input format (the highest available sampling standard in 1991, compatible with the AES/EBU professional digital input studio standard) were the main reasons to later adopt the characteristics of MUSICAM as the basic features for an advanced digital music compression codec.\n\nDuring the development of the MUSICAM encoding software, Stoll and Dehery's team made a thorough use of a set of high quality audio assessment material selected by a group of audio professionals from the European Broadcasting Union and later used as a reference for the assessment of music compression codecs . The subband coding technique was found to be efficient, not only for the perceptual coding of the high quality sound materials but especially for the encoding of critical percussive sound materials (drums, triangle, ..) due to the specific temporal masking effect of the MUSICAM sub-band filterbank (this advantage being a specific feature of short transform coding techniques).\n\nAs a doctoral student at Germany's University of Erlangen-Nuremberg, Karlheinz Brandenburg began working on digital music compression in the early 1980s, focusing on how people perceive music. He completed his doctoral work in 1989. MP3 is directly descended from OCF and PXFM, representing the outcome of the collaboration of Brandenburg—working as a postdoc at AT&T-Bell Labs with James D. Johnston (\"JJ\") of AT&T-Bell Labs—with the Fraunhofer Institute for Integrated Circuits, Erlangen (where he worked with Bernhard Grill and four other researchers – \"The Original Six\"), with relatively minor contributions from the MP2 branch of psychoacoustic sub-band coders. In 1990, Brandenburg became an assistant professor at Erlangen-Nuremberg. While there, he continued to work on music compression with scientists at the Fraunhofer Society (in 1993 he joined the staff of the Fraunhofer Institute). The song \"Tom's Diner\" by Suzanne Vega was the first song used by Karlheinz Brandenburg to develop the MP3. Brandenburg adopted the song for testing purposes, listening to it again and again each time refining the scheme, making sure it did not adversely affect the subtlety of Vega's voice.\n\nIn 1991, there were two available proposals that were assessed for an MPEG audio standard: MUSICAM (Masking pattern adapted Universal Subband Integrated Coding And Multiplexing) and ASPEC (Adaptive Spectral Perceptual Entropy Coding). As proposed by the Dutch corporation Philips, the French research institute CCETT, and the German standards organization Institute for Broadcast Technology, the MUSICAM technique was chosen due to its simplicity and error robustness, as well as for its high level of computational efficiency. The MUSICAM format, based on sub-band coding, became the basis for the MPEG Audio compression format, incorporating, for example, its frame structure, header format, sample rates, etc.\n\nWhile much of MUSICAM technology and ideas were incorporated into the definition of MPEG Audio Layer I and Layer II, the filter bank alone and the data structure based on 1152 samples framing (file format and byte oriented stream) of MUSICAM remained in the Layer III (MP3) format, as part of the computationally inefficient hybrid filter bank. Under the chairmanship of Professor Musmann of the University of Hanover, the editing of the standard was delegated to Dutchman Leon van de Kerkhof, to German Gerhard Stoll, to Frenchman Yves-François Dehery, who worked on Layer I and Layer II. ASPEC was the joint proposal of AT&T Bell Laboratories, Thomson Consumer Electronics, Fraunhofer Society and CNET. It provided the highest coding efficiency.\n\nA working group consisting of van de Kerkhof, Stoll, Italian Leonardo Chiariglione (CSELT VP for Media), Frenchman Yves-François Dehery, German Karlheinz Brandenburg, and American James D. Johnston (United States) took ideas from ASPEC, integrated the filter bank from Layer II, added some of their own ideas such as the joint stereo coding of MUSICAM and created the MP3 format, which was designed to achieve the same quality at 128 kbit/s as MP2 at 192 kbit/s.\n\nThe algorithms for MPEG-1 Audio Layer I, II and III were approved in 1991 and finalized in 1992 as part of MPEG-1, the first standard suite by MPEG, which resulted in the international standard ISO/IEC 11172-3 (a.k.a. \"MPEG-1 Audio\" or \"MPEG-1 Part 3\"), published in 1993. Files or data streams conforming to this standard must handle sample rates of 48k, 44100 and 32k and continue to be supported by current MP3 players and decoders. Thus the first generation of MP3 defined 14*3=42 interpretations of MP3 frame data structures and size layouts.\n\nFurther work on MPEG audio was finalized in 1994 as part of the second suite of MPEG standards, MPEG-2, more formally known as international standard ISO/IEC 13818-3 (a.k.a. \"MPEG-2 Part 3\" or backwards compatible \"MPEG-2 Audio\" or \"MPEG-2 Audio BC\"), originally published in 1995. MPEG-2 Part 3 (ISO/IEC 13818-3) defined 42 additional bit rates and sample rates for MPEG-1 Audio Layer I, II and III. The new sampling rates are exactly half that of those originally defined in MPEG-1 Audio. This reduction in sampling rate serves to cut the available frequency fidelity in half while likewise cutting the bitrate by 50%.\nMPEG-2 Part 3 also enhanced MPEG-1's audio by allowing the coding of audio programs with more than two channels, up to 5.1 multichannel. An MP3 coded with MPEG-2 results in half of the bandwidth reproduction of MPEG-1 appropriate for piano and singing.\n\nA third generation of \"MP3\" style data streams (files) extended the \"MPEG-2\" ideas and implementation but was named \"MPEG-2.5\" audio, since MPEG-3 already had a different meaning. This extension was developed at Fraunhofer IIS, the registered patent holders of MP3 by reducing the frame sync field in the MP3 header from 12 to 11 bits. As in the transition from MPEG-1 to MPEG-2, MPEG-2.5 adds additional sampling rates exactly half of those available using MPEG-2. It thus widens the scope of MP3 to include human speech and other applications yet requires only 25% of the bandwidth (frequency reproduction) possible using MPEG-1 sampling rates. While not an ISO recognized standard, MPEG-2.5 is widely supported by both inexpensive Chinese and brand name digital audio players as well as computer software based MP3 encoders (LAME), decoders (FFmpeg) and players (MPC) adding 3*8=24 additional MP3 frame types. Each generation of MP3 thus supports 3 sampling rates exactly half that of the previous generation for a total of 9 varieties of MP3 format files. The sample rate comparison table between MPEG-1, 2 and 2.5 is given later in the article. MPEG-2.5 is supported by LAME (since 2000), Media Player Classic (MPC), iTunes, and FFmpeg.\n\nMPEG-2.5 was not developed by MPEG (see above) and was never approved as an international standard. MPEG-2.5 is thus an unofficial or proprietary extension to the MP3 format. It is nonetheless ubiquitous and especially advantageous for low-bit rate human speech applications.\nThe ISO standard ISO/IEC 11172-3 (a.k.a. MPEG-1 Audio) defined three formats: the MPEG-1 Audio Layer I, Layer II and Layer III. The ISO standard ISO/IEC 13818-3 (a.k.a. MPEG-2 Audio) defined extended version of the MPEG-1 Audio: MPEG-2 Audio Layer I, Layer II and Layer III. MPEG-2 Audio (MPEG-2 Part 3) should not be confused with MPEG-2 AAC (MPEG-2 Part 7 – ISO/IEC 13818-7).\n\nCompression efficiency of encoders is typically defined by the bit rate, because compression ratio depends on the bit depth and sampling rate of the input signal. Nevertheless, compression ratios are often published. They may use the Compact Disc (CD) parameters as references (44.1 kHz, 2 channels at 16 bits per channel or 2×16 bit), or sometimes the Digital Audio Tape (DAT) SP parameters (48 kHz, 2×16 bit). Compression ratios with this latter reference are higher, which demonstrates the problem with use of the term \"compression ratio\" for lossy encoders.\n\nKarlheinz Brandenburg used a CD recording of Suzanne Vega's song \"Tom's Diner\" to assess and refine the MP3 compression algorithm. This song was chosen because of its nearly monophonic nature and wide spectral content, making it easier to hear imperfections in the compression format during playbacks. Some refer to Suzanne Vega as \"The mother of MP3\". This particular track has an interesting property in that the two channels are almost, but not completely, the same, leading to a case where Binaural Masking Level Depression causes spatial unmasking of noise artifacts unless the encoder properly recognizes the situation and applies corrections similar to those detailed in the MPEG-2 AAC psychoacoustic model. Some more critical audio excerpts (glockenspiel, triangle, accordion, etc.) were taken from the EBU V3/SQAM reference compact disc and have been used by professional sound engineers to assess the subjective quality of the MPEG Audio formats. LAME is the most advanced MP3 encoder. LAME includes a VBR variable bit rate encoding which uses a quality parameter rather than a bit rate goal. Later versions 2008+) support an n.nnn quality goal which automatically selects MPEG-2 or MPEG-2.5 sampling rates as appropriate for human speech recordings which need only 5512 Hz bandwidth resolution.\n\nA reference simulation software implementation, written in the C language and later known as \"ISO 11172-5\", was developed (in 1991–1996) by the members of the ISO MPEG Audio committee in order to produce bit compliant MPEG Audio files (Layer 1, Layer 2, Layer 3). It was approved as a committee draft of ISO/IEC technical report in March 1994 and printed as document CD 11172-5 in April 1994. It was approved as a draft technical report (DTR/DIS) in November 1994, finalized in 1996 and published as international standard ISO/IEC TR 11172-5:1998 in 1998. The reference software in C language was later published as a freely available ISO standard. Working in non-real time on a number of operating systems, it was able to demonstrate the first real time hardware decoding (DSP based) of compressed audio. Some other real time implementation of MPEG Audio encoders and decoders were available for the purpose of digital broadcasting (radio DAB, television DVB) towards consumer receivers and set top boxes.\n\nOn 7 July 1994, the Fraunhofer Society released the first software MP3 encoder called l3enc. The filename extension \".mp3\" was chosen by the Fraunhofer team on 14 July 1995 (previously, the files had been named \".bit\"). With the first real-time software MP3 player WinPlay3 (released 9 September 1995) many people were able to encode and play back MP3 files on their PCs. Because of the relatively small hard drives back in that time (≈500–1000 MB) lossy compression was essential to store non-instrument based (see tracker and MIDI) music for playback on computer. As sound scholar Jonathan Sterne notes, \"An Australian hacker acquired l3enc using a stolen credit card. The hacker then reverse-engineered the software, wrote a new user interface, and redistributed it for free, naming it \"thank you Fraunhofer\"\".\n\nIn the second half of the 1990s, MP3 files began to spread on the Internet, often via underground pirated song networks. The first known experiment in Internet distribution was organized in the early 1990s by the Internet Underground Music Archive, better known by the acronym IUMA. After some experiments using uncompressed audio files, this archive started to deliver on the native worldwide low speed Internet some compressed MPEG Audio files using the MP2 (Layer II) format and later on used MP3 files when the standard was fully completed. The popularity of MP3s began to rise rapidly with the advent of Nullsoft's audio player Winamp, released in 1997. In 1998, the first portable solid state digital audio player MPMan, developed by SaeHan Information Systems which is headquartered in Seoul, South Korea, was released and the Rio PMP300 was sold afterwards in 1998, despite legal suppression efforts by the RIAA.\n\nIn November 1997, the website mp3.com was offering thousands of MP3s created by independent artists for free. The small size of MP3 files enabled widespread peer-to-peer file sharing of music ripped from CDs, which would have previously been nearly impossible. The first large peer-to-peer filesharing network, Napster, was launched in 1999. The ease of creating and sharing MP3s resulted in widespread copyright infringement. Major record companies argued that this free sharing of music reduced sales, and called it \"music piracy\". They reacted by pursuing lawsuits against Napster (which was eventually shut down and later sold) and against individual users who engaged in file sharing.\n\nUnauthorized MP3 file sharing continues on next-generation peer-to-peer networks. Some authorized services, such as Beatport, Bleep, Juno Records, eMusic, Zune Marketplace, Walmart.com, Rhapsody, the recording industry approved re-incarnation of Napster, and Amazon.com sell unrestricted music in the MP3 format.\n\nAn MP3 file is made up of MP3 frames, which consist of a header and a data block. This sequence of frames is called an elementary stream. Due to the \"byte reservoir\", frames are not independent items and cannot usually be extracted on arbitrary frame boundaries. The MP3 Data blocks contain the (compressed) audio information in terms of frequencies and amplitudes. The diagram shows that the MP3 Header consists of a sync word, which is used to identify the beginning of a valid frame. This is followed by a bit indicating that this is the MPEG standard and two bits that indicate that layer 3 is used; hence MPEG-1 Audio Layer 3 or MP3. After this, the values will differ, depending on the MP3 file. \"ISO/IEC 11172-3\" defines the range of values for each section of the header along with the specification of the header. Most MP3 files today contain ID3 metadata, which precedes or follows the MP3 frames, as noted in the diagram. The data stream can contain an optional checksum.\n\nJoint stereo is done only on a frame-to-frame basis.\n\nThe MPEG-1 standard does not include a precise specification for an MP3 encoder, but does provide example psychoacoustic models, rate loop, and the like in the non-normative part of the original standard.\nMPEG-2 doubles the number of sampling rates which are supported and MPEG-2.5 adds 3 more. When this was written, the suggested implementations were quite dated. Implementers of the standard were supposed to devise their own algorithms suitable for removing parts of the information from the audio input. As a result, many different MP3 encoders became available, each producing files of differing quality. Comparisons were widely available, so it was easy for a prospective user of an encoder to research the best choice. Some encoders that were proficient at encoding at higher bit rates (such as LAME) were not necessarily as good at lower bit rates. Over time, LAME evolved on the SourceForge website until it became the de facto CBR MP3 encoder. Later an ABR mode was added. Work progressed on true variable bit rate using a quality goal between 0 and 10. Eventually numbers (such as -V 9.600) could generate excellent quality low bit rate voice encoding at only 41 kbit/s using the MPEG-2.5 extensions.\n\nDuring encoding, 576 time-domain samples are taken and are transformed to 576 frequency-domain samples. If there is a transient, 192 samples are taken instead of 576. This is done to limit the temporal spread of quantization noise accompanying the transient. (See psychoacoustics.) Frequency resolution is limited by the small long block window size, which decreases coding efficiency. Time resolution can be too low for highly transient signals and may cause smearing of percussive sounds.\n\nDue to the tree structure of the filter bank, pre-echo problems are made worse, as the combined impulse response of the two filter banks does not, and cannot, provide an optimum solution in time/frequency resolution. Additionally, the combining of the two filter banks' outputs creates aliasing problems that must be handled partially by the \"aliasing compensation\" stage; however, that creates excess energy to be coded in the frequency domain, thereby decreasing coding efficiency.\n\nDecoding, on the other hand, is carefully defined in the standard. Most decoders are \"bitstream compliant\", which means that the decompressed output that they produce from a given MP3 file will be the same, within a specified degree of rounding tolerance, as the output specified mathematically in the ISO/IEC high standard document (ISO/IEC 11172-3). Therefore, comparison of decoders is usually based on how computationally efficient they are (i.e., how much memory or CPU time they use in the decoding process). Over time this concern has become less of an issue as CPU speeds transitioned from MHz to GHz. Encoder/decoder overall delay is not defined, which means there is no official provision for gapless playback. However, some encoders such as LAME can attach additional metadata that will allow players that can handle it to deliver seamless playback.\n\nWhen performing lossy audio encoding, such as creating an MP3 data stream, there is a trade-off between the amount of data generated and the sound quality of the results. The person generating an MP3 selects a bit rate, which specifies how many kilobits per second of audio is desired. The higher the bit rate, the larger the MP3 data stream will be, and, generally, the closer it will sound to the original recording. With too low a bit rate, compression artifacts (i.e., sounds that were not present in the original recording) may be audible in the reproduction. Some audio is hard to compress because of its randomness and sharp attacks. When this type of audio is compressed, artifacts such as ringing or pre-echo are usually heard. A sample of applause or a triangle instrument with a relatively low bit rate provide good examples of compression artifacts. Most subjective testings of perceptual codecs tend to avoid using these types of sound materials, however, the artifacts generated by percussive sounds are barely perceptible due to the specific temporal masking feature of the 32 sub-band filterbank of Layer II on which the format is based.\n\nBesides the bit rate of an encoded piece of audio, the quality of MP3 encoded sound also depends on the quality of the encoder algorithm as well as the complexity of the signal being encoded. As the MP3 standard allows quite a bit of freedom with encoding algorithms, different encoders do feature quite different quality, even with identical bit rates. As an example, in a public listening test featuring two early MP3 encoders set at about 128 kbit/s, one scored 3.66 on a 1–5 scale, while the other scored only 2.22. Quality is dependent on the choice of encoder and encoding parameters.\n\nThis observation caused a revolution in audio encoding. Early on bitrate was the prime and only consideration. At the time MP3 files were of the very simplest type: they used the same bit rate for the entire file: this process is known as Constant Bit Rate (CBR) encoding. Using a constant bit rate makes encoding simpler and less CPU intensive. However, it is also possible to create files where the bit rate changes throughout the file. These are known as Variable Bit Rate The bit reservoir and VBR encoding were actually part of the original MPEG-1 standard. The concept behind them is that, in any piece of audio, some sections are easier to compress, such as silence or music containing only a few tones, while others will be more difficult to compress. So, the overall quality of the file may be increased by using a lower bit rate for the less complex passages and a higher one for the more complex parts. With some advanced MP3 encoders, it is possible to specify a given quality, and the encoder will adjust the bit rate accordingly. Users that desire a particular \"quality setting\" that is transparent to their ears can use this value when encoding all of their music, and generally speaking not need to worry about performing personal listening tests on each piece of music to determine the correct bit rate.\n\nPerceived quality can be influenced by listening environment (ambient noise), listener attention, and listener training and in most cases by listener audio equipment (such as sound cards, speakers and headphones). Furthermore, sufficient quality may be achieved by a lesser quality setting for lectures and human speech applications and reduces encoding time and complexity. A test given to new students by Stanford University Music Professor Jonathan Berger showed that student preference for MP3-quality music has risen each year. Berger said the students seem to prefer the 'sizzle' sounds that MP3s bring to music.\n\nAn in-depth study of MP3 audio quality, sound artist and composer Ryan Maguire's project \"The Ghost in the MP3\" isolates the sounds lost during MP3 compression. In 2015, he released the track \"moDernisT\" (an anagram of \"Tom's Diner\"), composed exclusively from the sounds deleted during MP3 compression of the song \"Tom's Diner\", the track originally used in the formulation of the MP3 standard. A detailed account of the techniques used to isolate the sounds deleted during MP3 compression, along with the conceptual motivation for the project, was published in the 2014 Proceedings of the International Computer Music Conference.\n\nBitrate is the product of the sample rate and number of bits per sample used to encode the music. CD audio is 44100 samples per second. The number of bits per sample also depends on the number of audio channels. CD is stereo and 16 bits per channel. So, multiplying 44100 by 32 gives 1411200—the bitrate of uncompressed CD digital audio. MP3 was designed to encode this 1411 kbit/s data at 320 kbit/s or less. As less complex passages are detected by MP3 algorithms then lower bitrates may be employed. When using MPEG-2 instead of MPEG-1, MP3 supports only lower sampling rates (16000, 22050 or 24000 samples per second) and offers choices of bitrate as low as 8 kbit/s but no higher than 160 kbit/s. By lowering the sampling rate, MPEG-2 layer III removes all frequencies above half the new sampling rate that may have been present in the source audio.\n\nAs shown in these two tables, 14 selected bit rates are allowed in MPEG-1 Audio Layer III standard: 32, 40, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256 and 320 kbit/s, along with the 3 highest available sampling frequencies of 32, 44.1 and 48 kHz. MPEG-2 Audio Layer III also allows 14 somewhat different (and mostly lower) bit rates of 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160 kbit/s with sampling frequencies of 16, 22.05 and 24 kHz which are exactly half that of MPEG-1 MPEG-2.5 Audio Layer III frames are limited to only 8 bit rates of 8, 16, 24, 32, 40, 48, 56 and 64 kbit/s with 3 even lower sampling frequencies of 8, 11.025, and 12 kHz.\n\nMPEG-1 frames contain the most detail in 320 kbit/s mode with silence and simple tones still requiring 32 kbit/s. MPEG-2 frames can capture up to 12 kHz sound reproductions needed up to 160 kbit/s. MP3 files made with MPEG-2 don't have 20 kHz bandwidth because of the Nyquist–Shannon sampling theorem. Frequency reproduction is always strictly less than half of the sampling frequency, and imperfect filters require a larger margin for error (noise level versus sharpness of filter), so an 8 kHz sampling rate limits the maximum frequency to 4 kHz, while a 48 kHz sampling rate limits an MP3 to a maximum 24 kHz sound reproduction. MPEG-2 uses half and MPEG-2.5 only a quarter of MPEG-1 sample rates.\n\nFor the general field of human speech reproduction, a bandwidth of 5512 Hz is sufficient to produce excellent results (for voice) using the sampling rate of 11025 and VBR encoding from 44100 (standard) wave files.. This is easily accomplished using LAME version 3.99.5 and the command line \"lame -V 9.6 lecture.WAV\" English speakers average 41–42 kbit/s with -V 9.6 setting but this may vary with amount of silence recorded or the rate of delivery (wpm). Resampling to 12000 (6K bandwidth) is selected by the LAME parameter -V 9.4 Likewise -V 9.2 selects 16000 sample rate and a resultant 8K lowpass filtering. For more info see Nyquist – Shannon. Older versions of LAME and FFmpeg only support integer arguments for variable bit rate quality selection parameter. The n.nnn quality parameter (-V) is documented at lame.sourceforge.net but is only supported in LAME with the new style VBR variable bit rate quality selector—not average bit rate (ABR).\n\nA sample rate of 44.1 kHz is commonly used for music reproduction, because this is also used for CD audio, the main source used for creating MP3 files. A great variety of bit rates are used on the Internet. A bit rate of 128 kbit/s is commonly used, at a compression ratio of 11:1, offering adequate audio quality in a relatively small space. As Internet bandwidth availability and hard drive sizes have increased, higher bit rates up to 320 kbit/s are widespread. Uncompressed audio as stored on an audio-CD has a bit rate of 1,411.2 kbit/s, (16 bit/sample × 44100 samples/second × 2 channels / 1000 bits/kilobit), so the bitrates 128, 160 and 192 kbit/s represent compression ratios of approximately 11:1, 9:1 and 7:1 respectively.\n\nNon-standard bit rates up to 640 kbit/s can be achieved with the LAME encoder and the freeformat option, although few MP3 players can play those files. According to the ISO standard, decoders are only required to be able to decode streams up to 320 kbit/s. Early MPEG Layer III encoders used what is now called Constant Bit Rate (CBR). The software was only able to use a uniform bitrate on all frames in an MP3 file. Later more sophisticated MP3 encoders were able to use the bit reservoir to target an average bit rate selecting the encoding rate for each frame based on the complexity of the sound in that portion of the recording.\n\nA more sophisticated MP3 encoder can produce variable bitrate audio. MPEG audio may use bitrate switching on a per-frame basis, but only layer III decoders must support it. VBR is used when the goal is to achieve a fixed level of quality. The final file size of a VBR encoding is less predictable than with constant bitrate. Average bitrate is a type of VBR implemented as a compromise between the two: the bitrate is allowed to vary for more consistent quality, but is controlled to remain near an average value chosen by the user, for predictable file sizes. Although an MP3 decoder must support VBR to be standards compliant, historically some decoders have bugs with VBR decoding, particularly before VBR encoders became widespread. The most evolved LAME MP3 encoder supports the generation of VBR, ABR, and even the ancient CBR MP3 formats.\n\nLayer III audio can also use a \"bit reservoir\", a partially full frame's ability to hold part of the next frame's audio data, allowing temporary changes in effective bitrate, even in a constant bitrate stream. Internal handling of the bit reservoir increases encoding delay. There is no scale factor band 21 (sfb21) for frequencies above approx 16 kHz, forcing the encoder to choose between less accurate representation in band 21 or less efficient storage in all bands below band 21, the latter resulting in wasted bitrate in VBR encoding.\n\nThe ancillary data field can be used to store user defined data. The ancillary data is optional and the number of bits available is not explicitly given. The ancillary data is located after the Huffman code bits and ranges to where the next frame's main_data_begin points to. mp3PRO uses ancillary data to encode their bits to improve audio quality.\n\nA \"tag\" in an audio file is a section of the file that contains metadata such as the title, artist, album, track number or other information about the file's contents. The MP3 standards do not define tag formats for MP3 files, nor is there a standard container format that would support metadata and obviate the need for tags. However, several \"de facto\" standards for tag formats exist. As of 2010, the most widespread are ID3v1 and ID3v2, and the more recently introduced APEv2. These tags are normally embedded at the beginning or end of MP3 files, separate from the actual MP3 frame data. MP3 decoders either extract information from the tags, or just treat them as ignorable, non-MP3 junk data.\n\nPlaying & editing software often contains tag editing functionality, but there are also tag editor applications dedicated to the purpose. Aside from metadata pertaining to the audio content, tags may also be used for DRM. ReplayGain is a standard for measuring and storing the loudness of an MP3 file (audio normalization) in its metadata tag, enabling a ReplayGain-compliant player to automatically adjust the overall playback volume for each file. MP3Gain may be used to reversibly modify files based on ReplayGain measurements so that adjusted playback can be achieved on players without ReplayGain capability.\n\nThe basic MP3 decoding and encoding technology is patent-free in the European Union, all patents having expired there by 2012 at the latest. In the United States, the technology became substantially patent-free on 16 April 2017 (see below). MP3 patents expired in the US between 2007 and 2017. In the past, many organizations have claimed ownership of patents related to MP3 decoding or encoding. These claims led to a number of legal threats and actions from a variety of sources. As a result, uncertainty about which patents must be licensed in order to create MP3 products without committing patent infringement in countries that allow software patents was a common feature of the early stages of adoption of the technology.\n\nThe initial near-complete MPEG-1 standard (parts 1, 2 and 3) was publicly available on 6 December 1991 as ISO CD 11172. In most countries, patents cannot be filed after prior art has been made public, and patents expire 20 years after the initial filing date, which can be up to 12 months later for filings in other countries. As a result, patents required to implement MP3 expired in most countries by December 2012, 21 years after the publication of ISO CD 11172.\n\nAn exception is the United States, where patents in force but filed prior to 8 June 1995 expire after the later of 17 years from the issue date or 20 years from the priority date. A lengthy patent prosecution process may result in a patent issuing much later than normally expected (see submarine patents). The various MP3-related patents expired on dates ranging from 2007 to 2017 in the United States. Patents for anything disclosed in ISO CD 11172 filed a year or more after its publication are questionable. If only the known MP3 patents filed by December 1992 are considered, then MP3 decoding has been patent-free in the US since 22 September 2015, when , which had a PCT filing in October 1992, expired. If the longest-running patent mentioned in the aforementioned references is taken as a measure, then the MP3 technology became patent-free in the United States on 16 April 2017, when , held and administered by Technicolor, expired. As a result, many free and open-source software projects, such as the Fedora operating system, have decided to start shipping MP3 support by default, and users will no longer have to resort to installing unofficial packages maintained by third party software repositories for MP3 playback or encoding.\n\nTechnicolor (formerly called Thomson Consumer Electronics) claimed to control MP3 licensing of the Layer 3 patents in many countries, including the United States, Japan, Canada and EU countries. Technicolor had been actively enforcing these patents. MP3 license revenues from Technicolor's administration generated about €100 million for the Fraunhofer Society in 2005. In September 1998, the Fraunhofer Institute sent a letter to several developers of MP3 software stating that a license was required to \"distribute and/or sell decoders and/or encoders\". The letter claimed that unlicensed products \"infringe the patent rights of Fraunhofer and Thomson. To make, sell or distribute products using the [MPEG Layer-3] standard and thus our patents, you need to obtain a license under these patents from us.\" This led to the situation where the LAME MP3 encoder project could not offer its users official binaries that could run on their computer. The project's position was that as source code, LAME was simply a description of how an MP3 encoder \"could\" be implemented. Unofficially, compiled binaries were available from other sources.\n\nSisvel S.p.A. and its United States subsidiary Audio MPEG, Inc. previously sued Thomson for patent infringement on MP3 technology, but those disputes were resolved in November 2005 with Sisvel granting Thomson a license to their patents. Motorola followed soon after, and signed with Sisvel to license MP3-related patents in December 2005. Except for three patents, the US patents administered by Sisvel had all expired in 2015. The three exceptions are: , expired February 2017; , expired February 2017; and , expired 9 April 2017.\n\nIn September 2006, German officials seized MP3 players from SanDisk's booth at the IFA show in Berlin after an Italian patents firm won an injunction on behalf of Sisvel against SanDisk in a dispute over licensing rights. The injunction was later reversed by a Berlin judge, but that reversal was in turn blocked the same day by another judge from the same court, \"bringing the Patent Wild West to Germany\" in the words of one commentator. In February 2007, Texas MP3 Technologies sued Apple, Samsung Electronics and Sandisk in eastern Texas federal court, claiming infringement of a portable MP3 player patent that Texas MP3 said it had been assigned. Apple, Samsung, and Sandisk all settled the claims against them in January 2009.\n\nAlcatel-Lucent has asserted several MP3 coding and compression patents, allegedly inherited from AT&T-Bell Labs, in litigation of its own. In November 2006, before the companies' merger, Alcatel sued Microsoft for allegedly infringing seven patents. On 23 February 2007, a San Diego jury awarded Alcatel-Lucent US $1.52 billion in damages for infringement of two of them. The court subsequently revoked the award, however, finding that one patent had not been infringed and that the other was not owned by Alcatel-Lucent; it was co-owned by AT&T and Fraunhofer, who had licensed it to Microsoft, the judge ruled. That defense judgment was upheld on appeal in 2008. See Alcatel-Lucent v. Microsoft for more information.\n\nOther lossy formats exist. Among these, mp3PRO, AAC, and MP2 are all members of the same technological family as MP3 and depend on roughly similar psychoacoustic models. The Fraunhofer Society owns many of the basic patents underlying these formats as well, with others held by Alcatel-Lucent, and Thomson Consumer Electronics. There are also open compression formats like Opus and Vorbis that are available free of charge and without any known patent restrictions. Some of the newer audio compression formats, such as AAC, WMA Pro and Vorbis, are free of some limitations inherent to the MP3 format that cannot be overcome by any MP3 encoder.\n\nBesides lossy compression methods, lossless formats are a significant alternative to MP3 because they provide unaltered audio content, though with an increased file size compared to lossy compression. Lossless formats include FLAC (Free Lossless Audio Codec), Apple Lossless and many others.\n\n\n"}
{"id": "19674", "url": "https://en.wikipedia.org/wiki?curid=19674", "title": "May 15", "text": "May 15\n\n\n\n"}
{"id": "19675", "url": "https://en.wikipedia.org/wiki?curid=19675", "title": "May 13", "text": "May 13\n\n\n\n"}
{"id": "19676", "url": "https://en.wikipedia.org/wiki?curid=19676", "title": "May 14", "text": "May 14\n\n\n\n"}
{"id": "19677", "url": "https://en.wikipedia.org/wiki?curid=19677", "title": "May 20", "text": "May 20\n\n\n\n"}
