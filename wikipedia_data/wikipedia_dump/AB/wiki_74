{"id": "21149", "url": "https://en.wikipedia.org/wiki?curid=21149", "title": "N.W.A", "text": "N.W.A\n\nN.W.A (an abbreviation for Niggaz Wit Attitudes) was an American hip hop group from Los Angeles, California. They were among the earliest and most significant popularizers and controversial figures of the gangsta rap subgenre, and are widely considered one of the greatest and most influential groups in the history of hip hop music.\n\nActive from 1986 to 1991, the rap group endured controversy owing to their music's explicit lyrics, which many viewed as being disrespectful to women, as well as to its glorification of drugs and crime. The group was subsequently banned from many mainstream American radio stations. In spite of this, the group has sold over 10 million units in the United States alone. Drawing on their own experiences of racism and excessive policing, the group made inherently political music. They were known for their deep hatred of the police system, which sparked much controversy over the years.\n\nThe original lineup, formed in 1986, consisted of Arabian Prince, Dr. Dre, Eazy-E, and Ice Cube. DJ Yella and MC Ren joined later in 1987. They released their first compilation album as a group in 1987 called N.W.A. and the Posse which peaked at #39 on Billboard magazine's Top R&B/Hip-Hop Albums chart. Arabian Prince eventually left shortly after the release of their debut studio album, \"Straight Outta Compton\", in 1988 and Ice Cube following suit in December 1989. Eazy-E, Ice Cube, MC Ren and Dr. Dre would later become platinum-selling solo artists in the 1990s. Their debut album marked the beginning of the new gangsta rap era as the production and social commentary in their lyrics were revolutionary within the genre. N.W.A's second studio album, \"Niggaz4Life\", was the first hardcore rap album to debut at number one on the \"Billboard\" 200 sales charts.\n\n\"Rolling Stone\" ranked N.W.A number 83 on their list of the \"100 Greatest Artists of All Time\". In 2016, the group was inducted into the Rock and Roll Hall of Fame, following three previous nominations.\n\nN.W.A was assembled by Compton-based Eazy-E, who co-founded Ruthless Records with Jerry Heller. Eazy-E sought an introduction to Steve Yano. Although initially rebuffed, Yano was impressed by Eazy-E's persistence and arranged a meeting with Dr. Dre. Initially, N.W.A consisted of Eazy-E and Dr. Dre. Together with fellow producer Arabian Prince, Ice Cube was added to the roster after he had started out as a rapper for the group C.I.A. Dre would later bring DJ Yella on board as well. Dre and Yella were both formerly members of the World Class Wreckin' Cru as DJs and producers. Ruthless released the single \"Panic Zone\" in 1987 with Macola Records, which was later included on the compilation album \"N.W.A. and the Posse\". N.W.A was still in its developing stages, and is only credited on three of the eleven tracks, notably the uncharacteristic record \"Panic Zone\", \"8-Ball\", and \"Dopeman\", which marked the first collaboration of Arabian Prince, DJ Yella, Dr. Dre, and Ice Cube. Mexican rapper Krazy-Dee co-wrote \"Panic Zone\", which was originally called \"Hispanic Zone\", but the title was later changed when Dr. Dre advised Krazy-Dee that the word \"hispanic\" would hinder sales. Also included was Eazy-E's solo track \"Boyz-n-the-Hood\".\n\nN.W.A released their debut studio album, \"Straight Outta Compton\", in 1988. With its famous opening salvo of three tracks, the group reflected the rising anger of the urban youth. The opening song \"Straight Outta Compton\" introduced the group, \"Fuck tha Police\" protested police brutality and racial profiling, and \"Gangsta Gangsta\" painted the worldview of the inner-city youth. While the group was later credited with pioneering the burgeoning subgenre of gangsta rap, N.W.A referred to their music as \"reality rap\". Twenty-seven years later, member and co-producer of the \"Straight Outta Compton\" film, Ice Cube, commented \"they were talking about what really led into the style that we ended up doing, which is now called hardcore gangster rap.\" Dr. Dre and DJ Yella, as HighPowered Productions, composed the beats for each song, with Dre making occasional rapping appearances. The D.O.C., Ice Cube, and MC Ren wrote most of the group's lyrics, including \"Fuck tha Police\", perhaps the group's most notorious song, which brought them into conflict with various law enforcement agencies. Under pressure from Focus on the Family, Milt Ahlerich, an assistant director of the FBI, sent a letter to Ruthless and its distributing company Priority Records, advising the rappers that \"advocating violence and assault is wrong and we in the law enforcement community take exception to such action.\" This letter can still be seen at the Rock and Roll Hall of Fame in Cleveland, Ohio. Policemen refused to provide security for the group's concerts, hurting their plans to tour. Nonetheless, the FBI's letter only served to draw more publicity to the group.\n\n\"Straight Outta Compton\" was also one of the first albums to adhere to the new Parental Advisory label scheme, then still in its early stages: the label at the time consisted of \"WARNING: Moderate impact coarse language and/or themes\" only. However, the taboo nature of N.W.A's music was the most important factor of its mass appeal. Media coverage compensated for N.W.A's lack of airplay and their album eventually went double platinum. One month after \"Straight Outta Compton\", Eazy-E's solo debut \"Eazy-Duz-It\" was released. The album was dominated by Eazy's persona (MC Ren was the only guest rapper) but behind the scenes it was a group effort. Music was handled by Dr. Dre and DJ Yella; the lyrics were largely written by MC Ren, with contributions from Ice Cube and The D.O.C. The album was another double platinum success for Ruthless (in addition to girl group J.J. Fad in 1988 and singer Michel'le in 1989). 1989 saw the re-issue of \"N.W.A and the Posse\" and \"Straight Outta Compton\" on CD, and the release of The D.O.C.'s \"No One Can Do It Better\". His album was essentially a collaboration with Dr. Dre and notably free of \"gangsta rap\" content, including the N.W.A posse cut \"The Grand Finalé\". It would become another #1 album for the record label.\n\nIce Cube left the group in December 1989 over royalty disputes; having written almost half of the lyrics on \"Straight Outta Compton\" himself, he felt he was not getting a fair share of the profits. A lawsuit brought by Ice Cube against band manager Jerry Heller was settled out of court. He wasted little time putting together his solo debut, 1990's \"AmeriKKKa's Most Wanted\", but he avoided mentioning his former label mates. N.W.A's title track from their 1990 EP \"100 Miles and Runnin'\", however, included a diss of Ice Cube:\n\"\"We started with five, but yo / One couldn't take it—So now it's four / Cuz the fifth couldn't make it.\"\" The video for the song depicted the remaining members of N.W.A together in a jail cell, while an Ice Cube look-alike is released. Also heard on the EP (which found its way on the \"Efil4zaggin\" CD re-issue) was \"Real Niggaz\", a full-blown diss on Ice Cube where the remaining members accuse him of cowardice, and question his authenticity, longevity and originality: \"\"How the fuck you think a rapper lasts / With your ass sayin' shit that was said in the past / Yo, be original, your shit is sloppy / Get off the dick, you motherfuckin' carbon-copy\"\", and \"\"We started out with too much cargo / So I'm glad we got rid of Benedict Arnold, yo.\"\" The song \"100 Miles and Runnin'\" was Dr. Dre's final uptempo recording, which had been a common feature of late 1980s hip hop. After this, he focused on a midtempo, synthesizer based sound which would become known as G-funk, starting with \"Alwayz Into Somethin'\" from \"Efil4zaggin\" in 1991. The G-funk style dominated both the West and East Coast hip hop music scene for several years to come. N.W.A is referenced on Ice Cube's 1990 EP, \"Kill at Will\", where he name-checks his former group (likely in a mocking manner) on the song \"Jackin' For Beats\". On \"I Gotta Say What Up!!!\", Ice Cube gives shout-outs to his rap peers at the time, among them Public Enemy, Geto Boys, and Sir Jinx. At the end of the track, in what appears to be an on-the-phone interview, Ice Cube is asked, \"Since you went solo, what's up with the rest of the crew?\" and the phone is abruptly hung up on the interviewer.\n\nThe group's second full-length release, 1991's \"Efil4zaggin\" (\"Niggaz4Life\" spelled backwards), re-established the band in the face of Ice Cube's continued solo success. The album is considered by many Dr. Dre's finest production work, and it heralded the beginning of the G-Funk era. It also showed a clear animosity towards their former member, and derogatory references to Ice Cube are found in several songs. The interlude \"A Message to B.A.\" echoes the beginning of his song \"Turn Off the Radio\" from \"AmeriKKKa's Most Wanted\": Ice Cube is first addressed by the name Benedict Arnold (after the infamous traitor of the American Revolution) but then named outright in a torrent of abuse from both the group and its fans: \"\"When we see yo' ass, we gon' cut yo' hair off and fuck you with a broomstick\"\" spoken by MC Ren. The N.W.A–Ice Cube feud eventually escalated, both on record and in real life. \"AmeriKKKa's Most Wanted\" had avoided direct attacks on N.W.A, but on \"Death Certificate\", Ice Cube's second full-length release, he retaliated. He sampled and mocked the \"Message to B.A.\" skit before embarking on a full-blown tirade, the infamous \"No Vaseline\". In a series of verses, Ice Cube verbally assaulted the group: \"\"You lookin' like straight bozos / I saw it comin' that's why I went solo / Kept on stompin' / When y'all Muthafuckas moved Straight outta Compton / You got jealous when I got my own company / But I'm a man, and ain't nobody helpin' me.\"\" He also responded to members MC Ren, Dr. Dre, and Eazy-E individually to \"100 Miles and Runnin'\", claiming \"\"I started off with too much cargo / Dropped four niggaz and now I'm makin' all the dough\"\", using homophobic metaphors to describe their unequal business relationship with Jerry Heller, who became the target of harsh insults:\n\"\"Get rid of that devil real simple / Put a bullet in his temple / Cuz you can't be the 'Niggaz 4 Life' crew / With a white Jew tellin' you what to do.\"\" The song attracted controversy for its antisemitism (the beginning of such accusations against Ice Cube during his affiliation with the Nation of Islam), based on the bashing of Heller's religion. The track was omitted from the UK release, and later pressings included a censored version of the song. In September 1990, members of hip hop act Above the Law clashed with Ice Cube and his posse Da Lench Mob during the annual New Music Seminar conference, forcing the latter to flee the premises of Times Square's Marriott Marquis, the venue of the event. On January 27, 1991, Dr. Dre assaulted Dee Barnes, host of the hip hop show \"Pump It Up\", after its coverage of the N.W.A/Ice Cube beef. According to \"Rolling Stone\" reporter Alan Light:\nIn response, Dre commented: \"People talk all this shit, but you know, if somebody fucks with me, I'm gonna fuck with them. I just did it, you know. Ain't nothing you can do now by talking about it. Besides, it ain't no big thing—I just threw her through a door.\"\n\n1991's \"Niggaz4Life\" would be the group's final album. After Dr. Dre, The D.O.C. and Michel'le departed from Ruthless to join Death Row Records and allegations over Eazy-E being coerced into signing away their contracts (while however retaining a portion of their publishing rights), a bitter rivalry ensued. Dr. Dre began the exchange with Death Row's first release, 1992's \"Fuck Wit Dre Day (And Everybody's Celebratin')\", and its accompanying video featured a character named \"Sleazy-E\" who ran around desperately trying to get money. The insults continued on \"The Chronic\" with \"Bitches Ain't Shit\". Eazy-E responded in 1993 with the EP \"It's On (Dr. Dre) 187um Killa\" on the tracks \"Real Muthaphuckkin G's\" and \"It's On\". Eazy-E accused Dr. Dre of being a homosexual, calling him a \"she thang\", and criticizing Dre's new image by calling him and Snoop \"studio gangsters\". The music video for \"Real Muthaphuckkin G's\" showed a still of Dre wearing make-up and a sequined jumpsuit. The photos dated back to Dr. Dre's World Class Wreckin' Cru days, when such fashion was common among West Coast electro hop artists, prior to N.W.A's popularization of gangsta rap. Eazy-E kept dissing Dre and Death Row on most of his songs until his AIDS-related death on March 26, 1995.\n\nEven Eazy-E's longtime friend MC Ren voiced his dislike for Eazy-E in 1994, calling Eazy-E a \"big-head\" and \"wannabe mega-star\", and even suggesting that N.W.A should reunite without Eazy-E. MC Ren later said that the only relationship he had with Eazy-E was through Ruthless Records, where he released several gold and platinum selling albums, including \"Kizz My Black Azz\" and \"Shock of the Hour\". Eazy-E and MC Ren would squash their beef shortly before Eazy-E's death in their 1995 duet '\"Tha Muthaphukkin' Real\" after two years of not talking to each other. All bad blood finally ceased within the rest of the group. Dr. Dre, MC Ren and Ice Cube would later express their re-evaluated feelings to their old friend on 1998's \"Ruthless for Life\", 1999's \"What's the Difference\" and \"Chin Check\", 2000's \"Hello\", 2006's \"Growin' Up\", and in the 2011 music video \"I Need a Doctor\".\n\nHaving both parted with Ruthless Records on bad terms, tensions between Ice Cube and Dr. Dre eventually eased on their own. After Ice Cube made a cameo appearance in Dr. Dre's \"Let Me Ride\" video in 1993, the two recorded the hit song \"Natural Born Killaz\" for Snoop Doggy Dogg's 1994 short film and soundtrack \"Murder Was the Case\". Ice Cube also later appeared on MC Ren's album \"Ruthless for Life\" on the track \"Comin' After You\". MC Ren appeared on Dre's 1999 album \"2001\", and the three remaining N.W.A emcees would reunite for \"Hello\" on Ice Cube's 2000 album \"War & Peace Vol. 2 (The Peace Disc)\", and the song \"Chin Check\" for the \"Next Friday\" soundtrack, a movie starring Ice Cube.\n\nThe West Coast and \"gangsta\" music scene had however fallen out of the spotlight since the death of Tupac Shakur in 1996, and it was only after Dr. Dre's successful patronage of Eminem and Dre's ensuing comeback album \"2001\" that the genre and its artists would regain the national spotlight. 2000's all-star Up In Smoke Tour would reunite much of the N.W.A and Death Row families, and during time spent on the road, Dre, Ice Cube, MC Ren, guest star Snoop Dogg and Eminem began recording in a mobile studio. A comeback album entitled \"Not These Niggaz Again\" was planned (and would include DJ Yella, who had not been present on the tour).\n\nHowever, due to busy and conflicting schedules as well as the obstacles of coordinating three different record labels (Priority, No Limit and Interscope), obtaining the rights to the name N.W.A and endorsing the whole project to gain exclusive rights, the album never materialized. Only two tracks from these sessions would be released: the aforementioned \"Chin Check\" (with Snoop Dogg as a member of N.W.A) from 2000's \"Next Friday\" soundtrack and \"Hello\" from Ice Cube's 2000 album \"War & Peace Vol. 2 (The Peace Disc)\". Both songs would also appear on N.W.A's remastered \"Greatest Hits\". There would also be partial reunions on other projects, notably \"Set It Off\", from Snoop Dogg's \"Tha Last Meal\" (2000), which featured MC Ren and Ice Cube, and The D.O.C.'s \"The Shit\", from his 2003 album \"Deuce\", featuring MC Ren, Ice Cube, Snoop Dogg and Six-Two. Dr. Dre and DJ Yella were present in the studio for the latter song.\n\nIn addition to the \"Greatest Hits\" initially released by Priority in 1996, Capitol and Ruthless Records jointly released \"\" in 1999, a compilation that contained songs by other rap artists and only three songs from the actual group but various solo tracks from the five members. The success of the album prompted a second volume, \"The N.W.A Legacy, Vol. 2\", three years later. It emulated the format of its predecessor, containing only three genuine N.W.A tracks and many solo efforts by the crew members. In 2007, a new greatest hits package was released, entitled \"\".\n\nIn 2014, Ice Cube appeared on MC Ren's remix for \"Rebel Music\". This was the first time the duo had worked together since the N.W.A reunion in 2000.\n\nOn June 27, 2015, MC Ren and DJ Yella joined Ice Cube during his solo set as part of the BET Experience show at the Staples Center in Los Angeles, California. This marked the first reunion performance of the group (minus Dr. Dre) in 15 years. Following a 27-year hiatus, the group reunited with surviving members Ice Cube, MC Ren, Dr. Dre and DJ Yella taking the stage during the second weekend of the Coachella Valley Music and Arts Festival in April 2016, just days following the group's Rock N' Roll Hall of Fame induction.\n\n\nNew Line Cinema representatives announced to \"Entertainment Weekly's\" \"Hollywood Insider Blog\" that N.W.A's story was in development to become a feature film for theatrical release in 2012. However, it was delayed to sometime in 2014. The script was researched and written by filmmaker S. Leigh Savidge and radio veteran Alan Wenkus, who worked closely with Eazy-E's widow, Tomica Woods-Wright. Ice Cube and Dr. Dre act as producers of the film. In September 2011, John Singleton was selected as director. Ice Cube and Singleton previously collaborated on \"Boyz n the Hood\", a movie that was nominated for an Academy Award, and Ice Cube also played the part of the character \"Fudge\" in Singleton's \"Higher Learning\". Casting calls began in the summer of 2010. There were rumors of Lil Eazy-E playing his late father Eazy-E, and Ice Cube's son and fellow rapper O'Shea Jackson Jr. playing his father as well. Ice Cube stated of the movie, \"We're taking it to the nooks and crannies, I think deeper than any other article or documentary on the group,\" he said. \"These are the intimate conversations that helped forge N.W.A. To me, I think it's interesting to anybody who loves that era and I don't know any other movie where you can mix Gangster Rap, the F.B.I., L.A. riots, HIV, and fucking feuding with each other. This movie has everything from Darryl Gates and the battering ram.\"\n\nIn August 2012, F. Gary Gray was selected as director rather than Singleton. The film, named \"Straight Outta Compton\", had been picked up by Universal Pictures who hired Jonathan Herman in December 2013 to draft a new script and brought in Will Packer to executive produce. On February 21, 2014, director F. Gary Gray announced a March 9, 2014 open casting call for the film via his Twitter account. There were also open casting calls in Atlanta and Chicago. Rapper YG auditioned to play MC Ren in the film. The project was scheduled to start filming in April 2014 but was pushed backed due to casting delays.\n\nOn June 18, 2014, Universal officially announced that the N.W.A biopic \"Straight Outta Compton\" would be released August 14, 2015. It was also confirmed that Ice Cube's son, O'Shea Jackson Jr., would play a younger version of his father in the movie. O'Shea Jr. joined Jason Mitchell and Corey Hawkins who will portray group members Eazy-E and Dr. Dre, respectively, in the film. To round out the cast of N.W.A, Aldis Hodge plays MC Ren and Neil Brown Jr. portrays DJ Yella. In early July 2014, casting directors for the N.W.A biopic issued a casting call for extras and vintage cars in the Los Angeles area for scenes in the movie. According to the casting call release, the film began filming in August 2014 and was released a year later on August 14, 2015. The film received positive reviews and grossed over $200 million worldwide.\n\nAlthough the group disbanded in 1991, they remain one of the greatest and most influential hip-hop groups, leaving a lasting legacy on hip hop music in the following decades. Their influence, from their funky, bass-driven beats to their exaggerated lyrics, was evident throughout the 1990s and even into the present, and is often credited as bridging the white/black American musical lines with their appeal to white Americans in the late 1980s. In Dr. Dre's 1999 single \"Forgot About Dre\", Eminem pays homage to the group, rapping \"So what do you say to somebody you hate or anyone tryna bring trouble your way, Wanna resolve things in a bloodier way, Then just study a tape of N.W.A\" referring to the negative reception of N.W.A's works by the mainstream radio, which considered their songs to be violent. A scene in the music video for the 2005 single \"Hate It or Love It\" by The Game featuring 50 Cent shows Tequan Richmond and Zachary Williams (portraying a youthful Game & 50 Cent respectively) being caught spraypainting \"N.W.A\" on a wall, resulting in their subsequent arrest by two policemen. The Game also has a tattoo that says \"N.W.A\" on the right side of his chest.\n\n\n\n"}
{"id": "21150", "url": "https://en.wikipedia.org/wiki?curid=21150", "title": "Nibble", "text": "Nibble\n\nIn computing, a nibble (occasionally nybble or nyble to match the spelling of byte) is a four-bit aggregation, or half an octet. It is also known as half-byte or tetrade.\nIn a networking or telecommunication context, the nibble is often called a semi-octet, quadbit, or quartet.\nA nibble has sixteen (2) possible values. A nibble can be represented by a single hexadecimal digit and called a hex digit.\n\nA full byte (octet) is represented by two hexadecimal digits; therefore, it is common to display a byte of information as two nibbles. Sometimes the set of all 256 byte values is represented as a 16×16 table, which gives easily readable hexadecimal codes for each value.\n\nFour-bit computer architectures use groups of four bits as their fundamental unit. Such architectures were used in early microprocessors, pocket calculators and pocket computers. They continue to be used in some microcontrollers.\n\nThe term 'nibble' originates from its representing 'half a byte', with 'byte' a homophone of the English word 'bite'.\nIn 2014, David B. Benson, a professor emeritus at Washington State University, remembered that he playfully used (and may have possibly coined) the term nibble as \"half a byte\" and unit of storage required to hold a binary-coded decimal (BCD) decimal digit around 1958, when talking to a programmer of Los Alamos Scientific Laboratory.\nThe alternative spelling 'nybble' reflects the spelling of 'byte', as noted in editorials of \"Kilobaud\" and \"Byte\" in the early 1980s.\nAnother early recorded use of the term 'nybble' was in 1977 within the consumer-banking technology group at Citibank. It created a pre-ISO 8583 standard for transactional messages between cash machines and Citibank's data centers that used the basic informational unit 'NABBLE'.\n\nThe nibble is used to describe the amount of memory used to store a digit of a number stored in packed decimal format (BCD) within an IBM mainframe. This technique is used to make computations faster and debugging easier. An 8-bit byte is split in half and each nibble is used to store one decimal digit. The last (rightmost) nibble of the variable is reserved for the sign. Thus a variable which can store up to nine digits would be \"packed\" into 5 bytes. Ease of debugging resulted from the numbers being readable in a hex dump where two hex numbers are used to represent the value of a byte, as 16×16 = 2. For example, a five-byte BCD value of      represents a decimal value of .\n\nHistorically, there are cases where nybble was used for a group of bits greater than 4. In the Apple II microcomputer line, much of the disk drive control and group-coded recording was implemented in software. Writing data to a disk was done by converting 256-byte pages into sets of 5-bit (later, 6-bit) nibbles and loading disk data required the reverse. Moreover, 1982 documentation for the Integrated Woz Machine refers consistently to an \"8 bit nibble\".\nThe term \"byte\" once had the same ambiguity and meant a set of bits but not necessarily 8, hence the distinction of \"bytes\" and \"octets\" or of \"nibbles\" and \"quartets\" (or \"quadbits\"). Today, the terms 'byte' and 'nibble' almost always refer to 8-bit and 4-bit collections respectively and are very rarely used to express any other sizes.\n\nThe sixteen nibbles and their equivalents in other numeral systems:\n\nThe terms \"low nibble\" and \"high nibble\" are used to denote the nibbles containing, respectively, the less significant bits and the more significant bits within a byte. In graphical representations of bits within a byte, the leftmost bit could represent the most significant bit (MSB), corresponding to ordinary decimal notation in which the digit at the left of a number is the most significant. In such illustrations the four bits on the left end of the byte form the high nibble, and the remaining four bits form the low nibble. For example,\n\nninety-seven = 97 = (0110 0001)\n\nthe high nibble is 0110 (6), and the low nibble is 0001 (1). The total value is high-nibble × 16 + low-nibble (6×16+1=97).\n\nIn the C programming language:\n\nwhere codice_1 must be a variable or constant of an integral data type, and only the least-significant byte of codice_1 is used.\n\nFor example, codice_3 and codice_4.\n\nIn Common Lisp:\n\n"}
{"id": "21151", "url": "https://en.wikipedia.org/wiki?curid=21151", "title": "New wave music", "text": "New wave music\n\nNew wave is a genre of rock music popular in the late 1970s and the 1980s with ties to mid-1970s punk rock. New wave moved away from blues and rock and roll sounds to create rock music (early new wave) or pop music (later) that incorporated disco, mod, and electronic music. Initially new wave was similar to punk rock, before becoming a distinct genre. It subsequently engendered subgenres and fusions, including synth-pop.\n\nNew wave differs from other movements with ties to first-wave punk as it displays characteristics common to pop music, rather than the more \"artsy\" post-punk. Although it incorporates much of the original punk rock sound and ethos, new wave exhibits greater complexity in both music and lyrics. Common characteristics of new wave music include the use of synthesizers and electronic productions, and a distinctive visual style featured in music videos and fashion.\n\nNew wave has been called one of the definitive genres of the 1980s, after it was promoted heavily by MTV (the Buggles' \"Video Killed the Radio Star\" music video was broadcast as the first music video to promote the channel's launch). The popularity of several new wave artists is often attributed to their exposure on the channel. In the mid-1980s, differences between new wave and other music genres began to blur. New wave has enjoyed resurgences since the 1990s, after a rising \"nostalgia\" for several new wave-influenced artists. Subsequently, the genre influenced other genres. During the 2000s, a number of acts, such as the Strokes, Interpol, Franz Ferdinand and The Killers explored new wave and post-punk influences. These acts were sometimes labeled \"new wave of new wave\".\n\nThe catch-all nature of new wave music has been a source of much confusion and controversy. The 1985 discography \"Who's New Wave in Music\" listed artists in over 130 separate categories. The \"New Rolling Stone Encyclopedia of Rock\" calls the term \"virtually meaningless\", while AllMusic mentions \"stylistic diversity\".\n\nNew wave first emerged as a rock genre in the early 1970s, used by critics including Nick Kent and Dave Marsh to classify such New York-based groups as the Velvet Underground and New York Dolls. It gained currency beginning in 1976 when it appeared in UK punk fanzines such as \"Sniffin' Glue\" and newsagent music weeklies such as \"Melody Maker\" and \"New Musical Express\". In November 1976 Caroline Coon used Malcolm McLaren's term \"new wave\" to designate music by bands not exactly punk, but related to the same musical scene. The term was also used in that sense by music journalist Charles Shaar Murray in his comments about the Boomtown Rats. For a period of time in 1976 and 1977, the terms new wave and punk were somewhat interchangeable. By the end of 1977, \"new wave\" had replaced \"punk\" as the definition for new underground music in the UK.\nIn the United States, Sire Records chairman Seymour Stein, believing that the term \"punk\" would mean poor sales for Sire's acts who had frequently played the club CBGB, launched a \"Don't Call It Punk\" campaign designed to replace the term with \"new wave\". As radio consultants in the United States had advised their clients that punk rock was a fad, they settled on the term \"new wave\". Like the filmmakers of the French new wave movement (after whom the genre was named), its new artists were anti-corporate and experimental (e.g. Ramones and Talking Heads). At first, most U.S. writers exclusively used the term \"new wave\" for British punk acts. Starting in December 1976, \"The New York Rocker\", which was suspicious of the term \"punk\", became the first American journal to enthusiastically use the term starting with British acts, later appropriating it to acts associated with the CBGB scene. Part of what attracted Stein and others to new wave was the music's stripped back style and upbeat tempos, which they viewed as a much needed return to the energetic rush of rock and roll and 1960s rock that had dwindled in the 1970s with the ascendance of overblown progressive rock and stadium spectacles.\n\nMusic historian Vernon Joynson claimed that new wave emerged in the UK in late 1976, when many bands began disassociating themselves from punk. Music that followed the anarchic garage band ethos of the Sex Pistols was distinguished as \"punk\", while music that tended toward experimentation, lyrical complexity or more polished production, came to be categorized as \"new wave\". In the U.S., the first new wavers were the not-so-punk acts associated with the New York club CBGB (e.g. Talking Heads, Mink DeVille and Blondie).\n\nCBGB owner Hilly Kristal, referring to the first show of the band Television at his club in March 1974, said, \"I think of that as the beginning of new wave.\" Furthermore, many artists who would have originally been classified as punk were also termed new wave. A 1977 Phonogram Records compilation album of the same name (\"New Wave\") features US artists including the Dead Boys, Ramones, Talking Heads and the Runaways.\n\nNew wave is much more closely tied to punk and came and went more quickly in the United Kingdom than in the United States. At the time punk began, it was a major phenomenon in the United Kingdom and a minor one in the United States. Thus when new wave acts started getting noticed in America, punk meant little to the mainstream audience and it was common for rock clubs and discos to play British dance mixes and videos between live sets by American guitar acts.\n\nPost-punk music developments in the UK became mainstream and were considered unique cultural events. By the early 1980s, British journalists largely had abandoned the term \"new wave\" in favor of subgenre terms such as \"synthpop\". By 1983, the term of choice for the US music industry had become \"new music\", while to the majority of US fans it was still a \"new wave\" reacting to album-based rock.\n\nNew wave died out in the mid-1980s, knocked out by guitar-driven rock reacting against new wave.\n\nIn the 21st century United States, \"new wave\" was used to describe artists such as Morrissey, Duran Duran, Cyndi Lauper and Devo. Late 1970s new wave acts such as the Pretenders and the Cars were more likely to be found on classic rock playlists than on new wave playlists there. Reflecting its British origins, the 2004 study \"Popular Music Genres: An Introduction\" had one paragraph dedicated to 1970s new wave artists in its punk chapter in contrast to a 20-page chapter on early 1980s synthpop.\n\nNew wave represented a break from the blues and rock & roll sounds of late 1960s to mid-1970s rock music. According to Simon Reynolds, the music had a twitchy, agitated feel to it. New wave musicians often played choppy rhythm guitars with fast tempos, and keyboards were common as were stop-start song structures and melodies. Reynolds noted that new wave vocalists sounded high-pitched, geeky and suburban. A nervous, nerdy persona was a common characteristic of new wave fans and acts such as Talking Heads, Devo and Elvis Costello. This took the forms of robotic dancing, jittery high-pitched vocals and clothing fashions such as suits and big glasses that hid the body.\nThis seemed radical to audiences accustomed to post-counterculture forms such as disco dancing and macho \"cock rock\" that emphasized a \"hang loose\" philosophy, open sexuality and sexual bravado. The majority of American male new wave acts of the late 1970s were from Caucasian middle-class backgrounds, and Theo Cateforis of Syracuse University theorized that these acts intentionally presented these exaggerated nerdy tendencies associated with their \"whiteness\" either to criticize it and/or to reflect their identity.\n\nThe British pub rock scene of the mid-1970s was the source of new wave acts such as Ian Dury, Nick Lowe, Eddie and the Hot Rods and Dr. Feelgood.\n\nSinger-songwriters who were \"angry\" and \"intelligent\" and who \"approached pop music with the sardonic attitude and tense, aggressive energy of punk\" such as Elvis Costello, Joe Jackson and Graham Parker were also part of the new wave music scene.\n\nA British revival of ska music on the 2 Tone label, led by the Specials, Madness, the English Beat, and Selecter were more politically oriented than other new wave genres.\n\nThe idea of rock music as a serious art form started in the late 1960s and was the dominant view of the genre at the time of new wave's arrival. New wave looked back or borrowed in various ways from the years just prior to this occurrence. One way this was done was by taking an ironic look at consumer and pop culture of the 1950s and early 1960s. The B-52's became most noted for a kitsch and camp presentation with their bouffant wigs, beach party and sci-fi movie references. Other groups that referenced the pre-progressive rock era were the Go-Go's, Blondie and Devo.\n\nIn the early 1980s, new wave acts embraced a crossover of rock music with African and African-American styles. Adam and the Ants and Bow Wow Wow, both acts with ties to former Sex Pistols manager Malcolm McLaren, used Burundi-style drumming. The Talking Heads album \"Remain in Light\" was marketed and positivity reviewed as a breakthrough melding of new wave and African styles, although drummer Chris Frantz said that he found out about this supposed African influence after the fact. The 1981 U.S. number 1 single \"Rapture\" by Blondie was an homage to rap music. The song name-checked rap artists and Fab 5 Freddie appeared in the video for the song. Second British Invasion acts were influenced by funk and disco.\n\nThe genre produced numerous one-hit wonders.\n\nPower pop continued the guitar-based, singles-oriented British invasion sound of the mid-1960s into the 1970s and the present day. Although the name \"power pop\" had been around before punk (it is believed to have been coined by Pete Townshend in 1967) it became widely associated with new wave when \"Bomp\" and \"Trouser Press\" magazines (respectively in March and April 1978) wrote cover stories touting power pop as a sound that could continue new wave's directness without the negativity associated with punk. Cheap Trick, the Romantics, the Records, Shoes, the Motors, the Only Ones, the Plimsouls, the dB's, the Beat, XTC, the Vapors, 20/20 and Squeeze were groups that found success playing this style. The Jam was the prime example of the mod sensibility of British power pop. By the end of 1979 a backlash had developed against power pop in general, particularly in regards to the Los Angeles scene. The skinny ties worn by LA power pop groups, epitomized by the Knack, became symbolic of the supposed lack of authenticity of the genre. Power pop's association with the genre was later forgotten.\n\nThe term \"post-punk\" was coined to describe groups such as Siouxsie and the Banshees, Wire, Magazine, Public Image Ltd, Joy Division, Gang of Four, the Fall, The Cure, the Psychedelic Furs and Echo and the Bunnymen which were initially considered part of new wave but were more ambitious, serious and challenging, as well as darker and less pop-oriented. Some of these groups would later adopt synths. While punk rock wielded a major influence on the popular music scene in the UK, in the US it remained a fixture of the underground. \n\nThe New Romantic scene developed in the London nightclubs Billy's and the Blitz in the late 1970s. Club-goers wore flamboyant, eccentric costumes and make-up derived from the historical Romantic era. Beginning at \"Bowie and Roxy Music\" themed nights at these clubs, the scene was spearheaded by Steve Strange of Visage, with other soon-to-be pop acts also as regular fixtures such as Boy George of Culture Club, and Spandau Ballet. Around the same time, Duran Duran emerged from a similar scene in Birmingham. Many of the acts that arose from the New Romantic club scene adopted synthpop in their own music, though all would credit David Bowie and Roxy Music as primary influences, both musically and visually.\n\nKraftwerk were acclaimed for their groundbreaking use of synthesizers. Their 1975 pop single \"Autobahn\" reached number 11 in the United Kingdom. In 1978, Gary Numan saw a synthesizer left by another music act and started playing around with it. In 1979, he released two number one albums and two number one singles (one of each under his band name Tubeway Army). Numan's admitted amateurism and deliberate lack of emotion was a sea change from the masculine and professional image that professional synth players had in an era when elaborate, lengthy solos were the norm. His open desire to be a pop star broke from punk orthodoxy. The decreasing price and ease of use of the instrument led acts to follow in Kraftwerk and Numan's footsteps. While Numan also utilized conventional rock instruments, several acts that followed used only synthesizers. Synthpop (or \"technopop\" as it was described by the U.S. press) filled a void left by disco, and grew into a broad genre that included groups such as the Human League, Eurythmics, Depeche Mode, Soft Cell, a-ha, New Order, Orchestral Manoeuvres in the Dark, Japan, Yazoo, Ultravox, Kajagoogoo, and the Thompson Twins.\n\nIn the summer of 1977 both \"Time\" and \"Newsweek\" wrote favorable lead stories on the \"punk/new wave\" movement.<ref name=\"punk/newwave\">[ Genre Punk/New Wave Allmusic]</ref> Acts associated with the movement received little or no radio airplay or music industry support. Small scenes developed in major cities. Continuing into the next year, public support remained limited to select elements of the artistic, bohemian and intellectual population, as arena rock and disco dominated the charts.\n\nStarting in late 1978 and continuing into 1979, acts associated with punk and acts that mixed punk with other genres began to make chart appearances and receive airplay on rock stations and rock discos. Blondie, Talking Heads, the Police and the Cars charted during this period. \"My Sharona\", a single from the Knack, was \"Billboard\" magazine's number one single of 1979. The success of \"My Sharona\" combined with the fact that new wave albums were much cheaper to produce during a time when the music industry was in its worst slump in decades, prompted record companies to sign new wave groups. New wave music scenes developed in Ohio and the college town of Athens, Georgia, with legendary bands like The B-52s and R.E.M.. 1980 saw brief forays into new wave-styled music by non-new wave artists Billy Joel, Donna Summer and Linda Ronstadt.\n\nEarly in 1980, influential radio consultant Lee Abrams wrote a memo saying that with a few exceptions, \"we're not going to be seeing many of the new wave circuit acts happening very big over here (referring to America). As a movement, we don't expect it to have much influence.\" Lee Ferguson, a consultant to KWST, said in an interview that Los Angeles radio stations were banning disc jockeys from using the term and noted, \"Most of the people who call music new wave are the ones looking for a way not to play it.\" Despite the success of Devo's socially critical but widely misperceived song \"Whip It\", second albums by artists who had successful debut albums, along with newly signed artists, failed to sell, and radio pulled most new wave programming.\n\nThe arrival of MTV in 1981 would usher in new wave's most successful era in the United States. British artists, unlike many of their American counterparts, had learned how to use the music video early on. Several British acts on independent labels were able to outmarket and outsell American artists on major labels. Journalists labeled this phenomenon a \"Second British Invasion\". MTV continued its heavy rotation of videos by new wave-oriented acts until 1987, when it changed to a heavy metal and rock dominated format.\n\nIn a December 1982 Gallup poll, 14% of teenagers rated new wave music as their favorite type of music, making it the third most popular. New wave had its greatest popularity on the West Coast. Unlike other genres, race was not a factor in the popularity of new wave music, according to the poll. Urban Contemporary radio stations were the first to play dance-oriented new wave artists such as the B-52's, Culture Club, Duran Duran and ABC.\n\nNew wave soundtracks were used in mainstream Brat Pack films such as \"Valley Girl\", \"Sixteen Candles\", \"Pretty in Pink,\" and \"The Breakfast Club\". John Hughes, the director of several of these films, was enthralled with British new wave music and placed songs from acts such as the Psychedelic Furs, Simple Minds, Orchestral Manoeuvres in the Dark and Echo and the Bunnymen in his films, helping to keep new wave in the mainstream. Several of these songs remain standards of the era. Critics described the MTV acts of the period as shallow or vapid. The homophobic slurs \"faggot\" and \"art fag\" were openly used to describe new wave musicians. Despite the criticism, the danceable quality of the music and the quirky fashion sense associated with new wave artists appealed to audiences.\n\nIn September 1988, \"Billboard\" launched their Modern Rock chart. While the acts on the chart reflected a wide variety of stylistic influences, new wave's legacy remained in the large influx of acts from Great Britain and acts that were popular in rock discos, as well as the chart's name, which reflected how new wave had been marketed as \"modern\". New wave's indie spirit would be crucial to the development of college rock and grunge/alternative rock in the latter half of the 1980s and beyond.\n\nIn the aftermath of grunge, the British music press launched a campaign to promote the New Wave of New Wave. This campaign involved overtly punk and new wave-influenced acts such as Elastica but was eclipsed by Britpop. Other acts of note during the 1990s included No Doubt, Metric, Six Finger Satellite and Brainiac. During that decade, the synthesizer-heavy dance sounds of British and European new wave acts influenced various incarnations of Euro disco and trance. Chris Martin was inspired to start Coldplay by a-ha.\n\nDuring the 2000s, a number of acts emerged that mined a diversity of new wave and post-punk influences. Among these were the Strokes, the Bravery, Interpol, Yeah Yeah Yeahs, Franz Ferdinand, the Epoxies, VHS or Beta, the Rapture, She Wants Revenge, Bloc Party, Foals, Kaiser Chiefs and the Killers. These acts were sometimes labeled \"New New Wave\". The new wave revival reached its apex during the mid-2000s with acts such as the Sounds, the Ting Tings, Melody Club, Hot Chip, Passion Pit, the Presets, La Roux, Ladytron, Shiny Toy Guns, Hockey, Gwen Stefani and Ladyhawke. While some journalists and fans regarded this as a revival, others argued that the phenomenon was a continuation of the original movements.\n\nThe Drums are an example of the trend in the U.S. indie pop scene that employs both the sounds and attitudes of the British new wave era. A new wave-influenced genre called chillwave also developed in the late 2000s, exemplified by artists like Toro Y Moi, Neon Indian, Twin Shadow and Washed Out.\n\nNew wave had a seminal role in the development and popularity of contemporary electronic music.\n\nDuring the late 1990s, new wave received a sudden surge of attention when it was fused with electro and techno during the short-lived electroclash movement. It received popular attention from musical acts such as I-F, Peaches, Fischerspooner and Vitalic, but largely faded when it combined with tech house to form the electro house genre.\n\nDuring the mid 2000s, new rave combined new wave with elements from several other genres, such as indie rock and electro house, and added aesthetic elements archetypal of a rave, such as light shows and glow sticks. Despite the term itself stimulating controversy to the point where many affiliated artists rejected it, new rave as a musical genre was adopted by artists such as the Klaxons, NYPC, Shitdisco and Hadouken!\n\nIn the 2010s, Nostalgia for 1980s new wave has seen a resurgence in the form of synthwave, which is primarily characterized by new wave, soundtrack influences and a retrofuturistic, cyberpunk-like visual aesthetic. This term is applied to the music of artists such as Kavinsky, College, Power Glove, Mitch Murder, and , as well as soundtracks of films and video games such as \"Drive\", \"\", \"Hotline Miami\", \"Kung Fury\", \"Turbo Kid\", and \"\".\n\nNew wave music experienced a lot of mainstream success during the late 1970s and also during the 1980s. During the late 1970s and early 1980s, Blondie had 4 songs on at number 1 on the \"Billboard\" Hot 100. The Clash's song \"Rock the Casbah\" went to number 8 on the \"Billboard\" Hot 100 on 22 January 1983. The Clash's album \"Combat Rock\" was certified platinum by the Recording Industry Association of America (RIAA) on 10 January 1983 was later certified 2x platinum by the RIAA in 1995. Men at Work's albums \"Business as Usual\" and \"Cargo\" were certified 4x platinum by the RIAA on 19 October 1984 and 2x platinum by the RIAA on 19 October 1984, respectively. Men at Work's song \"Who Can It Be Now?\" peaked at number 1 on the \"Billboard\" Hot 100 in 1982 and the band's song \"Down Under\" peaked at number 1 on the \"Billboard\" Hot 100 in 1983. In 1983, Men at Work's songs \"Overkill\" and \"It's a Mistake\" peaked at number 3 and number 6 on the \"Billboard\" Hot 100, respectively. Men at Work's album \"Business as Usual\" peaked at number 1 on the \"Billboard\" 200 on 13 November 1982 and was at number 1 on the chart from 13 November 1982 – 19 February 1983.\n\nThe Police had six top ten hits on the \"Billboard\" Hot 100 during the first half of the 1980s, with one of those top ten hits, \"Every Breath You Take\" peaking at number 1 on the \"Billboard\" Hot 100 in 1983. During that time, the band's songs \"Spirits in the Material World\" and \"Synchronicity II\" peaked at number 11 on the \"Billboard\" Hot 100 and number 16 on the \"Billboard\" Hot 100, respectively. The Police's album \"Synchronicity\", released in June 1983, was certified 4x platinum by the RIAA on 14 November 1984 and was later certified 8x platinum by the RIAA in December 2001. The Police's album \"Ghost in the Machine\", released at the beginning of October 1981, was certified platinum by the RIAA less than 3 months after being released. The album was certified 2x platinum by the RIAA in November 1984 and was certified 3x platinum by the RIAA in December 2001. The Police's album \"Synchronicity\" peaked at number 1 on the \"Billboard\" 200 on 23 July 1983. \"Synchronicity\" was at number 1 on the \"Billboard\" 200 both from 23 July 1983 – 3 September 1983 and from 17 September 1983 – 19 November 1983.\n\nThe Cars' self-titled debut album was certified 6x platinum by the RIAA. The band's album \"Candy-O\" was certified 4x platinum by the RIAA. The Cars' album \"Heartbeat City\", released in March 1984, was certified 2x platinum in October 1984 and was certified 3x platinum in July 1985 by the RIAA. The Cars had four top ten hits on the \"Billboard\" Hot 100 during the 1980s. The Cars' song \"Magic\" peaked at number 12 in July 1984 and the band's song \"Let's Go\" peaked at number 14 on the \"Billboard\" Hot 100 in 1979. Duran Duran had nine top ten hits on the \"Billboard\" Hot 100 during the 1980s, with two of those top ten hits, \"A View to a Kill\" and \"The Reflex\", peaked at number 1 on the \"Billboard\" Hot 100 in 1985 and 1984, respectively. Duran Duran's live album \"Arena\", released in November 1984, was certified 2x platinum by the RIAA on 5 February 1985. Duran Duran's album \"Notorious\", released in November 1986, was certified platinum by the RIAA on 20 January 1987. The Fixx's \"Reach The Beach\" album was certified 2x platinum in 1983, its first year of release. The band also had seven songs reach the top ten throughout the decade, with three of those, \"Are We Ourselves?\", \"Secret Separation\" and \"Driven Out\" reaching number 1 on the \"Billboard\" Mainstream Rock Chart. Devo's song \"Whip It\" went to number 14 on the \"Billboard\" Hot 100 in the year 1980. Kim Wilde's song \"Kids in America\" peaked at number 25 on the \"Billboard\" Hot 100 in 1982. \"Kids in America\" was at number 2 on the UK Singles Chart in March 1981.\n\nTears for Fears' album \"Songs from the Big Chair\" was certified 4x platinum by the RIAA less than a year after being released. Tears for Fears had four top ten hits on the \"Billboard\" Hot 100 during the second half of the 1980s, with two of those hits both peaking at number 1 on the \"Billboard\" Hot 100 in 1985. Talking Heads' song \"Burning Down the House\" peaked at number 9 on the \"Billboard\" Hot 100 in 1983. The song \"Love Shack\" by the band the B-52's peaked at number 3 on the \"Billboard\" Hot 100 in 1989. The band's album \"Cosmic Thing\", released on 27 June 1989, was certified 2x platinum by the RIAA a little less than nine months after being released. The Human League had three top ten hits on the \"Billboard\" Hot 100 during the 1980s, with two of those hits peaking at number 1 on the \"Billboard\" Hot 100. In 1982, the songs \"We Got the Beat\" and \"Vacation\" by the band the Go-Go's peaked at number 2 on the \"Billboard\" Hot 100 and number 8 on the \"Billboard\" Hot 100, respectively. The band's song \"Head Over Heels\" peaked at number 11 on the \"Billboard\" Hot 100 in 1984. The album \"Beauty and the Beat\" by the Go-Go's, released in July 1981, was certified 2x platinum by the RIAA on 14 November 1984. The album \"Beauty and the Beat\" peaked at number 1 on the \"Billboard\" 200 in 1982 and was at number 1 on the chart from 6 March 1982 – 10 April 1982.\n\n\n\n\n"}
{"id": "21160", "url": "https://en.wikipedia.org/wiki?curid=21160", "title": "Telecommunications in the Netherlands", "text": "Telecommunications in the Netherlands\n\nCommunications in the Netherlands.\n\nThe postal service in the Netherlands is performed by PostNL in most cases—which has, as of 2008, a monopoly on letters lighter than 50 g. The monopoly is planned to expire in 2009. PostNL's competitors include Selekt Mail and Sandd. Post offices that are owned by Postbank and TNT Post have been earmarked for closure between 2008 and 2013.\n\nPostal codes in the Netherlands are formed of four digits then two letters (in capitals), separated by a space—1234 AB, for example.\n\nTelephones - main lines in use:\n8.000.000 (2007)\n\nTelephones - mobile cellular subscribers:\n17.200.000 (2007)\n\nTelephone system:\ngeneral assessment: highly developed and well maintained\n\ndomestic: extensive fixed-line fiber-optic network; cellular telephone system is one of the largest in Europe with three major network operators utilizing the third generation of the Global System for Mobile Communications (GSM).\n\n\"international:\"\n9 submarine cables; satellite earth stations - 3 Intelsat (1 Indian Ocean and 2 Atlantic Ocean), 1 Eutelsat, and 1 Inmarsat (Atlantic and Indian Ocean regions) (2004)\n\n\n\"Starting with\"\n\nRadio broadcast stations:\nAM 4, FM 58, shortwave 3 (1998)\nRadios:\n15.3 million (1996)\n\nTelevision broadcast stations:\n25\n\nTelevisions:\n6.700.000 (2002, CBS)\nInternet Service Providers (ISPs):\n33 (2007)\n\nCountry code (Top level domain): .nl\n\n\n"}
{"id": "21161", "url": "https://en.wikipedia.org/wiki?curid=21161", "title": "Transport in the Netherlands", "text": "Transport in the Netherlands\n\nThe Netherlands is both a very densely populated and a highly developed country, in which transport is a key factor of the economy. Correspondingly it has a very dense and modern infrastructure, facilitating transport with road, rail, air and water networks. In its Global Competitiveness Report for 2014-2015, the World Economic Forum ranked the Dutch transport infrastructure fourth in the world.\n\nWith a total road network of 139,000 km, including 3,530 km of expressways, the Netherlands has one of the densest road networks in the world; much denser than Germany and France, but still not as dense as Belgium. The Dutch also have a well developed railway network, that connects most major towns and cities, as well as a comprehensive dedicated cycling infrastructure, featuring some 35,000 km of track physically segregated from motorised traffic.\nThe port of Rotterdam is the world's largest seaport outside East Asia, and by far the largest port of Europe. It connects with its hinterland in Germany, Switzerland and France through rivers Rhine and Meuse. Two thirds of all inland water freight shipping within the EU, and 40% of containers, pass through the Netherlands.\n\nMobility in the Netherlands is considerable. On the roads it has grown continuously since the 1950s and now exceeds 200 billion km travelled per year, three quarters of which are done by car. Around half of all trips in the Netherlands are made by car, 25% by bicycle, 20% walking, and 5% by public transport. Additionally, Dutch airports handled at least 70 million passengers in 2016. Excluding air travel, the Dutch journey more than 30 km a day on average, which takes them just over an hour.\nIn 2010, 1.65 billion tons of goods traffic was registered, half of which moved by sea and inland shipping, and 40% by road transport. The remainder was mostly by pipelines; rail transport only handles 2% of freight movements through the Netherlands.\n\nWith 139,000 km of public roads, the Netherlands has one of the most dense road networks in the world - much denser than Germany and France, but still not as dense as Belgium. In 2013, 5,191 km were national roads, 7,778 km were provincial roads, and 125,230 km were municipality and other roads.\nDutch roads include 3,530 km of motorways and expressways, and with a motorway density of 64 kilometres per 1,000 km², the country also has one of the densest motorway networks in the world.\nThe Netherlands' main highway network (\"hoofdwegennet\") - comparable to Britain's network of trunk roads - consists of most of its 5,200 km of national roads, supplemented with the most prominent provincial roads. Although only about 2,500 km are fully constructed to motorway standards, much of the remainder are also expressways for fast motor vehicles only.\nMobility on Dutch roads has grown continuously since the 1950s and now exceeds 200 billion km travelled per year, three quarters of which are done by car, meaning that while Dutch roads are numerous, they are also used more intensely than in almost any other country. Car ownership in the Netherlands is high but not exceptional, and slightly lower than in surrounding countries. Goods vehicles account for 20% of total traffic.\n\nThe busiest Dutch motorway is the A13 between The Hague and Rotterdam, with a traffic volume of 140,000 motor vehicles per day. The widest Dutch motorway is the A15/A16 just south of Rotterdam with 16 lanes in a 4+4+4+4 setup.\n\nTraffic congestion is common in the Netherlands. The high population density generates significant traffic volumes on both motorways and regular highways. Most congestion occurs in the Randstad, but congestion is a daily structural problem around many larger cities. The Netherlands tries to counter this with an advanced motorway network, with Variable Message Signs and electronic signalization across most of the network. The number of passing motorised vehicles is counted every minute of the day at some 20,000 measuring stations on the Dutch motorway network. A special feature of the motorways is the use of Porous Asphalt Concrete, which reduces noise levels, and allows rain water to be drained efficiently, for safety and expedient traffic flow under precipitation.\n\nCycling is a ubiquitous mode of transport in the Netherlands. 27% of all trips are by bicycle - the highest modal share of any country in the world. Moreover: 36% of the Dutch list the bike as their most frequent mode of transport on a typical day. Some 85% of the people own at least one bicycle. All in all the Dutch are estimated to have at least 18 million functioning bikes, which makes more than one per capita, and much more than the 11.3 million motor vehicles registered on the road. Almost as many passenger kilometres are covered by bicycle as by train.\n\nCycling infrastructure is comprehensive, and public policy, urban planning & laws are bike-friendly. Most roads except for motorways support cyclists, and bikeways are clearly signposted, well maintained and well lit. Dedicated cycle tracks are common on busy roads - some 35,000 km of track has been physically segregated from motor traffic, equal to a quarter of the country's entire road network. Busy junctions often give priority to cyclists, or they are equipped with cycle-specific traffic lights.<br>\nThere are large bicycle parking facilities, particularly in city centres and at train stations. Since the start of the 21st century, parking spaces for 450,000 bicycles were built and modernized at over 400 train stations, and Dutch railways organizations ProRail and NS are calling for an expansion by another 250,000 by 2027. Already half of all Dutch train travelers cycle to the railway station, amounting to half a million cyclists daily.\n\nIn 2013, the European Cyclists' Federation ranked the Netherlands, together with Denmark as the most bike-friendly country in Europe. <br>\nHelmets are neither officially encouraged nor frequently worn.\n\nMost distance travelled on Dutch public transport goes by rail. Like many other European countries, the Netherlands has a dense railway network, totalling between and of track, or 3,013 \"route\" km, three quarters of which has been electrified. The network is mostly focused on passenger transport and connects almost all major towns and cities, counting just over 400 train stations, more than there are municipalities in the Netherlands. The national rail infrastructure is managed by public task company ProRail, and a number of different operators have concessions to run their trains. ProRail also coordinates the totality of scheduling and proper meshing of the Dutch railway services.\n\nPublic passenger rail transport is operated mainly by Nederlandse Spoorwegen (NS) (\"Dutch Railways\"); minor parts by Arriva, Keolis Nederland, Connexxion, Breng, DB Regio, NMBS, Veolia and DB Regionalbahn Westfalen. During week days all railway stations are serviced at least twice an hour in each direction. Large parts of the network are serviced by two to four trains per hour on average. Heavily used routes can be serviced by 8 to 16 trains an hour.\n\nIn recent years, the four largest railway stations in the Netherlands, the central stations of each of the largest cities: Amsterdam, Rotterdam, The Hague and Utrecht, have all entered into major reconstruction and expansion. Rotterdam Central station was completely rebuilt, and was the first to complete, reopening in March 2014.\nThe Hague Central station and Utrecht Central station were reopened, after extensive reconstructions, in February and December 2016, respectively.\nAmsterdam Central station has been undergoing a string of reconstruction works that started in 1997, and is yet to complete.\n\nIn 2015 a consultancy comparison of Europe's railway systems found the Dutch network the most cost effective for its performance, together with Finland's. Per kilometre of track, the Dutch rail network is the busiest in the European Union, handling over a million passengers a day. For 2019 some 2.2 million train journeys are scheduled to travel a record number of 165 million \"train\" kilometres (103 million train miles) — a growth of 28%, up from 124 million km in 2004. Until 2030 ProRail projects a further growth of \"passenger\" kilometres by another 45 percent. For 2019, also 8 percent more freight trains are scheduled than in 2018.\n\nOn the initiative of two European parties: RailNetEurope and Forum Train Europe, a project called \"Redesign of the International Timetabling Process (TTR)\" should help to harmonise planning freight- and passenger trains across Europe, to optimize usage of existing rail tracks. Currently, almost all freight trains (96%) deviate from their original schedule, due to the dynamic nature of cargo transport. The new TTR must facilitate ProRail to let unscheduled freight trains run more easily, without requiring complex shifting in the regular passenger train timetables. Furthermore, as of 2020, timetables will be detailed to tenths of minutes (six second units), instead of whole minutes, to further optimise planning.\n\nIn the long term, significant capacity gains could only be achieved by adding more rail tracks, but there is virtually no more available space, \"or\" transforming large portions of the Dutch railway system to run more like a metro / subway system, which could support up to 24 to 30 trains per hour on the busiest lines. This would however require a structural disentanglement of the current reality, in which trains, train drivers and conductors all have their own work schedules, following the Japanese model. However, at the moment there are no real plans for such steps.\n\nFor longer distances the main public transport in the Netherlands is the train. Long-distance buses are limited to a few missing railway connections. Regional / rural public transport, serving small(er) towns is by bus. Local / urban public transport is also generally by bus, but the three biggest cities (Amsterdam, Rotterdam and The Hague) all have extensive tram systems, that in each case also connect with adjacent cities in their respective urban agglomerations.<br>\nIn addition, Amsterdam and Rotterdam also have several metro lines. Amsterdam's subway was expanded by a new \"\"North-South\"\" line in July 2018, after 15 years of construction, costing € 3.1 Billion.\n\nAdditionally, Rotterdam, The Hague and suburbs in between are connected by a light rail system called RandstadRail, and one line of the Rotterdam metro system connects all the way to The Hague Central station. Utrecht has its own light rail system, called fast tram, connecting the city with adjacent Nieuwegein and IJsselstein. Arnhem is the only Dutch town that still operates a trolleybus system.\n\nDue to the large amount of waterways in the Netherlands, not every road connection has been bridged, and there are still some ferries in operation. In the Rotterdam and \"-Drecht\" towns region, a water bus public transport service operates as well.\n\nPublic transport operators are both the public transport companies run by the big cities: GVB (Amsterdam), RET (Rotterdam) and HTM (The Hague), as well as private enterprise companies like Arriva, Connexxion, Qbuzz and Keolis Nederland.\n\nAmsterdam Airport Schiphol, located southwest of Amsterdam, is the main international airport in the Netherlands, and the third busiest airport in Europe in terms of passengers. Moreover, offering direct flights to 326 destination airports around the planet, Schiphol is the world's second best connected airport.\n\nSchiphol is the primary hub for Dutch flag carrier airline KLM and its regional affiliate KLM Cityhopper, as well as for other Dutch airlines Corendon Dutch Airlines, Martinair, Transavia and TUI Airlines Netherlands. The airport also serves as a European hub for Delta Air Lines and Jet Airways, and as a base for EasyJet and Vueling airlines.\nAccording to Schiphol's preliminary data, the airport handled 63.6 million passengers in 2016, a growth of 9.1% over 2015. Opened in 1916 as a military airbase, Schiphol saw 479,000 flights in 2016, and airfreight tonnage increased by 1.8% to 1.7 million metric tons.\n\nIn other regions there are much smaller international airports, the most prominent being Eindhoven Airport, Rotterdam The Hague Airport, Maastricht Aachen Airport and Groningen Airport. The airports of Eindhoven and Rotterdam / The Hague are both part of the Schiphol Group, and both experienced growth in 2016. Eindhoven Airport grew by 9.3% to 4.7 million passengers, whereas Rotterdam The Hague Airport's growth was a modest 0.2%, reaching 1.6 million travellers in 2016.\nOn Maastricht Aachen, and Groningen airports, a considerable share of flights is seasonal in nature. For transport within the country, air travel is hardly used.\n\nBased on Schiphol Group's preliminary data, its airports alone handled a total of 70 million passengers in 2016.\nIn 2015 Dutch airports handled passengers at a ratio of 47 million on European flights versus 18 million on intercontinental flights, and in 2013 a slightly less 1.6 million metric tons of airfreight.\n\nThe Netherlands has thirteen seaports, three of which have international significance. Handling 440 million metric tons of cargo in 2013, the port of Rotterdam is the biggest port of Europe – as big as the next three biggest combined, and the eighth largest in the world. The Amsterdam seaport is the second in the country, and the fifth largest in Europe. Additionally, since 1998 the ports of Flushing and Terneuzen are working as one, under the name of Zeeland Seaports. Handling 34 million metric tons of cargo in 2012, this is now the third biggest Dutch seaport. For comparison: the nearby port of London handled 44 million tons in that year.\nThrough the rivers Rhine and Meuse, Rotterdam has excellent access to its hinterland upstream, reaching to Germany, France and Switzerland. The port's main activities are petrochemical industries and general cargo handling and transshipment. The harbour functions as an important transit point for bulk materials and between the European continent and overseas. From Rotterdam goods are transported by ship, river barge, train or road. In 2007, the Betuweroute, a new fast freight railway from Rotterdam to Germany, was completed.\n\nThree Dutch ports are deepwater ports, that can handle fully laden Panamax ships: Rotterdam, Zeeland Seaports and the port of IJmuiden. Besides Rotterdam, Amsterdam and Zeeland, the ports of Moerdijk and Vlaardingen also support container liner shipping. Other notable port cities are Dordrecht, Haarlem and Den Helder, as well as Groningen, which controls the seaports of Delfzijl and Eemshaven. Den Helder is home to the Netherlands' main naval base.\n\n\n\"note:\" many Dutch-owned ships are also operating under the registry of Netherlands Antilles (1998 est.)\n\n6,237 km of rivers and canals are navigable for ships of 50 tons. Some 3,740 km of this consists of canals.\nAt least 4,326 km of waterways are usable by craft up to 400 metric ton capacity, and over 3,000 km are usable by ships up to 1,250 metric ton capacity. Although another source states that all of 6,230 km is navigable for craft up to 400 tons, and over 4,000 km is usable by ships up to 1,500 metric ton capacity.\nThe Dutch inland shipping fleet is the biggest in Europe. Consisting of some 7,000 vessels, it takes a share of 35% of the national total annual freight transport, and as much as 80% of bulk transport. Also two thirds of all inland water freight transports within the EU, and 40% of the EU's inland container shipping, pass through the Netherlands. All in all the Netherlands has so many waterways that virtually all major industrial areas and population centres can be reached by water via inland ports (200) and transhipment terminals (350).\n\nCrude oil: 418 km; petroleum products: 965 km.\n\nThe distribution network for natural gas is the most dense in Europe and of very high quality, with a total length of 12,200 kilometres of transmission pipelines and 136,400 kilometres of distribution pipelines. A technical investigation has concluded that the existing Dutch high-pressure gas infrastructure could feasibly be converted for transport of hydrogen in the future.\n\nTransport in the Netherlands falls under the Ministry of Infrastructure and Water Management. With regard to public transport, not involving national rail, a total of 14 public bodies have been delegated the authority to grant concessions to public transport operators, namely the twelve provinces, plus the two transport-regions specifically for Amsterdam and Rotterdam / The Hague. These 14 parties are united in a cooperation called \"DOVA\" (\"Decentrale Openbaar Vervoer Autoriteiten\"), or \"Decentralised Public Transport Authorities\". The provinces in turn sometimes delegate this authority to their municipalities.\n\nRoads are controlled by authorities at all four administrative levels in the Netherlands. About 5,200 km of national roads (\"Rijkswegen\") are controlled by central national government agency Rijkswaterstaat, and the country's twelve provinces manage about 7,800 km of provincial roads. Most motorways are national roads, and the remaining national roads are mostly expressways. Only a few motorways are provincial ones, and they are much shorter and serve mostly regional traffic. Frequently, they were previously national roads.\n\nMunicipality roads make up the bulk of the network, totalling some 120,000 km. They are mostly local roads. Aside from the division in provinces, the Netherlands is also divided in 21 water management boards. Together with miscellaneous authorities, they own and control another 7,500 km of roads. For some roads, it is because they are a physical element of water barriers, like dikes and dams while others provide primary access to critical water control structures and may not even be open to the public.\n\nAlthough transport economics is much more than just the economy of the transport sector itself, the latter is much easier to quantify. In 2012 the Dutch goods transport and storage sectors by themselves accounted for almost 400,000 full-time jobs, employing some 500,000 people. Gross revenues totalled 77 billion euro, leading to results of 4.3 billion euro.\n\n\n"}
{"id": "21162", "url": "https://en.wikipedia.org/wiki?curid=21162", "title": "Armed forces of the Netherlands", "text": "Armed forces of the Netherlands\n\nThe Armed forces of the Netherlands consist of the Army, Navy, and Air Force.\n\nThe service branches consist of:\n\nIn addition, within the Kingdom of the Netherlands, there are small local conscript forces on the islands of Aruba (Arumil) and Curaçao (Antmil). These operate under the auspices of the Royal Netherlands Navy and Marines.\n\nThe military ranks of the Dutch armed forces have similarities with British and U.S. military ranks. The highest-ranking officer in the Dutch military is the Chief of Defence (Netherlands), who is a four-star officer (NATO OF-9).\n\nThe Dutch armed forces exist by declaration in the constitution of the Netherlands. Article 97 of this constitution determines that the armed forces exist\n\nThis means that the role and responsibility of the Dutch military in international stability and peacekeeping is constitutionally determined.\n\nThe same article of the constitution determines that supreme command of the Dutch military resides with the Government of the Netherlands. This has been the case since the constitution was changed in 1983; before then, supreme command of the armed forces of the Netherlands was held by the King of the Netherlands.\n\nIn addition, a second major change in military affairs was made in 2003. Before then, all citizens of the Netherlands were tasked with the defense of the kingdom. In keeping with the move to a professional military, this article was dropped.\n\nThe Netherlands' military is currently a fully professional military. Conscription in the Netherlands was suspended in 1996 with the exception of Aruba and Curaçao. All military branches and specialties are open to female recruits.In October 2018 the Dutch Ministry of Defence announced that the submarine service will also accept female recruits for positions as officer, NCO and sailor.\n\nThe Dutch Ministry of Defence employs almost 70,000 personnel, including both civilian and military personnel.\n\nThe Dutch military is part of the NATO militaries and therefore conforms to the structure of a NATO military. It also uses conforming rank structures.\nAll Dutch military personnel, officers and enlisted personnel, are required to take an oath of allegiance. This oath is recorded in the law on General Military Personnel Regulations (Algemeen Militair Ambtenarenregelement) in Article 126a.\n\nUnlike many military organizations, Dutch military members are allowed to form and join unions.\n\nThere are four of these unions:\n\nAll unions represent both current and retired military personnel and/or civilian personnel.\n\nSince the 1990s, the Dutch military has been involved in four major military campaigns:\n\nAs part of Operation Enduring Freedom as a response to those attacks, the Netherlands deployed aircraft as part of the European Participating Air Force (EPAF) in support of ground operations in Afghanistan as well as Dutch naval frigates to police the waters of the Middle East/Indian Ocean. The Netherlands deployed further troops and helicopters to Afghanistan in 2006 as part of a new security operation in the south of the country. Dutch ground and air forces totalled almost 2,000 personnel during 2006, taking part in combat operations alongside British and Canadian forces as part of NATO's ISAF force in the south.\n\nThe Netherlands announced in December 2007 that it would begin withdrawing its troops from Afghanistan, which were mainly in Uruzgan Province, in July 2010. \"I do not have assurances that other countries will be ready to replace Netherlands troops, but I am certain that Dutch troops will leave in 2010,\" Foreign Minister Maxime Verhagen said. \"I indicated that in writing ... to the NATO secretary general, who has confirmed it.\" In January 2009, Prime Minister Jan Peter Balkenende reiterated that the 1,600 Dutch troops in Afghanistan would end their mission in 2010, saying \"We will stop in Uruzgan in 2010.\" He ruled out the possibility of the Netherlands keeping its troops in Afghanistan past 2010 with any force comparable to its former deployment.\n\nIn December 2009, reacting to three requests received from the side of the U.S. by Vice President Biden, the special American representative to Afghanistan Holbrooke and Secretary of State Clinton and a request by Secretary General of NATO Rasmussen as well, the Dutch government announced that the final decision on the continuation of the mission in Uruzgan would be on its agenda in March 2010. Two ministers from the Labour Party (PvdA), Koenders (Development Aid) and Bos (Finance and Vice PM) in the meantime pleaded termination, which was also the opinion of the majority of the Dutch parliament.\n\nOn 10 December 2009, the Dutch daily newspaper De Telegraaf reported that the government was exploring areas elsewhere in Afghanistan to set up a new mission. The northern province of Kunduz was mentioned, where at the moment German and Belgian troops were deployed. On 9 December, allegedly PM Balkenende (CDA), the vice-PM's Bos (PvdA) and Rouvoet (ChristenUnie) and the three involved ministers Verhagen (CDA, Foreign Affairs), Van Middelkoop (ChristenUnie, Defense) and Koenders (PvdA, (Development Aid) secretly discussed the future Dutch engagement in Afghanistan, together with Commander of the Forces general Van Uhm.\n\nIn early February 2010, the disagreement between the PvdA on the one hand and CDA and ChristenUnie on the other about a request from NATO, by improper channels, for a renewed Dutch commitment in Afghanistan, came to a head. CDA and ChristenUnie wanted the freedom to consider this request—in spite of the decisions by the Minister of Defence and the votes in Parliament—whereas PvdA and a majority of the parties in the Dutch parliament stood by the earlier decision and refused any consideration of further Dutch involvement in Afghanistan. Thus, on 20 February, the PvdA had no choice but to resign their ministers from the Cabinet, leading to a collapse of the Dutch government. As a result, the NATO request could not be considered and Dutch troops withdrew later in 2010 according to the schedule agreed in 2007.\n\nOn 1 August 2010 the Dutch military formally declared its withdrawal from its four-year mission in Afghanistan; most soldiers are expected to be back in the Netherlands by September, excepting those working on the reset, redistribution and repatriation of materiel and supplies. The AH-64 Apache and F-16 squadron will remain longer in Afghanistan to support the withdrawal process and transports. The Dutch contingent has been replaced by soldiers from the U.S., Australia, Slovakia, and Singapore.\n\n"}
{"id": "21163", "url": "https://en.wikipedia.org/wiki?curid=21163", "title": "Foreign relations of the Netherlands", "text": "Foreign relations of the Netherlands\n\nThe foreign policy of the Netherlands is based on four basic commitments: to the Atlantic cooperation, to European integration, to international development and to international law. While historically the Kingdom of the Netherlands was a neutral state, since 1945 it has become a member of NATO, the United Nations, the European Union and many other international organisations. The Dutch economy is very open and relies on international trade. During and after the 17th century—its Golden Age--the Dutch built up a commercial and colonial empire. It was a leading shipping and naval power and was often at war with England, its main rival. Its main colonial holding was Indonesia, which fought for and achieved independence after 1945. The historical ties inherited from its colonial past still influence the foreign relations of the Netherlands. Foreign trade policy is handled by the European Union. The Dutch have been active in international peacekeeping roles.\n\nIn the Dutch Golden Age, which had its zenith around 1667, there was a flowering of trade, industry, the arts and the sciences. A rich worldwide Dutch empire developed and the Dutch East India Company became one of the earliest and most important of national mercantile companies based on entrepreneurship and trade.\n\nDuring the 18th century the power and wealth of the Netherlands declined. A series of wars with the more powerful British and French neighbors weakened it. Britain seized the North American colony of New Amsterdam, turning it into New York. There was growing unrest and conflict between the Orangists and the Patriots. The French Revolution spilled over after 1789, and a pro-French Batavian Republic was established in 1795–1806. Napoleon made it a satellite state, the Kingdom of Holland (1806–1810), and later simply a French imperial province.\n\nIn 1815–1940 it was neutral and played a minor role in world diplomacy, apart from a failed effort to control Belgium before giving up in 1839. It was invaded and cruelly treated by Germany in 1940–45, with starvation and killing the Jews the main Nazi policies.\n\nThe Dutch Government conducted a review of foreign policy main themes, organization, and funding in 1995. The document \"The Foreign Policy of the Netherlands: A Review\" outlined the new direction of Dutch foreign policy. The Netherlands prioritizes enhancing European integration, maintaining relations with neighboring states, ensuring European security and stability (mainly through the mechanism of NATO and emphasizing the important role the United States plays in the security of Europe), and participating in conflict management and peacekeeping missions. The foreign policy review also resulted in the reorganization of the Ministry of Foreign Affairs. Through the creation of regional departments, the Ministry coordinates tasks previously divided among the international cooperation, foreign affairs, and economic affairs sections.\n\nDutch security policy is based primarily on membership in NATO, which the Netherlands co-founded in 1949. Because of Dutch participation in NATO nuclear weapons are stationed in the Netherlands, see Volkel Air Base.\n\nThe Dutch also pursue defense cooperation within Europe, both multilaterally – in the context of the Western European Union and the European Security and Defence Policy of the EU – and bilaterally, as in the German-Netherlands Corps. In recent years, the Dutch have become significant contributors to UN peacekeeping efforts around the world as well as to the Stabililzation Force in Bosnia and Herzegovina (SFOR) in Bosnia.\n\nThe Dutch have been strong advocates of European integration, and most aspects of their foreign, economic, and trade policies are coordinated through the European Union (EU). The Dutch postwar customs union with Belgium and Luxembourg (the Benelux group) paved the way for the formation of the European Community (precursor to the EU), of which the Netherlands was a founding member. Likewise, the Benelux abolition of internal border controls was a model for the wider Schengen Accord, which today has 29 European signatories (including the Netherlands) pledged to common visa policies and free movement of people across common borders.\n\nThe Dutch stood at the cradle of the 1992 Maastricht Treaty and have been the architects of the Treaty of Amsterdam concluded in 1998. The Dutch have thus played an important role in European political and monetary integration; indeed, until the year 2003, Dutchman Wim Duisenberg headed the European Central Bank. In addition, Dutch financial minister Gerrit Zalm was the main critic of the violation of the Stability and Growth Pact by France and Germany in 2004 and 2005.\n\nThe Netherlands is among the world's leading aid donors, giving almost $8 billion, about 0.8% of its gross national income (GNI) in official development assistance (ODA). It is one of five countries worldwide that meets the longstanding UN ODA target of 0.7% ODA/GNI. The country consistently contributes large amounts of aid through multilateral channels, especially the United Nations Development Programme, the international financial institutions, and EU programs. A large portion of Dutch aid funds also are channeled through private (\"co-financing\") organizations that have almost total autonomy in choice of projects.\n\nThe Netherlands is a member of the European Bank for Reconstruction and Development, which recently initiated economic reforms in central Europe. The Dutch strongly support the Middle East peace process and in 1998 earmarked $29 million in contributions to international donor-coordinated activities for the occupied territories and also for projects in which they worked directly with Palestinian authorities. These projects included improving environmental conditions and support for multilateral programs in cooperation with local non-governmental organizations. In 1998, the Dutch provided significant amounts of aid to the former Yugoslavia and Africa. The Dutch consistently provide significant amounts of relief aid to victims of natural disasters, such as Hurricane Mitch in Central America, the 2004 tsunami in Southeast Asia, and more recent catastrophes in Pakistan and Burma.\n\n\"Developing countries aspiring to purchase foreign goods and services to invest in, inter alia, port facilities, roads, public transport, health care, or drinking water facilities may be eligible for a special Dutch grant facility. The grant facility, known as ORET (a Dutch acronym for Ontwikkelingsrelevante Exporttransacties, or Development-Related Export) serves to award grants to governments of developing countries for making payments to foreign suppliers.\"\n\nA centuries-old tradition of legal scholarship has made the Netherlands the home of the International Court of Justice; the Iran-United States Claims Tribunal; the International Criminal Tribunal for the former Yugoslavia; the International Criminal Tribunal for Rwanda; and the International Criminal Court (ICC). In addition it hosts the European police organization, Europol; and the Organisation for the Prohibition of Chemical Weapons.\n\nAs a relatively small country, the Netherlands generally pursues its foreign policy interests within the framework of multilateral organizations. The Netherlands is an active and responsible participant in the United Nations system as well as other multilateral organizations such as the Organization for Security and Cooperation in Europe, Organisation for Economic Co-operation and Development (OECD), World Trade Organization (WTO), and International Monetary Fund.\n\nThe Netherlands is one of the founding members of what today is the European Union. It was one of the first countries to start European integration, through the Benelux in 1944 and the European Coal and Steel Community in 1952. Being a small country with a history of neutrality it was the host country for the important Maastricht Treaty and Amsterdam Treaty and is the seat of the International Court of Justice.\n\nThe country is one of the major producers of illicit amphetamines and other synthetic drugs. It also functions as an important gateway for cocaine, heroin, and hashish entering Europe. A large portion of the world's XTC consumption is supplied by illegal laboratories from the Netherlands.\n\nThe Dutch also work with the U.S. and other countries on international programs against drug trafficking and organized crime. The Dutch-U.S. cooperation focuses on joint anti-drug operations in the Caribbean, including an agreement establishing Forward Operating Locations on the Dutch Kingdom islands of Curaçao and Aruba. The Netherlands is a signatory to international counter-narcotics agreements, a member of the United Nations International Drug Control Program, the UN Commission on Narcotic Drugs, and is a contributor to international counter-narcotics.\n\nFrom June 26 until December 22, 2006, two children, Ammar (12–13) and Sara (10–11), lived in the Dutch embassy in Damascus because of a child custody dispute between the Dutch mother, supported by Dutch law and the Hague Convention on the Civil Aspects of International Child Abduction, and the Syrian father, supported by Syrian law (Syria is no participant of this convention). The children had been living in Syria since 2004, after an alleged international child abduction by the father from the Netherlands to Syria, during a family contact in which he supposedly would visit Paris with them. The children fled to the embassy because they would like to live with their mother in the Netherlands. Minister of Foreign Affairs Ben Bot travelled to Damascus, negotiated and on December 22 the children finally could return to the Netherlands.\n\nThe father claims that the Dutch government has promised not to prosecute him for the abduction. However, a Dutch prosecutor claims that he is free to prosecute the father and may well do that, and that the Dutch have only retracted the international request to arrest him outside the Netherlands.\n\nThe Caribbean islands of Aruba, Curaçao, Sint Maarten, Bonaire, Sint Eustatius and Saba are dependencies of the Netherlands. The latter three are part of the Netherlands proper and are collectively known as the Caribbean Netherlands. Suriname and Indonesia became independent of the Netherlands in the period of decolonization: Suriname in 1975 and Indonesia in 1945 (it was not until August 16, 2005 that the Dutch government recognized 1945 and not 1949 as the latter's year of independence).\n\n\n"}
{"id": "21164", "url": "https://en.wikipedia.org/wiki?curid=21164", "title": "Drug policy of the Netherlands", "text": "Drug policy of the Netherlands\n\nWhile recreational use, possession and trade of non-medicinal drugs described by the Opium Law are all technically illegal under Dutch law, official policy since the late 20th century has been to openly tolerate all recreational use while tolerating the other two under certain circumstances. This pragmatic approach was motivated by the idea that a drug-free Dutch society is unrealistic and unattainable, and efforts would be better spent trying to minimize harm caused by recreational drug use. As a result of this gedoogbeleid (lit. \"tolerance policy\" or \"policy of tolerance\"), the Netherlands is typically seen as much more tolerant of drugs than most other countries.\n\nLegal distinctions are made in the Opium Law between drugs with a low risk of harm and/or addiction, called soft drugs, and drugs with a high risk of harm and/or addiction, called hard drugs. Soft drugs include hash, marijuana, sleeping pills and sedatives, while hard drugs include heroin, cocaine, amphetamine, LSD and ecstasy. Policy has been to largely tolerate the sale of soft drugs while strongly suppressing the sale, circulation and use of hard drugs, effectively separating it into two markets. Establishments that have been permitted to sell soft drugs under certain circumstances are called \"coffee shops\". Laws established in January 2013 required visitors of coffee shops to be Dutch residents, but these laws were only applied in Zeeland, North Brabant and Limburg after much local criticism. Possession of a soft drug for personal use in quantities below a certain threshold (5 grams of cannabis or 5 cannabis plants) is tolerated, but larger quantities or possession of hard drugs may lead to prosecution. Prosecution for possession, trade and (in some rare cases) use are typically handled by the municipal government except where large-scale criminal activity is suspected.\n\nNotably absent from toleration of drugs is its production, particularly the cultivation of weed. This has led to a seemingly paradoxical system where coffee shops are allowed to buy and sell soft drugs but where production is nearly always punished. Because coffee shops have to get their goods from somewhere, criticism has been raised over the years against continued prosecution of soft drug producers. It was first challenged in court in 2014 when a judge found two people guilty of producing weed in large quantities but refused to punish them. A breakthrough occurred in early 2017, when a slight majority in the House of Representatives allowed for a law to pass that would partly legalize production of weed. In late 2017, the newly formed coalition announced that they would seek to implement an experimental new system in certain cities where coffee shops could legally acquire weed from a state-appointed producer.\n\nWhile the legalization of cannabis remains controversial, the introduction of heroin-assisted treatment in 1998 has been lauded for considerably improving the health and social situation of opiate-dependent patients in the Netherlands.\n\nLarge-scale dealing, production, import and export are prosecuted to the fullest extent of the law, even if it does not supply end users or \"coffeeshops\" with more than the allowed amounts. Exactly how coffeeshops get their supplies is rarely investigated, however. The average concentration of THC in the cannabis sold in coffeeshops has increased from 9% in 1998 to 18% in 2005. This means that less plant material has to be consumed to achieve the same effect. One of the reasons is plant breeding and use of greenhouse technology for illegal growing of cannabis in Netherlands.\nThe former minister of Justice Piet Hein Donner announced in June 2007 that cultivation of cannabis shall continue to be illegal.\n\nThe drug policy of the Netherlands is marked by its distinguishing between so called soft and hard drugs. An often used argument is that alcohol, which is claimed by some scientists as a hard drug, is legal and a soft drug can't be more dangerous to society if it's controlled. This may refer to the Prohibition in the 1920s, when the U.S. government decided to ban all alcohol. Prohibition created a golden opportunity for organized crime syndicates to smuggle alcohol, and as a result the syndicates were able to gain considerable power in some major cities.\nCannabis remains a controlled substance in the Netherlands and both possession and production for personal use are still misdemeanors, punishable by fines. Coffeeshops are also technically illegal but are flourishing nonetheless. However, a policy of non-enforcement has led to a situation where reliance upon non-enforcement has become common, and because of this the courts have ruled against the government when individual cases were prosecuted.\n\nThis is because the Dutch Ministry of Justice applies a \"gedoogbeleid\" (tolerance policy) with regard to the category of soft drugs: an official set of guidelines telling public prosecutors under which circumstances offenders should not be prosecuted. This is a more official version of a common practice in other European countries wherein law enforcement sets priorities regarding offenses on which it is important enough to spend limited resources.\n\nAccording to current \"gedoogbeleid\" the possession of a maximum amount of five grams cannabis for personal use is not prosecuted. Cultivation is treated in a similar way. Cultivation of 5 plants or less is usually not prosecuted when they are renounced by the cultivator.\n\nProponents of \"gedoogbeleid\" argue that such a policy practices more consistency in legal protection than without it. Opponents of the Dutch drug policy either call for full legalization, or argue that laws should penalize morally wrong or deviant behavior, whether enforceable or not. In the Dutch courts, however, it has long been determined that the institutionalized non-enforcement of statutes with well defined limits constitutes \"de facto\" decriminalization. The statutes are kept on the books mainly due to international pressure and in adherence with international treaties. A November 2008 poll showed that a 60% majority of the Dutch population support the legalisation of soft drugs. The same poll showed that 85% supported closing of all cannabis coffeeshops within 250 meters walking distance from schools.\n\nImporting and exporting of any classified drug is a serious offence. The penalty can run up to 12 to 16 years if it is hard drug trade, maximum 4 years for import or export of large quantities of cannabis. It is prohibited to operate a motor vehicle while under the influence of any drug that affects driving ability to such an extent that you are unable to drive properly. (Section 8 of the 1994 Road Traffic Act section 1). The Dutch police have the right to do a drug test if they suspect influenced driving. For example, anybody involved in a traffic accident may be tested. Causing an accident that inflicts bodily harm, while under influence of any drug, is seen as a crime that may be punished by up to 3 years in prison (9 years in case of a fatal accident). Suspension of driving license is also normal in such a case (maximum 5 years). Schiphol, a large international airport near Amsterdam, has long practiced a zero tolerance policy regarding airline passengers carrying drugs. In 2006 there were 20,769 drug crimes registered by public prosecutors and 4,392 persons received an unconditional prison sentence The rate of imprisonment for drug crimes is about the same as in Sweden, which has a zero tolerance policy for drug crimes.\n\nDespite the high priority given by the Dutch government to fighting illegal drug trafficking, the Netherlands continue to be an important transit point for drugs entering Europe. The Netherlands is a major producer and leading distributor of cannabis, heroin, cocaine, amphetamines and other synthetic drugs, and a medium consumer of illicit drugs. Despite the crackdown by Interpol on traffic and illicit manufacture of temazepam, the country has also become a major exporter of illicit temazepam of the jelly variety, trafficking it to the United Kingdom and other European nations. The government has intensified cooperation with neighbouring countries and stepped up border controls. In recent years, it also introduced so-called 100% checks and bodyscans at Schiphol Airport on incoming flights from Dutch overseas territories Aruba and Netherlands Antilles to prevent importing cocaine by means of swallowing balloons by mules.\n\nAlthough drug use, as opposed to trafficking, is seen primarily as a public health issue, responsibility for drug policy is shared by both the Ministry of Health, Welfare, and Sports, and the Ministry of Justice.\n\nThe Netherlands spends more than €130 million annually on facilities for addicts, of which about fifty percent goes to drug addicts. The Netherlands has extensive demand reduction programs, reaching about ninety percent of the country's 25,000 to 28,000 hard drug users. The number of hard drug addicts has stabilized in the past few years and their average age has risen to 38 years, which is generally seen as a positive trend. Notably, the number of drug-related deaths in the country remains amongst the lowest in Europe.\n\nOn 27 November 2003, the Dutch Justice Minister Piet Hein Donner announced that his government was considering rules under which coffeeshops would only be allowed to sell soft drugs to Dutch residents in order to satisfy both European neighbors' concerns about the influx of drugs from the Netherlands, as well as those of Netherlands border town residents unhappy with the influx of \"drug tourists\" from elsewhere in Europe. The European Court of Justice ruled in December 2010 that Dutch authorities can ban coffeeshops from selling cannabis to foreigners. The EU court said the southern Dutch city of Maastricht was within its rights when it introduced a \"weed passport\" in 2005 to prevent foreigners from entering cafés that sell cannabis.\n\nIn 2010 the owner of Netherlands's largest cannabis selling coffeeshop was fined 10 million euros for breaking drug laws by keeping more than the tolerated amount of cannabis in the shop. He was also sentenced to a 16-week prison term.\n\nCriminal investigations into more serious forms of organized crime mainly involve drugs (72%). Most of these are investigations of hard drug crime (specifically cocaine and synthetic drugs) although the number of soft drug cases is rising and currently accounts for 69% of criminal investigations.\n\nIn a study of the levels of cannabis, cocaine, MDMA, methamphetamine and other amphetamine in wastewater from 42 major cities in Europe Amsterdam came near the top of the list in every category but methamphetamine.\n\nIn the province of North-Brabant in the south of the Netherlands, the organized crime organizations form the main producer of MDMA, amphetamine and cannabis in Europe. Together with the proximity of the ports of Antwerp and especially Rotterdam where heroin and cocaine enter the European continent, this causes these substances to be readily available for a relative low price. Therefore, there is a large quantity drugs of a relative high quality with few pollution available. This means that users will not have to rely on more polluted substances with greater health risks. Together with an approach that focuses on easily accessible health care, harm reduction and prevention, this causes the medical condition of the Dutch addicts to be less severe than that of many other countries.\n\nThe Netherlands is a party to the 1961 Single Convention on Narcotic Drugs, the 1971 Convention on Psychotropic Substances, and the 1988 United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances. The 1961 convention prohibits cultivation and trade of naturally occurring drugs such as cannabis; the 1971 treaty bans the manufacture and trafficking of synthetic drugs such as barbiturates and amphetamines; and the 1988 convention requires states to criminalize illicit drug possession:\nSubject to its constitutional principles and the basic concepts of its legal system, each Party shall adopt such measures as may be necessary to establish as a criminal offence under its domestic law, when committed intentionally, the possession, purchase or cultivation of narcotic drugs or psychotropic substances for personal consumption contrary to the provisions of the 1961 Convention, the 1961 Convention as amended or the 1971 Convention.\nThe International Narcotics Control Board typically interprets this provision to mean that states must prosecute drug possession offenses. The conventions clearly state that controlled substances are to be restricted to scientific and medical uses. However, Cindy Fazey, former Chief of Demand Reduction for the United Nations Drug Control Programme, believes that the treaties have enough ambiguities and loopholes to allow some room to maneuver. In her report entitled \"The Mechanics and Dynamics of the UN System for International Drug Control\", she notes:\nMany countries have now decided not to use the full weight of criminal sanctions against people who are in possession of drugs that are for their personal consumption. The Conventions say that there must be an offence under domestic criminal law, it does not say that the law has to be enforced, or that when it is what sanctions should apply. . . . Despite such grey areas latitude is by no means unlimited. The centrality of the principle of limiting narcotic and psychotropic drugs for medical and scientific purposes leaves no room for the legal possibility of recreational use. . . . Nations may currently be pushing the boundaries of the international system, but the pursuit of any action to formally legalize non-medical and non-scientific drug use would require either treaty revision or a complete or partial withdrawal from the current regime.\nThe Dutch policy of keeping anti-drug laws on the books while limiting enforcement of certain offenses is carefully designed to reduce harm while still complying with the letter of international drug control treaties. This is necessary in order to avoid criticism from the International Narcotics Board, which historically has taken a dim view of any moves to relax official drug policy. In their annual report, the Board has criticised many governments, including Canada, for permitting the medicinal use of cannabis, Australia for providing injecting rooms and the United Kingdom for proposing to downgrade the classification of cannabis, which it has since done (although this change was reversed by the Home Secretary on 7 May 2008 against the advice of its own commissioned report).\n\nThe liberal drug policy of the authorities in the Netherlands especially led to problems in \"border hot spots\" that attracted \"drug tourism\" as well as trafficking and related law enforcement problems in towns like Enschede in the East and Terneuzen, Venlo, Maastricht and Heerlen in the South. In 2006, Gerd Leers, then mayor of the border city of Maastricht, on the Dutch-Belgian border, criticised the current policy as inconsistent, by recording a song with the Dutch punk rock band De Heideroosjes. By allowing possession and retail sales of cannabis, but not cultivation or wholesale, the government creates numerous problems of crime and public safety, he alleges, and therefore he would like to switch to either legalising and regulating production, or to the full repression that his party (CDA) officially advocates. The latter suggestion has widely been interpreted as rhetorical. Leers's comments have garnered support from other local authorities and put the cultivation issue back on the agenda.\n\nIn November 2008, Pieter van Geel, the leader of the CDA (Christian Democrats) in the Dutch parliament, called for a ban on the cafés where cannabis is sold. He said the practice of allowing so-called coffeeshops to operate had failed. The CDA had the support of its smaller coalition partner, the CU (ChristenUnie), but the third party in government, PvdA (Labour), opposed. The coalition agreement worked out by the three coalition parties in 2007 stated that there would be no change in the policy of tolerance. Prominent CDA member Gerd Leers spoke out against him: cannabis users who now cause no trouble would be viewed as criminals if an outright ban was to be implemented. Van Geel later said that he respected the coalition agreement and would not press for a ban during the current government's tenure .\n\nBy 2009, 27 \"coffeeshops\" selling cannabis in Rotterdam, all within 200 metres from schools, must close down. This is nearly half of the \"coffeeshops\" that currently operate within its municipality. This is due to the new policy of city mayor Ivo Opstelten and the town council. The higher levels of the active ingredient in cannabis in Netherlands create a growing opposition to the traditional Dutch view of cannabis as a relatively innocent soft drug. Supporters of \"coffeeshops\" state that such claims are often exaggerated and ignore the fact that higher content means a user needs to use less of the plant to get the desired effects, making it in effect safer. Dutch research has however shown that an increase of THC content also increase the occurrence of impaired psychomotor skills, particularly among younger or inexperienced cannabis smokers, who do not adapt their smoking-style to the higher THC content. Closing of \"coffeeshops\" is not unique for Rotterdam. Many other towns have done the same in the last 10 years.\n\nIn 2008, the municipality of Utrecht imposed a Zero Tolerance Policy to all events like the big dance party Trance Energy held in Jaarbeurs. However, such zero-tolerance policy at dance parties are now becoming common in the Netherlands and are even stricter in cities like Arnhem.\n\nThe two towns Roosendaal and Bergen op Zoom announced in October 2008 that they would start closing all \"coffeeshops\", each week visited by up to 25000 French and Belgian drug tourists, with closures beginning in February 2009.\n\nIn May 2011 the Dutch government announced that tourist are to be banned from Dutch coffeeshops, starting in the southern provinces and at the end of 2011 in the rest of the country. In a letter to the parliament, the Dutch health and justice ministers said that, \"In order to tackle the nuisance and criminality associated with coffeeshops and drug trafficking, the open-door policy of coffeeshops will end\".\n\nA government committee delivered in June 2011 a report about Cannabis to the Dutch government. It includes a proposal that cannabis with more than 15 percent THC should be labeled as hard drugs. Higher concentrations of THC and drug tourism have challenged the current policy and led to a re-examination of the current approach; e.g. ban of all sales of cannabis to tourists in coffeeshops from end of 2011 was proposed but currently only the border city of Maastricht has adopted the measure in order to test out its feasibility. According to the initial measure, starting in 2012, each coffeeshop was to operate like a private club with some 1,000 to 1,500 members. In order to qualify for a membership card, applicants would have to be adult Dutch citizens, membership was only to be allowed in one club.\n\nIn Amsterdam 26 coffeeshops in the De Wallen area will have to close their doors between 1 September 2012 and 31 August 2015.\n\nA Dutch judge has ruled that tourists can legally be banned from entering cannabis cafés, as part of new restrictions which come into force in 2012.\n\nA study conducted by the European Monitoring Centre of Drugs and Drug Addiction report that 25.3% of Irish people smoked cannabis at some stage in their life. Whereas 25.7% of Dutch people have tried cannabis.\n\nIn October 2007, the prohibition of hallucinogenic or \"magic mushrooms\" was announced by the Dutch authorities.\n\nOn April 25, 2008, the Dutch government, backed by a majority of members of parliament, decided to ban cultivation and use of all magic mushrooms. Amsterdam mayor Job Cohen proposed a three-day cooling period in which clients would be informed three days before actually procuring the mushrooms and if they would still like to go through with it they could pick up their spores from the smart shop.\nThe ban has been considered a retreat from liberal drug policies. This followed a few deadly incidents mostly involving tourists. These deaths were not directly caused by the use of the drug \"per se\", but by deadly accidents occurring while under the influence of magic mushrooms.\n\nAs of December 1, 2008, all psychedelic mushrooms are banned. However, schlerotia (what are termed as \"truffles\"), mushroom spores, and active mycellium cultures remained legal and are readily available in the \"smartshops\", the stores in the Dutch cities that sell legal drugs, herbs and related gadgets.\n\nThe relatively recent increase in the cocaine trafficking business has been largely focused on the Caribbean area. Since early 2003, a special law court with prison facilities has been operational at Schiphol airport. Since the beginning of 2005, there has been 100% control of all flights from key countries in the Caribbean. In 2004, an average of 290 drug couriers per month were arrested, decreasing to 80 per month by early 2006.\n\n\n\n"}
{"id": "21168", "url": "https://en.wikipedia.org/wiki?curid=21168", "title": "2001 in the Netherlands", "text": "2001 in the Netherlands\n\nThis article lists some of the events that took place in the Netherlands in 2001.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21170", "url": "https://en.wikipedia.org/wiki?curid=21170", "title": "Numeral system", "text": "Numeral system\n\nA numeral system (or system of numeration) is a writing system for expressing numbers; that is, a mathematical notation for representing numbers of a given set, using digits or other symbols in a consistent manner.\n\nThe same sequence of symbols may represent different numbers in different numeral systems. For example, \"11\" represents the number \"eleven\" in the decimal numeral system (used in common life), the number \"three\" in the binary numeral system (used in computers), and the number two in the unary numeral system.\n\nThe number the numeral represents is called its value.\n\nIdeally, a numeral system will:\n\nFor example, the usual decimal representation of whole numbers gives every nonzero whole number a unique representation as a finite sequence of digits, beginning with a non-zero digit. However, when decimal representation is used for the rational or real numbers, such numbers in general have an infinite number of representations, for example 2.31 can also be written as 2.310, 2.3100000, 2.309999999..., etc., all of which have the same meaning except for some scientific and other contexts where greater precision is implied by a larger number of figures shown.\n\nNumeral systems are sometimes called \"number systems\", but that name is ambiguous, as it could refer to different systems of numbers, such as the system of real numbers, the system of complex numbers, the system of \"p\"-adic numbers, etc. Such systems are, however, not the topic of this article.\n\nThe most commonly used system of numerals is the Hindu–Arabic numeral system. Two Indian mathematicians are credited with developing it. Aryabhata of Kusumapura developed the place-value notation in the 5th century and a century later Brahmagupta introduced the symbol for zero. The numeral system and the zero concept, developed by the Hindus in India, slowly spread to other surrounding countries due to their commercial and military activities with India. The Arabs adopted and modified it. Even today, the Arabs call the numerals which they use \"Rakam Al-Hind\" or the Hindu numeral system. The Arabs translated Hindu texts on numerology and spread them to the western world due to their trade links with them. The Western world modified them and called them the Arabic numerals, as they learned them from the Arabs. Hence the current western numeral system is the modified version of the Hindu numeral system developed in India. It also exhibits a great similarity to the Sanskrit–Devanagari notation, which is still used in India and neighbouring Nepal.\n\nThe simplest numeral system is the unary numeral system, in which every natural number is represented by a corresponding number of symbols. If the symbol / is chosen, for example, then the number seven would be represented by ///////. Tally marks represent one such system still in common use. The unary system is only useful for small numbers, although it plays an important role in theoretical computer science. Elias gamma coding, which is commonly used in data compression, expresses arbitrary-sized numbers by using unary to indicate the length of a binary numeral.\n\nThe unary notation can be abbreviated by introducing different symbols for certain new values. Very commonly, these values are powers of 10; so for instance, if / stands for one, − for ten and + for 100, then the number 304 can be compactly represented as +++ //// and the number 123 as + − − /// without any need for zero. This is called sign-value notation. The ancient Egyptian numeral system was of this type, and the Roman numeral system was a modification of this idea.\n\nMore useful still are systems which employ special abbreviations for repetitions of symbols; for example, using the first nine letters of the alphabet for these abbreviations, with A standing for \"one occurrence\", B \"two occurrences\", and so on, one could then write C+ D/ for the number 304. This system is used when writing Chinese numerals and other East Asian numerals based on Chinese. The number system of the English language is of this type (\"three hundred [and] four\"), as are those of other spoken languages, regardless of what written systems they have adopted. However, many languages use mixtures of bases, and other features, for instance 79 in French is \"soixante dix-neuf\" () and in Welsh is \"pedwar ar bymtheg a thrigain\" () or (somewhat archaic) \"pedwar ugain namyn un\" (). In English, one could say \"four score less one\", as in the famous Gettysburg Address representing \"87 years ago\" as \"four score and seven years ago\".\n\nMore elegant is a \"positional system\", also known as place-value notation. Again working in base 10, ten different digits 0, ..., 9 are used and the position of a digit is used to signify the power of ten that the digit is to be multiplied with, as in or more precisely . Note that zero, which is not needed in the other systems, is of crucial importance here, in order to be able to \"skip\" a power. The Hindu–Arabic numeral system, which originated in India and is now used throughout the world, is a positional base 10 system.\n\nArithmetic is much easier in positional systems than in the earlier additive ones; furthermore, additive systems need a large number of different symbols for the different powers of 10; a positional system needs only ten different symbols (assuming that it uses base 10).\n\nThe positional decimal system is presently universally used in human writing. The base 1000 is also used (albeit not universally), by grouping the digits and considering a sequence of three decimal digits as a single digit. This is the meaning of the common notation 1,000,234,567 used for very large numbers.\n\nIn computers, the main numeral systems are based on the positional system in base 2 (binary numeral system), with two binary digits, 0 and 1. Positional systems obtained by grouping binary digits by three (octal numeral system) or four (hexadecimal numeral system) are commonly used. For very large integers, bases 2 or 2 (grouping binary digits by 32 or 64, the length of the machine word) are used, as, for example, in GMP.\n\nIn certain biological systems the unary coding system is employed. Unary numerals used in the neural circuits responsible for birdsong production. The nucleus in the brain of the songbirds that plays a part in both the learning and the production of bird song is the HVC (high vocal center). The command signals for different notes in the birdsong emanate from different points in the HVC. This coding works as space coding which is an efficient strategy for biological circuits due to its inherent simplicity and robustness.\n\nThe numerals used when writing numbers with digits or symbols can be divided into two types that might be called the arithmetic numerals (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) and the geometric numerals (1, 10, 100, 1000, 10000 ...), respectively. The sign-value systems use only the geometric numerals and the positional systems use only the arithmetic numerals. A sign-value system does not need arithmetic numerals because they are made by repetition (except for the Ionic system), and a positional system does not need geometric numerals because they are made by position. However, the spoken language uses \"both\" arithmetic and geometric numerals.\n\nIn certain areas of computer science, a modified base \"k\" positional system is used, called bijective numeration, with digits 1, 2, ..., \"k\" (), and zero being represented by an empty string. This establishes a bijection between the set of all such digit-strings and the set of non-negative integers, avoiding the non-uniqueness caused by leading zeros. Bijective base-\"k\" numeration is also called \"k\"-adic notation, not to be confused with \"p\"-adic numbers. Bijective base 1 is the same as unary.\n\nIn a positional base \"b\" numeral system (with \"b\" a natural number greater than 1 known as the radix), \"b\" basic symbols (or digits) corresponding to the first \"b\" natural numbers including zero are used. To generate the rest of the numerals, the position of the symbol in the figure is used. The symbol in the last position has its own value, and as it moves to the left its value is multiplied by \"b\".\n\nFor example, in the decimal system (base 10), the numeral 4327 means , noting that .\n\nIn general, if \"b\" is the base, one writes a number in the numeral system of base \"b\" by expressing it in the form and writing the enumerated digits in descending order. The digits are natural numbers between 0 and , inclusive.\n\nIf a text (such as this one) discusses multiple bases, and if ambiguity exists, the base (itself represented in base 10) is added in subscript to the right of the number, like this: number. Unless specified by context, numbers without subscript are considered to be decimal.\n\nBy using a dot to divide the digits into two groups, one can also write fractions in the positional system. For example, the base 2 numeral 10.11 denotes .\n\nIn general, numbers in the base \"b\" system are of the form:\n\nThe numbers \"b\" and \"b\" are the weights of the corresponding digits. The position \"k\" is the logarithm of the corresponding weight \"w\", that is formula_2. The highest used position is close to the order of magnitude of the number.\n\nThe number of tally marks required in the unary numeral system for \"describing the weight\" would have been w. In the positional system, the number of digits required to describe it is only formula_3, for \"k\" ≥ 0. For example, to describe the weight 1000 then four digits are needed because formula_4. The number of digits required to \"describe the position\" is formula_5 (in positions 1, 10, 100... only for simplicity in the decimal example).\n\nNote that a number has a terminating or repeating expansion if and only if it is rational; this does not depend on the base. A number that terminates in one base may repeat in another (thus ). An irrational number stays aperiodic (with an infinite number of non-repeating digits) in all integral bases. Thus, for example in base 2, can be written as the aperiodic 11.001001000011111...\n\nPutting overscores, , or dots, \"ṅ\", above the common digits is a convention used to represent repeating rational expansions. Thus:\n\nIf \"b\" = \"p\" is a prime number, one can define base-\"p\" numerals whose expansion to the left never stops; these are called the \"p\"-adic numbers.\n\nMore general is using a mixed radix notation (here written little-endian) like formula_7 for formula_8, etc.\n\nThis is used in punycode, one aspect of which is the representation of a sequence of non-negative integers of arbitrary size in the form of a sequence without delimiters, of \"digits\" from a collection of 36: a–z and 0–9, representing 0–25 and 26–35 respectively. A digit lower than a threshold value marks that it is the most-significant digit, hence the end of the number. The threshold value depends on the position in the number. For example, if the threshold value for the first digit is b (i.e. 1) then a (i.e. 0) marks the end of the number (it has just one digit), so in numbers of more than one digit, range is only b–9 (1–35), therefore the weight \"b\" is 35 instead of 36. Suppose the threshold values for the second and third digits are c (2), then the third digit has a weight 34 × 35 = 1190 and we have the following sequence:\n\na (0), ba (1), ca (2), .., 9a (35), bb (36), cb (37), .., 9b (70), bca (71), .., 99a (1260), bcb (1261), etc.\n\nUnlike a regular based numeral system, there are numbers like 9b where 9 and b each represents 35; yet the representation is unique because ac and aca are not allowed – the a would terminate the number.\n\nThe flexibility in choosing threshold values allows optimization depending on the frequency of occurrence of numbers of various sizes.\n\nThe case with all threshold values equal to 1 corresponds to bijective numeration, where the zeros correspond to separators of numbers with digits which are non-zero.\n\n"}
{"id": "21173", "url": "https://en.wikipedia.org/wiki?curid=21173", "title": "Natural language", "text": "Natural language\n\nIn neuropsychology, linguistics, and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.\n\nThough the exact definition varies between scholars, natural language can broadly be defined in contrast to artificial or constructed languages (such as computer programming languages and international auxiliary languages) and to other communication systems in nature. Such examples include bees' waggle dance and whale song, to which researchers have found or applied the linguistic cognates of dialect and even syntax.\n\nAll language varieties of world languages are natural languages, although some varieties are subject to greater degrees of published prescriptivism or language regulation than others. Thus nonstandard dialects can be viewed as a wild type in comparison with standard languages. But even an official language with a regulating academy, such as Standard French with the French Academy, is classified as a natural language (for example, in the field of natural language processing), as its prescriptive points do not make it either constructed enough to be classified as a constructed language or controlled enough to be classified as a controlled natural language.\n\nControlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind the development and implementation of a controlled natural language typically is to aid non-native speakers of a natural language in understanding it, or to ease computer processing of a natural language. An example of a widely used controlled natural language is Simplified English, which was originally developed for aerospace industry maintenance manuals.\n\nLove\nConstructed international auxiliary languages such as Esperanto and Interlingua (even those that have native speakers) are not generally considered natural languages. Natural languages have been used to communicate and have evolved in a natural way, whereas Esperanto was designed by L.L. Zamenhof selecting elements from natural languages, not grown from natural fluctuations in vocabulary and syntax. Some natural languages have become naturally \"standardized\" by children's natural tendency to correct for illogical grammatical structures in their parents' speech, which can be seen in the development of pidgin languages into creole languages (as explained by Steven Pinker in \"The Language Instinct\"), but this is not the case in many languages, including constructed languages such as Esperanto, where strict rules are in place as an attempt to consciously remove such irregularities. The possible exception to this are true native speakers of such languages. More substantive basis for this designation is that the vocabulary, grammar, and orthography of Interlingua are natural; they have been standardized and presented by a linguistic research body, but they predated it and are not themselves considered a product of human invention. Most experts, however, consider Interlingua to be naturalistic rather than natural. Latino Sine Flexione, a second naturalistic auxiliary language, is also naturalistic in content but is no longer widely spoken.\n\n\n"}
{"id": "21174", "url": "https://en.wikipedia.org/wiki?curid=21174", "title": "Nanook of the North", "text": "Nanook of the North\n\nNanook of the North (also known as Nanook of the North: A Story Of Life and Love In the Actual Arctic) is a 1922 American silent documentary film by Robert J. Flaherty, with elements of docudrama, at a time when the concept of separating films into documentary and drama did not yet exist.\n\nIn the tradition of what would later be called salvage ethnography, Flaherty captured the struggles of the Inuk man named Nanook and his family in the Canadian Arctic. Some have criticized Flaherty for staging several sequences, but the film is generally viewed as standing \"alone in its stark regard for the courage and ingenuity of its heroes.\"\n\nIn 1989, \"Nanook of the North\" was one of the first 25 films to be selected for preservation in the United States National Film Registry by the Library of Congress as being \"culturally, historically, or aesthetically significant\".\n\nThe documentary follows the lives of an Inuk, Nanook, and his family as they travel, search for food, and trade in the Ungava Peninsula of northern Quebec, Canada. Nanook; his wife, Nyla; and their family are introduced as fearless heroes who endure rigors no other race could survive. The audience sees Nanook, often with his family, hunt a walrus, build an igloo, go about his day, and perform other tasks.\n\nIn 1910 Flaherty was hired as an explorer and prospector along the Hudson Bay for the Canadian Pacific Railway. Learning about the lands and people there, Flaherty decided to bring a camera with him on his third expedition in 1913, but knowing nothing about film, Flaherty took a three-week course on cinematography in Rochester, New York\n\nUsing a Bell & Howell camera, a portable developing and printing machine, and some lighting equipment, Flaherty spent 1914 and 1915 shooting hours of film of Inuit life. By 1916, Flaherty had enough footage that he began test screenings and was met with wide enthusiasm. However, in 1916, Flaherty dropped a cigarette onto the original camera negative (which was highly flammable nitrate stock) and lost 30,000 feet of film. With his first attempt ruined, Flaherty decided to not only return for new footage, but also to refocus the film on one Inuit family as he felt his earlier footage was too much of travelogue. Spending four years raising money, Flaherty was eventually funded by French fur company Revillon Frères and returned to the North and shot from August 1920 to August 1921. As a main character, Flaherty chose the celebrated hunter of the Itivimuit tribe, Allakariallak. The full collaboration of the Inuit was key to Flaherty's success as the Inuit were his film crew and many of them knew his camera better than he did.\n\nFlaherty has been criticized for deceptively portraying staged events as reality. \"Nanook\" was in fact named Allakariallak (), while the \"wife\" shown in the film was not really his wife. According to Charlie Nayoumealuk, who was interviewed in \"Nanook Revisited\" (1990), \"the two women in \"Nanook\" - Nyla (Alice [?] Nuvalinga) and Cunayou (whose real name we do not know) were not Allakariallak's wives, but were in fact common-law wives of Flaherty.\" And although Allakariallak normally used a gun when hunting, Flaherty encouraged him to hunt after the fashion of his recent ancestors in order to capture the way the Inuit lived before European influence. On the other hand, while Flaherty made his Inuit actors use spears instead of guns during the walrus and seal hunts, the prey shown in the film were genuine, wild animals. Flaherty also exaggerated the peril to Inuit hunters with his claim, often repeated, that Allakariallak had died of starvation two years after the film was completed, whereas in fact he died at home, likely of tuberculosis.\n\nThe building of the igloo is one of the most celebrated sequences in the film, but interior photography presented a problem. Building an igloo large enough for a camera to enter resulted in the dome collapsing, and when they finally succeeded in making the igloo it was too dark for photography. Instead, the images of the inside of the igloo in the film were actually shot in a special three-walled igloo for Flaherty's bulky camera so that there would be enough light for it to capture interior shots.\n\nIn the \"Trade Post of the White Man\" scene, Nanook and his family arrive in a kayak at the trading post and one family member after another emerge from a small kayak, akin to a clown car at the circus. Going to trade his hunt from the year, including the skins of foxes, seals, and polar bears, Nanook comes in contact with the white man and there is a funny interaction as the two cultures meet. The trader plays music on a gramophone and tries to explain how a man 'cans' his voice. Bending forward and staring at the machine, Nanook puts his ear closer as the trader cranks the mechanism again. The trader removes the record and hands it to Nanook who at first peers at it and then puts it in his mouth and bites it. The scene is meant to be a comical one as the audience laughs at the naivete of Nanook and people isolated from Western culture. In truth, the scene was entirely scripted and Allakariallak knew what a gramophone was.\n\nIt has been noted that in the 1920s when Nanook was filmed, the Inuit had already begun integrating the use of Western clothing and were using rifles to hunt rather than harpoons, but this does not negate that the Inuit knew how to make traditional clothing from animals found in their environment, could still fashion traditional weapons and were perfectly able to make use of them if found to be preferable for a given situation.\nFlaherty defended his work by stating, \"one often has to distort a thing in order to catch its true spirit.\" Later filmmakers have pointed out that the only cameras available to Flaherty at the time were both large and immobile, making it impossible to effectively capture most interior shots or unstructured exterior scenes without significantly modifying the environment and subject action.\n\nAs the first \"nonfiction\" work of its scale, \"Nanook of the North\" was ground-breaking cinema. It captured many authentic details of a culture little-known to outsiders, and was filmed in a remote location. Hailed almost unanimously by critics, the film was a box office success in the United States and abroad. In the following years, many others would try to follow in Flaherty's success with \"primitive peoples\" films. In 2005 film critic Roger Ebert described the film's central figure, Nanook, as \"one of the most vital and unforgettable human beings ever recorded on film.\" In a 2014 \"Sight and Sound\" poll, film critics voted \"Nanook of the North\" the seventh best documentary film of all time.\n\nAt the time, few documentaries had been filmed and there was little precedent to guide Flaherty's work. Since Flaherty's time, staging, attempting to steer documentary action, or presenting re-enactment as naturally captured footage has come to be considered unethical.\n\nIn its earliest years (approx. 1895–1902), film production was dominated by actualities—short pictures of real people in real places. Robert Flaherty's great innovation was simply to combine the two forms of actuality, infusing the exotic journey with the details of indigenous work and play and life.\n\nIn 1999, \"Nanook of the North\" was digitally remastered and released on DVD by The Criterion Collection. It includes an interview with Flaherty's widow (and \"Nanook of the North\" co-editor), Frances Flaherty, photos from Flaherty's trip to the arctic, and excerpts from a TV documentary, \"Flaherty and Film.\" In 2013, Flicker Alley released a remastered Blu-ray version that includes six other arctic films.\n\n\n\n\n\n"}
{"id": "21175", "url": "https://en.wikipedia.org/wiki?curid=21175", "title": "Nitrogen", "text": "Nitrogen\n\nNitrogen is a chemical element with symbol N and atomic number 7. It was first discovered and isolated by Scottish physician Daniel Rutherford in 1772. Although Carl Wilhelm Scheele and Henry Cavendish had independently done so at about the same time, Rutherford is generally accorded the credit because his work was published first. The name \"nitrogène\" was suggested by French chemist Jean-Antoine-Claude Chaptal in 1790, when it was found that nitrogen was present in nitric acid and nitrates. Antoine Lavoisier suggested instead the name azote, from the Greek άζωτικός \"no life\", as it is an asphyxiant gas; this name is instead used in many languages, such as French, Russian, and Turkish, and appears in the English names of some nitrogen compounds such as hydrazine, azides and azo compounds.\n\nNitrogen is the lightest member of group 15 of the periodic table, often called the pnictogens. The name comes from the Greek πνίγειν \"to choke\", directly referencing nitrogen's asphyxiating properties. It is a common element in the universe, estimated at about seventh in total abundance in the Milky Way and the Solar System. At standard temperature and pressure, two atoms of the element bind to form dinitrogen, a colourless and odorless diatomic gas with the formula N. Dinitrogen forms about 78% of Earth's atmosphere, making it the most abundant uncombined element. Nitrogen occurs in all organisms, primarily in amino acids (and thus proteins), in the nucleic acids (DNA and RNA) and in the energy transfer molecule adenosine triphosphate. The human body contains about 3% nitrogen by mass, the fourth most abundant element in the body after oxygen, carbon, and hydrogen. The nitrogen cycle describes movement of the element from the air, into the biosphere and organic compounds, then back into the atmosphere.\n\nMany industrially important compounds, such as ammonia, nitric acid, organic nitrates (propellants and explosives), and cyanides, contain nitrogen. The extremely strong triple bond in elemental nitrogen (N≡N), the second strongest bond in any diatomic molecule after carbon monoxide (CO), dominates nitrogen chemistry. This causes difficulty for both organisms and industry in converting N into useful compounds, but at the same time means that burning, exploding, or decomposing nitrogen compounds to form nitrogen gas releases large amounts of often useful energy. Synthetically produced ammonia and nitrates are key industrial fertilisers, and fertiliser nitrates are key pollutants in the eutrophication of water systems.\n\nApart from its use in fertilisers and energy-stores, nitrogen is a constituent of organic compounds as diverse as Kevlar used in high-strength fabric and cyanoacrylate used in superglue. Nitrogen is a constituent of every major pharmacological drug class, including antibiotics. Many drugs are mimics or prodrugs of natural nitrogen-containing signal molecules: for example, the organic nitrates nitroglycerin and nitroprusside control blood pressure by metabolizing into nitric oxide. Many notable nitrogen-containing drugs, such as the natural caffeine and morphine or the synthetic amphetamines, act on receptors of animal neurotransmitters.\n\nNitrogen compounds have a very long history, ammonium chloride having been known to Herodotus. They were well known by the Middle Ages. Alchemists knew nitric acid as \"aqua fortis\" (strong water), as well as other nitrogen compounds such as ammonium salts and nitrate salts. The mixture of nitric and hydrochloric acids was known as \"aqua regia\" (royal water), celebrated for its ability to dissolve gold, the king of metals.\n\nThe discovery of nitrogen is attributed to the Scottish physician Daniel Rutherford in 1772, who called it \"noxious air\". Though he did not recognise it as an entirely different chemical substance, he clearly distinguished it from Joseph Black's \"fixed air\", or carbon dioxide. The fact that there was a component of air that does not support combustion was clear to Rutherford, although he was not aware that it was an element. Nitrogen was also studied at about the same time by Carl Wilhelm Scheele, Henry Cavendish, and Joseph Priestley, who referred to it as \"burnt air\" or \"phlogisticated air\". Nitrogen gas was inert enough that Antoine Lavoisier referred to it as \"mephitic air\" or \"azote\", from the Greek word (azotikos), \"no life\". In an atmosphere of pure nitrogen, animals died and flames were extinguished. Though Lavoisier's name was not accepted in English, since it was pointed out that almost all gases (indeed, with the sole exception of oxygen) are mephitic, it is used in many languages (French, Italian, Portuguese, Polish, Russian, Albanian, Turkish, etc.; the German \"Stickstoff\" similarly refers to the same characteristic, viz. \"sticken\" \"to choke or suffocate\") and still remains in English in the common names of many nitrogen compounds, such as hydrazine and compounds of the azide ion. Finally, it led to the name \"pnictogens\" for the group headed by nitrogen, from the Greek πνίγειν \"to choke\".\n\nThe English word nitrogen (1794) entered the language from the French \"nitrogène\", coined in 1790 by French chemist Jean-Antoine Chaptal (1756–1832), from the French \"nitre\" (potassium nitrate, also called saltpeter) and the French suffix \"-gène\", \"producing\", from the Greek -γενής (-genes, \"begotten\"). Chaptal's meaning was that nitrogen is the essential part of nitric acid, which in turn was produced from niter. In earlier times, niter had been confused with Egyptian \"natron\" (sodium carbonate) – called νίτρον (nitron) in Greek – which, despite the name, contained no nitrate.\n\nThe earliest military, industrial, and agricultural applications of nitrogen compounds used saltpeter (sodium nitrate or potassium nitrate), most notably in gunpowder, and later as fertiliser. In 1910, Lord Rayleigh discovered that an electrical discharge in nitrogen gas produced \"active nitrogen\", a monatomic allotrope of nitrogen. The \"whirling cloud of brilliant yellow light\" produced by his apparatus reacted with mercury to produce explosive mercury nitride.\n\nFor a long time, sources of nitrogen compounds were limited. Natural sources originated either from biology or deposits of nitrates produced by atmospheric reactions. Nitrogen fixation by industrial processes like the Frank–Caro process (1895–1899) and Haber–Bosch process (1908–1913) eased this shortage of nitrogen compounds, to the extent that half of global food production (see Applications) now relies on synthetic nitrogen fertilisers. At the same time, use of the Ostwald process (1902) to produce nitrates from industrial nitrogen fixation allowed the large-scale industrial production of nitrates as feedstock in the manufacture of explosives in the World Wars of the 20th century.\n\nA nitrogen atom has seven electrons. In the ground state, they are arranged in the electron configuration 1s2s2p2p2p. It therefore has five valence electrons in the 2s and 2p orbitals, three of which (the p-electrons) are unpaired. It has one of the highest electronegativities among the elements (3.04 on the Pauling scale), exceeded only by chlorine (3.16), oxygen (3.44), and fluorine (3.98). Following periodic trends, its single-bond covalent radius of 71 pm is smaller than those of boron (84 pm) and carbon (76 pm), while it is larger than those of oxygen (66 pm) and fluorine (57 pm). The nitride anion, N, is much larger at 146 pm, similar to that of the oxide (O: 140 pm) and fluoride (F: 133 pm) anions. The first three ionisation energies of nitrogen are 1.402, 2.856, and 4.577 MJ·mol, and the sum of the fourth and fifth is 16.920 MJ·mol. Due to these very high figures, nitrogen has no simple cationic chemistry.\n\nThe lack of radial nodes in the 2p subshell is directly responsible for many of the anomalous properties of the first row of the p-block, especially in nitrogen, oxygen, and fluorine. The 2p subshell is very small and has a very similar radius to the 2s shell, facilitating orbital hybridisation. It also results in very large electrostatic forces of attraction between the nucleus and the valence electrons in the 2s and 2p shells, resulting in very high electronegativities. Hypervalency is almost unknown in the 2p elements for the same reason, because the high electronegativity makes it difficult for a small nitrogen atom to be a central atom in an electron-rich three-center four-electron bond since it would tend to attract the electrons strongly to itself. Thus, despite nitrogen's position at the head of group 15 in the periodic table, its chemistry shows huge differences from that of its heavier congeners phosphorus, arsenic, antimony, and bismuth.\n\nNitrogen may be usefully compared to its horizontal neighbours carbon and oxygen as well as its vertical neighbours in the pnictogen column (phosphorus, arsenic, antimony, and bismuth). Although each period 2 element from lithium to nitrogen shows some similarities to the period 3 element in the next group from magnesium to sulfur (known as the diagonal relationships), their degree drops off quite abruptly past the boron–silicon pair, so that the similarities of nitrogen to sulfur are mostly limited to sulfur nitride ring compounds when both elements are the only ones present. Nitrogen resembles oxygen far more than it does carbon with its high electronegativity and concomitant capability for hydrogen bonding and the ability to form coordination complexes by donating its lone pairs of electrons. It does not share carbon's proclivity for catenation, with the longest chain of nitrogen yet discovered being composed of only eight nitrogen atoms (PhN=N–N(Ph)–N=N–N(Ph)–N=NPh). One property nitrogen does share with both its horizontal neighbours is its preferentially forming multiple bonds, typically with carbon, nitrogen, or oxygen atoms, through p–p interactions; thus, for example, nitrogen occurs as diatomic molecules and thus has very much lower melting (−210 °C) and boiling points (−196 °C) than the rest of its group, as the N molecules are only held together by weak van der Waals interactions and there are very few electrons available to create significant instantaneous dipoles. This is not possible for its vertical neighbours; thus, the nitrogen oxides, nitrites, nitrates, nitro-, nitroso-, azo-, and diazo-compounds, azides, cyanates, thiocyanates, and imino-derivatives find no echo with phosphorus, arsenic, antimony, or bismuth. By the same token, however, the complexity of the phosphorus oxoacids finds no echo with nitrogen.\n\nNitrogen has two stable isotopes: N and N. The first is much more common, making up 99.634% of natural nitrogen, and the second (which is slightly heavier) makes up the remaining 0.366%. This leads to an atomic weight of around 14.007 u. Both of these stable isotopes are produced in the CNO cycle in stars, but N is more common as its neutron capture is the rate-limiting step. N is one of the five stable odd–odd nuclides (a nuclide having an odd number of protons and neutrons); the other four are H, Li, B, and Ta.\n\nThe relative abundance of N and N is practically constant in the atmosphere but can vary elsewhere, due to natural isotopic fractionation from biological redox reactions and the evaporation of natural ammonia or nitric acid. Biologically mediated reactions (e.g., assimilation, nitrification, and denitrification) strongly control nitrogen dynamics in the soil. These reactions typically result in N enrichment of the substrate and depletion of the product.\n\nThe heavy isotope N was first discovered by S. M. Naudé in 1929, soon after heavy isotopes of the neighbouring elements oxygen and carbon were discovered. It presents one of the lowest thermal neutron capture cross-sections of all isotopes. It is frequently used in nuclear magnetic resonance (NMR) spectroscopy to determine the structures of nitrogen-containing molecules, due to its fractional nuclear spin of one-half, which offers advantages for NMR such as narrower line width. N, though also theoretically usable, has an integer nuclear spin of one and thus has a quadrupole moment that leads to wider and less useful spectra. N NMR nevertheless has complications not encountered in the more common H and C NMR spectroscopy. The low natural abundance of N (0.36%) significantly reduces sensitivity, a problem which is only exacerbated by its low gyromagnetic ratio, (only 10.14% that of H). As a result, the signal-to-noise ratio for H is about 300 times as much as that for N at the same magnetic field strength. This may be somewhat alleviated by isotopic enrichment of N by chemical exchange or fractional distillation. N-enriched compounds have the advantage that under standard conditions, they do not undergo chemical exchange of their nitrogen atoms with atmospheric nitrogen, unlike compounds with labelled hydrogen, carbon, and oxygen isotopes that must be kept away from the atmosphere. The N:N ratio is commonly used in stable isotope analysis in the fields of geochemistry, hydrology, paleoclimatology and paleoceanography, where it is called \"δ\"N.\n\nOf the ten other isotopes produced synthetically, ranging from N to N, N has a half-life of ten minutes and the remaining isotopes have half-lives on the order of seconds (N and N) or even milliseconds. No other nitrogen isotopes are possible as they would fall outside the nuclear drip lines, leaking out a proton or neutron. Given the half-life difference, N is the most important nitrogen radioisotope, being relatively long-lived enough to use in positron emission tomography (PET), although its half-life is still short and thus it must be produced at the venue of the PET, for example in a cyclotron via proton bombardment of O producing N and an alpha particle.\n\nThe radioisotope N is the dominant radionuclide in the coolant of pressurised water reactors or boiling water reactors during normal operation, and thus it is a sensitive and immediate indicator of leaks from the primary coolant system to the secondary steam cycle, and is the primary means of detection for such leaks. It is produced from O (in water) via an (n,p) reaction in which the O atom captures a neutron and expels a proton. It has a short half-life of about 7.1 s, but during its decay back to O produces high-energy gamma radiation (5 to 7 MeV). Because of this, access to the primary coolant piping in a pressurised water reactor must be restricted during reactor power operation.\n\nAtomic nitrogen, also known as active nitrogen, is highly reactive, being a triradical with three unpaired electrons. Free nitrogen atoms easily react with most elements to form nitrides, and even when two free nitrogen atoms collide to produce an excited N molecule, they may release so much energy on collision with even such stable molecules as carbon dioxide and water to cause homolytic fission into radicals such as CO and O or OH and H. Atomic nitrogen is prepared by passing an electric discharge through nitrogen gas at 0.1–2 mmHg, which produces atomic nitrogen along with a peach-yellow emission that fades slowly as an afterglow for several minutes even after the discharge terminates.\n\nGiven the great reactivity of atomic nitrogen, elemental nitrogen usually occurs as molecular N, dinitrogen. This molecule is a colourless, odourless, and tasteless diamagnetic gas at standard conditions: it melts at −210 °C and boils at −196 °C. Dinitrogen is mostly unreactive at room temperature, but it will nevertheless react with lithium metal and some transition metal complexes. This is due to its bonding, which is unique among the diatomic elements at standard conditions in that it has an N≡N triple bond. Triple bonds have short bond lengths (in this case, 109.76 pm) and high dissociation energies (in this case, 945.41 kJ/mol), and are thus very strong, explaining dinitrogen's chemical inertness.\n\nThere are some theoretical indications that other nitrogen oligomers and polymers may be possible. If they could be synthesised, they may have potential applications as materials with a very high energy density, that could be used as powerful propellants or explosives. This is because they should all decompose to dinitrogen, whose N≡N triple bond (bond energy 946 kJ⋅mol) is much stronger than those of the N=N double bond (418 kJ⋅mol) or the N–N single bond (160 kJ⋅mol): indeed, the triple bond has more than thrice the energy of the single bond. (The opposite is true for the heavier pnictogens, which prefer polyatomic allotropes.) A great disadvantage is that most neutral polynitrogens are not expected to have a large barrier towards decomposition, and that the few exceptions would be even more challenging to synthesise than the long-sought but still unknown tetrahedrane. This stands in contrast to the well-characterised cationic and anionic polynitrogens azide (), pentazenium (), and pentazolide (cyclic aromatic ). Under extremely high pressures (1.1 million atm) and high temperatures (2000 K), as produced in a diamond anvil cell, nitrogen polymerises into the single-bonded cubic gauche crystal structure. This structure is similar to that of diamond, and both have extremely strong covalent bonds, resulting in its nickname \"nitrogen diamond\".\nAt atmospheric pressure, molecular nitrogen condenses (liquefies) at 77 K (−195.79 °C) and freezes at 63 K (−210.01 °C) into the beta hexagonal close-packed crystal allotropic form. Below 35.4 K (−237.6 °C) nitrogen assumes the cubic crystal allotropic form (called the alpha phase). Liquid nitrogen, a colourless fluid resembling water in appearance, but with 80.8% of the density (the density of liquid nitrogen at its boiling point is 0.808 g/mL), is a common cryogen. Solid nitrogen has many crystalline modifications. It forms a significant dynamic surface coverage on Pluto and outer moons of the Solar System such as Triton. Even at the low temperatures of solid nitrogen it is fairly volatile and can sublime to form an atmosphere, or condense back into nitrogen frost. It is very weak and flows in the form of glaciers and on Triton geysers of nitrogen gas come from the polar ice cap region.\n\nThe first example of a dinitrogen complex to be discovered was [Ru(NH)(N)] (see figure at right), and soon many other such complexes were discovered. These complexes, in which a nitrogen molecule donates at least one lone pair of electrons to a central metal cation, illustrate how N might bind to the metal(s) in nitrogenase and the catalyst for the Haber process: these processes involving dinitrogen activation are vitally important in biology and in the production of fertilisers.\n\nDinitrogen is able to coordinate to metals in five different ways. The more well-characterised ways are the end-on M←N≡N (\"η\") and M←N≡N→M (\"μ\", bis-\"η\"), in which the lone pairs on the nitrogen atoms are donated to the metal cation. The less well-characterised ways involve dinitrogen donating electron pairs from the triple bond, either as a bridging ligand to two metal cations (\"μ\", bis-\"η\") or to just one (\"η\"). The fifth and unique method involves triple-coordination as a bridging ligand, donating all three electron pairs from the triple bond (\"μ\"-N). A few complexes feature multiple N ligands and some feature N bonded in multiple ways. Since N is isoelectronic with carbon monoxide (CO) and acetylene (CH), the bonding in dinitrogen complexes is closely allied to that in carbonyl compounds, although N is a weaker \"σ\"-donor and \"π\"-acceptor than CO. Theoretical studies show that \"σ\" donation is a more important factor allowing the formation of the M–N bond than \"π\" back-donation, which mostly only weakens the N–N bond, and end-on (\"η\") donation is more readily accomplished than side-on (\"η\") donation.\n\nToday, dinitrogen complexes are known for almost all the transition metals, accounting for several hundred compounds. They are normally prepared by three methods:\nOccasionally the N≡N bond may be formed directly within a metal complex, for example by directly reacting coordinated ammonia (NH) with nitrous acid (HNO), but this is not generally applicable. Most dinitrogen complexes have colours within the range white-yellow-orange-red-brown; a few exceptions are known, such as the blue [{Ti(\"η\"-CH)}-(N)].\n\nNitrogen bonds to almost all the elements in the periodic table except the first three noble gases, helium, neon, and argon, and some of the very short-lived elements after bismuth, creating an immense variety of binary compounds with varying properties and applications. Many binary compounds are known: with the exception of the nitrogen hydrides, oxides, and fluorides, these are typically called nitrides. Many stoichiometric phases are usually present for most elements (e.g. MnN, MnN, MnN, MnN, MnN, and MnN for 9.2 < \"x\" < 25.3). They may be classified as \"salt-like\" (mostly ionic), covalent, \"diamond-like\", and metallic (or interstitial), although this classification has limitations generally stemming from the continuity of bonding types instead of the discrete and separate types that it implies. They are normally prepared by directly reacting a metal with nitrogen or ammonia (sometimes after heating), or by thermal decomposition of metal amides:\nMany variants on these processes are possible.The most ionic of these nitrides are those of the alkali metals and alkaline earth metals, LiN (Na, K, Rb, and Cs do not form stable nitrides for steric reasons) and MN (M = Be, Mg, Ca, Sr, Ba). These can formally be thought of as salts of the N anion, although charge separation is not actually complete even for these highly electropositive elements. However, the alkali metal azides NaN and KN, featuring the linear anion, are well-known, as are Sr(N) and Ba(N). Azides of the B-subgroup metals (those in groups 11 through 16) are much less ionic, have more complicated structures, and detonate readily when shocked.\nMany covalent binary nitrides are known. Examples include cyanogen ((CN)), triphosphorus pentanitride (PN), disulfur dinitride (SN), and tetrasulfur tetranitride (SN). The essentially covalent silicon nitride (SiN) and germanium nitride (GeN) are also known: silicon nitride in particular would make a promising ceramic if not for the difficulty of working with and sintering it. In particular, the group 13 nitrides, most of which are promising semiconductors, are isoelectronic with graphite, diamond, and silicon carbide and have similar structures: their bonding changes from covalent to partially ionic to metallic as the group is descended. In particular, since the B–N unit is isoelectronic to C–C, and carbon is essentially intermediate in size between boron and nitrogen, much of organic chemistry finds an echo in boron–nitrogen chemistry, such as in borazine (\"inorganic benzene\"). Nevertheless, the analogy is not exact due to the ease of nucleophilic attack at boron due to its deficiency in electrons, which is not possible in a wholly carbon-containing ring.\n\nThe largest category of nitrides are the interstitial nitrides of formulae MN, MN, and MN (although variable composition is perfectly possible), where the small nitrogen atoms are positioned in the gaps in a metallic cubic or hexagonal close-packed lattice. They are opaque, very hard, and chemically inert, melting only at very high temperatures (generally over 2500 °C). They have a metallic lustre and conduct electricity as do metals. They hydrolyse only very slowly to give ammonia or nitrogen.\n\nThe nitride anion (N) is the strongest \"π\" donor known amongst ligands (the second-strongest is O). Nitrido complexes are generally made by thermal decomposition of azides or by deprotonating ammonia, and they usually involve a terminal {≡N} group. The linear azide anion (), being isoelectronic with nitrous oxide, carbon dioxide, and cyanate, forms many coordination complexes. Further catenation is rare, although (isoelectronic with carbonate and nitrate) is known.\n\nIndustrially, ammonia (NH) is the most important compound of nitrogen and is prepared in larger amounts than any other compound, because it contributes significantly to the nutritional needs of terrestrial organisms by serving as a precursor to food and fertilisers. It is a colourless alkaline gas with a characteristic pungent smell. The presence of hydrogen bonding has very significant effects on ammonia, conferring on it its high melting (−78 °C) and boiling (−33 °C) points. As a liquid, it is a very good solvent with a high heat of vaporisation (enabling it to be used in vacuum flasks), that also has a low viscosity and electrical conductivity and high dielectric constant, and is less dense than water. However, the hydrogen bonding in NH is weaker than that in HO due to the lower electronegativity of nitrogen compared to oxygen and the presence of only one lone pair in NH rather than two in HO. It is a weak base in aqueous solution (p\"K\" 4.74); its conjugate acid is ammonium, . It can also act as an extremely weak acid, losing a proton to produce the amide anion, . It thus undergoes self-dissociation, similar to water, to produce ammonium and amide. Ammonia burns in air or oxygen, though not readily, to produce nitrogen gas; it burns in fluorine with a greenish-yellow flame to give nitrogen trifluoride. Reactions with the other nonmetals are very complex and tend to lead to a mixture of products. Ammonia reacts on heating with metals to give nitrides.\n\nMany other binary nitrogen hydrides are known, but the most important are hydrazine (NH) and hydrogen azide (HN). Although it is not a nitrogen hydride, hydroxylamine (NHOH) is similar in properties and structure to ammonia and hydrazine as well. Hydrazine is a fuming, colourless liquid that smells similarly to ammonia. Its physical properties are very similar to those of water (melting point 2.0 °C, boiling point 113.5 °C, density 1.00 g/cm). Despite it being an endothermic compound, it is kinetically stable. It burns quickly and completely in air very exothermically to give nitrogen and water vapour. It is a very useful and versatile reducing agent and is a weaker base than ammonia. It is also commonly used as a rocket fuel.\n\nHydrazine is generally made by reaction of ammonia with alkaline sodium hypochlorite in the presence of gelatin or glue:\n(The attacks by hydroxide and ammonia may be reversed, thus passing through the intermediate NHCl instead.) The reason for adding gelatin is that it removes metal ions such as Cu that catalyses the destruction of hydrazine by reaction with chloramine (NHCl) to produce ammonium chloride and nitrogen.\n\nHydrogen azide (HN) was first produced in 1890 by the oxidation of aqueous hydrazine by nitrous acid. It is very explosive and even dilute solutions can be dangerous. It has a disagreeable and irritating smell and is a potentially lethal (but not cumulative) poison. It may be considered the conjugate acid of the azide anion, and is similarly analogous to the hydrohalic acids.\n\nAll four simple nitrogen trihalides are known. A few mixed halides and hydrohalides are known, but are mostly unstable and uninteresting: examples include NClF, NClF, NBrF, NFH, NClH, and NClH.\n\nFive nitrogen fluorides are known. Nitrogen trifluoride (NF, first prepared in 1928) is a colourless and odourless gas that is thermodynamically stable, and most readily produced by the electrolysis of molten ammonium fluoride dissolved in anhydrous hydrogen fluoride. Like carbon tetrafluoride, it is not at all reactive and is stable in water or dilute aqueous acids or alkalis. Only when heated does it act as a fluorinating agent, and it reacts with copper, arsenic, antimony, and bismuth on contact at high temperatures to give tetrafluorohydrazine (NF). The cations and are also known (the latter from reacting tetrafluorohydrazine with strong fluoride-acceptors such as arsenic pentafluoride), as is ONF, which has aroused interest due to the short N–O distance implying partial double bonding and the highly polar and long N–F bond. Tetrafluorohydrazine, unlike hydrazine itself, can dissociate at room temperature and above to give the radical NF•. Fluorine azide (FN) is very explosive and thermally unstable. Dinitrogen difluoride (NF) exists as thermally interconvertible \"cis\" and \"trans\" isomers, and was first found as a product of the thermal decomposition of FN.\n\nNitrogen trichloride (NCl) is a dense, volatile, and explosive liquid whose physical properties are similar to those of carbon tetrachloride, although one difference is that NCl is easily hydrolysed by water while CCl is not. It was first synthesised in 1811 by Pierre Louis Dulong, who lost three fingers and an eye to its explosive tendencies. As a dilute gas it is less dangerous and is thus used industrially to bleach and sterilise flour. Nitrogen tribromide (NBr), first prepared in 1975, is a deep red, temperature-sensitive, volatile solid that is explosive even at −100 °C. Nitrogen triiodide (NI) is still more unstable and was only prepared in 1990. Its adduct with ammonia, which was known earlier, is very shock-sensitive: it can be set off by the touch of a feather, shifting air currents, or even alpha particles. For this reason, small amounts of nitrogen triiodide are sometimes synthesised as a demonstration to high school chemistry students or as an act of \"chemical magic\". Chlorine azide (ClN) and bromine azide (BrN) are extremely sensitive and explosive.\n\nTwo series of nitrogen oxohalides are known: the nitrosyl halides (XNO) and the nitryl halides (XNO). The first are very reactive gases that can be made by directly halogenating nitrous oxide. Nitrosyl fluoride (NOF) is colourless and a vigorous fluorinating agent. Nitrosyl chloride (NOCl) behaves in much the same way and has often been used as an ionising solvent. Nitrosyl bromide (NOBr) is red. The reactions of the nitryl halides are mostly similar: nitryl fluoride (FNO) and nitryl chloride (ClNO) are likewise reactive gases and vigorous halogenating agents.\n\nNitrogen forms nine molecular oxides, some of which were the first gases to be identified: NO (nitrous oxide), NO (nitric oxide), NO (dinitrogen trioxide), NO (nitrogen dioxide), NO (dinitrogen tetroxide), NO (dinitrogen pentoxide), NO (nitrosylazide), and N(NO) (trinitramide). All are thermally unstable towards decomposition to their elements. One other possible oxide that has not yet been synthesised is oxatetrazole (NO), an aromatic ring.\n\nNitrous oxide (NO), better known as laughing gas, is made by thermal decomposition of molten ammonium nitrate at 250 °C. This is a redox reaction and thus nitric oxide and nitrogen are also produced as byproducts. It is mostly used as a propellant and aerating agent for sprayed canned whipped cream, and was formerly commonly used as an anaesthetic. Despite appearances, it cannot be considered to be the anhydride of hyponitrous acid (HNO) because that acid is not produced by the dissolution of nitrous oxide in water. It is rather unreactive (not reacting with the halogens, the alkali metals, or ozone at room temperature, although reactivity increases upon heating) and has the unsymmetrical structure N–N–O (N≡NO↔N=N=O): above 600 °C it dissociates by breaking the weaker N–O bond.\n\nNitric oxide (NO) is the simplest stable molecule with an odd number of electrons. In mammals, including humans, it is an important cellular signaling molecule involved in many physiological and pathological processes. It is formed by catalytic oxidation of ammonia. It is a colourless paramagnetic gas that, being thermodynamically unstable, decomposes to nitrogen and oxygen gas at 1100–1200 °C. Its bonding is similar to that in nitrogen, but one extra electron is added to a \"π\"* antibonding orbital and thus the bond order has been reduced to approximately 2.5; hence dimerisation to O=N–N=O is unfavourable except below the boiling point (where the \"cis\" isomer is more stable) because it does not actually increase the total bond order and because the unpaired electron is delocalised across the NO molecule, granting it stability. There is also evidence for the asymmetric red dimer O=N–O=N when nitric oxide is condensed with polar molecules. It reacts with oxygen to give brown nitrogen dioxide and with halogens to give nitrosyl halides. It also reacts with transition metal compounds to give nitrosyl complexes, most of which are deeply coloured.\n\nBlue dinitrogen trioxide (NO) is only available as a solid because it rapidly dissociates above its melting point to give nitric oxide, nitrogen dioxide (NO), and dinitrogen tetroxide (NO). The latter two compounds are somewhat difficult to study individually because of the equilibrium between them. although sometimes dinitrogen tetroxide can react by heterolytic fission to nitrosonium and nitrate in a medium with high dielectric constant. Nitrogen dioxide is an acrid, corrosive brown gas. Both compounds may be easily prepared by decomposing a dry metal nitrate. Both react with water to form nitric acid. Dinitrogen tetroxide is very useful for the preparation of anhydrous metal nitrates and nitrato complexes, and it became the storable oxidiser of choice for many rockets in both the United States and USSR by the late 1950s. This is because it is a hypergolic propellant in combination with a hydrazine-based rocket fuel and can be easily stored since it is liquid at room temperature.\n\nThe thermally unstable and very reactive dinitrogen pentoxide (NO) is the anhydride of nitric acid, and can be made from it by dehydration with phosphorus pentoxide. It is of interest for the preparation of explosives. It is a deliquescent, colourless crystalline solid that is sensitive to light. In the solid state it is ionic with structure [NO][NO]; as a gas and in solution it is molecular ON–O–NO. Hydration to nitric acid comes readily, as does analogous reaction with hydrogen peroxide giving peroxonitric acid (HOONO). It is a violent oxidising agent. Gaseous dinitrogen pentoxide decomposes as follows:\n\nMany nitrogen oxoacids are known, though most of them are unstable as pure compounds and are known only as aqueous solution or as salts. Hyponitrous acid (HNO) is a weak diprotic acid with the structure HON=NOH (p\"K\" 6.9, p\"K\" 11.6). Acidic solutions are quite stable but above pH 4 base-catalysed decomposition occurs via [HONNO] to nitrous oxide and the hydroxide anion. Hyponitrites (involving the anion) are stable to reducing agents and more commonly act as reducing agents themselves. They are an intermediate step in the oxidation of ammonia to nitrite, which occurs in the nitrogen cycle. Hyponitrite can act as a bridging or chelating bidentate ligand.\n\nNitrous acid (HNO) is not known as a pure compound, but is a common component in gaseous equilibria and is an important aqueous reagent: its aqueous solutions may be made from acidifying cool aqueous nitrite (, bent) solutions, although already at room temperature disproportionation to nitrate and nitric oxide is significant. It is a weak acid with p\"K\" 3.35 at 18 °C. They may be titrimetrically analysed by their oxidation to nitrate by permanganate. They are readily reduced to nitrous oxide and nitric oxide by sulfur dioxide, to hyponitrous acid with tin(II), and to ammonia with hydrogen sulfide. Salts of hydrazinium react with nitrous acid to produce azides which further react to give nitrous oxide and nitrogen. Sodium nitrite is mildly toxic in concentrations above 100 mg/kg, but small amounts are often used to cure meat and as a preservative to avoid bacterial spoilage. It is also used to synthesise hydroxylamine and to diazotise primary aromatic amines as follows:\n\nNitrite is also a common ligand that can coordinate in five ways. The most common are nitro (bonded from the nitrogen) and nitrito (bonded from an oxygen). Nitro-nitrito isomerism is common, where the nitrito form is usually less stable.\nNitric acid (HNO) is by far the most important and the most stable of the nitrogen oxoacids. It is one of the three most used acids (the other two being sulfuric acid and hydrochloric acid) and was first discovered by the alchemists in the 13th century. It is made by catalytic oxidation of ammonia to nitric oxide, which is oxidised to nitrogen dioxide, and then dissolved in water to give concentrated nitric acid. In the United States of America, over seven million tonnes of nitric acid are produced every year, most of which is used for nitrate production for fertilisers and explosives, among other uses. Anhydrous nitric acid may be made by distilling concentrated nitric acid with phosphorus pentoxide at low pressure in glass apparatus in the dark. It can only be made in the solid state, because upon melting it spontaneously decomposes to nitrogen dioxide, and liquid nitric acid undergoes self-ionisation to a larger extent than any other covalent liquid as follows:\nTwo hydrates, HNO·HO and HNO·3HO, are known that can be crystallised. It is a strong acid and concentrated solutions are strong oxidising agents, though gold, platinum, rhodium, and iridium are immune to attack. A 3:1 mixture of concentrated hydrochloric acid and nitric acid, called \"aqua regia\", is still stronger and successfully dissolves gold and platinum, because free chlorine and nitrosyl chloride are formed and chloride anions can form strong complexes. In concentrated sulfuric acid, nitric acid is protonated to form nitronium, which can act as an electrophile for aromatic nitration:\nThe thermal stabilities of nitrates (involving the trigonal planar anion) depends on the basicity of the metal, and so do the products of decomposition (thermolysis), which can vary between the nitrite (for example, sodium), the oxide (potassium and lead), or even the metal itself (silver) depending on their relative stabilities. Nitrate is also a common ligand with many modes of coordination.\n\nFinally, although orthonitric acid (HNO), which would be analogous to orthophosphoric acid, does not exist, the tetrahedral orthonitrate anion is known in its sodium and potassium salts:\nThese white crystalline salts are very sensitive to water vapour and carbon dioxide in the air:\nDespite its limited chemistry, the orthonitrate anion is interesting from a structural point of view due to its regular tetrahedral shape and the short N–O bond lengths, implying significant polar character to the bonding.\n\nNitrogen is one of the most important elements in organic chemistry. Many organic functional groups involve a carbon–nitrogen bond, such as amides (RCONR), amines (RN), imines (RC(=NR)R), imides (RCO)NR, azides (RN), azo compounds (RNR), cyanates and isocyanates (ROCN or RCNO), nitrates (RONO), nitriles and isonitriles (RCN or RNC), nitrites (RONO), nitro compounds (RNO), nitroso compounds (RNO), oximes (RCR=NOH), and pyridine derivatives. C–N bonds are strongly polarised towards nitrogen. In these compounds, nitrogen is usually trivalent (though it can be tetravalent in quaternary ammonium salts, RN), with a lone pair that can confer basicity on the compound by being coordinated to a proton. This may be offset by other factors: for example, amides are not basic because the lone pair is delocalised into a double bond (though they may act as acids at very low pH, being protonated at the oxygen), and pyrrole is not acidic because the lone pair is delocalised as part of an aromatic ring. The amount of nitrogen in a chemical substance can be determined by the Kjeldahl method. In particular, nitrogen is an essential component of nucleic acids, amino acids and thus proteins, and the energy-carrying molecule adenosine triphosphate and is thus vital to all life on Earth.\n\nNitrogen is the most common pure element in the earth, making up 78.1% of the entire volume of the atmosphere. Despite this, it is not very abundant in Earth's crust, making up only 19 parts per million of this, on par with niobium, gallium, and lithium. The only important nitrogen minerals are nitre (potassium nitrate, saltpetre) and sodanitre (sodium nitrate, Chilean saltpetre). However, these have not been an important source of nitrates since the 1920s, when the industrial synthesis of ammonia and nitric acid became common.\n\nNitrogen compounds constantly interchange between the atmosphere and living organisms. Nitrogen must first be processed, or \"fixed\", into a plant-usable form, usually ammonia. Some nitrogen fixation is done by lightning strikes producing the nitrogen oxides, but most is done by diazotrophic bacteria through enzymes known as nitrogenases (although today industrial nitrogen fixation to ammonia is also significant). When the ammonia is taken up by plants, it is used to synthesise proteins. These plants are then digested by animals who use the nitrogen compounds to synthesise their own proteins and excrete nitrogen–bearing waste. Finally, these organisms die and decompose, undergoing bacterial and environmental oxidation and denitrification, returning free dinitrogen to the atmosphere. Industrial nitrogen fixation by the Haber process is mostly used as fertiliser, although excess nitrogen–bearing waste, when leached, leads to eutrophication of freshwater and the creation of marine dead zones, as nitrogen-driven bacterial growth depletes water oxygen to the point that all higher organisms die. Furthermore, nitrous oxide, which is produced during denitrification, attacks the atmospheric ozone layer.\n\nMany saltwater fish manufacture large amounts of trimethylamine oxide to protect them from the high osmotic effects of their environment; conversion of this compound to dimethylamine is responsible for the early odour in unfresh saltwater fish. In animals, free radical nitric oxide (derived from an amino acid), serves as an important regulatory molecule for circulation.\n\nNitric oxide's rapid reaction with water in animals results in production of its metabolite nitrite. Animal metabolism of nitrogen in proteins, in general, results in excretion of urea, while animal metabolism of nucleic acids results in excretion of urea and uric acid. The characteristic odour of animal flesh decay is caused by the creation of long-chain, nitrogen-containing amines, such as putrescine and cadaverine, which are breakdown products of the amino acids ornithine and lysine, respectively, in decaying proteins.\n\nNitrogen gas is an industrial gas produced by the fractional distillation of liquid air, or by mechanical means using gaseous air (pressurised reverse osmosis membrane or pressure swing adsorption). Nitrogen gas generators using membranes or pressure swing adsorption (PSA) are typically more cost and energy efficient than bulk delivered nitrogen. Commercial nitrogen is often a byproduct of air-processing for industrial concentration of oxygen for steelmaking and other purposes. When supplied compressed in cylinders it is often called OFN (oxygen-free nitrogen). Commercial-grade nitrogen already contains at most 20 ppm oxygen, and specially purified grades containing at most 2 ppm oxygen and 10 ppm argon are also available.\n\nIn a chemical laboratory, it is prepared by treating an aqueous solution of ammonium chloride with sodium nitrite.\n\nSmall amounts of the impurities NO and HNO are also formed in this reaction. The impurities can be removed by passing the gas through aqueous sulfuric acid containing potassium dichromate. Very pure nitrogen can be prepared by the thermal decomposition of barium azide or sodium azide.\n\nThe applications of nitrogen compounds are naturally extremely widely varied due to the huge size of this class: hence, only applications of pure nitrogen itself will be considered here. Two-thirds of nitrogen produced by industry is sold as the gas and the remaining one-third as the liquid. The gas is mostly used as an inert atmosphere whenever the oxygen in the air would pose a fire, explosion, or oxidising hazard. Some examples include:\n\nNitrogen is commonly used during sample preparation in chemical analysis. It is used to concentrate and reduce the volume of liquid samples. Directing a pressurised stream of nitrogen gas perpendicular to the surface of the liquid causes the solvent to evaporate while leaving the solute(s) and un-evaporated solvent behind.\n\nNitrogen can be used as a replacement, or in combination with, carbon dioxide to pressurise kegs of some beers, particularly stouts and British ales, due to the smaller bubbles it produces, which makes the dispensed beer smoother and headier. A pressure-sensitive nitrogen capsule known commonly as a \"widget\" allows nitrogen-charged beers to be packaged in cans and bottles. Nitrogen tanks are also replacing carbon dioxide as the main power source for paintball guns. Nitrogen must be kept at higher pressure than CO, making N tanks heavier and more expensive. Nitrogen gas has become the inert gas of choice for inert gas asphyxiation, and is under consideration as a replacement for lethal injection in Oklahoma. Nitrogen gas, formed from the decomposition of sodium azide, is used for the inflation of airbags.\n\nLiquid nitrogen is a cryogenic liquid. When insulated in proper containers such as Dewar flasks, it can be transported without much evaporative loss.\nLike dry ice, the main use of liquid nitrogen is as a refrigerant. Among other things, it is used in the cryopreservation of blood, reproductive cells (sperm and egg), and other biological samples and materials. It is used in the clinical setting in cryotherapy to remove cysts and warts on the skin. It is used in cold traps for certain laboratory equipment and to cool infrared detectors or X-ray detectors. It has also been used to cool central processing units and other devices in computers that are overclocked, and that produce more heat than during normal operation. Other uses include freeze-grinding and machining materials that are soft or rubbery at room temperature, shrink-fitting and assembling engineering components, and more generally to attain very low temperatures whenever necessary (around −200 °C). Because of its low cost, liquid nitrogen is also often used when such low temperatures are not strictly necessary, such as refrigeration of food, freeze-branding livestock, freezing pipes to halt flow when valves are not present, and consolidating unstable soil by freezing whenever excavation is going on underneath.\n\nAlthough nitrogen is non-toxic, when released into an enclosed space it can displace oxygen, and therefore presents an asphyxiation hazard. This may happen with few warning symptoms, since the human carotid body is a relatively poor and slow low-oxygen (hypoxia) sensing system. An example occurred shortly before the launch of the first Space Shuttle mission in 1981, when two technicians died from asphyxiation after they walked into a space located in the Shuttle's Mobile Launcher Platform that was pressurised with pure nitrogen as a precaution against fire.\n\nWhen inhaled at high partial pressures (more than about 4 bar, encountered at depths below about 30 m in scuba diving), nitrogen is an anesthetic agent, causing nitrogen narcosis, a temporary state of mental impairment similar to nitrous oxide intoxication.\n\nNitrogen dissolves in the blood and body fats. Rapid decompression (as when divers ascend too quickly or astronauts decompress too quickly from cabin pressure to spacesuit pressure) can lead to a potentially fatal condition called decompression sickness (formerly known as caisson sickness or \"the bends\"), when nitrogen bubbles form in the bloodstream, nerves, joints, and other sensitive or vital areas. Bubbles from other \"inert\" gases (gases other than carbon dioxide and oxygen) cause the same effects, so replacement of nitrogen in breathing gases may prevent nitrogen narcosis, but does not prevent decompression sickness.\n\nAs a cryogenic liquid, liquid nitrogen can be dangerous by causing cold burns on contact, although the Leidenfrost effect provides protection for very short exposure (about one second). Ingestion of liquid nitrogen can cause severe internal damage. For example, in 2012, a young woman in England had to have her stomach removed after ingesting a cocktail made with liquid nitrogen.\n\nBecause the liquid-to-gas expansion ratio of nitrogen is 1:694 at 20 °C, a tremendous amount of force can be generated if liquid nitrogen is rapidly vaporised in an enclosed space. In an incident on January 12, 2006 at Texas A&M University, the pressure-relief devices of a tank of liquid nitrogen were malfunctioning and later sealed. As a result of the subsequent pressure buildup, the tank failed catastrophically. The force of the explosion was sufficient to propel the tank through the ceiling immediately above it, shatter a reinforced concrete beam immediately below it, and blow the walls of the laboratory 0.1–0.2 m off their foundations.\n\nLiquid nitrogen readily evaporates to form gaseous nitrogen, and hence the precautions associated with gaseous nitrogen also apply to liquid nitrogen. For example, oxygen sensors are sometimes used as a safety precaution when working with liquid nitrogen to alert workers of gas spills into a confined space.\n\nVessels containing liquid nitrogen can condense oxygen from air. The liquid in such a vessel becomes increasingly enriched in oxygen (boiling point −183 °C, higher than that of nitrogen) as the nitrogen evaporates, and can cause violent oxidation of organic material.\n\n\n"}
{"id": "21176", "url": "https://en.wikipedia.org/wiki?curid=21176", "title": "Nominalism", "text": "Nominalism\n\nIn metaphysics, nominalism is a philosophical view which denies the existence of universals and abstract objects, but affirms the existence of general or abstract terms and predicates. There are at least two main versions of nominalism. One version denies the existence of universals – things that can be instantiated or exemplified by many particular things (e.g., strength, humanity). The other version specifically denies the existence of abstract objects – objects that do not exist in space and time.\n\nMost nominalists have held that only physical particulars in space and time are real, and that universals exist only \"post res\", that is, subsequent to particular things. However, some versions of nominalism hold that some particulars are abstract entities (e.g., numbers), while others are concrete entities – entities that do exist in space and time (e.g., pillars, snakes, bananas).\n\nNominalism is primarily a position on the problem of universals, which dates back at least to Plato, and is opposed to realist philosophies, such as Platonic realism, which assert that universals do exist over and above particulars. However, the name \"nominalism\" emerged from debates in medieval philosophy with Roscellinus.\n\nThe term 'nominalism' stems from the Latin \"nomen\", \"name\". For example, John Stuart Mill once wrote, that \"there is nothing general except names\".\n\nIn philosophy of law, nominalism finds its application in what is called constitutional nominalism.\n\nPlato was perhaps the first writer in Western philosophy to clearly state a non-nominalist position:\n\nWhat about someone who believes in beautiful things, but doesn't believe in the beautiful itself…? Don't you think he is living in a dream rather than a wakened state? (\"Republic\" 476c)\n\nThe Platonic universals corresponding to the names \"bed\" and \"beautiful\" were the Form of the Bed and the Form of the Beautiful, or the \"Bed Itself\" and the \"Beautiful Itself\". Platonic Forms were the first universals posited as such in philosophy.\n\nOur term \"universal\" is due to the English translation of Aristotle's technical term \"katholou\" which he coined specially for the purpose of discussing the problem of universals. \"Katholou\" is a contraction of the phrase \"kata holou\", meaning \"on the whole\".\n\nAristotle famously rejected certain aspects of Plato's Theory of Forms, but he clearly rejected nominalism as well:\n\n...'Man', and indeed every general predicate, signifies not an individual, but some quality, or quantity or relation, or something of that sort. (\"Sophistical Refutations\" xxii, 178b37, trans. Pickard-Cambridge)\n\nThe first philosophers to explicitly describe nominalist arguments were the Stoics, especially Chrysippus.\n\nIn medieval philosophy, the French philosopher and theologian Roscellinus (c. 1050 – c. 1125) was an early, prominent proponent of nominalism. Nominalist ideas can be found in the work of Peter Abelard and reached their flowering in William of Ockham, who was the most influential and thorough nominalist. Abelard's and Ockham's version of nominalism is sometimes called conceptualism, which presents itself as a middle way between nominalism and realism, asserting that there \"is\" something in common among like individuals, but that it is a concept in the mind, rather than a real entity existing independently of the mind. Ockham argued that only individuals existed and that universals were only mental ways of referring to sets of individuals. \"I maintain\", he wrote, \"that a universal is not something real that exists in a subject... but that it has a being only as a thought-object in the mind [objectivum in anima]\". As a general rule, Ockham argued against assuming any entities that were not necessary for explanations. Accordingly, he wrote, there is no reason to believe that there is an entity called \"humanity\" that resides inside, say, Socrates, and nothing further is explained by making this claim. This is in accord with the analytical method that has since come to be called Ockham's razor, the principle that the explanation of any phenomenon should make as few assumptions as possible. Critics argue that conceptualist approaches only answer the psychological question of universals. If the same concept is \"correctly\" and non-arbitrarily applied to two individuals, there must be some resemblance or shared property between the two individuals that justifies their falling under the same concept and that is just the metaphysical problem that universals were brought in to address, the starting-point of the whole problem (MacLeod & Rubenstein, 2006, §3d). If resemblances between individuals are asserted, conceptualism becomes moderate realism; if they are denied, it collapses into nominalism.\n\nIn modern philosophy, nominalism was revived by Thomas Hobbes and Pierre Gassendi.\n\nIn contemporary analytic philosophy, it has been defended by Rudolf Carnap, Nelson Goodman, H. H. Price, and D. C. Williams.\n\nNominalism arose in reaction to the problem of universals, specifically accounting for the fact that some things are of the same type. For example, Fluffy and Kitzler are both cats, or, the fact that certain properties are repeatable, such as: the grass, the shirt, and Kermit the Frog are green. One wants to know by virtue of \"what\" are Fluffy and Kitzler both cats, and \"what\" makes the grass, the shirt, and Kermit green.\n\nThe Platonist answer is that all the green things are green in virtue of the existence of a universal: a single abstract thing that, in this case, is a part of all the green things. With respect to the color of the grass, the shirt and Kermit, one of their parts is identical. In this respect, the three parts are literally one. Greenness is repeatable because there is one thing that manifests itself wherever there are green things.\n\nNominalism denies the existence of universals. The motivation for this flows from several concerns, the first one being where they might exist. Plato famously held, on one interpretation, that there is a realm of abstract forms or universals apart from the physical world (see theory of the forms). Particular physical objects merely exemplify or instantiate the universal. But this raises the question: Where is this universal realm? One possibility is that it is outside space and time. A view sympathetic with this possibility holds that, precisely because some form is immanent in several physical objects, it must also transcend each of those physical objects; in this way, the forms are \"transcendent\" only insofar as they are \"immanent\" in many physical objects. In other words, immanence implies transcendence; they are not opposed to one another. (Nor, in this view, would there be a separate \"world\" or \"realm\" of forms that is distinct from the physical world, thus shirking much of the worry about where to locate a \"universal realm\".) However, naturalists assert that nothing is outside of space and time. Some Neoplatonists, such as the pagan philosopher Plotinus and the Christian philosopher Augustine, imply (anticipating conceptualism) that universals are contained within the \"mind\" of God. To complicate things, what is the nature of the instantiation or exemplification relation?\n\nConceptualists hold a position intermediate between nominalism and realism, saying that universals exist only within the mind and have no external or substantial reality.\n\nModerate realists hold that there is no realm in which universals exist, but rather universals are located in space and time wherever they are manifest. Now, recall that a universal, like greenness, is supposed to be a single thing. Nominalists consider it unusual that there could be a single thing that exists in multiple places simultaneously. The realist maintains that all the instances of greenness are held together by the exemplification relation, but this relation cannot be explained.\n\nFinally, many philosophers prefer simpler ontologies populated with only the bare minimum of types of entities, or as W. V. O. Quine said \"They have a taste for 'desert landscapes.'\" They try to express everything that they want to explain without using universals such as \"catness\" or \"greenness.\"\n\nThere are various forms of nominalism ranging from extreme to almost-realist. One extreme is predicate nominalism, which states that Fluffy and Kitzler, for example, are both cats simply because the predicate 'is a cat' applies to both of them. And this is the case for all similarity of attribute among objects. The main criticism of this view is that it does not provide a sufficient solution to the problem of universals. It fails to provide an account of what makes it the case that a group of things warrant having the same predicate applied to them.\n\nProponents of resemblance nominalism believe that 'cat' applies to both cats because Fluffy and Kitzler resemble an exemplar cat closely enough to be classed together with it as members of its kind, or that they differ from each other (and other cats) quite less than they differ from other things, and this warrants classing them together. Some resemblance nominalists will concede that the resemblance relation is itself a universal, but is the only universal necessary. Others argue that each resemblance relation is a particular, and is a resemblance relation simply in virtue of its resemblance to other resemblance relations. This generates an infinite regress, but many argue that it is not vicious.\n\nClass nominalism argues that class membership forms the metaphysical backing for property relationships: two particular red balls share a property in that they are both members of classes corresponding to their properties—that of being red and being balls. A version of class nominalism that sees some classes as \"natural classes\" is held by Anthony Quinton.\n\nConceptualism is a philosophical theory that explains universality of particulars as conceptualized frameworks situated within the thinking mind. The conceptualist view approaches the metaphysical concept of universals from a perspective that denies their presence in particulars outside of the mind's perception of them.\n\nAnother form of nominalism is trope nominalism. A trope is a particular instance of a property, like the specific greenness of a shirt. One might argue that there is a primitive, objective resemblance relation that holds among like tropes. Another route is to argue that all apparent tropes are constructed out of more primitive tropes and that the most primitive tropes are the entities of complete physics. Primitive trope resemblance may thus be accounted for in terms of causal indiscernibility. Two tropes are exactly resembling if substituting one for the other would make no difference to the events in which they are taking part. Varying degrees of resemblance at the macro level can be explained by varying degrees of resemblance at the micro level, and micro-level resemblance is explained in terms of something no less robustly physical than causal power. David Armstrong, perhaps the most prominent contemporary realist, argues that such a trope-based variant of nominalism has promise, but holds that it is unable to account for the laws of nature in the way his theory of universals can.\n\nIan Hacking has also argued that much of what is called social constructionism of science in contemporary times is actually motivated by an unstated nominalist metaphysical view. For this reason, he claims, scientists and constructionists tend to \"shout past each other\".\n\nA notion that philosophy, especially ontology and the philosophy of mathematics should abstain from set theory owes much to the writings of Nelson Goodman (see especially Goodman 1940 and 1977), who argued that concrete and abstract entities having no parts, called \"individuals\" exist. Collections of individuals likewise exist, but two collections having the same individuals are the same collection. Goodman was himself drawing heavily on the work of Stanisław Leśniewski, especially his mereology, which was itself a reaction to the paradoxes associated with Cantorian set theory. Leśniewski denied the existence of the empty set and held that any singleton was identical to the individual inside it. Classes corresponding to what are held to be species or genera are concrete sums of their concrete constituting individuals. For example, the class of philosophers is nothing but the sum of all concrete, individual philosophers.\n\nThe principle of extensionality in set theory assures us that any matching pair of curly braces enclosing one or more instances of the same individuals denote the same set. Hence {\"a\", \"b\"}, {\"b\", \"a\"}, {\"a\", \"b\", \"a\", \"b\"} are all the same set. For Goodman and other nominalists, {\"a\", \"b\"} is also identical to {\"a\", {\"b\"} }, {\"b\", {\"a\", \"b\"} }, and any combination of matching curly braces and one or more instances of \"a\" and \"b\", as long as \"a\" and \"b\" are names of individuals and not of collections of individuals. Goodman, Richard Milton Martin, and Willard Quine all advocated reasoning about collectivities by means of a theory of \"virtual sets\" (see especially Quine 1969), one making possible all elementary operations on sets except that the universe of a quantified variable cannot contain any virtual sets.\n\nIn the foundation of mathematics, nominalism has come to mean doing mathematics without assuming that sets in the mathematical sense exist. In practice, this means that quantified variables may range over universes of numbers, points, primitive ordered pairs, and other abstract ontological primitives, but not over sets whose members are such individuals. To date, only a small fraction of the corpus of modern mathematics can be rederived in a nominalistic fashion.\n\nAs a category of late medieval thought, the concept of 'nominalism' has been increasingly queried. Traditionally, the fourteenth century has been regarded as the heyday of nominalism, with figures such as John Buridan and William of Ockham viewed as founding figures. However, the concept of 'nominalism' as a movement (generally contrasted with 'realism'), first emerged only in the late fourteenth century, and only gradually became widespread during the fifteenth century. The notion of two distinct ways, a \"via antiqua\", associated with realism, and a \"via moderna\", associated with nominalism, became widespread only in the later fifteenth century – a dispute which eventually dried up in the sixteenth century.\n\nAware that explicit thinking in terms of a divide between 'nominalism' and 'realism' only emerged in the fifteenth century, scholars have increasingly questioned whether a fourteenth-century school of nominalism can really be said to have existed. While one might speak of family resemblances between Ockham, Buridan, Marsilius and others, there are also striking differences. More fundamentally, Robert Pasnau has questioned whether any kind of coherent body of thought that could be called 'nominalism' can be discerned in fourteenth century writing. This makes it difficult, it has been argued, to follow the twentieth century narrative which portrayed late scholastic philosophy as a dispute which emerged in the fourteenth century between the \"via moderna\", nominalism, and the \"via antiqua\", realism, with the nominalist ideas of William of Ockham foreshadowing the eventual rejection of scholasticism in the seventeenth century.\n\nA critique of nominalist reconstructions in mathematics was undertaken by Burgess (1983) and Burgess and Rosen (1997). Burgess distinguished two types of nominalist reconstructions. Thus, \"hermeneutic nominalism\" is the hypothesis that science, properly interpreted, already dispenses with mathematical objects\n(entities) such as numbers and sets. Meanwhile, \"revolutionary nominalism\" is the project of replacing current scientific theories by alternatives dispensing with mathematical objects (see Burgess, 1983, p. 96). A recent study extends the Burgessian critique to three nominalistic reconstructions: the reconstruction of analysis by Georg Cantor, Richard Dedekind, and Karl Weierstrass that dispensed with infinitesimals; the constructivist re-reconstruction of Weierstrassian analysis by Errett Bishop that dispensed with the law of excluded middle; and the hermeneutic reconstruction, by Carl Boyer, Judith Grabiner, and others, of Cauchy's foundational contribution to analysis that dispensed with Cauchy's infinitesimals.\n\n\n\n"}
{"id": "21178", "url": "https://en.wikipedia.org/wiki?curid=21178", "title": "Non-cognitivism", "text": "Non-cognitivism\n\nNon-cognitivism is the meta-ethical view that ethical sentences do not express propositions (i.e., statements) and thus cannot be true or false (they are not truth-apt). A noncognitivist denies the cognitivist claim that \"moral judgments are capable of being objectively true, because they describe some feature of the world\". If moral statements cannot be true, and if one cannot know something that is not true, noncognitivism implies that moral knowledge is impossible.\n\nNon-cognitivism entails that non-cognitive attitudes underlie moral discourse and this discourse therefore consists of non-declarative speech acts, although accepting that its surface features may consistently and efficiently work as if moral discourse were cognitive. The point of interpreting moral claims as non-declarative speech acts is to explain what moral claims mean if they are neither true nor false (as philosophies such as logical positivism entail). Utterances like \"Boo to killing!\" and \"Don't kill\" are not candidates for truth or falsity, but have non-cognitive meaning.\n\nEmotivism, associated with A. J. Ayer, the Vienna Circle and C. L. Stevenson, suggests that ethical sentences are primarily emotional expressions of one's own attitudes and are intended to influence the actions of the listener. Under this view, \"Killing is wrong\" is translated as \"Killing, boo!\" or \"I disapprove of killing.\"\n\nA close cousin of emotivism, developed by R. M. Hare, is called universal prescriptivism. Prescriptivists interpret ethical statements as being universal \"imperatives\", prescribing behavior for all to follow. According to prescriptivism, \nphrases like \"Thou shalt not murder!\" or \"Do not steal!\" are the clearest expressions of morality, while reformulations like \"Killing is wrong\" tend to obscure the meaning of moral sentences.\n\nOther forms of non-cognitivism include Simon Blackburn's quasi-realism and Allan Gibbard's norm-expressivism.\n\nArguments for prescriptivism focus on the \"function\" of normative statements.\n\nPrescriptivists argue that factual statements and prescriptions are totally different, because of different expectations of change in cases of a clash between word and world.\nIn a descriptive sentence, if one premises that \"red is a number\" then according to the rules of English grammar said statement would be false. Since said premise describes the objects \"red\" and \"number\", anyone with an adequate understanding of English would notice the falseness of such description and the falseness of said statement. However, if the norm \"thou shalt not kill!\" is uttered, and this premise is negated (by the fact of a person being murdered), the speaker is not to change his sentence upon observation of this into \"kill other people!\", but is to reiterate the moral outrage of the act of killing. Adjusting statements based upon objective reality and adjusting reality based upon statements are contrary uses of language, so descriptive statement are a different kind of sentences than norms. If truth is understood according to correspondence theory, the question of the truth or falsity of sentences not contingent upon external phenomena cannot be tested (see tautologies).\n\nSome cognitivists argue that some expressions like \"courageous\" have both a factual as well as a normative component which cannot be distinguished by analysis. Prescriptivists argue that according to context, either the factual or the normative component of the meaning is dominant. The sentence \"Hero A behaved courageously\" is wrong, if A ran away in the face of danger. But the sentence \"Be brave and fight for the glory of your country!\" has no truth value and cannot be falsified by someone who doesn't join the army.\n\nPrescriptivism is also supported by the actual way of speaking. Many moral statements are de facto uttered as recommendations or commands, e.g. when parents or teachers forbid children to do wrong actions. The most famous moral ideas are prescriptions: the Ten Commandments, the command of charity, the categorical imperative, and the Golden Rule command to do or not to do something rather than state that something is or is not the case.\n\nPrescriptivism can fit the theist idea of morality as obedience towards god. It is however different from the cognitivist supernaturalism which interprets morality as subjective will of god, while prescriptivism claims that moral rules are universal and can be found by reason alone without reference to a god.\n\nAccording to Hare, prescriptivists cannot argue that amoralists are logically wrong or contradictive. Everyone can choose to follow moral commands or not. This is the human condition according to the Christian reinterpretation of the Choice of Heracles. According to prescriptivism, morality is not about knowledge (of moral facts), but about character (to choose to do the right thing). Actors cannot externalize their responsibility and freedom of will towards some moral truth in the world, virtuous people don't need to wait for some cognition to choose what's right.\n\nPrescriptivism is also supported by imperative logic, in which there are no truth values for imperatives, and by the idea of the naturalistic fallacy: even if someone could prove the existence of an ethical property and express it in a factual statement, he could never derive any command from this statement, so the search for ethical properties is pointless.\n\nAs with other anti-realist meta-ethical theories, non-cognitivism is largely supported by the argument from queerness: ethical properties, if they existed, would be different from any other thing in the universe, since they have no observable effect on the world. People generally have a negative attitude towards murder, which presumably keeps most of us from murdering. But does the actual \"wrongness\" of murder play an \"independent\" role? Is there any evidence that there is a property of wrongness that some types of acts have? Some people might think that the strong feelings we have when we see or consider a murder provide evidence of murder's wrongness. But it is not difficult to explain these feelings without saying that \"wrongness\" was their cause. Thus there is no way of discerning which, if any, ethical properties exist; by Occam's razor, the simplest assumption is that none do. The non-cognitivist then asserts that, since a proposition about an ethical property would have no referent, ethical statements must be something else.\n\nArguments for emotivism focus on what normative statements \"express\" when uttered by a speaker. A person who says that killing is wrong certainly expresses her disapproval of killing. Emotivists claim that this is \"all\" she does, that the statement \"killing is wrong\" is not a truth-apt declaration, and that the burden of evidence is on the cognitivists who want to show that in addition to expressing disapproval, the claim \"killing is wrong\" is also true. Emotivists ask whether there really is evidence that killing is wrong. We have evidence that Jupiter has a magnetic field and that birds are oviparous, but as yet, we do not seem to have found evidence of moral properties, such as \"goodness\". Emotivists ask why, without such evidence, we should think there \"is\" such a property. Ethical intuitionists think the evidence comes not from science or reason but from our own feelings: good deeds make us feel a certain way and bad deeds make us feel very differently. But is this enough to show that there are genuinely good and bad deeds? Emotivists think not, claiming that we do not need to postulate the existence of moral \"badness\" or \"wrongness\" to explain why considering certain deeds makes us feel disapproval; that all we really observe when we introspect are feelings of disapproval. Thus the emotivist asks why not adopt the simple explanation and say that this is all there is, rather than insist that some intrinsic \"badness\" (of murder, for example) must be causing feelings when a simpler explanation is available.\n\nOne argument against non-cognitivism is that it ignores the external \"causes\" of emotional and prescriptive reactions. If someone says, \"John is a good person,\" something about John must have inspired that reaction. If John gives to the poor, takes care of his sick grandmother, and is friendly to others, and these are what inspire the speaker to think well of him, it is plausible to say, \"John is a good person because he gives to the poor, takes care of his sick grandmother, and is friendly to others.\" If, in turn, the speaker responds positively to the idea of giving to the poor, then some aspect of that idea must have inspired a positive response; one could argue that that aspect is also the basis of its goodness.\n\nAnother argument is the \"embedding problem.\" Consider the following sentences:\n\nAttempts to translate these sentences in an emotivist framework seem to fail (e.g. \"She does not realize, 'Boo on eating meat!'\"). Prescriptivist translations fare only slightly better (\"She does not realize that she is not to eat meat\"). Even the act of forming such a construction indicates some sort of cognition in the process.\n\nAccording to some non-cognitivist points of view, these sentences simply assume the false premise that ethical statements are either true or false. They might be literally translated as:\n\nThese translations, however, seem divorced from the way people actually use language. A non-cognitivist would have to disagree with someone saying, \"'Eating meat is wrong' is a false statement\" (since \"Eating meat is wrong\" is not truth-apt at all), but may be tempted to agree with a person saying, \"Eating meat is not wrong.\"\n\nOne might more constructively interpret these statements to describe the underlying emotional statement that they express, i.e.: I disapprove/do not disapprove of eating meat, I used to, he doesn't, I do and she doesn't, etc.; however, this interpretation is closer to ethical subjectivism than to non-cognitivism proper.\n\nA similar argument against non-cognitivism is that of ethical argument. A common argument might be, \"If killing an innocent human is always wrong, and all fetuses are innocent humans, then killing a fetus is always wrong.\" Most people would consider such an utterance to represent an analytic proposition which is true \"a priori\". However, if ethical statements do not represent cognitions, it seems odd to use them as premises in an argument, and even odder to assume they follow the same rules of syllogism as true propositions. However, R.M. Hare, proponent of universal prescriptivism, has argued that the rules of logic are independent of grammatical mood, and thus the same logical relations may hold between imperatives as hold between indicatives.\n\nMany objections to non-cognitivism based on the linguistic characteristics of what purport to be moral judgments were originally raised by Peter Glassen in \"The Cognitivity of Moral Judgments\", published in \"Mind\" in January 1959, and in Glassen's follow-up article in the January 1963 issue of the same journal.\n\n\n"}
{"id": "21179", "url": "https://en.wikipedia.org/wiki?curid=21179", "title": "North Sea", "text": "North Sea\n\nThe North Sea is a marginal sea of the Atlantic Ocean located between the United Kingdom (particularly England and Scotland), Denmark, Norway, Sweden, Germany, the Netherlands, Belgium and France. An epeiric (or \"shelf\") sea on the European continental shelf, it connects to the ocean through the English Channel in the south and the Norwegian Sea in the north. It is more than long and wide, with an area of around .\n\nThe North Sea has long been the site of important European shipping lanes as well as a major fishery. The sea is a popular destination for recreation and tourism in bordering countries and more recently has developed into a rich source of energy resources including fossil fuels, wind, and early efforts in wave power.\n\nHistorically, the North Sea has featured prominently in geopolitical and military affairs, particularly in Northern Europe. It was also important globally through the power northern Europeans projected worldwide during much of the Middle Ages and into the modern era. The North Sea was the centre of the Vikings' rise. Subsequently, the Hanseatic League, the Netherlands, and the British each sought to dominate the North Sea and thus access to the world's markets and resources. As Germany's only outlet to the ocean, the North Sea continued to be strategically important through both World Wars.\n\nThe coast of the North Sea presents a diversity of geological and geographical features. In the north, deep fjords and sheer cliffs mark the Norwegian and Scottish coastlines, whereas in the south, the coast consists primarily of sandy beaches and wide mudflats. Due to the dense population, heavy industrialization, and intense use of the sea and area surrounding it, there have been various environmental issues affecting the sea's ecosystems. Adverse environmental issues – commonly including overfishing, industrial and agricultural runoff, dredging, and dumping, among others – have led to a number of efforts to prevent degradation of the sea while still making use of its economic potential.\n\nThe North Sea is bounded by the Orkney Islands and east coast of Great Britain to the west and the northern and central European mainland to the east and south, including Norway, Denmark, Germany, the Netherlands, Belgium, and France. In the southwest, beyond the Straits of Dover, the North Sea becomes the English Channel connecting to the Atlantic Ocean. In the east, it connects to the Baltic Sea via the Skagerrak and Kattegat, narrow straits that separate Denmark from Norway and Sweden respectively. In the north it is bordered by the Shetland Islands, and connects with the Norwegian Sea, which lies in the very north-eastern part of the Atlantic.\n\nThe North Sea is more than long and wide, with an area of and a volume of . Around the edges of the North Sea are sizeable islands and archipelagos, including Shetland, Orkney, and the Frisian Islands. The North Sea receives freshwater from a number of European continental watersheds, as well as the British Isles. A large part of the European drainage basin empties into the North Sea, including water from the Baltic Sea. The largest and most important rivers flowing into the North Sea are the Elbe and the Rhine – Meuse watershed. Around 185 million people live in the catchment area of the rivers discharging into the North Sea encompassing some highly industrialized areas.\n\nFor the most part, the sea lies on the European continental shelf with a mean depth of . The only exception is the Norwegian trench, which extends parallel to the Norwegian shoreline from Oslo to an area north of Bergen. It is between wide and has a maximum depth of .\n\nThe Dogger Bank, a vast moraine, or accumulation of unconsolidated glacial debris, rises to a mere below the surface. This feature has produced the finest fishing location of the North Sea. The Long Forties and the Broad Fourteens are large areas with roughly uniform depth in fathoms, (forty fathoms and fourteen fathoms or deep respectively). These great banks and others make the North Sea particularly hazardous to navigate, which has been alleviated by the implementation of satellite navigation systems. The Devil's Hole lies east of Dundee, Scotland. The feature is a series of asymmetrical trenches between long, wide and up to deep.\n\nOther areas which are less deep are Cleaver Bank, Fisher Bank and Noordhinder Bank.\n\nThe International Hydrographic Organization defines the limits of the North Sea as follows:\n\n\"On the Southwest.\" A line joining the Walde Lighthouse (France, 1°55'E) and Leathercoat Point (England, 51°10'N).\n\n\"On the Northwest.\" From Dunnet Head (3°22'W) in Scotland to Tor Ness (58°47'N) in the Island of Hoy, thence through this island to the Kame of Hoy (58°55'N) on to Breck Ness on Mainland (58°58'N) through this island to Costa Head (3°14'W) and to Inga Ness (59'17'N) in Westray through Westray, to Bow Head, across to Mull Head (North point of Papa Westray) and on to Seal Skerry (North point of North Ronaldsay) and thence to Horse Island (South point of the Shetland Islands).\n\n\"On the North.\" From the North point (Fethaland Point) of the Mainland of the Shetland Islands, across to Graveland Ness (60°39'N) in the Island of Yell, through Yell to Gloup Ness (1°04'W) and across to Spoo Ness (60°45'N) in Unst island, through Unst to Herma Ness (60°51'N), on to the SW point of the Rumblings and to Muckle Flugga () all these being included in the North Sea area; thence up the meridian of 0°53' West to the parallel of 61°00' North and eastward along this parallel to the coast of Norway, the whole of Viking Bank being thus included in the North Sea.\n\n\"On the East.\" The Western limit of the Skagerrak [A line joining Hanstholm () and the Naze (Lindesnes, )].\n\nThe average temperature in summer is and in the winter. The average temperatures have been trending higher since 1988, which has been attributed to climate change. Air temperatures in January range on average between and in July between . The winter months see frequent gales and storms.\n\nThe salinity averages between of water. The salinity has the highest variability where there is fresh water inflow, such as at the Rhine and Elbe estuaries, the Baltic Sea exit and along the coast of Norway.\n\nThe main pattern to the flow of water in the North Sea is an anti-clockwise rotation along the edges.\n\nThe North Sea is an arm of the Atlantic Ocean receiving the majority of ocean current from the northwest opening, and a lesser portion of warm current from the smaller opening at the English Channel. These tidal currents leave along the Norwegian coast. Surface and deep water currents may move in different directions. Low salinity surface coastal waters move offshore, and deeper, denser high salinity waters move in shore.\n\nThe North Sea located on the continental shelf has different waves from those in deep ocean water. The wave speeds are diminished and the wave amplitudes are increased. In the North Sea there are two amphidromic systems and a third incomplete amphidromic system. In the North Sea the average tide difference in wave amplitude is between zero to .\n\nThe Kelvin tide of the Atlantic ocean is a semidiurnal wave that travels northward. Some of the energy from this wave travels through the English Channel into the North Sea. The wave still travels northward in the Atlantic Ocean, and once past the northern tip of Great Britain, the Kelvin wave turns east and south and once again enters into the North Sea.\n\nThe eastern and western coasts of the North Sea are jagged, formed by glaciers during the ice ages. The coastlines along the southernmost part are covered with the remains of deposited glacial sediment. The Norwegian mountains plunge into the sea creating deep fjords and archipelagos. South of Stavanger, the coast softens, the islands become fewer. The eastern Scottish coast is similar, though less severe than Norway. From north east of England, the cliffs become lower and are composed of less resistant moraine, which erodes more easily, so that the coasts have more rounded contours. In the Netherlands, Belgium and in East Anglia the littoral is low and marshy. The east coast and south-east of the North Sea (Wadden Sea) have coastlines that are mainly sandy and straight owing to longshore drift, particularly along Belgium and Denmark.\n\nThe southern coastal areas were originally amphibious flood plains and swampy land. In areas especially vulnerable to storm surges, people settled behind elevated levees and on natural areas of high ground such as spits and geestland. As early as 500 BC, people were constructing artificial dwelling hills higher than the prevailing flood levels. It was only around the beginning of the High Middle Ages, in 1200 AD, that inhabitants began to connect single ring dikes into a dike line along the entire coast, thereby turning amphibious regions between the land and the sea into permanent solid ground.\n\nThe modern form of the dikes supplemented by overflow and lateral diversion channels, began to appear in the 17th and 18th centuries, built in the Netherlands. The North Sea Floods of 1953 and 1962 were impetus for further raising of the dikes as well as the shortening of the coast line so as to present as little surface area as possible to the punishment of the sea and the storms. Currently, 27% of the Netherlands is below sea level protected by dikes, dunes, and beach flats.\n\nCoastal management today consists of several levels. The dike slope reduces the energy of the incoming sea, so that the dike itself does not receive the full impact. Dikes that lie directly on the sea are especially reinforced. The dikes have, over the years, been repeatedly raised, sometimes up to and have been made flatter to better reduce wave erosion. Where the dunes are sufficient to protect the land behind them from the sea, these dunes are planted with beach grass (\"Ammophila arenaria\") to protect them from erosion by wind, water, and foot traffic.\n\nStorm surges threaten, in particular, the coasts of the Netherlands, Belgium, Germany, and Denmark and low lying areas of eastern England particularly around The Wash and Fens.\nStorm surges are caused by changes in barometric pressure combined with strong wind created wave action.\n\nThe first recorded storm tide flood was the \"Julianenflut\", on 17 February 1164. In its wake the Jadebusen, (a bay on the coast of Germany), began to form.\nA storm tide in 1228 is recorded to have killed more than 100,000 people. In 1362, the Second Marcellus Flood, also known as the \"Grote Manndrenke\", hit the entire southern coast of the North Sea. Chronicles of the time again record more than 100,000 deaths as large parts of the coast were lost permanently to the sea, including the now legendary lost city of Rungholt.\nIn the 20th century, the North Sea flood of 1953 flooded several nations' coasts and cost more than 2,000 lives.\n315 citizens of Hamburg died in the North Sea flood of 1962.\n\nThough rare, the North Sea has been the site of a number of historically documented tsunamis. The Storegga Slides were a series of underwater landslides, in which a piece of the Norwegian continental shelf slid into the Norwegian Sea. The immense landslips occurred between 8150 BCE and 6000 BCE, and caused a tsunami up to high that swept through the North Sea, having the greatest effect on Scotland and the Faeroe Islands.\nThe Dover Straits earthquake of 1580 is among the first recorded earthquakes in the North Sea measuring between 5.6 and 5.9 on the Richter scale. This event caused extensive damage in Calais both through its tremors and possibly triggered a tsunami, though this has never been confirmed. The theory is a vast underwater landslide in the English Channel was triggered by the earthquake, which in turn caused a tsunami. The tsunami triggered by the 1755 Lisbon earthquake reached Holland, although the waves had lost their destructive power. The largest earthquake ever recorded in the United Kingdom was the 1931 Dogger Bank earthquake, which measured 6.1 on the Richter magnitude scale and caused a small tsunami that flooded parts of the British coast.\n\nShallow epicontinental seas like the current North Sea have since long existed on the European continental shelf. The rifting that formed the northern part of the Atlantic Ocean during the Jurassic and Cretaceous periods, from about , caused tectonic uplift in the British Isles. Since then, a shallow sea has almost continuously existed between the uplands of the Fennoscandian Shield and the British Isles. This precursor of the current North Sea has grown and shrunk with the rise and fall of the eustatic sea level during geologic time. Sometimes it was connected with other shallow seas, such as the sea above the Paris Basin to the south-west, the Paratethys Sea to the south-east, or the Tethys Ocean to the south.\n\nDuring the Late Cretaceous, about , all of modern mainland Europe except for Scandinavia was a scattering of islands. By the Early Oligocene, , the emergence of Western and Central Europe had almost completely separated the North Sea from the Tethys Ocean, which gradually shrank to become the Mediterranean as Southern Europe and South West Asia became dry land. The North Sea was cut off from the English Channel by a narrow land bridge until that was breached by at least two catastrophic floods between 450,000 and 180,000 years ago. Since the start of the Quaternary period about , the eustatic sea level has fallen during each glacial period and then risen again. Every time the ice sheet reached its greatest extent, the North Sea became almost completely dry. The present-day coastline formed after the Last Glacial Maximum when the sea began to flood the European continental shelf.\n\nIn 2006 a bone fragment was found while drilling for oil in the north sea. Analysis indicated that it was a Plateosaurus from 199 to 216 million years ago. This was the deepest dinosaur fossil ever found and the first find for Norway.\n\nCopepods and other zooplankton are plentiful in the North Sea. These tiny organisms are crucial elements of the food chain supporting many species of fish. Over 230 species of fish live in the North Sea. Cod, haddock, whiting, saithe, plaice, sole, mackerel, herring, pouting, sprat, and sandeel are all very common and are fished commercially. Due to the various depths of the North Sea trenches and differences in salinity, temperature, and water movement, some fish such as blue-mouth redfish and rabbitfish reside only in small areas of the North Sea.\n\nCrustaceans are also commonly found throughout the sea. Norway lobster, deep-water prawns, and brown shrimp are all commercially fished, but other species of lobster, shrimp, oyster, mussels and clams all live in the North Sea. Recently non-indigenous species have become established including the Pacific oyster and Atlantic jackknife clam.\n\nThe coasts of the North Sea are home to nature reserves including the Ythan Estuary, Fowlsheugh Nature Preserve, and Farne Islands in the UK and the Wadden Sea National Parks in Denmark, Germany and the Netherlands. These locations provide breeding habitat for dozens of bird species. Tens of millions of birds make use of the North Sea for breeding, feeding, or migratory stopovers every year. Populations of black legged kittiwakes, Atlantic puffins, northern fulmars, and species of petrels, gannets, seaducks, loons (divers), cormorants, gulls, auks, and terns, and many other seabirds make these coasts popular for birdwatching.\n\nThe North Sea is also home to marine mammals. Common seals, and harbour porpoises can be found along the coasts, at marine installations, and on islands. The very northern North Sea islands such as the Shetland Islands are occasionally home to a larger variety of pinnipeds including bearded, harp, hooded and ringed seals, and even walrus. North Sea cetaceans include various porpoise, dolphin and whale species.\n\nPlant species in the North Sea include species of wrack, among them bladder wrack, knotted wrack, and serrated wrack. Algae, macroalgal, and kelp, such as oarweed and laminaria hyperboria, and species of maerl are found as well. Eelgrass, formerly common in the entirety of the Wadden Sea, was nearly wiped out in the 20th century by a disease. Similarly, sea grass used to coat huge tracts of ocean floor, but have been damaged by trawling and dredging have diminished its habitat and prevented its return. Invasive Japanese seaweed has spread along the shores of the sea clogging harbours and inlets and has become a nuisance.\n\nDue to the heavy human populations and high level of industrialization along its shores, the wildlife of the North Sea has suffered from pollution, overhunting, and overfishing. Flamingos and pelicans were once found along the southern shores of the North Sea, but became extinct over the 2nd millennium. Walruses frequented the Orkney Islands through the mid-16th century, as both Sable Island and Orkney Islands lay within its normal range. Gray whales also resided in the North Sea but were driven to extinction in the Atlantic in the 17th century Other species have dramatically declined in population, though they are still found. North Atlantic right whales, sturgeon, shad, rays, skates, salmon, and other species were common in the North Sea until the 20th century, when numbers declined due to overfishing. Other factors like the introduction of non-indigenous species, industrial and agricultural pollution, trawling and dredging, human-induced eutrophication, construction on coastal breeding and feeding grounds, sand and gravel extraction, offshore construction, and heavy shipping traffic have also contributed to the decline.\n\nThe OSPAR commission manages the OSPAR convention to counteract the harmful effects of human activity on wildlife in the North Sea, preserve endangered species, and provide environmental protection. All North Sea border states are signatories of the MARPOL 73/78 Accords, which preserve the marine environment by preventing pollution from ships.\nGermany, Denmark, and the Netherlands also have a trilateral agreement for the protection of the Wadden Sea, or mudflats, which run along the coasts of the three countries on the southern edge of the North Sea.\n\nThrough history various names have been used for the North Sea. One of the earliest recorded names was \"Septentrionalis Oceanus\", or \"Northern Ocean,\" which was cited by Pliny. The name \"North Sea\" probably came into English, however, via the Dutch \"Noordzee\", who named it thus either in contrast with the Zuiderzee (\"South Sea\"), located south of Frisia, or because the sea is generally to the north of the Netherlands. Before the adoption of \"North Sea,\" the names used in English were \"German Sea\" or \"German Ocean\", referred to the Latin names \"Mare Gemanicum\" and \"Oceanus Germanicus\", and these persisted in use until the First World War.\n\nOther common names in use for long periods were the Latin terms \"Mare Frisicum\", as well as the English equivalent, \"Frisian Sea\".\n\nThe modern names of the sea in the other local languages are: or \"Nordsøen\", , , , , , , Northern Frisian: \"Weestsiie\" (literally meaning \"West Sea\"), , , , , , and Zeeuws: \"Noôrdzeê\".\n\nNorth Sea has provided waterway access for commerce and conquest. Many areas have access to the North Sea because of its long coastline and the European rivers that empty into it. The British Isles had been protected from invasion by the North Sea waters until the Roman conquest of Britain in 43 CE. The Romans established organised ports, which increased shipping, and began sustained trade. When the Romans abandoned Britain in 410, the Germanic Angles, Saxons, and Jutes began the next great migration across the North Sea during the Migration Period. They made successive invasions of the island.\n\nThe Viking Age began in 793 with the attack on Lindisfarne; for the next quarter-millennium the Vikings ruled the North Sea. In their superior longships, they raided, traded, and established colonies and outposts along the coasts of the sea. From the Middle Ages through the 15th century, the northern European coastal ports exported domestic goods, dyes, linen, salt, metal goods and wine. The Scandinavian and Baltic areas shipped grain, fish, naval necessities, and timber. In turn the North Sea countries imported high-grade cloths, spices, and fruits from the Mediterranean region. Commerce during this era was mainly conducted by maritime trade due to underdeveloped roadways.\n\nIn the 13th century the Hanseatic League, though centred on the Baltic Sea, started to control most of the trade through important members and outposts on the North Sea. The League lost its dominance in the 16th century, as neighbouring states took control of former Hanseatic cities and outposts. Their internal conflict prevented effective cooperation and defence. As the League lost control of its maritime cities, new trade routes emerged that provided Europe with Asian, American, and African goods.\n\nThe 17th century Dutch Golden Age during which Dutch herring, cod and whale fisheries reached an all time high saw Dutch power at its zenith. Important overseas colonies, a vast merchant marine, powerful navy and large profits made the Dutch the main challengers to an ambitious England. This rivalry led to the first three Anglo-Dutch Wars between 1652 and 1673, which ended with Dutch victories. After the Glorious Revolution in 1688, the Dutch prince William ascended to the English throne. With unified leadership, commercial, military, and political power began to shift from Amsterdam to London.\nThe British did not face a challenge to their dominance of the North Sea until the 20th century.\n\nTensions in the North Sea were again heightened in 1904 by the Dogger Bank incident. During the Russo-Japanese War, several ships of the Russian Baltic Fleet, which was on its way to the Far East, mistook British fishing boats for Japanese ships and fired on them, and then upon each other, near the Dogger Bank, nearly causing Britain to enter the war on the side of Japan.\n\nDuring the First World War, Great Britain's Grand Fleet and Germany's Kaiserliche Marine faced each other in the North Sea, which became the main theatre of the war for surface action. Britain's larger fleet and North Sea Mine Barrage were able to establish an effective blockade for most of the war, which restricted the Central Powers' access to many crucial resources. Major battles included the Battle of Heligoland Bight, the Battle of the Dogger Bank, and the Battle of Jutland.\nWorld War I also brought the first extensive use of submarine warfare, and a number of submarine actions occurred in the North Sea.\n\nThe Second World War also saw action in the North Sea, though it was restricted more to aircraft reconnaissance, and action by fighter/bomber aircraft, submarines, and smaller vessels such as minesweepers and torpedo boats.\n\nIn the aftermath of the war, hundreds of thousands of tons of chemical weapons were disposed of by being dumped in the North Sea.\n\nAfter the war, the North Sea lost much of its military significance because it is bordered only by NATO member-states. However, it gained significant economic importance in the 1960s as the states around the North Sea began full-scale exploitation of its oil and gas resources. The North Sea continues to be an active trade route.\n\nCountries that border the North Sea all claim the of territorial waters, within which they have exclusive fishing rights. The Common Fisheries Policy of the European Union (EU) exists to coordinate fishing rights and assist with disputes between EU states and the EU border state of Norway.\n\nAfter the discovery of mineral resources in the North Sea, the Convention on the Continental Shelf established country rights largely divided along the median line. The median line is defined as the line \"every point of which is equidistant from the nearest points of the baselines from which the breadth of the territorial sea of each State is measured.\"\nThe ocean floor border between Germany, the Netherlands, and Denmark was only reapportioned after protracted negotiations and a judgement of the International Court of Justice.\n\nAs early as 1859, oil was discovered in onshore areas around the North Sea and natural gas as early as 1910. Onshore resources, for example the K12-B field in the Netherlands continue to be exploited today.\n\nOffshore test drilling began in 1966 and then, in 1969, Phillips Petroleum Company discovered the Ekofisk oil field distinguished by valuable, low-sulphur oil. Commercial exploitation began in 1971 with tankers and, after 1975, by a pipeline, first to Teesside, England and then, after 1977, also to Emden, Germany.\n\nThe exploitation of the North Sea oil reserves began just before the 1973 oil crisis, and the climb of international oil prices made the large investments needed for extraction much more attractive.\n\nAlthough the production costs are relatively high, the quality of the oil, the political stability of the region, and the proximity of important markets in western Europe has made the North Sea an important oil-producing region. The largest single humanitarian catastrophe in the North Sea oil industry was the destruction of the offshore oil platform Piper Alpha in 1988 in which 167 people lost their lives.\n\nBesides the Ekofisk oil field, the Statfjord oil field is also notable as it was the cause of the first pipeline to span the Norwegian trench. The largest natural gas field in the North Sea, Troll gas field, lies in the Norwegian trench, dropping over , requiring the construction of the enormous Troll A platform to access it.\n\nThe price of Brent Crude, one of the first types of oil extracted from the North Sea, is used today as a standard price for comparison for crude oil from the rest of the world. The North Sea contains western Europe's largest oil and natural gas reserves and is one of the world's key non-OPEC producing regions.\n\nIn the UK sector of the North Sea, the oil industry invested £14.4 billion in 2013, and was on track to spend £13 billion in 2014. Industry body Oil & Gas UK put the decline down to rising costs, lower production, high tax rates, and less exploration.\n\nAs of January 2008 The North Sea region contains 184 offshore rigs, which makes it the region with the highest number of offshore rigs in the world.\n\nThe North Sea is Europe's main fishery accounting for over 5% of international commercial fish caught. Fishing in the North Sea is concentrated in the southern part of the coastal waters. The main method of fishing is trawling.\nIn 1995, the total volume of fish and shellfish caught in the North Sea was approximately 3.5 million tonnes. Besides fish, it is estimated that one million tonnes of unmarketable by-catch is caught and discarded each year.\n\nIn recent decades, overfishing has left many fisheries unproductive, disturbing marine food chain dynamics and costing jobs in the fishing industry. Herring, cod and plaice fisheries may soon face the same plight as mackerel fishing, which ceased in the 1970s due to overfishing.\nThe objective of the European Union Common Fisheries Policy is to minimize the environmental impact associated with resource use by reducing fish discards, increasing productivity of fisheries, stabilising markets of fisheries and fish processing, and supplying fish at reasonable prices for the consumer.\n\nWhaling was an important economic activity from the 9th until the 13th century for Flemish whalers. The medieval Flemish, Basque and Norwegian whalers who were replaced in the 16th century by Dutch, English, Danes and Germans, took massive numbers of whales and dolphins and nearly depleted the right whales. This activity likely led to the extinction of the Atlantic population of the once common gray whale. By 1902 the whaling had ended. After being absent for 300 years a single gray whale returned, it probably was the first of many more to find its way through the now ice-free Northwest Passage. Once \"fish\" were taken in large quantities at the mouth of the River Seine.\n\nIn addition to oil, gas, and fish, the states along the North Sea also take millions of cubic metres per year of sand and gravel from the ocean floor. These are used for beach nourishment, land reclamation\nand construction.\nRolled pieces of amber may be picked up on the east coast of England.\n\nDue to the strong prevailing winds, and shallow water, countries on the North Sea, particularly Germany and Denmark, have used the shore for wind power since the 1990s. The North Sea is the home of one of the first large-scale offshore wind farms in the world, Horns Rev 1, completed in 2002. Since then many other wind farms have been commissioned in the North Sea (and elsewhere). As of 2013 the 630 megawatt (MW) London Array is the largest offshore wind farm in the world, with the 504 (MW) Greater Gabbard wind farm the second largest, followed by the 367 MW Walney Wind Farm. All are off the coast of the UK. These projects will be dwarfed by subsequent wind farms that are in the pipeline, including Dogger Bank at 4,800 MW, Norfolk Bank (7,200 MW), and Irish Sea (4,200 MW). At the end of June 2013 total European combined offshore wind energy capacity was 6,040 MW. UK installed 513.5 MW offshore windpower in the first half-year of 2013.\n\nThe expansion of offshore wind farms has met with some resistance. Concerns have included shipping collisions and environmental effects on ocean ecology and wildlife such as fish and migratory birds, however, these concerns were found to be negligible in a long-term study in Denmark released in 2006 and again in a UK government study in 2009.\nThere are also concerns about reliability, and the rising costs of constructing and maintaining offshore wind farms. Despite these, development of North Sea wind power is continuing, with plans for additional wind farms off the coasts of Germany, the Netherlands, and the UK. There have also been proposals for a transnational power grid in the North Sea to connect new offshore wind farms.\n\nEnergy production from tidal power is still in a pre-commercial stage. The European Marine Energy Centre has installed a wave testing system at Billia Croo on the Orkney mainland and a tidal power testing station on the nearby island of Eday. Since 2003, a prototype Wave Dragon energy converter has been in operation at Nissum Bredning fjord of northern Denmark.\n\nThe beaches and coastal waters of the North Sea are destinations for tourists. The Belgian, Dutch, German and Danish coasts are developed for tourism. The North Sea coast of the United Kingdom has tourist destinations with beach resorts and golf courses. Fife in Scotland is famous for its links golf courses. The coastal City of St. Andrews being renowned as the \"Home of Golf\". The coast of North East England has several tourist towns such as Scarborough, Bridlington, Seahouses, Whitby, Robin Hood's Bay and Seaton Carew. The coast of North East England has long sandy beaches and links golfing locations such as Seaton Carew Golf Club and Goswick Golf Club.\nThe North Sea Trail is a long-distance trail linking seven countries around the North Sea. Windsurfing and sailing are popular sports because of the strong winds. Mudflat hiking, recreational fishing and birdwatching are among other activities.\n\nThe climatic conditions on the North Sea coast have been claimed to be healthful. As early as the 19th century, travellers used their stays on the North Sea coast as curative and restorative vacations. The sea air, temperature, wind, water, and sunshine are counted among the beneficial conditions that are said to activate the body's defences, improve circulation, strengthen the immune system, and have healing effects on the skin and the respiratory system.\n\nThe North Sea is important for marine transport and its shipping lanes are among the busiest in the world. Major ports are located along its coasts: Rotterdam, the busiest port in Europe and the fourth busiest port in the world by tonnage , Antwerp (was 16th) and Hamburg (was 27th), Bremen/Bremerhaven and Felixstowe, both in the top 30 busiest container seaports, as well as the Port of Bruges-Zeebrugge, Europe's leading ro-ro port.\n\nFishing boats, service boats for offshore industries, sport and pleasure craft, and merchant ships to and from North Sea ports and Baltic ports must share routes on the North Sea. The Dover Strait alone sees more than 400 commercial vessels a day. Because of this volume, navigation in the North Sea can be difficult in high traffic zones, so ports have established elaborate vessel traffic services to monitor and direct ships into and out of port.\n\nThe North Sea coasts are home to numerous canals and canal systems to facilitate traffic between and among rivers, artificial harbours, and the sea. The Kiel Canal, connecting the North Sea with the Baltic Sea, is the most heavily used artificial seaway in the world reporting an average of 89 ships per day not including sporting boats and other small watercraft in 2009. It saves an average of , instead of the voyage around the Jutland peninsula. The North Sea Canal connects Amsterdam with the North Sea.\n\n\n\n"}
{"id": "21180", "url": "https://en.wikipedia.org/wiki?curid=21180", "title": "Natural Born Killers", "text": "Natural Born Killers\n\nNatural Born Killers is a 1994 American satirical black comedy crime film directed by Oliver Stone and starring Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones. The film tells the story of two victims of traumatic childhoods who became lovers and mass murderers, and are irresponsibly glorified by the mass media.\n\nThe film is based on an original screenplay by Quentin Tarantino that was heavily revised by Stone, writer David Veloz, and associate producer Richard Rutowski. Tarantino received a story credit. Jane Hamsher, Don Murphy, and Clayton Townsend produced the film, with Arnon Milchan, Thom Mount, and Stone as executive producers.\n\nThe film was released on August 26, 1994 in the United States, and also screened at the Venice Film Festival on August 29, 1994. It was a box office success, grossing over $50 million against a production budget of $34 million. Critics praised the cast's performances, the humor, plot and combination of action and romance; some found the film too violent and graphic. Notorious for its violent content and inspiring \"copycat\" crimes, the film was named the eighth most controversial film in history by \"Entertainment Weekly\" in 2006.\n\nMickey Knox and his wife Mallory stop at a diner in the New Mexico desert. A duo of rednecks arrive and one begins sexually harassing Mallory while she is dancing. She briefly encourages him before beating him to a pulp, screaming \"How sexy am I now, huh flirty boy?\". Mickey and Mallory then murder all but one of the diner's staff, and one customer, culminating in a morbid game of Eeny, meeny, miny, moe to decide who lives and dies. After killing the waitress Mabel, the couple ensures that the only survivor remembers their names so that he can pass on the legend of Mickey and Mallory before they embrace and declare their undying love.\n\nMickey and Mallory camp out in the desert, and Mallory reminisces about when they first met. A dramatised flashback (done in the style of a TV sitcom, including a laughtrack) shows Mickey as a meat deliveryman who came to the house where Mallory lived with her sexually abusive father, her neglectful mother, and her younger brother, Kevin. Mickey and Mallory fall in love instantly and leave together, stealing Mallory's father's car. Soon Mickey is arrested and imprisoned for auto theft, but he subsequently escapes from prison during a tornado and returns to Mallory's house. The two kill Mallory's parents, and Mallory tells Kevin he is \"free\". Mickey and Mallory then go on the road together and get \"married\" on the side of a bridge, celebrating by holding a young woman hostage in their hotel room. Angered by Mickey's desire for a threesome, Mallory drives to a nearby gas station, where she flirts with the mechanic. They begin to have sex on the hood of a car, but Mallory kills him when he recognizes her as a wanted killer. During this time, Mickey rapes the hostage.\n\nThe pair continue their killing spree, ultimately claiming fifty-two victims in New Mexico, Arizona, and Nevada. Pursuing them is Detective Jack Scagnetti, who became obsessed with mass murderers after witnessing his mother being shot and killed by Charles Whitman when he was eight. Beneath his heroic facade, he is a violent psychopath. The audience witnesses Scagnetti violently strangle a prostitute to death. The killers are also followed by self-serving tabloid journalist Wayne Gale. Gale profiles Mickey and Mallory on his show, \"American Maniacs\", soon elevating them to cult hero status.\n\nMickey and Mallory get lost in the desert after taking psychedelic mushrooms. They come across a ranch where they encounter Warren Red Cloud, a Navajo Indian, and his grandson. The Navajo provides them with food and shelter. After the two fall asleep, the Navajo, hoping to expel the demon he perceives in Mickey, begins chanting beside the fire. This provokes nightmares and hallucinations in Mickey about his abusive parents, and he wakes up in a panicked rage and shoots Red Cloud before he realizes what he is doing. Mallory wakes up and begins yelling at Mickey. This is the first time the pair feel guilt following a murder. Fleeing from the scene, they come across a gigantic field of rattlesnakes and are both badly bitten.\n\nThey frantically drive to a drugstore to find snakebite antidote, however the store is completely sold out. Mallory falls to the ground while Mickey runs to the pharmacy with the hopes that there will be more medication in stock. The pharmacist recognizes Mickey and sets off the silent alarm before Mickey kills him. Soon police cars arrive and Mallory is captured and subsequently beaten by the police. A gunfight breaks out between Mickey and the others while Mallory is held hostage. Scagnetti arrives and tells Mickey that unless he surrenders, he will mutilate Mallory. Mickey gives up his guns, but attacks Scagnetti with a knife. The police taser him and the scene ends with Mickey and Mallory being beaten by a group of vengeful policemen as a news crew films the action.\n\nThe story picks up one year later: Mickey and Mallory have been kept in solitary confinement, and are due to be transferred to a mental hospital after being declared insane. Scagnetti arrives at the prison and encounters Warden Dwight McClusky, who tells Scagnetti to kill the Knoxes during their transfer, and then claim they tried to escape. Scagnetti agrees to the plan. Meanwhile, Gale has persuaded Mickey to agree to a live interview that will air immediately after the Super Bowl. During the interview, Mickey gives a speech about how murder provides enlightenment and declares himself a \"natural born killer\". His words inspire the other inmates (who are watching the interview on TV in the recreation room) and incite them to riot.\n\nMcClusky, upon learning of the riot, orders the interview terminated despite Gale's vehement protests. Mickey is left alone with Gale, the film crew and several guards. Using a lengthy joke as a diversion, Mickey overpowers a guard and grabs his shotgun. He kills most of the guards with it and takes the survivors hostage, leading them through the prison riot. Gale follows, giving a live television report as people are beaten and killed around him.\n\nWhile this goes on, Scagnetti enters Mallory's cell and attempts to seduce her. Mallory, feigning reciprocation at first, then violently smashes his face against the wall and breaks his nose. The two guards outside the cell struggle to enter while Mallory attacks Scagnetti. Finally, the two guards subdue her, and Scagnetti sprays her face with tear gas in revenge. Still live on national television, Mickey and Gale arrive at Mallory's cell, where he kills the guards and engages in a Mexican standoff with Scagnetti, eventually feigning a concession just before Mallory slashes Scagnetti's throat with a shank from behind. Mallory then picks up Scagnetti's gun, saying \"You still like me now, Jack?\" and kills him.\n\nMickey and Mallory continue to escape through the riot torn prison, with Gale's entire TV crew getting killed. Gale himself snaps, succumbing to Stockholm syndrome as well as indulging in his own longtime fascination for murder, and begins to shoot at the guards, claiming he finally feels \"alive\". After being rescued by a mysterious prisoner named Owen Traft, the trio of Mickey, Mallory, and Gale run into McClusky and a heavily armed posse of guards. The trio takes cover in a blood-splattered shower room. McClusky threatens to storm the shower room; Mickey, in turn, threatens to kill both Gale and a guard on live TV, and the prisoners walk out the front door, to McClusky's utter dismay, as he helplessly threatens to hunt them down. McClusky and his guards are then quickly massacred by hordes of inmates.\n\nMickey and Mallory steal a van and escape to a forest. They give a final interview to Gale, before they tell him he must die also. He attempts various arguments to change their minds, finally appealing to their trademark practice of leaving one survivor; Mickey informs him they are leaving a witness to tell the tale, his camera. Gale accepts his fate and extends his arms as if on a cross as they shoot him dead while his unattended camera continues to roll. Unbeknownst to the three, the entire exchange is transmitted to a horrified news anchor through Gale's in-ear microphone. The couple is shown several years later, in an RV, with Mickey driving and a pregnant Mallory watching their two children play.\n\n\n\"Natural Born Killers\" was based upon a screenplay written by Quentin Tarantino, in which a married couple suddenly decide to go on a killing spree. Tarantino had sold an option for his script to producers Jane Hamsher and Don Murphy for $10,000 after he had tried, and failed, to direct it himself for $500,000. Hamsher and Murphy subsequently sold the screenplay to Warner Bros. Around the same time, Oliver Stone was made aware of the script. He was keen to find something more straightforward than his previous production, \"Heaven & Earth\"; a difficult shoot which had left him exhausted, and he felt that \"Natural Born Killers\" could be what he was looking for.\n\nDavid Veloz, associate producer Richard Rutowski, and Stone rewrote the script, keeping much of the dialogue but changing the focus of the film from journalist Wayne Gale to Mickey and Mallory. The script was changed so much that as per WGA rules, Tarantino was credited for the film's story only. In a 1993 interview, Tarantino stated that he did not hold any animosity towards Stone, and that he wished the film well.\n\nInitially, when producers Hamsher and Murphy had first brought the script to Stone's attention, he had seen it as an action film; \"something Arnold Schwarzenegger would be proud of.\" As the project developed however, incidents such as the O.J. Simpson case, the Menendez brothers case, the Tonya Harding/Nancy Kerrigan incident, the Rodney King incident, and the Federal assault of the Branch Davidian sect all took place. Stone came to feel that the media was heavily involved in the outcome of all of these cases, and that the media had become an all-pervasive entity which marketed violence and suffering for the good of ratings. As such, he changed the tone of the movie from one of simple action to a satirical critique of the media in general. Also coloring Stone's approach, and contributing to the violent nature of the film, were the anger and sadness he felt at the breakdown of his second marriage. He also said in an interview that the film was influenced by the \"vitality\" of Indian cinema. He let Rodney Dangerfield write or rewrite all of his own character's lines.\n\nDuring pre-production, to prepare for the role of Wayne Gale, Downey spent time with Australian TV shock-king Steve Dunleavy, and later convinced Stone to allow him to portray Gale with an Australian accent. Also during pre-production, Stone tried to convince actress Juliette Lewis to bulk up for the role of Mallory so that she looked tougher, but she refused, saying she wanted the character to look like a pushover, not a bodybuilder.\n\nPrincipal photography took only 56 days to shoot, but the editing process went on for 11 months, with the final film containing almost 3,000 cuts (most films have 600–700). Filming locations included the Rio Grande Gorge Bridge just west of Taos, New Mexico, where the wedding scene was filmed, and Stateville Correctional Center in Joliet, Illinois, where the prison riot was filmed. In Stateville, 80% of the prisoners are incarcerated for violent crimes. For the first two weeks on location at the prison, the extras were actual inmates with rubber weapons. For the subsequent two weeks, 200 extras were needed because the Stateville inmates were on lockdown. According to Tom Sizemore, during filming on the prison set, Stone would play African tribal music at full blast between takes to keep the frantic energy up. While shooting the POV scene wherein Mallory runs into the wire mesh, director of photography Robert Richardson broke his finger and the replacement cameraman cut his eye. According to Oliver Stone, he was not popular with the camera department on set that day. For the scenes involving rear projection, the projected footage was shot prior to principal photography, then edited together, and projected onto the stage, behind the live actors. For example, when Mallory drives past a building and flames are projected onto the wall, this was shot live using footage projected onto the facade of a real building.\n\nThe famous Coca-Cola polar bear ad is seen twice during the film. According to Stone, Coca-Cola approved the use of the ad without having a full idea of what the film was about. When they saw the completed film, they were furious.\n\nThe film's soundtrack was produced by Stone and Trent Reznor of Nine Inch Nails, who reportedly watched the film over 50 times to \"get in the mood\". Reznor reportedly produced the soundtrack while on tour. On his approach to compiling the soundtrack, Reznor told MTV:\n\nI suggested to Oliver [Stone] to try to turn the soundtrack into a collage-of-sound, kind of the way the movie used music: make edits, add dialog, and make it something interesting, rather than a bunch of previously released music.\n\nSome songs were written especially for the film or soundtrack, such as \"Burn\" by Nine Inch Nails.\n\n\"Natural Born Killers\" is shot and edited in a frenzied and psychedelic style consisting of black and white, animation, and other unusual color schemes, and employing a wide range of camera angles, filters, film stocks, lenses, and special effects. Much of the film is told via parodies of television shows, including a scene (\"I Love Mallory\") presented in the style of a sitcom about a dysfunctional family. Commercials which were commonly on the air at the time of the film's release make brief, intermittent appearances. In his DVD director's commentary, Stone goes into great detail about the look of the film, explaining scene by scene why a particular look was chosen for a particular scene.\n\nAccording to Hollywood.com, \"Natural Born Killers\" is a satirical crime film, while Foster Hirsch deemed it \"the crime film as would-be social and cultural satire\". Stone considered it his road film, specifically naming \"Bonnie and Clyde\" as a source of inspiration. The famous death scene in \"Bonnie and Clyde\" used innovative editing techniques provided by multiple cameras shot from different angles at different speeds; this sporadic interchange between fast-paced and slow-motion editing that concludes Arthur Penn's film is used throughout the entirety of \"Natural Born Killers\".\n\nFurthermore, both films fall under the road film genre through their constant challenges of the society in which the characters live. While Bonnie and Clyde attempt to disintegrate the weakened economic and social landscape of the 1930s, Mickey and Mallory try to free America from the overarching conventions which influence the common masses, primarily the media. However, whilst \"Bonnie and Clyde\" concludes with a pessimistic outlook regarding individual freedom within the American sphere of influence, Oliver Stone sees \"Natural Born Killers\" as having an optimistic finale. In \"Bonnie and Clyde\", the police's ambush of the couple exhibits the empirical control of law enforcement over the individual. \"Natural Born Killers\", however, ends with the couple symbolically destroying the mass media, as represented by Wayne Gale, and successfully fleeing together to live a relatively \"normal\" life. As Stone himself says, \"\"In its own way, \"Natural Born Killers\" is ultimately a very optimistic film about the future. It's about freedom, and the ability of every human being to get it\".\"\n\nIn its opening weekend, \"Natural Born Killers\" grossed a total of $11.2 million in 1,510 theaters, finishing 1st. It finished its theatrical run with a total gross of $50.3 million, against its $34 million budget.\n\nOn review aggregator Rotten Tomatoes, the film has an approval rating of 47% based on 38 reviews, with an average rating of 5.7/10. On Metacritic, the film has an average weighted score of 74 out of 100, based on 20 critics, indicating \"generally favorable reviews\". Audiences polled by CinemaScore gave the film an average grade of \"B–\" on an A+ to F scale.\n\nRoger Ebert of the \"Chicago Sun-Times\" gave the film four stars out of four and wrote, \"Seeing this movie once is not enough. The first time is for the visceral experience, the second time is for the meaning.\" On his television show, his partner Gene Siskel agreed with him, adding extra praise to the scene featuring Rodney Dangerfield.\n\nOther critics found the film unsuccessful in its aims. Hal Hinson of \"The Washington Post\" claimed that \"Stone's sensibility is white-hot and personal. As much as he'd like us to believe that his camera is turned outward on the culture, it's vividly clear that he can't resist turning it inward on himself. This wouldn't be so troublesome if Stone didn't confuse the public and the private.\" Janet Maslin of \"The New York Times\" wrote, \"for all its surface passions, \"Natural Born Killers\" never digs deep enough to touch the madness of such events, or even to send them up in any surprising way. Mr. Stone's vision is impassioned, alarming, visually inventive, characteristically overpowering. But it's no match for the awful truth.\"\n\nJames Berardinelli gave the film a negative review but his criticism was different from many other such pans, which generally said that Oliver Stone was a hypocrite for making an ultra-violent film in the guise of a critique of American attitudes. Berardinelli noted that the movie \"hits the bullseye\" as a satire of America's lust for bloodshed, but repeated Stone's main point so often and so loudly that it became unbearable.\n\nLionsgate Films released a director's cut on DVD. Stone himself retained ownership of his preferred cut. Distribution rights to the director's cut reverted from Lionsgate to Warner Bros. in 2009, giving Warner all distribution rights.\n\nAfter Quentin Tarantino attempted to publish his original screenplay to \"Natural Born Killers\" as a paperback book, as he had done with his scripts for \"True Romance\" and his own directorial efforts, \"Reservoir Dogs\" and \"Pulp Fiction\", the producers of \"Natural Born Killers\" filed a lawsuit against Tarantino, claiming that when he sold the script to them, he had forfeited the publishing rights; eventually, Tarantino was allowed to publish his original script.\n\nTarantino disowned the film, saying, \"I hated that fucking movie. If you like my stuff, don't watch that movie.\"\n\nWhen the film was first handed in to the MPAA, they told Stone they would give it an NC-17 unless he cut it. As such, Stone toned down the violence by cutting approximately four minutes of footage, and the MPAA re-rated the film as an R. In 1996, a Director's Cut was released on home video by Vidmark Entertainment and Pioneer Entertainment, as Warner Bros. wanted nothing to do with that particular version. Warner Home Video later released this cut on Blu-ray.\n\nThe film was banned completely upon release in Ireland, including – controversially – from cinema clubs. The ban was later quietly lifted.\n\nIn the UK, though the cinema release was delayed while the BBFC investigated reports that the film caused copycat murders in the USA and France, it was finally shown in cinemas in February 1995.\n\nThe original intended UK home video release in March 1996 was cancelled due to the Dunblane massacre in Scotland. In the meantime, Channel Five showed the film in November 1997. It was finally released on video in July 2001.\n\n\"Entertainment Weekly\" ranked the film as the eighth most controversial film ever.\n\nFrom almost the moment of its release, the film has been accused of encouraging and inspiring numerous murderers in North America, including the Heath High School shooting and the Columbine High School massacre.\n\nThe soundtrack was released August 23, 1994 by Interscope Records.\n\nTracks 10, 13, 18, 20, 23, and 25 are assembled from various recordings and dialogue from the film.\n\n\n\n \n"}
{"id": "21181", "url": "https://en.wikipedia.org/wiki?curid=21181", "title": "Nancy Reagan", "text": "Nancy Reagan\n\nNancy Davis Reagan (born Anne Frances Robbins; July 6, 1921 – March 6, 2016) was an American film actress and the wife of Ronald Reagan, the 40th President of the United States. She was the First Lady of the United States from 1981 to 1989.\n\nShe was born in New York City. After her parents separated, she lived in Maryland with an aunt and uncle for several years. When her mother remarried in 1929, she moved to Chicago and later took the name Davis from her stepfather. As Nancy Davis, she was a Hollywood actress in the 1940s and 1950s, starring in films such as \"The Next Voice You Hear...\", \"Night into Morning\", and \"Donovan's Brain\". In 1952, she married Ronald Reagan, who was then president of the Screen Actors Guild. They had two children together. Reagan was the First Lady of California when her husband was Governor from 1967 to 1975, and she began to work with the Foster Grandparents Program.\n\nReagan became First Lady of the United States in January 1981, following her husband's victory in the 1980 presidential election. Early in his first term, she was criticized largely due to her decision to replace the White House china, which had been paid for by private donations. Following years of lax formality, she decided to restore a Kennedyesque glamour to the White House, and her interest in high-end fashion garnered much attention as well as criticism. She championed recreational drug prevention causes when she founded the \"Just Say No\" drug awareness campaign, which was considered her major initiative as First Lady. More discussion of her role ensued following a 1988 revelation that she had consulted an astrologer to assist in planning the president's schedule after the attempted assassination of her husband in 1981. She generally had a strong influence on her husband and played a role in a few of his personnel and diplomatic decisions.\n\nAfter Ronald Reagan left the White House in 1989, the couple retired to their home in Bel Air, Los Angeles, California. Nancy devoted most of her time to caring for her husband, who was diagnosed with Alzheimer's disease in 1994, until his death at the age of 93 on June 5, 2004. Reagan remained active within the Reagan Library and in politics, particularly in support of embryonic stem cell research, until her death from congestive heart failure at age 94 on March 6, 2016.\nAnne Frances Robbins was born on July 6, 1921, at Sloane Hospital for Women, located in Midtown Manhattan. She was the only child of Kenneth Seymour Robbins (1894–1972), a farmer turned car salesman who had been born into a once-prosperous family, and his actress wife, Edith Prescott Luckett (1888–1987). Her godmother was silent-film-star Alla Nazimova. From birth, she was commonly called Nancy.\n\nShe lived her first two years in Flushing, Queens, an outer borough of New York City, in a two-story house on Roosevelt Avenue between 149th and 150th Streets. Her parents separated soon after her birth and were divorced in 1928. After their separation, her mother traveled the country to pursue acting jobs and Robbins was raised in Bethesda, Maryland, for six years by her aunt, Virginia Luckett, and uncle, Audley Gailbraith. Nancy later described longing for her mother during those years: \"My favorite times were when Mother had a job in New York, and Aunt Virgie would take me by train to stay with her.\"\n\nIn 1929, her mother married Loyal Edward Davis (1896–1982), a prominent conservative neurosurgeon who moved the family to Chicago. Nancy and her stepfather got along very well; she later wrote that he was \"a man of great integrity who exemplified old-fashioned values.\" He formally adopted her in 1935, and she would always refer to him as her father. At the time of the adoption, her name was legally changed to Nancy Davis. She attended the Girls' Latin School of Chicago (describing herself as an average student), graduated in 1939, and later attended Smith College in Massachusetts, where she majored in English and drama, and graduated in 1943.\n\nIn 1940, a young Davis had appeared as a National Foundation for Infantile Paralysis volunteer in a memorable short subject shown in movie theaters to raise donations for the crusade against polio. \"The Crippler\" featured a sinister figure spreading over playgrounds and farms, laughing over its victims, until finally dispelled by the volunteer. It was very effective in raising contributions.\n\nFollowing her graduation from college, Davis held jobs in Chicago as a sales clerk in Marshall Field's department store and as a nurse's aide. With the help of her mother's colleagues in theatre, including ZaSu Pitts, Walter Huston, and Spencer Tracy, she pursued a professional career as an actress. She first gained a part in Pitts' 1945 road tour of \"Ramshackle Inn\", moving to New York City. She landed the role of Si-Tchun, a lady-in-waiting, in the 1946 Broadway musical about the Orient, \"Lute Song\", starring Mary Martin and a pre-fame Yul Brynner. The show's producer told her, \"You look like you could be Chinese.\"\n\nAfter passing a screen test, she moved to California and signed a seven-year contract with Metro-Goldwyn-Mayer Studios Inc. (MGM) in 1949; she later remarked, \"Joining Metro was like walking into a dream world.\" Her combination of attractive appearance—centered on her large eyes—and somewhat distant and understated manner made her hard at first for MGM to cast and publicize. Davis appeared in eleven feature films, usually typecast as a \"loyal housewife\", \"responsible young mother\", or \"the steady woman\". Jane Powell, Debbie Reynolds, Leslie Caron, and Janet Leigh were among the actresses with whom she competed for roles at MGM.\n\nDavis' film career began with small supporting roles in two films that were released in 1949, \"The Doctor and the Girl\" with Glenn Ford and \"East Side, West Side\" starring Barbara Stanwyck. She played a child psychiatrist in the film noir \"Shadow on the Wall\" (1950) with Ann Sothern and Zachary Scott; her performance was called \"beautiful and convincing\" by \"New York Times\" critic A. H. Weiler. She co-starred in 1950's \"The Next Voice You Hear...\", playing a pregnant housewife who hears the voice of God from her radio. Influential reviewer Bosley Crowther of \"The New York Times\" wrote that \"Nancy Davis [is] delightful as [a] gentle, plain, and understanding wife.\" In 1951, Davis appeared in \"Night into Morning\", her favorite screen role, a study of bereavement starring Ray Milland. Crowther said that Davis \"does nicely as the fiancée who is widowed herself and knows the loneliness of grief,\" while another noted critic, \"The Washington Post\"<nowiki>'</nowiki>s Richard L. Coe, said Davis \"is splendid as the understanding widow.\" MGM released Davis from her contract in 1952; she sought a broader range of parts, but also married Reagan, keeping her professional name as Davis, and had her first child that year. She soon starred in the science fiction film \"Donovan's Brain\" (1953); Crowther said that Davis, playing the role of a possessed scientist's \"sadly baffled wife,\" \"walked through it all in stark confusion\" in an \"utterly silly\" film. In her next-to-last movie, \"Hellcats of the Navy\" (1957), she played nurse Lieutenant Helen Blair, and appeared in a film for the only time with her husband, playing what one critic called \"a housewife who came along for the ride.\" Another reviewer, however, stated that Davis plays her part satisfactorily, and \"does well with what she has to work with.\"\n\nAuthor Garry Wills has said that Davis was generally underrated as an actress because her constrained part in \"Hellcats\" was her most widely seen performance. In addition, Davis downplayed her Hollywood goals: promotional material from MGM in 1949 said that her \"greatest ambition\" was to have a \"successful happy marriage\"; decades later, in 1975, she would say, \"I was never really a career woman but [became one] only because I hadn't found the man I wanted to marry. I couldn't sit around and do nothing, so I became an actress.\" Ronald Reagan biographer Lou Cannon nevertheless characterized her as a \"reliable\" and \"solid\" performer who held her own in performances with better-known actors. After her final film, \"Crash Landing\" (1958), Davis appeared for a brief time as a guest star in television dramas, such as the \"Zane Grey Theatre\" episode \"The Long Shadow\" (1961), where she played opposite Ronald Reagan, as well as \"Wagon Train\" and \"The Tall Man\", until she retired as an actress in 1962.\n\nDuring her career, Davis served for nearly ten years on the board of directors of the Screen Actors Guild. Decades later, Albert Brooks attempted to coax her out of acting retirement by offering her the title role opposite himself in his 1996 film \"Mother\". She declined in order to care for her husband, and Debbie Reynolds played the part.\n\nDuring her Hollywood career, Davis dated many actors, including Clark Gable, Robert Stack, and Peter Lawford; she later called Gable the nicest of the stars she had met. On November 15, 1949, she met Ronald Reagan, who was then president of the Screen Actors Guild. She had noticed that her name had appeared on the Hollywood blacklist, and sought Ronald Reagan's help to maintain her employment as a guild actress in Hollywood, and for assistance in having her name removed from the list. Ronald Reagan informed her that she had been confused with another actress of the same name. The two began dating and their relationship was the subject of many gossip columns; one Hollywood press account described their nightclub-free times together as \"the romance of a couple who have no vices\". Ronald Reagan was skeptical about marriage, however, following his painful 1949 divorce from Jane Wyman, and he still saw other women.\n\nAfter three years of dating, they eventually decided to marry while discussing the issue in the couple's favorite booth at Chasen's, a restaurant in Beverly Hills. They married on March 4, 1952, at the Little Brown Church in the San Fernando Valley of Los Angeles, in a simple and hastily arranged ceremony designed to avoid the press. The only people in attendance were fellow actor William Holden (the best man) and his wife, actress Brenda Marshall (the matron of honor). Nancy was perhaps already pregnant during the ceremony; the couple's first child, Patricia Ann Reagan (later better known by her professional name, Patti Davis), was born less than eight months later on October 21, 1952. Their son, Ronald Prescott Reagan (later better known as Ron Reagan) was born six years later on May 20, 1958. Reagan also became stepmother to Maureen Reagan (1941–2001) and Michael Reagan (born 1945), her husband's children from his first marriage to Jane Wyman.\nObservers described Nancy and Ronald's relationship as intimate. As President and First Lady, the Reagans were reported to display their affection frequently, with one press secretary noting, \"They never took each other for granted. They never stopped courting.\" Ronald often called Nancy \"Mommy\"; she called him \"Ronnie\". While the President was recuperating in the hospital after the 1981 assassination attempt, Nancy wrote in her diary, \"Nothing can happen to my Ronnie. My life would be over.\" In a letter to Nancy, Ronald wrote, \"whatever I treasure and enjoy ... all would be without meaning if I didn't have you.\" When her husband was afflicted with Alzheimer's disease In 1998, Nancy told \"Vanity Fair\", \"Our relationship is very special. We were very much in love and still are. When I say my life began with Ronnie, well, it's true. It did. I can't imagine life without him.\" Nancy was known for the focused and attentive look, termed \"the Gaze\", that she fastened upon her husband during his speeches and appearances.\n\nPresident Reagan's death in June 2004 ended what Charlton Heston called \"the greatest love affair in the history of the American Presidency.\"\n\nNancy's relationship with her children was not always as close as the bond with her husband. She frequently quarreled with her children and her stepchildren. Her relationship with Patti was the most contentious; Patti flouted American conservatism, rebelled against her parents by joining the nuclear freeze movement, and authored many anti-Reagan books. The nearly 20 years of family feuding left Patti very much estranged from both her mother and father. Soon after her father was diagnosed with Alzheimer's disease, Patti and her mother reconciled and began to speak on a daily basis. Nancy's disagreements with Michael were also public matters; in 1984, she was quoted as saying that the two were in an \"estrangement right now\". Michael responded that Nancy was trying to cover up for the fact she had not met his daughter, Ashley, who had been born nearly a year earlier. They too eventually made peace. Nancy was thought to be closest to her stepdaughter Maureen during the White House years, but each of the Reagan children experienced periods of estrangement from their parents.\n\nNancy Reagan was First Lady of California during her husband's two terms as governor. She disliked living in the state capital of Sacramento, which lacked the excitement, social life, and mild climate to which she was accustomed in Los Angeles. She first attracted controversy early in 1967, when, after four months' residence in the California Governor's Mansion in Sacramento, she moved her family into a wealthy suburb, because fire officials had labeled the mansion as a \"firetrap.\" Though the Reagans leased the new house at their expense, the move was viewed as snobbish when the matter was brought to the attention of the greater public. Reagan defended her actions as being for the good of her family, a judgment with which her husband readily agreed. Friends of the family later helped support the cost of the leased house, while Reagan supervised construction of a new ranch-style governor's residence in nearby Carmichael. The new residence was finished just as Ronald Reagan left office in 1975, but his successor, Jerry Brown, refused to live there. It was sold in 1982, and California governors lived in improvised arrangements until Brown moved into the Governor's Mansion in 2015.\n\nIn 1967, Governor Reagan appointed his wife to the California Arts Commission, and a year later she was named \"Los Angeles Times\"' Woman of the Year; in its profile, the \"Times\" labeled her \"A Model First Lady\". Her glamour, style, and youthfulness,\nmade her a frequent subject for press photographers. As first lady, Reagan visited veterans, the elderly, and the handicapped, and worked with a number of charities. She became involved with the Foster Grandparents Program, helping to popularize it in the United States and Australia. She later expanded her work with the organization after arriving in Washington, and wrote about her experiences in her 1982 book \"To Love a Child\". The Reagans held dinners for former POWs and Vietnam War veterans while governor and first lady.\n\nGovernor Reagan's term ended in 1975, and he did not run for a third; instead, he met with advisors to discuss a possible bid for the presidency in 1976, challenging incumbent President Gerald Ford. Ronald still needed to convince a reluctant Nancy before running, however. She feared for her husband's health and his career as a whole, though she felt that he was the right man for the job and eventually approved. Nancy took on a more traditional role in the campaign, holding coffees, luncheons, and talks, with senior citizens. With that, she oversaw personnel, monitored her husband's schedule, and occasionally provided press conferences. The 1976 campaign included the so-called \"battle of the queens\", contrasting Nancy with First Lady Betty Ford. They both spoke out over the course of the campaign on similar issues, but with different approaches. Nancy was particularly upset by the warmonger image that the Ford campaign had drawn of her husband.\n\nThough he lost the 1976 Republican nomination, Ronald Reagan ran again for the presidency in 1980, and succeeded in winning the nomination and election. During this second campaign, Nancy played a very prominent role, and her management of staff became more apparent. She organized a meeting among feuding campaign managers John Sears and Michael Deaver, and her husband, which resulted in Deaver leaving the campaign and Sears being given full control. After the Reagan camp lost the Iowa Caucus and fell behind in New Hampshire polls, Nancy organized a second meeting and decided it was time to fire Sears and his associates; she gave Sears a copy of the press release announcing his dismissal. Her influence on her husband became particularly notable; her presence at rallies, luncheons, and receptions increased his confidence.\n\nReagan became the First Lady of the United States when Ronald Reagan was inaugurated as president in January 1981. Early in her husband's presidency, Reagan stated her desire to create a more suitable \"first home\" in the White House, as the building had fallen into a state of disrepair following years of neglect. White House aide Michael Deaver described the second and third floor family residence as having \"cracked plaster walls, chipped paint [and] beaten up floors\"; rather than use government funds to renovate and redecorate, she sought private donations. In 1981, Reagan directed a major renovation of several White House rooms, including all of the second and third floors and rooms adjacent to the Oval Office, including the press briefing room. The renovation included repainting walls, refinishing floors, repairing fireplaces, and replacing antique pipes, windows, and wires. The closet in the master bedroom was converted into a beauty parlor and dressing room, and the West bedroom was made into a small gymnasium.\n\nThe First Lady secured the assistance of renowned interior designer Ted Graber, popular with affluent West Coast social figures, to redecorate the family living quarters. A Chinese-pattern, handpainted wallpaper was added to the master bedroom. Family furniture was placed in the president's private study. The First Lady and her designer retrieved a number of White House antiques, which had been in storage, and placed them throughout the mansion. In addition, many of Reagan's own collectibles were put out for display, including around twenty-five Limoges Boxes, as well as some porcelain eggs and a collection of plates.\n\nThe extensive redecoration was paid for by private donations. Many significant and long-lasting changes occurred as a result of the renovation and refurbishment, of which Reagan said, \"This house belongs to all Americans, and I want it to be something of which they can be proud.\" The renovations received some criticisms for being funded by tax-deductible donations, meaning some of it eventually did indirectly come from the tax-paying public.\n\nReagan's interest in fashion was another one of her trademarks. While her husband was still president-elect, press reports speculated about Reagan's social life and interest in fashion. In many press accounts, Reagan's sense of style was favorably compared to that of a previous First Lady, Jacqueline Kennedy. Friends and those close to her remarked that, while fashionable like Kennedy, she would be different from other first ladies; close friend Harriet Deutsch was quoted as saying, \"Nancy has her own imprint.\"\n\nReagan's wardrobe consisted of dresses, gowns, and suits made by luxury designers, including James Galanos, Bill Blass, and Oscar de la Renta. Her white, hand-beaded, one shoulder Galanos 1981 inaugural gown was estimated to cost $10,000, while the overall price of her inaugural wardrobe was said to cost $25,000. She favored the color red, calling it \"a picker-upper\", and wore it accordingly. Her wardrobe included red so often that the fire-engine shade became known as \"Reagan red\". She employed two private hairdressers, who would style her hair on a regular basis in the White House.\n\nFashion designers were pleased with the emphasis Reagan placed on clothing. Adolfo said the first lady embodied an \"elegant, affluent, well-bred, chic American look\", while Bill Blass commented, \"I don't think there's been anyone in the White House since Jacqueline Kennedy Onassis who has her flair.\" William Fine, president of cosmetic company Frances Denney, noted that she \"stays in style, but she doesn't become trendy.\"\n\nThough her elegant fashions and wardrobe were hailed as a \"glamorous paragon of chic\", they were also controversial subjects. In 1982, she revealed that she had accepted thousands of dollars in clothing, jewelry, and other gifts, but defended her actions by stating that she had borrowed the clothes, and that they would either be returned or donated to museums, and that she was promoting the American fashion industry. Facing criticism, she soon said she would no longer accept such loans. While often buying her clothes, she continued to borrow and sometimes keep designer clothes throughout her time as first lady, which came to light in 1988. None of this had been included on financial disclosure forms; the non-reporting of loans under $10,000 in liability was in violation of a voluntary agreement the White House had made in 1982, while not reporting more valuable loans or clothes not returned was a possible violation of the Ethics in Government Act. Reagan expressed through her press secretary \"regrets that she failed to heed counsel's advice\" on disclosing them.\n\nDespite the controversy, many designers who allowed her to borrow clothing, noted that the arrangement was good for their businesses, as well as for the American fashion industry overall. In 1989, Reagan was honored at the annual gala awards dinner of the Council of Fashion Designers of America, during which she received the council's lifetime achievement award. Barbara Walters said of her, \"She has served every day for eight long years the word 'style.'\"\n\nApproximately a year into her husband's first term, Nancy explored the idea of ordering new state china service for the White House. A full china service had not been purchased since the Truman administration in the 1940s, as only a partial service was ordered in the Johnson administration. She was quoted as saying, \"The White House really badly, badly needs china.\" Working with Lenox, the primary porcelain manufacturer in America, the first lady chose a design scheme of a red with etched gold band, bordering the scarlet and cream colored ivory plates with a raised presidential seal etched in gold in the center. The full service comprised 4,370 pieces, with 19 pieces per individual set. The service totaled $209,508. Although it was paid for by private donations, some from the private J. P. Knapp Foundation, the purchase generated quite a controversy, for it was ordered at a time when the nation was undergoing an economic recession. Furthermore, news of the china purchase emerged at the same time that her husband's administration had proposed school lunch regulations that would allow ketchup to be counted as a vegetable.\nThe new china, White House renovations, expensive clothing, and her attendance at the wedding of Charles and Diana, Prince and Princess of Wales, gave her an aura of being \"out of touch\" with the American people during the recession. This built upon the reputation she had coming to Washington, wherein many people concluded that Reagan was a vain and shallow woman, and her taste for splendor inspired the derogatory nickname \"Queen Nancy\". While Jacqueline Kennedy had also faced some press criticism for her spending habits, Reagan's treatment was much more consistent and negative. In an attempt to deflect the criticism, she self-deprecatingly donned a baglady costume at the 1982 Gridiron Dinner and sang \"Second-Hand Clothes\", mimicking the song \"Second-Hand Rose\". The skit helped to restore her reputation.\n\nReagan reflected on the criticisms in her 1989 autobiography, \"My Turn\". She described lunching with former Democratic National Committee chairman Robert S. Strauss, wherein Strauss said to her, \"When you first came to town, Nancy, I didn't like you at all. But after I got to know you, I changed my mind and said, 'She's some broad!'\" Reagan responded, \"Bob, based on the press reports I read then, I wouldn't have liked me either!\"\nAfter the presidency of Jimmy Carter (who dramatically reduced the formality of presidential functions), Reagan brought a Kennedy-esque glamour back into the White House. She hosted 56 state dinners over eight years. She remarked that hosting the dinners is \"the easiest thing in the world. You don't have to do anything. Just have a good time and do a little business. And that's the way Washington works.\" The White House residence staff found Reagan demanding to work for during the preparation for the state dinners, with the First Lady overseeing every aspect of meal presentations, and sometimes requesting one dessert after another be prepared, before finally settling on one she approved of.\n\nIn general, the First Lady's desire for everything to appear just right in the White House led the residence staff to consider her not easy to work for, with tirades following what she perceived as mistakes. One staffer later recalled, \"I remember hearing her call for her personal maid one day and it scared the dickens out of me—just her tone. I never wanted to be on the wrong side of her.\" She did show loyalty and respect to a number of the staff. In particular, she came to the public defense of a maid who was indicted on charges of helping to smuggle ammunition to Paraguay, providing an affidavit to the maid's good character (even though it was politically inopportune to do so at the time of the Iran–Contra affair); charges were subsequently dropped, and the maid returned to work at the White House.\n\nIn 1987, Mikhail Gorbachev became the first Soviet leader to visit Washington, D.C. since Nikita Khrushchev made the trip in 1959 at the height of the Cold War. Nancy was in charge of planning and hosting the important and highly anticipated state dinner, with the goal to impress both the Soviet leader and especially his wife Raisa Gorbacheva. After the meal, she recruited pianist Van Cliburn to play a rendition of \"Moscow Nights\" for the Soviet delegation, to which Mikhail and Raisa broke out into song. Secretary of State George P. Shultz later commented on the evening, saying \"We felt the ice of the Cold War crumbling.\" Reagan concluded, \"It was a perfect ending for one of the great evenings of my husband's presidency.\"\n\nThe First Lady launched the \"Just Say No\" drug awareness campaign in 1982, which was her primary project and major initiative as first lady. Reagan first became aware of the need to educate young people about drugs during a 1980 campaign stop in Daytop village, New York. She remarked in 1981 that \"Understanding what drugs can do to your children, understanding peer pressure and understanding why they turn to drugs is ... the first step in solving the problem.\" Her campaign focused on drug education and informing the youth of the danger of drug abuse.\n\nIn 1982, Reagan was asked by a schoolgirl what to do when offered drugs; Reagan responded: \"Just say no.\" The phrase proliferated in the popular culture of the 1980s, and was eventually adopted as the name of club organizations and school anti-drug programs. Reagan became actively involved by traveling more than throughout the United States and several nations, visiting drug abuse prevention programs and drug rehabilitation centers. She also appeared on television talk shows, recorded public service announcements, and wrote guest articles. She appeared in single episodes of the television drama \"Dynasty\" and the sitcom \"Diff'rent Strokes\", to underscore support for the \"Just Say No\" campaign, and in a rock music video, \"Stop the Madness\" (1985).\n\nIn 1985, Reagan expanded the campaign to an international level by inviting the First Ladies of various nations to the White House for a conference on drug abuse. On October 27, 1986, President Reagan signed a drug enforcement bill into law, which granted $1.7 billion in funding to fight the perceived crisis and ensured a mandatory minimum penalty for drug offenses. Although the bill was criticized, Reagan considered it a personal victory. In 1988, she became the first First Lady invited to address the United Nations General Assembly, where she spoke on international drug interdiction and trafficking laws.\n\nCritics of Reagan's efforts questioned their purpose, labelled Reagan's approach to promoting drug awareness as simplistic, and argued that the program did not address many social issues associated with drug use, including unemployment, poverty, and family dissolution. A number of \"Just Say No\" clubs and organizations remain in operation around the country.\n\nReagan assumed the role of unofficial \"protector\" for her husband after the attempted assassination of him in 1981. On March 30 of that year, President Reagan and three others were shot by troubled 25-year old John Hinckley, Jr as they left the Washington Hilton hotel. Nancy was alerted and arrived at George Washington University Hospital, where the President was hospitalized. She recalled having seen \"emergency rooms before, but I had never seen one like this – with my husband in it.\" She was escorted into a waiting room, and when granted access to see her husband, he quipped to her, \"Honey, I forgot to duck\", borrowing the defeated boxer Jack Dempsey's jest to his wife.\n\nAn early example of the First Lady's protective nature occurred when Senator Strom Thurmond entered the President's hospital room that day in March, passing the Secret Service detail by claiming he was the President's \"close friend\", presumably to acquire media attention. Nancy was outraged and demanded he leave. While the President recuperated in the hospital, the First Lady slept with one of his shirts to be comforted by the scent. When Ronald Reagan was released from the hospital on April 12, she escorted him back to the White House.\n\nPress accounts framed Reagan as her husband's \"chief protector\", an extension of their general initial framing of her as a helpmate and a Cold War domestic ideal. As it happened, the day after her husband was shot, Reagan fell off a chair while trying to take down a picture to bring to him in the hospital; she suffered several broken ribs, but was determined to not reveal it publicly.\n\nDonald Regan's 1988 memoir, \"For the Record: From Wall Street to Washington\", exposes his disagreements with Reagan, for the first time revealing publicly that she had a personal astrologer (then-yet-unnamed Joan Quigley), with whom she consulted and who helped steer the President's decisions. Regan wrote:\n\nNancy Reagan stated in her memoirs, \"I felt panicky every time [Ronald Reagan] left the White House\" following the assassination attempt, and made it her concern to know her husband's schedule: the events he would be attending, and with whom. Eventually, this protectiveness led to her consulting an astrologer, Joan Quigley, who offered insight on which days were \"good\", \"neutral\", or should be avoided, which influenced her husband's White House schedule. Days were color-coded according to the astrologer's advice to discern precisely which days and times would be optimal for the president's safety and success.\n\nThe White House Chief of Staff, Donald Regan, grew frustrated with this regimen, which created friction between him and the First Lady. This escalated with the revelation of the Iran–Contra affair, an administration scandal, in which the First Lady felt Regan was damaging the president. She thought he should resign, and expressed this to her husband, although he did not share her view. Regan wanted President Reagan to address the Iran-Contra matter in early 1987 by means of a press conference, though Reagan refused to allow her husband to overexert himself due to a recent prostate surgery and astrological warnings. Regan became so angry with Reagan that he hung up on her during a 1987 telephone conversation. According to the recollections of ABC News correspondent Sam Donaldson, when the President heard of this treatment, he demanded—and eventually received—Regan's resignation. Vice President George H. W. Bush is also reported to have suggested to Reagan to have Regan fired.\nIn his 1988 memoirs, Regan wrote about Reagan's consultations with the astrologer, the first public mention of them, which resulted in embarrassment for the First Lady. Reagan later wrote, \"Astrology was simply one of the ways I coped with the fear I felt after my husband almost died ... Was astrology one of the reasons [further attempts did not occur]? I don't \"really\" believe it was, but I don't \"really\" believe it wasn't.\"\nNancy wielded a powerful influence over President Reagan. Again stemming from the assassination attempt, she strictly controlled access to the president and even occasionally attempted to influence her husband's decision making.\n\nBeginning in 1985, she strongly encouraged her husband to hold \"summit\" conferences with Soviet general secretary Mikhail Gorbachev, and suggested they form a personal relationship beforehand. Both Ronald Reagan and Gorbachev had developed a productive relationship through their summit negotiations. The relationship between Nancy Reagan and Raisa Gorbacheva was anything but the friendly, diplomatic one between their husbands; Reagan found Gorbacheva hard to converse with and their relationship was described as \"frosty\". The two women usually had tea and discussed differences between the USSR and the United States. Visiting the United States for the first time in 1987, Gorbacheva irked Reagan with lectures on subjects ranging from architecture to socialism, reportedly prompting the American president's wife to quip, \"Who does that dame think she is?\"\n\nPress framing of Reagan changed from that of just helpmate and protector to someone with hidden power. As the image of her as a political interloper grew, she sought to explicitly deny that she was the power behind the throne. At the end of her time as First Lady, however, she said that her husband had not been well-served by his staff. She acknowledged her role in reaction in influencing him on personnel decisions, saying \"In no way do I apologize for it.\" She wrote in her memoirs, \"I don't think I was as bad, or as extreme in my power or my weakness, as I was depicted,\" but went on, \"However the first lady fits in, she has a unique and important role to play in looking after her husband. And it's only natural that she'll let him know what she thinks. I always did that for Ronnie, and I always will.\"\n\nIn October 1987, a mammogram detected a lesion in Reagan's left breast and she was subsequently diagnosed with breast cancer. She chose to undergo a mastectomy rather than a lumpectomy, and the breast was removed on October 17, 1987. Ten days after the operation, her 99-year-old mother, Edith Luckett Davis, died in Phoenix, Arizona, leading Reagan to dub the period \"a terrible month\".\n\nAfter the surgery, more women across the country had mammograms, which exemplified the influence that the First Lady possessed.\n\nThough Reagan was a controversial First Lady, 56 percent of Americans had a favorable opinion of her when her husband left office on January 20, 1989, with 18 percent having an unfavorable opinion, and the balance not giving an opinion. Compared to fellow First Ladies when their husbands left office, Reagan's approval was higher than those of Rosalynn Carter and Hillary Clinton. However, she was less popular than Barbara Bush, and her disapproval rating was double that of Carter's.\n\nUpon leaving the White House, the couple returned to California, where they purchased a home in the wealthy East Gate Old Bel Air neighborhood of Bel Air, Los Angeles, dividing their time between Bel Air and the Reagan Ranch in Santa Barbara, California. Ronald and Nancy regularly attended the Bel Air Church as well. After leaving Washington, Reagan made numerous public appearances, many on behalf of her husband. She continued to reside at the Bel Air home, where she lived with her husband until he died on June 5, 2004.\n\nIn late 1989, the former First Lady established the Nancy Reagan Foundation, which aimed to continue to educate people about the dangers of substance abuse. The Foundation teamed with the BEST Foundation For A Drug-Free Tomorrow in 1994, and developed the Nancy Reagan Afterschool Program. She continued to travel around the United States, speaking out against drug and alcohol abuse.\n\nHer memoirs, \"My Turn: The Memoirs of Nancy Reagan\" (1989), are an account of her life in the White House, commenting openly about her influence within the Reagan administration, and discussing the myths and controversies that surrounded the couple. In 1991, the author Kitty Kelley wrote an unauthorized and largely uncited biography about Reagan, repeating accounts of a poor relationship with her children, and introducing rumors of alleged sexual relations with singer Frank Sinatra. A wide range of sources commented that Kelley's largely unsupported claims are most likely false.\n\nIn 1989, the Internal Revenue Service began investigating the Reagans over allegations they owed additional tax on the gifts and loans of high-fashion clothes and jewelry to the First Lady during their time in the White House (recipients benefiting from the display of such items recognize taxable income even if they are returned). In 1992, the IRS determined the Reagans had failed to include some $3 million worth of fashion items between 1983 and 1988 on their tax returns; they were billed for a large amount of back taxes and interest, which was subsequently paid.\n\nAfter President Reagan revealed that he had been diagnosed with Alzheimer's disease in 1994, she made herself his primary caregiver, and became actively involved with the National Alzheimer's Association and its affiliate, the Ronald and Nancy Reagan Research Institute in Chicago, Illinois.\n\nIn April 1997, Nancy Reagan joined President Bill Clinton and former Presidents Ford and Bush in signing the Summit Declaration of Commitment in advocating for participation by private citizens in solving domestic issues within the United States.\n\nNancy Reagan was awarded the Presidential Medal of Freedom, the nation's highest civilian honor, by President George W. Bush on July 9, 2002. President Reagan received his own Presidential Medal of Freedom in January 1993. Reagan and her husband were jointly awarded the Congressional Gold Medal on May 16, 2002, at the United States Capitol building, and were only the third President and First Lady to receive it; she accepted the medal on behalf of both of them.\n\nRonald Reagan died in their Bel Air home on June 5, 2004. During the seven-day state funeral, Nancy, accompanied by her children and military escort, led the nation in mourning. She kept a strong composure, traveling from her home to the Reagan Library for a memorial service, then to Washington, D.C., where her husband's body lay in state for 34 hours prior to a national funeral service in the Washington National Cathedral. She returned to the library in California for a sunset memorial service and interment, where, overcome with emotion, she lost her composure and cried in public for the first time during the week. After accepting the folded flag, she kissed the casket and mouthed \"I love you\" before leaving. CNN journalist Wolf Blitzer said of Reagan during the week, \"She's a very, very strong woman, even though she looks frail.\"\n\nShe had directed the detailed planning of the funeral, which included scheduling all the major events and asking former President George H. W. Bush, as well as former British Prime Minister Margaret Thatcher, former Soviet Union Leader Mikhail Gorbachev, and former Canadian Prime Minister Brian Mulroney to speak during the National Cathedral Service. She paid very close attention to the details, something she had always done in her husband's life. Betsy Bloomingdale, one of Reagan's closest friends, stated, \"She looks a little frail. But she is very strong inside. She is. She has the strength. She is doing her last thing for Ronnie. And she is going to get it right.\" The funeral marked her first major public appearance since she delivered a speech to the 1996 Republican National Convention on her husband's behalf.\n\nThe funeral had a great impact on her public image. Following substantial criticism during her tenure as first lady, she was seen somewhat as a national heroine, praised by many for supporting and caring for her husband while he suffered from Alzheimer's disease. \"U.S. News & World Report\" opined, \"after a decade in the shadows, a different, softer Nancy Reagan emerged.\"\n\nFollowing her husband's death, Reagan remained active in politics, particularly relating to stem cell research. Beginning in 2004, she favored what many consider to be the Democratic Party's position, and urged President George W. Bush to support federally funded embryonic stem cell research, in the hope that this science could lead to a cure for Alzheimer's disease. Although she failed to change the president's position, she did support his campaign for a second term.\nIn 2005, Reagan was honored at a gala dinner at the Ronald Reagan Building in Washington, D.C., where guests included Dick Cheney, Harry Reid, and Condoleezza Rice. The event was her first major public appearance since her husband's funeral. When asked about her plans, Reagan shook her head and responded, \"I don't know. I'll know when I'll know. But the Reagan library is Ronnie, so that's where I spend my time.\"\n\nIn 2007, she attended the national funeral service for Gerald Ford in the Washington National Cathedral. Reagan hosted two 2008 Republican presidential debates at the Reagan Presidential Library, the first in May 2007 and the second in January 2008. While she did not participate in the discussions, she sat in the front row and listened as the men vying to become the nation's 44th president claimed to be a rightful successor to her husband. Though some speculation arose as to whether Reagan might support New York City Mayor Michael Bloomberg in a presidential bid, nothing came of it. On March 25, she formally endorsed Senator John McCain, then the presumptive Republican party nominee for president, but McCain would go on to lose the election to Barack Obama.\n\nShe attended the funeral of Lady Bird Johnson in Austin, Texas, on July 14, 2007, and three days later accepted the highest Polish distinction, the Order of the White Eagle, on behalf of Ronald Reagan at the Reagan Library. The Reagan Library opened the temporary exhibit \"Nancy Reagan: A First Lady's Style\", which displayed over eighty designer dresses belonging to her.\n\nReagan's health and well-being became a prominent concern in 2008. In February, she suffered a fall at her Bel Air home and was taken to Saint John's Health Center in Santa Monica, California. Doctors reported that she did not break her hip as feared, and she was released from the hospital two days later. News commentators noted that Reagan's step had slowed significantly, as the following month she walked in very slow strides with John McCain. NBC's Brian Williams, who attended a dinner with Reagan in mid-2008, recalled, \"Mrs. Reagan's vision isn't what it always was so she was taking very halting steps as a lot of folks her age do ... It is so important for folks in her age bracket and in her bracket of life to remain upright and captain of their own ship. She very much is captain of her own ship.\" As for her mental ability, Williams remarked, \"She's as sharp as ever and enjoys a robust life with her friends in California, but falling is always a danger of course. She's a very stoic, hardy person full of joy and excitement for life ... She is not without opinions on politics and political types these days ... She is, as most of her friends described her, a pistol.\"\n\nIn October 2008, Reagan was admitted to Ronald Reagan UCLA Medical Center after falling at home. Doctors determined that the 87-year-old had fractured her pelvis and sacrum, and could recuperate at home with a regimen of physical therapy. As a result of her mishap, medical articles were published containing information on how to prevent falls. In January 2009, Reagan was said to be \"improving every day and starting to get out more and more.\"\nIn March 2009, she praised President Barack Obama for reversing the ban on federally funded embryonic stem cell research. She traveled to Washington, D.C. in June 2009 to unveil a statue of her late husband in the Capitol rotunda. She was also on hand as President Obama signed the Ronald Reagan Centennial Commission Act, and lunched privately with Michelle Obama. Reagan revealed in an interview with \"Vanity Fair\" that Michelle Obama had telephoned her for advice on living and entertaining in the White House. Following the death of Senator Ted Kennedy in August 2009, she said she was \"terribly saddened ... Given our political differences, people are sometimes surprised how close Ronnie and I have been to the Kennedy family ... I will miss him.\" She attended the funeral of Betty Ford in Rancho Mirage, California, on July 12, 2011.\n\nReagan hosted a 2012 Republican presidential debate at the Reagan Presidential Library on September 7, 2011. She suffered a fall in March 2012. Two months later, she endured several broken ribs, which prevented her from attending a speech given by Paul Ryan in the Reagan Presidential Library in May 2012; her spokesperson said, \"Mrs. Reagan has been recovering slowly and has been adding a few appointments back onto her schedule, but was advised by her doctor today not to try and attend large events too far from home just yet.\" She endorsed Republican presidential candidate Mitt Romney on May 31, 2012, explaining that her husband would have liked Romney's business background and what she called \"strong principles\". Following the death of former British Prime Minister Margaret Thatcher in April 2013, she stated, \"The world has lost a true champion of freedom and democracy ... Ronnie and I knew her as a dear and trusted friend, and I will miss her.\"\n\nReagan was the second-longest-lived First Lady of the United States, after Bess Truman who died at the age of 97. Ronald Reagan was the fourth longest-lived President, trailing only Jimmy Carter, Gerald Ford, and George H.W. Bush.\n\nOn March 6, 2016, Reagan died of congestive heart failure at the age of 94. On March 7, 2016, President Barack Obama issued a presidential proclamation ordering the flag of the United States to be flown at half-staff until sunset on the day of Reagan's interment.\n\nHer funeral was held on March 11, 2016, at the Ronald Reagan Presidential Library in Simi Valley, California. Representatives from ten First Families were in attendance, including former President George W. Bush and fellow First Ladies Michelle Obama, Laura Bush, Hillary Clinton, and Rosalynn Carter. The other such representatives were presidential children Steven Ford, Tricia Nixon Cox, Luci Baines Johnson, and Caroline Kennedy, and presidential grandchild Anne Eisenhower Flottl.\n\nOther prominent individuals in attendance included California Governor Jerry Brown and former Governors Arnold Schwarzenegger and Pete Wilson, former House Speakers Nancy Pelosi and Newt Gingrich, and former members of the Reagan administration, including George P. Shultz and Edwin Meese. A sizable contingent from the Hollywood entertainment industry attended as well, including Mr. T, Maria Shriver, Wayne Newton, Johnny Mathis, Anjelica Huston, John Stamos, Tom Selleck, Bo Derek, and Melissa Rivers. In all there were some 1,000 guests.\n\nEulogies were given by former Prime Minister of Canada Brian Mulroney, former Secretary of State James Baker, Diane Sawyer, Tom Brokaw, and her children Patti Davis and Ron Reagan. After the funeral, Nancy Reagan was interred next to her husband.\n\nAs noted earlier, Nancy Reagan was awarded the Presidential Medal of Freedom in 2002 and the Congressional Gold Medal, in the same year. \nIn 1989, she received the Council of Fashion Designers of America's lifetime achievement award.\n\nAs First Lady, Nancy Reagan received an Honorary Doctorate of Laws degree from Pepperdine University in Malibu in 1983.\nLater, she received an Honorary Doctor of Humane Letters degree from Eureka College in Illinois, her husband's alma mater, in 2009.\n\n\nAs Nancy Davis, she also made a number of television appearances from 1953 to 1962, as a guest star in dramatic shows or installments of anthology series. These included \"Ford Television Theatre\" (her first appearance with Ronald Reagan came during a 1953 episode titled \"First Born\"), \"Schlitz Playhouse of Stars\", \"Dick Powell's Zane Grey Theatre\" (appearing with Ronald Reagan in the 1961 episode \"The Long Shadow\"), \"Wagon Train\", \"The Tall Man\", and \"General Electric Theater\" (hosted by Ronald Reagan).\n\n\n\n"}
{"id": "21182", "url": "https://en.wikipedia.org/wiki?curid=21182", "title": "New Brunswick", "text": "New Brunswick\n\nNew Brunswick (; ) is one of four Atlantic provinces on the east coast of Canada.\n\nThe indigenous inhabitants of the land at the time of European colonization were the Mi'kmaq, the Maliseet, and the Passamaquoddy peoples, aligned politically within the Wabanaki Confederacy, many of whom still reside in the area.\n\nBeing relatively close to Europe, New Brunswick was among the first places in North America to be explored and settled, starting with the French in the early 1600s, who eventually colonized most of the Maritimes and some of Maine as the colony of Acadia. The area was caught up in the global conflict between the British and French empires, including the 1722–25 Dummer's War against New England. In 1755 what is now New Brunswick was claimed by the British as part of Nova Scotia, to be partitioned off in 1784 following an influx of refugees from the American Revolutionary War. Large groups of English, Scottish, and French people had settled and become the majority population by this time.\n\nIn 1785, Saint John became the first incorporated city in what is now Canada. The same year, the University of New Brunswick became one of the first universities in North America. The province prospered in the early 1800s due to logging, shipbuilding, and related activities. The population grew rapidly in part due to waves of Irish immigration to Saint John and Miramichi regions, reaching about a quarter of a million by mid-century. In 1867 New Brunswick was one of four founding provinces of the Canadian Confederation, along with Nova Scotia, Quebec, and Ontario.\n\nAfter Confederation, wooden shipbuilding and lumbering declined, while protectionist policy disrupted traditional economic patterns with New England. The mid-1900s found New Brunswick to be one of the poorest regions of Canada, but that has been mitigated somewhat by federal transfer payments and improved support for rural areas.\n\nAs of 2002, provincial gross domestic product was derived as follows: services (about half being government services and public administration) 43%; construction, manufacturing, and utilities 24%; real estate rental 12%; wholesale and retail 11%; agriculture, forestry, fishing, hunting, mining, oil and gas extraction 5%; transportation and warehousing 5%.\n\nAccording to the Constitution of Canada New Brunswick is the only bilingual province. About two thirds of the population declare themselves anglophones and a third francophones. One third of the overall population describe themselves as bilingual. Atypically for Canada, only about half of the population lives in urban areas, mostly in Greater Moncton, Greater Saint John and the capital Fredericton.\n\nUnlike the other Maritime provinces, New Brunswick's terrain is mostly forested uplands, with much of the land further from the coast, giving it a harsher climate. New Brunswick is 83% forested, and less densely-populated than the rest of the Maritimes.\n\nTourism accounts for about 9% of the labour force directly or indirectly. Popular destinations include Fundy National Park and the Hopewell Rocks, Kouchibouguac National Park, and Roosevelt Campobello International Park. In 2013, 64 cruise ships called at Port of Saint John carrying on average 2600 passengers each.\n\nIndigenous peoples have been present in the area since about 7000 BC. At the time of European contact, inhabitants were the Mi'kmaq, the Maliseet, and the Passamaquoddy. Although these tribes did not leave a written record, their language is present in many placenames, such as Aroostook, Bouctouche, Petitcodiac, Quispamsis, and Shediac.\n\nNew Brunswick may have been part of Vinland, the area explored by Norwegian Vikings, and the Bay of Fundy may have been visited in the early 1500s by Basque, Breton, and Norman fishermen.\n\nThe first documented European visits were by Jacques Cartier in 1534. A party led by Pierre du Gua de Monts and Samuel de Champlain visited the mouth of the Saint John River on the eponymous Saint-Jean-Baptiste Day in 1604. Now Saint John, this was later the site of the first permanent European settlement in New Brunswick. French settlement extended up the river to the site of present-day Fredericton.\n\nOther settlements in the southeast extended from Beaubassin, near the present-day border with Nova Scotia, and extending to Baie Verte, and up the Petitcodiac, Memramcook, and Shepody Rivers.\n\nBy the early 1700s the area that is now New Brunswick was part of the French colony of Acadia, which was in turn part of New France. Acadia comprised most of what is now the Maritimes, as well as parts of Québec and Maine. The peace and prosperity of the colony was ended by rivalry between Britain and France for control of territory in Europe and North America starting in the early 1700s. With the 1713 Treaty of Utrecht, the part of Acadia today known as peninsular Nova Scotia became another British colony on the eastern seaboard. Île Saint-Jean (Prince Edward Island) and Île-Royale (Cape Breton Island) remained French. The ownership of New Brunswick was disputed, with an informal border on the Isthmus of Chignecto.\n\nTo defend the area, especially in the 1722–25 Dummer's War against New England, the French built Fort Nashwaak, Fort Boishebert, Fort Menagoueche in Bay of Fundy, and in the southeast Fort Gaspareaux and Fort Beauséjour. The latter was captured by British and New England troops in 1755, followed soon after by the Expulsion of the Acadians.\n\nPresent-day New Brunswick became part of the colony of Nova Scotia. Hostilities ended with the Treaty of Paris in 1763, and Acadians returning from exile discovered several thousand immigrants, mostly from New England, on their former lands. Some settled around Memramcook and along the Saint John River.\n\nSettlement was initially slow. Pennsylvanian immigrants founded Moncton in 1766. An American settlement also developed at Saint John, and English settlers from Yorkshire arrived in the Sackville area.\n\nAfter the American Revolution, about 10,000 loyalist refugees settled along the north shore of the Bay of Fundy, commemorated in the province's motto, (\"hope restored\"). The number of immigrants reached almost 14,000 by 1784. Perhaps 10% of the refugees to New Brunswick returned to the States as did an unknown number from Nova Scotia. The same year New Brunswick was partitioned from Nova Scotia and that year saw its first elected assembly. The election of 1786 was bitterly contested and pitted two concepts of loyalty to the Empire against one another: loyalty to the King and his appointed governors, and loyalty to the King with local affairs handled by the locals. Hundreds who protested a rigged election and signed a petition to call another election were arrested for sedition: the issue of what loyalty meant was at the center of Canadian 19th century politics. This event replicated the pre-1775 behavior and attitude of both Tories and Whigs in the southern 13 Colonies who protested their loyalty to the King and pride in belonging to the British Empire while insisting on their rights as British subjects, local rule and fair governance.\n\nThe colony was named New Brunswick in honour of George III, King of Great Britain, King of Ireland, and Prince-elector of Brunswick-Lüneburg in what is now Germany. In 1785 Saint John became Canada's first incorporated city. The population of the colony reached 26,000 in 1806 and 35,000 in 1812.\n\nAlthough New Brunswick has limited arable land, the 1800s saw an age of prosperity based on wood export and shipbuilding, bolstered by The Canadian–American Reciprocity Treaty of 1854 and demand from the American Civil War. St. Martins became the third most productive shipbuilding town in the Maritimes, producing over 500 vessels.\n\nThe first half of the 1800s saw large-scale immigration from Ireland and Scotland, with the population reaching 252,047 by 1861.\n\nIn 1848, responsible home government was granted and the 1850s saw the emergence of political parties largely organised along religious and ethnic lines.\n\nThe notion of unifying the separate colonies of British North America was discussed increasingly in the 1860s. Many felt that the American Civil war was the result of weak central government, and wished to avoid such violence and chaos. The 1864 Charlottetown Conference had been intended to discuss a Maritime Union, but concerns over possible conquest by the Americans coupled with a belief that Britain was unwilling to defend its colonies against American attack led to a request from the Province of Canada (now Ontario and Quebec) to expand the scope of the meeting. In 1866 the US cancelled the Canadian–American Reciprocity Treaty leading to loss of trade with New England and prompting a desire to build trade within British North America, while Fenian raids increased support for union.\n\nOn 1 July 1867 New Brunswick entered the Canadian Confederation along with Nova Scotia, Quebec and Ontario.\n\nConfederation brought into existence the Intercolonial Railway in 1872, a consolidation of the existing Nova Scotia Railway, European and North American Railway, and Grand Trunk Railway. In 1879 John A. Macdonald's Conservatives enacted the National Policy which called for high tariffs and opposed free trade, disrupting the trading relationship between the Maritimes and New England. The economic situation was worsened by the decline of the wooden ship building industry. The railways and tariffs did foster the growth of new industries in the province such as textile manufacturing, iron mills, and sugar refineries, many of which eventually failed to compete with better capitalized industry in central Canada.\n\nIn 1937 New Brunswick had the highest infant mortality and illiteracy rates in Canada. At the end of the Great Depression the New Brunswick standard of living was much below the Canadian average. In 1940 the Rowell–Sirois Commission reported that federal government attempts to manage the depression illustrated grave flaws in the Canadian constitution. While the federal government had most of the revenue gathering powers, the provinces had many expenditure responsibilities such as healthcare, education, and welfare, which were becoming increasingly expensive. The Commission recommended the creation of equalization payments, implemented in 1957.\n\nThe Acadians in northern New Brunswick had long been geographically and linguistically isolated from the more numerous English speakers to the south. The population of French origin grew dramatically after Confederation, from about 16 per cent in 1871 to 34 per cent in 1931. Government services were often not available in French, and the infrastructure in Francophone areas was less developed than elsewhere. In 1960 Premier Louis Robichaud embarked on the New Brunswick Equal Opportunity program, in which education, rural road maintenance, and healthcare fell under the sole jurisdiction of a provincial government that insisted on equal coverage throughout the province, rather than the former county-based system.\n\nThe flag of New Brunswick, based on the coat of arms, was adopted in 1965. The conventional heraldic representations of a lion and a ship represent colonial ties with Europe, and the importance of shipping at the time the coat of arms was assigned.\n\nRoughly square, New Brunswick is bordered on the north by Quebec, on the east by the Atlantic Ocean, on the south by the Bay of Fundy, and on the west by the US state of Maine. The southeast corner of the province is connected to Nova Scotia at the isthmus of Chignecto.\n\nGlaciation has left much of New Brunswick's uplands with only shallow, acidic soils which have discouraged settlement, but are home to enormous forests.\n\nNew Brunswick is within the Appalachian Mountains and is divided into:\n\nAll of the rivers of New Brunswick drain either into the Gulf of Saint Lawrence to the east, or into the Bay of Fundy to the south. These watersheds include lands in Quebec and Maine.\n\nDuring the glacial period New Brunswick was covered by thick layers of ice. It cut U-shaped valleys in the Saint John and Nepisiguit River valleys, and pushed granite boulders from the Miramichi highlands south and east, leaving them as erratics when the ice receded at the end of the Wisconsin glaciation, along with deposits such as the eskers between Woodstock and St George, today sources of sand and gravel.\n\nMost of New Brunswick is forested with secondary forest or tertiary forest. At the start of European settlement, the Maritimes were covered from coast to coast by a forest of mature trees, giants by today's standards. Today less than one per cent of old-growth Acadian forest remains, and the World Wide Fund for Nature lists the Acadian Forest as endangered. Following the frequent large scale disturbances caused by settlement and timber harvesting, the Acadian forest is not growing back as it was, but is subject to borealization. This means that exposure-resistant species that are well adapted to the frequent large scale disturbances common in the boreal forest are increasingly abundant. These include jack pine, balsam fir, black spruce, white birch, and poplar.\n\nForest ecosystems support large carnivores such as the bobcat, Canada lynx, and black bear, and the large herbivores moose and white-tailed deer.\n\nFiddlehead greens are harvested from the Ostrich fern which grows on riverbanks.\n\nFurbish's lousewort a perennial herb endemic to the shores of the upper Saint John River, is an endangered species threatened by habitat destruction, riverside development, forestry, littering and recreational use of the riverbank. Many wetlands are being disrupted by the highly invasive Introduced species purple loosestrife.\n\nNew Brunswick's climate is more severe than that of the other Maritime provinces, which are lower and have more shoreline along the moderating sea. New Brunswick has a humid continental climate, with slightly milder winters on the Gulf of St. Lawrence coastline. Elevated parts of the far north of the province have a subarctic climate.\n\nBelow is data for Fredericton from 1981 to 2010. The climate is similar across the province, although places near the coast are slightly cooler in July (Saint John's maximum 22 °C to Fredericton's 25 °C) and milder in January (Saint John's maximum −2 °C to Fredericton's −4 °C).\n\nOn average Fredericton gets 130 frost-free days from 18 May to 24 September.\nEvidence of climate change in New Brunswick includes: more intense precipitation events, more frequent winter thaws, and one quarter to half the amount of snowpack.\n\nToday the sea level is about 30 cm higher than it was 100 years ago, and is expected to rise twice that much again by the year 2100.\n\nBedrock types range from 1 billion to 200 million years old.\nMuch of the bedrock in the west and north derives from ocean deposits in the Ordovician, which were then subject to folding and igneous intrusion, and were eventually covered with lava during the Paleozoic, peaking during the Acadian orogeny.\n\nDuring the Carboniferous era, about 340 million years ago, New Brunswick was in the Maritimes Basin, a sedimentary basin near the equator. Sediments, brought by rivers from surrounding highlands, accumulated there and after being compressed, producing the Albert oil shales of southern New Brunswick. Eventually sea water from the Panthalassic Ocean invaded the basin, forming the Windsor Sea. Once this receded, conglomerates, sandstones, and shales accumulated. The rust colour of these was caused by the oxidation of iron in the beds between wet and dry periods. Such late carboniferous rock formed the Hopewell Rocks, which have been shaped by the extreme tidal range of the Bay of Fundy.\n\nIn the early Triassic, as Pangea drifted north it was rent apart, forming the rift valley that is the Bay of Fundy. Magma pushed up through the cracks, forming basalt columns on Grand Manan.\n\nThe four Atlantic Provinces are Canada's least populated, with New Brunswick is the third least populous at 747,101 in 2016. The Atlantic provinces also have higher rural populations. New Brunswick was largely rural until 1951 since when the rural urban split has been roughly even. Population density in the Maritimes is above average among Canadian provinces, which reflects their small size and the fact that they do not possess large unpopulated hinterlands, as do the other seven provinces and three territories.\n\nNew Brunswick's 107 municipalities cover of the province's land mass but are home to of its population. The three major urban areas are in the south of the province and are:\n\nIn the 2001 census, the most commonly reported ethnicities were British and Irish 60%, French Canadian or Acadian 31%, other European 7%, First Nations 3%, Asian Canadian 2%. Each person could choose more than one ethnicity.\n\nAccording to the Canadian Constitution, both English and French are the official languages of New Brunswick, making it the only officially bilingual province. Anglophone New Brunswickers make up roughly two-thirds of the population, with about one-third being Francophone. Recently there has been growth in the numbers of people reporting themselves as bilingual, with 34% reporting that they speak both English and French. This reflects a trend across Canada.\n\nIn the 2011 census, 84% of provincial residents reported themselves as Christian: 52% were Roman Catholic, 8% Baptist, 8% United Church of Canada, and 7% Anglican. Fifteen percent of residents reported no religion.\n\nAs of October 2017 seasonally-adjusted employment is 73,400 for the goods-producing sector, and 280,900 for the services-producing sector.\n\nThose in the goods producing industries are mostly employed in manufacturing or construction, while those in services work in social assistance, trades, and health care.\n\nThe US is the province's largest export market, accounting for 92% of a foreign trade valued in 2014 at almost $13 billion, with refined petroleum making up 63% of that, followed by seafood products, pulp, paper and sawmill products and non-metallic minerals (chiefly potash).\n\nMore than 13,000 New Brunswickers work in agriculture, shipping products worth over $1 billion, half of which is from crops, and half of that from potatoes, mostly in the Saint John River valley. McCain Foods is one of the world's largest manufacturers of frozen potato products. Other products include apples, cranberries, and maple syrup. New Brunswick was in 2015 the biggest producer of wild blueberries in Canada.\n\nThe value of the livestock sector is about a quarter of a billion dollars, nearly half of which is dairy. Other sectors include poultry, fur, and goats, sheep, and pigs.\n\nThe value of exports, mostly to the United States, was $1.6 billion in 2016. About half of that came from lobster. Other products include salmon, crab, and herring.\n\nAbout 83% of New Brunswick is forested. Historically important, it accounted for more than 80% of exports in the mid 1800s. By the end of the 1800s the industry, and shipbuilding, were declining due to external economic factors. The 1920s saw the development of a pulp and paper industry. In the mid-1960s, forestry practices changed from the controlled harvests of a commodity to the cultivation of the forests.\n\nThe industry employs nearly 12,000, generating revenues around $437 million.\n\nMining was historically unimportant in the province, but since the 1950s has grown and in 2012 was an estimated $1.1 billion. Mines in New Brunswick produce lead, zinc, copper, and potash.\n\nIn 2015, spending on non-resident tourism in New Brunswick was $441 million, which provided $87 million in tax revenue.\n\nThe Department of Transportation and Infrastructure maintains government facilities and the province's highway network and ferries. The Trans-Canada Highway is not under federal jurisdiction, and traverses the province from Edmundston following the Saint John River Valley, through Fredericton, Moncton, and on to Nova Scotia and Prince Edward Island.\n\nVia Rail's Ocean service, which connects Montreal to Halifax, is currently the oldest continuously operated passenger route in North America, with stops from west to east at Campbellton, Charlo, Jacquet River, Petit Rocher, Bathurst, Miramichi, Rogersville, Moncton, and Sackville.\n\nCanadian National Railway operates freight services along the same route, as well as a subdivision from Moncton to Saint John.\n\nThe New Brunswick Southern Railway, a division of J. D. Irving Limited, together with its sister company Eastern Maine Railway form a continuous main line connecting Saint John and Brownville Junction, Maine.\n\nPublicly-owned NB Power operates 13 of New Brunswick's generating stations, deriving power from fuel oil and diesel (1497 MW), hydro (889 MW), nuclear (660 MW), and coal (467 MW). There were 30 active natural gas production sites in 2012.\n\nUnder Canadian federalism, power is divided between federal and provincial governments. Among areas under federal jurisdiction are citizenship, foreign affairs, national defence, fisheries, criminal law, Indian policies, and many others. Provincial jurisdiction covers public lands, health, education, and local government, among other things. Jurisdiction is shared for immigration, pensions, agriculture, and welfare.\n\nThe parliamentary system of government is modelled on the British Westminster system. Forty-nine representatives, nearly always members of political parties, are elected to the Legislative Assembly of New Brunswick. The head of government is the Premier of New Brunswick, normally the leader of the party or coalition with the most seats in the legislative assembly. Governance is handled by the executive council (cabinet), with about 32 ministries.\n\nCeremonial duties of the Monarchy in New Brunswick are mostly carried out by the Lieutenant Governor of New Brunswick.\n\nUnder amendments to the province's Legislative Assembly Act in 2007, a provincial election is held every four years. The two largest political parties are the New Brunswick Liberal Association and the Progressive Conservative Party of New Brunswick. Since the 2018 election, minor parties are the Green Party of New Brunswick and the People's Alliance of New Brunswick.\n\nThe Court of Appeal of New Brunswick is the highest provincial court. It hears appeals from:\n\nThe system consists of eight Judicial Districts, loosely based on the counties. The Chief Justice of New Brunswick serves at the apex of this court structure.\n\nNinety-two per cent of the land in the province, inhabited by about 35% of the population, is under provincial administration and has no local, elected representation. The 51% of the province that is Crown land is administered by the Department of Energy and Resource Development.\n\nMost of the province is administrated as a local service district (LSD), an unincorporated unit of local governance. As of 2017 there are 237 LSDs. Services, paid for by property taxes, include a variety of services such as fire protection, solid waste management, street lighting, and dog regulation. LSDs may elect advisory committees and work with the Department of Local Government to recommend how to spend locally collected taxes.\n\nIn 2006 there were three rural communities. This is a relatively new entity, and to be created requires a population of 3,000 and a tax base of $200 million.\n\nIn 2006 there were 101 municipalities.\n\nRegional Service Commissions, which number 12, were introduced in 2013 to regulate regional planning and solid waste disposal, and provide a forum for discussion on a regional level of police and emergency services, climate change adaptation planning, and regional sport, recreational and cultural facilities. The commissions' administrative councils are populated by the mayors of each municipality or rural community within a region.\n\nHistorically the province was divided into counties with elected governance, but this was abolished in 1966. These were subdivided into 152 parishes which also lost their political significance in 1966, but are still used as census subdivisions by Statistics Canada.\n\nNew Brunswick has the most poorly-performing economy of any Canadian province, with a per capita income of $28,000. The government has historically run at a large deficit. With about half of the population being rural, it is expensive for the government to provide education and health services, which account for 60 per cent of government expenditure. Thirty-six per cent of the provincial budget is covered by federal cash transfers.\n\nThe government has frequently attempted to create employment through subsidies, which has often failed to generate long-term economic prosperity and resulted in bad debt, examples of which include Bricklin, Atcon, and the Marriott call centre in Fredericton.\n\nAccording to a 2014 study by the Atlantic Institute for Market Studies the large public debt is a very serious problem. Government revenues are shrinking because of a decline in federal transfer payments. Though expenditures are down (through government pension reform and a reduction in the number of public employees), they have increased relative to GDP, necessitating further measures to reduce debt in the future.\n\nIn the 2014–15 fiscal year, provincial debt reached $12.2 billion or 37.7 per cent of nominal GDP, an increase over the $10.1 billion recorded in 2011–12. The debt-to-GDP ratio is projected to reach 41.9% in 2017–18, compared to a ratio of 25% in 2007–08.\n\nPublic education in the province is administered by the NB Department of Education. New Brunswick has a parallel system of Anglophone and Francophone public schools. There are also secular and religious private schools in the province, such as the Moncton Flight College.\n\nThe New Brunswick Community College system has campuses in all regions of the province.\n\nThe two comprehensive provincial universities are the University of New Brunswick (Fredericton and Saint John) and the Université de Moncton (Moncton, Shippegan and Edmundston). These have extensive postgraduate programs and Law schools. Medical education programs have also been established at both the Université de Moncton and at UNBSJ in Saint John (affiliated with Université de Sherbrooke and Dalhousie University respectively). Other public funded universities include Mount Allison University in Sackville and Saint Thomas University in Fredericton. Mount Allison University is a highly regarded undergraduate university with over 50 Rhodes Scholars to its credit. There are several private universities in the province as well, the largest being Crandall University in Moncton.\n\nJulia Catherine Beckwith born in Fredericton, was Canada's first published novelist. Charles G. D. Roberts was one of the first Canadians to achieve international fame. Antonine Maillet was the first non-European winner of France's Prix Goncourt. Other modern writers include Alfred Bailey, Alden Nowlan, John Thompson, Douglas Lochhead, K. V. Johansen, David Adams Richards, Raymond Fraser, and France Daigle. A recent New Brunswick Lieutenant-Governor, Herménégilde Chiasson, is a poet and playwright.\n\nThe Fiddlehead, established in 1945 at UNB, is Canada's oldest literary magazine.\n\nMount Allison University in Sackville began offering classes in 1854. The program came into its own under John A. Hammond, from 1893 to 1916. Alex Colville and Lawren Harris later studied and taught art there and both Christopher Pratt and Mary Pratt were trained at Mount Allison. The University's art gallery – which opened in 1895 and is named for its patron, John Owens of Saint John – is Canada's oldest. Modern New Brunswick artists include landscape painter Jack Humphrey, sculptor Claude Roussel, and Miller Brittain.\n\nMusic of New Brunswick includes artists such as Henry Burr, Roch Voisine, Lenny Breau, and Édith Butler. Symphony New Brunswick, based in Saint John, tours extensively in the province.\n\nSymphony New Brunswick based in Saint John and the Atlantic Ballet Theatre of Canada (based in Moncton), tours nationally and internationally. Theatre New Brunswick (based in Fredericton), tours plays around the province. Canadian playwright Norm Foster saw his early works premiere at TNB. Other live theatre troops include the Théatre populaire d'Acadie in Caraquet, and Live Bait Theatre in Sackville. The refurbished Imperial and Capitol Theatres are found in Saint John and Moncton, respectively; the more modern Playhouse is located in Fredericton.\n\nNew Brunswick has four daily newspapers: the \"Times & Transcript\", serving eastern New Brunswick, the \"Telegraph-Journal\", based in Saint John and distributed province-wide, \"The Daily Gleaner\", based in Fredericton, and \"L'Acadie Nouvelle\", based in Caraquet. The three English-language dailies and the majority of the weeklies are owned and operated by Brunswick News, privately owned by J. K. Irving.\n\nThe Canadian Broadcasting Corporation has Anglophone television and radio operations in Fredericton. Télévision de Radio-Canada is based in Moncton.\n\nThere are about 61 historic places in New Brunswick, including Fort Beauséjour, Kings Landing Historical Settlement and the Village Historique Acadien, and many New Brunswick museums.\n\n"}
{"id": "21184", "url": "https://en.wikipedia.org/wiki?curid=21184", "title": "Nova Scotia", "text": "Nova Scotia\n\nNova Scotia (; Latin for \"New Scotland\"; ; Scottish Gaelic: \"Alba Nuadh\") is one of Canada's three Maritime Provinces, and one of the four provinces that form Atlantic Canada. Its provincial capital is Halifax. Nova Scotia is the second-smallest of Canada's ten provinces, with an area of 55,284 square kilometres (21,300 sq mi), including Cape Breton and another 3,800 coastal islands. As of 2016, the population was 923,598. Nova Scotia is Canada's second-most-densely populated province, after Prince Edward Island, with 17.4 inhabitants per square kilometre (45/sq mi).\n\n\"Nova Scotia\" means \"New Scotland\" in Latin and is the recognized English-language name for the province. In both French and Scottish Gaelic, the province is directly translated as \"New Scotland\" (French: '. Gaelic: '). In general, Romance and Slavic languages use a direct translation of \"New Scotland\", while most other languages use direct transliterations of the Latin / English name. The province was first named in the 1621 Royal Charter granting to Sir William Alexander in 1632 the right to settle lands including modern Nova Scotia, Cape Breton Island, Prince Edward Island, New Brunswick and the Gaspé Peninsula.\n\nNova Scotia is Canada's smallest province in area after Prince Edward Island. The province's mainland is the Nova Scotia peninsula surrounded by the Atlantic Ocean, including numerous bays and estuaries. Nowhere in Nova Scotia is more than from the ocean. Cape Breton Island, a large island to the northeast of the Nova Scotia mainland, is also part of the province, as is Sable Island, a small island notorious for its shipwrecks, approximately from the province's southern coast.\n\nNova Scotia has many ancient fossil-bearing rock formations. These formations are particularly rich on the Bay of Fundy's shores. Blue Beach near Hantsport, Joggins Fossil Cliffs, on the Bay of Fundy's shores, has yielded an abundance of Carboniferous-age fossils. Wasson's Bluff, near the town of Parrsboro, has yielded both Triassic- and Jurassic-age fossils.\n\nThe province contains 5,400 lakes.\n\nNova Scotia lies in the mid-temperate zone and, although the province is almost surrounded by water, the climate is closer to continental climate rather than maritime. The winter and summer temperature extremes of the continental climate are moderated by the ocean. However, winters are cold enough to be classified as continental—still being nearer the freezing point than inland areas to the west. The Nova Scotian climate is in many ways similar to the central Baltic Sea coast in Northern Europe, only wetter and snowier. This is true in spite of Nova Scotia's being some fifteen parallels south. Areas not on the Atlantic coast experience warmer summers more typical of inland areas, and winter lows a little colder.\n\nDescribed on the provincial vehicle licence plate as Canada's Ocean Playground, Nova Scotia is surrounded by four major bodies of water: the Gulf of Saint Lawrence to the north, the Bay of Fundy to the west, the Gulf of Maine to the southwest, and Atlantic Ocean to the east.\n\nThe province includes regions of the Mi'kmaq nation of Mi'kma'ki (\"). The Mi'kmaq people inhabited Nova Scotia at the time the first European colonists arrived. In 1605, French colonists established the first permanent European settlement in the future Canada (and the first north of Florida) at Port Royal, founding what would become known as Acadia.\n\nThe British conquest of Acadia took place in 1710. The Treaty of Utrecht in 1713 formally recognized this and returned Cape Breton Island (\") to the French. Present-day New Brunswick then still formed a part of the French colony of Acadia. Immediately after the capture of Port Royal in 1710, Francis Nicholson announced it would be renamed Annapolis Royal in honor of Queen Anne. In 1749, the capital of Nova Scotia moved from Annapolis Royal to the newly established Halifax. In 1755 the vast majority of the French population (the Acadians) was forcibly removed in the Expulsion of the Acadians; New England Planters arrived between 1759 and 1768 to replace them.\n\nIn 1763, most of Acadia (Cape Breton Island, St. John's Island (now Prince Edward Island), and New Brunswick) became part of Nova Scotia. In 1769, St. John's Island became a separate colony. Nova Scotia included present-day New Brunswick until that province's establishment in 1784, after the arrival of United Empire Loyalists. In 1867, Nova Scotia became one of the four founding provinces of the Canadian Confederation.\n\nThe warfare on Nova Scotian soil during the 17th and 18th centuries significantly influenced the history of Nova Scotia. The Mi'kmaq had lived in Nova Scotia for centuries. The French arrived in 1604, and Catholic Mi'kmaq and Acadians formed the majority of the population of the colony for the next 150 years. During the first 80 years the French and Acadians lived in Nova Scotia, nine significant military clashes took place as the English and Scottish (later British), Dutch and French fought for possession of the area. These encounters happened at Port Royal, Saint John, Cap de Sable (present-day Port La Tour, Nova Scotia), Jemseg (1674 and 1758) and Baleine (1629). The Acadian Civil War took place from 1640 to 1645.\n\nBeginning with King William's War in 1688, six wars took place in Nova Scotia before the British defeated the French (and ultimately expelled much of their population) and made peace with the Mi'kmaq:\n\n\nThe battles during these wars took place primarily Port Royal, Saint John, Canso, Chignecto, Dartmouth (1751), Lunenburg (1756) and Grand-Pré. Despite the British conquest of Acadia in 1710, Nova Scotia remained primarily occupied by Catholic Acadians and Mi'kmaq, who confined British forces to Annapolis and to Canso.\n\nThe Mi'kmaq signed a series of peace and friendship treaties with Great Britain, beginning after Father Rale's War (1725). In 1725, the British signed a treaty (or \"agreement\") with the Mi'kmaq, but the authorities have often disputed its definition of the rights of the Mi'kmaq to hunt and fish on their lands.\nA generation later, Father Le Loutre's War began when Edward Cornwallis arrived to establish Halifax with 13 transports on June 21, 1749. A General Court, made up of the governor and the Council, was the highest court in the colony at the time. Jonathan Belcher was sworn in as chief justice of the Nova Scotia Supreme Court on October 21, 1754. The first legislative assembly in Halifax, under the Governorship of Charles Lawrence, met on October 2, 1758. During the French and Indian War of 1754–63 (the North American theatre of the Seven Years' War of 1756-1763), the British deported the Acadians and recruited New England Planters to resettle the colony. The 75-year period of war ended with the Burial of the Hatchet Ceremony between the British and the Mi'kmaq (1761). After the war, some Acadians were allowed to return and the British made treaties with the Mi’kmaq.\n\nThe American Revolution (1775–1783) had a significant impact on shaping Nova Scotia. Initially, Nova Scotia—\"the 14th American Colony\" as some called it—displayed ambivalence over whether the colony should join the more southern colonies in their defiance of Britain, and rebellion flared at the Battle of Fort Cumberland (1776) and at the Siege of Saint John (1777). Throughout the war, American privateers devastated the maritime economy by capturing ships and looting almost every community outside of Halifax. These American raids alienated many sympathetic or neutral Nova Scotians into supporting the British. By the end of the war Nova Scotia had outfitted a number of privateers to attack American shipping. British military forces based at Halifax succeeded in preventing American support for rebels in Nova Scotia and deterred any invasion of Nova Scotia. However the British navy failed to establish naval supremacy. While the British captured many American privateers in battles such as the Naval battle off Halifax (1782), many more continued attacks on shipping and settlements until the final months of the war. The Royal Navy struggled to maintain British supply lines, defending convoys from American and French attacks as in the fiercely fought convoy battle, the Naval battle off Cape Breton (1781).\nAfter the Thirteen Colonies and their French allies forced the British forces to surrender (1781), approximately 33,000 Loyalists (the King's Loyal Americans, allowed to place \"United Empire Loyalist\" after their names) settled in Nova Scotia (14,000 of them in what became New Brunswick) on lands granted by the Crown as some compensation for their losses. (The British administration divided Nova Scotia and carved out the present-day province of New Brunswick in 1784). The Loyalist exodus created new communities across Nova Scotia, including Shelburne, which briefly became one of the larger British settlements in North America, and infused Nova Scotia with additional capital and skills. However the migration also caused political tensions between Loyalist leaders and the leaders of the existing New England Planters settlement. The Loyalist influx also pushed Nova Scotia's Mi'kmaq People to the margins as Loyalist land grants encroached on ill-defined native lands. As part of the Loyalist migration, about 3,000 Black Loyalists arrived; they founded the largest free Black settlement in North America at Birchtown, near Shelburne. However, unfair treatment and harsh conditions caused about one-third of the Black Loyalists to resettle in Sierra Leone in 1792, where they founded Freetown and became known in Africa as the Nova Scotian Settlers.\n\nDuring the War of 1812, Nova Scotia's contribution to the British war effort involved communities either purchasing or building various privateer ships to attack U.S. vessels. Perhaps the most dramatic moment in the war for Nova Scotia occurred when HMS \"Shannon\" escorted the captured American frigate USS \"Chesapeake\" into Halifax Harbour (1813). Many of the U.S. prisoners were kept at Deadman's Island, Halifax.\n\nDuring this century, Nova Scotia became the first colony in British North America and in the British Empire to achieve responsible government in January–February 1848 and become self-governing through the efforts of Joseph Howe. Nova Scotia had established representative government in 1758, an achievement later commemorated by the erection of the Dingle Tower in 1908.\nNova Scotians fought in the Crimean War of 1853–1856. The Welsford-Parker Monument in Halifax is the second-oldest war monument in Canada (1860) and the only Crimean War monument in North America. It commemorates the 1854–55 Siege of Sevastopol.\n\nThousands of Nova Scotians fought in the American Civil War (1861–1865), primarily on behalf of the North.\nThe British Empire (including Nova Scotia) in the conflict. As a result, Britain (and Nova Scotia) continued to trade with both the South and the North. Nova Scotia's economy boomed during the Civil War.\n\nSoon after the American Civil War, Pro-Canadian Confederation premier Charles Tupper led Nova Scotia into the Canadian Confederation on July 1, 1867, along with New Brunswick and the Province of Canada. The Anti-Confederation Party was led by Joseph Howe. Almost three months later, in the election of September 18, 1867, the Anti-Confederation Party won 18 out of 19 federal seats, and 36 out of 38 seats in the provincial legislature.\n\nNova Scotia became a world leader in both building and owning wooden sailing ships in the second half of the 19th century. Nova Scotia produced internationally recognized shipbuilders Donald McKay and William Dawson Lawrence. The fame Nova Scotia achieved from sailors was assured when Joshua Slocum became the first man to sail single-handedly around the world (1895). International attention continued into the following century with the many racing victories of the \"Bluenose\" schooner. Nova Scotia was also the birthplace and home of Samuel Cunard, a British shipping magnate (born at Halifax, Nova Scotia) who founded the Cunard Line.\n\nThroughout the 19th century, numerous businesses developed in Nova Scotia became of pan-Canadian and international importance: the Starr Manufacturing Company (first skate-manufacturer in Canada), the Bank of Nova Scotia, Cunard Line, Alexander Keith's Brewery, Morse's Tea Company (first tea company in Canada), among others. (Early in the 20th century Sobey's was established, as was Maritime Life.)\n\nAccording to the 2006 Canadian census the largest ethnic group in Nova Scotia is Scottish (31.9%), followed by English (31.8%), Irish (21.6%), French (17.9%), German (11.3%), Aboriginal origin (5.3%), Dutch (4.1%), Black Canadians (2.8%), Welsh (1.9%) Italian (1.5%), and Scandinavian (1.4%). 40.9% of respondents identified their ethnicity as \"Canadian\".\n\nNova Scotia has a long history of social justice work to address issues such as racism and sexism within its borders. The Nova Scotia legislature was the third in Canada to pass human rights legislation (1963). The Nova Scotia Human Rights Commission was established in 1967.\n\nThe 2011 Canadian census showed a population of 921,727. Of the 904,285 singular responses to the census question concerning mother tongue the most commonly reported languages were:\n\nFigures shown are for the number of single-language responses and the percentage of total single-language responses.\n\nNova Scotia is home to the largest Scottish Gaelic-speaking community outside of Scotland, with a small number of native speakers in Pictou County, Antigonish County, and Cape Breton Island, and the language is taught in a number of secondary schools throughout the province.\n\nIn 2018 the government launched a new Gaelic vehicle license plate to raise awareness of the language and help fund Gaelic language and culture initiatives. They estimated that there were 2,000 Gaelic speakers in the province.\n\nIn 1871, the largest religious denominations were Protestant with 103,500 (27%); Roman Catholic with 102,000 (26%); Baptist with 73,295 (19%); Anglican with 55,124 (14%); Methodist with 40,748 (10%), Lutheran with 4,958 (1.3%); and Congregationalist with 2,538 (0.65%).\n\nAccording to the 2001 census, the largest denominations by number of adherents were the Roman Catholic Church with 327,940 (37%); the United Church of Canada with 142,520 (17%); and the Anglican Church of Canada with 120,315 (13%).There are also 8,505 (0.9%) Muslims according to 2011 census.\n\nNova Scotia's per capita GDP in 2010 was $38,475, significantly lower than the national average per capita GDP of $47,605 and a little more than half of Canada's richest province, Alberta. GDP growth has lagged behind the rest of the country for at least the past decade.\n\nNova Scotia's traditionally resource-based economy has diversified in recent decades. The rise of Nova Scotia as a viable jurisdiction in North America, historically, was driven by the ready availability of natural resources, especially the fish stocks off the Scotian Shelf. The fishery was a pillar of the economy since its development as part of New France in the 17th century; however, the fishery suffered a sharp decline due to overfishing in the late 20th century. The collapse of the cod stocks and the closure of this sector resulted in a loss of approximately 20,000 jobs in 1992.\n\nOther sectors in the province were also hit hard, particularly during the last two decades: coal mining in Cape Breton and northern mainland Nova Scotia has virtually ceased, and a large steel mill in Sydney closed during the 1990s. More recently, the high value of the Canadian dollar relative to the US dollar has hurt the forestry industry, leading to the shutdown of a long-running pulp and paper mill near Liverpool. Mining, especially of gypsum and salt and to a lesser extent silica, peat and barite, is also a significant sector. Since 1991, offshore oil and gas has become an important part of the economy, although production and revenue are now declining. Agriculture remains an important sector in the province, particularly in the Annapolis Valley.\n\nNova Scotia’s defence and aerospace sector generates approximately $500 million in revenues and contributes about $1.5 billion to the provincial economy each year. To date, 40% of Canada’s military assets reside in Nova Scotia. Nova Scotia has the fourth-largest film industry in Canada hosting over 100 productions yearly, more than half of which are the products of international film and television producers. In 2015, the government of Nova Scotia eliminated tax credits to film production in the province, jeopardizing the industry given most other jurisdictions continue to offer such credits.\n\nThe Nova Scotia tourism industry includes more than 6,500 direct businesses, supporting nearly 40,000 jobs. Two hundred thousand cruise-ship passengers from around the world flow through the Port of Halifax, Nova Scotia each year. This industry contributes approximately $1.3 billion annually to the economy. The province also boasts a rapidly developing Information & Communication Technology (ICT) sector which consists of over 500 companies, and employs roughly 15,000 people. In 2006, the manufacturing sector brought in over $2.6 billion in chained GDP, the largest output of any industrial sector in Nova Scotia. Michelin remains by far the largest single employer in this sector, operating three production plants in the province.\n\nAs of 2012, the median family income in Nova Scotia was $67,910, below the national average of $74,540; in Halifax the figure rises to $80,490.\n\nThe province is the world’s largest exporter of Christmas trees, lobster, gypsum, and wild berries. Its export value of fish exceeds $1 billion, and fish products are received by 90 countries around the world. Nevertheless, the province's imports far exceed its exports. While these numbers were roughly equal from 1992 until 2004, since that time the trade deficit has ballooned. In 2012, exports from Nova Scotia were 12.1% of provincial GDP, while imports were 22.6%.\n\nNova Scotia is ordered by a parliamentary government within the construct of constitutional monarchy; the monarchy in Nova Scotia is the foundation of the executive, legislative, and judicial branches. The sovereign is Queen Elizabeth II, who also serves as head of state of 15 other Commonwealth countries, each of Canada's nine other provinces, and the Canadian federal realm, and resides predominantly in the United Kingdom. As such, the Queen's representative, the Lieutenant Governor of Nova Scotia (at present Arthur Joseph LeBlanc), carries out most of the royal duties in Nova Scotia.\n\nIn 1937, Everett Farmer was the last person hanged (for murder) in Nova Scotia. \nThe direct participation of the royal and viceroyal figures in any of these areas of governance is limited, though; in practice, their use of the executive powers is directed by the Executive Council, a committee of ministers of the Crown responsible to the unicameral, elected House of Assembly and chosen and headed by the Premier of Nova Scotia (presently Stephen McNeil), the head of government. To ensure the stability of government, the lieutenant governor will usually appoint as premier the person who is the current leader of the political party that can obtain the confidence of a plurality in the House of Assembly. The leader of the party with the second-most seats usually becomes the Leader of Her Majesty's Loyal Opposition (presently Jamie Baillie) and is part of an adversarial parliamentary system intended to keep the government in check.\n\nEach of the 51 Members of the Legislative Assembly in the House of Assembly is elected by single member plurality in an electoral district or riding. General elections must be called by the lieutenant governor on the advice of the premier, or may be triggered by the government losing a confidence vote in the House. There are three dominant political parties in Nova Scotia: the Liberal Party, the New Democratic Party, and the Progressive Conservative Party. The other two registered parties are the Green Party of Nova Scotia and the Atlantica Party, neither of which has a seat in the House of Assembly.\n\nThe province's revenue comes mainly from the taxation of personal and corporate income, although taxes on tobacco and alcohol, its stake in the Atlantic Lottery Corporation, and oil and gas royalties are also significant. In 2006–07, the province passed a budget of $6.9 billion, with a projected $72 million surplus. Federal equalization payments account for $1.385 billion, or 20.07% of the provincial revenue. The province participates in the HST, a blended sales tax collected by the federal government using the GST tax system.\n\nNova Scotia no longer has any incorporated cities; they were amalgamated into Regional Municipalities in 1996.\n\nNova Scotia has long been a centre for artistic and cultural excellence. The capital, Halifax, hosts institutions such as Nova Scotia College of Art and Design University, Art Gallery of Nova Scotia, Neptune Theatre, Dalhousie Arts Centre, Two Planks and a Passion Theatre, Ship's Company Theatre and the Symphony Nova Scotia. The province is home to avant-garde visual art and traditional crafting, writing and publishing and a film industry.\n\nMuch of the historic public art sculptures in the province were made by New York sculptor J. Massey Rhind as well as Canadian sculptors Hamilton MacCarthy, George Hill, Emanuel Hahn and Louis-Philippe Hébert. Some of this public art was also created by Nova Scotian John Wilson (sculptor). Nova Scotian George Lang was a stone sculptor who also built many landmark buildings in the province, including the Welsford-Parker Monument.\n\nTwo valuable sculptures/ monuments in the province are in St. Paul's Church (Halifax): one by John Gibson (for Richard John Uniacke, Jr.) and another monument by Sir Francis Leggatt Chantrey (for Amelia Ann Smyth). Both Gibson and Chantry were famous British sculptors during the Victorian era and have numerous sculptures in the Tate, Museum of Fine Arts, Boston and Westminster Abbey.\n\nSome of the province's greatest painters were William Valentine, Maria Morris, Jack L. Gray, Mabel Killiam Day, Ernest Lawson, Frances Bannerman, Alex Colville, Tom Forrestall and ship portrait artist John O'Brien. Some of most notable artists whose works have been acquired by Nova Scotia are British artist Joshua Reynolds (collection of Art Gallery of Nova Scotia); William Gush and William J. Weaver (both have works in Province House); Robert Field (Government House), as well as leading American artists Benjamin West (self portrait in The Halifax Club, portrait of chief justice in Nova Scotia Supreme Court), John Singleton Copley, Robert Feke, and Robert Field (the latter three have works in the Uniacke Estate).\n\nTwo famous Nova Scotian photographers are Wallace R. MacAskill and Sherman Hines. Three of the most accomplished illustrators were George Wylie Hutchinson, Bob Chambers (cartoonist) and Donald A. Mackay.\n\nRenowned American artists like sculptor Richard Serra, composer Philip Glass and abstract painter John Beardman spent part of the year in Nova Scotia.\n\nNova Scotia has produced numerous film actors. Academy Award nominee Ellen Page (\"Juno\", \"Inception\") was born in Halifax, Nova Scotia; five-time Academy Award nominee Arthur Kennedy (\"Lawrence of Arabia\", \"High Sierra\") called Nova Scotia his home; and two time Golden Globe winner Donald Sutherland (\"MASH\", \"Ordinary People\") spent most of his youth in the province. Other actors include John Paul Tremblay, Robb Wells, Mike Smith and John Dunsworth of \"Trailer Park Boys\" and actress Joanne Kelly of \"Warehouse 13\".\n\nNova Scotia has also produced numerous film directors such as Thom Fitzgerald (\"The Hanging Garden\"), Daniel Petrie (\"Resurrection\"—Academy Award nominee) and Acadian film director Phil Comeau's multiple award-winning local story (\"Le secret de Jérôme\").\n\nNova Scotian stories are the subject of numerous feature films: \"Margaret's Museum\" (starring Helena Bonham Carter); \"The Bay Boy\" (directed by Daniel Petrie and starring Kiefer Sutherland); \"New Waterford Girl\"; \"The Story of Adele H.\" (the story of unrequited love of Adèle Hugo); and two films of \"Evangeline\" (one starring Miriam Cooper and another starring Dolores del Río).\n\nThere is a significant film industry in Nova Scotia. Feature filmmaking began in Canada with \"Evangeline\" (1913), made by Canadian Bioscope Company in Halifax, which released six films before it closed. The film has since been lost. Some of the award-winning feature films made in the province are \"Titanic\" (starring Leonardo DiCaprio and Kate Winslet); \"The Shipping News\" (starring Kevin Spacey and Julianne Moore); \"\" (starring Harrison Ford and Liam Neeson) and \"Amelia\" (starring Hilary Swank, Richard Gere and Ewan McGregor).\n\nNova Scotia has also produced numerous television series: \"This Hour Has 22 Minutes\", \"Don Messer's Jubilee\", \"Black Harbour\", \"Haven\", \"Trailer Park Boys\", \"Mr. D\", \"Call Me Fitz\", and \"Theodore Tugboat\". The \"Jesse Stone\" film series on CBS starring Tom Selleck is also routinely produced in the province.\n\nThere are numerous Nova Scotian authors who have achieved international fame: Thomas Chandler Haliburton (\"The Clockmaker\"); Alistair MacLeod (\"No Great Mischief\"); Margaret Marshall Saunders (\"Beautiful Joe\"), Laurence B. Dakin (Marco Polo), and Joshua Slocum (\"Sailing Alone Around the World\"). Other authors include Johanna Skibsrud (\"The Sentimentalists\"), Alden Nowlan (\"Bread, Wine and Salt\"), George Elliott Clarke (\"Execution Poems\"), Lesley Choyce (\"Nova Scotia: Shaped by the Sea\"), Thomas Raddall (\"Halifax: Warden of the North\"), Donna Morrissey (\"Kit's Law\"), Frank Parker Day (\"Rockbound\").\n\nNova Scotia has also been the subject of numerous literary books. Some of the international best-sellers are: \"Last Man Out: The Story of the Springhill Mining Disaster\" (by Melissa Fay Greene) ; \"Curse of the Narrows: The Halifax Explosion 1917\" (by Laura MacDonald); \"In the Village\" (short story by Pulitzer Prize–winning author Elizabeth Bishop); and National Book Critics Circle Award winner \"Rough Crossings\" (by Simon Schama). Other authors who have written novels about Nova Scotian stories include: Linden MacIntyre (\"The Bishop's Man\"); Hugh MacLennan (\"Barometer Rising\"); Rebecca McNutt (\"Mandy and Alecto\"); Ernest Buckler (\"The Valley and the Mountain\"); Archibald MacMechan (\"Red Snow on Grand Pré\"), Henry Wadsworth Longfellow (long poem \"Evangeline\"); Lawrence Hill (\"The Book of Negroes\") and John Mack Faragher (\"Great and Nobel Scheme\").\n\nNova Scotia has produced numerous musicians. The Grammy Award winners include Denny Doherty (from The Mamas & the Papas), Anne Murray, and Sarah McLachlan. Other musicians include country singer Hank Snow, country singer George Canyon, jazz singer Holly Cole, opera singers Portia White and Barbara Hannigan, multi-Juno Award nominated rapper Classified, Rita MacNeil, Matt Mays, Sloan, Feist, Todd Fancey, The Rankin Family, April Wine, Buck 65, Joel Plaskett, Grand Dérangement, and country music singer Drake Jensen.\n\nThere are numerous songs written about Nova Scotia: The Ballad of Springhill (written by Peggy Seeger and performed by Irish folk singer Luke Kelly a member of The Dubliners, U2); numerous songs by Stan Rogers including Bluenose, The Jeannie C (mentions Little Dover, NS), Barrett's Privateers, Giant, and The Rawdon Hills; Farewell to Nova Scotia (traditional); Blue Nose (Stompin' Tom Connors); She's Called Nova Scotia (by Rita MacNeil); Cape Breton (by David Myles); Acadian Driftwood (by Robbie Robertson); Acadie (by Daniel Lanois); and My Nova Scotia Home (by Hank Snow).\n\nNova Scotia has also produced some significant songwriters such as Grammy Award winning Gordie Sampson. Sampson has written songs for Carrie Underwood (\"Jesus, Take the Wheel\", \"Just a Dream\", \"Get Out of This Town\"), Martina McBride (\"If I Had Your Name\", \"You're Not Leavin Me\"), LeAnn Rimes (\"Long Night\", \"Save Myself\"), and George Canyon (\"My Name\"). Another successful Nova Scotia songwriter was Hank Snow whose songs have been recorded by The Rolling Stones, Elvis Presley, and Johnny Cash.\n\nMusic producer Brian Ahern is a Nova Scotian. He got his start by being music director for CBC television's Singalong Jubilee. He later produced 12 albums for Anne Murray (\"Snowbird\", \"Danny’s Song” and \"You Won't See Me\"); 11 albums for Emmylou Harris (whom he married at his home in Halifax on January 9, 1977). He also produced discs for Johnny Cash, George Jones, Roy Orbison, Glen Campbell, Don Williams, Jesse Winchester and Linda Ronstadt. Another noted writer is Cape Bretoner Leon Dubinsky, who wrote the anthem, \"Rise Again\", among many other songs performed by various Canadian artists.\n\nSport is an important part of Nova Scotia culture. There are numerous semi pro, university and amateur sports teams, for example, The Halifax Mooseheads, 2013 Canadian Hockey League Memorial Cup Champions, and the Cape Breton Screaming Eagles, both of the Quebec Major Junior Hockey League. The Halifax Hurricanes of the National Basketball League of Canada is another team that calls Nova Scotia home, and were 2016 league champions.\n\nThe Nova Scotia Open is a professional golf tournament on the Web.com Tour since 2014.\n\nThe province has also produced numerous athletes such as Sidney Crosby (ice hockey), Nathan Mackinnon (ice hockey), Brad Marchand (ice hockey), Colleen Jones (curling), Al MacInnis (ice hockey), TJ Grant (mixed martial arts), Rocky Johnson (wrestling, and father of Dwayne \"The Rock\" Johnson), George Dixon (boxing) and Kirk Johnson (boxing). The achievements of Nova Scotian athletes are presented at the Nova Scotia Sport Hall of Fame.\n\nThe cuisine of Nova Scotia is typically Canadian with an emphasis on local seafood. One endemic dish (in the sense of \"peculiar to\" and \"originating from\") is the Halifax donair, a distant variant of the doner kebab prepared using thinly sliced beef meatloaf and a sweet condensed milk sauce. As well, hodge podge, a creamy soup of fresh baby vegetables, is native to Nova Scotia.\n\nThe province is also known for blueberry grunt.\n\nThere are a number of festivals and cultural events that are recurring in Nova Scotia, or notable in its history. The following is an incomplete list of festivals and other cultural gatherings in the province:\n\nNova Scotia's tourism industry showcases Nova Scotia's culture, scenery and coastline.\nNova Scotia has many museums reflecting its ethnic heritage, including the Glooscap Heritage Centre, Grand-Pré National Historic Site, Hector Heritage Quay and the Black Cultural Centre for Nova Scotia. Other museums tell the story of its working history, such as the Cape Breton Miners' Museum, and the Maritime Museum of the Atlantic.\n\nNova Scotia is home to several internationally renowned musicians and there are visitor centres in the home towns of Hank Snow, Rita MacNeil, and Anne Murray Centre. There are also numerous music and cultural festivals such as the Stan Rogers Folk Festival, Celtic Colours, the Nova Scotia Gaelic Mod, Royal Nova Scotia International Tattoo, the Atlantic Film Festival and the Atlantic Fringe Festival.\n\nThe province has 87 National Historic Sites of Canada, including the Habitation at Port-Royal, the Fortress of Louisbourg and Citadel Hill (Fort George) in Halifax.\n\nNova Scotia has two national parks, Kejimkujik and Cape Breton Highlands, and many other protected areas. The Bay of Fundy has the highest tidal range in the world, and the iconic Peggys Cove is internationally recognized and receives 600,000-plus visitors a year.\n\nAcadian Skies and Mi'kmaq Lands is a starlight reserve in southwestern Nova Scotia. It is the first certified UNESCO-Starlight Tourist Destination. Starlight tourist destinations are locations that offer conditions for observations of stars which are protected from light pollution.\n\nCruise ships pay regular visits to the province. In 2010, Halifax received 261,000 passengers and Sydney 69,000.\n\nA 2008 Nova Scotia tourism campaign included advertising a fictional mobile phone called Pomegranate and establishing website, which after reading about \"new phone\" redirected to tourism info about region.\n\nThe Minister of Education is responsible for the administration and delivery of education, as defined by the Education Act and other acts relating to colleges, universities and private schools. The powers of the Minister and the Department of Education are defined by the Ministerial regulations and constrained by the Governor-In-Council regulations.\n\nNova Scotia has more than 450 public schools for children. The public system offers primary to Grade 12. There are also private schools in the province. Public education is administered by seven regional school boards, responsible primarily for English instruction and French immersion, and also province-wide by the Conseil Scolaire Acadien Provincial, which administers French instruction to students for whom the primary language is French.\n\nThe Nova Scotia Community College system has 13 campuses around the province. The community college, with its focus on training and education, was established in 1988 by amalgamating the province's former vocational schools.\n\nIn addition to its community college system the province has 10 universities, including Dalhousie University, University of King's College, Saint Mary's University, Mount Saint Vincent University, NSCAD University, Acadia University, Université Sainte-Anne, Saint Francis Xavier University, Cape Breton University and the Atlantic School of Theology.\n\nThere are also more than 90 registered private commercial colleges in Nova Scotia.\n\n\n"}
{"id": "21186", "url": "https://en.wikipedia.org/wiki?curid=21186", "title": "Northwest Territories", "text": "Northwest Territories\n\nThe Northwest Territories (NT or NWT; French: \"les Territoires du Nord-Ouest\", \"TNO\"; Athabaskan languages: \"Denendeh\"; Inuinnaqtun: \"Nunatsiaq\"; Inuktitut: ᓄᓇᑦᓯᐊᖅ) is a federal territory of Canada. At a land area of approximately and a 2011 population of 41,462, it is the second-largest and the most populous of the three territories in Northern Canada. Its estimated population as of 2016 is 44,291. Yellowknife became the territorial capital in 1967, following recommendations by the Carrothers Commission.\n\nThe Northwest Territories, a portion of the old North-Western Territory, entered the Canadian Confederation on July 15, 1870, but the current borders were formed on April 1, 1999, when the territory was subdivided to create Nunavut to the east, via the \"Nunavut Act\" and the \"Nunavut Land Claims Agreement\". While Nunavut is mostly Arctic tundra, the Northwest Territories has a slightly warmer climate and is both boreal forest (taiga), and tundra, and its most northern regions form part of the Canadian Arctic Archipelago.\n\nThe Northwest Territories is bordered by Canada's two other territories, Nunavut to the east and Yukon to the west, and by the provinces of British Columbia, Alberta, and Saskatchewan to the south.\n\nThe name is descriptive, adopted by the British government during the colonial era to indicate where it lay in relation to Rupert's Land. It is shortened from North-Western Territory (\"see\" History). In Inuktitut, the Northwest Territories are referred to as ᓄᓇᑦᓯᐊᖅ (\"Nunatsiaq\"), \"beautiful land.\"\n\nThere was some discussion of changing the name of the Northwest Territories after the splitting off of Nunavut, possibly to a term from an Aboriginal language. One proposal was \"Denendeh\" (an Athabaskan language word meaning \"our land\"), as advocated by the former premier Stephen Kakfwi, among others. One of the most popular proposals for a new name – one to name the territory \"\"Bob\"\" – began as a prank, but for a while it was at or near the top in the public-opinion polls.\n\nIn the end a poll conducted prior to division showed that strong support remained to keep the name \"Northwest Territories\". This name arguably became more appropriate following division than it had been when the territories extended far into Canada's north-central and northeastern areas.\n\nLocated in northern Canada, the territory borders Canada's two other territories, Yukon to the west and Nunavut to the east, and three provinces: British Columbia to the southwest, and Alberta and Saskatchewan to the south. It possibly meets Manitoba at a quadripoint to the extreme southeast, though surveys have not been completed. It has a land area of .\n\nGeographical features include Great Bear Lake, the largest lake entirely within Canada, and Great Slave Lake, the deepest body of water in North America at , as well as the Mackenzie River and the canyons of the Nahanni National Park Reserve, a national park and UNESCO World Heritage Site. Territorial islands in the Canadian Arctic Archipelago include Banks Island, Borden Island, Prince Patrick Island, and parts of Victoria Island and Melville Island. Its highest point is Mount Nirvana near the border with Yukon at an elevation of .\n\nThe Northwest Territories extends for more than and has a large climate variant from south to north. The southern part of the territory (most of the mainland portion) has a subarctic climate, while the islands and northern coast have a polar climate.\n\nSummers in the north are short and cool, with daytime highs of 14-17 Celsius (60° to 70 °F), and lows of 1-5 Celsius (45° to 55 °F). Winters are long and harsh, daytime highs in the mid and lows around . Extremes are common with summer highs in the south reaching and lows reaching into the negatives. In winter in the south, it is not uncommon for the temperatures to reach , but they can also reach the low teens during the day. In the north, temperatures can reach highs of , and lows can reach into the low negatives.\nIn winter in the north it is not uncommon for the temperatures to reach but they can also reach the single digits during the day. Thunderstorms are not rare in the south. In the north they are very rare, but do occur. Tornadoes are extremely rare but have happened with the most notable one happening just outside Yellowknife that destroyed a communications tower. The Territory has a fairly dry climate due to the mountains in the west.\n\nAbout half of the territory is above the tree line. There are not many trees in most of the eastern areas of the territory, or in the north islands.\n\nThe present-day territory came under government authority in July 1870, after the Hudson's Bay Company transferred Rupert's Land and the North-Western Territory to the British Crown, which subsequently transferred them to the Dominion of Canada, giving it the name the North-west Territories. This immense region comprised all of today's Canada except that which was encompassed within the early signers of Canadian Confederation, that is, British Columbia, early forms of present-day Ontario and Quebec (which encompassed the coast of the Great Lakes, the Saint Lawrence River valley and the southern third of Quebec), the Maritimes (NS, PEI and NB), Newfoundland, the Labrador coast, and the Arctic Islands, except the southern half of Baffin Island (the Arctic Islands remained under direct British claim until 1880).\n\nAfter the 1870 transfer, some of the North-west Territories was whittled away. The province of Manitoba was created on July 15, 1870, at first a small square area around Winnipeg, and then enlarged in 1881 to a larger rectangular region composing the modern province's south. By the time British Columbia joined Confederation on July 20, 1871, it had already (1866) been granted the portion of North-Western Territory south of 60 degrees north and west of 120 degrees west, an area that comprised most of the Stickeen Territories.\nIn the meantime, the Province of Ontario was enlarged northwestward in 1882. Quebec was also extended northwards in 1898. Yukon was made a separate territory that year, due to the Klondike Gold Rush, to free the North-west Territories government in Regina from the burden of addressing the problems caused by the sudden boom of population and economic activity, and the influx of non-Canadians.\n\nThe provinces of Alberta and Saskatchewan were created in 1905. In 1906, the Parliament of Canada renamed the \"North-West Territories\" as the \"Northwest Territories\", dropping all hyphenated forms of it.\n\nManitoba, Ontario, and Quebec acquired the last addition to their modern landmass from the Northwest Territories in 1912. This left only the districts of Mackenzie, Franklin (which absorbed the remnants of Ungava in 1920), and Keewatin within what was then given the name Northwest Territories. In 1925, the boundaries of the Northwest Territories were extended all the way to the North Pole on the sector principle, vastly expanding its territory onto the northern ice cap. Between 1925 and 1999, the Northwest Territories covered a land area of – larger than that of India.\n\nOn April 1, 1999, the existing Northwest Territories was split into two parts, with a separate Nunavut territory being formed to represent the Inuit people.\n\nThe NWT is one of two jurisdictions in Canada – Nunavut being the other – where Aboriginal peoples are in the majority, constituting 50.4% of the population.\n\nAccording to the 2016 Canadian census, the 10 major ethnic groups were:\n\nPopulation of the Northwest Territories since 1871\n\nThe largest denominations by number of adherents according to the 2001 census were Roman Catholic with 16,940 (46.7%); the Anglican Church of Canada with 5,510 (14.9%); and the United Church of Canada with 2,230 (6.0%), while a total of 6,465 (17.4%) people stated no religion.\n\nFrench was made an official language in 1877 by the territorial government. After a lengthy and bitter debate resulting from a speech from the throne in 1888 by Lieutenant Governor Joseph Royal the members of the day voted on more than one occasion to nullify that and make English the only language used in the assembly. After some conflict with the Confederation Government in Ottawa, and a decisive vote on January 19, 1892, the assembly members voted for an English-only territory.\n\nThe Northwest Territories' Official Languages Act recognizes the following eleven official languages:\nNWT residents have a right to use any of the above languages in a territorial court, and in the debates and proceedings of the legislature. However, the laws are legally binding only in their French and English versions, and the NWT government only publishes laws and other documents in the territory's other official languages when the legislature asks it to. Furthermore, access to services in any language is limited to institutions and circumstances where there is a significant demand for that language or where it is reasonable to expect it given the nature of the services requested. In practical terms, English language services are universally available, and there is no guarantee that other languages, including French, will be used by any particular government service, except for the courts.\n\nThe 2016 census returns showed a population of 41,786. Of the 40,565 singular responses to the census question regarding each inhabitant's \"mother tongue\", the most reported languages were the following (italics indicate an official language of the NWT):\nThere were also 630 responses of both English and a \"non-official language\"; 35 of both French and a \"non-official language\"; 145 of both English and French, and about 400 people who either did not respond to the question, or reported multiple non-official languages, or else gave some other unenumerable response. The Northwest Territories' official languages are shown in bold.\n\nAs of 2014 there are 33 official communities in the NWT. These range in size from Yellowknife with a population of 19,569 to Kakisa with 36 people. Governance of each community differs, some are run under various types of First Nations control, while others are designated as a city, town, village or hamlet, but most communities are municipal corporations. Yellowknife is the largest community and has the largest number of Aboriginal peoples, 4,520 (23.4%) people. However, Behchokǫ̀, with a population of 1,874, is the largest First Nations community, 1,696 (90.9%), and Inuvik with 3,243 people is the largest Inuvialuit community, 1,315 (40.5%). There is one Indian reserve in the NWT, Hay River Reserve, located on the south shore of the Hay River.\n\nThe NWT's geological resources include gold, diamonds, natural gas and petroleum. BP is the only oil company currently producing oil in the Territory. NWT diamonds are promoted as an alternative to purchasing blood diamonds. Two of the biggest mineral resource companies in the world, BHP Billiton and Rio Tinto mine many of their diamonds from the NWT. In 2010, NWT accounted for 28.5% of Rio Tinto's total diamond production (3.9 million carats, 17% more than in 2009, from the Diavik Diamond Mine) and 100% of BHP's (3.05 million carats from the EKATI mine).\n\nThe Northwest Territories has the highest per capita GDP of all provinces or territories in Canada, C$76,000 in 2009. However, as production at the current mines started to wind down, no new mines opened and the public service shrank, the territory lost 1,200 jobs between November 2013 and November 2014.\n\n\nAs a territory, the NWT has fewer rights than the provinces. During his term, Premier Kakfwi pushed to have the federal government accord more rights to the territory, including having a greater share of the returns from the territory's natural resources go to the territory. Devolution of powers to the territory was an issue in the 20th general election in 2003, and has been ever since the territory began electing members in 1881.\n\nThe Commissioner of the NWT is the chief executive and is appointed by the Governor-in-Council of Canada on the recommendation of the federal Minister of Aboriginal Affairs and Northern Development. The position used to be more administrative and governmental, but with the devolution of more powers to the elected assembly since 1967, the position has become symbolic. The Commissioner had full governmental powers until 1980 when the territories were given greater self-government. The Legislative Assembly then began electing a cabinet and \"Government Leader\", later known as the Premier. Since 1985 the Commissioner no longer chairs meetings of the Executive Council (or cabinet), and the federal government has instructed commissioners to behave like a provincial Lieutenant Governor. Unlike Lieutenant Governors, the Commissioner of the Northwest Territories is not a formal representative of the Queen of Canada.\n\nUnlike provincial governments and the government of Yukon, the government of the Northwest Territories does not have political parties, except for the period between 1898 and 1905. It is a consensus government called the Legislative Assembly. This group is composed of one member elected from each of the nineteen constituencies. After each general election, the new Assembly elects the Premier and the Speaker by secret ballot. Seven MLAs are also chosen as cabinet ministers, with the remainder forming the opposition.\n\nThe current Legislative Assembly is the 18th and the most recent election was held November 23, 2015. The Premier is Bob McLeod. The member of Parliament for the Northwest Territories is Michael McLeod (Liberal Party). The Commissioner of the Northwest Territories is George Tuccaro and the Deputy Commissioner is Margaret Thom.\n\nIn the Parliament of Canada, the NWT comprises a single Senate division and a single House of Commons electoral district, titled Northwest Territories (\"Western Arctic\" until 2014).\n\nThe Government of Northwest Territories comprises the following departments:\n\nThe Northwest Territories is divided into five administrative regions (with regional seat):\n\nExact regional borders are undefined.\n\nNorthwest Territories has eight numbered highways. The longest is the Mackenzie Highway which stretches from the Alberta Highway 35's northern terminus in the south at the Alberta – Northwest Territories border at the 60th parallel to Wrigley, Northwest Territories in the north. Ice roads and winter roads are also prominent and provide road access in winter to towns and mines which would otherwise be fly-in locations. Yellowknife Highway branches out from Mackenzie Highway and connects it to Yellowknife. Dempster Highway is the continuation of Klondike Highway. It starts just west of Dawson City, Yukon, and continues east for over to Inuvik.\n\nYellowknife did not have an all-season road access to the rest of Canada's highway network until the completion of Deh Cho Bridge in 2012. Prior to that, traffic relied on ferry service in summer and ice road in winter to cross the Mackenzie River. This became a problem during spring and fall time when the ice was not thick enough to handle vehicle load but the ferry could not pass through the ice, which would require all goods from fuel to groceries to be airlifted during the transition period.\n\nYellowknife Transit is the public transportation agency in the city, and is the only transit system within the Northwest Territories.\n\nYellowknife Airport is the largest airport in the territory in terms of aircraft movements and passengers. It is the gateway airport to other destinations within the Northwest Territories. As the airport of the territory capital, it is part of the National Airports System. It is the hub of multiple regional airlines. Major airlines serving destinations within Northwest Territories include Buffalo Airways, Canadian North, First Air, North-Wright Airways.\n\nAboriginal issues in the Northwest Territories include the fate of the Dene who, in the 1940s, were employed to carry radioactive uranium ore from the mines on Great Bear Lake. Of the thirty plus miners who worked at the Port Radium site, at least fourteen have died due to various forms of cancer. A study was done in the community of Deline, called \"A Village of Widows\" by Cindy Kenny-Gilday, which indicated that the number of people involved were too small to be able to confirm or deny a link.\n\nThere has been racial tension based on a history of violent conflict between the Dene and the Inuit, who have now taken recent steps towards reconciliation.\n\nLand claims in the NWT began with the Inuvialuit Final Agreement, signed on June 5, 1984. It was the first Land Claim signed in the Territory, and the second in Canada. It culminated with the creation of the Inuit homeland of Nunavut, the result of the Nunavut Land Claims Agreement, the largest land claim in Canadian history.\n\nAnother land claims agreement with the Tłı̨chǫ people created a region within the NWT called Tli Cho, between Great Bear and Great Slave Lakes, which gives the Tłı̨chǫ their own legislative bodies, taxes, resource royalties, and other affairs, though the NWT still maintains control over such areas as health and education. This area includes two of Canada's three diamond mines at Ekati and Diavik.\n\nAmong the festivals in the region are the Great Northern Arts Festival, the Snowking Winter Festival, Folk on the Rocks music festival in Yellowknife, and Rockin the Rocks.\n\nDuring the winter, many international visitors go to Yellowknife to watch the auroras.\n\nAulavik National Park is in the northern part of Northwest Territories.\n\n\n"}
{"id": "21187", "url": "https://en.wikipedia.org/wiki?curid=21187", "title": "Nez Perce people", "text": "Nez Perce people\n\nThe Nez Perce (; autonym: , meaning \"the walking people\" or \"we, the people\") are an Indigenous people of the Plateau who have lived on the Columbia River Plateau in the Pacific Northwest region of the United States for a long time.\n\nMembers of the Sahaptin language group, the Niimíipuu were the dominant people of the Columbia Plateau for much of that time, especially after acquiring the horses that led them to breed the appaloosa horse in the 18th century.\n\nPrior to \"first contact\" with Western civilization the Nimiipuu were economically and culturally influential in trade and war, interacting with other indigenous nations in a vast network from the western shores of Oregon and Washington, the high plains of Montana, and the northern Great Basin in southern Idaho and northern Nevada.\n\nAfter first contact, the name \"Nez Perce\" was given to the Niimíipuu and the nearby Chinook people by French explorers and trappers. The name means \"pierced nose,\" but only the Chinook used that form of decoration.\n\nToday they are a federally recognized tribe, the Nez Perce Tribe of Idaho, and govern their Indian reservation in Idaho through a central government headquartered in Lapwai, Idaho known as the Nez Perce Tribal Executive Committee (NPTEC). They are one of five federally recognized tribes in the state of Idaho. Some still speak their traditional language, and the Tribe owns and operates two casinos along the Clearwater River in Idaho in Kamiah, Idaho and outside of Lewiston, Idaho, health clinics, a police force and court, community centers, salmon fisheries, radio station, and other things that promote economic and cultural self-determination.\n\nCut off from most of their horticultural sites throughout the Camas Prairie by an 1863 treaty, confinement to reservations in Idaho, Washington and Oklahoma Indian Territory after the Nez Perce War of 1877, and Dawes Act of 1887 land allotments (today some Nez Perce lease land to farmers or loggers, but the Nez Perce only own 12% of their own reservation), the Nez Perce remain as a distinct culture and political economic influence within and outside their reservation. Today, hatching, harvesting and eating salmon is an important cultural and economic strength of the Nez Perce through full ownership or co-management of various salmon fish hatcheries, such as the Kooskia National Fish Hatchery in Kooskia, Idaho or the Dworshak National Fish Hatchery in Orofino, Idaho.\n\nTheir name for themselves is \"Nimíipuu\" (pronounced ), meaning, \"The People\", in their language, part of the Sahaptin family.\n\n\"Nez Percé\" is an exonym given by French Canadian fur traders who visited the area regularly in the late 18th century, meaning literally \"pierced nose.\" English-speaking traders and settlers adopted the name in turn. Since the late 20th century, the Nez Perce identify most often as Niimíipuu in Sahaptin. The Lakota/ Dakota named them the \"Watopala\", or \"Canoe\" people, from \"Watopa\". However, after Nez Perce became a more common name, they changed it to \"Watopahlute\". This comes from \"pahlute\", nasal passage & is simply a play on words. If translated literally, it would come out as either \"\"Nasal Passage of the Canoe,\"\" (Watopa-pahlute) or \"\"Nasal Passage of the Grass.\"\" (Wato-pahlute) The tribe also uses the term \"Nez Perce,\" as does the United States Government in its official dealings with them, and contemporary historians. Older historical ethnological works and documents use the French spelling of \"Nez Percé,\" with the diacritic. The original French pronunciation is , with three syllables.\n\nThe interpreter of the Lewis and Clark Expedition mistakenly identified this people as the Nez Perce when the team encountered the tribe in 1805. Writing in 1889, anthropologist Alice Fletcher, who the U.S. government had sent to Idaho to allot the Nez Perce Reservation, explained the mistaken naming. She wrote, \"It is never easy to come at the name of an Indian or even of an Indian tribe. A tribe has always at least two names; one they call themselves by and one by which they are known to other tribes. All the tribes living west of the Rocky Mountains were called \"Chupnit-pa-lu,\" which means people of the pierced noses; it also means emerging from the bushes or forest; the people from the woods. The tribes on the Columbia river used to pierce the nose and wear in it some ornament as you have seen some old fashioned white ladies wear in their ears. Lewis and Clark had with them an interpreter whose wife was a Shoshone or Snake woman and so it came about that when it was asked \"What Indians are these?\" the answer was \"They are 'Chupnit-pa-lu'\" and it was written down in the journal; spelled rather queerly, for white people's ears do not always catch Indian tones and of course the Indians could not spell any word.\"\n\nIn his journals, William Clark referred to the people as the Chopunnish , a transliteration of a Sahaptin term. According to D.E. Walker in 1998, writing for the Smithsonian, this term is an adaptation of the term \"cú·pŉitpeľu\" (the Nez Perce people). The term is formed from \"cú·pŉit\" (piercing with a pointed object) and \"peľu\" (people). By contrast, the \"Nez Perce Language Dictionary\" has a different analysis than did Walker for the term \"cúpnitpelu.\" The prefix \"cú\"- means \"in single file.\" This prefix, combined with the verb \"-piní\", \"to come out (e.g. of forest, bushes, ice)\". Finally, with the suffix of \"-pelú\", meaning \"people or inhabitants of.\" Together, these three elements: \"cú\"- + -\"piní\" + \"pelú\" = \"cúpnitpelu\", or \"the People Walking Single File Out of the Forest.\" Nez Perce oral tradition indicates the name \"Cuupn'itpel'uu\" meant \"we walked out of the woods or walked out of the mountains\" and referred to the time before the Nez Perce had horses.\n\nThe Nez Perce language, or Niimiipuutímt, is a Sahaptian language related to the several dialects of Sahaptin. The Sahaptian sub-family is one of the branches of the Plateau Penutian family, which in turn may be related to a larger Penutian grouping.\n\nThe Nez Perce territory at the time of Lewis and Clark (1804–1806) was approximately and covered parts of present-day Washington, Oregon, Montana, and Idaho, in an area surrounding the Snake (Weyikespe), Grande Ronde River, Salmon (Naco’x kuus) (\"Chinook salmon Water\") and the Clearwater (Koos-Kai-Kai) (\"Clear Water\") rivers. The tribal area extended from the Bitterroots in the east (the door to the Northwestern Plains of Montana) to the Blue Mountains in the west between latitudes 45°N and 47°N.\n\nIn 1800, the Nez Perce had more than 100 permanent villages, ranging from 50 to 600 individuals, depending on the season and social grouping. Archeologists have identified a total of about 300 related sites including camps and villages, mostly in the Salmon River Canyon. In 1805, the Nez Perce were the largest tribe on the Columbia River Plateau, with a population of about 6,000. By the beginning of the 20th century, the Nez Perce had declined to about 1,800 due to epidemics, conflicts with non-Indians, and other factors. A total of 3,499 Nez Perce were counted in the 2010 Census.\n\nLike other Plateau tribes, the Nez Perce had seasonal villages and camps in order to take advantage of natural resources throughout the year. Their migration followed a recurring pattern from permanent winter villages through several temporary camps, nearly always returning to the same locations each year. The Nez Perce traveled via the Lolo Trail (Salish: Naptnišaqs - \"Nez Perce Trail\") (Khoo-say-ne-ise-kit) far east as the Plains (Khoo-sayn / Kuseyn) (\"Buffalo country\") of Montana to hunt buffalo (Qoq'a lx) and as far west as the Pacific Coast (’Eteyekuus) (\"Big Water\"). Before 1957 construction of The Dalles Dam, which flooded this area, Celilo Falls (Silayloo) was a favored location on the Columbia River (Xuyelp) (\"The Great River\") for salmon (lé'wliks)-fishing.\n\nThe Nez Perce had many allies and trading partners among neighboring peoples, but also enemies and ongoing antagonist tribes. To the north of them lived the Coeur d’Alene (Schitsu'umsh) (’Iskíicu’mix), Spokane (Sqeliz) (Heyéeynimuu), and further north the Kalispel (Ql̓ispé) (Qem’éespel’uu, both meaning \"Camas People\"), Colville (Páapspaloo) and Kootenay / Kootenai (Ktunaxa) (Kuuspel’úu), to the northwest lived the Palus (Pelúucpuu) and to the west the Cayuse (Lik-si-yu) (Weyíiletpuu - \"Ryegrass People\"), west bound there were found the Umatilla (Imatalamłáma) (Hiyówatalampoo), Walla Walla, Wasco (Wecq’úupuu) and Sk'in (Tike’éspel’uu) and northwest of the latter various Yakama bands (Lexéyuu), to the south lived the Snake Indians (various Northern Paiute (Numu) bands (Hey’ǘuxcpel’uu) in the southwest and Bannock (Nimi Pan a'kwati)-Northern Shoshone (Newe) bands (Tiwélqe) in the southeast), to the east lived the Lemhi Shoshone (Lémhaay), north of them the Bitterroot Salish / Flathead (Seliš) (Séelix), further east and northeast on the Northern Plains were the Crow (Apsáalooke) (’Isúuxe) and two powerful alliances - the Iron Confedery (Nehiyaw-Pwat) (named after the dominating Plains and Woods Cree (Paskwāwiyiniwak and Sakāwithiniwak) and Assiniboine (Nakoda) (Wihnen’íipel’uu), an alliance of northern plains Native American nations based around the fur trade, and later included the Stoney (Nakoda), Western Saulteaux / Plains Ojibwe (Bungi or Nakawē), and Métis) and the Blackfoot Confederacy (Niitsitapi or Siksikaitsitapi) (’Isq’óyxnix) (composed of three Blackfoot speaking peoples - the Piegan or Peigan (Piikáni), the Kainai or Bloods (Káínaa), and the Siksika or Blackfoot (Siksikáwa), later joined by the unrelated Sarcee (Tsuu T'ina) and (for a time) by Gros Ventre or Atsina (A'aninin)).\n\n\nBecause of great inter-marriage between Nez Perce bands and neighboring tribes or bands to forge alliances and peace (often living in mixed bilingual villages together), the following bands were also counted to the Nez Perce (which today are viewed as being linguistically and culturally closely related, but separate ethnic groups):\n\nThe semi-sedentary Nez Percés were Hunter-gatherer without agriculture living in a society in which most or all food is obtained by foraging (collecting wild plants and roots and pursuing wild animals). They depended on hunting, fishing, and the gathering of wild roots and berries.\n\nNez Perce people historically depended on various Pacific salmon and Pacific trout for their food: Chinook salmon or \"\"nacoox\"\" (Oncorhynchus tschawytscha) were eaten the most, but other species such as Pacific lamprey (Entosphenus tridentatus or Lampetra tridentata), and chiselmouth. Other important fishes included the Sockeye salmon (Oncorhynchus nerka), Silver salmon or \"ka'llay\" (Oncorhynchus kisutch), Chum salmon or dog salmon or \"ka'llay\" (Oncorhynchus keta), Mountain whitefish or \"\"ci'mey\"\" (Prosopium williamsoni), White sturgeon (Acipenser transmontanus), White sucker or \"\"mu'quc\"\" (Catostomus commersonii), and varieties of trout - West Coast steelhead or \"\"heyey\"\" (Oncorhynchus mykiss), brook trout or \"\"pi'ckatyo\"\" (Salvelinus fontinalis), bull trout or \"\"i'slam\"\" (Salvelinus confluentus), and Cutthroat trout or \"\"wawa'lam\"\" (Oncorhynchus clarkii).\n\nPrior to contact with Europeans, the Nez Perce's traditional hunting and fishing areas spanned from the Cascade Range in the west to the Bitterroot Mountains in the east.\n\nHistorically, in late May and early June, Nez Perce villagers crowded to communal fishing sites to trap eels, steelhead, and chinook salmon, or haul in fish with large dip nets. Fishing took place throughout the summer and fall, first on the lower streams and then on the higher tributaries, and catches also included salmon, sturgeon, whitefish, suckers, and varieties of trout. Most of the supplies for winter use came from a second run in the fall, when large numbers of Sockeye salmon, silver, and dog salmon appeared in the rivers.\n\nFishing is traditionally an important ceremonial and commercial activity for the Nez Perce tribe. Today Nez Perce fishers participate in tribal fisheries in the mainstream Columbia River between Bonneville and McNary dams. The Nez Perce also fish for spring and summer Chinook salmon and Rainbow trout/steelhead in the Snake River and its tributaries. The Nez Perce tribe runs the Nez Perce Tribal Hatchery on the Clearwater River, as well as several satellite hatchery programs. \nThe first fishing of the season was accompanied by prescribed rituals and a ceremonial feast known as \"\"kooyit\"\". Thanksgiving was offered to the Creator and to the fish for having returned and given themselves to the people as food. In this way, it was hoped that the fish would return the next year.\n\nLike salmon, plants contributed to traditional Nez Perce culture in both material and spiritual dimensions.\n\nAside from fish and game, Plant foods provided over half of the dietary calories, with winter survival depending largely on dried roots, especially Kouse, or \"\"qáamsit\"\" (when fresh) and \"\"qáaws\"\" (when peeled and dried) (Lomatium especially Lomatium cous), and Camas, or \"\"qém'es\"\" (Nez Perce: \"sweet\") (Camassia quamash), the first being roasted in pits, while the other was ground in mortars and molded into cakes for future use, both plants had been traditionally an important food and trade item. Women were primarily responsible for the gathering and preparing of these root crops. Camas bulbs were gathered in the region between the Salmon and Clearwater river drainages. Techniques for preparing and storing winter foods enabled people to survive times of colder winters with little or no fresh foods.\n\nFavorite fruits dried for winter were serviceberries or \"\"kel\"\" (Amelanchier alnifolia or Saskatoon berry), black huckleberries or \"\"cemi'tk\"\" (Vaccinium membranaceum), red elderberries or \"\"mi'ttip\"\" (Sambucus racemosa var. melanocarpa), and chokecherries or \"\"ti'ms\"\" (Prunus virginiana var. melanocarpa). Nez Perce textiles were made primarily from dogbane or \"\"qeemu\"\" (Apocynum cannabinum or Indian hemp), tules or \"\"to'ko\"\" (Schoenoplectus acutus var. acutus), and western redcedar or \"\"tala'tat\"\" (Thuja plicata). The most important industrial woods were redcedar, ponderosa pine or \"\"la'qa\"\" (Pinus ponderosa), Douglas fir or \"\"pa'ps\"\" (Pseudotsuga menziesii), sandbar willow or \"\"tax's\"\" (Salix exigua), and hard woods such as Pacific yew or \"\"ta'mqay\"\" (Taxus brevifolia) and syringa or \"\"sise'qiy\"\" (Philadelphus lewisii or Indian arrowwood).\n\nMany fishes and plants important to Nez Perce culture are today state symbols: the black huckleberry or \"\"cemi'tk\"\" is the official state fruit and the Indian arrowwood or \"\"sise'qiy\"\", the Douglas fir or \"\"pa'ps\"\" is the state tree of Oregon and the ponderosa pine or \"\"la'qa\"\" of Montana, the Chinook salmon is the state fish of Oregon, the cutthroat trout or \"\"wawa'lam\"\" of Idaho, Montana and Wyoming, and the West Coast steelhead or \"heyey\" of Washington.\nThe Nez Perce believed in spirits called \"weyekins\" (Wie-a-kins) which would, they thought, offer a link to the invisible world of spiritual power\". The weyekin would protect one from harm and become a personal guardian spirit. To receive a weyekin, a seeker would go to the mountains alone on a vision quest. This included fasting and meditation over several days. While on the quest, the individual may receive a vision of a spirit, which would take the form of a mammal or bird. This vision could appear physically or in a dream or trance. The weyekin was to bestow the animal's powers on its bearer—for example; a deer might give its bearer swiftness. A person's weyekin was very personal. It was rarely shared with anyone and was contemplated in private. The weyekin stayed with the person until death.\n\nHelen Hunt Jackson, author of \"A Century of Dishonor\", written in 1889 refers to the Nez Perce as \"the richest, noblest, and most gentle\" of Indian peoples as well as the most industrious.\n\nThe museum at the Nez Perce National Historical Park, headquartered in Spalding, Idaho, and managed by the National Park Service includes a research center, archives, and library. Historical records are available for on-site study and interpretation of Nez Perce history and culture. The park includes 38 sites associated with the Nez Perce in the states of Idaho, Montana, Oregon, and Washington, many of which are managed by local and state agencies.\n\nIn 1805 William Clark was the first known Euro-American to meet any of the tribe, excluding the aforementioned French Canadian traders. While he, Meriwether Lewis and their men were crossing the Bitterroot Mountains, they ran low of food, and Clark took six hunters and hurried ahead to hunt. On September 20, 1805, near the western end of the Lolo Trail, he found a small camp at the edge of the camas-digging ground, which is now called Weippe Prairie. The explorers were favorably impressed by the Nez Perce whom they met. Preparing to make the remainder of their journey to the Pacific by boats on rivers, they entrusted the keeping of their horses until they returned to \"2 brothers and one son of one of the Chiefs.\" One of these Indians was \"Walammottinin\" (meaning \"Hair Bunched and tied,\" but more commonly known as Twisted Hair). He was the father of Chief Lawyer, who by 1877 was a prominent member of the \"Treaty\" faction of the tribe. The Nez Perce were generally faithful to the trust; the party recovered their horses without serious difficulty when they returned.\n\nRecollecting the Nez Perce encounter with the Lewis and Clark party, in 1889 anthropologist Alice Fletcher wrote that \"the Lewis and Clark explorers were the first white men that many of the people had ever seen and the women thought them beautiful.\" She wrote that the Nez Perce \"were kind to the tired and hungry party. They furnished fresh horses and dried meat and fish with wild potatoes and other roots which were good to eat, and the refreshed white men went further on, westward, leaving their bony, wornout horses for the Indians to take care of and have fat and strong when Lewis and Clark should come back on their way home.\" On their return trip they arrived at the Nez Perce encampment the following spring, again hungry and exhausted. The tribe constructed a large tent for them and again fed them. Desiring fresh red meat, the party offered an exchange for a Nez Perce horse. Quoting from the Lewis and Clark diary, Fletcher writes, \"The hospitality of the Chiefs was offended at the idea of an exchange. He observed that his people had an abundance of young horses and that if we were disposed to use that food, we might have as many as we wanted.\" The party stayed with the Nez Perce for a month before moving on.\n\nThe Nez Perce were one of the tribal nations at the Walla Walla Council (1855) (along with the Cayuse, Umatilla, Walla Walla, and Yakama), which signed the Treaty of Walla Walla.\n\nUnder pressure from the European Americans, in the late 19th century the Nez Perce split into two groups: one side accepted the coerced relocation to a reservation and the other refused to give up their fertile land in Idaho and Oregon. Those willing to go to a reservation made a treaty in 1877. The flight of the non-treaty Nez Perce began on June 15, 1877, with Chief Joseph, Looking Glass, White Bird, Ollokot, Lean Elk (Poker Joe) and Toohoolhoolzote leading 2,900 men, women and children in an attempt to reach a peaceful sanctuary. They intended to seek shelter with their allies the Crow but, upon the Crow's refusal to offer help, the Nez Perce tried to reach the camp in Canada of Lakota Chief Sitting Bull. He had migrated there instead of surrendering after the decisive Indian victory at the Battle of the Little Bighorn.\nThe Nez Perce were pursued by over 2,000 soldiers of the U.S. Army on an epic flight to freedom of more than across four states and multiple mountain ranges. The 800 Nez Perce warriors defeated or held off the pursuing troops in 18 battles, skirmishes, and engagements. More than 300 US soldiers and 1,000 Nez Perce (including women and children) were killed in these conflicts.\n\nA majority of the surviving Nez Perce were finally forced to surrender on October 5, 1877, after the Battle of the Bear Paw Mountains in Montana, from the Canada–US border. Chief Joseph surrendered to General Oliver O. Howard of the U.S. Cavalry. During the surrender negotiations, Chief Joseph sent a message, usually described as a speech, to the US soldiers. It has become renowned as one of the greatest American speeches: \"...Hear me, my chiefs, I am tired. My heart is sick and sad. From where the sun now stands, I will fight no more forever.\"\n\nThe route of the Nez Perce flight is preserved by the Nez Perce National Historic Trail. The annual Cypress Hills ride in June commemorates the Nez Perce people's attempt to escape to Canada.\n\nIn 1994 the Nez Perce tribe began a breeding program, based on crossbreeding the Appaloosa and a Central Asian breed called Akhal-Teke, to produce what they called the Nez Perce Horse. They wanted to restore part of their traditional horse culture, where they had conducted selective breeding of their horses, long considered a marker of wealth and status, and trained their members in a high quality of horsemanship. Social disruption due to reservation life and assimilationist pressures by Americans and the government resulted in the destruction of their horse culture in the 19th century. The 20th-century breeding program was financed by the United States Department of Health and Human Services, the Nez Perce tribe, and the nonprofit called the First Nations Development Institute. It has promoted businesses in Native American country that reflect values and traditions of the peoples. The Nez Perce Horse breed is noted for its speed.\n\nThe current tribal lands consist of a reservation in north central Idaho at , primarily in the Camas Prairie region south of the Clearwater River, in parts of four counties. In descending order of surface area, the counties are Nez Perce, Lewis, Idaho, and Clearwater. The total land area is about , and the reservation's population at the 2000 census was 17,959.\n\nDue to tribal loss of lands, the population on the reservation is predominantly white, nearly 90% in 1988. The largest community is the city of Orofino, near its northeast corner. Lapwai is the seat of tribal government, and it has the highest percentage of Nez Percé people as residents, at about 81.4 percent.\n\nSimilar to the opening of Native American lands in Oklahoma by allowing acquisition of surplus by non-natives after households received plots, the U.S. government opened the Nez Percé reservation for general settlement on November 18, 1895. The proclamation had been signed less than two weeks earlier by President Grover Cleveland. Thousands rushed to grab land on the reservation, staking out their claims even on land owned by Nez Percé families.\n\n\n\n\nIn addition, the Colville Indian Reservation in eastern Washington contains the Joseph band of Nez Percé.\n\n\n\n"}
{"id": "21189", "url": "https://en.wikipedia.org/wiki?curid=21189", "title": "Neolithic", "text": "Neolithic\n\nThe Neolithic (, also known as the \"New Stone Age\"), the final division of the Stone Age, began about 12,000 years ago when the first development of farming appeared in the Epipalaeolithic Near East, and later in other parts of the world. \nThe division lasted until the transitional period of the Chalcolithic from about 6,500 years ago (4500 BC), marked by the development of metallurgy, leading up to the Bronze Age and Iron Age.\nIn Northern Europe, the Neolithic lasted until about 1700 BC, while in China it extended until 1200 BC.\nOther parts of the world (the New World) remained in the Neolithic stage of development until European contact.\n\nThe Neolithic comprises a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals.\n\nThe term \"Neolithic\" derives from the Greek , \"new\" and , \"stone\", literally meaning \"New Stone Age\". The term was coined by Sir John Lubbock in 1865 as a refinement of the three-age system.\n\nFollowing the ASPRO chronology, the Neolithic arises at 10,200 BC in the Levant, arising from the Natufian culture, where pioneering use of wild cereals evolved into early farming. The Natufian period or \"proto-Neolithic\" lasted from 12,500 to 9,500 BC, and taken to overlap with the Pre-Pottery Neolithic (PPNA) of 10,200–8800 BC. \nAs the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas (about 10,000 BC) are thought to have forced people to develop farming.\n\nBy 10,200–8800 BC farming communities had arisen in the Levant and spread to Asia Minor, North Africa and North Mesopotamia. Mesopotamia is the site of the earliest developments of the Neolithic Revolution from around 10,000 BC.\n\nEarly Neolithic farming was limited to a narrow range of plants, both wild and domesticated, which included einkorn wheat, millet and spelt, and to the keeping of dogs, sheep and goats. By about 6900–6400 BC, it included domesticated cattle and pigs, the establishment of permanently or seasonally inhabited settlements, and the use of pottery.\n\nNot all of these cultural elements characteristic of the Neolithic appeared everywhere in the same order: the earliest farming societies in the Near East did not use pottery. In other parts of the world, such as Africa, South Asia and Southeast Asia, independent domestication events led to their own regionally distinctive Neolithic cultures that arose completely independently of those in Europe and Southwest Asia. Early Japanese societies and other East Asian cultures used pottery \"before\" developing agriculture.\n\nIn the Middle East, cultures identified as Neolithic began appearing in the 10th millennium BC. Early development occurred in the Levant (e.g., Pre-Pottery Neolithic A and Pre-Pottery Neolithic B) and from there spread eastwards and westwards. Neolithic cultures are also attested in southeastern Anatolia and northern Mesopotamia by around 8000 BC.\n\nThe prehistoric Beifudi site near Yixian in Hebei Province, China, contains relics of a culture contemporaneous with the Cishan and Xinglongwa cultures of about 6000–5000 BC, neolithic cultures east of the Taihang Mountains, filling in an archaeological gap between the two Northern Chinese cultures. The total excavated area is more than , and the collection of neolithic findings at the site encompasses two phases.\n\nThe Neolithic 1 (PPNA) period began roughly around 10,000 BC in the Levant. A temple area in southeastern Turkey at Göbekli Tepe dated around 9500 BC may be regarded as the beginning of the period. This site was developed by nomadic hunter-gatherer tribes, evidenced by the lack of permanent housing in the vicinity and may be the oldest known human-made place of worship. At least seven stone circles, covering , contain limestone pillars carved with animals, insects, and birds. Stone tools were used by perhaps as many as hundreds of people to create the pillars, which might have supported roofs. Other early PPNA sites dating to around 9500–9000 BC have been found in Jericho, West Bank (notably Ain Mallaha, Nahal Oren, and Kfar HaHoresh), Gilgal in the Jordan Valley, and Byblos, Lebanon. The start of Neolithic 1 overlaps the Tahunian and Heavy Neolithic periods to some degree.\n\nThe major advance of Neolithic 1 was true farming. In the proto-Neolithic Natufian cultures, wild cereals were harvested, and perhaps early seed selection and re-seeding occurred. The grain was ground into flour. Emmer wheat was domesticated, and animals were herded and domesticated (animal husbandry and selective breeding).\n\nIn 2006, remains of figs were discovered in a house in Jericho dated to 9400 BC. The figs are of a mutant variety that cannot be pollinated by insects, and therefore the trees can only reproduce from cuttings. This evidence suggests that figs were the first cultivated crop and mark the invention of the technology of farming. This occurred centuries before the first cultivation of grains.\n\nSettlements became more permanent with circular houses, much like those of the Natufians, with single rooms. However, these houses were for the first time made of mudbrick. The settlement had a surrounding stone wall and perhaps a stone tower (as in Jericho). The wall served as protection from nearby groups, as protection from floods, or to keep animals penned. Some of the enclosures also suggest grain and meat storage.\n\nThe Neolithic 2 (PPNB) began around 8800 BC according to the ASPRO chronology in the Levant (Jericho, Palestine). As with the PPNA dates, there are two versions from the same laboratories noted above. This system of terminology, however, is not convenient for southeast Anatolia and settlements of the middle Anatolia basin. A settlement of 3,000 inhabitants was found in the outskirts of Amman, Jordan. Considered to be one of the largest prehistoric settlements in the Near East, called 'Ain Ghazal, it was continuously inhabited from approximately 7250 BC to approximately 5000 BC.\n\nSettlements have rectangular mud-brick houses where the family lived together in single or multiple rooms. Burial findings suggest an ancestor cult where people preserved skulls of the dead, which were plastered with mud to make facial features. The rest of the corpse could have been left outside the settlement to decay until only the bones were left, then the bones were buried inside the settlement underneath the floor or between houses.\n\nThe Neolithic 3 (PN) began around 6,400 BC in the Fertile Crescent. By then distinctive cultures emerged, with pottery like the Halafian (Turkey, Syria, Northern Mesopotamia) and Ubaid (Southern Mesopotamia). This period has been further divided into PNA (Pottery Neolithic A) and PNB (Pottery Neolithic B) at some sites.\n\nThe Chalcolithic (Stone-Bronze) period began about 4500 BC, then the Bronze Age began about 3500 BC, replacing the Neolithic cultures.\n\nAround 10,000 BC the first fully developed Neolithic cultures belonging to the phase Pre-Pottery Neolithic A (PPNA) appeared in the Fertile Crescent. Around 10,700–9400 BC a settlement was established in Tell Qaramel, north of Aleppo. The settlement included two temples dating to 9650 BC. Around 9000 BC during the PPNA, one of the world's first towns, Jericho, appeared in the Levant. It was surrounded by a stone wall and contained a population of 2,000–3,000 people and a massive stone tower. Around 6400 BC the Halaf culture appeared in Lebanon, Israel and Palestine, Syria, Anatolia, and Northern Mesopotamia and subsisted on dryland agriculture.\n\nIn 1981 a team of researchers from the Maison de l'Orient et de la Méditerranée, including Jacques Cauvin and Oliver Aurenche divided Near East neolithic chronology into ten periods (0 to 9) based on social, economic and cultural characteristics. In 2002 Danielle Stordeur and Frédéric Abbès advanced this system with a division into five periods. \nThey also advanced the idea of a transitional stage between the PPNA and PPNB between 8800 and 8600 BC at sites like Jerf el Ahmar and Tell Aswad.\n\nAlluvial plains (Sumer/Elam). Little rainfall makes irrigation systems necessary. Ubaid culture from 6,900 BC.\n\nDomestication of sheep and goats reached Egypt from the Near East possibly as early as 6000 BC. Graeme Barker states \"The first indisputable evidence for domestic plants and animals in the Nile valley is not until the early fifth millennium BC in northern Egypt and a thousand years later further south, in both cases as part of strategies that still relied heavily on fishing, hunting, and the gathering of wild plants\" and suggests that these subsistence changes were not due to farmers migrating from the Near East but was an indigenous development, with cereals either indigenous or obtained through exchange. Other scholars argue that the primary stimulus for agriculture and domesticated animals (as well as mud-brick architecture and other Neolithic cultural features) in Egypt was from the Middle East.\n\nIn southeast Europe agrarian societies first appeared in the 7th millennium BC, attested by one of the earliest farming sites of Europe, discovered in Vashtëmi, southeastern Albania and dating back to 6500 BC. In Northwest Europe it is much later, typically lasting just under 3,000 years from c. 4500 BC–1700 BC. \n\nAnthropomorphic figurines have been found in the Balkans from 6000 BC, and in Central Europe by around 5800 BC (La Hoguette). Among the earliest cultural complexes of this area are the Sesklo culture in Thessaly, which later expanded in the Balkans giving rise to Starčevo-Körös (Cris), Linearbandkeramik, and Vinča. Through a combination of cultural diffusion and migration of peoples, the Neolithic traditions spread west and northwards to reach northwestern Europe by around 4500 BC. The Vinča culture may have created the earliest system of writing, the Vinča signs, though archaeologist Shan Winn believes they most likely represented pictograms and ideograms rather than a truly developed form of writing. \n\nThe Cucuteni-Trypillian culture built enormous settlements in Romania, Moldova and Ukraine from 5300 to 2300 BC. The megalithic temple complexes of Ġgantija on the Mediterranean island of Gozo (in the Maltese archipelago) and of Mnajdra (Malta) are notable for their gigantic Neolithic structures, the oldest of which date back to around 3600 BC. The Hypogeum of Ħal-Saflieni, Paola, Malta, is a subterranean structure excavated around 2500 BC; originally a sanctuary, it became a necropolis, the only prehistoric underground temple in the world, and showing a degree of artistry in stone sculpture unique in prehistory to the Maltese islands. After 2500 BC, the Maltese Islands were depopulated for several decades until the arrival of a new influx of Bronze Age immigrants, a culture that cremated its dead and introduced smaller megalithic structures called dolmens to Malta. In most cases there are small chambers here, with the cover made of a large slab placed on upright stones. They are claimed to belong to a population certainly different from that which built the previous megalithic temples. It is presumed the population arrived from Sicily because of the similarity of Maltese dolmens to some small constructions found in the largest island of the Mediterranean sea.\n\nThe earliest Neolithic sites in South Asia are Bhirrana in Haryana dated to 7570-6200 BC, and Mehrgarh, dated to between 6500 and 5500 BC, in the Kachi plain of Baluchistan, Pakistan; the site has evidence of farming (wheat and barley) and herding (cattle, sheep and goats).\n\nIn South India, the Neolithic began by 6500 BC and lasted until around 1400 BC when the Megalithic transition period began. South Indian Neolithic is characterized by Ashmounds since 2500 BC in Karnataka region, expanded later to Tamil Nadu.\n\nIn East Asia, the earliest sites include Nanzhuangtou culture around 9500–9000 BC, Pengtoushan culture around 7500–6100 BC, and Peiligang culture around 7000–5000 BC.\n\nThe 'Neolithic' (defined in this paragraph as using polished stone implements) remains a living tradition in small and extremely remote and inaccessible pockets of West Papua (Indonesian New Guinea). Polished stone adze and axes are used in the present day () in areas where the availability of metal implements is limited. This is likely to cease altogether in the next few years as the older generation die off and steel blades and chainsaws prevail.\n\nIn 2012, news was released about a new farming site discovered in Munam-ri, Goseong, Gangwon Province, South Korea, which may be the earliest farmland known to date in east Asia. \"No remains of an agricultural field from the Neolithic period have been found in any East Asian country before, the institute said, adding that the discovery reveals that the history of agricultural cultivation at least began during the period on the Korean Peninsula\". The farm was dated between 3600 and 3000 BC. Pottery, stone projectile points, and possible houses were also found. \"In 2002, researchers discovered prehistoric earthenware, jade earrings, among other items in the area\". The research team will perform accelerator mass spectrometry (AMS) dating to retrieve a more precise date for the site.\n\nIn Mesoamerica, a similar set of events (i.e., crop domestication and sedentary lifestyles) occurred by around 4500 BC, but possibly as early as 11,000–10,000 BC. These cultures are usually not referred to as belonging to the Neolithic; in America different terms are used such as Formative stage instead of mid-late Neolithic, Archaic Era instead of Early Neolithic and Paleo-Indian for the preceding period. The Formative stage is equivalent to the Neolithic Revolution period in Europe, Asia, and Africa. In the southwestern United States it occurred from 500 to 1200 AD when there was a dramatic increase in population and development of large villages supported by agriculture based on dryland farming of maize, and later, beans, squash, and domesticated turkeys. During this period the bow and arrow and ceramic pottery were also introduced.\n\nAustralia, in contrast to New Guinea, has generally been held not to have had a Neolithic period, with a hunter-gatherer lifestyle continuing until the arrival of Europeans. This view can be challenged in terms of the definition of agriculture, but \"Neolithic\" remains a rarely-used and not very useful concept in discussing Australian prehistory.\n\nDuring most of the Neolithic age of Eurasia, people lived in small tribes composed of multiple bands or lineages. There is little scientific evidence of developed social stratification in most Neolithic societies; social stratification is more associated with the later Bronze Age. Although some late Eurasian Neolithic societies formed complex stratified chiefdoms or even states, generally states evolved in Eurasia only with the rise of metallurgy, and most Neolithic societies on the whole were relatively simple and egalitarian. Beyond Eurasia, however, states were formed during the local Neolithic in three areas, namely in the Preceramic Andes with the Norte Chico Civilization, Formative Mesoamerica and Ancient Hawaiʻi. However, most Neolithic societies were noticeably more hierarchical than the Upper Paleolithic cultures that preceded them and hunter-gatherer cultures in general.\n\nThe domestication of large animals (c. 8000 BC) resulted in a dramatic increase in social inequality in most of the areas where it occurred; New Guinea being a notable exception. Possession of livestock allowed competition between households and resulted in inherited inequalities of wealth. Neolithic pastoralists who controlled large herds gradually acquired more livestock, and this made economic inequalities more pronounced. However, evidence of social inequality is still disputed, as settlements such as Catal Huyuk reveal a striking lack of difference in the size of homes and burial sites, suggesting a more egalitarian society with no evidence of the concept of capital, although some homes do appear slightly larger or more elaborately decorated than others.\n\nFamilies and households were still largely independent economically, and the household was probably the center of life. However, excavations in Central Europe have revealed that early Neolithic Linear Ceramic cultures (\"\"Linearbandkeramik\"\") were building large arrangements of circular ditches between 4800 and 4600 BC. These structures (and their later counterparts such as causewayed enclosures, burial mounds, and henge) required considerable time and labour to construct, which suggests that some influential individuals were able to organise and direct human labour — though non-hierarchical and voluntary work remain possibilities.\n\nThere is a large body of evidence for fortified settlements at \"Linearbandkeramik\" sites along the Rhine, as at least some villages were fortified for some time with a palisade and an outer ditch. Settlements with palisades and weapon-traumatized bones, such as those found at the Talheim Death Pit, have been discovered and demonstrate that \"...systematic violence between groups\" and warfare was probably much more common during the Neolithic than in the preceding Paleolithic period. This supplanted an earlier view of the Linear Pottery Culture as living a \"peaceful, unfortified lifestyle\".\n\nControl of labour and inter-group conflict is characteristic of tribal groups with social rank that are headed by a charismatic individual — either a 'big man' or a proto-chief — functioning as a lineage-group head. Whether a non-hierarchical system of organization existed is debatable, and there is no evidence that explicitly suggests that Neolithic societies functioned under any dominating class or individual, as was the case in the chiefdoms of the European Early Bronze Age. Theories to explain the apparent implied egalitarianism of Neolithic (and Paleolithic) societies have arisen, notably the Marxist concept of primitive communism.\n\nThe shelter of the early people changed dramatically from the Upper Paleolithic to the Neolithic era. In the Paleolithic, people did not normally live in permanent constructions. In the Neolithic, mud brick houses started appearing that were coated with plaster. The growth of agriculture made permanent houses possible. Doorways were made on the roof, with ladders positioned both on the inside and outside of the houses. The roof was supported by beams from the inside. The rough ground was covered by platforms, mats, and skins on which residents slept. Stilt-houses settlements were common in the Alpine and Pianura Padana (Terramare) region. Remains have been found at the Ljubljana Marshes in Slovenia and at the Mondsee and Attersee lakes in Upper Austria, for example.\n\nA significant and far-reaching shift in human subsistence and lifestyle was to be brought about in areas where crop farming and cultivation were first developed: the previous reliance on an essentially nomadic hunter-gatherer subsistence technique or pastoral transhumance was at first supplemented, and then increasingly replaced by, a reliance upon the foods produced from cultivated lands. These developments are also believed to have greatly encouraged the growth of settlements, since it may be supposed that the increased need to spend more time and labor in tending crop fields required more localized dwellings. This trend would continue into the Bronze Age, eventually giving rise to permanently settled farming towns, and later cities and states whose larger populations could be sustained by the increased productivity from cultivated lands.\n\nThe profound differences in human interactions and subsistence methods associated with the onset of early agricultural practices in the Neolithic have been called the \"Neolithic Revolution\", a term coined in the 1920s by the Australian archaeologist Vere Gordon Childe.\n\nOne potential benefit of the development and increasing sophistication of farming technology was the possibility of producing surplus crop yields, in other words, food supplies in excess of the immediate needs of the community. Surpluses could be stored for later use, or possibly traded for other necessities or luxuries. Agricultural life afforded securities that nomadic life could not, and sedentary farming populations grew faster than nomadic.\n\nHowever, early farmers were also adversely affected in times of famine, such as may be caused by drought or pests. In instances where agriculture had become the predominant way of life, the sensitivity to these shortages could be particularly acute, affecting agrarian populations to an extent that otherwise may not have been routinely experienced by prior hunter-gatherer communities. Nevertheless, agrarian communities generally proved successful, and their growth and the expansion of territory under cultivation continued.\n\nAnother significant change undergone by many of these newly agrarian communities was one of diet. Pre-agrarian diets varied by region, season, available local plant and animal resources and degree of pastoralism and hunting. Post-agrarian diet was restricted to a limited package of successfully cultivated cereal grains, plants and to a variable extent domesticated animals and animal products. Supplementation of diet by hunting and gathering was to variable degrees precluded by the increase in population above the carrying capacity of the land and a high sedentary local population concentration. In some cultures, there would have been a significant shift toward increased starch and plant protein. The relative nutritional benefits and drawbacks of these dietary changes and their overall impact on early societal development is still debated.\n\nIn addition, increased population density, decreased population mobility, increased continuous proximity to domesticated animals, and continuous occupation of comparatively population-dense sites would have altered sanitation needs and patterns of disease.\n\nThe identifying characteristic of Neolithic technology is the use of polished or ground stone tools, in contrast to the flaked stone tools used during the Paleolithic era.\n\nNeolithic people were skilled farmers, manufacturing a range of tools necessary for the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production (e.g. pottery, bone implements). They were also skilled manufacturers of a range of other types of stone tools and ornaments, including projectile points, beads, and statuettes. But what allowed forest clearance on a large scale was the polished stone axe above all other tools. Together with the adze, fashioning wood for shelter, structures and canoes for example, this enabled them to exploit their newly won farmland.\n\nNeolithic peoples in the Levant, Anatolia, Syria, northern Mesopotamia and Central Asia were also accomplished builders, utilizing mud-brick to construct houses and villages. At Çatalhöyük, houses were plastered and painted with elaborate scenes of humans and animals. In Europe, long houses built from wattle and daub were constructed. Elaborate tombs were built for the dead. These tombs are particularly numerous in Ireland, where there are many thousand still in existence. Neolithic people in the British Isles built long barrows and chamber tombs for their dead and causewayed camps, henges, flint mines and cursus monuments. It was also important to figure out ways of preserving food for future months, such as fashioning relatively airtight containers, and using substances like salt as preservatives.\n\nThe peoples of the Americas and the Pacific mostly retained the Neolithic level of tool technology until the time of European contact. Exceptions include copper hatchets and spearheads in the Great Lakes region.\n\nMost clothing appears to have been made of animal skins, as indicated by finds of large numbers of bone and antler pins that are ideal for fastening leather. Wool cloth and linen might have become available during the later Neolithic, as suggested by finds of perforated stones that (depending on size) may have served as spindle whorls or loom weights. The clothing worn in the Neolithic Age might be similar to that worn by Ötzi the Iceman, although he was not Neolithic (since he belonged to the later Copper age).\n\nNeolithic human settlements include:\n\nThe world's oldest known engineered roadway, the Sweet Track in England, dates from 3800 BC and the world's oldest freestanding structure is the neolithic temple of Ġgantija in Gozo, Malta.\n\n\"Note: Dates are very approximate, and are only given for a rough estimate; consult each culture for specific time periods.\"\n\nEarly Neolithic \n\"Periodization: The Levant: 9500–8000 BC; Europe: 5000–4000 BC; Elsewhere: varies greatly, depending on region.\"\n\nMiddle Neolithic\n\"Periodization: The Levant: 8000–6000 BC; Europe: 4000–3500 BC; Elsewhere: varies greatly, depending on region.\"\n\nLater Neolithic \n\"Periodization: 6500–4500 BC; Europe: 3500–3000 BC; Elsewhere: varies greatly, depending on region.\"\n\n\n\"Periodization: Near East: 4500–3300 BC; Europe: 3000–1700 BC; Elsewhere: varies greatly, depending on region. In the Americas, the Eneolithic ended as late as the 19th century AD for some peoples.\"\n\n"}
{"id": "21190", "url": "https://en.wikipedia.org/wiki?curid=21190", "title": "Nomic", "text": "Nomic\n\nNomic is a game created in 1982 by philosopher Peter Suber in which the rules of the game include mechanisms for the players to change those rules, usually beginning through a system of democratic voting. \n\nThe initial ruleset was designed by Peter Suber, but first published in Douglas Hofstadter's column \"Metamagical Themas\" in \"Scientific American\" in June 1982. The column discussed Suber's then-upcoming book, \"The Paradox of Self-Amendment\", which was published some years later. Nomic now refers to a large number of games, all based on the initial ruleset.\n\nThe game is in some ways modeled on modern government systems. It demonstrates that in any system where rule changes are possible, a situation may arise in which the resulting laws are contradictory or insufficient to determine what is in fact legal. Because the game models (and exposes conceptual questions about) a legal system and the problems of legal interpretation, it is named after (\"\"), Greek for \"law\".\n\nWhile the victory condition in Suber's initial ruleset is the accumulation of 100 points by the roll of dice, he once said that \"this rule is deliberately boring so that players will quickly amend it to please themselves\". Players can change the rules to such a degree that points can become irrelevant in favor of a true currency, or make victory an unimportant concern. Any rule in the game, including the rules specifying the criteria for winning and even the rule that rules must be obeyed, can be changed. Any loophole in the ruleset, however, may allow the first player to discover it the chance to pull a \"scam\" and modify the rules to win the game. Complicating this process is the fact that Suber's initial ruleset allows for the appointment of judges to preside over issues of rule interpretation.\n\nThe game can be played face-to-face with as many written notes as are required, or through any of a number of Internet media (usually an archived mailing list or Internet forum).\n\nInitially, gameplay occurs in clockwise order, with each player taking a turn. In that turn, they propose a change in rules that all the other players vote on, and then roll a die to determine the number of points they add to their score. If this rule change is passed, it comes into effect at the end of their round. Any rule can be changed with varying degrees of difficulty, including the core rules of the game itself. As such, the gameplay may quickly change.\n\nUnder Suber's initial ruleset, rules are divided into two types: mutable and immutable. The main difference between these is that immutable rules must be changed into mutable rules (called \"transmuting\") before they can be modified or removed. Immutable rules also take precedence over mutable ones. A rule change may be:\n\n\nAlternative starting rulesets exist for Internet and mail games, wherein gameplay occurs in alphabetical order by surname, and points added to the score are based on the success of a proposed rule change rather than random dice rolls.\n\nNot only can every aspect of the rules be altered in some way over the course of a game of Nomic, but myriad variants also exist: some that have themes, begin with a single rule, or begin with a dictator instead of a democratic process to validate rules. Others combine Nomic with an existing game (such as Monopoly, chess, or in one humorously paradoxical attempt, Mornington Crescent). There is even a version in which the players are games of Nomic themselves. Even more unusual variants include a ruleset in which the rules are hidden from players' view, and a game which, instead of allowing voting on rules, splits into two sub-games, one with the rule, and one without it.\n\nOnline versions often have initial rulesets where play is not turn-based; typically, players in such games may propose rule changes at any time, rather than having to wait for their turn.\n\nOne spin-off of a now-defunct Nomic (Nomic World) is called the Fantasy Rules Committee; it adds every legal rule submitted by a player to the ruleset until the players run out of ideas, after which all the \"fantasy rules\" are repealed and the game begins again.\n\nThe game of Nomic is particularly suited to being played online, where all proposals and rules can be shared in web pages or email archives for ease of reference. Such games of Nomic sometimes last for a very long time – Agora has been running since 1993. The longevity of nomic games can pose a serious problem, in that the rulesets can grow so complex that current players do not fully understand them and prospective players are deterred from joining. One currently active game, BlogNomic, gets around this problem by dividing the game into \"dynasties\"; every time someone wins, a new dynasty begins, and all the rules except a privileged few are repealed. This keeps the game relatively simple and accessible. Nomicron (now defunct) was similar in that it had rounds – when a player won a round, a convention was started to plan for the next round. A game of Nomic on reddit, nommit (now defunct), uses a similar mechanism modeled on Nomicron's system.\n\nAnother facet of Nomic is the way in which the implementation of the rules affects the way the game of Nomic itself works. ThermodyNomic, for example, had a ruleset in which rule changes were carefully considered before implementation, and rules were rarely introduced which provide loopholes for the players to exploit. B Nomic, by contrast, was once described by one of its players as \"the equivalent of throwing logical hand grenades\".\n\nThis is essentially part of the differentiation between \"procedural\" games, where the aim (acknowledged or otherwise) is to tie the entire ruleset into a paradoxical condition during each turn (a player who has no legal move available wins), and \"substantive\" games, which try to avoid paradox and reward winning by achieving certain goals, such as attaining a given number of points.\n\nWhile \"Nomic\" is traditionally capitalized as the proper name of the game it describes, it has also sometimes been used in a more informal way as a lowercased generic term, \"nomic\", referring to anything with Nomic-like characteristics, including games where the rules may be changed during play as well as non-gaming situations where it can be alleged that \"rules lawyers\" are tinkering with the process used to amend rules and policies (in an organization or community) in a manner akin to a game of Nomic.\n\nIn a computerized Nomic, the rules are interpreted by a computer, rather than by humans. This implies that the rules should be written in a language that a computer can understand, typically some sort of programming language or Game Description Language. Nomyx (now defunct) is such an implementation.\n\n"}
{"id": "21197", "url": "https://en.wikipedia.org/wiki?curid=21197", "title": "Nintendo", "text": "Nintendo\n\nNintendo Co., Ltd. is a Japanese multinational consumer electronics and video game company headquartered in Kyoto. Nintendo is one of the world's largest video game companies by market capitalization, creating some of the best-known and top-selling video game franchises, such as \"Mario\", \"The Legend of Zelda\", and \"Pokémon\".\n\nFounded on 23 September 1889 by Fusajiro Yamauchi, it originally produced handmade hanafuda playing cards. By 1963, the company had tried several small niche businesses, such as cab services and love hotels. Abandoning previous ventures in favor of toys in the 1960s, Nintendo developed into a video game company in the 1970s, ultimately becoming one of the most influential in the industry and Japan's third most-valuable company with a market value of over $85 billion.\n\nIn January 2019 Nintendo announced it reach $958 million of profit with $5.59 billion of revenue.\n\nNintendo was founded as a playing card company by Fusajiro Yamauchi on 23 September 1889. Based in Kyoto, the business produced and marketed \"Hanafuda\" cards. The handmade cards soon became popular, and Yamauchi hired assistants to mass-produce cards to satisfy demand. In 1949, the company adopted the name Nintendo Karuta Co., Ltd., doing business as The Nintendo Playing Card Co. outside Japan. Nintendo continues to manufacture playing cards in Japan and organizes its own contract bridge tournament called the \"Nintendo Cup\". The word \"Nintendo\" can be translated as \"leave luck to heaven\", or alternatively as \"the temple of free hanafuda\".\n\nIn 1956, Hiroshi Yamauchi, grandson of Fusajiro Yamauchi, visited the U.S. to talk with the United States Playing Card Company, the dominant playing card manufacturer there. He found that the biggest playing card company in the world was using only a small office. Yamauchi's realization that the playing card business had limited potential was a turning point. He then acquired the license to use Disney characters on playing cards to drive sales.\n\nIn 1963, Yamauchi renamed Nintendo Playing Card Co. Ltd. to Nintendo Co., Ltd. The company then began to experiment in other areas of business using newly injected capital during the period of time between 1963 and 1968. Nintendo set up a taxi company called \"Daiya\". This business was initially successful. However, Nintendo was forced to sell it because problems with the labour unions were making it too expensive to run the service. It also set up a love hotel chain, a TV network, a food company (selling instant rice) and several other ventures. All of these ventures eventually failed, and after the 1964 Tokyo Olympics, playing card sales dropped, and Nintendo's stock price plummeted to its lowest recorded level of ¥60.\n\nIn 1966, Nintendo moved into the Japanese toy industry with the Ultra Hand, an extendable arm developed by its maintenance engineer Gunpei Yokoi in his free time. Yokoi was moved from maintenance to the new \"Nintendo Games\" department as a product developer. Nintendo continued to produce popular toys, including the Ultra Machine, Love Tester and the \"Kousenjuu\" series of light gun games. Despite some successful products, Nintendo struggled to meet the fast development and manufacturing turnaround required in the toy market, and fell behind the well-established companies such as Bandai and Tomy. In 1973, its focus shifted to family entertainment venues with the Laser Clay Shooting System, using the same light gun technology used in Nintendo's \"Kousenjuu\" series of toys, and set up in abandoned bowling alleys. Following some success, Nintendo developed several more light gun machines (such as the light gun shooter game \"Wild Gunman\") for the emerging arcade scene. While the Laser Clay Shooting System ranges had to be shut down following excessive costs, Nintendo had found a new market.\n\nNintendo's first venture into the video gaming industry was securing rights to distribute the Magnavox Odyssey video game console in Japan in 1974. Nintendo began to produce its own hardware in 1977, with the Color TV-Game home video game consoles. Four versions of these consoles were produced, each including variations of a single game (for example, Color TV Game 6 featured six versions of \"Light Tennis\").\n\nA student product developer named Shigeru Miyamoto was hired by Nintendo at this time. He worked for Yokoi, and one of his first tasks was to design the casing for several of the Color TV-Game consoles. Miyamoto went on to create, direct and produce some of Nintendo's most famous video games and become one of the most recognizable figures in the video game industry.\n\nIn 1975, Nintendo moved into the video arcade game industry with \"EVR Race\", designed by their first game designer, Genyo Takeda, and several more games followed. Nintendo had some small success with this venture, but the release of \"Donkey Kong\" in 1981, designed by Miyamoto, changed Nintendo's fortunes dramatically. The success of the game and many licensing opportunities (such as ports on the Atari 2600, Intellivision and ColecoVision) gave Nintendo a huge boost in profit and in addition, the game also introduced an early iteration of Mario, then known in Japan as Jumpman, the eventual company mascot.\n\nIn 1979, Gunpei Yokoi conceived the idea of a handheld video game, while observing a fellow bullet train commuter who passed the time by interacting idly with a portable LCD calculator, which gave birth to \"Game & Watch\". In 1980, Nintendo launched \"Game & Watch\"—a handheld video game series developed by Yokoi. These systems do not contain interchangeable cartridges and thus the hardware was tied to the game. The first Game & Watch game, \"Ball\", was distributed worldwide. The modern \"cross\" D-pad design was developed in 1982, by Yokoi for a \"Donkey Kong\" version. Proven to be popular, the design was patented by Nintendo. It later earned a Technology & Engineering Emmy Award.\n\nIn 1983, Nintendo launched the Family Computer (colloquialized as \"Famicom\") home video game console in Japan, alongside ports of its most popular arcade games. In 1985, a cosmetically reworked version of the system known outside Japan as the Nintendo Entertainment System or NES, launched in North America. The practice of bundling the system along with select games helped to make \"Super Mario Bros.\" one of the best-selling video games in history.\n\nIn 1988, Gunpei Yokoi and his team at Nintendo R&D1 conceived the new Game Boy handheld system, with the purpose of merging the two very successful ideas of the Game & Watch's portability along with the NES's cartridge interchangeability. Nintendo released the Game Boy in Japan on 21 April 1989, and in North America on 31 July 1989. Nintendo of America president Minoru Arakawa managed a deal to bundle the popular third-party game \"Tetris\" along with the Game Boy, and the pair launched as an instant success.\n\nIn 1989, Nintendo announced plans to release the successor to the Famicom, the Super Famicom. Based on a 16-bit processor, Nintendo boasted significantly superior hardware specifications of graphics, sound, and game speed over the original 8-bit Famicom. The Super Famicom was finally released relatively late to the market in Japan on 21 November 1990, and released as the Super Nintendo Entertainment System (officially abbreviated the Super NES or SNES and commonly shortened to Super Nintendo) in North America on 23 August 1991 and in Europe in 1992. Its main rival was the 16-bit Mega Drive, known in North America as Genesis, which had been advertised aggressively against the nascent 8-bit NES. A console war between Sega and Nintendo ensued during the early 1990s. From 1990 to 1992, Nintendo opened \"World of Nintendo\" shops in the United States where consumers could test and buy Nintendo products.\n\nIn August 1993, Nintendo announced the SNES's successor, codenamed \"Project Reality\". Featuring 64-bit graphics, the new system was developed as a joint venture between Nintendo and North-American-based technology company Silicon Graphics. The system was announced to be released by the end of 1995, but was subsequently delayed. Meanwhile, Nintendo continued the Nintendo Entertainment System family with the release of the NES-101, a smaller redesign of the original NES. Nintendo also announced a CD drive peripheral called the Super NES CD-ROM Adapter, which was co-developed first by Sony with the name \"Play Station\" and then by Philips. Bearing prototypes and joint announcements at the Consumer Electronics Show, it was on track for a 1994 release, but was controversially cancelled.\n\nIn 1995, Nintendo announced that it had sold one billion game cartridges worldwide, ten percent of those being from the \"Mario\" franchise. Nintendo deemed 1994 the \"Year of the Cartridge\". To further their support for cartridges, Nintendo announced that Project Reality, which had now been renamed the Ultra 64, would not use a CD format as expected, but would rather use cartridges as its primary media format. Nintendo IRD general manager Genyo Takeda was impressed by video game development company Rare's progress with real-time 3D graphics technology, using state of the art Silicon Graphics workstations. As a result, Nintendo bought a 25% stake in the company, eventually expanding to 49%, and offered their catalogue of characters to create a CGI game around, making Rare Nintendo's first western-based second-party developer. Their first game as partners with Nintendo was \"Donkey Kong Country\". The game was a critical success and sold over eight million copies worldwide, making it the second best-selling game in the SNES library. In September 1994, Nintendo, along with six other video game giants including Sega, Electronic Arts, Atari, Acclaim, Philips, and 3DO approached the United States Senate and demanded a ratings system for video games to be enforced, which prompted the decision to create the Entertainment Software Rating Board.\n\nAiming to produce an affordable virtual reality console, Nintendo released the Virtual Boy in 1995, designed by Gunpei Yokoi. The console consists of a head-mounted semi-portable system with one red-colored screen for each of the user's eyes, featuring stereoscopic graphics. Games are viewed through a binocular eyepiece and controlled using an affixed gamepad. Critics were generally disappointed with the quality of the games and the red-colored graphics, and complained of gameplay-induced headaches. The system sold poorly and was quietly discontinued. Amid the system's failure, Yokoi retired from Nintendo. During the same year, Nintendo launched the Satellaview in Japan, a peripheral for the Super Famicom. The accessory allowed users to play video games via broadcast for a set period of time. Various games were made exclusively for the platform, as well as various remakes.\n\nIn 1996, Nintendo released the Ultra 64 as the Nintendo 64 in Japan and North America. The console was later released in Europe and Australia in 1997. The Nintendo 64 continued what had become a Nintendo tradition of hardware design which is focused less on high performance specifications than on design innovations intended to inspire game development. With its market shares slipping to the Sega Saturn and partner-turned-rival Sony PlayStation, Nintendo revitalized its brand by launching a $185 million marketing campaign centered around the \"Play it Loud\" slogan. During the same year, Nintendo also released the Game Boy Pocket in Japan, a smaller version of the Game Boy that generated more sales for the platform. On 4 October 1997, famed Nintendo developer Gunpei Yokoi died in a car crash. In 1997, Nintendo released the SNS-101 (called Super Famicom Jr. in Japan), a smaller redesigned version of the Super Nintendo Entertainment System.\n\nIn 1998, the successor to the Game Boy, the Game Boy Color, was released. The system had improved technical specifications allowing it to run games made specifically for the system as well as games released for the Game Boy, albeit with added color. The Game Boy Camera and Printer were also released as accessories. In October 1998, Retro Studios was founded as an alliance between Nintendo and former Iguana Entertainment founder Jeff Spangenberg. Nintendo saw an opportunity for the new studio to create games for the upcoming GameCube targeting an older demographic, in the same vein as Iguana Entertainment's successful \"\" series for the Nintendo 64.\n\nIn 2001, Nintendo introduced the redesigned Game Boy Advance. The same year, Nintendo also released the GameCube to lukewarm sales, and it ultimately failed to regain the market share lost by the Nintendo 64. When Yamauchi, company president since 1949, retired on 24 May 2002, Satoru Iwata became first Nintendo president who was unrelated to the Yamauchi family through blood or marriage since its founding in 1889.\n\nIn 2003, Nintendo released the Game Boy Advance SP, a redesign of the Game Boy Advance that featured a clamshell design that would later be used in Nintendo's DS and 3DS handheld video game systems.\n\nIn 2004, Nintendo released the Nintendo DS, its fourth major handheld system. The DS is a dual screened handheld featuring touch screen capabilities, which respond to either a stylus or the touch of a finger. Former Nintendo president and now chairman Hiroshi Yamauchi was translated by GameScience as explaining, \"If we can increase the scope of the industry, we can re-energise the global market and lift Japan out of depression – that is Nintendo's mission.\" Regarding lukewarm GameCube sales which had yielded the company's first reported operating loss in over 100 years, Yamauchi continued: \"The DS represents a critical moment for Nintendo's success over the next two years. If it succeeds, we rise to the heavens, if it fails, we sink into hell.\" Thanks to games such as Nintendogs and Mario Kart DS, the DS became a success. In 2005, Nintendo released the Game Boy Micro in North America, a redesign of the Game Boy Advance. The last system in the Game Boy line, it was also the smallest Game Boy, and the least successful. In the middle of 2005, Nintendo opened the Nintendo World Store in New York City, which would sell Nintendo games, present a museum of Nintendo history, and host public parties such as for product launches. The store was renovated and renamed as Nintendo New York in 2016.\nIn the first half of 2006, Nintendo released the Nintendo DS Lite, a version of the original Nintendo DS with lighter weight, brighter screen, and better battery life. In addition to this streamlined design, its prolific subset of casual games appealed to the masses, such as the \"Brain Age\" series. Meanwhile, \"New Super Mario Bros.\" provided a substantial addition to the \"Mario\" series when it was launched to the top of sales charts. The successful direction of the Nintendo DS had a big influence on Nintendo's next home console (including the common Nintendo Wi-Fi Connection), which had been codenamed \"Revolution\" and was now renamed to \"Wii\". In August 2006, Nintendo published ES, a now-dormant, open source research operating system project designed around web application integration but for no specific purpose.\n\nIn the latter half of 2006, Nintendo released the Wii as the backward-compatible successor to the GameCube. Based upon intricate Wii Remote motion controls and a balance board, the Wii inspired several new game franchises, some targeted at entirely new market segments of casual and fitness gaming. Selling more than 100 million worldwide, the Wii was the best selling console of the seventh generation, regaining market share lost during the tenures of the Nintendo 64 and GameCube.\nOn 1 May 2007, Nintendo acquired an 80% stake on video game development company Monolith Soft, previously owned by Bandai Namco. Monolith Soft is best known for developing role-playing games such as the Xenosaga and Baten Kaitos series.\n\nDuring the holiday season of 2008, Nintendo followed up the success of the DS with the release of the Nintendo DSi in Japan. The system features a more powerful CPU and more RAM, two cameras, one facing towards the player and one facing outwards, and had an online distribution store called DSiWare. The DSi was later released worldwide during 2009. In the latter half of 2009, Nintendo released the Nintendo DSi XL in Japan, a larger version of the DSi. This updated system was later released worldwide in 2010.\n\nIn 2011, Nintendo released the Nintendo 3DS, based upon a glasses-free stereoscopic 3D display. In February 2012, Nintendo acquired Mobiclip, a France-based research and development company specialized in highly optimized software technologies such as video compression. The company's name was later changed to Nintendo European Research & Development. During the fourth quarter of 2012, Nintendo released the Wii U. It sold slower than expected, despite being the first eighth generation console. By September 2013, however, sales had rebounded. Intending to broaden the 3DS market, Nintendo released 2013's cost-reduced Nintendo 2DS. The 2DS is compatible with but lacks the 3DS's more expensive but cosmetic autostereoscopic 3D feature. Nintendo also released the Wii Mini, a cheaper and non-networked redesign of the Wii.\n\nOn 25 September 2013, Nintendo announced it had purchased a 28% stake in a Panasonic spin-off company called PUX Corporation. The company specializes in face and voice recognition technology, with which Nintendo intends to improve the usability of future game systems. Nintendo has also worked with this company in the past to create character recognition software for a Nintendo DS touchscreen. After announcing a 30% dive in profits for the April to December 2013 period, president Satoru Iwata announced he would take a 50% pay-cut, with other executives seeing reductions by 20%–30%.\n\nIn January 2015, Nintendo announced its exit from the Brazilian market after four years of distributing products in the country. Nintendo cited high import duties and lack of local manufacturing operation as reasons for leaving. Nintendo continues its partnership with Juegos de Video Latinoamérica to distribute products to the rest of Latin America.\n\nOn 11 July 2015, Iwata died from a bile duct tumor at the age of 55. Following his death, representative directors Genyo Takeda and Shigeru Miyamoto jointly led the company on an interim basis until the appointment of Tatsumi Kimishima as Iwata's successor on 16 September 2015. In addition to Kimishima's appointment, the company's management organization was also restructured—Miyamoto was named \"Creative Fellow\" and Takeda was named \"Technology Fellow\".\n\nOn 17 March 2015, Nintendo announced a partnership with Japanese mobile developer DeNA to produce games for smart devices. The first of these, \"Miitomo\", was released in March 2016.\n\nOn the same day, Nintendo announced a new \"dedicated games platform with a brand new concept\" with the codename \"NX\" that would be further revealed in 2016. Reggie Fils-Aimé, president of Nintendo of America, referred to NX as \"our next home console\" in a June 2015 interview with \"The Wall Street Journal\". In a later article from October 2015, \"The Wall Street Journal\" relayed speculation from unnamed inside sources that the NX was intended to feature \"industry leading\" hardware specifications and be usable as both a home and portable console. It was also reported that Nintendo had begun distributing software development kits (SDKs) for it to third-party developers, with the unnamed source further speculating that these moves suggested that the company was on track to introduce it as early as 2016. At an investor's meeting on 27 April 2016, Nintendo announced that the NX would be released worldwide in March 2017. In an interview with \"Asahi Shimbun\" in May 2016, Kimishima stated that the NX was a new concept that would not succeed the 3DS or Wii U product lines. At a shareholders' meeting following E3 2016, Shigeru Miyamoto stated that the company chose not to present the NX during the conference due to concerns that competitors could copy from it if they revealed it too soon. The same day, Kimishima also revealed during a Q&A session with investors that they were also researching virtual reality.\n\nIn May 2015, Universal Parks & Resorts announced that it was partnering with Nintendo to create attractions at Universal theme parks based upon Nintendo properties. In May 2016, Nintendo also expressed a desire to enter the animated film market. In November 2016, it was stated that the area to be created at Universal theme parks is known as Super Nintendo World, which will be completed by 2020 at Universal Studios Japan in time of the 2020 Tokyo Olympics, whereas Universal Orlando Resort and Universal Studios Hollywood will get the themed area in an unspecified date after the Japanese version.\n\nIn July 2016, the company announced it was bringing back the NES in the form of the NES Classic Edition (called Nintendo Classic Mini in Europe). The plug-and-play console will support HDMI, two-player modes, and have a controller similar to the original NES controller. The controller would be able to connect to a Wii Remote for use with Wii and Wii U Virtual Console games. The NES Classic Edition came with 30 games pre-installed, including \"Final Fantasy\", \"Kid Icarus\", \"The Legend of Zelda\", \"\", and \"Dr. Mario\", among others. It was released in November 2016. Additional controllers were also available.\n\nThe July 2016 release of the \"Pokémon Go\" mobile app by Niantic caused shares in Nintendo to double, due to investor misunderstanding that the software was the property of Nintendo. Later that month, Nintendo released a statement clarifying its relation with Niantic, Nintendo stated it owned 32% of Pokémon intellectual property owner The Pokémon Company, and though it would receive some licensing and other revenues from the game it expected the impact on Nintendo's total income to be limited. As a result of the statement Nintendo's share price fell substantially, losing 17% in one day of trading. After a reduction in shareprice from the \"Pokémon Go\" peak, the company was still valued at over 100 times its net income, a price–earnings ratio greatly exceeding the average on the Nikkei 225. Analysts speaking to Bloomberg L.P. and the \"Financial Times\" both commented on the potential future value of Nintendo's IP if transferred to the mobile phone game business.\n\nIn August 2016, Nintendo of America sold 90% of its controlling stake (55%) in the Seattle Mariners to a group of investors led by mobile phone businessman John Stanton for $640 million.\n\nAfter the announcement of the mobile game \"Super Mario Run\" in September 2016, Nintendo's stock soared to just under its recent high point after the release and success of \"Pokémon Go\" earlier in the year, something noted by journalists as even more significant than \"Pokémon Go\", as \"Super Mario Run\" was developed in-house by Nintendo, which was not the case with \"Pokémon Go\". In a December 2016 interview prior to the release of \"Super Mario Run\", Miyamoto explained that the company believed that with some of their game franchises, \"the longer you continue to make a series, the more complex the gameplay becomes, and the harder it becomes for new players to be able to get into the series\", and that the company sees mobile games with simplified controls, such as \"Super Mario Run\", not only allows them to \"make a game that the broadest audience of people could play\", but to also reintroduce these properties to newer audiences and draw them to their consoles.\n\nOn 20 October 2016, Nintendo released a preview trailer about the NX, revealing the official name to be the Nintendo Switch. According to Fils-Aimé, the console gave game developers new abilities to bring their creative concepts to life by opening up the concept of gaming without limits. In December 2016, Nintendo released \"Super Mario Run\" for iOS devices, with the game surpassing over 50 million downloads within a week of its release. Kimishima stated that Nintendo would release a couple of mobile games each year from then on.\n\nIn September 2017, Nintendo announced a partnership with the Chinese gaming company Tencent to publish a global version of their commercially successful mobile game, \"Honor of Kings\", for the Nintendo Switch. The announcement lead some to believe that Nintendo could soon have a bigger footprint in China, a region where the Switch is not sold and is largely dominated by Tencent. In November 2017, it was reported that Nintendo would be teaming up with Illumination Entertainment, an animation division of Universal Pictures, to make an animated \"Mario\" film. In April 2018, Nintendo announced that Kimishima would be stepping down as company president that June, with Shuntaro Furukawa, former managing executive officer and outside director of The Pokémon Company, succeeding him.\n\nReleased in 1977, Japan's highest selling first generation console is Nintendo's Color TV Game, with over three million units sold.\n\nThe Nintendo Entertainment System (abbreviated as NES) is an 8-bit video game console, which released in North America in 1985, and in Europe throughout 1986 and 1987. The console was initially released in Japan as the Family Computer (abbreviated as Famicom) in 1983. The best-selling gaming console of its time, the NES helped revitalise the US video game industry following the video game crash of 1983. With the NES, Nintendo introduced a now-standard business model of licensing third-party developers, authorizing them to produce and distribute games for Nintendo's platform. The NES was bundled with \"Super Mario Bros.\", one of the best-selling video games of all time, and received ports of Nintendo's most popular arcade games.\n\nNintendo also produced a limited run of the NES Classic Edition in 2016. The NES Classic System was a dedicated console modeled after an NES with 30 built-in classic first- and third-party games from the NES library. By the end of its production in April 2017, Nintendo shipped over two million units.\n\nThe Super Nintendo Entertainment System (abbreviated as the Super NES or SNES) is a 16-bit video game console, which was released in North America in 1991, and in Europe in 1992. The console was initially released in Japan in 1990 as the Super Famicom, officially adopting the colloquially abbreviated name of its predecessor. The console introduced advanced graphics and sound capabilities compared with other consoles at the time. Soon, the development of a variety of enhancement chips which were integrated onto each new game cartridge's circuit boards, progressed the SNES's competitive edge. While even crude three-dimensional graphics had previously rarely been seen on home consoles, the Super NES's enhancement chips suddenly enabled a new caliber of games containing increasingly sophisticated faux 3D effects as seen in 1991's \"Pilotwings\" and 1992's \"Super Mario Kart\". Argonaut Games developed the Super FX chip in order to replicate 3D graphics from their earlier Atari ST and Amiga \"Starglider\" series on the Super NES (more specifically, \"Starglider 2\"), starting with \"Star Fox\" in 1993. The SNES is the best-selling console of the 16-bit era although having experienced a relatively late start and fierce competition from Sega's Mega Drive/Genesis console.\n\nNintendo also released a limited run of the Super NES Classic Edition in September 2017 through the end of the year. Like the NES Classic Edition, the Super NES Classic Edition is a dedicated console with 21 built-in games from its library, including the never-before-released \"Starfox 2\".\n\nThe Nintendo 64 was released in 1996, featuring 3D polygon model rendering capabilities and built-in multiplayer for up to four players. The system's controller introduced the analog stick and later introduced the Rumble Pak, an accessory for the controller that produces force feedback with compatible games. Both are the first such features to have come to market for home console gaming and eventually became the \"de facto\" industry standard. Announced in 1995, prior to the console's 1996 launch, the 64DD (\"DD\" standing for \"Disk Drive\") was designed to enable the development of new genre of video games by way of 64 MB writable magnetic disks, video editing, and Internet connectivity. Eventually released only in Japan in 1999, the 64DD peripheral's commercial failure there resulted in only nine games being released and precluded further worldwide release.\n\nThe GameCube (officially called Nintendo GameCube, abbreviated NGC in Japan and GCN in North America) was released in 2001, in Japan and North America, and in 2002 worldwide. The sixth-generation console is the successor to the Nintendo 64 and competed with Sony's PlayStation 2, Microsoft's Xbox, and Sega's Dreamcast. The GameCube is the first Nintendo console to use optical discs as its primary storage medium. The discs are similar to the miniDVD format, but the system was not designed to play standard DVDs or audio CDs. Nintendo introduced a variety of connectivity options for the GameCube. The GameCube's game library has sparse support for Internet gaming, a feature that requires the use of the aftermarket Nintendo GameCube Broadband Adapter and Modem Adapter. The GameCube supports connectivity to the Game Boy Advance, allowing players to access exclusive in-game features using the handheld as a second screen and controller.\n\nThe Wii was released during the holiday season of 2006 worldwide. The system features the Wii Remote controller, which can be used as a handheld pointing device and which detects movement in three dimensions. Another notable feature of the console is WiiConnect24, which enables it to receive messages and updates over the Internet while in standby mode. It also features a game download service, called \"Virtual Console\", which features emulated games from past systems. Since its release, the Wii has spawned many peripheral devices, including the Wii Balance Board and Motion Plus, and has had several hardware revisions. The \"Wii Family Edition\" variant is identical to the original model, but is designed to sit horizontally and removes the GameCube compatibility. The \"Wii Mini\" is a smaller, redesigned Wii which lacks GameCube compatibility, online connectivity, the SD card slot and Wi-Fi support, and has only one USB port unlike the previous models' two.\n\nThe Wii U, the successor to the Wii, was released during the holiday season of 2012 worldwide. The Wii U is the first Nintendo console to support high-definition graphics. The Wii U's primary controller is the Wii U GamePad, which features an embedded touchscreen. Each game may be designed to use this touchscreen as supplemental to the main TV, or as the only screen for Off-TV Play. The system supports most Wii controllers and accessories, and the more classically shaped Wii U Pro Controller. The system is backward compatible with Wii software and accessories; this mode also utilizes Wii-based controllers, and it optionally offers the GamePad as its primary Wii display and motion sensor bar. The console has various online services powered by Nintendo Network, including: the Nintendo eShop for online distribution of software and content; and Miiverse, a social network which can be variously integrated with games and applications. As of 31 March 2018, worldwide Wii U sales had totalled over 13 million units, with over 100 million games and other software for it sold.\n\nOn 17 March 2015, Nintendo announced a new \"dedicated games platform with a brand new concept\" with the codename \"NX\" that would be further revealed in 2016. Reggie Fils-Aimé, president of Nintendo of America, referred to NX as \"our next home console\" in a June 2015 interview with \"The Wall Street Journal\". In a later article on 16 October 2015, \"The Wall Street Journal\" relayed speculation from unnamed inside sources that, although the NX hardware specifications were unknown, it may be intended to feature \"industry leading\" hardware specifications and include both a console and a mobile unit that could either be used with the console or taken on the road for separate use. It was also reported that Nintendo had begun distributing software development kits (SDKs) for NX to third-party developers, with the unnamed source further speculating that these moves \"[suggest that] the company is on track to introduce [NX] as early as [2016].\" At an investor's meeting on 27 April 2016, Nintendo announced that the NX would be released worldwide in March 2017. In an interview with Asahi Shimbun in May 2016, Kimishima referred to the NX as \"neither the successor to the Wii U nor to the 3DS\", as well as it being a \"new way of playing games,\" but it would \"slow Wii U sales\" upon reveal and dissemination. In June 2016, Miyamoto stated that the reason Nintendo had not released any information on the \"NX\" up until that point was because they were afraid of imitators, saying he and Nintendo thought other companies could copy \"an idea that [they're] working on.\" The same day, Kimishima revealed during a Q&A session with investors that they were also researching virtual reality. On 19 October 2016, Nintendo announced they would release a trailer for the console the following day. The next day, Nintendo unveiled the trailer that revealed the final name of the platform called Nintendo Switch. By March 2018, over 17 million Switch units had been sold worldwide.\n\nGame & Watch is a line of handheld electronic games produced by Nintendo from 1980 to 1991. Created by game designer Gunpei Yokoi, each \"Game & Watch\" features a single game to be played on an LCD screen in addition to a clock, an alarm, or both. It was the earliest Nintendo product to garner major success.\n\nAfter the success of the \"Game & Watch\" series, Yokoi developed the Game Boy handheld console, which was released in 1989. Eventually becoming the best-selling handheld of all time, the Game Boy remained dominant for more than a decade, seeing critically and commercially popular games such as \"Pokémon Yellow\" released as late as 1998 in Japan, 1999 in North America, and 2000 in Europe. Incremental updates of the Game Boy, including \"Game Boy Pocket\", \"Game Boy Light\" and \"Game Boy Color\", did little to change the original formula, though the latter introduced color graphics to the Game Boy line.\n\nThe first major update to its handheld line since 1989, the Game Boy Advance features improved technical specifications similar to those of the SNES. The \"Game Boy Advance SP\" was the first revision to the GBA line and introduced screen lighting and a clam shell design, while later iteration, the \"Game Boy Micro\", brought a smaller form factor.\n\nAlthough originally advertised as an alternative to the Game Boy Advance, the Nintendo DS replaced the Game Boy line after its initial release in 2004. It was distinctive for its dual screens and a microphone, as well as a touch-sensitive lower screen. The \"Nintendo DS Lite\" brought a smaller form factor while the \"Nintendo DSi\" features larger screens and two cameras, and was followed by an even larger model, the \"Nintendo DSi XL\", with a 90% bigger screen.\n\nFurther expanding the Nintendo DS line, the Nintendo 3DS uses the process of autostereoscopy to produce a stereoscopic three-dimensional effect without glasses. Released to major markets during 2011, the 3DS got off to a slow start, initially missing many key features that were promised before the system launched. Partially as a result of slow sales, Nintendo stock declined in value. Subsequent price cuts and game releases helped to boost 3DS and 3DS software sales and to renew investor confidence in the company. As of August 2013, the 3DS was the best selling console in the United States for four consecutive months. The \"Nintendo 3DS XL\" was introduced in August 2012 and includes a 90% larger screen, a 4 GB SD card and extended battery life. In August 2013, Nintendo announced the cost-reduced \"Nintendo 2DS\", a version of the 3DS without the 3D display. It has a slate-like design as opposed to the hinged, clamshell design of its predecessors.\n\nA hardware revision, \"New Nintendo 3DS\", was unveiled in August 2014. It is produced in a standard-sized model and a larger XL model; both models feature upgraded processors and additional RAM, an eye-tracking sensor to improve the stability of the autostereoscopic 3D image, colored face buttons, and near-field communication support for native use of Amiibo products. The standard-sized model also features slightly larger screens, and support for faceplate accessories.\n\nNintendo of America has engaged in several high-profile marketing campaigns to define and position its brand. One of its earliest and most enduring slogans was \"Now you're playing with power!\", used first to promote its Nintendo Entertainment System. It modified the slogan to include \"SUPER power\" for the Super Nintendo Entertainment System, and \"PORTABLE power\" for the Game Boy. Its 1994 \"Play It Loud!\" campaign played upon teenage rebellion and fostered an edgy reputation. During the Nintendo 64 era, the slogan was \"Get N or get out.\" During the GameCube era, the \"Who Are You?\" suggested a link between the games and the players' identities. The company promoted its Nintendo DS handheld with the tagline \"Touching is Good.\" For the Wii, they used the \"Wii would like to play\" slogan to promote the console with the people who tried the games including \"Super Mario Galaxy\" and \"Super Paper Mario\". The Nintendo 3DS used the slogan \"Take a look inside\". The Wii U used the slogan \"How U will play next.\" The Nintendo Switch uses the slogan \"Switch and Play\" in North America, and \"Play anywhere, anytime, with anyone\" in Europe.\n\n\n\n\n\n\nNintendo's internal research and development operations are divided into three main divisions: Nintendo Entertainment Planning & Development (or EPD), the main software development division of Nintendo, which focuses on video game and software development; Nintendo Platform Technology Development (or PTD), which focuses on home and handheld video game console hardware development; and Nintendo Business Development (or NBD), which focuses on refining business strategy and is responsible for overseeing the smart device arm of the business.\n\nThe Nintendo Entertainment Planning & Development division is the primary software development division at Nintendo, formed as a merger between their former Entertainment Analysis & Development and Software Planning & Development divisions in 2015. Led by Shinya Takahashi, the division holds the largest concentration of staff at the company, housing more than 800 engineers and designers. The division is primarily located in the central Kyoto R&D building, where they are overseen by Katsuya Eguchi, and also has a studio in Tokyo overseen by Yoshiaki Koizumi.\n\nThe Nintendo Platform Technology Development division is a combination of Nintendo's former Integrated Research & Development (or IRD) and System Development (or SDD) divisions. Led by Ko Shiota, the division is responsible for designing hardware and developing Nintendo's operating systems, developer environment and internal network as well as maintenance of the Nintendo Network.\n\nThe Nintendo Business Development division was formed following Nintendo's foray into software development for smart devices such as mobile phones and tablets. They are responsible for refining Nintendo's business model for the dedicated video game system business, and for furthering Nintendo's venture into development for smart devices.\n\nHeadquartered in Kyoto, Japan since the beginning, Nintendo Co., Ltd. oversees the organization's global operations and manages Japanese operations specifically. The company's two major subsidiaries, Nintendo of America and Nintendo of Europe, manage operations in North America and Europe respectively. Nintendo Co., Ltd. moved from its original Kyoto location to a new office in Higashiyama-ku, Kyoto,; in 2000, this became the research and development building when the head office relocated to its location in Minami-ku, Kyoto.\n\nNintendo's North American subsidiary is based in Redmond, Washington. Originally, the American headquarters handled sales, marketing, and advertising. However, the office in Redwood City, California now directs those functions. The company maintains distribution centers in Atlanta (Nintendo Atlanta) and North Bend, Washington (Nintendo North Bend). The Nintendo North Bend facility processes more than 20,000 orders a day to Nintendo customers, which include retail stores that sell Nintendo products in addition to consumers who shop Nintendo's web site. Nintendo of America also operates two retail stores in the United States, Nintendo New York in Rockefeller Plaza, New York City, which is open to the public, and Nintendo Redmond, co-located at NOA headquarters in Redmond, Washington, which is open only to Nintendo employees and invited guests. Nintendo of America's Canadian branch, Nintendo of Canada, is based in Vancouver, British Columbia with a distribution center in Toronto, Ontario. Nintendo of America's localization team, dubbed Nintendo Treehouse, is composed of around eighty staff, who are responsible for translating text from Japanese to English, creating videos and marketing plans, and ensuring that Nintendo's games release in a polished state.\n\nNintendo's European subsidiary was established in June 1990, based in Großostheim, close to Frankfurt, Germany. The company handles operations in Europe and South Africa. Nintendo of Europe's United Kingdom branch (Nintendo UK) handles operations in that country and in Ireland from its headquarters in Windsor, Berkshire. In June 2014, NOE initiated a reduction and consolidation process, yielding a combined 130 layoffs: the closing of its office and warehouse, and termination of all employment, in Großostheim; and the consolidation of all of those operations into, and terminating some employment at, its Frankfurt location. As of July 2018, the company employs 850 people.\n\nNintendo's Australian subsidiary is based in Melbourne, Victoria. It handles the publishing, distribution, sales and marketing of Nintendo products in Australia, New Zealand, and Oceania (Cook Islands, Fiji, New Caledonia, Papua New Guinea, Samoa, and Vanuatu). It also manufactures some Wii games locally. Nintendo Australia is also a third-party distributor of some games from Rising Star Games, Bandai Namco Entertainment, Atlus, The Tetris Company, Sega, Koei Tecmo, and Capcom.\n\nA Chinese joint venture between its founder, Wei Yen, and Nintendo, manufactures and distributes official Nintendo consoles and games for the mainland Chinese market, under the iQue brand. The product lineup for the Chinese market is considerably different from that for other markets. For example, Nintendo's only console in China is the iQue Player, a modified version of the Nintendo 64. The company has not released its more modern GameCube or Wii to the market, although a version of the Nintendo 3DS XL was released in 2012. As of 2013, it is a 100% Nintendo-owned subsidiary.\n\nNintendo's South Korean subsidiary was established on 7 July 2006, and is based in Seoul. In March 2016, the subsidiary was heavily downsized due to a corporate restructuring after analyzing shifts in the current market, laying off 80% of its employees, leaving only ten people, including CEO Hiroyuki Fukuda. This did not affect any games scheduled for release in South Korea, and Nintendo continued operations there as usual.\n\nAlthough most of the Research & Development is being done in Japan, there are some R&D facilities in the United States and Europe that are focused on developing software and hardware technologies used in Nintendo products. Although they all are subsidiaries of Nintendo (and therefore first party), they are often referred to as external resources when being involved in joint development processes with Nintendo's internal developers by the Japanese personal involved. This can be seen in a variety of \"Iwata asks...\" interviews. Nintendo Software Technology (NST) and Nintendo Technology Development (NTD) are located in Redmond, Washington, United States, while Nintendo European Research & Development (\"NERD\") is located in Paris, France, and Nintendo Network Service Database (NSD) is located in Kyoto, Japan.\n\nMost external first-party software development is done in Japan, since the only overseas subsidiary is Retro Studios in the United States. Although these studios are all subsidiaries of Nintendo, they are often referred to as external resources when being involved in joint development processes with Nintendo's internal developers by the Nintendo Entertainment Planning & Development (EPD) division. 1-Up Studio and Nd Cube are located in Tokyo, Japan, while Monolith Soft has one studio located in Tokyo and another in Kyoto. Retro Studios is located in Austin, Texas.\n\nNintendo also established The Pokémon Company alongside Creatures and Game Freak in order to effectively manage the Pokémon brand. Similarly, Warpstar Inc. was formed through a joint investment with HAL Laboratory, which was in charge of the animated series.\n\nFor many years, Nintendo had a policy of strict content guidelines for video games published on its consoles. Although Nintendo allowed graphic violence in its video games released in Japan, nudity and sexuality were strictly prohibited. Former Nintendo president Hiroshi Yamauchi believed that if the company allowed the licensing of pornographic games, the company's image would be forever tarnished. Nintendo of America went further in that games released for Nintendo consoles could not feature nudity, sexuality, profanity (including racism, sexism or slurs), blood, graphic or domestic violence, drugs, political messages or religious symbols (with the exception of widely unpracticed religions, such as the Greek Pantheon). The Japanese parent company was concerned that it may be viewed as a \"Japanese Invasion\" by forcing Japanese community standards on North American and European children. Despite the strict guidelines, some exceptions have occurred: \"Bionic Commando\" (though swastikas were eliminated in the US version), \"Smash TV\" and \"\" contained human violence, the latter also containing implied sexuality and tobacco use; \"River City Ransom\" and \"\" contained nudity, and the latter also contained religious images, as did \"\" and \"\".\n\nA known side effect of this policy was the Genesis version of \"Mortal Kombat\" selling over double the number of the Super NES version, mainly because Nintendo had forced publisher Acclaim to recolor the red blood to look like white sweat and replace some of the more gory graphics in its release of the game, making it less violent. By contrast, Sega allowed blood and gore to remain in the Genesis version (though a code was required to unlock the gore). Nintendo allowed the Super NES version of \"Mortal Kombat II\" to ship uncensored the following year with a content warning on the packaging.\n\nIn 1994 and 2003, when the ESRB and PEGI (respectively) video game ratings systems were introduced, Nintendo chose to abolish most of these policies in favor of consumers making their own choices about the content of the games they played. Today, changes to the content of games are done primarily by the game's developer or, occasionally, at the request of Nintendo. The only clear-set rule is that ESRB AO-rated games will not be licensed on Nintendo consoles in North America, a practice which is also enforced by Sony and Microsoft, its two greatest competitors in the present market. Nintendo has since allowed several mature-content games to be published on its consoles, including: \"Perfect Dark\", \"Conker's Bad Fur Day\", \"Doom\" and \"Doom 64\", \"BMX XXX\", the \"Resident Evil\" series, \"Killer7\", the \"Mortal Kombat\" series, \"\", \"BloodRayne\", \"Geist\", \"\", \"Bayonetta 2\", \"Devil's Third\" and \"\". Certain games have continued to be modified, however. For example, Konami was forced to remove all references to cigarettes in the 2000 Game Boy Color game \"Metal Gear Solid\" (although the previous NES version of \"Metal Gear\" and the subsequent GameCube game \"\" both included such references, as did Wii game \"MadWorld\"), and maiming and blood were removed from the Nintendo 64 port of \"Cruis'n USA\". Another example is in the Game Boy Advance game \"Mega Man Zero 3\", in which one of the bosses, called Hellbat Schilt in the Japanese and European releases, was renamed Devilbat Schilt in the North American localisation. In North America releases of the \"Mega Man Zero\" games, enemies and bosses killed with a saber attack would not gush blood as they did in the Japanese versions. However, the release of the Wii was accompanied by a number of even more controversial games, such as \"Manhunt 2\", \"No More Heroes\", \"\", and \"MadWorld\", the latter three of which were published exclusively for the console.\n\nNintendo of America also had guidelines before 1993 that had to be followed by its licensees to make games for the Nintendo Entertainment System, in addition to the above content guidelines. Guidelines were enforced through the 10NES lock-out chip.\n\nThe last rule was circumvented in a number of ways; for example, Konami, wanting to produce more games for Nintendo's consoles, formed Ultra Games and later Palcom to produce more games as a technically different publisher. This disadvantaged smaller or emerging companies, as they could not afford to start additional companies. In another side effect, Square Co (now Square Enix) executives have suggested that the price of publishing games on the Nintendo 64 along with the degree of censorship and control that Nintendo enforced over its games, most notably \"Final Fantasy VI\", were factors in switching its focus towards Sony's PlayStation console.\n\nIn 1993, a class action suit was taken against Nintendo under allegations that their lock-out chip enabled unfair business practices. The case was settled, with the condition that California consumers were entitled to a $3 discount coupon for a game of Nintendo's choice.\n\nNintendo is opposed to any third-party emulation of its video games and consoles, stating that it is the single largest threat to the intellectual property rights of video game developers. However, emulators have been used by Nintendo and licensed third party companies as a means to re-release older games, with Virtual Console, which re-released classic games as downloadable titles, and with dedicated consoles like the NES Mini and SNES Mini. On 19 July 2018, Nintendo sued Jacob Mathias, the owner of ROM image distribution websites LoveROMs and LoveRetro, for \"brazen and mass-scale infringement of Nintendo’s intellectual property rights.” Nintendo settled with Mathias in November 2018 for over along with relinguishing all ROM images in their ownership. While Nintendo is likely to have agreed to a smaller fine in private, the large amount was seen as a deterrent to prevent similar sites from sharing ROM images.\n\nThe gold sunburst seal was first used by Nintendo of America, and later Nintendo of Europe. It is displayed on any game, system, or accessory licensed for use on one of its video game consoles, denoting the game has been properly approved by Nintendo. The seal is also displayed on any Nintendo-licensed merchandise, such as trading cards, game guides, or apparel, albeit with the words \"Official Nintendo Licensed Product\".\n\nIn 2008, game designer Sid Meier cited the Seal of Quality as one of the three most important innovations in video game history, as it helped set a standard for game quality that protected consumers from shovelware.\n\nIn NTSC regions, this seal is an elliptical starburst named the \"Official Nintendo Seal\". Originally, for NTSC countries, the seal was a large, black and gold circular starburst. The seal read as follows: \"This seal is your assurance that NINTENDO has approved and guaranteed the quality of this product.\" This seal was later altered in 1988: \"approved and guaranteed\" was changed to \"evaluated and approved.\" In 1989, the seal became gold and white, as it currently appears, with a shortened phrase, \"Official Nintendo Seal of Quality.\" It was changed in 2003 to read \"Official Nintendo Seal.\"\n\nThe seal currently reads:\nIn PAL regions, the seal is a circular starburst named the \"Original Nintendo Seal of Quality.\" Text near the seal in the Australian Wii manual states:\nIn 1992, Nintendo teamed with the Starlight Children's Foundation to build Starlight Fun Center mobile entertainment units and install them in hospitals. 1,000 Starlight Nintendo Fun Center units were installed by the end of 1995. These units combine several forms of multimedia entertainment, including gaming, and serve as a distraction to brighten moods and boost kids' morale during hospital stays.\n\nNintendo has consistently been ranked last in Greenpeace's \"Guide to Greener Electronics\" due to Nintendo's failure to publish information. Similarly, they are ranked last in the Enough Project's \"Conflict Minerals Company Rankings\" due to Nintendo's refusal to respond to multiple requests for information.\n\nLike many other electronics companies, Nintendo offers a take-back recycling program which allows customers to mail in old products they no longer use. Nintendo of America claimed that it took in 548 tons of returned products in 2011, 98% of which was either reused or recycled.\n\nDuring the peak of Nintendo's success in the video game industry in the 1990s, their name was ubiquitously used to refer to any video game console, regardless of the manufacturer. To prevent their trademark from becoming generic, Nintendo pushed usage of the term \"game console\", and succeeded in preserving their trademark.\n\n\n"}
{"id": "21201", "url": "https://en.wikipedia.org/wiki?curid=21201", "title": "Nobel Prize", "text": "Nobel Prize\n\nThe Nobel Prize (, ; Swedish definite form, singular: \"Nobelpriset\"; ) is a set of annual international awards bestowed in several categories by Swedish and Norwegian institutions in recognition of academic, cultural, or scientific advances.\n\nThe will of the Swedish scientist Alfred Nobel established the five Nobel prizes in 1895. The prizes in Chemistry, Literature, Peace, Physics, and Physiology or Medicine were first awarded in 1901. The prizes are widely regarded as the most prestigious awards available in the fields of chemistry, literature, peace activism, physics, and physiology or medicine.\n\nIn 1968, Sweden's central bank, Sveriges Riksbank, established the \"Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel\", which, although not a Nobel Prize, has become informally known as the \"Nobel Prize in Economics\". \n\nThe Royal Swedish Academy of Sciences awards the Nobel Prize in Chemistry, the Nobel Prize in Physics, and the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel; the Nobel Assembly at the Karolinska Institute awards the Nobel Prize in Physiology or Medicine; the Swedish Academy grants the Nobel Prize in Literature; and the Norwegian Nobel Committee awards the Nobel Peace Prize.\n\nBetween 1901 and 2018, the Nobel Prizes (and the Prizes in Economic Sciences, from 1969 on) were awarded 590 times to 935 people and organizations. With some receiving the Nobel Prize more than once, this makes a total of 27 organizations and 908 individuals. The prize ceremonies take place annually in Stockholm, Sweden (with the exception of the Peace Prize ceremony, which is held in Oslo, Norway). Each recipient (known as a \"laureate\") receives a gold medal, a diploma, and a sum of money that has been decided by the Nobel Foundation. (, each prize is worth 9,000,000 SEK, or about , €944,000, £836,000 or ₹73,800,000.) Medals made before 1980 were struck in 23-carat gold, and later in 18-carat green gold plated with a 24-carat gold coating.\n\nThe prize is not awarded posthumously; however, if a person is awarded a prize and dies before receiving it, the prize may still be presented. A prize may not be shared among more than three individuals, although the Nobel Peace Prize can be awarded to organizations of more than three people.\n\nAlfred Nobel () was born on 21 October 1833 in Stockholm, Sweden, into a family of engineers. He was a chemist, engineer, and inventor. In 1894, Nobel purchased the Bofors iron and steel mill, which he made into a major armaments manufacturer. Nobel also invented ballistite. This invention was a precursor to many smokeless military explosives, especially the British smokeless powder cordite. As a consequence of his patent claims, Nobel was eventually involved in a patent infringement lawsuit over cordite. Nobel amassed a fortune during his lifetime, with most of his wealth coming from his 355 inventions, of which dynamite is the most famous.\n\nIn 1888, Nobel was astonished to read his own obituary, titled \"The merchant of death is dead\", in a French newspaper. As it was Alfred's brother Ludvig who had died, the obituary was eight years premature. The article disconcerted Nobel and made him apprehensive about how he would be remembered. This inspired him to change his will. On 10 December 1896, Alfred Nobel died in his villa in San Remo, Italy, from a cerebral haemorrhage. He was 63 years old.\n\nNobel wrote several wills during his lifetime. He composed the last over a year before he died, signing it at the Swedish–Norwegian Club in Paris on 27 November 1895. To widespread astonishment, Nobel's last will specified that his fortune be used to create a series of prizes for those who confer the \"greatest benefit on mankind\" in physics, chemistry, physiology or medicine, literature, and peace. Nobel bequeathed 94% of his total assets, 31 million SEK (c. US$186 million, €150 million in 2008), to establish the five Nobel Prizes.\nBecause of skepticism surrounding the will, it was not until 26 April 1897 that it was approved by the Storting in Norway. The executors of Nobel's will, Ragnar Sohlman and Rudolf Lilljequist, formed the Nobel Foundation to take care of Nobel's fortune and organised the award of prizes.\n\nNobel's instructions named a Norwegian Nobel Committee to award the Peace Prize, the members of whom were appointed shortly after the will was approved in April 1897. Soon thereafter, the other prize-awarding organizations were designated. These were Karolinska Institutet on 7 June, the Swedish Academy on 9 June, and the Royal Swedish Academy of Sciences on 11 June. The Nobel Foundation reached an agreement on guidelines for how the prizes should be awarded; and, in 1900, the Nobel Foundation's newly created statutes were promulgated by King Oscar II. In 1905, the personal union between Sweden and Norway was dissolved.\n\nAccording to his will and testament read in Stockholm on 30 December 1896, a foundation established by Alfred Nobel would reward those who serve humanity. The Nobel Prize was funded by Alfred Nobel's personal fortune. According to the official sources, Alfred Nobel bequeathed from the shares 94% of his fortune to the Nobel Foundation that now forms the economic base of the Nobel Prize.\n\nThe Nobel Foundation was founded as a private organization on 29 June 1900. Its function is to manage the finances and administration of the Nobel Prizes. In accordance with Nobel's will, the primary task of the Foundation is to manage the fortune Nobel left. Robert and Ludvig Nobel were involved in the oil business in Azerbaijan, and according to Swedish historian E. Bargengren, who accessed the Nobel family archives, it was this \"decision to allow withdrawal of Alfred's money from Baku that became the decisive factor that enabled the Nobel Prizes to be established\". Another important task of the Nobel Foundation is to market the prizes internationally and to oversee informal administration related to the prizes. The Foundation is not involved in the process of selecting the Nobel laureates. In many ways, the Nobel Foundation is similar to an investment company, in that it invests Nobel's money to create a solid funding base for the prizes and the administrative activities. The Nobel Foundation is exempt from all taxes in Sweden (since 1946) and from investment taxes in the United States (since 1953). Since the 1980s, the Foundation's investments have become more profitable and as of 31 December 2007, the assets controlled by the Nobel Foundation amounted to 3.628 billion Swedish \"kronor\" (c. US$560 million).\n\nAccording to the statutes, the Foundation consists of a board of five Swedish or Norwegian citizens, with its seat in Stockholm. The Chairman of the Board is appointed by the Swedish King in Council, with the other four members appointed by the trustees of the prize-awarding institutions. An Executive Director is chosen from among the board members, a Deputy Director is appointed by the King in Council, and two deputies are appointed by the trustees. However, since 1995, all the members of the board have been chosen by the trustees, and the Executive Director and the Deputy Director appointed by the board itself. As well as the board, the Nobel Foundation is made up of the prize-awarding institutions (the Royal Swedish Academy of Sciences, the Nobel Assembly at Karolinska Institute, the Swedish Academy, and the Norwegian Nobel Committee), the trustees of these institutions, and auditors.\n\nThe capital of the Nobel Foundation today is invested 50 % in shares, 20 % bonds and 30 % other investments (e.g. hedge funds or real estate). The distribution can vary by 10 percent. At the beginning of 2008, 64 % of the funds were invested mainly in American and European stocks, 20 % in bonds, plus 12% in real estate and hedge funds.\n\nIn 2011, the total annual cost was approximately 120 million krona, with 50 million krona as the prize money. Further costs to pay institutions and persons engaged in giving the prizes were 27,4 million krona. The events during the Nobel week in Stockholm and Oslo cost 20,2 million krona. The administration, Nobel symposium, and similar items had costs of 22.4 million krona. The cost of the Economic Sciences prize of 16.5 Million krona is paid by the Sveriges Riksbank.\n\nOnce the Nobel Foundation and its guidelines were in place, the Nobel Committees began collecting nominations for the inaugural prizes. Subsequently, they sent a list of preliminary candidates to the prize-awarding institutions.\n\nThe Nobel Committee's Physics Prize shortlist cited Wilhelm Röntgen's discovery of X-rays and Philipp Lenard's work on cathode rays. The Academy of Sciences selected Röntgen for the prize. In the last decades of the 19th century, many chemists had made significant contributions. Thus, with the Chemistry Prize, the Academy \"was chiefly faced with merely deciding the order in which these scientists should be awarded the prize\". The Academy received 20 nominations, eleven of them for Jacobus van 't Hoff. Van 't Hoff was awarded the prize for his contributions in chemical thermodynamics.\n\nThe Swedish Academy chose the poet Sully Prudhomme for the first Nobel Prize in Literature. A group including 42 Swedish writers, artists, and literary critics protested against this decision, having expected Leo Tolstoy to be awarded. Some, including Burton Feldman, have criticised this prize because they consider Prudhomme a mediocre poet. Feldman's explanation is that most of the Academy members preferred Victorian literature and thus selected a Victorian poet. The first Physiology or Medicine Prize went to the German physiologist and microbiologist Emil von Behring. During the 1890s, von Behring developed an antitoxin to treat diphtheria, which until then was causing thousands of deaths each year.\n\nThe first Nobel Peace Prize went to the Swiss Jean Henri Dunant for his role in founding the International Red Cross Movement and initiating the Geneva Convention, and jointly given to French pacifist Frédéric Passy, founder of the Peace League and active with Dunant in the Alliance for Order and Civilization.\n\nIn 1938 and 1939, Adolf Hitler's Third Reich forbade three laureates from Germany (Richard Kuhn, Adolf Friedrich Johann Butenandt, and Gerhard Domagk) from accepting their prizes. Each man was later able to receive the diploma and medal. Even though Sweden was officially neutral during the Second World War, the prizes were awarded irregularly. In 1939, the Peace Prize was not awarded. No prize was awarded in any category from 1940 to 1942, due to the occupation of Norway by Germany. In the subsequent year, all prizes were awarded except those for literature and peace.\n\nDuring the occupation of Norway, three members of the Norwegian Nobel Committee fled into exile. The remaining members escaped persecution from the Germans when the Nobel Foundation stated that the Committee building in Oslo was Swedish property. Thus it was a safe haven from the German military, which was not at war with Sweden. These members kept the work of the Committee going, but did not award any prizes. In 1944, the Nobel Foundation, together with the three members in exile, made sure that nominations were submitted for the Peace Prize and that the prize could be awarded once again.\n\nIn 1968, Sveriges Riksbank (Sweden's central bank) celebrated its 300th anniversary by donating a large sum of money to the Nobel Foundation to be used to set up a prize in honor of Nobel. The following year, the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel was awarded for the first time. The Royal Swedish Academy of Sciences became responsible for selecting laureates. The first laureates for the Economics Prize were Jan Tinbergen and Ragnar Frisch \"for having developed and applied dynamic models for the analysis of economic processes\". The Board of the Nobel Foundation decided that after this addition, it would allow no further new prizes.\n\nThe award process is similar for all of the Nobel Prizes; the main difference is in who can make nominations for each of them.\nNomination forms are sent by the Nobel Committee to about 3,000 individuals, usually in September the year before the prizes are awarded. These individuals are generally prominent academics working in a relevant area. Regarding the Peace Prize, inquiries are also sent to governments, former Peace Prize laureates, and current or former members of the Norwegian Nobel Committee. The deadline for the return of the nomination forms is 31 January of the year of the award. The Nobel Committee nominates about 300 potential laureates from these forms and additional names. The nominees are not publicly named, nor are they told that they are being considered for the prize. All nomination records for a prize are sealed for 50 years from the awarding of the prize.\n\nThe Nobel Committee then prepares a report reflecting the advice of experts in the relevant fields. This, along with the list of preliminary candidates, is submitted to the prize-awarding institutions. The institutions meet to choose the laureate or laureates in each field by a majority vote. Their decision, which cannot be appealed, is announced immediately after the vote. A maximum of three laureates and two different works may be selected per award. Except for the Peace Prize, which can be awarded to institutions, the awards can only be given to individuals.\n\nAlthough posthumous nominations are not presently permitted, individuals who died in the months between their nomination and the decision of the prize committee were originally eligible to receive the prize. This has occurred twice: the 1931 Literature Prize awarded to Erik Axel Karlfeldt, and the 1961 Peace Prize awarded to UN Secretary General Dag Hammarskjöld. Since 1974, laureates must be thought alive at the time of the October announcement. There has been one laureate, William Vickrey, who in 1996 died after the prize (in Economics) was announced but before it could be presented. On 3 October 2011, the laureates for the Nobel Prize in Physiology or Medicine were announced; however, the committee was not aware that one of the laureates, Ralph M. Steinman, had died three days earlier. The committee was debating about Steinman's prize, since the rule is that the prize is not awarded posthumously. The committee later decided that as the decision to award Steinman the prize \"was made in good faith\", it would remain unchanged.\n\nNobel's will provided for prizes to be awarded in recognition of discoveries made \"during the preceding year\". Early on, the awards usually recognised recent discoveries. However, some of those early discoveries were later discredited. For example, Johannes Fibiger was awarded the 1926 Prize in Physiology or Medicine for his purported discovery of a parasite that caused cancer. To avoid repeating this embarrassment, the awards increasingly recognised scientific discoveries that had withstood the test of time. According to Ralf Pettersson, former chairman of the Nobel Prize Committee for Physiology or Medicine, \"the criterion 'the previous year' is interpreted by the Nobel Assembly as the year when the full impact of the discovery has become evident.\"\nThe interval between the award and the accomplishment it recognises varies from discipline to discipline. The Literature Prize is typically awarded to recognise a cumulative lifetime body of work rather than a single achievement. The Peace Prize can also be awarded for a lifetime body of work. For example, 2008 laureate Martti Ahtisaari was awarded for his work to resolve international conflicts. However, they can also be awarded for specific recent events. For instance, Kofi Annan was awarded the 2001 Peace Prize just four years after becoming the Secretary-General of the United Nations. Similarly Yasser Arafat, Yitzhak Rabin, and Shimon Peres received the 1994 award, about a year after they successfully concluded the Oslo Accords.\n\nAwards for physics, chemistry, and medicine are typically awarded once the achievement has been widely accepted. Sometimes, this takes decades – for example, Subrahmanyan Chandrasekhar shared the 1983 Physics Prize for his 1930s work on stellar structure and evolution. Not all scientists live long enough for their work to be recognised. Some discoveries can never be considered for a prize if their impact is realised after the discoverers have died.\n\nExcept for the Peace Prize, the Nobel Prizes are presented in Stockholm, Sweden, at the annual Prize Award Ceremony on 10 December, the anniversary of Nobel's death. The recipients' lectures are normally held in the days prior to the award ceremony. The Peace Prize and its recipients' lectures are presented at the annual Prize Award Ceremony in Oslo, Norway, usually on 10 December. The award ceremonies and the associated banquets are typically major international events. The Prizes awarded in Sweden's ceremonies' are held at the Stockholm Concert Hall, with the Nobel banquet following immediately at Stockholm City Hall. The Nobel Peace Prize ceremony has been held at the Norwegian Nobel Institute (1905–1946), at the auditorium of the University of Oslo (1947–1989), and at Oslo City Hall (1990–present).\n\nThe highlight of the Nobel Prize Award Ceremony in Stockholm occurs when each Nobel laureate steps forward to receive the prize from the hands of the King of Sweden. In Oslo, the Chairman of the Norwegian Nobel Committee presents the Nobel Peace Prize in the presence of the King of Norway. At first, King Oscar II did not approve of awarding grand prizes to foreigners. It is said that he changed his mind once his attention had been drawn to the publicity value of the prizes for Sweden.\n\nAfter the award ceremony in Sweden, a banquet is held in the Blue Hall at the Stockholm City Hall, which is attended by the Swedish Royal Family and around 1,300 guests.\n\nThe Nobel Peace Prize banquet is held in Norway at the Oslo Grand Hotel after the award ceremony. Apart from the laureate, guests include the President of the Storting, the Swedish prime minister, and, since 2006, the King and Queen of Norway. In total, about 250 guests attend.\n\nAccording to the statutes of the Nobel Foundation, each laureate is required to give a public lecture on a subject related to the topic of their prize. The Nobel lecture as a rhetorical genre took decades to reach its current format. These lectures normally occur during Nobel Week (the week leading up to the award ceremony and banquet, which begins with the laureates arriving in Stockholm and normally ends with the Nobel banquet), but this is not mandatory. The laureate is only obliged to give the lecture within six months of receiving the prize. Some have happened even later. For example, US President Theodore Roosevelt received the Peace Prize in 1906 but gave his lecture in 1910, after his term in office. The lectures are organized by the same association which selected the laureates.\n\nIt was announced on 30 May 2012 that the Nobel Foundation had awarded the contract for the production of the five (Swedish) Nobel Prize medals to Svenska Medalj AB. Formerly, the Nobel Prize medals were minted by Myntverket (the Swedish Mint) from 1902 to 2010. Myntverket, Sweden's oldest company, ceased operations in 2011 after 1,017 years. In 2011, the Mint of Norway, located in Kongsberg, made the medals. The Nobel Prize medals are registered trademarks of the Nobel Foundation. Each medal features an image of Alfred Nobel in left profile on the obverse. The medals for physics, chemistry, physiology or medicine, and literature have identical obverses, showing the image of Alfred Nobel and the years of his birth and death. Nobel's portrait also appears on the obverse of the Peace Prize medal and the medal for the Economics Prize, but with a slightly different design. For instance, the laureate's name is engraved on the rim of the Economics medal. The image on the reverse of a medal varies according to the institution awarding the prize. The reverse sides of the medals for chemistry and physics share the same design.\n\nAll medals made before 1980 were struck in 23 carat gold. Since then, they have been struck in 18 carat green gold plated with 24 carat gold. The weight of each medal varies with the value of gold, but averages about for each medal. The diameter is and the thickness varies between and . Because of the high value of their gold content and tendency to be on public display, Nobel medals are subject to medal theft. During World War II, the medals of German scientists Max von Laue and James Franck were sent to Copenhagen for safekeeping. When Germany invaded Denmark, Hungarian chemist (and Nobel laureate himself) George de Hevesy dissolved them in aqua regia (nitro-hydrochloric acid), to prevent confiscation by Nazi Germany and to prevent legal problems for the holders. After the war, the gold was recovered from solution, and the medals re-cast.\n\nNobel laureates receive a diploma directly from the hands of the King of Sweden, or in the case of the peace prize, the Chairman of the Norwegian Nobel Committee. Each diploma is uniquely designed by the prize-awarding institutions for the laureates that receive them. The diploma contains a picture and text in Swedish which states the name of the laureate and normally a citation of why they received the prize. None of the Nobel Peace Prize laureates has ever had a citation on their diplomas.\n\nThe laureates are given a sum of money when they receive their prizes, in the form of a document confirming the amount awarded. The amount of prize money depends upon how much money the Nobel Foundation can award each year. The purse has increased since the 1980s, when the prize money was 880,000 SEK per prize (c. 2.6 million SEK altogether, US$350,000 today). In 2009, the monetary award was 10 million SEK (US$1.4 million). In June 2012, it was lowered to 8 million SEK. If there are two laureates in a particular category, the award grant is divided equally between the recipients. If there are three, the awarding committee has the option of dividing the grant equally, or awarding one-half to one recipient and one-quarter to each of the others. It is common for recipients to donate prize money to benefit scientific, cultural, or humanitarian causes.\n\nAmong other criticisms, the Nobel Committees have been accused of having a political agenda, and of omitting more deserving candidates. They have also been accused of Eurocentrism, especially for the Literature Prize.\n\n\nAmong the most criticised Nobel Peace Prizes was the one awarded to Henry Kissinger and Lê Đức Thọ. This led to the resignation of two Norwegian Nobel Committee members. Kissinger and Thọ were awarded the prize for negotiating a ceasefire between North Vietnam and the United States in January 1973. However, when the award was announced, both sides were still engaging in hostilities. Critics sympathetic to the North announced that Kissinger was not a peace-maker but the opposite, responsible for widening the war. Those hostile to the North and what they considered its deceptive practices during negotiations were deprived of a chance to criticise Lê Đức Thọ, as he declined the award. \n\nYasser Arafat, Shimon Peres, and Yitzhak Rabin received the Peace Prize in 1994 for their efforts in making peace between Israel and Palestine. Immediately after the award was announced, one of the five Norwegian Nobel Committee members denounced Arafat as a terrorist and resigned. Additional misgivings about Arafat were widely expressed in various newspapers.\n\nAnother controversial Peace Prize was that awarded to Barack Obama in 2009. Nominations had closed only eleven days after Obama took office as President of the United States, but the actual evaluation occurred over the next eight months. Obama himself stated that he did not feel deserving of the award, or worthy of the company it would place him in. Past Peace Prize laureates were divided, some saying that Obama deserved the award, and others saying he had not secured the achievements to yet merit such an accolade. Obama's award, along with the previous Peace Prizes for Jimmy Carter and Al Gore, also prompted accusations of a left-wing bias.\n\n\nThe award of the 2004 Literature Prize to Elfriede Jelinek drew a protest from a member of the Swedish Academy, Knut Ahnlund. Ahnlund resigned, alleging that the selection of Jelinek had caused \"irreparable damage to all progressive forces, it has also confused the general view of literature as an art\". He alleged that Jelinek's works were \"a mass of text shovelled together without artistic structure\". The 2009 Literature Prize to Herta Müller also generated criticism. According to \"The Washington Post\", many US literary critics and professors were ignorant of her work. This made those critics feel the prizes were too Eurocentric.\n\n\nIn 1949, the neurologist António Egas Moniz received the Physiology or Medicine Prize for his development of the prefrontal leucotomy. The previous year, Dr. Walter Freeman had developed a version of the procedure which was faster and easier to carry out. Due in part to the publicity surrounding the original procedure, Freeman's procedure was prescribed without due consideration or regard for modern medical ethics. Endorsed by such influential publications as \"The New England Journal of Medicine\", leucotomy or \"lobotomy\" became so popular that about 5,000 lobotomies were performed in the United States in the three years immediately following Moniz's receipt of the Prize.\n\nThe Norwegian Nobel Committee confirmed that Mahatma Gandhi was nominated for the Peace Prize in 1937–39, 1947, and a few days before he was assassinated in January 1948. Later, members of the Norwegian Nobel Committee expressed regret that he was not given the prize. Geir Lundestad, Secretary of Norwegian Nobel Committee in 2006, said, \"The greatest omission in our 106 year history is undoubtedly that Mahatma Gandhi never received the Nobel Peace prize. Gandhi could do without the Nobel Peace prize. Whether Nobel committee can do without Gandhi is the question\". In 1948, the year of Gandhi's death, the Nobel Committee declined to award a prize on the grounds that \"there was no suitable living candidate\" that year. Later, when the 14th Dalai Lama was awarded the Peace Prize in 1989, the chairman of the committee said that this was \"in part a tribute to the memory of Mahatma Gandhi\". Other high-profile individuals with widely recognised contributions to peace have been missed out. \"Foreign Policy\" lists Eleanor Roosevelt, Václav Havel, Ken Saro-Wiwa, Sari Nusseibeh, and Corazon Aquino as people who \"never won the prize, but should have\".\n\nIn 1965, UN Secretary General U Thant was informed by the Norwegian Permanent Representative to the UN that he would be awarded that year's prize and asked whether or not he would accept. He consulted staff and later replied that he would. At the same time, Chairman Gunnar Jahn of the Nobel Peace prize committee, lobbied heavily against giving U Thant the prize and the prize was at the last minute awarded to UNICEF. The rest of the committee all wanted the prize to go to U Thant, for his work in defusing the Cuban Missile Crisis, ending the war in the Congo, and his ongoing work to mediate an end to the Vietnam War. The disagreement lasted three years and in 1966 and 1967 no prize was given, with Gunnar Jahn effectively vetoing an award to U Thant.\nThe Literature Prize also has controversial omissions. Adam Kirsch has suggested that many notable writers have missed out on the award for political or extra-literary reasons. The heavy focus on European and Swedish authors has been a subject of criticism. The Eurocentric nature of the award was acknowledged by Peter Englund, the 2009 Permanent Secretary of the Swedish Academy, as a problem with the award and was attributed to the tendency for the academy to relate more to European authors. This tendency towards European authors still leaves some European writers on a list of notable writers that have been overlooked for the Literature Prize, including Europe's Leo Tolstoy, Anton Chekhov, J. R. R. Tolkien, Émile Zola, Marcel Proust, Vladimir Nabokov, James Joyce, August Strindberg, Simon Vestdijk, Karel Čapek, the New World's Jorge Luis Borges, Ezra Pound, John Updike, Arthur Miller, Mark Twain, and Africa's Chinua Achebe.\n\nCandidates can receive multiple nominations the same year. Gaston Ramon received a total of 155 nominations in physiology or medicine from 1930 to 1953, the last year with public nomination data for that award . He died in 1963 without being awarded. Pierre Paul Émile Roux received 115 nominations in physiology or medicine, and Arnold Sommerfeld received 84 in physics. These are the three most nominated scientists without awards in the data published . Otto Stern received 79 nominations in physics 1925–43 before being awarded in 1943.\n\nThe strict rule against awarding a prize to more than three people is also controversial. When a prize is awarded to recognise an achievement by a team of more than three collaborators, one or more will miss out. For example, in 2002, the prize was awarded to Koichi Tanaka and John Fenn for the development of mass spectrometry in protein chemistry, an award that did not recognise the achievements of Franz Hillenkamp and Michael Karas of the Institute for Physical and Theoretical Chemistry at the University of Frankfurt. According to one of the nominees for the prize in physics, the three person limit deprived him and two other members of his team of the honor in 2013: the team of Carl Hagen, Gerald Guralnik, and Tom Kibble published a paper in 1964 that gave answers to how the cosmos began, but did not share the 2013 Physics Prize awarded to Peter Higgs and François Englert, who had also published papers in 1964 concerning the subject. All five physicists arrived at the same conclusion, albeit from different angles. Hagen contends that an equitable solution is to either abandon the three limit restriction, or expand the time period of recognition for a given achievement to two years.\n\nSimilarly, the prohibition of posthumous awards fails to recognise achievements by an individual or collaborator who dies before the prize is awarded. The Economics Prize was not awarded to Fischer Black, who died in 1995, when his co-author Myron Scholes received the honor in 1997 for their landmark work on option pricing along with Robert C. Merton, another pioneer in the development of valuation of stock options. In the announcement of the award that year, the Nobel committee prominently mentioned Black's key role.\n\nPolitical subterfuge may also deny proper recognition. Lise Meitner and Fritz Strassmann, who co-discovered nuclear fission along with Otto Hahn, may have been denied a share of Hahn's 1944 Nobel Chemistry Award due to having fled Germany when the Nazis came to power. The Meitner and Strassmann roles in the research was not fully recognised until years later, when they joined Hahn in receiving the 1966 Enrico Fermi Award.\n\nAlfred Nobel left his fortune to finance annual prizes to be awarded \"to those who, during the preceding year, shall have conferred the greatest benefit on mankind\". He stated that the Nobel Prizes in Physics should be given \"to the person who shall have made the most important 'discovery' or 'invention' within the field of physics\". Nobel did not emphasise discoveries, but they have historically been held in higher respect by the Nobel Prize Committee than inventions: 77% of the Physics Prizes have been given to discoveries, compared with only 23% to inventions. Christoph Bartneck and Matthias Rauterberg, in papers published in \"Nature\" and \"Technoetic Arts\", have argued this emphasis on discoveries has moved the Nobel Prize away from its original intention of rewarding the greatest contribution to society.\n\nIn terms of the most prestigious awards in STEM fields, only a small proportion have been awarded to women. Out of 210 laureates in Physics, 181 in Chemistry and 216 in Medicine between 1901 and 2018, there were only three female laureates in physics, five in chemistry and 12 in medicine. Factors proposed to contribute to the discrepancy between this and the roughly equal human sex ratio include biased nominations, fewer women than men being active in the relevant fields, Nobel Prizes typically being awarded decades after the research was done (reflecting a time when gender bias in the relevant fields was greater), a greater delay in awarding Nobel Prizes for women's achievements making longevity a more important factor for women (Nobel Prizes are not awarded posthumously), and a tendency to omit women from jointly awarded Nobel Prizes.\n\nFour people have received two Nobel Prizes. Marie Curie received the Physics Prize in 1903 for her work on radioactivity and the Chemistry Prize in 1911 for the isolation of pure radium, making her the only person to be awarded a Nobel Prize in two different sciences. Linus Pauling was awarded the 1954 Chemistry Prize for his research into the chemical bond and its application to the structure of complex substances. Pauling was also awarded the Peace Prize in 1962 for his activism against nuclear weapons, making him the only laureate of two unshared prizes. John Bardeen received the Physics Prize twice: in 1956 for the invention of the transistor and in 1972 for the theory of superconductivity. Frederick Sanger received the prize twice in Chemistry: in 1958 for determining the structure of the insulin molecule and in 1980 for inventing a method of determining base sequences in DNA.\n\nTwo organizations have received the Peace Prize multiple times. The International Committee of the Red Cross received it three times: in 1917 and 1944 for its work during the world wars; and in 1963 during the year of its centenary. The United Nations High Commissioner for Refugees has been awarded the Peace Prize twice for assisting refugees: in 1954 and 1981.\n\nThe Curie family has received the most prizes, with four prizes awarded to five individual laureates. Marie Curie received the prizes in Physics (in 1903) and Chemistry (in 1911). Her husband, Pierre Curie, shared the 1903 Physics prize with her. Their daughter, Irène Joliot-Curie, received the Chemistry Prize in 1935 together with her husband Frédéric Joliot-Curie. In addition, the husband of Marie Curie's second daughter, Henry Labouisse, was the director of UNICEF when he accepted the Nobel Peace Prize in 1965 on that organisation's behalf.\n\nAlthough no family matches the Curie family's record, there have been several with two laureates. The husband-and-wife team of Gerty Cori and Carl Ferdinand Cori shared the 1947 Prize in Physiology or Medicine as did the husband-and-wife team of May-Britt Moser and Edvard Moser in 2014 (along with John O'Keefe). J. J. Thomson was awarded the Physics Prize in 1906 for showing that electrons are particles. His son, George Paget Thomson, received the same prize in 1937 for showing that they also have the properties of waves. William Henry Bragg and his son, William Lawrence Bragg, shared the Physics Prize in 1915 for inventing the X-ray spectrometer. Niels Bohr was awarded the Physics prize in 1922, as was his son, Aage Bohr, in 1975. Manne Siegbahn, who received the Physics Prize in 1924, was the father of Kai Siegbahn, who received the Physics Prize in 1981. Hans von Euler-Chelpin, who received the Chemistry Prize in 1929, was the father of Ulf von Euler, who was awarded the Physiology or Medicine Prize in 1970. C. V. Raman was awarded the Physics Prize in 1930 and was the uncle of Subrahmanyan Chandrasekhar, who was awarded the same prize in 1983. Arthur Kornberg received the Physiology or Medicine Prize in 1959; Kornberg's son, Roger later received the Chemistry Prize in 2006. Jan Tinbergen, who was awarded the first Economics Prize in 1969, was the brother of Nikolaas Tinbergen, who received the 1973 Physiology or Medicine Prize. Alva Myrdal, Peace Prize laureate in 1982, was the wife of Gunnar Myrdal who was awarded the Economics Prize in 1974. Economics laureates Paul Samuelson and Kenneth Arrow were brothers-in-law. Frits Zernike, who was awarded the 1953 Physics Prize, is the great-uncle of 1999 Physics laureate Gerard 't Hooft.\n\nBeing a symbol of scientific or literary achievement that is recognisable worldwide, the Nobel Prize is often depicted in fiction. This includes films like \"The Prize\" and \"Nobel Son\" about fictional Nobel laureates as well as fictionalised accounts of stories surrounding real prizes such as \"Nobel Chor\", a film based on the unsolved theft of Rabindranath Tagore's prize.\n\nTwo laureates have voluntarily declined the Nobel Prize. In 1964, Jean-Paul Sartre was awarded the Literature Prize but refused, stating, \"A writer must refuse to allow himself to be transformed into an institution, even if it takes place in the most honourable form.\" Lê Đức Thọ, chosen for the 1973 Peace Prize for his role in the Paris Peace Accords, declined, stating that there was no actual peace in Vietnam.\n\nDuring the Third Reich, Adolf Hitler hindered Richard Kuhn, Adolf Butenandt, and Gerhard Domagk from accepting their prizes. All of them were awarded their diplomas and gold medals after World War II. In 1958, Boris Pasternak declined his prize for literature due to fear of what the Soviet Union government might do if he travelled to Stockholm to accept his prize. In return, the Swedish Academy refused his refusal, saying \"this refusal, of course, in no way alters the validity of the award.\" The Academy announced with regret that the presentation of the Literature Prize could not take place that year, holding it back until 1989 when Pasternak's son accepted the prize on his behalf. Aung San Suu Kyi was awarded the Nobel Peace Prize in 1991, but her children accepted the prize because she had been placed under house arrest in Burma; Suu Kyi delivered her speech two decades later, in 2012. Liu Xiaobo was awarded the Nobel Peace Prize in 2010 while he and his wife were under house arrest in China as political prisoners, and he was unable to accept the prize in his lifetime.\n\nThe memorial symbol \"Planet of Alfred Nobel\" was opened in Dnipropetrovsk University of Economics and Law in 2008. On the globe, there are 802 Nobel laureates' reliefs made of a composite alloy obtained when disposing of military strategic missiles.\n\n\n"}
{"id": "21210", "url": "https://en.wikipedia.org/wiki?curid=21210", "title": "Niels Bohr", "text": "Niels Bohr\n\nNiels Henrik David Bohr (; 7 October 1885 – 18 November 1962) was a Danish physicist who made foundational contributions to understanding atomic structure and quantum theory, for which he received the Nobel Prize in Physics in 1922. Bohr was also a philosopher and a promoter of scientific research.\n\nBohr developed the Bohr model of the atom, in which he proposed that energy levels of electrons are discrete and that the electrons revolve in stable orbits around the atomic nucleus but can jump from one energy level (or orbit) to another. Although the Bohr model has been supplanted by other models, its underlying principles remain valid. He conceived the principle of complementarity: that items could be separately analysed in terms of contradictory properties, like behaving as a wave or a stream of particles. The notion of complementarity dominated Bohr's thinking in both science and philosophy.\n\nBohr founded the Institute of Theoretical Physics at the University of Copenhagen, now known as the Niels Bohr Institute, which opened in 1920. Bohr mentored and collaborated with physicists including Hans Kramers, Oskar Klein, George de Hevesy, and Werner Heisenberg. He predicted the existence of a new zirconium-like element, which was named hafnium, after the Latin name for Copenhagen, where it was discovered. Later, the element bohrium was named after him.\n\nDuring the 1930s, Bohr helped refugees from Nazism. After Denmark was occupied by the Germans, he had a famous meeting with Heisenberg, who had become the head of the German nuclear weapon project. In September 1943, word reached Bohr that he was about to be arrested by the Germans, and he fled to Sweden. From there, he was flown to Britain, where he joined the British Tube Alloys nuclear weapons project, and was part of the British mission to the Manhattan Project. After the war, Bohr called for international cooperation on nuclear energy. He was involved with the establishment of CERN and the Research Establishment Risø of the Danish Atomic Energy Commission and became the first chairman of the Nordic Institute for Theoretical Physics in 1957.\n\nBohr was born in Copenhagen, Denmark, on 7 October 1885, the second of three children of Christian Bohr, a professor of physiology at the University of Copenhagen, and Ellen Adler Bohr, who came from a wealthy Danish Jewish family prominent in banking and parliamentary circles. He had an elder sister, Jenny, and a younger brother Harald. Jenny became a teacher, while Harald became a mathematician and Olympic footballer who played for the Danish national team at the 1908 Summer Olympics in London. Bohr was a passionate footballer as well, and the two brothers played several matches for the Copenhagen-based Akademisk Boldklub (Academic Football Club), with Bohr as goalkeeper.\nBohr was educated at Gammelholm Latin School, starting when he was seven. In 1903, Bohr enrolled as an undergraduate at Copenhagen University. His major was physics, which he studied under Professor Christian Christiansen, the university's only professor of physics at that time. He also studied astronomy and mathematics under Professor Thorvald Thiele, and philosophy under Professor Harald Høffding, a friend of his father.\n\nIn 1905, a gold medal competition was sponsored by the Royal Danish Academy of Sciences and Letters to investigate a method for measuring the surface tension of liquids that had been proposed by Lord Rayleigh in 1879. This involved measuring the frequency of oscillation of the radius of a water jet. Bohr conducted a series of experiments using his father's laboratory in the university; the university itself had no physics laboratory. To complete his experiments, he had to make his own glassware, creating test tubes with the required elliptical cross-sections. He went beyond the original task, incorporating improvements into both Rayleigh's theory and his method, by taking into account the viscosity of the water, and by working with finite amplitudes instead of just infinitesimal ones. His essay, which he submitted at the last minute, won the prize. He later submitted an improved version of the paper to the Royal Society in London for publication in the \"Philosophical Transactions of the Royal Society\".\n\nHarald became the first of the two Bohr brothers to earn a master's degree, which he earned for mathematics in April 1909. Niels took another nine months to earn his. Students had to submit a thesis on a subject assigned by their supervisor. Bohr's supervisor was Christiansen, and the topic he chose was the electron theory of metals. Bohr subsequently elaborated his master's thesis into his much-larger Doctor of Philosophy (dr. phil.) thesis. He surveyed the literature on the subject, settling on a model postulated by Paul Drude and elaborated by Hendrik Lorentz, in which the electrons in a metal are considered to behave like a gas. Bohr extended Lorentz's model, but was still unable to account for phenomena like the Hall effect, and concluded that electron theory could not fully explain the magnetic properties of metals. The thesis was accepted in April 1911, and Bohr conducted his formal defence on 13 May. Harald had received his doctorate the previous year. Bohr's thesis was groundbreaking, but attracted little interest outside Scandinavia because it was written in Danish, a Copenhagen University requirement at the time. In 1921, the Dutch physicist Hendrika Johanna van Leeuwen would independently derive a theorem from Bohr's thesis that is today known as the Bohr–van Leeuwen theorem.\nIn 1910, Bohr met Margrethe Nørlund, the sister of the mathematician Niels Erik Nørlund. Bohr resigned his membership in the Church of Denmark on 16 April 1912, and he and Margrethe were married in a civil ceremony at the town hall in Slagelse on 1 August. Years later, his brother Harald similarly left the church before getting married. Bohr and Margrethe had six sons. The oldest, Christian, died in a boating accident in 1934, and another, Harald, died from childhood meningitis. Aage Bohr became a successful physicist, and in 1975 was awarded the Nobel Prize in physics, like his father. became a physician; , a chemical engineer; and Ernest, a lawyer. Like his uncle Harald, Ernest Bohr became an Olympic athlete, playing field hockey for Denmark at the 1948 Summer Olympics in London.\n\nIn September 1911, Bohr, supported by a fellowship from the Carlsberg Foundation, travelled to England. At the time, it was where most of the theoretical work on the structure of atoms and molecules was being done. He met J. J. Thomson of the Cavendish Laboratory and Trinity College, Cambridge. He attended lectures on electromagnetism given by James Jeans and Joseph Larmor, and did some research on cathode rays, but failed to impress Thomson. He had more success with younger physicists like the Australian William Lawrence Bragg, and New Zealand's Ernest Rutherford, whose 1911 Rutherford model of the atom had challenged Thomson's 1904 plum pudding model. Bohr received an invitation from Rutherford to conduct post-doctoral work at Victoria University of Manchester, where Bohr met George de Hevesy and Charles Galton Darwin (whom Bohr referred to as \"the grandson of the real Darwin\").\n\nBohr returned to Denmark in July 1912 for his wedding, and travelled around England and Scotland on his honeymoon. On his return, he became a \"privatdocent\" at the University of Copenhagen, giving lectures on thermodynamics. Martin Knudsen put Bohr's name forward for a \"docent\", which was approved in July 1913, and Bohr then began teaching medical students. His three papers, which later became famous as \"the trilogy\", were published in \"Philosophical Magazine\" in July, September and November of that year. He adapted Rutherford's nuclear structure to Max Planck's quantum theory and so created his Bohr model of the atom.\n\nPlanetary models of atoms were not new, but Bohr's treatment was. Taking the 1912 paper by Darwin on the role of electrons in the interaction of alpha particles with a nucleus as his starting point, he advanced the theory of electrons travelling in orbits around the atom's nucleus, with the chemical properties of each element being largely determined by the number of electrons in the outer orbits of its atoms. He introduced the idea that an electron could drop from a higher-energy orbit to a lower one, in the process emitting a quantum of discrete energy. This became a basis for what is now known as the old quantum theory.\n\nIn 1885, Johann Balmer had come up with his Balmer series to describe the visible spectral lines of a hydrogen atom:\nwhere λ is the wavelength of the absorbed or emitted light and \"R\" is the Rydberg constant. Balmer's formula was corroborated by the discovery of additional spectral lines, but for thirty years, no one could explain why it worked. In the first paper of his trilogy, Bohr was able to derive it from his model:\nwhere \"m\" is the electron's mass, \"e\" is its charge, \"h\" is Planck's constant and \"Z\" is the atom's atomic number (1 for hydrogen).\n\nThe model's first hurdle was the Pickering series, lines which did not fit Balmer's formula. When challenged on this by Alfred Fowler, Bohr replied that they were caused by ionised helium, helium atoms with only one electron. The Bohr model was found to work for such ions. Many older physicists, like Thomson, Rayleigh and Hendrik Lorentz, did not like the trilogy, but the younger generation, including Rutherford, David Hilbert, Albert Einstein, Enrico Fermi, Max Born and Arnold Sommerfeld saw it as a breakthrough. The trilogy's acceptance was entirely due to its ability to explain phenomena which stymied other models, and to predict results that were subsequently verified by experiments. Today, the Bohr model of the atom has been superseded, but is still the best known model of the atom, as it often appears in high school physics and chemistry texts.\n\nBohr did not enjoy teaching medical students. He decided to return to Manchester, where Rutherford had offered him a job as a reader in place of Darwin, whose tenure had expired. Bohr accepted. He took a leave of absence from the University of Copenhagen, which he started by taking a holiday in Tyrol with his brother Harald and aunt Hanna Adler. There, he visited the University of Göttingen and the Ludwig Maximilian University of Munich, where he met Sommerfeld and conducted seminars on the trilogy. The First World War broke out while they were in Tyrol, greatly complicating the trip back to Denmark and Bohr's subsequent voyage with Margrethe to England, where he arrived in October 1914. They stayed until July 1916, by which time he had been appointed to the Chair of Theoretical Physics at the University of Copenhagen, a position created especially for him. His docentship was abolished at the same time, so he still had to teach physics to medical students. New professors were formally introduced to King Christian X, who expressed his delight at meeting such a famous football player.\n\nIn April 1917, Bohr began a campaign to establish an Institute of Theoretical Physics. He gained the support of the Danish government and the Carlsberg Foundation, and sizeable contributions were also made by industry and private donors, many of them Jewish. Legislation establishing the Institute was passed in November 1918. Now known as the Niels Bohr Institute, it opened on 3 March 1921, with Bohr as its director. His family moved into an apartment on the first floor. Bohr's institute served as a focal point for researchers into quantum mechanics and related subjects in the 1920s and 1930s, when most of the world's best known theoretical physicists spent some time in his company. Early arrivals included Hans Kramers from the Netherlands, Oskar Klein from Sweden, George de Hevesy from Hungary, Wojciech Rubinowicz from Poland and Svein Rosseland from Norway. Bohr became widely appreciated as their congenial host and eminent colleague. Klein and Rosseland produced the Institute's first publication even before it opened.\nThe Bohr model worked well for hydrogen, but could not explain more complex elements. By 1919, Bohr was moving away from the idea that electrons orbited the nucleus and developed heuristics to describe them. The rare-earth elements posed a particular classification problem for chemists, because they were so chemically similar. An important development came in 1924 with Wolfgang Pauli's discovery of the Pauli exclusion principle, which put Bohr's models on a firm theoretical footing. Bohr was then able to declare that the as-yet-undiscovered element 72 was not a rare-earth element, but an element with chemical properties similar to those of zirconium. He was immediately challenged by the French chemist Georges Urbain, who claimed to have discovered a rare-earth element 72, which he called \"celtium\". At the Institute in Copenhagen, Dirk Coster and George de Hevesy took up the challenge of proving Bohr right and Urbain wrong. Starting with a clear idea of the chemical properties of the unknown element greatly simplified the search process. They went through samples from Copenhagen's Museum of Mineralogy looking for a zirconium-like element and soon found it. The element, which they named hafnium (\"Hafnia\" being the Latin name for Copenhagen) turned out to be more common than gold.\n\nIn 1922, Bohr was awarded the Nobel Prize in Physics \"for his services in the investigation of the structure of atoms and of the radiation emanating from them\". The award thus recognised both the Trilogy and his early leading work in the emerging field of quantum mechanics. For his Nobel lecture, Bohr gave his audience a comprehensive survey of what was then known about the structure of the atom, including the correspondence principle, which he had formulated. This states that the behaviour of systems described by quantum theory reproduces classical physics in the limit of large quantum numbers.\n\nThe discovery of Compton scattering by Arthur Holly Compton in 1923 convinced most physicists that light was composed of photons, and that energy and momentum were conserved in collisions between electrons and photons. In 1924, Bohr, Kramers and John C. Slater, an American physicist working at the Institute in Copenhagen, proposed the Bohr–Kramers–Slater theory (BKS). It was more a programme than a full physical theory, as the ideas it developed were not worked out quantitatively. BKS theory became the final attempt at understanding the interaction of matter and electromagnetic radiation on the basis of the old quantum theory, in which quantum phenomena were treated by imposing quantum restrictions on a classical wave description of the electromagnetic field.\n\nModelling atomic behaviour under incident electromagnetic radiation using \"virtual oscillators\" at the absorption and emission frequencies, rather than the (different) apparent frequencies of the Bohr orbits, led Max Born, Werner Heisenberg and Kramers to explore different mathematical models. They led to the development of matrix mechanics, the first form of modern quantum mechanics. The BKS theory also generated discussion of, and renewed attention to, difficulties in the foundations of the old quantum theory. The most provocative element of BKS – that momentum and energy would not necessarily be conserved in each interaction, but only statistically – was soon shown to be in conflict with experiments conducted by Walther Bothe and Hans Geiger. In light of these results, Bohr informed Darwin that \"there is nothing else to do than to give our revolutionary efforts as honourable a funeral as possible\".\n\nThe introduction of spin by George Uhlenbeck and Samuel Goudsmit in November 1925 was a milestone. The next month, Bohr travelled to Leiden to attend celebrations of the 50th anniversary of Hendrick Lorentz receiving his doctorate. When his train stopped in Hamburg, he was met by Wolfgang Pauli and Otto Stern, who asked for his opinion of the spin theory. Bohr pointed out that he had concerns about the interaction between electrons and magnetic fields. When he arrived in Leiden, Paul Ehrenfest and Albert Einstein informed Bohr that Einstein had resolved this problem using relativity. Bohr then had Uhlenbeck and Goudsmit incorporate this into their paper. Thus, when he met Werner Heisenberg and Pascual Jordan in Göttingen on the way back, he had become, in his own words, \"a prophet of the electron magnet gospel\".\n\nHeisenberg first came to Copenhagen in 1924, then returned to Göttingen in June 1925, shortly thereafter developing the mathematical foundations of quantum mechanics. When he showed his results to Max Born in Göttingen, Born realised that they could best be expressed using matrices. This work attracted the attention of the British physicist Paul Dirac, who came to Copenhagen for six months in September 1926. Austrian physicist Erwin Schrödinger also visited in 1926. His attempt at explaining quantum physics in classical terms using wave mechanics impressed Bohr, who believed it contributed \"so much to mathematical clarity and simplicity that it represents a gigantic advance over all previous forms of quantum mechanics\".\n\nWhen Kramers left the Institute in 1926 to take up a chair as professor of theoretical physics at the Utrecht University, Bohr arranged for Heisenberg to return and take Kramers's place as a \"lektor\" at the University of Copenhagen. Heisenberg worked in Copenhagen as a university lecturer and assistant to Bohr from 1926 to 1927.\n\nBohr became convinced that light behaved like both waves and particles, and in 1927, experiments confirmed the de Broglie hypothesis that matter (like electrons) also behaved like waves. He conceived the philosophical principle of complementarity: that items could have apparently mutually exclusive properties, such as being a wave or a stream of particles, depending on the experimental framework. He felt that it was not fully understood by professional philosophers.\n\nIn Copenhagen in 1927 Heisenberg developed his uncertainty principle, which Bohr embraced. In a paper he presented at the Volta Conference at Como in September 1927, he demonstrated that the uncertainty principle could be derived from classical arguments, without quantum terminology or matrices. Einstein preferred the determinism of classical physics over the probabilistic new quantum physics to which he himself had contributed. Philosophical issues that arose from the novel aspects of quantum mechanics became widely celebrated subjects of discussion. Einstein and Bohr had good-natured arguments over such issues throughout their lives.\n\nIn 1914, Carl Jacobsen, the heir to Carlsberg breweries, bequeathed his mansion to be used for life by the Dane who had made the most prominent contribution to science, literature or the arts, as an honorary residence (). Harald Høffding had been the first occupant, and upon his death in July 1931, the Royal Danish Academy of Sciences and Letters gave Bohr occupancy. He and his family moved there in 1932. He was elected president of the Academy on 17 March 1939.\n\nBy 1929, the phenomenon of beta decay prompted Bohr to again suggest that the law of conservation of energy be abandoned, but Enrico Fermi's hypothetical neutrino and the subsequent 1932 discovery of the neutron provided another explanation. This prompted Bohr to create a new theory of the compound nucleus in 1936, which explained how neutrons could be captured by the nucleus. In this model, the nucleus could be deformed like a drop of liquid. He worked on this with a new collaborator, the Danish physicist Fritz Kalckar, who died suddenly in 1938.\n\nThe discovery of nuclear fission by Otto Hahn in December 1938 (and its theoretical explanation by Lise Meitner) generated intense interest among physicists. Bohr brought the news to the United States where he opened the Fifth Washington Conference on Theoretical Physics with Fermi on 26 January 1939. When Bohr told George Placzek that this resolved all the mysteries of transuranic elements, Placzek told him that one remained: the neutron capture energies of uranium did not match those of its decay. Bohr thought about it for a few minutes and then announced to Placzek, Léon Rosenfeld and John Wheeler that \"I have understood everything.\" Based on his liquid drop model of the nucleus, Bohr concluded that it was the uranium-235 isotope and not the more abundant uranium-238 that was primarily responsible for fission with thermal neutrons. In April 1940, John R. Dunning demonstrated that Bohr was correct. In the meantime, Bohr and Wheeler developed a theoretical treatment which they published in a September 1939 paper on \"The Mechanism of Nuclear Fission\".\n\nBohr read the 19th-century Danish Christian existentialist philosopher, Søren Kierkegaard. Richard Rhodes argued in \"The Making of the Atomic Bomb\" that Bohr was influenced by Kierkegaard through Høffding. In 1909, Bohr sent his brother Kierkegaard's \"Stages on Life's Way\" as a birthday gift. In the enclosed letter, Bohr wrote, \"It is the only thing I have to send home; but I do not believe that it would be very easy to find anything better ... I even think it is one of the most delightful things I have ever read.\" Bohr enjoyed Kierkegaard's language and literary style, but mentioned that he had some disagreement with Kierkegaard's philosophy. Some of Bohr's biographers suggested that this disagreement stemmed from Kierkegaard's advocacy of Christianity, while Bohr was an atheist.\n\nThere has been some dispute over the extent to which Kierkegaard influenced Bohr's philosophy and science. David Favrholdt argued that Kierkegaard had minimal influence over Bohr's work, taking Bohr's statement about disagreeing with Kierkegaard at face value, while Jan Faye argued that one can disagree with the content of a theory while accepting its general premises and structure.\n\nThe rise of Nazism in Germany prompted many scholars to flee their countries, either because they were Jewish or because they were political opponents of the Nazi regime. In 1933, the Rockefeller Foundation created a fund to help support refugee academics, and Bohr discussed this programme with the President of the Rockefeller Foundation, Max Mason, in May 1933 during a visit to the United States. Bohr offered the refugees temporary jobs at the Institute, provided them with financial support, arranged for them to be awarded fellowships from the Rockefeller Foundation, and ultimately found them places at institutions around the world. Those that he helped included Guido Beck, Felix Bloch, James Franck, George de Hevesy, Otto Frisch, Hilde Levi, Lise Meitner, George Placzek, Eugene Rabinowitch, Stefan Rozental, Erich Ernst Schneider, Edward Teller, Arthur von Hippel and Victor Weisskopf.\n\nIn April 1940, early in the Second World War, Nazi Germany invaded and occupied Denmark. To prevent the Germans from discovering Max von Laue's and James Franck's gold Nobel medals, Bohr had de Hevesy dissolve them in aqua regia. In this form, they were stored on a shelf at the Institute until after the war, when the gold was precipitated and the medals re-struck by the Nobel Foundation. Bohr kept the Institute running, but all the foreign scholars departed.\n\nBohr was aware of the possibility of using uranium-235 to construct an atomic bomb, referring to it in lectures in Britain and Denmark shortly before and after the war started, but he did not believe that it was technically feasible to extract a sufficient quantity of uranium-235. In September 1941, Heisenberg, who had become head of the German nuclear energy project, visited Bohr in Copenhagen. During this meeting the two men took a private moment outside, the content of which has caused much speculation, as both gave differing accounts.\nAccording to Heisenberg, he began to address nuclear energy, morality and the war, to which Bohr seems to have reacted by terminating the conversation abruptly while not giving Heisenberg hints about his own opinions. Ivan Supek, one of Heisenberg's students and friends, claimed that the main subject of the meeting was Carl Friedrich von Weizsäcker, who had proposed trying to persuade Bohr to mediate peace between Britain and Germany.\n\nIn 1957, Heisenberg wrote to Robert Jungk, who was then working on the book \"\". Heisenberg explained that he had visited Copenhagen to communicate to Bohr the views of several German scientists, that production of a nuclear weapon was possible with great efforts, and this raised enormous responsibilities on the world's scientists on both sides. When Bohr saw Jungk's depiction in the Danish translation of the book, he drafted (but never sent) a letter to Heisenberg, stating that he never understood the purpose of Heisenberg's visit, was shocked by Heisenberg's opinion that Germany would win the war, and that atomic weapons could be decisive.\n\nMichael Frayn's 1998 play \"Copenhagen\" explores what might have happened at the 1941 meeting between Heisenberg and Bohr. A BBC television film version of the play was first screened on 26 September 2002, with Stephen Rea as Bohr, and Daniel Craig as Heisenberg. The same meeting had previously been dramatised by the BBC's \"Horizon\" science documentary series in 1992, with Anthony Bate as Bohr, and Philip Anthony as Heisenberg. The meeting is also dramatized in the Norwegian/Danish/British miniseries \"The Heavy Water War\".\n\nIn September 1943, word reached Bohr and his brother Harald that the Nazis considered their family to be Jewish, since their mother was Jewish, and that they were therefore in danger of being arrested. The Danish resistance helped Bohr and his wife escape by sea to Sweden on 29 September. The next day, Bohr persuaded King Gustaf V of Sweden to make public Sweden's willingness to provide asylum to Jewish refugees. On 2 October 1943, Swedish radio broadcast that Sweden was ready to offer asylum, and the mass rescue of the Danish Jews by their countrymen followed swiftly thereafter. Some historians claim that Bohr's actions led directly to the mass rescue, while others say that, though Bohr did all that he could for his countrymen, his actions were not a decisive influence on the wider events. Eventually, over 7,000 Danish Jews escaped to Sweden.\nWhen the news of Bohr's escape reached Britain, Lord Cherwell sent a telegram to Bohr asking him to come to Britain. Bohr arrived in Scotland on 6 October in a de Havilland Mosquito operated by the British Overseas Airways Corporation (BOAC). The Mosquitos were unarmed high-speed bomber aircraft that had been converted to carry small, valuable cargoes or important passengers. By flying at high speed and high altitude, they could cross German-occupied Norway, and yet avoid German fighters. Bohr, equipped with parachute, flying suit and oxygen mask, spent the three-hour flight lying on a mattress in the aircraft's bomb bay. During the flight, Bohr did not wear his flying helmet as it was too small, and consequently did not hear the pilot's intercom instruction to turn on his oxygen supply when the aircraft climbed to high altitude to overfly Norway. He passed out from oxygen starvation and only revived when the aircraft descended to lower altitude over the North Sea. Bohr's son Aage followed his father to Britain on another flight a week later, and became his personal assistant.\n\nBohr was warmly received by James Chadwick and Sir John Anderson, but for security reasons Bohr was kept out of sight. He was given an apartment at St James's Palace and an office with the British Tube Alloys nuclear weapons development team. Bohr was astonished at the amount of progress that had been made. Chadwick arranged for Bohr to visit the United States as a Tube Alloys consultant, with Aage as his assistant. On 8 December 1943, Bohr arrived in Washington, D.C., where he met with the director of the Manhattan Project, Brigadier General Leslie R. Groves, Jr. He visited Einstein and Pauli at the Institute for Advanced Study in Princeton, New Jersey, and went to Los Alamos in New Mexico, where the nuclear weapons were being designed. For security reasons, he went under the name of \"Nicholas Baker\" in the United States, while Aage became \"James Baker\". In May 1944 the Danish resistance newspaper De frie Danske reported that they had learned that 'the famous son of Denmark Professor Niels Bohr' in October the previous year had fled his country via Sweden to London and from there travelled to Moscow from where he could be assumed to support the war effort.\n\nBohr did not remain at Los Alamos, but paid a series of extended visits over the course of the next two years. Robert Oppenheimer credited Bohr with acting \"as a scientific father figure to the younger men\", most notably Richard Feynman. Bohr is quoted as saying, \"They didn't need my help in making the atom bomb.\" Oppenheimer gave Bohr credit for an important contribution to the work on modulated neutron initiators. \"This device remained a stubborn puzzle,\" Oppenheimer noted, \"but in early February 1945 Niels Bohr clarified what had to be done.\"\n\nBohr recognised early that nuclear weapons would change international relations. In April 1944, he received a letter from Peter Kapitza, written some months before when Bohr was in Sweden, inviting him to come to the Soviet Union. The letter convinced Bohr that the Soviets were aware of the Anglo-American project, and would strive to catch up. He sent Kapitza a non-committal response, which he showed to the authorities in Britain before posting. Bohr met Churchill on 16 May 1944, but found that \"we did not speak the same language\". Churchill disagreed with the idea of openness towards the Russians to the point that he wrote in a letter: \"It seems to me Bohr ought to be confined or at any rate made to see that he is very near the edge of mortal crimes.\"\n\nOppenheimer suggested that Bohr visit President Franklin D. Roosevelt to convince him that the Manhattan Project should be shared with the Soviets in the hope of speeding up its results. Bohr's friend, Supreme Court Justice Felix Frankfurter, informed President Roosevelt about Bohr's opinions, and a meeting between them took place on 26 August 1944. Roosevelt suggested that Bohr return to the United Kingdom to try to win British approval. When Churchill and Roosevelt met at Hyde Park on 19 September 1944, they rejected the idea of informing the world about the project, and the aide-mémoire of their conversation contained a rider that \"enquiries should be made regarding the activities of Professor Bohr and steps taken to ensure that he is responsible for no leakage of information, particularly to the Russians\".\n\nIn June 1950, Bohr addressed an \"Open Letter\" to the United Nations calling for international cooperation on nuclear energy. In the 1950s, after the Soviet Union's first nuclear weapon test, the International Atomic Energy Agency was created along the lines of Bohr's suggestion. In 1957 he received the first ever Atoms for Peace Award.\n\nWith the war now ended, Bohr returned to Copenhagen on 25 August 1945, and was re-elected President of the Royal Danish Academy of Arts and Sciences on 21 September. At a memorial meeting of the Academy on 17 October 1947 for King Christian X, who had died in April, the new king, Frederick IX, announced that he was conferring the Order of the Elephant on Bohr. This award was normally awarded only to royalty and heads of state, but the king said that it honoured not just Bohr personally, but Danish science. Bohr designed his own coat of arms which featured a taijitu (symbol of yin and yang) and a motto in , \"opposites are complementary\".\n\nThe Second World War demonstrated that science, and physics in particular, now required considerable financial and material resources. To avoid a brain drain to the United States, twelve European countries banded together to create CERN, a research organisation along the lines of the national laboratories in the United States, designed to undertake Big Science projects beyond the resources of any one of them alone. Questions soon arose regarding the best location for the facilities. Bohr and Kramers felt that the Institute in Copenhagen would be the ideal site. Pierre Auger, who organised the preliminary discussions, disagreed; he felt that both Bohr and his Institute were past their prime, and that Bohr's presence would overshadow others. After a long debate, Bohr pledged his support to CERN in February 1952, and Geneva was chosen as the site in October. The CERN Theory Group was based in Copenhagen until their new accommodation in Geneva was ready in 1957. Victor Weisskopf, who later became the Director General of CERN, summed up Bohr's role, saying that \"there were other personalities who started and conceived the idea of CERN. The enthusiasm and ideas of the other people would not have been enough, however, if a man of his stature had not supported it.\"\n\nMeanwhile, Scandinavian countries formed the Nordic Institute for Theoretical Physics in 1957, with Bohr as its chairman. He was also involved with the founding of the Research Establishment Risø of the Danish Atomic Energy Commission, and served as its first chairman from February 1956.\n\nBohr died of heart failure at his home in Carlsberg on 18 November 1962. He was cremated, and his ashes were buried in the family plot in the Assistens Cemetery in the Nørrebro section of Copenhagen, along with those of his parents, his brother Harald, and his son Christian. Years later, his wife's ashes were also interred there. On 7 October 1965, on what would have been his 80th birthday, the Institute for Theoretical Physics at the University of Copenhagen was officially renamed to what it had been called unofficially for many years: the Niels Bohr Institute.\n\nBohr received numerous honours and accolades. In addition to the Nobel Prize, he received the Hughes Medal in 1921, the Matteucci Medal in 1923, the Franklin Medal in 1926, the Copley Medal in 1938, the Order of the Elephant in 1947, the Atoms for Peace Award in 1957 and the Sonning Prize in 1961. He became foreign member of the Royal Netherlands Academy of Arts and Sciences in 1923, and of the Royal Society in 1926. The Bohr model's semicentennial was commemorated in Denmark on 21 November 1963 with a postage stamp depicting Bohr, the hydrogen atom and the formula for the difference of any two hydrogen energy levels: formula_3. Several other countries have also issued postage stamps depicting Bohr. In 1997, the Danish National Bank began circulating the 500-krone banknote with the portrait of Bohr smoking a pipe. An asteroid, 3948 Bohr, was named after him, as was the Bohr lunar crater and bohrium, the chemical element with atomic number 107.\n\n\n\n"}
{"id": "21211", "url": "https://en.wikipedia.org/wiki?curid=21211", "title": "National Football League", "text": "National Football League\n\nThe National Football League (NFL) is a professional American football league consisting of 32 teams, divided equally between the National Football Conference (NFC) and the American Football Conference (AFC). The NFL is one of the four major professional sports leagues in North America, and the highest professional level of American football in the world. The NFL's 17-week regular season runs from early September to late December, with each team playing 16 games and having one bye week. Following the conclusion of the regular season, six teams from each conference (four division winners and two wild card teams) advance to the playoffs, a single-elimination tournament culminating in the Super Bowl, which is usually held in the first Sunday in February, and is played between the champions of the NFC and AFC.\n\nThe NFL was formed in 1920 as the American Professional Football Association (APFA) before renaming itself the National Football League for the 1922 season. The NFL agreed to merge with the American Football League (AFL) in 1966, and the first Super Bowl was held at the end of that season; the merger was completed in 1970. Today, the NFL has the highest average attendance (67,591) of any professional sports league in the world and is the most popular sports league in the United States. The Super Bowl is among the biggest club sporting events in the world and individual Super Bowl games account for many of the most watched television programs in American history, all occupying the Nielsen's Top 5 tally of the all-time most watched U.S. television broadcasts by 2015. The NFL's executive officer is the commissioner, who has broad authority in governing the league. The players in the league belong to the National Football League Players Association.\n\nThe team with the most NFL championships is the Green Bay Packers with thirteen (nine NFL titles before the Super Bowl era, and four Super Bowl championships afterwards); the team with the most Super Bowl championships is the Pittsburgh Steelers with six. The current NFL champions are the Philadelphia Eagles, who defeated the New England Patriots in Super Bowl LII, their first Super Bowl championship after winning three NFL titles before the Super Bowl era.\n\nOn August 20, 1920, a meeting was held by representatives of the Akron Pros, Canton Bulldogs, Cleveland Indians, and Dayton Triangles at the Jordan and Hupmobile auto showroom in Canton, Ohio. This meeting resulted in the formation of the American Professional Football Conference (APFC), a group who, according to the \"Canton Evening Repository\", intended to \"raise the standard of professional football in every way possible, to eliminate bidding for players between rival clubs and to secure cooperation in the formation of schedules\". \n\nAnother meeting was held on September 17, 1920 with representatives from teams from four states-Akron, Canton, Cleveland, and Dayton from Ohio; the Hammond Pros and Muncie Flyers from Indiana; the Rochester Jeffersons from New York; and the Rock Island Independents, Decatur Staleys, and Racine (Chicago) Cardinals from Illinois. The league was renamed to the American Professional Football Association (APFA). The league elected Jim Thorpe as its first president, and consisted of 14 teams (the Buffalo All-Americans, Chicago Tigers, Columbus Panhandles, and Detroit Heralds joined the league during the year). The Massillon Tigers from Massillon, Ohio was also at the September 17 meeting, but did not field a team in 1920. Only two of these teams, the Decatur Staleys (now the Chicago Bears) and the Chicago Cardinals (now the Arizona Cardinals), remain.\nAlthough the league did not maintain official standings for its 1920 inaugural season and teams played schedules that included non-league opponents, the APFA awarded the Akron Pros the championship by virtue of their 8–0–3 (8 wins, 0 losses, and 3 ties) record. The first event occurred on September 26, 1920 when the Rock Island Independents defeated the non-league St. Paul Ideals 48–0 at Douglas Park. On October 3, 1920, the first full week of league play occurred.\nThe following season resulted in the Chicago Staleys controversially winning the title over the Buffalo All-Americans. On June 24, 1922, the APFA changed its name to the National Football League (NFL).\n\nIn 1932, the season ended with the Chicago Bears (6–1–6) and the Portsmouth Spartans (6–1–4) tied for first in the league standings. At the time, teams were ranked on a single table and the team with the highest winning percentage (not including ties, which were not counted towards the standings) at the end of the season was declared the champion; the only tiebreaker was that in the event of a tie, if two teams played twice in a season, the result of the second game determined the title (the source of the 1921 controversy). This method had been used since the league's creation in 1920, but no situation had been encountered where two teams were tied for first. The league quickly determined that a playoff game between Chicago and Portsmouth was needed to decide the league's champion. The teams were originally scheduled to play the playoff game, officially a regular season game that would count towards the regular season standings, at Wrigley Field in Chicago, but a combination of heavy snow and extreme cold forced the game to be moved indoors to Chicago Stadium, which did not have a regulation-size football field. Playing with altered rules to accommodate the smaller playing field, the Bears won the game 9–0 and thus won the championship. Fan interest in the \"de facto\" championship game led the NFL, beginning in 1933, to split into two divisions with a championship game to be played between the division champions. The 1934 season also marked the first of 12 seasons in which African Americans were absent from the league. The \"de facto\" ban was rescinded in 1946, following public pressure and coinciding with the removal of a similar ban in Major League Baseball.\n\nThe NFL was always the foremost professional football league in the United States; it nevertheless faced a large number of rival professional leagues through the 1930s and 1940s. Rival leagues included at least three separate American Football Leagues and the All-America Football Conference (AAFC), on top of various regional leagues of varying caliber. Three NFL teams trace their histories to these rival leagues, including the Los Angeles Rams (who came from a 1936 iteration of the American Football League), the Cleveland Browns and San Francisco 49ers (the last two of which came from the AAFC). By the 1950s, the NFL had an effective monopoly on professional football in the United States; its only competition in North America was the professional Canadian football circuit, which formally became the Canadian Football League (CFL) in 1958. With Canadian football being a different football code than the American game, the CFL established a niche market in Canada and still survives as an independent league.\n\nA new professional league, the fourth American Football League (AFL), began play in 1960. The upstart AFL began to challenge the established NFL in popularity, gaining lucrative television contracts and engaging in a bidding war with the NFL for free agents and draft picks. The two leagues announced a merger on June 8, 1966, to take full effect in 1970. In the meantime, the leagues would hold a common draft and championship game. The game, the Super Bowl, was held four times before the merger, with the NFL winning Super Bowl I and Super Bowl II, and the AFL winning Super Bowl III and Super Bowl IV. After the league merged, it was reorganized into two conferences: the National Football Conference (NFC), consisting of most of the pre-merger NFL teams, and the American Football Conference (AFC), consisting of all of the AFL teams as well as three pre-merger NFL teams.\n\nToday, the NFL is considered the most popular sports league in North America; much of its growth is attributed to former Commissioner Pete Rozelle, who led the league from 1960 to 1989. Overall annual attendance increased from three million at the beginning of his tenure to seventeen million by the end of his tenure, and 400 million viewers watched 1989's Super Bowl XXIII. The NFL established NFL Properties in 1963. The league's licensing wing, NFL Properties earns the league billions of dollars annually; Rozelle's tenure also marked the creation of NFL Charities and a national partnership with United Way. Paul Tagliabue was elected as commissioner to succeed Rozelle; his seventeen-year tenure, which ended in 2006, was marked by large increases in television contracts and the addition of four expansion teams, as well as the introduction of league initiatives to increase the number of minorities in league and team management roles. The league's current Commissioner, Roger Goodell, has focused on reducing the number of illegal hits and making the sport safer, mainly through fining or suspending players who break rules. These actions are among many the NFL is taking to reduce concussions and improve player safety.\n\nFrom 1920 to 1934, the NFL did not have a set number of games for teams to play, instead setting a minimum. The league mandated a 12-game regular season for each team beginning in 1935, later shortening this to 11 games in 1937 and 10 games in 1943, mainly due to World War II. After the war ended, the number of games returned to 11 games in 1946 and to 12 in 1947. The NFL went to a 14-game schedule in 1961, which it retained until switching to the current 16-game schedule in 1978. Proposals to increase the regular season to 18 games have been made, but have been rejected in labor negotiations with the National Football League Players Association (NFLPA).\n\nThe NFL operated in a two-conference system from 1933 to 1966, where the champions of each conference would meet in the NFL Championship Game. If two teams tied for the conference lead, they would meet in a one-game playoff to determine the conference champion. In 1967, the NFL expanded from 15 teams to 16 teams. Instead of just evening out the conferences by adding the expansion New Orleans Saints to the seven-member Western Conference, the NFL realigned the conferences and split each into two four-team divisions. The four division champions would meet in the NFL playoffs, a two-round playoff. The NFL also operated the Playoff Bowl (officially the Bert Bell Benefit Bowl) from 1960 to 1969. Effectively a third-place game, pitting the two conference runners-up against each other, the league considers Playoff Bowls to have been exhibitions rather than playoff games. The league discontinued the Playoff Bowl in 1970 due to its perception as a game for losers.\n\nFollowing the addition of the former AFL teams into the NFL in 1970, the NFL split into two conferences with three divisions each. The expanded league, now with twenty-six teams, would also feature an expanded eight-team playoff, the participants being the three division champions from each conference as well as one 'wild card' team (the team with the best win percentage) from each conference. In 1978, the league added a second wild card team from each conference, bringing the total number of playoff teams to ten, and a further two wild card teams were added in 1990 to bring the total to twelve. When the NFL expanded to 32 teams in 2002, the league realigned, changing the division structure from three divisions in each conference to four divisions in each conference. As each division champion gets a playoff bid, the number of wild card teams from each conference dropped from three to two.\n\nAt the corporate level, the National Football League considers itself a trade association made up of and financed by its 32 member teams.<ref name=\"https://www.nytimes.com/2008/08/12/sports/football/12nfltax.html\"></ref> Up until 2015, the league was an unincorporated nonprofit 501(c)(6) association. Section 501(c)(6) of the Internal Revenue Code provides an exemption from federal income taxation for \"Business leagues, chambers of commerce, real-estate boards, boards of trade, or professional football leagues (whether or not administering a pension fund for football players), not organized for profit and no part of the net earnings of which inures to the benefit of any private shareholder or individual.\". In contrast, each individual team (except the non-profit Green Bay Packers) is subject to tax because they make a profit.\n\nThe NFL gave up the tax exempt status in 2015 following public criticism; in a letter to the club owners, Commissioner Roger Goodell labeled it a \"distraction\", saying \"the effects of the tax exempt status of the league office have been mischaracterized repeatedly in recent years... Every dollar of income generated through television rights fees, licensing agreements, sponsorships, ticket sales, and other means is earned by the 32 clubs and is taxable there. This will remain the case even when the league office and Management Council file returns as taxable entities, and the change in filing status will make no material difference to our business.\" As a result, the league office might owe around US$10 million in income taxes, but it is no longer required to disclose the salaries of its executive officers.\n\nThe league has three defined officers: the commissioner, secretary, and treasurer. Each conference has one defined officer, the president, which is essentially an honorary position with few powers and mostly ceremonial duties (such as awarding the conference championship trophy).\n\nThe commissioner is elected by affirmative vote of two-thirds or 18 (whichever is greater) of the members of the league, while the president of each conference is elected by an affirmative vote of three-fourths or ten of the conference members. The commissioner appoints the secretary and treasurer and has broad authority in disputes between clubs, players, coaches, and employees. He is the \"principal executive officer\" of the NFL and also has authority in hiring league employees, negotiating television contracts, disciplining individuals that own part or all of an NFL team, clubs, or employed individuals of an NFL club if they have violated league bylaws or committed \"conduct detrimental to the welfare of the League or professional football\". The commissioner can, in the event of misconduct by a party associated with the league, suspend individuals, hand down a fine of up to US$500,000, cancel contracts with the league, and award or strip teams of draft picks.\n\nIn extreme cases, the commissioner can offer recommendations to the NFL's Executive Committee up to and including the \"cancellation or forfeiture\" of a club's franchise or any other action he deems necessary. The commissioner can also issue sanctions up to and including a lifetime ban from the league if an individual connected to the NFL has bet on games or failed to notify the league of conspiracies or plans to bet on or fix games. The current Commissioner of the National Football League is Roger Goodell, who was elected in 2006 after Paul Tagliabue, the previous commissioner, retired.\n\nAccording to economist Richard Wolff, the NFL redistributes its wealth to all NFL teams equally in contravention of the typical corporate structure. By redistributing profits to all teams the NFL is ensuring that one team will not dominate the league through excessive earnings.\n\nThe NFL consists of 32 clubs divided into two conferences of 16 teams in each. Each conference is divided into four divisions of four clubs in each. During the regular season, each team is allowed a maximum of 53 players on its roster; only 46 of these may be active (eligible to play) on game days. Each team can also have a 10-player practice squad separate from its main roster, but the practice squad may only be composed of players who were not active for at least nine games in any of their seasons in the league. A player can only be on a practice squad for a maximum of three seasons.\n\nEach NFL club is granted a franchise, the league's authorization for the team to operate in its home city. This franchise covers 'Home Territory' (the 75 miles surrounding the city limits, or, if the team is within 100 miles of another league city, half the distance between the two cities) and 'Home Marketing Area' (Home Territory plus the rest of the state the club operates in, as well as the area the team operates its training camp in for the duration of the camp). Each NFL member has the exclusive right to host professional football games inside its Home Territory and the exclusive right to advertise, promote, and host events in its Home Marketing Area. There are several exceptions to this rule, mostly relating to teams with close proximity to each other: the San Francisco 49ers and Oakland Raiders only have exclusive rights in their cities and share rights outside of it; and teams that operate in the same city (e.g. New York City and Los Angeles) or the same state (e.g. California, Florida, and Texas) share the rights to the city's Home Territory and the state's Home Marketing Area, respectively.\n\nEvery NFL team is based in the contiguous United States. Although no team is based in a foreign country, the Jacksonville Jaguars began playing one home game a year at Wembley Stadium in London, England, in 2013 as part of the NFL International Series. The Jaguars' agreement with Wembley was originally set to expire in 2016, but has since been extended through 2020. The Buffalo Bills played one home game every season at Rogers Centre in Toronto, Ontario, Canada, as part of the Bills Toronto Series from to . Mexico also hosted an NFL regular-season game, a 2005 game between the San Francisco 49ers and Arizona Cardinals known as \"Fútbol Americano\", and 39 international preseason games were played from 1986 to 2005 as part of the American Bowl series. The Raiders and Houston Texans played a game in Mexico City at Estadio Azteca on November 21, 2016.\n\nAccording to Forbes, the Dallas Cowboys, at approximately US$4 billion, are the most valuable NFL franchise and the most valuable sports team in the world. Also, all 32 NFL teams rank among the Top 50 most valuable sports teams in the world; and 14 of the NFL's owners are listed on the Forbes 400, the most of any sports league or organization.\n\nThe 32 teams are organized into eight geographic divisions of four teams each. These divisions are further organized into two conferences, the National Football Conference and the American Football Conference. The two-conference structure has its origins in a time when major American professional football was organized into two independent leagues, the National Football League and its younger rival, the American Football League. The leagues merged in the late 1960s, adopting the older league's name and reorganizing slightly to ensure the same number of teams in both conferences.\n\nThe NFL season format consists of a four-week preseason, a seventeen-week regular season (each team plays 16 games), and a twelve-team single-elimination playoff culminating in the Super Bowl, the league's championship game.\n\nThe NFL preseason begins with the Pro Football Hall of Fame Game, played at Fawcett Stadium in Canton.<ref name=\"NFL/Hall of Fame Game\"></ref> Each NFL team is required to schedule four preseason games, two of which must be at its home stadium, but the teams involved in the Hall of Fame game, as well as any teams playing in an American Bowl game, play five preseason games. Preseason games are exhibition matches and do not count towards regular-season totals. Because the preseason does not count towards standings, teams generally do not focus on winning games; instead, they are used by coaches to evaluate their teams and by players to show their performance, both to their current team and to other teams if they get cut. The quality of preseason games has been criticized by some fans, who dislike having to pay full price for exhibition games, as well as by some players and coaches, who dislike the risk of injury the games have, while others have felt the preseason is a necessary part of the NFL season.\n\nCurrently, the thirteen opponents each team faces over the 16-game regular season schedule are set using a pre-determined formula: The league runs a seventeen-week, 256-game regular season. Since 2001, the season has begun the week after Labor Day (first Monday in September) and concluded the week after Christmas. The opening game of the season is normally a home game on a Thursday for the league's defending champion.\n\nMost NFL games are played on Sundays, with a Monday night game typically held at least once a week and Thursday night games occurring on most weeks as well. NFL games are not normally played on Fridays or Saturdays until late in the regular season, as federal law prohibits professional football leagues from competing with college or high school football. Because high school and college teams typically play games on Friday and Saturday, respectively, the NFL cannot hold games on those days until the third Friday in December. While Saturday games late in the season are common, the league rarely holds Friday games, the most recent one being on Christmas Day in 2009. NFL games are rarely scheduled for Tuesday or Wednesday, and those days have only been used twice since 1948: in 2010, when a Sunday game was rescheduled to Tuesday due to a blizzard, and in 2012, when the Kickoff game was moved from Thursday to Wednesday to avoid conflict with the Democratic National Convention.\n\nNFL regular season matchups are determined according to a scheduling formula. Within a division, all four teams play fourteen out of their sixteen games against common opponents – two games (home and away) are played against the other three teams in the division, while one game is held against all the members of a division from the NFC and a division from the AFC as determined by a rotating cycle (three years for the conference the team is in, and four years in the conference they are not in). The other two games are intraconference games, determined by the standings of the previous year – for example, if a team finishes first in its division, it will play two other first-place teams in its conference, while a team that finishes last would play two other last-place teams in the conference. In total, each team plays sixteen games and has one bye week, where they do not play any games.\n\nAlthough a team's home and away opponents are known by the end of the previous year's regular season, the exact dates and times for NFL games are not determined until much later because the league has to account for, among other things, the Major League Baseball postseason and local events that could pose a scheduling conflict with NFL games. During the 2010 season, over 500,000 potential schedules were created by computers, 5,000 of which were considered \"playable schedules\" and were reviewed by the NFL's scheduling team. After arriving at what they felt was the best schedule out of the group, nearly 50 more potential schedules were developed to try and ensure that the chosen schedule would be the best possible one.\n\nFollowing the conclusion of the regular season, the NFL Playoffs, a twelve-team single elimination tournament, is then held. Six teams are selected from each conference: the winners of each of the four divisions as well as two wild card teams (the two remaining teams with the best overall record). These teams are seeded according to overall record, with the division champions always ranking higher than either of the wild card teams. The top two teams (seeded one and two) from each conference are awarded a bye week, while the remaining four teams (seeded 3–6) from each conference compete in the first round of the playoffs, the Wild Card round, with the third seed competing against the sixth seed and the fourth seed competing against the fifth seed. The winners of the Wild Card round advance to the Divisional Round, which matches the lower seeded team against the first seed and the higher seeded team against the second seed. The winners of those games then compete in the Conference Championships, with the higher remaining seed hosting the lower remaining seed. The AFC and NFC champions then compete in the Super Bowl to determine the league champion.\n\nThe only other postseason event hosted by the NFL is the Pro Bowl, the league's all-star game. Since 2009, the Pro Bowl has been held the week before the Super Bowl; in previous years, the game was held the week following the Super Bowl, but in an effort to boost ratings, the game was moved to the week before. Because of this, players from the teams participating in the Super Bowl are exempt from participating in the game. The Pro Bowl is not considered as competitive as a regular-season game because the biggest concern of teams is to avoid injuries to the players.\n\nThe National Football League has used three different trophies to honor its champion over its existence. The first trophy, the Brunswick-Balke Collender Cup, was donated to the NFL (then APFA) in 1920 by the Brunswick-Balke Collender Corporation. The trophy, the appearance of which is only known by its description as a \"silver loving cup\", was intended to be a traveling trophy and not to become permanent until a team had won at least three titles. The league awarded it to the Akron Pros, champions of the inaugural 1920 season; however, the trophy was discontinued and its current whereabouts are unknown.\n\nA second trophy, the Ed Thorp Memorial Trophy, was issued by the NFL from 1934 to 1969. The trophy's namesake, Ed Thorp, was a referee in the league and a friend to many early league owners; upon his death in 1934, the league created the trophy to honor him. In addition to the main trophy, which would be in the possession of the current league champion, the league issued a smaller replica trophy to each champion, who would maintain permanent control over it. The current location of the Ed Thorp Memorial Trophy, long thought to be lost, is believed to be possessed by the Green Bay Packers Hall of Fame.\n\nThe current trophy of the NFL is the Vince Lombardi Trophy. The Super Bowl trophy was officially renamed in 1970 after Vince Lombardi, who as head coach led the Green Bay Packers to victories in the first two Super Bowls. Unlike the previous trophies, a new Vince Lombardi Trophy is issued to each year's champion, who maintains permanent control of it. Lombardi Trophies are made by Tiffany & Co. out of sterling silver and are worth anywhere from US$25,000 to US$300,000. Additionally, each player on the winning team as well as coaches and personnel are awarded Super Bowl rings to commemorate their victory. The winning team chooses the company that makes the rings; each ring design varies, with the NFL mandating certain ring specifications (which have a degree of room for deviation), in addition to requiring the Super Bowl logo be on at least one side of the ring. The losing team are also awarded rings, which must be no more than half as valuable as the winners' rings, but those are almost never worn.\n\nThe conference champions receive trophies for their achievement. The champions of the NFC receive the George Halas Trophy, named after Chicago Bears founder George Halas, who is also considered as one of the co-founders of the NFL. The AFC champions receive the Lamar Hunt Trophy, named after Lamar Hunt, the founder of the Kansas City Chiefs and the principal founder of the American Football League. Players on the winning team also receive a conference championship ring.\n\nThe NFL recognizes a number of awards for its players and coaches at its annual NFL Honors presentation. The most prestigious award is the AP Most Valuable Player (MVP) award. Other major awards include the AP Offensive Player of the Year, AP Defensive Player of the Year, AP Comeback Player of the Year, and the AP Offensive and Defensive Rookie of the Year awards. Another prestigious award is the Walter Payton Man of the Year Award, which recognizes a player's off-field work in addition to his on-field performance. The NFL Coach of the Year award is the highest coaching award. The NFL also gives out weekly awards such as the FedEx Air & Ground NFL Players of the Week and the Pepsi MAX NFL Rookie of the Week awards.\n\nIn the United States, the National Football League has television contracts with four networks: CBS, ESPN, Fox, and NBC. Collectively, these contracts cover every regular season and postseason game. In general, CBS televises afternoon games in which the away team is an AFC team, and Fox carries afternoon games in which the away team belongs to the NFC. These afternoon games are not carried on all affiliates, as multiple games are being played at once; each network affiliate is assigned one game per time slot, according to a complicated set of rules. Since 2011, the league has reserved the right to give Sunday games that, under the contract, would normally air on one network to the other network (known as \"flexible scheduling\"). The only way to legally watch a regionally televised game not being carried on the local network affiliates is to purchase NFL Sunday Ticket, the league's out-of-market sports package, which is only available to subscribers to the DirecTV satellite service. The league also provides RedZone, an omnibus telecast that cuts to the most relevant plays in each game, live as they happen.\n\nIn addition to the regional games, the league also has packages of telecasts, mostly in prime time, that are carried nationwide. NBC broadcasts the primetime \"Sunday Night Football\" package', which includes the Thursday NFL Kickoff game that starts the regular season and a primetime Thanksgiving Day game. ESPN carries all Monday Night Football games. The NFL's own network, NFL Network, broadcasts a series titled \"Thursday Night Football\", which was originally exclusive to the network, but which in recent years has had several games simulcast on CBS (since 2014) and NBC (since 2016) (except the Thanksgiving and kickoff games, which remain exclusive to NBC). For the 2017 season, the NFL Network will broadcast 18 regular season games under its \"Thursday Night Football\" brand, 16 Thursday-evening contests (10 of which are simulcast on either NBC or CBS) as well as one of the NFL International Series games on a Sunday morning and one of the 2017 Christmas afternoon games. In addition, 10 of the Thursday night games will be streamed live on Amazon Prime. In 2017, the NFL games occupied the top three rates for a 30-second advertisement: $699,602 for Sunday Night Football, $550,709 for Thursday Night Football (NBC), and $549,791 for Thursday Night Football (CBS).\n\nThe Super Bowl television rights are rotated on a three-year basis between CBS, Fox, and NBC. In 2011, all four stations signed new nine-year contracts with the NFL, each running until 2022; CBS, Fox, and NBC are estimated by \"Forbes\" to pay a combined total of US$3 billion a year, while ESPN will pay US$1.9 billion a year. The league also has deals with Spanish-language broadcasters NBC Universo, Fox Deportes, and ESPN Deportes, which air Spanish language dubs of their respective English-language sister networks' games. The league's contracts do not cover preseason games, which individual teams are free to sell to local stations directly; a minority of preseason games are distributed among the league's national television partners.\n\nThrough the 2014 season, the NFL had a blackout policy in which games were 'blacked out' on local television in the home team's area if the home stadium was not sold out. Clubs could elect to set this requirement at only 85%, but they would have to give more ticket revenue to the visiting team; teams could also request a specific exemption from the NFL for the game. The vast majority of NFL games were not blacked out; only 6% of games were blacked out during the 2011 season, and only two games were blacked out in and none in . The NFL announced in March 2015 that it would suspend its blackout policy for at least the 2015 season. According to Nielsen, the NFL regular season since 2012 was watched by at least 200 million individuals, accounting for 80% of all television households in the United States and 69% of all potential viewers in the United States. NFL regular season games accounted for 31 out of the top 32 most-watched programs in the fall season and an NFL game ranked as the most-watched television show in all 17 weeks of the regular season. At the local level, NFL games were the highest-ranked shows in NFL markets 92% of the time. Super Bowls account for the 22 most-watched programs (based on total audience) in US history, including a record 167 million people that watched Super Bowl XLVIII, the conclusion to the 2013 season.\n\nIn addition to radio networks run by each NFL team, select NFL games are broadcast nationally by Westwood One (known as Dial Global for the 2012 season). These games are broadcast on over 500 networks, giving all NFL markets access to each primetime game. The NFL's deal with Westwood One was extended in 2012 and will run through 2017.\n\nSome broadcasting innovations have either been introduced or popularized during NFL telecasts. Among them, the Skycam camera system was used for the first time in a live telecast, at a 1984 preseason NFL game in San Diego between the Chargers and 49ers, and televised by CBS. Commentator John Madden famously used a telestrator during games between the early 1980s to the mid-2000s, boosting the device's popularity.\n\nThe NFL, as a one-time experiment, distributed the October 25, 2015 International Series game from Wembley Stadium in London between the Buffalo Bills and Jacksonville Jaguars. The game was live streamed on the Internet exclusively via Yahoo!, except for over-the-air broadcasts on the local CBS-TV affiliates in the Buffalo and Jacksonville markets.\n\nIn 2015, the NFL began sponsoring a series of public service announcements to bring attention to domestic abuse and sexual assault in response to what was seen as poor handling of incidents of violence by players.\nEach April (excluding 2014 when it took place in May), the NFL holds a draft of college players. The draft consists of seven rounds, with each of the 32 clubs getting one pick in each round. The draft order for non-playoff teams is determined by regular-season record; among playoff teams, teams are first ranked by the furthest round of the playoffs they reached, and then are ranked by regular-season record. For example, any team that reached the divisional round will be given a higher pick than any team that reached the conference championships, but will be given a lower pick than any team that did not make the divisional round. The Super Bowl champion always drafts last, and the losing team from the Super Bowl always drafts next-to-last. All potential draftees must be at least three years removed from high school in order to be eligible for the draft. Underclassmen that have met that criterion to be eligible for the draft must write an application to the NFL by January 15 renouncing their remaining college eligibility. Clubs can trade away picks for future draft picks, but cannot trade the rights to players they have selected in previous drafts.\n\nAside from the 32 picks each club gets, compensatory draft picks are given to teams that have lost more compensatory free agents than they have gained. These are spread out from rounds 3 to 7, and a total of 32 are given. Clubs are required to make their selection within a certain period of time, the exact time depending on which round the pick is made in. If they fail to do so on time, the clubs behind them can begin to select their players in order, but they do not lose the pick outright. This happened in the 2003 draft, when the Minnesota Vikings failed to make their selection on time. The Jacksonville Jaguars and Carolina Panthers were able to make their picks before the Vikings were able to use theirs. Selected players are only allowed to negotiate contracts with the team that picked them, but if they choose not to sign they become eligible for the next year's draft. Under the current collective bargaining contract, all contracts to drafted players must be four-year deals with a club option for a fifth. Contracts themselves are limited to a certain amount of money, depending on the exact draft pick the player was selected with. Players who were draft eligible but not picked in the draft are free to sign with any club.\n\nThe NFL operates several other drafts in addition to the NFL draft. The league holds a supplemental draft annually. Clubs submit emails to the league stating the player they wish to select and the round they will do so, and the team with the highest bid wins the rights to that player. The exact order is determined by a lottery held before the draft, and a successful bid for a player will result in the team forfeiting the rights to its pick in the equivalent round of the next NFL draft. Players are only eligible for the supplemental draft after being granted a petition for special eligibility. The league holds expansion drafts, the most recent happening in 2002 when the Houston Texans began play as an expansion team. Other drafts held by the league include an allocation draft in 1950 to allocate players from several teams that played in the dissolved All-America Football Conference and a supplemental draft in 1984 to give NFL teams the rights to players who had been eligible for the main draft but had not been drafted because they had signed contracts with the United States Football League or Canadian Football League.\n\nLike the other major sports leagues in the United States, the NFL maintains protocol for a disaster draft. In the event of a 'near disaster' (less than 15 players killed or disabled) that caused the club to lose a quarterback, they could draft one from a team with at least three quarterbacks. In the event of a 'disaster' (15 or more players killed or disabled) that results in a club's season being canceled, a restocking draft would be held. Neither of these protocols has ever had to be implemented.\n\nFree agents in the National Football League are divided into restricted free agents, who have three accrued seasons and whose current contract has expired, and unrestricted free agents, who have four or more accrued seasons and whose contract has expired. An accrued season is defined as \"six or more regular-season games on a club's active/inactive, reserved/injured or reserve/physically unable to perform lists\". Restricted free agents are allowed to negotiate with other clubs besides their former club, but the former club has the right to match any offer. If they choose not to, they are compensated with draft picks. Unrestricted free agents are free to sign with any club, and no compensation is owed if they sign with a different club.\n\nClubs are given one franchise tag to offer to any unrestricted free agent. The franchise tag is a one-year deal that pays the player 120% of his previous contract or no less than the average of the five highest-paid players at his position, whichever is greater. There are two types of franchise tags: exclusive tags, which do not allow the player to negotiate with other clubs, and non-exclusive tags, which allow the player to negotiate with other clubs but gives his former club the right to match any offer and two first-round draft picks if they decline to match it.\n\nClubs also have the option to use a transition tag, which is similar to the non-exclusive franchise tag but offers no compensation if the former club refuses to match the offer. Due to that stipulation, the transition tag is rarely used, even with the removal of the \"poison pill\" strategy (offering a contract with stipulations that the former club would be unable to match) that essentially ended the usage of the tag league-wide. Each club is subject to a salary cap, which is set at US$143.28 million for the 2015 season, US$10 million more than in 2014 and US$20 million more than in 2013. The salary cap for the 2016 NFL season was $155.27 million. The salary cap for the 2017 NFL season was about $167 million.\n\nMembers of clubs' practice squads, despite being paid by and working for their respective clubs, are also simultaneously a kind of free agent and are able to sign to any other club's active roster (provided their new club is not their previous club's next opponent within a set number of days) without compensation to their previous club; practice squad players cannot be signed to other clubs' practice squads, however, unless released by their original club first.\n\n\nExplanatory notes\nCitations\nBibliography\n"}
{"id": "21212", "url": "https://en.wikipedia.org/wiki?curid=21212", "title": "Nazi Germany", "text": "Nazi Germany\n\nNazi Germany is the common English name for Germany between 1933 and 1945, when Adolf Hitler and his Nazi Party (NSDAP) controlled the country through a dictatorship. Under Hitler's rule, Germany was transformed into a totalitarian state that controlled nearly all aspects of life via the \"Gleichschaltung\" legal process. The official name of the state was Deutsches Reich (\"German Reich\") until 1943 and Großdeutsches Reich (\"Greater German Reich\") from 1943 to 1945. Nazi Germany is also known as the Third Reich, from German \"Drittes Reich\", meaning \"Third Realm\" or \"Third Empire\", the first two being the Holy Roman Empire and the German Empire. The Nazi regime ended after the Allies defeated Germany in May 1945, ending World War II in Europe.\n\nHitler was appointed Chancellor of Germany by the President of the Weimar Republic, Paul von Hindenburg, on 30 January 1933. The NSDAP then began to eliminate all political opposition and consolidate its power. Hindenburg died on 2 August 1934 and Hitler became dictator of Germany by merging the offices and powers of the Chancellery and Presidency. A national referendum held 19 August 1934 confirmed Hitler as sole Führer (leader) of Germany. All power was centralised in Hitler's person and his word became the highest law. The government was not a coordinated, co-operating body, but a collection of factions struggling for power and Hitler's favour. In the midst of the Great Depression, the Nazis restored economic stability and ended mass unemployment using heavy military spending and a mixed economy. Extensive public works were undertaken, including the construction of \"Autobahnen\" (motorways). The return to economic stability boosted the regime's popularity.\n\nRacism, especially antisemitism, was a central feature of the regime. The Germanic peoples were considered by the Nazis to be the master race, the purest branch of the Aryan race. Discrimination and persecution against Jews and Romani or Gypsy people began in earnest after the seizure of power. The first concentration camps were established in March 1933. Jews and others deemed undesirable were imprisoned, and liberals, socialists, and communists were killed, imprisoned, or exiled. Christian churches and citizens that opposed Hitler's rule were oppressed, and many leaders imprisoned. Education focused on racial biology, population policy, and fitness for military service. Career and educational opportunities for women were curtailed. Recreation and tourism were organised via the Strength Through Joy program, and the 1936 Summer Olympics showcased Germany on the international stage. Propaganda Minister Joseph Goebbels made effective use of film, mass rallies, and Hitler's hypnotic oratory to influence public opinion. The government controlled artistic expression, promoting specific art forms and banning or discouraging others.\n\nThe Nazi regime dominated neighbours through military threats in the years leading up to war. Nazi Germany made increasingly aggressive territorial demands, threatening war if these were not met. It seized Austria and Czechoslovakia in 1938 and 1939. Hitler made a non-aggression pact with Joseph Stalin and invaded Poland in September 1939, launching World War II in Europe. By early 1941, Germany controlled much of Europe. \"Reichskommissariats\" took control of conquered areas and a German administration was established in what was left of Poland. Germany exploited the raw materials and labour of both its occupied territories and its allies. Millions of Jews and other peoples deemed undesirable by the state were imprisoned, murdered in Nazi concentration camps and extermination camps, or shot in the Holocaust, through war crimes, and other crimes against humanity.\n\nWhile the German invasion of the Soviet Union in 1941 was initially successful, the Soviet resurgence and entry of the US into the war meant the Wehrmacht lost the initiative on the Eastern Front in 1943 and by late 1944 had been pushed back to the pre-1939 border. Large-scale aerial bombing of Germany escalated in 1944 and the Axis powers were driven back in Eastern and Southern Europe. After the Allied invasion of France, Germany was conquered by the Soviet Union from the east and the other Allies from the west and capitulated in May 1945. Hitler's refusal to admit defeat led to massive destruction of German infrastructure and additional war-related deaths in the closing months of the war. The victorious Allies initiated a policy of denazification and put many of the surviving Nazi leadership on trial for war crimes at the Nuremberg trials.\n\nThe official name of the state was \"Deutsches Reich\" from 1933 to 1943 and \"Großdeutsches Reich\" from 1943 to 1945, while common English terms are \"Nazi Germany\" and \"Third Reich\". The latter, adopted by Nazi propaganda as \"Drittes Reich\", was first used in a 1923 book by Arthur Moeller van den Bruck. The book counted the Holy Roman Empire (962–1806) as the first Reich and the German Empire (1871–1918) as the second.\n\nGermany was known as the Weimar Republic during the years 1919 to 1933. It was a republic with a semi-presidential system. During its tenure, it faced numerous problems, including hyperinflation, political extremism including violence from both left- and right-wing paramilitaries, contentious relationships with the Allied victors of World War I, and a series of failed attempts at coalition government by divided political parties. Severe setbacks to the German economy began after the war ended, partly because of reparations payments required under the 1919 Treaty of Versailles. The government printed money to make the payments and to repay the country's war debt, but the resulting hyperinflation led to inflated prices for consumer goods, economic chaos, and food riots. When the government defaulted on their reparations payments in January 1923, French troops occupied German industrial areas along the Ruhr and widespread civil unrest followed. \n\nThe National Socialist German Workers' Party (\"Nationalsozialistische Deutsche Arbeiterpartei\", NSDAP; Nazi Party) was founded in 1920. It was the renamed successor of the German Workers' Party (DAP) formed one year earlier, and one of several far-right political parties then active in Germany. The NSDAP party platform included destruction of the Weimar Republic, rejection of the terms of the Treaty of Versailles, radical antisemitism, and anti-Bolshevism. They promised a strong central government, increased \"Lebensraum\" (\"living space\") for Germanic peoples, formation of a national community based on race, and racial cleansing via the active suppression of Jews, who would be stripped of their citizenship and civil rights. The Nazis proposed national and cultural renewal based upon the \"Völkisch\" movement. The party, especially its paramilitary organisation \"Sturmabteilung\" (SA; Storm Detachment; Brownshirts), used physical violence to advance and strengthen their political position, disrupting the meetings of rival organisations and attacking their members (as well as Jewish people) on the streets. Such far-right armed groups were common in Bavaria, and were tolerated by the sympathetic far-right state government of Gustav Ritter von Kahr.\n\nWhen the stock market in the United States crashed on 24 October 1929, the effect in Germany was dire. Millions were thrown out of work and several major banks collapsed. Hitler and the NSDAP prepared to take advantage of the emergency to gain support for their party. They promised to strengthen the economy and provide jobs. Many voters decided the NSDAP was capable of restoring order, quelling civil unrest, and improving Germany's international reputation. After the federal election of 1932, the NSDAP was the largest party in the Reichstag, holding 230 seats with 37.4 percent of the popular vote.\n\nAlthough the Nazis won the greatest share of the popular vote in the two Reichstag general elections of 1932, they did not have a majority. Hitler therefore led a short-lived coalition government formed with the German National People's Party. Under pressure from politicians, industrialists, and the business community, President Paul von Hindenburg appointed Hitler as Chancellor of Germany on 30 January 1933. This event is known as the \"Machtergreifung\" (\"seizure of power\"). \n\nOn the night of 27 February 1933, the Reichstag building was set afire. Marinus van der Lubbe, a Dutch communist, was found guilty of starting the blaze. Hitler proclaimed that the arson marked the start of a communist uprising. The Reichstag Fire Decree, imposed on 28 February 1933, rescinded most civil liberties, including rights of assembly and freedom of the press. The decree also allowed the police to detain people indefinitely without charges or a court order. The legislation was accompanied by a propaganda campaign that led to public support for the measure. Violent suppression of communists by the SA was undertaken nationwide and 4,000 members of the Communist Party of Germany were arrested. \n\nIn March 1933, the Enabling Act, an amendment to the Weimar Constitution, passed in the Reichstag by a vote of 444 to 94. This amendment allowed Hitler and his cabinet to pass laws—even laws that violated the constitution—without the consent of the president or the Reichstag. As the bill required a two-thirds majority to pass, the Nazis used intimidation tactics as well as the provisions of the Reichstag Fire Decree to keep several Social Democratic deputies from attending, and the Communists had already been banned. On 10 May, the government seized the assets of the Social Democrats, and they were banned on 22 June. On 21 June, the SA raided the offices of the German National People's Party – their former coalition partners – and they disbanded on 29 June. The remaining major political parties followed suit: the Bavarian People's Party, Centre Party, and the German People's Party all disbanded. On 14 July 1933 Germany became a one-party state with the passage of a law decreeing the NSDAP to be the sole legal party in Germany. The founding of new parties was also made illegal, and all remaining political parties which had not already been dissolved were banned. The Enabling Act would subsequently serve as the legal foundation for the dictatorship the NSDAP established. Further elections in November 1933, 1936 and 1938 were Nazi-controlled, with only members of the NSDAP and a small number of independents elected.\n\nIn the months following the seizure of power in January 1933, the Hitler cabinet used the terms of the Reichstag Fire Decree and later the Enabling Act to initiate the process of \"Gleichschaltung\" (\"co-ordination\"), which brought all aspects of life under party control. Individual states not controlled by elected Nazi governments or Nazi-led coalitions were forced to agree to the appointment of Reich Commissars to bring the states in line with the policies of the central government. These Commissars had the power to appoint and remove local governments, state parliaments, officials, and judges. In this way Germany became a \"de facto\" unitary state, with all state governments controlled by the central government under the NSDAP. The state parliaments and the \"Reichsrat\" (federal upper house) were abolished in January 1934, with all state powers being transferred to the central government. \n\nAll civilian organisations, including agricultural groups, volunteer organisations, and sports clubs, had their leadership replaced with Nazi sympathisers or party members; these civic organisations either merged with the NSDAP or faced dissolution. The Nazi government declared a \"Day of National Labor\" for May Day 1933, and invited many trade union delegates to Berlin for celebrations. The day after, SA stormtroopers demolished union offices around the country; all trade unions were forced to dissolve and their leaders were arrested. The Law for the Restoration of the Professional Civil Service, passed in April, removed from their jobs all teachers, professors, judges, magistrates, and government officials who were Jewish or whose commitment to the party was suspect. This meant the only non-political institutions not under control of the NSDAP were the churches.\n\nThe Nazi regime abolished the symbols of the Weimar Republic—including the black, red, and gold tricolour flag—and adopted reworked symbolism. The previous imperial black, white, and red tricolour was restored as one of Germany's two official flags; the second was the swastika flag of the NSDAP, which became the sole national flag in 1935. The NSDAP anthem \"Horst-Wessel-Lied\" (\"Horst Wessel Song\") became a second national anthem.\n\nHitler knew that reviving the economy was vital. Germany was still in a dire economic situation, as six million people were unemployed and the balance of trade deficit was daunting. Using deficit spending, public works projects were undertaken beginning in 1934, creating 1.7 million new jobs by the end of that year alone. Average wages both per hour and per week began to rise.\n\nShortly after the NSDAP's seizure of power, the SA continued to apply pressure for greater political and military power. In response, Hitler used the \"Schutzstaffel\" (SS) and Gestapo to purge the entire SA leadership. Hitler targeted SA \"Stabschef\" (Chief of Staff) Ernst Röhm and other SA leaders who—along with a number of Hitler's political adversaries (such as Gregor Strasser and former chancellor Kurt von Schleicher)—were rounded up, arrested, and shot. Up to 200 people were killed from 30 June to 2 July 1934 in an event that became known as the Night of the Long Knives.\n\nOn 2 August 1934, Hindenburg died. The previous day, the cabinet had enacted the \"Law Concerning the Highest State Office of the Reich\", which stated that upon Hindenburg's death the office of president would be abolished and its powers merged with those of the chancellor. Hitler thus became head of state as well as head of government and was formally named as \"Führer und Reichskanzler\" (\"Leader and Chancellor\") – although eventually \"Reichskanzler\" was quietly dropped. Germany was now a totalitarian state with Hitler at its head. As head of state, Hitler became Supreme Commander of the armed forces. The new law provide an altered loyalty oath for servicemen so that they affirmed loyalty to Hitler personally rather than the office of supreme commander or the state. On 19 August, the merger of the presidency with the chancellorship was approved by 90 percent of the electorate in a plebiscite.\nMost Germans were relieved that the conflicts and street fighting of the Weimar era had ended. They were deluged with propaganda orchestrated by Minister of Public Enlightenment and Propaganda Joseph Goebbels, who promised peace and plenty for all in a united, Marxist-free country without the constraints of the Versailles Treaty. The first major Nazi concentration camp, initially for political prisoners, was opened at Dachau in 1933. Hundreds of camps of varying size and function were created by the end of the war.\n\nBeginning in April 1933, scores of measures defining the status of Jews and their rights were instituted. Initiatives and legal mandates against the Jews culminated in the establishment of the Nuremberg Laws of 1935, stripping them of their basic rights. The Nazis would take from the Jews their wealth, their right to intermarry with non-Jews, and their right to occupy many fields of labour (such as practising law, medicine, or working as educators). Eventually the Nazis declared the Jews as undesirable to remain among German citizens and society. The NSDAP obtained and legitimised power through its initial revolutionary activities, then through manipulation of legal mechanisms, the use of police powers, and by taking control of the state and federal institutions.\n\nFollowing the passage of the Enabling Act and the NSDAP seizure of power in early 1933, Germany was without allies, and its military was drastically weakened by the terms of the Treaty of Versailles. France, Poland, Italy, and the Soviet Union each had reasons to object to Hitler's rise to power. Poland suggested to France that the two nations engage in a preventive war against Germany in March 1933. Fascist Italy objected to German claims in the Balkans and on Austria, which Duce Benito Mussolini considered to be in Italy's sphere of influence.\n\nAs early as February 1933, Hitler announced that rearmament must begin, albeit clandestinely at first, as to do so was in violation of the Versailles Treaty. On 17 May 1933, Hitler gave a speech before the Reichstag outlining his desire for world peace, while at the same time accepting an offer from American President Franklin D. Roosevelt for military disarmament, provided the other nations of Europe did the same. When the other European powers failed to accept this offer, Hitler pulled Germany out of the World Disarmament Conference and the League of Nations in October, claiming its disarmament clauses were unfair if they applied only to Germany. In a referendum held in November, 95 percent of voters supported Germany's withdrawal. \n\nIn 1934, Hitler told his military leaders that a war in the east should begin in 1942. The Saarland, which had been placed under League of Nations supervision for 15 years at the end of World War I, voted in January 1935 to become part of Germany. In March 1935, Hitler announced the creation of an air force, and that the \"Reichswehr\" would be increased to 550,000 men, Britain agreed that the Germans would be allowed to build a naval fleet with the signing of the Anglo-German Naval Agreement on 18 June 1935.\n\nWhen the Italian invasion of Ethiopia led to only mild protests by the British and French governments, on 7 March 1936 Hitler used the Franco-Soviet Treaty of Mutual Assistance as a pretext to order the army to march 3,000 troops into the demilitarised zone in the Rhineland in violation of the Versailles Treaty. As the territory was part of Germany, the British and French governments did not feel that attempting to enforce the treaty was worth the risk of war. In the one-party election held on 29 March, the NSDAP received 98.9 percent support. In 1936, Hitler signed an Anti-Comintern Pact with Japan and a non-aggression agreement with Mussolini, who was soon referring to a \"Rome-Berlin Axis\".\n\nHitler sent military supplies and assistance to the Nationalist forces of General Francisco Franco in the Spanish Civil War, which began in July 1936. The German Condor Legion included a range of aircraft and their crews, as well as a tank contingent. The aircraft of the Legion destroyed the city of Guernica in 1937. The Nationalists were victorious in 1939 and became an informal ally of Nazi Germany.\n\nIn February 1938, Hitler emphasised to Austrian Chancellor Kurt Schuschnigg the need for Germany to secure its frontiers. Schuschnigg scheduled a plebiscite regarding Austrian independence for 13 March, but Hitler sent an ultimatum to Schuschnigg on 11 March demanding that he hand over all power to the Austrian NSDAP or face an invasion. German troops entered Austria the next day, to be greeted with enthusiasm by the populace.\n\nThe Republic of Czechoslovakia was home to a substantial minority of Germans, who lived mostly in the Sudetenland. Under pressure from separatist groups within the Sudeten German Party, the Czechoslovak government offered economic concessions to the region. Hitler decided to incorporate not just the Sudetenland but all of Czechoslovakia into the Reich. The Nazis undertook a propaganda campaign to try to generate support for an invasion. Top leaders of the armed forces opposed the plan, as Germany was not yet ready for war.\n\nThe crisis led to war preparations by Britain, Czechoslovakia, and France (Czechoslovakia's ally). Attempting to avoid war, British Prime Minister Neville Chamberlain arranged a series of meetings, the result of which was the Munich Agreement, signed on 29 September 1938. The Czechoslovak government was forced to accept the Sudetenland's annexation into Germany. Chamberlain was greeted with cheers when he landed in London, saying it brought \"peace for our time\". The agreement lasted six months before Hitler seized the rest of Czech territory in March 1939. The Germans split Czechoslovakia into two parts, the puppet Slovakia Republic and the Protectorate of Bohemia and Moravia.\n\nAustrian and Czech foreign exchange reserves were seized by the Nazis, as were stockpiles of raw materials such as metals and completed goods such as weaponry and aircraft, which were shipped to Germany. The \"Reichswerke Hermann Göring\" industrial conglomerate took control of steel and coal production facilities in both countries.\n\nIn January 1934, Germany signed a non-aggression pact with Poland, which disrupted the French network of anti-German alliances in Eastern Europe. In March 1939, Hitler demanded the return of the Free City of Danzig and the Polish Corridor, a strip of land that separated East Prussia from the rest of Germany. The British announced they would come to the aid of Poland if it was attacked. Hitler, believing the British would not actually take action, ordered an invasion plan should be readied for September 1939. On 23 May, Hitler described to his generals his overall plan of not only seizing the Polish Corridor but greatly expanding German territory eastward at the expense of Poland. He expected this time they would be met by force.\n\nThe Germans reaffirmed their alliance with Italy and signed non-aggression pacts with Denmark, Estonia, and Latvia whilst trade links were formalised with Romania, Norway, and Sweden. Foreign Minister Joachim von Ribbentrop arranged in negotiations with the Soviet Union a non-aggression pact, the Molotov–Ribbentrop Pact, signed in August 1939. The treaty also contained secret protocols dividing Poland and the Baltic states into German and Soviet spheres of influence.\n\nGermany's wartime foreign policy involved the creation of allied governments controlled directly or indirectly from Berlin. They intended to obtain soldiers from allies such as Italy and Hungary and workers and food supplies from allies such as Vichy France. Hungary was the fourth nation to join the Axis, signing the Tripartite Pact on 27 September 1940. Bulgaria signed the pact on 17 November. German efforts to secure oil included negotiating a supply from their new ally, Romania, who signed the Pact on 23 November, alongside the Slovak Republic. By late 1942, there were 24 divisions from Romania on the Eastern Front, 10 from Italy, and 10 from Hungary. Germany assumed full control in France in 1942, Italy in 1943, and Hungary in 1944. Although Japan was a powerful ally, the relationship was distant, with little co-ordination or co-operation. For example, Germany refused to share their formula for synthetic oil from coal until late in the war.\n\nGermany invaded Poland and captured the Free City of Danzig on 1 September 1939, beginning World War II in Europe. Honouring their treaty obligations, Britain and France declared war on Germany two days later. Poland fell quickly, as the Soviet Union attacked from the east on 17 September. Reinhard Heydrich, chief of the \"Sicherheitspolizei\" (SiPo; Security Police) and \"Sicherheitsdienst\" (SD; Security Service), ordered on 21 September that Polish Jews should be rounded up and concentrated into cities with good rail links. Initially the intention was to deport them further east, or possibly to Madagascar. Using lists prepared in advance, some 65,000 Polish intelligentsia, noblemen, clergy, and teachers were killed by the end of 1939 in an attempt to destroy Poland's identity as a nation. Soviet forces advanced into Finland in the Winter War, and German forces saw action at sea. But little other activity occurred until May, so the period became known as the \"Phoney War\".\n\nFrom the start of the war, a British blockade on shipments to Germany affected its economy. Germany was particularly dependent on foreign supplies of oil, coal, and grain. Thanks to trade embargoes and the blockade, imports into Germany declined by 80 per cent. To safeguard Swedish iron ore shipments to Germany, Hitler ordered the invasion of Denmark and Norway, which began on 9 April. Denmark fell after less than a day, while most of Norway followed by the end of the month. By early June, Germany occupied all of Norway.\n\nAgainst the advice of many of his senior military officers, Hitler ordered an attack on France and the Low Countries, which began in May 1940. They quickly conquered Luxembourg and the Netherlands. After outmanoeuvring the Allies in Belgium and forcing the evacuation of many British and French troops at Dunkirk, France fell as well, surrendering to Germany on 22 June. The victory in France resulted in an upswing in Hitler's popularity and an upsurge in war fever in Germany.\n\nIn violation of the provisions of the Hague Convention, industrial firms in the Netherlands, France, and Belgium were put to work producing war materiel for Germany. Officials viewed this option as being preferable to their citizens being deported to the Reich as forced labour.\nThe Nazis seized from the French thousands of locomotives and rolling stock, stockpiles of weapons, and raw materials such as copper, tin, oil, and nickel. Payments for occupation costs were levied upon France, Belgium, and Norway. Barriers to trade led to hoarding, black markets, and uncertainty about the future. Food supplies were precarious; production dropped in most of Europe, but not as much as during World War I. Famine was experienced in many occupied countries during the war.\n\nHitler's peace overtures to the new British Prime Minister Winston Churchill were rejected in July 1940. Grand Admiral Erich Raeder had advised Hitler in June that air superiority was a pre-condition for a successful invasion of Britain, so Hitler ordered a series of aerial attacks on Royal Air Force (RAF) airbases and radar stations, as well as nightly air raids on British cities, including London, Plymouth, and Coventry. The German Luftwaffe failed to defeat the RAF in what became known as the Battle of Britain, and by the end of October, Hitler realised that air superiority would not be achieved. He permanently postponed the invasion, a plan which the commanders of the German army had never taken entirely seriously. Several historians, including Andrew Gordon, believe the primary reason for the failure of the invasion plan was due to the superiority of the Royal Navy, not the actions of the RAF.\n\nIn February 1941, the German \"Afrika Korps\" arrived in Libya to aid the Italians in the North African Campaign. On 6 April, Germany launched an invasion of Yugoslavia and Greece. All of Yugoslavia and parts of Greece were subsequently divided between Germany, Hungary, Italy, and Bulgaria. The Italian-backed Croatian fascist leader Ante Pavelić established the Independent State of Croatia.\n\nOn 22 June 1941, contravening the Molotov–Ribbentrop Pact, 5.5 million Axis troops attacked the Soviet Union. In addition to Hitler's stated purpose of acquiring \"Lebensraum\", this large-scale offensive—codenamed Operation Barbarossa—was intended to destroy the Soviet Union and seize its natural resources for subsequent aggression against the Western powers. The reaction among Germans was one of surprise and trepidation as many were concerned about how much longer the war would continue or suspected that Germany could not win a war fought on two fronts.\n\nThe invasion conquered a huge area, including the Baltic states, Belarus, and west Ukraine. After the successful Battle of Smolensk ended in early September 1941, Hitler ordered Army Group Centre to halt its advance to Moscow and temporarily divert its Panzer groups to aid in the encirclement of Leningrad and Kiev. This pause provided the Red Army with an opportunity to mobilise fresh reserves. The Moscow offensive, which resumed in October 1941, ended disastrously in December. On 7 December 1941, Japan attacked Pearl Harbor, Hawaii. Four days later, Germany declared war on the United States.\n\nFood was in short supply in the conquered areas of the Soviet Union and Poland, as the retreating armies had burned the crops in some areas, and much of the remainder was sent back to the Reich. In Germany, rations were cut in 1942. In his role as Plenipotentiary of the Four Year Plan, Hermann Göring demanded increased shipments of grain from France and fish from Norway. The 1942 harvest was good, and food supplies remained adequate in Western Europe.\n\nGermany and Europe as a whole was almost totally dependent on foreign oil imports. In an attempt to resolve the shortage, in June 1942 Germany launched \"Fall Blau\" (\"Case Blue\"), an offensive against the Caucasian oilfields. The Red Army launched a counter-offensive on 19 November and encircled the Axis forces, who were trapped in Stalingrad on 23 November. Göring assured Hitler that the 6th Army could be supplied by air, but this turned out to be infeasible. Hitler's refusal to allow a retreat led to the deaths of 200,000 German and Romanian soldiers; of the 91,000 men who surrendered in the city on 31 January 1943, only 6,000 survivors returned to Germany after the war.\n\nLosses continued to mount after Stalingrad, leading to a sharp reduction in the popularity of the Nazi Party and deteriorating morale among both the military and the civilian population. Soviet forces continued to push the invaders westward after the failed German offensive at the Battle of Kursk in the summer of 1943. By the end of 1943 the Germans had lost most of their eastern territorial gains. In Egypt, Field Marshal Erwin Rommel's \"Afrika Korps\" were defeated by British forces under Field Marshal Bernard Montgomery in October 1942. The Allies landed in Sicily in July 1943 and in Italy in September. Meanwhile, American and British bomber fleets based in Britain began operations against Germany. Many sorties were intentionally given civilian targets in an effort to destroy German morale. Soon German aircraft production could not keep pace with losses, and without air cover the Allied bombing campaign became even more devastating. By targeting oil refineries and factories, they crippled the German war effort by late 1944.\n\nOn 6 June 1944, American, British, and Canadian forces established a front in France with the D-Day landings in Normandy. On 20 July 1944, Hitler survived an assassination attempt using a planted bomb. He ordered brutal reprisals, resulting in 7,000 arrests and the execution of more than 4,900 people. The failed Ardennes Offensive (16 December 1944 – 25 January 1945) was the last major German offensive on the western front, and Soviet forces entered Germany on 27 January. Hitler's refusal to admit defeat and his repeated insistence that the war be fought to the last man led to unnecessary death and destruction in the war's closing months. Through his Justice Minister Otto Georg Thierack, Hitler ordered that anyone who was not prepared to fight should be court-martialed, and thousands of people were put to death. In many areas, people surrendered to the approaching Allies in spite of exhortations of local leaders to continue to fight. Hitler ordered the destruction of transport, bridges, industries, and other infrastructure—a scorched earth decree—but Armaments Minister Albert Speer prevented this order from being fully carried out.\n\nDuring the Battle of Berlin (16 April 1945 – 2 May 1945), Hitler and his staff lived in the underground \"Führerbunker\" while the Red Army approached. On 30 April, when Soviet troops were within two blocks of the Reich Chancellery, Hitler, along with his girlfriend and by then wife Eva Braun committed suicide. On 2 May, General Helmuth Weidling unconditionally surrendered Berlin to Soviet General Vasily Chuikov. Hitler was succeeded by Grand Admiral Karl Dönitz as Reich President and Goebbels as Reich Chancellor. Goebbels and his wife Magda committed suicide the next day after murdering their six children. Between 4 and 8 May 1945, most of the remaining German armed forces unconditionally surrendered. The German Instrument of Surrender was signed 8 May, marking the end of the Nazi regime and the end of World War II in Europe.\n\nPopular support for Hitler almost completely disappeared as the war drew to a close. Suicide rates in Germany increased, particularly in areas where the Red Army was advancing. Among soldiers and party personnel, suicide was often deemed an honourable and heroic alternative to surrender. First-hand accounts and propaganda about the uncivilised behaviour of the advancing Soviet troops caused panic among civilians on the Eastern Front, especially women, who feared being raped. More than a thousand people (out of a population of around 16,000) committed suicide in Demmin on and around 1 May 1945 as the 65th Army of 2nd Belorussian Front first broke into a distillery and then rampaged through the town, committing mass rapes, arbitrarily executing civilians, and setting fire to buildings. High numbers of suicides took place in many other locations, including Neubrandenburg (600 dead), Stolp in Pommern (1,000 dead), and Berlin, where at least 7,057 people committed suicide in 1945.\n\nEstimates of the total German war dead range from 5.5 to 6.9 million persons. A study by German historian Rüdiger Overmans puts the number of German military dead and missing at 5.3 million, including 900,000 men conscripted from outside of Germany's 1937 borders. Overy estimated in 2014 that about 353,000 civilians were killed in Allied air raids. Other civilian deaths include 300,000 Germans (including Jews) who were victims of Nazi political, racial, and religious persecution and 200,000 who were murdered in the Nazi euthanasia program. Political courts called \"Sondergerichte\" sentenced some 12,000 members of the German resistance to death, and civil courts sentenced an additional 40,000 Germans. Mass rapes of German women also took place.\n\nAt the end of the war, Europe had more than 40 million refugees, its economy had collapsed, and 70 percent of its industrial infrastructure was destroyed. Between twelve and fourteen million ethnic Germans fled or were expelled from central, eastern and southeastern Europe to Germany. The West German government estimated a death toll of 2.2 million civilians due to the flight and expulsion of Germans and through forced labour in the Soviet Union. This figure remained unchallenged until the 1990s, when some historians put the death toll at 500,000–600,000 confirmed deaths. In 2006, the German government reaffirmed its position that 2.0–2.5 million deaths occurred.\n\nAs a result of their defeat in World War I and the resulting Treaty of Versailles, Germany lost Alsace-Lorraine, Northern Schleswig, and Memel. The Saarland temporarily became a protectorate of France under the condition that its residents would later decide by referendum which country to join, and Poland became a separate nation and was given access to the sea by the creation of the Polish Corridor, which separated Prussia from the rest of Germany, while Danzig was made a free city.\n\nGermany regained control of the Saarland through a referendum held in 1935 and annexed Austria in the \"Anschluss\" of 1938. The Munich Agreement of 1938 gave Germany control of the Sudetenland, and they seized the remainder of Czechoslovakia six months later. Under threat of invasion by sea, Lithuania surrendered the Memel district in March 1939.\n\nBetween 1939 and 1941, German forces invaded Poland, Denmark, Norway, France, Luxembourg, the Netherlands, Belgium, Yugoslavia, Greece, and the Soviet Union. Germany annexed parts of northern Yugoslavia in April 1941, while Mussolini ceded Trieste, South Tyrol, and Istria to Germany in 1943. Two puppet districts were created in the area: the Operational Zone of the Adriatic Littoral and the Operational Zone of the Alpine Foothills.\n\nSome of the conquered territories were incorporated into Germany as part of Hitler's long-term goal of creating a Greater Germanic Reich. Several areas, such as Alsace-Lorraine, were placed under the authority of an adjacent \"Gau\" (regional district). Beyond the incorporated territories were the \"Reichskommissariate\" (Reich Commissariats), quasi-colonial regimes established in some occupied countries. Areas placed under German administration included the Protectorate of Bohemia and Moravia, \"Reichskommissariat Ostland\" (encompassing the Baltic states and Belarus), and \"Reichskommissariat Ukraine\". Conquered areas of Belgium and France were placed under control of the Military Administration in Belgium and Northern France. Belgian Eupen-Malmedy, which had been part of Germany until 1919, was annexed. Part of Poland was incorporated into the Reich, and the General Government was established in occupied central Poland. The governments of Denmark, Norway (\"Reichskommissariat Norwegen\"), and the Netherlands (\"Reichskommissariat Niederlande\") were placed under civilian administrations staffed largely by natives. Hitler intended to eventually incorporate many of these areas into the Reich. Germany also occupied the Italian protectorate of Albania and the Italian governorate of Montenegro in 1943. Elsewhere in the Balkans, Germany occupied the territory of Serbia and installed a puppet government.\n\nWith the issuance of the Berlin Declaration on 5 June 1945 and later creation of the Allied Control Council, the four Allied powers temporarily assumed governance of Germany. At the Potsdam Conference in August 1945, the Allies arranged for the Allied occupation and denazification of the country. Germany was split into four zones, each occupied by one of the Allied powers, who drew reparations from their zone. Since most of the industrial areas were in the western zones, the Soviet Union was transferred additional reparations. The Allied Control Council disestablished Prussia on 20 May 1947. Aid to Germany began arriving from the United States under the Marshall Plan in 1948. The occupation lasted until 1949, when the countries of East Germany and West Germany were created. In 1970, Germany finalised her border with Poland by signing the Treaty of Warsaw. Germany remained divided until 1990, when the Allies renounced all claims to German territory with the Treaty on the Final Settlement with Respect to Germany, under which Germany also renounced claims to territories lost during World War II.\n\nThe NSDAP was a far-right political party which arose during the social and financial upheavals that occurred following the end of World War I. The NSDAP remained small and marginalised, managing 2.6% of the federal vote in 1928, prior to the on-set of the Great Depression in 1929. By 1930 the NSDAP won 18.3% of the federal vote, making it the Reichstag's second largest political party. While in prison after the failed Beer Hall Putsch of 1923, Hitler wrote \"Mein Kampf\", which laid out his plan for transforming German society into one based on race. Nazi ideology brought together elements of antisemitism, racial hygiene, and eugenics, and combined them with pan-Germanism and territorial expansionism with the goal of obtaining more \"Lebensraum\" for the Germanic people. The regime attempted to obtain this new territory by attacking Poland and the Soviet Union, intending to deport or kill the Jews and Slavs living there, who were viewed as being inferior to the Aryan master race and part of a Jewish-Bolshevik conspiracy. The Nazi regime believed that only Germany could defeat the forces of Bolshevism and save humanity from world domination by International Jewry. Others deemed life unworthy of life by the Nazis included the mentally and physically disabled, Romani people, homosexuals, Jehovah's Witnesses, and social misfits.\n\nInfluenced by the \"Völkisch\" movement, the regime was against cultural modernism and supported the development of an extensive military at the expense of intellectualism. Creativity and art were stifled, except where they could serve as propaganda media. The party used symbols such as the Blood Flag and rituals such as the Nazi Party rallies to foster unity and bolster the regime's popularity.\n\nHitler ruled Germany autocratically by asserting the \"Führerprinzip\" (\"leader principle\"), which called for absolute obedience of all subordinates. He viewed the government structure as a pyramid, with himself—the infallible leader—at the apex. Party rank was not determined by elections, and positions were filled through appointment by those of higher rank. The party used propaganda to develop a cult of personality around Hitler. Historians such as Kershaw emphasise the psychological impact of Hitler's skill as an orator. Roger Gill states: \"His moving speeches captured the minds and hearts of a vast number of the German people: he virtually hypnotized his audiences\".\n\nWhile top officials reported to Hitler and followed his policies, they had considerable autonomy. He expected officials to \"work towards the Führer\" – to take the initiative in promoting policies and actions in line with party goals and Hitler's wishes, without his involvement in day-to-day decision-making. The government was not a coordinated, co-operating body, but rather a disorganised collection of factions led by the party elite, who struggled to amass power and gain the Führer's favour. Hitler's leadership style was to give contradictory orders to his subordinates and to place them in positions where their duties and responsibilities overlapped. In this way he fostered distrust, competition, and infighting among his subordinates to consolidate and maximise his own power.\n\nSuccessive \"Reichsstatthalter\" decrees between 1933 and 1935 abolished the existing \"Länder\" (constituent states) of Germany and replaced them with new administrative divisions, the \"Gaue\", governed by NSDAP leaders (\"Gauleiters\"). The change was never fully implemented, as the Länder were still used as administrative divisions for some government departments such as education. This led to a bureaucratic tangle of overlapping jurisdictions and responsibilities typical of the administrative style of the Nazi regime.\n\nJewish civil servants lost their jobs in 1933, except for those who had seen military service in World War I. Members of the NSDAP or party supporters were appointed in their place. As part of the process of \"Gleichschaltung\", the Reich Local Government Law of 1935 abolished local elections, and from henceforth mayors were appointed by the Ministry of the Interior.\n\nIn August 1934, civil servants and members of the military were required to swear an oath of unconditional obedience to Hitler. These laws became the basis of the \"Führerprinzip\", the concept that Hitler's word overrode all existing laws. Any acts that were sanctioned by Hitler—even murder—thus became legal. All legislation proposed by cabinet ministers had to be approved by the office of Deputy Führer Rudolf Hess, who could also veto top civil service appointments.\n\nMost of the judicial system and legal codes of the Weimar Republic remained in use during and after the Nazi era to deal with non-political crimes. The courts issued and carried out far more death sentences than before the Nazis took power. People who were convicted of three or more offences—even petty ones—could be deemed habitual offenders and jailed indefinitely. People such as prostitutes and pickpockets were judged to be inherently criminal and a threat to the racial community. Thousands were arrested and confined indefinitely without trial.\nA new type of court, the \"Volksgerichtshof\" (People's Court), was established in 1934 to deal with political cases. This court handed out over 5,000 death sentences until its dissolution in 1945. The death penalty could be issued for offences such as being a communist, printing seditious leaflets, or even making jokes about Hitler or other officials. The Gestapo was in charge of investigative policing to enforce National Socialist ideology as they located and confined political offenders, Jews, and others deemed undesirable. Political offenders who were released from prison were often immediately re-arrested by the Gestapo and confined in a concentration camp.\n\nThe Nazis used propaganda to promulgate the concept of \"Rassenschande\" (\"race defilement\") to justify the need for such laws. In September 1935, the Nuremberg Laws were enacted. These laws initially prohibited sexual relations and marriages between Aryans and Jews and were later extended to include \"Gypsies, Negroes or their bastard offspring\". The law also forbade the employment of German women under the age of 45 as domestic servants in Jewish households. The Reich Citizenship Law stated that only those of \"German or related blood\" were eligible for citizenship. Thus Jews and other non-Aryans were stripped of their German citizenship. The wording of the law also permitted the Nazis to deny citizenship to anyone who was not supportive enough of the regime. A supplementary decree issued in November defined as Jewish anyone with three Jewish grandparents, or two grandparents if the Jewish faith was followed.\n\nThe unified armed forces of Germany from 1935 to 1945 were called the \"Wehrmacht\" (defence force). This included the \"Heer\" (army), \"Kriegsmarine\" (navy), and the \"Luftwaffe\" (air force). From 2 August 1934, members of the armed forces were required to pledge an oath of unconditional obedience to Hitler personally. In contrast to the previous oath, which required allegiance to the constitution of the country and its lawful establishments, this new oath required members of the military to obey Hitler even if they were being ordered to do something illegal. Hitler decreed that the army would have to tolerate and even offer logistical support to the \"Einsatzgruppen\"—the mobile death squads responsible for millions of deaths in Eastern Europe—when it was tactically possible to do so. \"Wehrmacht\" troops also participated directly in the Holocaust by shooting civilians or committing genocide under the guise of anti-partisan operations. The party line was that the Jews were the instigators of the partisan struggle and therefore needed to be eliminated. On 8 July 1941, Heydrich announced that all Jews in the eastern conquered territories were to be regarded as partisans and gave the order for all male Jews between the ages of 15 and 45 to be shot. By August this was extended to include the entire Jewish population.\n\nIn spite of efforts to prepare the country militarily, the economy could not sustain a lengthy war of attrition. A strategy was developed based on the tactic of \"Blitzkrieg\" (\"lightning war\"), which involved using quick coordinated assaults that avoided enemy strong points. Attacks began with artillery bombardment, followed by bombing and strafing runs. Next the tanks would attack and finally the infantry would move in to secure the captured area. Victories continued through mid-1940, but the failure to defeat Britain was the first major turning point in the war. The decision to attack the Soviet Union and the decisive defeat at Stalingrad led to the retreat of the German armies and the eventual loss of the war. The total number of soldiers who served in the \"Wehrmacht\" from 1935 to 1945 was around 18.2 million, of whom 5.3 million died.\n\nThe \"Sturmabteilung\" (SA; Storm Detachment; Brownshirts), founded in 1921, was the first paramilitary wing of the NSDAP; their initial assignment was to protect Nazi leaders at rallies and assemblies. They also took part in street battles against the forces of rival political parties and violent actions against Jews and others. Under Ernst Röhm's leadership the SA had grown by 1934 to over half a million members—4.5 million including reserves—at a time when the regular army was still limited to 100,000 men by the Versailles Treaty.\n\nRöhm hoped to assume command of the army and absorb it into the ranks of the SA. Hindenburg and Defence Minister Werner von Blomberg threatened to impose martial law if the activities of the SA were not curtailed. Therefore, less than a year and a half after seizing power, Hitler ordered the deaths of the SA leadership, including Rohm. After the purge of 1934, the SA was no longer a major force.\n\nInitially a small bodyguard unit under the auspices of the SA, the \"Schutzstaffel\" (SS; Protection Squadron) grew to become one of the largest and most powerful groups in Nazi Germany. Led by \"Reichsführer-SS\" Heinrich Himmler from 1929, the SS overall had over a quarter million members by 1938. Himmler initially envisioned the SS as being an elite group of guards, Hitler's last line of defence. The Waffen-SS, the military branch of the SS, became arguably a \"de facto\" fourth branch of the \"Wehrmacht\", however, it was never a \"serious rival\" to the German Army. It never obtained total \"independence of command\" and was dependent on the army for heavy weaponry and equipment. By the end of 1942 as the Soviet Army pushed back in resurgence and losses of Waffen-SS troops mounted, the selection and racial requirements were no longer followed. With recruitment and conscription based only on expansion, by 1943 the Waffen-SS could not longer claim to be an elite fighting force. Its formations committed many war crimes against civilians and allied servicemen. Up to 60,000 Waffen-SS men served in the concentration and extermination camps. Further, a third of the \"Einsatzgruppen\" members that were responsible for mass murder, were recruited from Waffen-SS personnel. From 1935 forward, the SS spearheaded the persecution of Jews, who were rounded up into ghettos and concentration camps. With the outbreak of World War II, the SS \"Einsatzgruppen\" units followed the army into Poland and the Soviet Union, where from 1941 to 1945 they killed more than two million people, including 1.3 million Jews. The \"SS-Totenkopfverbände\" (death's head units) ran the concentration camps and extermination camps, where millions more were killed.\n\nIn 1931, Himmler organised an SS intelligence service which became known as the \"Sicherheitsdienst\" (SD; Security Service) under his deputy, Heydrich. This organisation was tasked with locating and arresting communists and other political opponents. Himmler established the beginnings of a parallel economy under the auspices of the SS Economy and Administration Head Office. This holding company owned housing corporations, factories, and publishing houses.\n\nThe most pressing economic matter the Nazis initially faced was the 30 percent national unemployment rate. Economist Dr. Hjalmar Schacht, President of the Reichsbank and Minister of Economics, created a scheme for deficit financing in May 1933. Capital projects were paid for with the issuance of promissory notes called Mefo bills. When the notes were presented for payment, the Reichsbank printed money. Hitler and his economic team expected that the upcoming territorial expansion would provide the means of repaying the soaring national debt. Schacht's administration achieved a rapid decline in the unemployment rate, the largest of any country during the Great Depression. Economic recovery was uneven, with reduced hours of work and erratic availability of necessities, leading to disenchantment with the regime as early as 1934.\n\nIn October 1933, the Junkers Aircraft Works was expropriated. In concert with other aircraft manufacturers and under the direction of Aviation Minister Göring, production was ramped up industry-wide. From a workforce of 3,200 people producing 100 units per year in 1932, the industry grew to employ a quarter of a million workers manufacturing over 10,000 technically advanced aircraft annually less than ten years later.\n\nAn elaborate bureaucracy was created to regulate imports of raw materials and finished goods with the intention of eliminating foreign competition in the German marketplace and improving the nation's balance of payments. The Nazis encouraged the development of synthetic replacements for materials such as oil and textiles. As the market was experiencing a glut and prices for petroleum were low, in 1933 the Nazi government made a profit-sharing agreement with IG Farben, guaranteeing them a 5 percent return on capital invested in their synthetic oil plant at Leuna. Any profits in excess of that amount would be turned over to the Reich. By 1936, Farben regretted making the deal, as the excess profits by then being generated had to be given to the government. In another attempt to secure an adequate wartime supply of petroleum, Germany intimidated Romania into signing a trade agreement in March 1939.\n\nMajor public works projects financed with deficit spending included the construction of a network of \"Autobahnen\" and providing funding for programmes initiated by the previous government for housing and agricultural improvements. To stimulate the construction industry, credit was offered to private businesses and subsidies were made available for home purchases and repairs. On the condition that the wife would leave the workforce, a loan of up to 1,000 Reichsmarks could be accessed by young couples of Aryan descent who intended to marry, and the amount that had to be repaid was reduced by 25 percent for each child born. The caveat that the woman had to remain unemployed was dropped by 1937 due to a shortage of skilled labourers.\n\nEnvisioning widespread car ownership as part of the new Germany, Hitler arranged for designer Ferdinand Porsche to draw up plans for the \"KdF-wagen\" (Strength Through Joy car), intended to be an automobile that everyone could afford. A prototype was displayed at the International Motor Show in Berlin on 17 February 1939. With the outbreak of World War II, the factory was converted to produce military vehicles. None were sold until after the war, when the vehicle was renamed the Volkswagen (people's car).\n\nSix million people were unemployed when the Nazis took power in 1933 and by 1937 there were fewer than a million. This was in part due to the removal of women from the workforce. Real wages dropped by 25 percent between 1933 and 1938. After the dissolution of the trade unions in May 1933, their funds were seized and their leadership arrested, including those who attempted to co-operate with the NSDAP. A new organisation, the German Labour Front, was created and placed under NSDAP functionary Robert Ley. The average German worked 43 hours a week in 1933; by 1939 this increased to 47 hours a week.\n\nBy early 1934, the focus shifted from funding work creation schemes towards rearmament. By 1935, military expenditures accounted for 73 percent of the government's purchases of goods and services. On 18 October 1936, Hitler named Göring as Plenipotentiary of the Four Year Plan, intended to speed up the rearmament programme. In addition to calling for the rapid construction of steel mills, synthetic rubber plants and other factories, Göring instituted wage and price controls and restricted the issuance of stock dividends. Large expenditures were made on rearmament in spite of growing deficits. Plans unveiled in late 1938 for massive increases to the navy and air force prior to commencing the war were impossible to fulfil, as Germany lacked the finances and material resources to build the planned units, as well as the necessary fuel required to keep them running. With the introduction of compulsory military service in 1935, the \"Reichswehr\", which had been limited to 100,000 by the terms of the Versailles Treaty, expanded to 750,000 on active service at the start of World War II, with a million more in the reserve. By January 1939, unemployment was down to 301,800 and it dropped to only 77,500 by September.\n\nThe Nazi war economy was a mixed economy that combined a free market with central planning. Historian Richard Overy described it as being somewhere in between the command economy of the Soviet Union and the capitalist system of the United States.\n\nIn 1942, after the death of Armaments Minister Fritz Todt, Hitler appointed Albert Speer as his replacement. Wartime rationing of consumer goods led to an increase in personal savings, funds which were in turn lent to the government to support the war effort. By 1944, the war was consuming 75 percent of Germany's gross domestic product, compared to 60 percent in the Soviet Union and 55 percent in Britain. Speer improved production by centralising planning and control, reducing production of consumer goods, and using forced labour and slavery. The wartime economy eventually relied heavily upon the large-scale employment of slave labour. Germany imported and enslaved some 12 million people from 20 European countries to work in factories and on farms. Approximately 75 percent were Eastern European. Many were casualties of Allied bombing, as they received poor air raid protection. Poor living conditions led to high rates of sickness, injury and death, as well as sabotage and criminal activity. The wartime economy also relied upon large-scale robbery, initially through the state seizing the property of Jewish citizens and later by plundering the resources of occupied territories.\n\nForeign workers brought into Germany were put into four different classifications: guest workers, military internees, civilian workers, and Eastern workers. Each group was subject to different regulations. In addition, the Nazis issued a ban on sexual relations between Germans and foreign workers.\n\nBy 1944 over a half million women served as auxiliaries in the German armed forces. The number of women in paid employment only increased by 271,000 (1.8 percent) from 1939 to 1944, but as the production of consumer goods had been cut back, women left those industries for employment in the war economy. They also took over jobs formerly held by men, especially on farms and in family-owned shops.\n\nVery heavy strategic bombing by the Allies targeted refineries producing synthetic oil and gasoline, as well as the German transportation system, especially rail yards and canals. The armaments industry began to break down by September 1944. By November, fuel coal was no longer reaching its destinations and the production of new armaments was no longer possible. Overy argues that the bombing strained the German war economy and forced it to divert up to one-fourth of its manpower and industry into anti-aircraft resources, which very likely shortened the war.\n\nDuring the course of the war, the Nazis extracted considerable amounts of plunder from occupied Europe. Historian and war correspondent William L. Shirer writes that: \"The total amount of [Nazi] loot will never be known; it has proved beyond man's capacity to accurately compute.\" Gold reserves and other foreign holdings were regularly seized from the national banks of occupied nations, while large \"occupation costs\" were usually imposed. By February 1944, the \"Reichsfinanzministerium\" (Ministry of Finance) had calculated that 48 billion Reichsmarks had been paid to Germany. By the end of the war, occupation costs were calculated by the Nazis to number 60 billion Reichsmarks, with France alone paying 31.5 billion. The Bank of France was also forced to provide 4.5 billion Reichsmarks in \"credits\" to Germany, while a further 500,000 Reichsmarks were assessed against Vichy France by the Nazis in the form of \"fees\" and other miscellaneous charges. The Nazis exploited other conquered nations in a similar way. After the war, the United States Strategic Bombing Survey concluded Germany had obtained 104 billion Reichsmarks in the form of occupation costs and other wealth transfers from occupied Europe, including two-thirds of the gross domestic product of Belgium and the Netherlands.\n\nNazi plunder included private and public art collections, artefacts, precious metals, books, and personal possessions. Hitler and Göring in particular were interested in acquiring looted art treasures from occupied Europe, the former planning to use the stolen art to fill the galleries of the planned \"Führermuseum\" (Leader's Museum), and the latter for his personal collection. Göring, having stripped almost all of occupied Poland of its artworks within six months of Germany's invasion, ultimately grew a collection valued at over 50 million Reichsmarks. In 1940, the Reichsleiter Rosenberg Taskforce was established to loot artwork and cultural material from public and private collections, libraries, and museums throughout Europe. France saw the greatest extent of Nazi plunder. Some 26,000 railroad cars of art treasures, furniture, and other looted items were sent to Germany from France. By January 1941, Rosenberg estimated the looted treasures from France to be valued at over one billion Reichsmarks. In addition, soldiers looted or purchased goods such as produce and clothing—items, which were becoming harder to obtain in Germany—for shipment home. \n\nGoods and raw materials were also taken. In France, an estimated of cereals were seized during the course of the war. 75 percent of the oats produced in France, as well as 80 percent of the country's oil and 74 percent of its steel production were also taken. The valuation of this loot is estimated to be 184.5 billion francs. In Poland, Nazi plunder of raw materials began even before the German invasion had concluded. In October 1939, Hans Frank, future Governor-General of occupied Poland, stated that the Nazis intended a \"ruthless exploitation\" of Poland's \"supplies, raw materials, machines, [and] factory instillation.\" Frank went on to state that \"Poland shall be treated as a colony. The Poles shall be the slaves of the Greater German Reich.\"\n\nFollowing Operation Barbarossa, the Soviet Union was also plundered. In 1943 alone, 9,000,000 tons of cereals, of fodder, of potatoes, and of meats were sent back to Germany. During the course of Germany's occupation of Soviet territory, some 12 million pigs and 13 million sheep had been taken. The total value of this plunder is estimated at 4 billion Reichsmarks. This relatively low number in comparison to the occupied nations of Western Europe can be attributed to the devastating fighting on the Eastern Front.\n\nRacism and antisemitism were basic tenets of the NSDAP and the Nazi regime. Nazi Germany's racial policy was based on their belief in the existence of a superior master race. The Nazis postulated the existence of a racial conflict between the Aryan master race and inferior races, particularly Jews, who were viewed as a mixed race that had infiltrated society and were responsible for the exploitation and repression of the Aryan race.\n\nDiscrimination against Jews began immediately after the seizure of power. Following a month-long series of attacks by members of the SA on Jewish businesses and synagogues, on 1 April 1933 Hitler declared a national boycott of Jewish businesses. The Law for the Restoration of the Professional Civil Service passed on 7 April and forced all non-Aryan civil servants to retire from the legal profession and civil service. Similar legislation soon deprived other Jewish professionals of their right to practise, and on 11 April a decree was promulgated that stated anyone who had even one Jewish parent or grandparent was considered non-Aryan. As part of the drive to remove Jewish influence from cultural life, members of the National Socialist Student League removed from libraries any books considered un-German, and a nationwide book burning was held on 10 May.\n\nThe regime used violence and economic pressure to encourage Jews to voluntarily leave the country. Jewish businesses were denied access to markets, forbidden to advertise, and deprived of access to government contracts. Citizens were harassed and subjected to violent attacks. Many towns posted signs forbidding entry to Jews.\n\nIn November 1938 a young Jewish man requested an interview with the German ambassador in Paris and met with a legation secretary, whom he shot and killed to protest his family's treatment in Germany. This incident provided the pretext for a pogrom the NSDAP incited against the Jews on 9 November 1938. Members of the SA damaged or destroyed synagogues and Jewish property throughout Germany. At least 91 German Jews were killed during this pogrom, later called \"Kristallnacht\", the Night of Broken Glass. Further restrictions were imposed on Jews in the coming months – they were forbidden to own businesses or work in retail shops, drive cars, go to the cinema, visit the library or own weapons, and Jewish pupils were removed from schools. The Jewish community was fined one billion marks to pay for the damage caused by \"Kristallnacht\" and told that any insurance settlements would be confiscated. By 1939, around 250,000 of Germany's 437,000 Jews had emigrated to the United States, Argentina, Great Britain, Palestine, and other countries. Many chose to stay in continental Europe. Emigrants to Palestine were allowed to transfer property there under the terms of the Haavara Agreement, but those moving to other countries had to leave virtually all their property behind, and it was seized by the government.\n\nLike the Jews, the Romani or Gypsy people were subjected to persecution from the early days of the regime. The Romani were forbidden to marry people of German extraction. They were also shipped to concentration camps starting in 1935 and killed in large numbers. Following the invasion of Poland, 2,500 Roma and Sinti people were deported from Germany to the General Government where they were imprisoned in labour camps. The survivors were likely exterminated at Bełżec, Sobibor, or Treblinka. A further 5,000 Sinti and Austrian Lalleri people were deported to the Łódź Ghetto in late 1941, where half were estimated to have died. The Romani survivors of the ghetto were subsequently moved to the Chełmno extermination camp in early 1942.\n\nThe Nazis intended on deporting all Romani people from Germany, and confined them to \"Zigeunerlager\" (Gypsy camps) for this purpose. Himmler ordered their deportation from Germany in December 1942, with few exceptions. A total of 23,000 Romani were deported to Auschwitz concentration camp, of whom 19,000 died. Outside of Germany, the Romani people were regularly used for forced labour, though many were killed. In the Baltic states and the Soviet Union, 30,000 Romani were killed by the SS, the German Army, and \"Einsatzgruppen\". In occupied Serbia, 1,000 to 12,000 Romani were killed, while nearly all 25,000 Romani living in the Independent State of Croatia were killed. The estimates at end of the war put the total death toll at around 220,000, which equalled approximately 25 percent of the Romani population in Europe.\n\nAction T4 was a programme of systematic murder of the physically and mentally handicapped and patients in psychiatric hospitals that took place mainly from 1939 to 1941, and continued until the end of the war. Initially the victims were shot by the \"Einsatzgruppen\" and others; gas chambers and gas vans using carbon monoxide were used by early 1940. Under the Law for the Prevention of Genetically Diseased Offspring, enacted on 14 July 1933, over 400,000 individuals underwent compulsory sterilisation. Over half were those considered mentally deficient, which included not only people who scored poorly on intelligence tests, but those who deviated from expected standards of behaviour regarding thrift, sexual behaviour, and cleanliness. Most of the victims came from disadvantaged groups such as prostitutes, the poor, the homeless, and criminals. Other groups persecuted and killed included Jehovah's Witnesses, homosexuals, social misfits, and members of the political and religious opposition.\n\nGermany's war in the East was based on Hitler's long-standing view that Jews were the great enemy of the German people and that \"Lebensraum\" was needed for Germany's expansion. Hitler focused his attention on Eastern Europe, aiming to defeat Poland and the Soviet Union. After the occupation of Poland in 1939, all Jews living in the General Government area were confined to ghettos and those who were physically fit were required to perform compulsory labour. In 1941 Hitler decided to destroy the Polish nation completely; within 15 to 20 years the General Government was to be cleared of ethnic Poles and resettled by German colonists. About 3.8 to 4 million Poles would remain as slaves, part of a slave labour force of 14 million the Nazis intended to create using citizens of conquered nations.\n\nThe \"Generalplan Ost\" (\"General Plan for the East\") called for deporting the population of occupied Eastern Europe and the Soviet Union to Siberia, for use as slave labour or to be murdered. To determine who should be killed, Himmler created the \"Volksliste\", a system of classification of people deemed to be of German blood. He ordered that those of Germanic descent who refused to be classified as ethnic Germans should be deported to concentration camps, have their children taken away, or be assigned to forced labour. The plan also included the kidnapping of children deemed to have Aryan-Nordic traits, who were presumed to be of German descent. The goal was to implement \"Generalplan Ost\" after the conquest of the Soviet Union, but when the invasion failed Hitler had to consider other options. One suggestion was a mass forced deportation of Jews to Poland, Palestine, or Madagascar.\n\nIn addition to eliminating Jews, the Nazis planned to reduce the population of the conquered territories by 30 million people through starvation in an action called the Hunger Plan. Food supplies would be diverted to the German army and German civilians. Cities would be razed and the land allowed to return to forest or resettled by German colonists. Together, the Hunger Plan and \"Generalplan Ost\" would have led to the starvation of 80 million people in the Soviet Union. These partially fulfilled plans resulted in the democidal deaths of an estimated 19.3 million civilians and prisoners of war (POWs). During the course of the war, the Soviet Union lost a total of 27 million people; less than nine million of these were combat deaths. One in four of the Soviet population were killed or wounded.\n\nAround the time of the failed offensive against Moscow in December 1941, Hitler resolved that the Jews of Europe were to be exterminated immediately. While the murder of Jewish civilians had been ongoing in the occupied territories of Poland and the Soviet Union, plans for the total eradication of the Jewish population of Europe—eleven million people—were formalised at the Wannsee Conference on 20 January 1942. Some would be worked to death and the rest would be killed in the implementation of the Final Solution to the Jewish Question. There was a progression in methods in how the victims were killed. Initially the victims were killed by \"Einsatzgruppen\" firing squads, then by stationary gas chambers or by gas vans, but these methods proved impractical for an operation of this scale. By 1942 extermination camps equipped with gas chambers were established at Auschwitz, Chełmno, Sobibor, Treblinka, and elsewhere. The total number of Jews murdered is estimated at 5.5 to six million, including over a million children. \n\nThe Allies received information about the murders from the Polish government-in-exile and Polish leadership in Warsaw, based mostly on intelligence from the Polish underground. German citizens also had access to information about what was happening, as soldiers returning from the occupied territories would report on what they had seen and done. Historian Richard J. Evans states that most German citizens disapproved of the genocide.\n\nPoles were viewed by Nazis as subhuman non-Aryans, and during the German occupation of Poland 2.7 million ethnic Poles were killed. Polish civilians were subject to forced labour in German industry, internment, wholesale expulsions to make way for German colonists, and mass executions. The German authorities engaged in a systematic effort to destroy Polish culture and national identity. During operation AB-Aktion, many university professors and members of the Polish intelligentsia were arrested, transported to concentration camps, or executed. During the war, Poland lost an estimated 39 to 45 percent of its physicians and dentists, 26 to 57 percent of its lawyers, 15 to 30 percent of its teachers, 30 to 40 percent of its scientists and university professors and 18 to 28 percent of its clergy.\n\nDuring the course of the war, the Nazis captured 5.75 million Soviet prisoners of war (POWs), more than were captured by the Germans from all the other Allied powers combined. Of these, an estimated 3.3 million were killed by the Nazis, with 2.8 million of them being killed between June 1941 and January 1942. Many POWs starved to death or resorted to cannibalism while being held in open-air pens at Auschwitz and elsewhere.\n\nFrom 1942 onward, Soviet POWs were viewed as a source of forced labour, and received better treatment so they could work. By December 1944, 750,000 Soviet POWs were working, including in German armaments factories (in violation of the Hague and Geneva conventions), mines, and farms.\n\nAntisemitic legislation passed in 1933 led to the removal of all Jewish teachers, professors and officials from the education system. Most teachers were required to belong to the \"Nationalsozialistischer Lehrerbund\" (NSLB; National Socialist Teachers League) and university professors were required to join the National Socialist German Lecturers. Teachers had to take an oath of loyalty and obedience to Hitler and those who failed to show sufficient conformity to party ideals were often reported by students or fellow teachers and dismissed. Lack of funding for salaries led to many teachers leaving the profession and the average class size increased from 37 in 1927 to 43 in 1938 due to the resulting teacher shortage.\n\nFrequent and often contradictory directives were issued by Interior Minister Wilhelm Frick, Bernhard Rust of the \"Reichserziehungsministerium\" (Ministry of Education), and various other agencies regarding content of lessons and acceptable textbooks for use in primary and secondary schools. Books deemed unacceptable to the regime were removed from school libraries. Indoctrination in National Socialist thought was made compulsory in January 1934. Students selected as future members of the party elite were indoctrinated from the age of 12 at Adolf Hitler Schools for primary education and National Political Institutes of Education for secondary education. Detailed National Socialist indoctrination of future holders of elite military rank was undertaken at Order Castles.\n\nPrimary and secondary education focused on racial biology, population policy, culture, geography, and especially physical fitness. The curriculum in most subjects, including biology, geography and even arithmetic, was altered to change the focus to race. Military education became the central component of physical education and education in physics was oriented toward subjects with military applications, such as ballistics and aerodynamics. Students were required to watch all films prepared by the school division of the \"Propagandaministerium\" (Ministry of Public Enlightenment and Propaganda).\n\nAt universities, appointments to top posts were the subject of power struggles between the education ministry, the university boards, and the National Socialist German Students' League. In spite of pressure from the League and various government ministries, most university professors did not make changes to their lectures or syllabus during the Nazi period. This was especially true of universities located in predominantly Catholic regions. Enrolment at German universities declined from 104,000 students in 1931 to 41,000 in 1939, but enrolment in medical schools rose sharply as Jewish doctors had been forced to leave the profession, so medical graduates had good job prospects. From 1934, university students were required to attend frequent and time-consuming military training sessions run by the SA. First-year students also had to serve six months in a labour camp for the \"Reichsarbeitsdienst\" (National Labour Service); an additional ten weeks service were required of second-year students.\n\nWomen were a cornerstone of Nazi social policy and the Nazis opposed the feminist movement, claiming that it was the creation of Jewish intellectuals, instead advocating a patriarchal society in which the German woman would recognise that her \"world is her husband, her family, her children, and her home\". Soon after the seizure of power, feminist groups were shut down or incorporated into the National Socialist Women's League, which coordinated groups throughout the country to promote motherhood and household activities. Courses were offered on childrearing, sewing and cooking. Prominent feminists, including Anita Augspurg, Lida Gustava Heymann and Helene Stöcker, felt forced to live in exile. The League published the \"NS-Frauen-Warte\", the only NSDAP-approved women's magazine in Nazi Germany; despite some propaganda aspects, it was predominantly an ordinary woman's magazine.\n\nWomen were encouraged to leave the workforce, and the creation of large families by racially suitable women was promoted through a propaganda campaign. Women received a bronze award—known as the \"Ehrenkreuz der Deutschen Mutter\" (Cross of Honour of the German Mother)—for giving birth to four children, silver for six and gold for eight or more. Large families received subsidies to help with their utilities, school fees and household expenses. Though the measures led to increases in the birth rate, the number of families having four or more children declined by five percent between 1935 and 1940. Removing women from the workforce did not have the intended effect of freeing up jobs for men as women were for the most part employed as domestic servants, weavers or in the food and drink industries—jobs that were not of interest to men. Nazi philosophy prevented large numbers of women from being hired to work in munitions factories in the build-up to the war, so foreign labourers were brought in. After the war started, slave labourers were extensively used. In January 1943, Hitler signed a decree requiring all women under the age of fifty to report for work assignments to help the war effort. Thereafter women were funnelled into agricultural and industrial jobs and by September 1944 14.9 million women were working in munitions production.\n\nThe Nazi regime discouraged women from seeking higher education since Nazi leaders held conservative views about women and endorsed the idea that rational and theoretical work was alien to a woman's nature since they were considered inherently emotional and instinctive – as such, engaging in academics and careerism would only \"divert them from motherhood\". The number of women allowed to enroll in universities dropped drastically, as a law passed in April 1933 limited the number of females admitted to university to ten percent of the number of male attendees. Female enrolment in secondary schools dropped from 437,000 in 1926 to 205,000 in 1937. The number of women enrolled in post-secondary schools dropped from 128,000 in 1933 to 51,000 in 1938. However, with the requirement that men be enlisted into the armed forces during the war, women comprised half of the enrolment in the post-secondary system by 1944.\n\nWomen were expected to be strong, healthy and vital. The sturdy peasant woman who worked the land and bore strong children was considered ideal and athletic women were praised for being tanned from working outdoors. Organisations were created for the indoctrination of Nazi values and from 25 March 1939 membership in the Hitler Youth became compulsory for all children over the age of ten. The \"Jungmädelbund\" (Young Girls League) section of the Hitler Youth was for girls age 10 to 14 and the \"Bund Deutscher Mädel\" (BDM; League of German Girls) was for young women age 14 to 18. The BDM's activities focused on physical education, with activities such as running, long jumping, somersaulting, tightrope walking, marching and swimming.\n\nThe Nazi regime promoted a liberal code of conduct regarding sexual matters and was sympathetic to women who bore children out of wedlock. Promiscuity increased as the war progressed, with unmarried soldiers often intimately involved with several women simultaneously. Soldier's wives were frequently involved in extramarital relationships. Sex was sometimes used as a commodity to obtain better work from a foreign labourer. Pamphlets enjoined German women to avoid sexual relations with foreign workers as a danger to their blood.\n\nWith Hitler's approval, Himmler intended that the new society of the Nazi regime should destigmatise illegitimate births, particularly of children fathered by members of the SS, who were vetted for racial purity. His hope was that each SS family would have between four and six children. The \"Lebensborn\" (Fountain of Life) association, founded by Himmler in 1935, created a series of maternity homes where single mothers could be accommodated during their pregnancies. Both parents were examined for racial suitability before acceptance. The resulting children were often adopted into SS families. The homes were also made available to the wives of SS and NSDAP members, who quickly filled over half the available spots.\n\nExisting laws banning abortion except for medical reasons were strictly enforced by the Nazi regime. The number of abortions declined from 35,000 per year at the start of the 1930s to fewer than 2,000 per year at the end of the decade, though in 1935 a law was passed allowing abortions for eugenics reasons.\n\nNazi Germany had a strong anti-tobacco movement as pioneering research by Franz H. Müller in 1939 demonstrated a causal link between tobacco smoking and lung cancer. The Reich Health Office took measures to try to limit smoking, including producing lectures and pamphlets. Smoking was banned in many workplaces, on trains and among on-duty members of the military. Government agencies also worked to control other carcinogenic substances such as asbestos and pesticides. As part of a general public health campaign, water supplies were cleaned up, lead and mercury were removed from consumer products and women were urged to undergo regular screenings for breast cancer.\n\nGovernment-run health care insurance plans were available, but Jews were denied coverage starting in 1933. That same year, Jewish doctors were forbidden to treat government-insured patients. In 1937, Jewish doctors were forbidden to treat non-Jewish patients and in 1938 their right to practice medicine was removed entirely.\n\nMedical experiments, many of them pseudoscientific, were performed on concentration camp inmates beginning in 1941. The most notorious doctor to perform medical experiments was SS-\"Hauptsturmführer\" Dr. Josef Mengele, camp doctor at Auschwitz. Many of his victims died or were intentionally killed. Concentration camp inmates were made available for purchase by pharmaceutical companies for drug testing and other experiments.\n\nNazi society had elements supportive of animal rights and many people were fond of zoos and wildlife. The government took several measures to ensure the protection of animals and the environment. In 1933, the Nazis enacted a stringent animal-protection law that affected what was allowed for medical research. However, the law was only loosely enforced and in spite of a ban on vivisection the Ministry of the Interior readily handed out permits for experiments on animals.\n\nThe Reich Forestry Office under Göring enforced regulations that required foresters to plant a wide variety of trees to ensure suitable habitat for wildlife and a new Reich Animal Protection Act became law in 1933. The regime enacted the Reich Nature Protection Act in 1935 to protect the natural landscape from excessive economic development and it allowed for the expropriation of privately owned land to create nature preserves and aided in long-range planning. Perfunctory efforts were made to curb air pollution, but little enforcement of existing legislation was undertaken once the war began.\n\nWhen the Nazis seized power in 1933, roughly 67 percent of the population of Germany was Protestant, 33 percent was Roman Catholic, while Jews made up less than 1 percent. According to 1939 census, 54 percent considered themselves Protestant, 40 percent Roman Catholic, 3.5 percent \"Gottgläubig\" (God-believing; a Nazi religious movement) and 1.5 percent nonreligious.\n\nUnder the \"Gleichschaltung\" process, Hitler attempted to create a unified Protestant Reich Church from Germany's 28 existing Protestant state churches, with the ultimate goal of eradication of the churches in Germany. Pro-Nazi Ludwig Müller was installed as Reich Bishop and the pro-Nazi pressure group German Christians gained control of the new church. They objected to the Old Testament because of its Jewish origins and demanded that converted Jews be barred from their church. Pastor Martin Niemöller responded with the formation of the Confessing Church, from which some clergymen opposed the Nazi regime. When in 1935 the Confessing Church synod protested the Nazi policy on religion, 700 of their pastors were arrested. Müller resigned and Hitler appointed Hanns Kerrl as Minister for Church Affairs to continue efforts to control Protestantism. In 1936, a Confessing Church envoy protested to Hitler against the religious persecutions and human rights abuses. Hundreds more pastors were arrested. The church continued to resist and by early 1937 Hitler abandoned his hope of uniting the Protestant churches. Niemöller was arrested on 1 July 1937 and spent most of the next seven years in Sachsenhausen concentration camp and Dachau. Theological universities were closed and pastors and theologians of other Protestant denominations were also arrested.\n\nPersecution of the Catholic Church in Germany followed the Nazi takeover. Hitler moved quickly to eliminate political Catholicism, rounding up functionaries of the Catholic-aligned Bavarian People's Party and Catholic Centre Party, which along with all other non-Nazi political parties ceased to exist by July. The \"Reichskonkordat\" (Reich Concordat) treaty with the Vatican was signed in 1933, amid continuing harassment of the church in Germany. The treaty required the regime to honour the independence of Catholic institutions and prohibited clergy from involvement in politics. Hitler routinely disregarded the Concordat, closing all Catholic institutions whose functions were not strictly religious. Clergy, nuns and lay leaders were targeted, with thousands of arrests over the ensuing years, often on trumped-up charges of currency smuggling or immorality. Several Catholic leaders were targeted in the 1934 Night of the Long Knives assassinations. Most Catholic youth groups refused to dissolve themselves and Hitler Youth leader Baldur von Schirach encouraged members to attack Catholic boys in the streets. Propaganda campaigns claimed the church was corrupt, restrictions were placed on public meetings and Catholic publications faced censorship. Catholic schools were required to reduce religious instruction and crucifixes were removed from state buildings. \n\nPope Pius XI had the \"\"Mit brennender Sorge\"\" (\"With Burning Concern\") encyclical smuggled into Germany for Passion Sunday 1937 and read from every pulpit as it denounced the systematic hostility of the regime toward the church. In response, Goebbels renewed the regime's crackdown and propaganda against Catholics. Enrolment in denominational schools dropped sharply and by 1939 all such schools were disbanded or converted to public facilities. Later Catholic protests included the 22 March 1942 pastoral letter by the German bishops on \"The Struggle against Christianity and the Church\". About 30 percent of Catholic priests were disciplined by police during the Nazi era. A vast security network spied on the activities of clergy and priests were frequently denounced, arrested or sent to concentration camps – many to the dedicated clergy barracks at Dachau. In the areas of Poland annexed in 1939, the Nazis instigated a brutal suppression and systematic dismantling of the Catholic Church.\n\nAlfred Rosenberg, head of the NSDAP Office of Foreign Affairs and Hitler's appointed cultural and educational leader for Nazi Germany, considered Catholicism to be among the Nazis' chief enemies. He planned the \"extermination of the foreign Christian faiths imported into Germany\", and for the Bible and Christian cross to be replaced in all churches, cathedrals, and chapels with copies of \"Mein Kampf\" and the swastika. Other sects of Christianity were also targeted, with Chief of the NSDAP Chancellery Martin Bormann publicly proclaiming in 1941, \"National Socialism and Christianity are irreconcilable.\" Shirer writes that opposition to Christianity within NSDAP leadership was so pronounced that, \"the Nazi regime intended to eventually destroy Christianity in Germany, if it could, and substitute the old paganism of the early tribal Germanic gods and the new paganism of the Nazi extremists.\"\n\nWhile no unified resistance movement opposing the Nazi regime existed, acts of defiance such as sabotage and labour slowdowns took place, as well as attempts to overthrow the regime or assassinate Hitler. The banned Communist and Social Democratic parties set up resistance networks in the mid-1930s. These networks achieved little beyond fomenting unrest and initiating short-lived strikes. Carl Friedrich Goerdeler, who initially supported Hitler, changed his mind in 1936 and was later a participant in the July 20 plot. The Red Orchestra spy ring provided information to the Allies about Nazi war crimes, helped orchestrate escapes from Germany, and distributed leaflets. The group was detected by the Gestapo and more than 50 members were tried and executed in 1942. Communist and Social Democratic resistance groups resumed activity in late 1942, but were unable to achieve much beyond distributing leaflets. The two groups saw themselves as potential rival parties in post-war Germany, and for the most part did not co-ordinate their activities. The White Rose resistance group was primarily active in 1942–43, and many of its members were arrested or executed, with the final arrests taking place in 1944. Another civilian resistance group, the Kreisau Circle, had some connections with the military conspirators, and many of its members were arrested after the failed July 20 plot.\n\nWhile civilian efforts had an impact on public opinion, the army was the only organisation with the capacity to overthrow the government. A major plot by men in the upper echelons of the military originated in 1938. They believed Britain would go to war over Hitler's planned invasion of Czechoslovakia, and Germany would lose. The plan was to overthrow Hitler or possibly assassinate him. Participants included Generaloberst Ludwig Beck, Generaloberst Walther von Brauchitsch, Generaloberst Franz Halder, Admiral Wilhelm Canaris, and Generalleutnant Erwin von Witzleben, who joined a conspiracy headed by Oberstleutnant Hans Oster and Major Helmuth Groscurth of the Abwehr. The planned coup was cancelled after the signing of the Munich Agreement in September 1938. Many of the same people were involved in a coup planned for 1940, but again the participants changed their minds and backed down, partly because of the popularity of the regime after the early victories in the war. Attempts to assassinate Hitler resumed in earnest in 1943, with Henning von Tresckow joining Oster's group and attempting to blow up Hitler's plane in 1943. Several more attempts followed before the failed 20 July 1944 plot, which was at least partly motivated by the increasing prospect of a German defeat in the war. The plot, part of Operation Valkyrie, involved Claus von Stauffenberg planting a bomb in the conference room at Wolf's Lair at Rastenburg. Hitler, who narrowly survived, later ordered savage reprisals resulting in the execution of more than 4,900 people.\n\nThe regime promoted the concept of \"Volksgemeinschaft\", a national German ethnic community. The goal was to build a classless society based on racial purity and the perceived need to prepare for warfare, conquest and a struggle against Marxism. The German Labour Front founded the \"Kraft durch Freude\" (KdF; Strength Through Joy) organisation in 1933. As well as taking control of tens of thousands of privately run recreational clubs, it offered highly regimented holidays and entertainment such as cruises, vacation destinations and concerts.\n\nThe \"Reichskulturkammer\" (Reich Chamber of Culture) was organised under the control of the Propaganda Ministry in September 1933. Sub-chambers were set up to control aspects of cultural life such as film, radio, newspapers, fine arts, music, theatre and literature. Members of these professions were required to join their respective organisation. Jews and people considered politically unreliable were prevented from working in the arts, and many emigrated. Books and scripts had to be approved by the Propaganda Ministry prior to publication. Standards deteriorated as the regime sought to use cultural outlets exclusively as propaganda media.\n\nRadio became popular in Germany during the 1930s; over 70 percent of households owned a receiver by 1939, more than any other country. By July 1933, radio station staffs were purged of leftists and others deemed undesirable. Propaganda and speeches were typical radio fare immediately after the seizure of power, but as time went on Goebbels insisted that more music be played so that listeners would not turn to foreign broadcasters for entertainment.\n\nNewspapers, like other media, were controlled by the state; the Reich Press Chamber shut down or bought newspapers and publishing houses. By 1939, over two thirds of the newspapers and magazines were directly owned by the Propaganda Ministry. The NSDAP daily newspaper, the \"Völkischer Beobachter\" (\"Ethnic Observer\"), was edited by Rosenberg, who also wrote \"The Myth of the Twentieth Century\", a book of racial theories espousing Nordic superiority. Goebbels controlled the wire services and insisted that all newspapers in Germany only publish content favourable to the regime. Under Goebbels, the Propaganda Ministry issued two dozen directives every week on exactly what news should be published and what angles to use; the typical newspaper followed the directives closely, especially regarding what to omit. Newspaper readership plummeted, partly because of the decreased quality of the content and partly because of the surge in popularity of radio. Propaganda became less effective towards the end of the war, as people were able to obtain information outside of official channels.\n\nAuthors of books left the country in droves and some wrote material critical of the regime while in exile. Goebbels recommended that the remaining authors concentrate on books themed on Germanic myths and the concept of blood and soil. By the end of 1933, over a thousand books—most of them by Jewish authors or featuring Jewish characters—had been banned by the Nazi regime. Nazi book burnings took place; nineteen such events were held on the night of 10 May 1933. Tens of thousands of books from dozens of figures, including Albert Einstein, Sigmund Freud, Helen Keller, Alfred Kerr, Marcel Proust, Erich Maria Remarque, Upton Sinclair, Jakob Wassermann, H. G. Wells, and Émile Zola were publicly burned. Pacifist works, and literature espousing liberal, democratic values were targeted for destruction, as well as any writings supporting the Weimar Republic or those written by Jewish authors.\n\nHitler took a personal interest in architecture and worked closely with state architects Paul Troost and Albert Speer to create public buildings in a neoclassical style based on Roman architecture. Speer constructed imposing structures such as the Nazi party rally grounds in Nuremberg and a new Reich Chancellery building in Berlin. Hitler's plans for rebuilding Berlin included a gigantic dome based on the Pantheon in Rome and a triumphal arch more than double the height of the Arc de Triomphe in Paris. Neither structure was built.\n\nHitler's belief that abstract, Dadaist, expressionist and modern art were decadent became the basis for policy. Many art museum directors lost their posts in 1933 and were replaced by party members. Some 6,500 modern works of art were removed from museums and replaced with works chosen by a Nazi jury. Exhibitions of the rejected pieces, under titles such as \"Decadence in Art\", were launched in sixteen different cities by 1935. The Degenerate Art Exhibition, organised by Goebbels, ran in Munich from July to November 1937. The exhibition proved wildly popular, attracting over two million visitors.\n\nComposer Richard Strauss was appointed president of the \"Reichsmusikkammer\" (Reich Music Chamber) on its founding in November 1933. As was the case with other art forms, the Nazis ostracised musicians who were deemed racially unacceptable and for the most part disapproved of music that was too modern or atonal. Jazz was considered especially inappropriate and foreign jazz musicians left the country or were expelled. Hitler favoured the music of Richard Wagner, especially pieces based on Germanic myths and heroic stories, and attended the Bayreuth Festival each year from 1933 to 1942.\n\nMovies were popular in Germany in the 1930s and 1940s, with admissions of over a billion people in 1942, 1943 and 1944. By 1934, German regulations restricting currency exports made it impossible for US film makers to take their profits back to America, so the major film studios closed their German branches. Exports of German films plummeted, as their antisemitic content made them impossible to show in other countries. The two largest film companies, Universum Film AG and Tobis, were purchased by the Propaganda Ministry, which by 1939 was producing most German films. The productions were not always overtly propagandistic, but generally had a political subtext and followed party lines regarding themes and content. Scripts were pre-censored.\n\nLeni Riefenstahl's \"Triumph of the Will\" (1935)—documenting the 1934 Nuremberg Rally—and \"Olympia\" (1938)—covering the 1936 Summer Olympics—pioneered techniques of camera movement and editing that influenced later films. New techniques such as telephoto lenses and cameras mounted on tracks were employed. Both films remain controversial, as their aesthetic merit is inseparable from their propagandising of National Socialist ideals.\n\nThe Allied powers organised war crimes trials, beginning with the Nuremberg trials, held from November 1945 to October 1946, of 23 top Nazi officials. They were charged with four counts—conspiracy to commit crimes, crimes against peace, war crimes and crimes against humanity—in violation of international laws governing warfare. All but three of the defendants were found guilty and twelve were sentenced to death. Twelve Subsequent Nuremberg trials of 184 defendants were held between 1946 and 1949. Between 1946 and 1949, the Allies investigated 3,887 cases, of which 489 were brought to trial. The result was convictions of 1,426 people; 297 of these were sentenced to death and 279 to life in prison, with the remainder receiving lesser sentences. About 65 percent of the death sentences were carried out. Poland was more active than other nations in investigating war crimes, for example prosecuting 673 of the total 789 Auschwitz staff brought to trial.\n\nThe political programme espoused by Hitler and the NSDAP brought about a world war, leaving behind a devastated and impoverished Europe. Germany itself suffered wholesale destruction, characterised as \"Stunde Null\" (Zero Hour). The number of civilians killed during the Second World War was unprecedented in the history of warfare. As a result, Nazi ideology and the actions taken by the regime are almost universally regarded as gravely immoral. Historians, philosophers, and politicians often use the word \"evil\" to describe Hitler and the Nazi regime. Interest in Nazi Germany continues in the media and the academic world. While Evans remarks that the era \"exerts an almost universal appeal because its murderous racism stands as a warning to the whole of humanity\", young neo-Nazis enjoy the shock value the use Nazi symbols or slogans provides. The display or use of Nazi symbolism such as flags, swastikas, or greetings is illegal in Germany and Austria.\n\nThe process of denazification, which was initiated by the Allies as a way to remove Nazi Party members was only partially successful, as the need for experts in such fields as medicine and engineering was too great. However, expression of Nazi views was frowned upon, and those who expressed such views were frequently dismissed from their jobs. From the immediate post-war period through the 1950s, people avoided talking about the Nazi regime or their own wartime experiences. While virtually every family suffered losses during the war has a story to tell, Germans kept quiet about their experiences and felt a sense of communal guilt, even if they were not directly involved in war crimes.\n\nThe trial of Adolf Eichmann in 1961 and the broadcast of the television miniseries \"Holocaust\" in 1979 brought the process of \"Vergangenheitsbewältigung\" (coping with the past) to the forefront for many Germans. Once study of Nazi Germany was introduced into the school curriculum starting in the 1970s, people began researching the experiences of their family members. Study of the era and a willingness to critically examine its mistakes has led to the development of a strong democracy in Germany, but with lingering undercurrents of antisemitism and neo-Nazi thought.\n\n"}
{"id": "21214", "url": "https://en.wikipedia.org/wiki?curid=21214", "title": "Naraoiidae", "text": "Naraoiidae\n\nNaraoiidae is a family, of extinct, soft-shelled trilobite-like arthropods, that belongs to the order Nectaspida. Species included in the Naraoiidae are known from the second half of the Lower Cambrian to the end of the Upper Silurian. The total number of collection sites is limited and distributed over a vast period of time: Maotianshan Shale and Balang Formation (China), Burgess Shale and Bertie Formation (Canada), the Šárka Formation (Czech Republic), Emu Bay Shale (Australia), Idaho and Utah (USA). This is probably due to the rare occurrence of the right circumstances for soft tissue preservation, needed for these non-calcified exoskeletons.\n\nNaraoiids probably were deposit feeders (\"Naraoia\" and \"Pseudonaraoia\"), predators or scavengers (\"Misszhouia\"), living on the sea floor.\n\nThe species of the family \"Naraoiidae\" are almost flat (dorso-ventrally). The upper (or dorsal) side of the body consists of a non-calcified transversely oval or semi-circular headshield (cephalon), and a circular to long oval tailshield (pygidium) equal to or longer than the cephalon, without any body segments in between. The body is narrowed at the articulation between cephalon and pygidium. The antennas are long and many-segmented. There are no eyes. The 17 to 25 pairs of legs have two branches on a common basis, like trilobites. The outer (dorsal) branches of the limbs (exopods) have flattened side branches (setae) on the shaft (probably acting as gills). The inner branches (or endopods are composed of 6 or 7 segments (or podomeres).\n\nNaraoiidae lack thoracic segments (or tergites), while the species of the sister family Liwiidae have between 3 and 6 tergites.\n\nThe taxonomic placement of the Naraoiidae has long been debated until detailed appendages were uncovered, that showed that \"N. compacta\" shares biramous legs of very comparable anatomy with trilobites. Some debate is still going on if the parent taxon \"Nektaspida\" should be included in the Trilobita, or is better placed as a sister group.\n\n"}
{"id": "21215", "url": "https://en.wikipedia.org/wiki?curid=21215", "title": "Northwest Passage", "text": "Northwest Passage\n\nThe Northwest Passage (NWP) is, from the European and northern Atlantic point of view, the sea route to the Pacific Ocean through the Arctic Ocean, along the northern coast of North America via waterways through the Canadian Arctic Archipelago. The eastern route along the Arctic coasts of Norway and Siberia is accordingly called Northeast Passage (NEP).\n\nThe various islands of the archipelago are separated from one another and from the Canadian mainland by a series of Arctic waterways collectively known as the Northwest Passages or Northwestern Passages.\n\nFor centuries, European explorers sought a navigable passage as a possible trade route to Asia. An ice-bound northern route was discovered in 1850 by the Irish explorer Robert McClure; it was through a more southerly opening in an area explored by the Scotsman John Rae in 1854 that Norwegian Roald Amundsen made the first complete passage in 1903–1906. Until 2009, the Arctic pack ice prevented regular marine shipping throughout most of the year. Arctic sea ice decline has rendered the waterways more navigable for ice navigation.\n\nThe contested sovereignty claims over the waters may complicate future shipping through the region: the Canadian government maintains that the Northwestern Passages are part of Canadian Internal Waters, but the United States and various European countries claim that they are an international strait and transit passage, allowing free and unencumbered passage. If, as has been claimed, parts of the eastern end of the Passage are barely deep, the route's viability as a Euro-Asian shipping route is reduced. A Chinese shipping line is planning regular voyages of cargo ships using the passage to the eastern United States and Europe, after a successful passage by \"Nordic Orion\" of 73,500 tonnes deadweight tonnage in September 2013. Fully loaded, \"Nordic Orion\" sat too deep in the water to sail through the Panama Canal.\n\nBefore the Little Ice Age (late Middle Ages to the 19th century), Norwegian Vikings sailed as far north and west as Ellesmere Island, Skraeling Island and Ruin Island for hunting expeditions and trading with the Inuit and people of the Dorset culture who already inhabited the region. Between the end of the 15th century and the 20th century, colonial powers from Europe dispatched explorers in an attempt to discover a commercial sea route north and west around North America. The Northwest Passage represented a new route to the established trading nations of Asia.\n\nEngland called the hypothetical northern route the \"Northwest Passage\". The desire to establish such a route motivated much of the European exploration of both coasts of North America. When it became apparent that there was no route through the heart of the continent, attention turned to the possibility of a passage through northern waters. There was a lack of scientific knowledge about conditions; for instance, some people believed that seawater was incapable of freezing. (As late as the mid-18th century, Captain James Cook had reported that Antarctic icebergs had yielded fresh water, seemingly confirming the hypothesis.) Explorers thought that an open water route close to the North Pole must exist. The belief that a route lay to the far north persisted for several centuries and led to numerous expeditions into the Arctic. Many ended in disaster, including that by Sir John Franklin in 1845. While searching for him the McClure Arctic Expedition discovered the Northwest Passage in 1850.\n\nIn 1906, the Norwegian explorer Roald Amundsen first successfully completed a passage from Greenland to Alaska in the sloop . Since that date, several fortified ships have made the journey.\n\nFrom east to west, the direction of most early exploration attempts, expeditions entered the passage from the Atlantic Ocean via the Davis Strait and through Baffin Bay. Five to seven routes have been taken through the Canadian Arctic Archipelago, via the McClure Strait, Dease Strait, and the Prince of Wales Strait, but not all of them are suitable for larger ships. From there ships passed through waterways through the Beaufort Sea, Chukchi Sea, and Bering Strait (separating Russia and Alaska), into the Pacific Ocean.\n\nIn the 21st century, major changes to the ice pack due to climate change have stirred speculation that the passage may become clear enough of ice to permit safe commercial shipping for at least part of the year. On August 21, 2007, the Northwest Passage became open to ships without the need of an icebreaker. According to Nalan Koc of the Norwegian Polar Institute, this was the first time the Passage has been clear since they began keeping records in 1972. The Northwest Passage opened again on August 25, 2008. It is usually reported in mainstream medias that ocean thawing will open up the Northwest Passage (and within it, the Northern Sea Route) for various kind of ships, making it possible to sail around the Arctic ice cap. and possibly cutting thousands of miles off shipping routes. Warning that the NASA satellite images indicated the Arctic may have entered a \"death spiral\" caused by climate change, Professor Mark Serreze, a sea ice specialist at the U.S. National Snow and Ice Data Center (NSIDC) said: \"The passages are open. It's a historic event. We are going to see this more and more as the years go by.\"\n\nOn the other hand, some thick sections of ice will remain hard to melt in the shorter term. Such drifting and large chunks of ice, especially in springtime, can be problematic as they can clog entire straits or severely damage a ship's hull. Cargo routes may therefore be slower and uncertain, depending on prevailing conditions and the ability to predict them. Because a plurality of containerized traffic operates in a just-in-time mode (which does not tolerate delays well) and the relative isolation of the passage (which impedes shipping companies from optimizing their operations by grouping multiple stopovers on a same itinerary), the Northwest Passage and other Arctic routes are not always seen as promising shipping lanes by industry insiders, at least for the time being. The uncertainty related to physical damages to ships is also thought to translate into higher insurance premiums, especially because of the technical challenges posed by Arctic navigation (as of 2014, only 12 percent of Canada's Arctic waters have been charted to modern standards).\n\nDue to Arctic shrinkage, the Beluga group of Bremen, Germany, sent the first Western commercial vessels through the Northern Sea Route (Northeast Passage) in 2009. Canada's Prime Minister Stephen Harper announced that \"ships entering the North-West passage should first report to his government.\"\n\nThe first commercial cargo ship to have sailed through the Northwest Passage was in August 1969. SS \"Manhattan\", of 115,000 deadweight tonnage, was the largest commercial vessel ever to navigate the Northwest Passage.\n\nThe largest passenger ship to navigate the Northwest Passage was the cruise liner of gross tonnage 69,000. Starting on August 10, 2016, the ship sailed from Vancouver to New York City with 1,500 passengers and crew, taking 28 days.\n\nIn 2018, two of the freighters leaving Baffinland's port in the Milne Inlet, on Baffin Island's north shore, were bound for ports in Asia. Those freighters did not sail west through the remainder of the Northwest Passage, they sailed east, rounded the tip of Greenland, and transitted Russia's Northern Sea Route.\n\nThe Northwest Passage includes three sections:\n\nMany attempts were made to find a salt water exit west from Hudson Bay, but the Fury and Hecla Strait in the far north is blocked by ice. The eastern entrance and main axis of the northwest passage, the Parry Channel, was found in 1819. The approach from the west through Bering Strait is impractical because of the need to sail around ice near Point Barrow. East of Point Barrow the coast is fairly clear in summer. This area was mapped in pieces from overland in 1821–1839. This leaves the large rectangle north of the coast, south of Parry Channel and east of Baffin Island. This area was mostly mapped in 1848–1854 by ships looking for Franklin's lost expedition. The first crossing was made by Amundsen in 1903–1905. He used a small ship and hugged the coast.\n\nThe International Hydrographic Organization defines the limits of the Northwestern Passages as follows:\n\nAs a result of their westward explorations and their settlement of Greenland, the Vikings sailed as far north and west as Ellesmere Island, Skraeling Island for hunting expeditions and trading with Inuit groups. The subsequent arrival of the Little Ice Age is thought to have been one of the reasons that European seafaring into the Northwest Passage ceased until the late 15th century.\n\nIn 1539, Hernán Cortés commissioned Francisco de Ulloa to sail along the Baja California Peninsula on the western coast of North America. Ulloa concluded that the Gulf of California was the southernmost section of a strait supposedly linking the Pacific with the Gulf of Saint Lawrence. His voyage perpetuated the notion of the Island of California and saw the beginning of a search for the Strait of Anián.\n\nThe strait probably took its name from Ania, a Chinese province mentioned in a 1559 edition of Marco Polo's book; it first appears on a map issued by Italian cartographer Giacomo Gastaldi about 1562. Five years later Bolognino Zaltieri issued a map showing a narrow and crooked Strait of Anian separating Asia from the Americas. The strait grew in European imagination as an easy sea lane linking Europe with the residence of Khagan (the Great Khan) in Cathay (northern China).\n\nCartographers and seamen tried to demonstrate its reality. Sir Francis Drake sought the western entrance in 1579. The Greek pilot Juan de Fuca, sailing from Acapulco (in Mexico) under the flag of the Spanish crown, claimed he had sailed the strait from the Pacific to the North Sea and back in 1592. The Spaniard Bartholomew de Fonte claimed to have sailed from Hudson Bay to the Pacific via the strait in 1640.\n\nThe first recorded attempt to discover the Northwest Passage was the east-west voyage of John Cabot in 1497, sent by Henry VII in search of a direct route to the Orient. In 1524, Charles V sent Estêvão Gomes to find a northern Atlantic passage to the Spice Islands. An English expedition was launched in 1576 by Martin Frobisher, who took three trips west to what is now the Canadian Arctic in order to find the passage. Frobisher Bay, which he first charted, is named after him.\n\nAs part of another expedition, in July 1583 Sir Humphrey Gilbert, who had written a treatise on the discovery of the passage and was a backer of Frobisher, claimed the territory of Newfoundland for the English crown. On August 8, 1585, the English explorer John Davis entered Cumberland Sound, Baffin Island.\n\nThe major rivers on the east coast were also explored in case they could lead to a transcontinental passage. Jacques Cartier's explorations of the Saint Lawrence River in 1535 were initiated in hope of finding a way through the continent. Cartier became persuaded that the St. Lawrence was the Passage; when he found the way blocked by rapids at what is now Montreal, he was so certain that these rapids were all that was keeping him from China (in French, \"la Chine\"), that he named the rapids for China. Samuel de Champlain renamed them Sault Saint-Louis in 1611, but the name was changed to Lachine Rapids in the mid-19th century.\n\nIn 1602, George Weymouth became the first European to explore what would later be called Hudson Strait when he sailed into the Strait. Weymouth's expedition to find the Northwest Passage was funded jointly by the British East India Company and the Muscovy Company. \"Discovery\" was the same ship used by Henry Hudson on his final voyage.\n\nJohn Knight, employed by the British East India Company and the Muscovy Company, set out in 1606 to follow up on Weymouth's discoveries and find the Northwest Passage. After his ship ran aground and was nearly crushed by ice, Knight disappeared while searching for a better anchorage.\n\nIn 1609, Henry Hudson sailed up what is now called the Hudson River in search of the Passage; encouraged by the saltiness of the water in the estuary, he reached present-day Albany, New York, before giving up. On September 14, 1609, the explorer Henry Hudson entered the Tappan Zee while sailing upstream from New York Harbor. At first, Hudson believed the widening of the river indicated that he had found the Northwest Passage. He proceeded upstream as far as present-day Troy before concluding that no such strait existed there. He later explored the Arctic and Hudson Bay.\n\nIn 1611, while in James Bay, Hudson's crew mutinied. They set Hudson and his teenage son John, along with seven sick, infirm, or loyal crewmen, adrift in a small open boat. He was never seen again. Cree oral legend reports that the survivors lived and traveled with the Cree for more than a year.\n\nA mission was sent out in 1612, again in \"Discovery\", commanded by Sir Thomas Button to find Henry Hudson and continue through the Northwest Passage. After failing to find Hudson, and exploring the west coast of Hudson Bay, Button returned home due to illness in the crew. In 1614, William Gibbons attempted to find the Passage, but was turned back by ice. The next year, 1615, Robert Bylot, a survivor of Hudson's crew, returned to Hudson Strait in \"Discovery\", but was turned back by ice. Bylot tried again in 1616 with William Baffin. They sailed as far as Lancaster Sound and reached 77°45′ North latitude, a record which stood for 236 years, before being blocked by ice.\n\nOn May 9, 1619, under the auspices of King Christian IV of Denmark–Norway, Jens Munk set out with 65 men and the king's two ships, \"Einhörningen\" (Unicorn), a small frigate, and \"Lamprenen\" (Lamprey), a sloop, which were outfitted under his own supervision. His mission was to discover the Northwest Passage to the Indies and China. Munk penetrated Davis Strait as far north as 69°, found Frobisher Bay, and then spent almost a month fighting his way through Hudson Strait. In September 1619, he found the entrance to Hudson Bay and spent the winter near the mouth of the Churchill River. Cold, famine, and scurvy destroyed so many of his men that only he and two other men survived. With these men, he sailed for home with \"Lamprey\" on July 16, 1620, reaching Bergen, Norway, on September 20, 1620.\n\nRené-Robert Cavelier, Sieur de La Salle built the sailing ship, , in his quest to find the Northwest Passage via the upper Great Lakes. \"Le Griffon\" disappeared in 1679 on the return trip of her maiden voyage. In the spring of 1682, La Salle made his famous voyage down the Mississippi River to the Gulf of Mexico. La Salle led an expedition from France in 1684 to establish a French colony on the Gulf of Mexico. He was murdered by his followers in 1687.\nHenry Ellis, born in Ireland, was part of a company aiming to discover the Northwest Passage in May 1746. After the difficult extinction of a fire on board the ship, he sailed to Greenland, where he traded goods with the Inuit peoples on July 8, 1746. He crossed to the town of Fort Nelson and spent the summer on the Hayes River. He renewed his efforts in June 1747, without success, before returning to England.\n\nIn 1772, Samuel Hearne travelled overland northwest from Hudson Bay to the Arctic Ocean, thereby proving that there was no strait connecting Hudson Bay to the Pacific Ocean.\n\n Most Northwest Passage expeditions originated in Europe or on the east coast of North America, seeking to traverse the Passage in the westbound direction. Some progress was made in exploring the western reaches of the imagined passage.\n\nIn 1728 Vitus Bering, a Danish Navy officer in Russian service, used the strait first discovered by Semyon Dezhnyov in 1648 but later accredited to and named after Bering (the Bering Strait). He concluded that North America and Russia were separate land masses by sailing between them. In 1741 with Lieutenant Aleksei Chirikov, he explored seeking further lands beyond Siberia. While they were separated, Chirikov discovered several of the Aleutian Islands while Bering charted the Alaskan region. His ship was wrecked off the Kamchatka Peninsula, as many of his crew were disabled by scurvy.\n\nThe Spanish made several voyages to the northwest coast of North America during the late 18th century. Determining whether a Northwest Passage existed was one of the motives for their efforts. Among the voyages that involved careful searches for a Passage included the 1775 and 1779 voyages of Juan Francisco de la Bodega y Quadra. The journal of Francisco Antonio Mourelle, who served as Quadra's second in command in 1775, fell into English hands. It was translated and published in London, stimulating exploration.\n\nCaptain James Cook made use of the journal during his explorations of the region. In 1791 Alessandro Malaspina sailed to Yakutat Bay, Alaska, which was rumoured to be a Passage. In 1790 and 1791 Francisco de Eliza led several exploring voyages into the Strait of Juan de Fuca, searching for a possible Northwest Passage and finding the Strait of Georgia. To fully explore this new inland sea, an expedition under Dionisio Alcalá Galiano was sent in 1792. He was explicitly ordered to explore all channels that might turn out to be a Northwest Passage.\n\nIn 1776 Captain James Cook was dispatched by the Admiralty in Great Britain on an expedition to explore the Passage. A 1745 act, when extended in 1775, promised a £20,000 prize for whoever discovered the passage. Initially the Admiralty had wanted Charles Clerke to lead the expedition, with Cook (in retirement following his exploits in the Pacific) acting as a consultant. However, Cook had researched Bering's expeditions, and the Admiralty ultimately placed their faith in the veteran explorer to lead, with Clerke accompanying him.\n\nAfter journeying through the Pacific, to make an attempt from the west, Cook began at Nootka Sound in April 1778. He headed north along the coastline, charting the lands and searching for the regions sailed by the Russians 40 years previously. The Admiralty's orders had commanded the expedition to ignore all inlets and rivers until they reached a latitude of 65°N. Cook, however, failed to make any progress in sighting a Northwestern Passage.\n\nVarious officers on the expedition, including William Bligh, George Vancouver, and John Gore, thought the existence of a route was 'improbable'. Before reaching 65°N they found the coastline pushing them further south, but Gore convinced Cook to sail on into the Cook Inlet in the hope of finding the route. They continued to the limits of the Alaskan peninsula and the start of the chain of Aleutian Islands. Despite reaching 70°N, they encountered nothing but icebergs.\n\nFrom 1792 to 1794, the Vancouver Expedition (led by George Vancouver who had previously accompanied Cook ) surveyed in detail all the passages from the Northwest Coast. He confirmed that there was no such passage south of the Bering Strait. This conclusion was supported by the evidence of Alexander MacKenzie, who explored the Arctic and Pacific oceans in 1793.\n\nIn the first half of the 19th century, some parts of the Northwest Passage (north of the Bering Strait) were explored separately by many expeditions, including those by John Ross, Elisha Kent Kane, William Edward Parry, and James Clark Ross; overland expeditions were also led by John Franklin, George Back, Peter Warren Dease, Thomas Simpson, and John Rae. In 1826 Frederick William Beechey explored the north coast of Alaska, discovering Point Barrow.\n\nSir Robert McClure was credited with the discovery of the Northwest Passage in 1851 when he looked across McClure Strait from Banks Island and viewed Melville Island. However, this strait was not navigable to ships at that time. The only usable route linking the entrances of Lancaster Sound and Dolphin and Union Strait was discovered by John Rae in 1854.\n\nIn 1845 a lavishly equipped two-ship expedition led by Sir John Franklin sailed to the Canadian Arctic to chart the last unknown swaths of the Northwest Passage. Confidence was high, as they estimated there was less than remaining of unexplored Arctic mainland coast. When the ships failed to return, relief expeditions and search parties explored the Canadian Arctic, which resulted in a thorough charting of the region, along with a possible passage. Many artifacts from the expedition were found over the next century and a half, including notes that the ships were ice-locked in 1846 near King William Island, about halfway through the passage, and unable to break free. Records showed Franklin died in 1847 and Captain Francis Rawdon Moira Crozier took over command. In 1848 the expedition abandoned the two ships and its members tried to escape south across the tundra by sledge. Although some of the crew may have survived into the early 1850s, no evidence has ever been found of any survivors. In 1853 explorer John Rae was told by local Inuit about the disastrous fate of Franklin's expedition, but his reports were not welcomed in Britain.\n\nStarvation, exposure and scurvy all contributed to the men's deaths. In 1981 Owen Beattie, an anthropologist from the University of Alberta, examined remains from sites associated with the expedition. This led to further investigations and the examination of tissue and bone from the frozen bodies of three seamen, John Torrington, William Braine and John Hartnell, exhumed from the permafrost of Beechey Island. Laboratory tests revealed high concentrations of lead in all three (the expedition carried 8,000 tins of food sealed with a lead-based solder). Another researcher has suggested botulism caused deaths among crew members. New evidence, confirming reports first made by John Rae in 1854 based on Inuit accounts, has shown that the last of the crew resorted to cannibalism of deceased members in an effort to survive.\n\nDuring the search for Franklin, Commander Robert McClure and his crew in traversed the Northwest Passage from west to east in the years 1850 to 1854, partly by ship and partly by sledge. McClure started out from England in December 1849, sailed the Atlantic Ocean south to Cape Horn and entered the Pacific Ocean. He sailed the Pacific north and passed through the Bering Strait, turning east at that point and reaching Banks Island.\n\nMcClure's ship was trapped in the ice for three winters near Banks Island, at the western end of Viscount Melville Sound. Finally McClure and his crew—who were by that time dying of starvation—were found by searchers who had travelled by sledge over the ice from a ship of Sir Edward Belcher's expedition. They rescued McClure and his crew, returning with them to Belcher's ships, which had entered the Sound from the east. McClure and his crew returned to England in 1854 on one of Belcher's ships. They were the first people known to circumnavigate the Americas and to discover and transit the Northwest Passage, albeit by ship and by sledge over the ice. (Both McClure and his ship were found by a party from HMS \"Resolute\", one of Belcher's ships, so his sledge journey was relatively short.)\n\nThis was an astonishing feat for that day and age, and McClure was knighted and promoted in rank. (He was made rear-admiral in 1867.) Both he and his crew also shared £10,000 awarded them by the British Parliament. In July 2010 Canadian archaeologists found his ship, HMS \"Investigator,\" fairly intact but sunk about below the surface.\n\nThe expeditions by Franklin and McClure were in the tradition of British exploration: well-funded ship expeditions using modern technology, and usually including British Naval personnel. By contrast, John Rae was an employee of the Hudson's Bay Company, which operated a far-flung trade network and drove exploration of the Canadian North. They adopted a pragmatic approach and tended to be land-based. While Franklin and McClure tried to explore the passage by sea, Rae explored by land. He used dog sleds and techniques of surviving in the environment which he had learned from the native Inuit. The Franklin and McClure expeditions each employed hundreds of personnel and multiple ships. John Rae's expeditions included fewer than ten people and succeeded. Rae was also the explorer with the best safety record, having lost only one man in years of traversing Arctic lands. In 1854, Rae returned to the cities with information from the Inuit about the disastrous fate of the Franklin expedition.\n\nThe first explorer to conquer the Northwest Passage solely by ship was the Norwegian explorer Roald Amundsen. In a three-year journey between 1903 and 1906, Amundsen explored the passage with a crew of six. Amundsen, who had sailed to escape creditors seeking to stop the expedition, completed the voyage in the converted 45 net register tonnage () herring boat \"Gjøa\". \"Gjøa\" was much smaller than vessels used by other Arctic expeditions and had a shallow draft. Amundsen intended to hug the shore, live off the limited resources of the land and sea through which he was to travel, and had determined that he needed to have a tiny crew to make this work. (Trying to support much larger crews had contributed to the catastrophic failure of John Franklin's expedition fifty years previously). The ship's shallow draft was intended to help her traverse the shoals of the Arctic straits.\n\nAmundsen set out from Kristiania (Oslo) in June 1903 and was west of the Boothia Peninsula by late September. \"Gjøa\" was put into a natural harbour on the south shore of King William Island; by October 3 she was iced in. There the expedition remained for nearly two years, with the expedition members learning from the local Inuit people and undertaking measurements to determine the location of the North Magnetic Pole. The harbour, now known as Gjoa Haven, later developed as the only permanent settlement on the island.\n\nAfter completing the Northwest Passage portion of this trip and having anchored near Herschel Island, Amundsen skied to the city of Eagle, Alaska. He sent a telegram announcing his success and skied the return to rejoin his companions. Although his chosen east–west route, via the Rae Strait, contained young ice and thus was navigable, some of the waterways were extremely shallow ( deep), making the route commercially impractical.\n\nThe first traversal of the Northwest Passage via dog sled was accomplished by Greenlander Knud Rasmussen while on the Fifth Thule Expedition (1921–1924). Rasmussen and two Greenland Inuit travelled from the Atlantic to the Pacific over the course of 16 months via dog sled.\n\nCanadian Royal Canadian Mounted Police officer Henry Larsen was the second to sail the passage, crossing west to east, leaving Vancouver on June 23, 1940 and arriving at Halifax on October 11, 1942. More than once on this trip, he was uncertain whether , a Royal Canadian Mounted Police \"ice-fortified\" schooner, would survive the pressures of the sea ice. At one point, Larsen wondered \"if we had come this far only to be crushed like a nut on a shoal and then buried by the ice.\" The ship and all but one of her crew survived the winter on Boothia Peninsula. Each of the men on the trip was awarded a medal by Canada's sovereign, King George VI, in recognition of this feat of Arctic navigation.\n\nLater in 1944, Larsen's return trip was far more swift than his first. He made the trip in 86 days to sail back from Halifax, Nova Scotia, to Vancouver, British Columbia. He set a record for traversing the route in a single season. The ship, after extensive upgrades, followed a more northerly, partially uncharted route.\n\nIn 1954, completed the east-to-west transit, under the command of Captain O.C.S. Robertson, conducting hydrographic soundings along the route. She was the first warship (and the first deep draft ship) to transit the Northwest Passage and the first warship to circumnavigate North America. In 1956, HMCS \"Labrador\" again completed the east-to-west transit, this time under the command of Captain T.C. Pullen.\n\nOn July 1, 1957, the United States Coast Guard cutter departed in company with and to search for a deep-draft channel through the Arctic Ocean and to collect hydrographic information. The US Coast Guard Squadron was escorted through Bellot Strait and the Eastern Arctic by HMCS \"Labrador\". Upon her return to Greenland waters, \"Storis\" became the first U.S.-registered vessel to circumnavigate North America. Shortly after her return in late 1957, she was reassigned to her new home port of Kodiak, Alaska.\n\nIn 1960, completed the first submarine transit of the Northwest Passage, heading east-to-west.\n\nIn 1969, SS \"Manhattan\" made the passage, accompanied by the Canadian icebreakers and . The U.S. Coast Guard icebreakers and also sailed in support of the expedition.\n\n\"Manhattan\" was a specially reinforced supertanker sent to test the viability of the passage for the transport of oil. While \"Manhattan\" succeeded, the route was deemed not to be cost-effective. The United States built the Alaska Pipeline instead.\n\nIn June 1977, sailor Willy de Roos left Belgium to attempt the Northwest Passage in his steel yacht \"Williwaw\". He reached the Bering Strait in September and after a stopover in Victoria, British Columbia, went on to round Cape Horn and sail back to Belgium, thus being the first sailor to circumnavigate the Americas entirely by ship.\n\nIn 1981 as part of the Transglobe Expedition, Ranulph Fiennes and Charles R. Burton completed the Northwest Passage. They left Tuktoyaktuk on July 26, 1981, in the open Boston Whaler and reached Tanquary Fiord on August 31, 1981. Their journey was the first open-boat transit from west to east and covered around , taking a route through Dolphin and Union Strait following the south coast of Victoria and King William islands, north to Resolute Bay via Franklin Strait and Peel Sound, around the south and east coasts of Devon Island, through Hell Gate and across Norwegian Bay to Eureka, Greely Bay and the head of Tanquary Fiord. Once they reached Tanquary Fiord, they had to trek via Lake Hazen to Alert before setting up their winter base camp.\n\nIn 1984, the commercial passenger vessel (which sank in the Antarctic Ocean in 2007) became the first cruise ship to navigate the Northwest Passage.\n\nIn July 1986, Jeff MacInnis and Mike Beedell set out on an catamaran called \"Perception\" on a 100-day sail, west to east, through the Northwest Passage. This pair was the first to sail the passage, although they had the benefit of doing so over a couple of summers.\n\nIn July 1986, David Scott Cowper set out from England in a lifeboat named \"Mabel El Holland\", and survived three Arctic winters in the Northwest Passage before reaching the Bering Strait in August 1989. He continued around the world via the Cape of Good Hope to return to England on September 24, 1990. His was the first vessel to circumnavigate the world via the Northwest Passage.\n\nOn July 1, 2000, the Royal Canadian Mounted Police patrol vessel , having assumed the name \"St Roch II\", departed Vancouver on a \"Voyage of Rediscovery\". \"Nadon\"s mission was to circumnavigate North America via the Northwest Passage and the Panama Canal, recreating the epic voyage of her predecessor, \"St. Roch.\" The Voyage of Rediscovery was intended to raise awareness concerning \"St. Roch\" and kick off the fund-raising efforts necessary to ensure the continued preservation of \"St. Roch\". The voyage was organized by the Vancouver Maritime Museum and supported by a variety of corporate sponsors and agencies of the Canadian government.\n\n\"Nadon\" is an aluminum, catamaran-hulled, high-speed patrol vessel. To make the voyage possible, she was escorted and supported by the Canadian Coast Guard icebreaker . The Coast Guard vessel was chartered by the Voyage of Rediscovery and crewed by volunteers. Throughout the voyage, she provided a variety of necessary services, including provisions and spares, fuel and water, helicopter facilities, and ice escort; she also conducted oceanographic research during the voyage. The Voyage of Rediscovery was completed in five and a half months, with \"Nadon\" reaching Vancouver on December 16, 2000.\n\nOn September 1, 2001, \"Northabout\", an aluminium sailboat with diesel engine, built and captained by Jarlath Cunnane, completed the Northwest Passage east-to-west from Ireland to the Bering Strait. The voyage from the Atlantic to the Pacific was completed in 24 days. Cunnane cruised in \"Northabout\" in Canada for two years before returning to Ireland in 2005 via the Northeast Passage; he completed the first east-to-west circumnavigation of the pole by a single sailboat. The Northeast Passage return along the coast of Russia was slower, starting in 2004, requiring an ice stop and winter over in Khatanga, Siberia. He returned to Ireland via the Norwegian coast in October 2005. On January 18, 2006, the Cruising Club of America awarded Jarlath Cunnane their Blue Water Medal, an award for \"meritorious seamanship and adventure upon the sea displayed by amateur sailors of all nationalities.\"\n\nOn July 18, 2003, a father-and-son team, Richard and Andrew Wood, with Zoe Birchenough, sailed the yacht \"Norwegian Blue\" into the Bering Strait. Two months later she sailed into the Davis Strait to become the first British yacht to transit the Northwest Passage from west to east. She also became the only British vessel to complete the Northwest Passage in one season, as well as the only British sailing yacht to return from there to British waters.\n\nIn 2006, a scheduled cruise liner () successfully ran the Northwest Passage, helped by satellite images telling the location of sea ice.\n\nOn May 19, 2007, a French sailor, Sébastien Roubinet, and one other crew member left Anchorage, Alaska, in \"Babouche\", a ice catamaran designed to sail on water and slide over ice. The goal was to navigate west to east through the Northwest Passage by sail only. Following a journey of more than , Roubinet reached Greenland on September 9, 2007, thereby completing the first Northwest Passage voyage made in one season without engine.\nIn April 2009, planetary scientist Pascal Lee and a team of four on the Northwest Passage Drive Expedition drove the HMP \"Okarian\" Humvee rover a record-setting on sea-ice from Kugluktuk to Cambridge Bay, Nunavut, the longest distance driven on sea-ice in a road vehicle. The HMP \"Okarian\" was being ferried from the North American mainland to the Haughton–Mars Project (HMP) Research Station on Devon Island, where it would be used as a simulator of future pressurized rovers for astronauts on the Moon and Mars. The HMP \"Okarian\" was eventually flown from Cambridge Bay to Resolute Bay in May 2009, and then driven again on sea-ice by Lee and a team of five from Resolute to the West coast of Devon Island in May 2010. The HMP \"Okarian\" reached the HMP Research Station in July 2011. The Northwest Passage Drive Expedition is captured in the motion picture documentary film \"Passage To Mars\" (2016).\n\nIn 2009, sea ice conditions were such that at least nine small vessels and two cruise ships completed the transit of the Northwest Passage. These trips included one by Eric Forsyth on board the Westsail sailboat \"Fiona\", a boat he built in the 1980s. Self-financed, Forsyth, a retired engineer from the Brookhaven National Laboratory, and winner of the Cruising Club of America's Blue Water Medal, sailed the Canadian Archipelago with sailor Joey Waits, airline captain Russ Roberts and carpenter David Wilson. After successfully sailing the Passage, the 77-year-old Forsyth completed the circumnavigation of North America, returning to his home port on Long Island, New York.\n\nCameron Dueck and his crew aboard the 40-foot sailing yacht Silent Sound also transited in the summer of 2009. Their voyage began in Victoria, BC on June 6 and they arrived in Halifax on October 10. Dueck wrote a book about the voyage called The New Northwest Passage.\n\nOn August 28, 2010, Bear Grylls and a team of five were the first rigid inflatable boat (RIB) crew to complete a point-to-point navigation between Pond Inlet on Baffin Island and Tuktoyaktuk in the Northwest Territories. A Northwest Passage requires crossing the Arctic Circle twice, once each in the Atlantic and the Pacific oceans.\n\nOn August 30, 2012 Sailing yacht , , an English SY, successfully completed the Northwest Passage in Nome, Alaska, while sailing a northern route never sailed by a sailing pleasure vessel before. After six cruising seasons in the Arctic (Greenland, Baffin Bay, Devon Island, Kane Basin, Lancaster Sound, Peel Sound, Regent Sound) and four seasons in the South (Antarctic Peninsula, Patagonia, Falkland Islands, South Georgia), SY \"Billy Budd\", owned by and under the command of an Italian sporting enthusiast, Mariacristina Rapisardi. Crewed by Marco Bonzanigo, five Italian friends, one Australian, one Dutch, one South African, and one New Zealander, it sailed through the Northwest Passage. The northernmost route was chosen. \"Billy Budd\" sailed through the Parry Channel, Viscount Melville Sound and Prince of Wales Strait, a channel long and wide which flows south into the Amundsen Gulf. During the passage \"Billy Budd\" – likely a first for a pleasure vessel – anchored in Winter Harbour in Melville Island, the very same site where almost 200 years ago Sir William Parry was blocked by ice and forced to winter.\n\nOn August 29, 2012, the Swedish yacht \"Belzebub II,\" a fibreglass cutter captained by Canadian Nicolas Peissel, Swede Edvin Buregren and Morgan Peissel, became the first sailboat in history to sail through McClure Strait, part of a journey of achieving the most northerly Northwest Passage. \"Belzebub II\" departed Newfoundland following the coast of Greenland to Qaanaaq before tracking the sea ice to Grise Fiord, Canada's most northern community. From there the team continued through Parry Channel into McClure Strait and the Beaufort Sea, tracking the highest latitudes of 2012's record sea ice depletion before completing their Northwest Passage September 14, 2012. The expedition received extensive media coverage, including recognition by former U.S. Vice President Al Gore. The accomplishment is recorded in the Polar Scott Institute's record of Northwest Passage Transits and recognized by the Explorers Club and the Royal Canadian Geographic Society.\n\nAt 18:45 GMT September 18, 2012, \"Best Explorer\", a steel cutter , skipper Nanni Acquarone, passing between the two Diomedes, was the first Italian sailboat to complete the Northwest Passage along the classical Amundsen route. Twenty-two Italian amateur sailors took part of the trip, in eight legs from Tromsø, Norway, to King Cove, Alaska, totalling .\n\nSetting sail from Nome, Alaska, on August 18, 2012, and reaching Nuuk, Greenland, on September 12, 2012, became the largest passenger vessel to transit the Northwest Passage. The ship, carrying 481 passengers, for 26 days and at sea, followed in the path of Captain Roald Amundsen. \"The World\" transit of the Northwest Passage was documented by \"National Geographic\" photographer Raul Touzon.\n\nIn September 2013, became the first commercial bulk carrier to transit the Northwest Passage. She was carrying a cargo of of coking coal from Port Metro Vancouver, Canada, to the Finnish Port of Pori, more than would have been possible via the traditional Panama Canal route. The Northwest Passage shortened the distance by compared to traditional route via the Panama Canal.\n\nIn August and September 2016 a cruise ship was sailed through the Northwest Passage. The ship \"Crystal Serenity\", (with 1,000 passengers, and 600 crew) left Seward, Alaska, used Amundsen's route and reached New York on September 17. Tickets for the 32-day trip started at $22,000 and were quickly sold out. The trip was repeated in 2017. In 2017 33 vessels made a complete transit, breaking the prior record of 20 in 2012.\n\nThe Canadian government claims that some of the waters of the Northwest Passage, particularly those in the Canadian Arctic Archipelago, are internal waters of Canada, giving Canada the right to bar transit through these waters. Most maritime nations, including the United States and those of the European Union, classify these waters as an international strait, where foreign vessels have the right of \"transit passage\". In such a regime, Canada would have the right to enact fishing and environmental regulation, and fiscal and smuggling laws, as well as laws intended for the safety of shipping, but not the right to close the passage. If the passage's deep waters become completely ice-free in summer months, they will be particularly enticing for supertankers that are too big to pass through the Panama Canal and must otherwise navigate around the tip of South America.\n\nThe dispute between Canada and the United States arose in 1969 with the trip of the U.S. oil tanker SS \"Manhattan\" through the Arctic Archipelago. The prospect of more American traffic headed to the Prudhoe Bay Oil Field made the Canadian government realize that political action was required should it decide to consider the archipelago as internal waters.\n\nIn 1985, the U.S. Coast Guard icebreaker passed through from Greenland to Alaska; the ship submitted to inspection by the Canadian Coast Guard before passing through, but the event infuriated the Canadian public and resulted in a diplomatic incident. The United States government, when asked by a Canadian reporter, indicated that they did not ask for permission as they were not legally required to. The Canadian government issued a declaration in 1986 reaffirming Canadian rights to the waters. The United States refused to recognize the Canadian claim. In 1988 the governments of Canada and the United States signed an agreement, \"Arctic Cooperation\", that resolved the practical issue without solving the sovereignty questions. Under the law of the sea, ships engaged in transit passage are not permitted to engage in research. The agreement states that all U.S. Coast Guard vessels are engaged in research, and so would require permission from the Government of Canada to pass through.\n\nIn late 2005, it was reported that U.S. nuclear submarines had travelled unannounced through Canadian Arctic waters, sparking outrage in Canada. In his first news conference after the 2006 federal election, Prime Minister-designate Stephen Harper contested an earlier statement made by the U.S. ambassador that Arctic waters were international, stating the Canadian government's intention to enforce its sovereignty there. The allegations arose after the U.S. Navy released photographs of surfaced at the North Pole.\n\nOn April 9, 2006, Canada's Joint Task Force (North) declared that the Canadian Forces will no longer refer to the region as the Northwest Passage, but as the Canadian Internal Waters. The declaration came after the successful completion of Operation Nunalivut (Inuktitut for \"the land is ours\"), which was an expedition into the region by five military patrols.\n\nIn 2006 a report prepared by the staff of the Parliamentary Information and Research Service of Canada suggested that because of the September 11 attacks, the United States might be less interested in pursuing the international waterways claim in the interests of having a more secure North American perimeter. This report was based on an earlier paper, \"The Northwest Passage Shipping Channel: Is Canada's Sovereignty Really Floating Away?\" by Andrea Charron, given to the 2004 Canadian Defence and Foreign Affairs Institute Symposium. Later in 2006 former United States Ambassador to Canada, Paul Cellucci agreed with this position; however, the succeeding ambassador, David Wilkins, stated that the Northwest Passage was in international waters.\n\nOn July 9, 2007, Prime Minister Harper announced the establishment of a deep-water port in the far North. In the government press release the Prime Minister is quoted as saying, \"Canada has a choice when it comes to defending our sovereignty over the Arctic. We either use it or lose it. And make no mistake, this Government intends to use it. Because Canada's Arctic is central to our national identity as a northern nation. It is part of our history. And it represents the tremendous potential of our future.\"\n\nOn July 10, 2007, Rear Admiral Timothy McGee of the United States Navy and Rear Admiral Brian Salerno of the United States Coast Guard announced that the United States would be increasing its ability to patrol the Arctic.\n\nIn the summer of 2000, two Canadian ships took advantage of thinning summer ice cover on the Arctic Ocean to make the crossing. It is thought that climate change is likely to open the passage for increasing periods, making it potentially attractive as a major shipping route. However, the passage through the Arctic Ocean would require significant investment in escort vessels and staging ports, and it would remain seasonal. Therefore, the Canadian commercial marine transport industry does not anticipate the route as a viable alternative to the Panama Canal within the next 10 to 20 years (as of 2004).\n\nOn September 14, 2007, the European Space Agency stated that ice loss that year had opened up the historically impassable passage, setting a new low of ice cover as seen in satellite measurements which went back to 1978. According to the Arctic Climate Impact Assessment, the latter part of the 20th century and the start of the 21st had seen marked shrinkage of ice cover. The extreme loss in 2007 rendered the passage \"fully navigable\". However, the ESA study was based only on analysis of satellite images and could in practice not confirm anything about the actual navigation of the waters of the passage. ESA suggested the passage would be navigable \"during reduced ice cover by multi-year ice pack\" (namely sea ice surviving one or more summers) where previously any traverse of the route had to be undertaken during favourable seasonable climatic conditions or by specialist vessels or expeditions. The agency's report speculated that the conditions prevalent in 2007 had shown the passage may \"open\" sooner than expected. An expedition in May 2008 reported that the passage was not yet continuously navigable even by an icebreaker and not yet ice-free.\n\nScientists at a meeting of the American Geophysical Union on December 13, 2007, revealed that NASA satellites observing the western Arctic showed a 16% decrease in cloud coverage during the summer of 2007 compared to 2006. This would have the effect of allowing more sunlight to penetrate Earth's atmosphere and warm the Arctic Ocean waters, thus melting sea ice and contributing to the opening the Northwest Passage.\n\nIn 2006 the cruise liner MS \"Bremen\" successfully ran the Northwest Passage, helped by satellite images telling where sea ice was.\n\nOn November 28, 2008, the Canadian Broadcasting Corporation reported that the Canadian Coast Guard confirmed the first commercial ship sailed through the Northwest Passage. In September 2008, , owned by Desgagnés Transarctik Inc. and, along with the Arctic Cooperative, is part of Nunavut Sealift and Supply Incorporated (NSSI), transported cargo from Montreal to the hamlets of Cambridge Bay, Kugluktuk, Gjoa Haven, and Taloyoak. A member of the crew is reported to have claimed that \"there was no ice whatsoever\". Shipping from the east was to resume in the fall of 2009. Although sealift is an annual feature of the Canadian Arctic this is the first time that the western communities have been serviced from the east. The western portion of the Canadian Arctic is normally supplied by Northern Transportation Company Limited (NTCL) from Hay River. The eastern portion by NNSI and NTCL from Churchill and Montreal.\n\nIn January 2010, the ongoing reduction in the Arctic sea ice led telecoms cable specialist Kodiak-Kenai Cable to propose the laying of a fiberoptic cable connecting London and Tokyo, by way of the Northwest Passage, saying the proposed system would nearly cut in half the time it takes to send messages from the United Kingdom to Japan.\n\nIn September 2013 the first large ice strengthened sea freighter, \"Nordic Orion\", used the passage.\n\nIn 2016 a new record was set when the cruise ship \"Crystal Serenity\" transited with 1,700 passengers.\n\nScientists believe that reduced sea ice in the Northwest Passage has permitted some new species to migrate across the Arctic Ocean. The gray whale \"Eschrichtius robustus\" has not been seen in the Atlantic since it was hunted to extinction there in the 18th century, but in May 2010, one such whale turned up in the Mediterranean. Scientists speculated the whale had followed its food sources through the Northwest Passage and simply kept on going.\n\nThe plankton species \"Neodenticula seminae\" had not been recorded in the Atlantic for 800,000 years. Over the past few years, however, it has become increasingly prevalent there. Again, scientists believe that it got there through the reopened Northwest Passage.\n\nIn August 2010, two bowhead whales from West Greenland and Alaska respectively, entered the Northwest Passage from opposite directions and spent approximately 10 days in the same area.\n\n\n\n"}
{"id": "21216", "url": "https://en.wikipedia.org/wiki?curid=21216", "title": "Nevada", "text": "Nevada\n\nNevada () is a state in the Western United States. It borders Oregon to the northwest, Idaho to the northeast, California to the west, Arizona to the southeast and Utah to the east. Nevada is the 7th most extensive, the 34th most populous, but the 9th least densely populated of the U.S. states. Nearly three-quarters of Nevada's people live in Clark County, which contains the Las Vegas–Paradise metropolitan area where three of the state's four largest incorporated cities are located. Nevada's capital, however, is Carson City.\n\nNevada is officially known as the \"Silver State\" because of the importance of silver to its history and economy. It is also known as the \"Battle Born State\", because it achieved statehood during the Civil War (the words \"Battle Born\" also appear on the state flag); as the \"Sagebrush State\", for the native plant of the same name; and as the \"Sage-hen State\".\n\nNevada is largely desert and semi-arid, much of it within the Great Basin. Areas south of the Great Basin are within the Mojave Desert, while Lake Tahoe and the Sierra Nevada lie on the western edge. About 86% of the state's land is managed by various jurisdictions of the U.S. federal government, both civilian and military.\n\nBefore European contact, Native Americans of the Paiute, Shoshone, and Washoe tribes inhabited the land that is now Nevada. The first Europeans to explore the region were Spanish. They called the region \"Nevada\" (snowy) because of the snow which covered the mountains in winter. The area formed part of the Viceroyalty of New Spain, and became part of Mexico when it gained independence in 1821. The United States annexed the area in 1848 after its victory in the Mexican–American War, and it was incorporated as part of Utah Territory in 1850. The discovery of silver at the Comstock Lode in 1859 led to a population boom that became an impetus to the creation of Nevada Territory out of western Utah Territory in 1861. Nevada became the 36th state on October 31, 1864, as the second of two states added to the Union during the Civil War (the first being West Virginia).\n\nNevada has a reputation for its libertarian laws. In 1940, with a population of just over 110,000 people, Nevada was by far the least-populated state, with less than half the population of the next least-populated state. However, legalized gambling and lenient marriage and divorce laws transformed Nevada into a major tourist destination in the 20th century. Nevada is the only U.S. state where prostitution is legal, though it is illegal in Clark County (Las Vegas), Washoe County (Reno) and Carson City (which, as an independent city, is not within the boundaries of any county). The tourism industry remains Nevada's largest employer, with mining continuing as a substantial sector of the economy: Nevada is the fourth-largest producer of gold in the world.\n\nThe name \"Nevada\" comes from the Spanish \"nevada\" , meaning \"snow-covered\", after the Sierra Nevada (\"snow-covered mountains\").\n\nMost Nevadans pronounce the second syllable of their state name using the vowel (). Many from outside the Western United States pronounce it with the vowel (). Although the latter pronunciation is closer to the Spanish pronunciation, it is not the pronunciation preferred by most Nevadans. State Assemblyman Harry Mortenson proposed a bill to recognize the alternate (quasi-Spanish) pronunciation of Nevada, though the bill was not supported by most legislators and never received a vote. The Nevadan pronunciation is the de facto official one, since it is the one used by the state legislature. At one time, the state's official tourism organization, TravelNevada, stylized the name of the state as \"Nevăda\", with a breve mark over the \"a\" indicating the locally preferred pronunciation which is also available as a license plate design.\n\nNevada is almost entirely within the Basin and Range Province, and is broken up by many north-south mountain ranges. Most of these ranges have endorheic valleys between them, which belies the image portrayed by the term Great Basin.\n\nMuch of the northern part of the state is within the Great Basin, a mild desert that experiences hot temperatures in the summer and cold temperatures in the winter. Occasionally, moisture from the Arizona Monsoon will cause summer thunderstorms; Pacific storms may blanket the area with snow. The state's highest recorded temperature was in Laughlin (elevation of ) on June 29, 1994. The coldest recorded temperature was set in San Jacinto in 1972, in the northeastern portion of the state.\n\nThe Humboldt River crosses the state from east to west across the northern part of the state, draining into the Humboldt Sink near Lovelock. Several rivers drain from the Sierra Nevada eastward, including the Walker, Truckee, and Carson rivers. All of these rivers are endorheic basins, ending in Walker Lake, Pyramid Lake, and the Carson Sink, respectively. However, not all of Nevada is within the Great Basin. Tributaries of the Snake River drain the far north, while the Colorado River, which also forms much of the boundary with Arizona, drains much of southern Nevada.\n\nThe mountain ranges, some of which have peaks above , harbor lush forests high above desert plains, creating sky islands for endemic species. The valleys are often no lower in elevation than , while some in central Nevada are above .\n\nThe southern third of the state, where the Las Vegas area is situated, is within the Mojave Desert. The area receives less rain in the winter but is closer to the Arizona Monsoon in the summer. The terrain is also lower, mostly below , creating conditions for hot summer days and cool to chilly winter nights.\n\nNevada and California have by far the longest diagonal line (in respect to the cardinal directions) as a state boundary at just over . This line begins in Lake Tahoe nearly offshore (in the direction of the boundary), and continues to the Colorado River where the Nevada, California, and Arizona boundaries merge southwest of the Laughlin Bridge.\n\nThe largest mountain range in the southern portion of the state is the Spring Mountain Range, just west of Las Vegas. The state's lowest point is along the Colorado River, south of Laughlin.\n\nNevada has 172 mountain summits with of prominence. Nevada ranks second in the United States by number of mountains, behind Alaska, and ahead of California, Montana, and Washington. Nevada is the most mountainous state in the contiguous United States.\n\nNevada is the driest state in the United States. It is made up of mostly desert and semi-arid climate regions, and, with the exception of the Las Vegas Valley, the average summer diurnal temperature range approaches in much of the state. While winters in northern Nevada are long and fairly cold, the winter season in the southern part of the state tends to be of short duration and mild. Most parts of Nevada receive scarce precipitation during the year. Most rain that falls in the state falls on the lee side (east and northeast slopes) of the Sierra Nevada.\n\nThe average annual rainfall per year is about ; the wettest parts get around . Nevada's highest recorded temperature is at Laughlin on June 29, 1994 and the lowest recorded temperature is at San Jacinto on January 8, 1937. Nevada's reading is the third highest statewide record high temperature of a U.S. state, just behind Arizona's reading and California's reading.\n\nThe vegetation of Nevada is diverse and differs by state area. Nevada contains six biotic zones: alpine, sub-alpine, ponderosa pine, pinion-juniper, sagebrush and creosotebush.\n\nNevada is divided into political jurisdictions designated as \"counties\". Carson City is officially a consolidated municipality; however, for many purposes under state law it is considered to be a county. As of 1919 there were 17 counties in the state, ranging from .\n\nLake County, one of the original nine counties formed in 1861, was renamed Roop County in 1862. Part of the county became Lassen County, California in 1864. In 1883, Washoe County annexed the portion that remained in Nevada.\n\nIn 1969, Ormsby County was dissolved and the Consolidated Municipality of Carson City was created by the Legislature in its place co-terminous with the old boundaries of Ormsby County.\n\nBullfrog County was formed in 1987 from part of Nye County. After the creation was declared unconstitutional, the county was abolished in 1989.\n\nHumboldt county was designated as a county in 1856 by Utah Territorial Legislature and again in 1861 by the new Nevada Legislature.\n\nClark County is the most populous county in Nevada, accounting for nearly three-quarters of its residents. Las Vegas, Nevada's most populous city, has been the county seat since the county was created in 1909 from a portion of Lincoln County, Nevada. Before that, it was a part of Arizona Territory. Clark County attracts numerous tourists: An estimated 44 million people visited Clark County in 2014.\n\nWashoe County is the second most populous county of Nevada. Its county seat is Reno. Washoe County includes the Reno–Sparks metropolitan area.\n\nLyon County is the third most populous county. It was one of the nine original counties created in 1861. It was named after Nathaniel Lyon, the first Union General to be killed in the Civil War. Its current county seat is Yerington. Its first county seat was established at Dayton on November 29, 1861.\n\nFrancisco Garcés was the first European in the area, Nevada was annexed as a part of the Spanish Empire in the northwestern territory of New Spain. Administratively, the area of Nevada was part of the Commandancy General of the Provincias Internas in the Viceroyalty of New Spain. Nevada became a part of Alta California (\"Upper California\") province in 1804 when the Californias were split. With the Mexican War of Independence won in 1821, the province of Alta California became a territory (state) of Mexico, with a small population. Jedediah Smith entered the Las Vegas Valley in 1827, and Peter Skene Ogden traveled the Humboldt River in 1828. When the Mormons created the State of Deseret in 1847, they laid claim to all of Nevada within the Great Basin and the Colorado watershed. They also founded the first white settlement in what is now Nevada, Mormon Station (modern day Genoa), in 1851. In June 1855, William Bringhurst and 29 fellow Mormon missionaries from Utah arrived at a site just northeast of downtown Las Vegas and built a 150-foot square adobe fort, the first permanent structure erected in the valley, which remained under the control of Salt Lake City until the winter of 1858–1859.\n\nAs a result of the Mexican–American War and the Treaty of Guadalupe Hidalgo, Mexico permanently lost Alta California in 1848. The new areas acquired by the United States continued to be administered as territories. As part of the Mexican Cession (1848) and the subsequent California Gold Rush that used Emigrant Trails through the area, the state's area evolved first as part of the Utah Territory, then the Nevada Territory (March 2, 1861; named for the Sierra Nevada).\n\nSee History of Utah, History of Las Vegas, and the discovery of the first major U.S. deposit of silver ore in Comstock Lode under Virginia City, Nevada in 1859.\n\nOn March 2, 1861, the Nevada Territory separated from the Utah Territory and adopted its current name, shortened from \"Sierra Nevada\" (Spanish for \"snow-covered mountain range\").\n\nThe 1861 southern boundary is commemorated by Nevada Historical Markers 57 and 58 in Lincoln and Nye counties.\n\nEight days before the presidential election of 1864, Nevada became the 36th state in the union. Rather than sending the Nevada State Constitution to Washington DC by Pony Express to save time the full text of the State Constitution was sent by Telegraph at a cost of $3,416.77—the most costly telegraph on file for a single dispatch. Finally the response from Washington DC on October 31, 1864 was \"the pain is over, the child is born, Nevada this day was admitted into the Union\". Statehood was rushed to the date of October 31 to help ensure Abraham Lincoln's reelection on November 8 and post-Civil War Republican dominance in Congress, as Nevada's mining-based economy tied it to the more industrialized Union. As it turned out, however, Lincoln and the Republicans won the election handily, and did not need Nevada's help.\n\nNevada is one of only two states to significantly expand its borders after admission to the Union. (The other is Missouri, which acquired additional territory in 1837 due to the Platte Purchase.)\n\nIn 1866 another part of the western Utah Territory was added to Nevada in the eastern part of the state, setting the current eastern boundary.\n\nNevada achieved its current southern boundaries on January 18, 1867, when it absorbed the portion of Pah-Ute County in the Arizona Territory west of the Colorado River, essentially all of present-day Nevada south of the 37th parallel. The transfer was prompted by the discovery of gold in the area, and officials thought Nevada would be better able to oversee the expected population boom. This area includes most of what is now Clark County and the Las Vegas metropolitan area.\n\nMining shaped Nevada's economy for many years (see \"Silver mining in Nevada\"). When Mark Twain lived in Nevada during the period described in \"Roughing It\", mining had led to an industry of speculation and immense wealth. However, both mining and population declined in the late 19th century. However, the rich silver strike at Tonopah in 1900, followed by strikes in Goldfield and Rhyolite, again put Nevada's population on an upward trend.\n\nUnregulated gambling was commonplace in the early Nevada mining towns but was outlawed in 1909 as part of a nationwide anti-gambling crusade. Because of subsequent declines in mining output and the decline of the agricultural sector during the Great Depression, Nevada again legalized gambling on March 19, 1931, with approval from the legislature. Governor Fred B. Balzar's signature enacted the most liberal divorce laws in the country and open gambling. The reforms came just eight days after the federal government presented the $49 million construction contract for Boulder Dam (now Hoover Dam).\n\nThe Nevada Test Site, northwest of the city of Las Vegas, was founded on January 11, 1951, for the testing of nuclear weapons. The site consists of about of desert and mountainous terrain. Nuclear testing at the Nevada Test Site began with a bomb dropped on Frenchman Flat on January 27, 1951. The last atmospheric test was conducted on July 17, 1962, and the underground testing of weapons continued until September 23, 1992. The location is known for having the highest concentration of nuclear-detonated weapons in the U.S.\n\nOver 80% of the state's area is owned by the federal government. The primary reason for this is homesteads were not permitted in large enough sizes to be viable in the arid conditions that prevail throughout desert Nevada. Instead, early settlers would homestead land surrounding a water source, and then graze livestock on the adjacent public land, which is useless for agriculture without access to water (this pattern of ranching still prevails).\n\nThe United States Census Bureau estimates the population of Nevada on July 1, 2016 was 2,940,058, an increase of 56,300 residents (1.95%) since the 2015 US Census estimate and an increase of 239,367 residents (8.86%) since the 2010 United States Census. Nevada had the second highest percentage growth in population from 2015 to 2016. At the 2010 Census, 6.9% of the state's population were reported as under 5, 24.6% were under 18, and 12.0% were 65 or older. Females made up about 49.5% of the population. \n\nSince the 2010 census, the population of Nevada had a natural increase of 87,581 (the net difference between 222,508 births and 134,927 deaths); and an increase due to net migration of 146,626 (of which 104,032 was due to domestic and 42,594 was due to international migration).\n\nThe center of population of Nevada is in southern Nye County. In this county, the unincorporated town of Pahrump, west of Las Vegas on the California state line, has grown very rapidly from 1980 to 2010. At the 2010 census, the town had 36,441 residents. Las Vegas grew from a gulch of 100 people in 1900 to 10,000 by 1950 to 100,000 by 1970, and was America's fastest-growing city and metropolitan area from 1960 to 2000.\n\nFrom about the 1940s until 2003, Nevada was the fastest-growing state in the US percentage-wise. Between 1990 and 2000, Nevada's population increased 66%, while the US's population increased 13%. Over two thirds of the population of the state lives in Clark County, which is coextensive with the Las Vegas metropolitan area. Thus, in terms of population, Nevada is one of the most centralized states in the nation.\n\nHenderson and North Las Vegas are among the top 20 fastest-growing U.S. cities with populations of over 100,000. The rural community of Mesquite northeast of Las Vegas was an example of micropolitan growth in the 1990s and 2000s. Other desert towns like Indian Springs and Searchlight on the outskirts of Las Vegas have seen some growth as well.\n\nLarge numbers of new residents in the state originate from California, which led some locals to feel their state is being \"Californicated\".\n\nThe table below shows the racial composition of Nevada's population as of 2016.\n\nAccording to the 2016 American Community Survey, 27.8% of Nevada's population were of Hispanic or Latino origin (of any race): Mexican (21.3%), Puerto Rican (0.9%), Cuban (0.9%), and other Hispanic or Latino origin (4.7%). The five largest non-Hispanic White ancestry groups were: German (11.3%), Irish (9.0%), English (6.9%), Italian (5.8%), and American (4.7%).\n\nIn 1980, non-Hispanic whites made up 83.3% of the state's population.\n\nAs of 2011, 63.6% of Nevada's population younger than age 1 were minorities.. Las Vegas is a minority majority city. According to the United States Census Bureau estimates, as of July 1, 2017, non-Hispanic Whites made up 49.1% of Nevada's population. This would make Nevada a majority minority state joining California, Texas, New Mexico, Hawaii, and the District of Columbia.\n\nIn Douglas, Mineral, and Pershing counties, a plurality of residents are of Mexican ancestry. In Nye County and Humboldt County, residents are mostly of German ancestry; Washoe County has many Irish Americans. Americans of English descent form pluralities in Lincoln County, Churchill County, Lyon County, White Pine County, and Eureka County.\n\nAsian Americans lived in the state since the California Gold Rush of the 1850s brought thousands of Chinese miners to Washoe county. They were followed by a few hundred Japanese farm workers in the late 19th century. By the late 20th century, many immigrants from China, Japan, Korea, the Philippines, Bangladesh, India, and Vietnam came to the Las Vegas metropolitan area. The city now has one of America's most prolific Asian American communities, with a mostly Chinese and Taiwanese area known as \"Chinatown\" west of I-15 on Spring Mountain Road. Filipino Americans form the largest Asian American group in the state, with a population of more than 113,000. They comprise 56.5% of the Asian American population in Nevada and constitute about 4.3% of the entire state's population.\n\nLargely African American sections of Las Vegas and Reno can be found. Many current African-American Nevadans are newly transplanted residents from California.\n\nLas Vegas was a major destination for immigrants from South Asia and Latin America seeking employment in the gaming and hospitality industries during the 1990s and first decade of the 21st century, but farming and construction are the biggest employers of immigrant labor.\n\nThe religious makeup of Nevadans includes large communities of Mormons, Roman Catholics, and Evangelicals; each is known for higher birth rates and a younger than national average age. American Jews represent a large proportion of the active adult retirement community.\n\n\"Note: Births within the table do not add up, due to Hispanics being counted both by their ethnicity and by their race, giving a higher overall number.\"\n\n\nA small percentage of Nevada's population lives in rural areas. The culture of these places differs significantly from the major metropolitan areas. People in these rural counties tend to be native Nevada residents, unlike in the Las Vegas and Reno areas, where the vast majority of the population was born in another state. The rural population is also less diverse in terms of race and ethnicity. Mining plays an important role in the economies of the rural counties, with tourism being less prominent. Ranching also has a long tradition in rural Nevada.\n\nChurch attendance in Nevada is among the lowest of all U.S. states. In a 2009 Gallup poll only 30% of Nevadans said they attended church weekly or almost weekly, compared to 42% of all Americans (only four states were found to have a lower attendance rate than Nevada).\n\nMajor religious affiliations of the people of Nevada are: Protestant 35%, no religion 28%, Roman Catholic 25%, Latter-day Saint 4%, Jewish 2%, Hindu less than 1%, Buddhist 0.5% and Islam less than 0.1%. Parts of Nevada (in the eastern parts of the state) are situated in the Mormon Corridor.\n\nThe largest denominations by number of adherents in 2010 were the Roman Catholic Church with 451,070; The Church of Jesus Christ of Latter-day Saints with 175,149; and the Southern Baptist Convention with 45,535; Buddhist congregations 14,727; Bahá'í 1,723; and Muslim 1,700. The Jewish community is represented by The Rohr Jewish Learning Institute and Chabad.\n\nThe economy of Nevada is tied to tourism (especially entertainment and gambling related), mining, and cattle ranching. Nevada's industrial outputs are tourism, mining, machinery, printing and publishing, food processing, and electric equipment. The Bureau of Economic Analysis estimates Nevada's total state product in 2010 was $126 billion. The state's per capita personal income in 2009 was $38,578, ranking nineteenth in the nation. Nevada's state debt in 2012 was calculated to be $7.5 billion, or $3,100 per taxpayer. As of December 2014, the state's unemployment rate was 6.8%.\n\nThe economy of Nevada has long been tied to vice industries. \"[Nevada was] founded on mining and refounded on sin—beginning with prizefighting and easy divorce a century ago and later extending to gaming and prostitution\", said the August 21, 2010 issue of \"The Economist\".\n\nIn portions of the state outside of the Las Vegas and Reno metropolitan areas mining plays a major economic role. By value, gold is by far the most important mineral mined. In 2004, of gold worth $2.84 billion were mined in Nevada, and the state accounted for 8.7% of world gold production (see \"Gold mining in Nevada\"). Silver is a distant second, with worth $69 million mined in 2004 (see \"Silver mining in Nevada\"). Other minerals mined in Nevada include construction aggregates, copper, gypsum, diatomite and lithium. Despite its rich deposits, the cost of mining in Nevada is generally high, and output is very sensitive to world commodity prices.\n\nCattle ranching is a major economic activity in rural Nevada. Nevada's agricultural outputs are cattle, hay, alfalfa, dairy products, onions, and potatoes. As of January 1, 2006, there were an estimated 500,000 head of cattle and 70,000 head of sheep in Nevada. Most of these animals forage on rangeland in the summer, with supplemental feed in the winter. Calves are generally shipped to out-of-state feedlots in the fall to be fattened for market. Over 90% of Nevada's of cropland is used to grow hay, mostly alfalfa, for livestock feed.\n\nThe largest employers in the state, as of the first fiscal quarter of 2011, are the following, according to the Nevada Department of Employment, Training and Rehabilitation:\n\nAmtrak's \"California Zephyr\" train uses the Union Pacific's original transcontinental railroad line in daily service from Chicago to Emeryville, California, serving Elko, Winnemucca, and Reno. Las Vegas has had no passenger train service since Amtrak's Desert Wind was discontinued in 1997. Amtrak Thruway Motorcoaches provide connecting service from Las Vegas to trains at Needles, California, Los Angeles, and Bakersfield, California; and from Stateline, Nevada, to Sacramento, California. There have been a number of proposals to re-introduce service to either Los Angeles or Southern California.\n\nThe Union Pacific Railroad has some railroads in the north and south of Nevada. Greyhound Lines provide some bus service to the state.\n\nInterstate 15 passes through the southern tip of the state, serving Las Vegas and other communities. I-215 and spur route I-515 also serve the Las Vegas metropolitan area. Interstate 80 crosses through the northern part of Nevada, roughly following the path of the Humboldt River from Utah in the east and the Truckee River westward through Reno into California. It has a spur route, I-580. Nevada also is served by several U.S. highways: US 6, US 50, US 93, US 95 and US 395. There are also 189 Nevada state routes. Many of Nevada's counties have a system of county routes as well, though many are not signed or paved in rural areas. Nevada is one of a few states in the U.S. that does not have a continuous interstate highway linking its two major population centers—the road connection between the Las Vegas and Reno areas is a combination of Interstate and U.S. highways.\n\nThe state is one of just a few in the country to allow semi-trailer trucks with three trailers—what might be called a \"road train\" in Australia. But American versions are usually smaller, in part because they must ascend and descend some fairly steep mountain passes.\n\nRTC Transit is the public transit system in the Las Vegas metropolitan area. The agency is the largest transit agency in the state and operates a network of bus service across the Las Vegas Valley, including the use of The Deuce, double-decker buses, on the Las Vegas Strip and several outlying routes. RTC RIDE operates a system of local transit bus service throughout the Reno-Sparks metropolitan area. Other transit systems in the state include Carson City's JAC. Most other counties in the state do not have public transportation at all.\n\nAdditionally, a monorail system provides public transportation in the Las Vegas area. The Las Vegas Monorail line services several casino properties and the Las Vegas Convention Center on the east side of the Las Vegas Strip, running near Paradise Road, with a possible future extension to McCarran International Airport. Several hotels also run their own monorail lines between each other, which are typically several blocks in length.\n\nMcCarran International Airport in Las Vegas is the busiest airport serving Nevada. The Reno-Tahoe International Airport (formerly known as the Reno Cannon International Airport) is the other major airport in the state.\n\nUnder the Constitution of the State of Nevada, the powers of the Nevada government are divided among three separate departments: the Executive consisting of the Governor of Nevada and their cabinet along with the other elected constitutional officers; the Legislative consisting of the Nevada Legislature, which includes the Assembly and the Senate; and the Judicial consisting of the Supreme Court of Nevada and lower courts.\n\nThe Governor of Nevada is the chief magistrate of Nevada, the head of the executive department of the state's government, and the commander-in-chief of the state's military forces. The current Governor of Nevada is Steve Sisolak, a Democract.\n\nThe Nevada Legislature is a bicameral body divided into an Assembly and Senate. Members of the Assembly serve for 2 years, and members of the Senate serve for 4 years. Both houses of the Nevada Legislature will be impacted by term limits starting in 2010, as Senators and Assemblymen/women will be limited to a maximum of 12 years service in each house (by appointment or election which is a lifetime limit)—a provision of the constitution which was recently upheld by the Supreme Court of Nevada in a unanimous decision. Each session of the Legislature meets for a constitutionally mandated 120 days in every odd-numbered year, or longer if the Governor calls a special session.\n\nOn December 18, 2018, Nevada the first in the United States with a female majority in its legislature. Females hold nine of the 21 seats in the Nevada Senate, and 23 of the 42 seats in the Nevada Assembly.\n\nThe Supreme Court of Nevada is the state supreme court and the head of the Nevada Judiciary. Original jurisdiction is divided between the district courts (with general jurisdiction), and justice courts and municipal courts (both of limited jurisdiction). Appeals from District Courts are made directly to the Nevada Supreme Court, which under a deflective model of jurisdiction, has the discretion to send cases to the Court of Appeals for final resolution.\n\nIncorporated towns in Nevada, known as cities, are given the authority to legislate anything not prohibited by law. A recent movement has begun to permit home rule in incorporated Nevada cities to give them more flexibility and fewer restrictions from the Legislature. Town Boards for unincorporated towns are limited local governments created by either the local county commission, or by referendum, and form a purely advisory role and in no way diminish the responsibilities of the county commission that creates them.\n\nState departments and agencies:\n\nIn 1900, Nevada's population was the smallest of all states and was shrinking, as the difficulties of living in a \"barren desert\" began to outweigh the lure of silver for many early settlers. Historian Lawrence Friedman has explained what happened next:\n\nNevada, in a burst of ingenuity, built an economy by exploiting its sovereignty. Its strategy was to legalize all sorts of things that were illegal in California ... after easy divorce came easy marriage and casino gaming. Even prostitution is legal in Nevada, in any county that decides to allow it. Quite a few of them do.\n\nWith the advent of air conditioning for summertime use and Southern Nevada's mild winters, the fortunes of the state began to turn around, as it did for Arizona, making these two states the fastest growing in the Union.\n\nNevada is the only state where prostitution is legal (under the form of licensed brothels).\n\nProstitution is specifically illegal by state law in the state's larger jurisdictions, which include Clark County (which contains Las Vegas), Washoe County (which contains Reno), and the independent city of Carson City. Otherwise, it is legal in those counties which specifically vote to permit it. When permitted, brothels are only in rural or isolated parts of counties.\n\nNevada's early reputation as a \"divorce haven\" arose from the fact that, before the no-fault divorce revolution in the 1970s, divorces were difficult to obtain in the United States. Already having legalized gambling and prostitution, Nevada continued the trend of boosting its profile by adopting one of the most liberal divorce statutes in the nation. This resulted in \"Williams v. North Carolina (1942)\", , in which the U.S. Supreme Court ruled North Carolina had to give \"full faith and credit\" to a Nevada divorce. The Court modified its decision in Williams v. North Carolina (1945), , by holding a state need not recognize a Nevada divorce unless one of the parties was domiciled there at the time the divorce was granted and the forum state was entitled to make its own determination.\n\nAs of 2009, Nevada's divorce rate was above the national average.\n\nNevada's tax laws are intended to draw new residents and businesses to the state. Nevada has no personal income tax or corporate income tax. Since Nevada does not collect income data it cannot share such information with the federal government, the IRS.\n\nThe state sales tax (similar to VAT or GST) in Nevada is variable depending upon the county. The statewide tax rate is 6.85%, with five counties (Elko, Esmeralda, Eureka, Humboldt, and Mineral) charging this amount. Counties may impose additional rates via voter approval or through approval of the state legislature; therefore, the applicable sales tax will vary by county from 6.85% to 8.1% (Clark County). In Clark County, which includes Las Vegas, imposes four separate county option taxes in addition to the statewide rate – 0.25% for flood control, 0.50% for mass transit, 0.25% for infrastructure, and 0.25% for more cops. In Washoe County, which includes Reno, the sales tax rate is 7.725 percent, due to county option rates for flood control, the ReTRAC train trench project, mass transit, and an additional county rate approved under the Local Government Tax Act of 1991. The minimum Nevada sales tax rate changed on July 1, 2009.\n\nThe lodging tax rate in unincorporated Clark County, which includes the Las Vegas Strip, is 12%. Within the boundaries of the cities of Las Vegas and Henderson, the lodging tax rate is 13%.\n\nCorporations such as Apple Inc. allegedly have set up investment companies and funds in Nevada to avoid paying taxes.\n\nIn 2009, the Nevada Legislature passed a bill creating a domestic partnership registry that enables gay couples to enjoy the same rights as married couples. As of 2019, gay marriage is legal in Nevada.\n\nNevada provides friendly environment for the formation of corporations, and many (especially California) businesses have incorporated in Nevada to take advantage of the benefits of the Nevada statute. Nevada corporations offer great flexibility to the Board of Directors and simplify or avoid many of the rules that are cumbersome to business managers in some other states. In addition, Nevada has no franchise tax, although it does require businesses to have a license for which the business has to pay the state.\n\nSimilarly, many U.S. states have usury laws limiting the amount of interest a lender can charge, but federal law allows corporations to 'import' these laws from their home state.\n\nNevada has very liberal alcohol laws. Bars are permitted to remain open 24 hours, with no \"last call\". Liquor stores, convenience stores and supermarkets may also sell alcohol 24 hours per day, and may sell beer, wine and spirits.\n\nIn 2016, Nevada voters approved Question 2, which legalized the possession, transportation and cultivation of personal use amounts of marijuana for adults age 21 years and older, and authorized the creation of a regulated market for the sale of marijuana to adults age 21 years and older through state-licensed retail outlets. Nevada voters had previously approved medical marijuana in 2000, but rejected marijuana legalization in a similar referendum in 2006. Marijuana in all forms remains illegal under federal law.\n\nAside from cannabis legalization, non-alcohol drug laws are a notable exception to Nevada's otherwise libertarian principles. It is notable for having the harshest penalties for drug offenders in the country. Nevada remains the only state to still use mandatory minimum sentencing guidelines for possession of drugs.\n\nNevada voters enacted a smoking ban (\"The Nevada Clean Indoor Air Act\") in November 2006 that became effective on December 8, 2006. It outlaws smoking in most workplaces and public places. Smoking is permitted in bars, but only if the bar serves no food, or the bar is inside a larger casino. Smoking is also permitted in casinos, certain hotel rooms, tobacco shops, and brothels. However, some businesses do not obey this law and the government tends not to enforce it. In 2011, smoking restrictions in Nevada were loosened for certain places which allow only people age 21 or older inside.\n\nIn 2006, the crime rate in Nevada was about 24% higher than the national average rate, though crime has since decreased. Property crimes accounted for about 85% of the total crime rate in Nevada, which was 21% higher than the national rate. The remaining 20.3% were violent crimes. A complete listing of crime data in the state for 2013 can be found here:\n\nDue to heavy growth in the southern portion of the state, there is a noticeable divide between politics of northern and southern Nevada. The north has long maintained control of key positions in state government, even while the population of southern Nevada is larger than the rest of the state combined. The north sees the high population south becoming more influential and perhaps commanding majority rule. The south sees the north as the \"old guard\" trying to rule as an oligarchy. This has fostered some resentment, however, due to a term limit amendment passed by Nevada voters in 1994, and again in 1996, some of the north's hold over key positions will soon be forfeited to the south, leaving northern Nevada with less power.\n\nHistorically, northern Nevada has been very Republican. The more rural counties of the north are among the most conservative regions of the country. Carson City, the state's capital, is a Republican-leaning swing city/county. Washoe County, home to Reno, has historically been strongly Republican, but now has become more of a Democratic-leaning swing county. Clark County, home to Las Vegas, has been a stronghold for the Democratic Party since it was founded in 1909, having voted Republican only six times and once for a third party candidate.\nClark and Washoe counties have long dominated the state's politics. Between them, they cast 87 percent of Nevada's vote, and elect a substantial majority of the state legislature. The last Republican to carry Clark County was George H.W. Bush in 1988, and the last Republican to carry Washoe County was George W. Bush in 2004. The great majority of the state's elected officials are either from Las Vegas or Reno.\n\nNevada voted for the winner in every presidential election from 1912 to 2012, except in 1976 when it voted for Gerald Ford over Jimmy Carter. This includes Nevada supporting Democrats John F. Kennedy and Lyndon B. Johnson in 1960 and 1964, respectively. Republican Richard Nixon in 1968 and in 1972, Republican Ronald Reagan in 1980 and in 1984, Republican George H.W. Bush in 1988, Democrat Bill Clinton in 1992 and 1996, Republican George W. Bush in 2000 and 2004, and Democrat Barack Obama winning the state in both 2008 and 2012. This gives the state status as a political bellwether. From 1912 to 2012, Nevada has been carried by the presidential victor the most out of any state (26 of 27 elections). In 2016, Nevada lost its bellwether status when it narrowly cast its votes for Hillary Clinton. Nevada was one of only three states won by John F. Kennedy in the American West in the election of 1960, albeit narrowly.\n\nThe state's U.S. Senators are Democrats Catherine Cortez Masto and Jacky Rosen. The Governorship is held by Steve Sisolak, a Democrat.\n\nNevada is the only U.S. state to have a none of the above option available on its ballots. Officially called None of These Candidates, the option was first added to the ballot in 1975 and is used in all statewide elections, including president, US Senate and all state constitutional positions. In the event \"None of These Candidates\" receives a plurality of votes in the election, the candidate with the next-highest total is elected.\nEducation in Nevada is achieved through public and private elementary, middle, and high schools, as well as colleges and universities.\n\nA May 2015 educational reform law expanded school choice options to 450,000 Nevada students who are at up to 185% of the federal poverty level. Education savings accounts (ESAs) are enabled by the new law to help pay the tuition for private schools. Alternatively, families \"can use funds in these accounts to also pay for textbooks and tutoring.\"\n\nPublic school districts in Nevada include:\n\n\nThe Nevada Aerospace Hall of Fame provides educational resources and promotes the aerospace and aviation history of the state.\n\n\n\nThere are 68 designated wilderness areas in Nevada, protecting some under the jurisdiction of the National Park Service, U.S. Forest Service, and Bureau of Land Management.\n\nThe Nevada state parks comprise protected areas managed by the state of Nevada, including state parks, state historic sites, and state recreation areas. There are 24 state park units, including Van Sickle Bi-State Park which opened in July 2011 and is operated in partnership with the state of California.\n\nResort areas like Las Vegas, Reno, Lake Tahoe, and Laughlin attract visitors from around the nation and world. In FY08 the total of 266 casinos with gaming revenue over $1m for the year, brought in revenue of $12 billion in gaming revenue, and $13 billion in non-gaming revenue. A review of gaming statistics can be found at Nevada gaming area.\n\nNevada has by far the most hotel rooms per capital in the United States. According to the American Hotel and Lodging Association, there were 187,301 rooms in 584 hotels (of 15 or more rooms). The state is ranked just below California, Texas, Florida, and New York in total number of rooms, but those states have much larger populations. Nevada has one hotel room for every 14 residents, far above the national average of one hotel room per 67 residents.\n\nProstitution is legal in parts of Nevada in licensed brothels, but only counties with populations under 400,000 have the option to legalize it. Although prostitution is not a major part of the Nevada economy, employing roughly 300 women as independent contractors, it is a very visible endeavor. Of the 14 counties permitted to legalize prostitution under state law, 8 have chosen to legalize brothels. State law prohibits prostitution in Clark County (which contains Las Vegas), and Washoe County (which contains Reno). However, prostitution is legal in Storey County, which is part of the Reno–Sparks metropolitan area.\n\nNevada is not well known for its professional sports teams, mainly because major league sports in the past feared having direct involvement with the sports gambling industry. However, this situation lessened after they embraced daily fantasy sports (DFS) in 2014. The Las Vegas Valley is home to the Vegas Golden Knights of the National Hockey League who began play in the 2017-18 NHL season at T-Mobile Arena on the Las Vegas Strip in Paradise, Nevada. The Golden Knights are the only major North American professional sports franchise in Nevada.\n\nThey will be joined by the Oakland Raiders who at the start of the 2016 NFL season expressed interest in moving their team to Las Vegas, and announced in January 2017 they would do so in either 2019 or 2020.\n\nNevada takes pride in college sports, most notably its college football. College teams in the state include the Nevada Wolf Pack (representing the University of Nevada, Reno) and the UNLV Rebels (representing the University of Nevada, Las Vegas), both in the Mountain West Conference (MW).\n\nUNLV is most remembered for its men's basketball program, which experienced its height of supremacy in the late 1980s and early 1990s. Coached by Jerry Tarkanian, the Runnin' Rebels became one of the most elite programs in the country. In 1990, UNLV won the Men's Division I Championship by defeating Duke 103–73, which set tournament records for most points scored by a team and largest margin of victory in the national title game.\n\nIn 1991, UNLV finished the regular season undefeated, a feat that would not be matched in Division I men's basketball for more than 20 years. Forward Larry Johnson won several awards, including the Naismith Award. UNLV reached the Final Four yet again, but lost their national semifinal against Duke 79–77. The Runnin' Rebels were the Associated Press pre-season No. 1 back to back (1989–90, 1990–91). North Carolina is the only other team to accomplish that (2007–08, 2008–09).\n\nThe state's involvement in major-college sports is not limited to its local schools. In the 21st century, the Las Vegas area has become a significant regional center for college basketball conference tournaments. The MW, West Coast Conference, and Western Athletic Conference all hold their men's and women's tournaments in the area, and the Pac-12 holds its men's tournament there as well. The Big Sky Conference, after decades of holding its men's and women's conference tournaments at campus sites, began holding both tournaments in Reno in 2016.\n\nLas Vegas has hosted several professional boxing matches, most recently at the MGM Grand Garden Arena with bouts such as Mike Tyson vs. Evander Holyfield, Evander Holyfield vs. Mike Tyson II, Oscar De La Hoya vs. Floyd Mayweather and Oscar De La Hoya vs. Manny Pacquiao and at the newer T-Mobile Arena with Canelo Álvarez vs. Amir Khan.\n\nAlong with significant rises in popularity in mixed martial arts (MMA), a number of fight leagues such as the UFC have taken interest in Las Vegas as a primary event location due to the number of suitable host venues. The Mandalay Bay Events Center and MGM Grand Garden Arena are among some of the more popular venues for fighting events such as MMA and have hosted several UFC and other MMA title fights. The city has held the most UFC events with 86 events.\n\nThe state is also home to the Las Vegas Motor Speedway, which hosts the Kobalt Tools 400. Two venues in the immediate Las Vegas area host major annual events in rodeo. The Thomas & Mack Center, built for UNLV men's basketball, hosts the National Finals Rodeo. The PBR World Finals, operated by the bull riding-only Professional Bull Riders, was also held at the Thomas & Mack Center before moving to T-Mobile Arena in 2016. Finally, Sam Boyd Stadium, home to the UNLV football team, also hosts the country's biggest rugby event, the USA Sevens tournament in the World Rugby Sevens Series, as well as the AMA Supercross Championship.\n\nThe state is also home to one of the most famous tennis players of all time, Andre Agassi, and current baseball superstar Bryce Harper.\n\nSeveral United States Navy ships have been named USS \"Nevada\" in honor of the state. They include:\n\nArea 51 is near Groom Lake, a dry salt lake bed. The much smaller Creech Air Force Base is in Indian Springs, Nevada; Hawthorne Army Depot in Hawthorne; the Tonopah Test Range near Tonopah; and Nellis AFB in the northeast part of the Las Vegas Valley. Naval Air Station Fallon in Fallon; NSAWC, (pronounced \"EN-SOCK\") in western Nevada. NSAWC consolidated three Command Centers into a single Command Structure under a flag officer on July 11, 1996. The Naval Strike Warfare Center (STRIKE \"U\") based at NAS Fallon since 1984, was joined with the Navy Fighter Weapons School (TOPGUN) and the Carrier Airborne Early Warning Weapons School (TOPDOME) which both moved from NAS Miramar as a result of a Base Realignment and Closure (BRAC) decision in 1993 which transferred that installation back to the Marine Corps as MCAS Miramar. The Seahawk Weapon School was added in 1998 to provide tactical training for Navy helicopters.\n\nThese bases host a number of activities including the Joint Unmanned Aerial Systems Center of Excellence, the Naval Strike and Air Warfare Center, Nevada Test and Training Range, Red Flag, the U.S. Air Force Thunderbirds, the United States Air Force Warfare Center, the United States Air Force Weapons School, and the United States Navy Fighter Weapons School.\n\nNevada enjoys many economic advantages, and the southern portion of the state enjoys mild winter weather, but rapid growth has led to some overcrowded roads and schools. Nevada has the nation's 5th largest school district in the Clark County School District (projected fall 2007 enrollment is 314,000 students grades K-12).\n\nCoyote Springs is a proposed community for 240,000 inhabitants in Clark and Lincoln counties. It would be Nevada's largest planned city. The town is being developed by Harvey Whittemore and has generated some controversy because of environmental concerns and allegations of political favoritism.\n\n\n"}
{"id": "21217", "url": "https://en.wikipedia.org/wiki?curid=21217", "title": "Native Americans in the United States", "text": "Native Americans in the United States\n\nNative Americans, also known as American Indians, Indigenous Americans and other terms, are the indigenous peoples of the United States, except Hawaii. There are over 500 federally recognized tribes within the US, about half of which are associated with Indian reservations. The term \"American Indian\" excludes Native Hawaiians and some Alaska Natives, while Native Americans (as defined by the US Census) are American Indians, plus Alaska Natives of all ethnicities. Native Hawaiians are not counted as Native Americans by the US Census, instead being included in the Census grouping of \"Native Hawaiian and other Pacific Islander\".\n\nThe ancestors of modern Native Americans arrived in what is now the United States at least 15,000 years ago, possibly much earlier, from Asia via Beringia. A vast variety of peoples, societies and cultures subsequently developed. Native Americans were greatly affected by the European colonization of the Americas, which began in 1492, and their population declined precipitously mainly due to introduced diseases as well as warfare, territorial confiscation and slavery. After the founding of the United States, many Native American peoples were subjected to warfare, removals and one-sided treaties, and they continued to suffer from discriminatory government policies into the 20th century. Since the 1960s, Native American self-determination movements have resulted in changes to the lives of Native Americans, though there are still many contemporary issues faced by Native Americans. Today, there are over five million Native Americans in the United States, 78% of whom live outside reservations. \n\nWhen the United States was created, established Native American tribes were generally considered semi-independent nations, as they generally lived in communities separate from British settlers. The federal government signed treaties at a government-to-government level until the Indian Appropriations Act of 1871 ended recognition of independent native nations, and started treating them as \"domestic dependent nations\" subject to federal law. This law did preserve the rights and privileges agreed to under the treaties, including a large degree of tribal sovereignty. For this reason, many (but not all) Native American reservations are still independent of state law and actions of tribal citizens on these reservations are subject only to tribal courts and federal law.\n\nThe Indian Citizenship Act of 1924 granted U.S. citizenship to all Native Americans born in the United States who had not yet obtained it. This emptied the \"Indians not taxed\" category established by the United States Constitution, allowed natives to vote in state and federal elections, and extended the Fourteenth Amendment protections granted to people \"subject to the jurisdiction\" of the United States. However, some states continued to deny Native Americans voting rights for several decades. Bill of Rights protections do not apply to tribal governments, except for those mandated by the Indian Civil Rights Act of 1968.\n\nSince the end of the 15th century, the migration of Europeans to the Americas has led to centuries of population, cultural, and agricultural transfer and adjustment between Old and New World societies, a process known as the Columbian exchange. As most Native American groups had historically preserved their histories by oral traditions and artwork, the first written sources of the conflict were written by Europeans.\nEthnographers commonly classify the indigenous peoples of North America into ten geographical regions with shared cultural traits, called cultural areas. Some scholars combine the Plateau and Great Basin regions into the Intermontane West, some separate Prairie peoples from Great Plains peoples, while some separate Great Lakes tribes from the Northeastern Woodlands. The ten cultural areas are as follows:\n\n\nAt the time of the first contact, the indigenous cultures were quite different from those of the proto-industrial and mostly Christian immigrants. Some Northeastern and Southwestern cultures, in particular, were matrilineal and operated on a more collective basis than the Europeans were familiar with. The majority of Indigenous American tribes maintained their hunting grounds and agricultural lands for use of the entire tribe. Europeans at that time had patriarchal cultures and had developed concepts of individual property rights with respect to land that were extremely different. The differences in cultures between the established Native Americans and immigrant Europeans, as well as shifting alliances among different nations in times of war, caused extensive political tension, ethnic violence, and social disruption. Even before the European settlement of what is now the United States, Native Americans suffered high fatalities from contact with new European diseases, to which they had not yet acquired immunity; the diseases were endemic to the Spanish and other Europeans, and spread by direct contact and likely through pigs that escaped from expeditions. Smallpox epidemics are thought to have caused the greatest loss of life for indigenous populations. William M Denevan, noted author and Professor Emeritus of Geography at the University of Wisconsin-Madison, said on this subject in his essay \"The Pristine Myth: The Landscape of the Americas in 1492\"; \"The decline of native American populations was rapid and severe, probably the greatest demographic disaster ever. Old World diseases were the primary killer. In many regions, particularly the tropical lowlands, populations fell by 90 percent or more in the first century after the contact. \"\n\nEstimates of the pre-Columbian population of what today constitutes the U.S. vary significantly, ranging from William M Denevan's 3.8 million in his 1992 work \"The Native Population of the Americas in 1492\", to 18 million in Henry F Dobyns's \"Their Number Become Thinned\" (1983). Henry F Dobyns' work, being the highest single point estimate by far within the realm of professional academic research on the topic, has been criticized for being \"politically motivated\". Perhaps Dobyns' most vehement critic is David Henige, a bibliographer of Africana at the University of Wisconsin, whose \"Numbers From Nowhere\" (1998) is described as \"a landmark in the literature of demographic fulmination\". \"Suspect in 1966, it is no less suspect nowadays,\" Henige wrote of Dobyns's work. \"If anything, it is worse.\"\n\nAfter the thirteen colonies revolted against Great Britain and established the United States, President George Washington and Henry Knox conceived of the idea of \"civilizing\" Native Americans in preparation for assimilation as U.S. citizens. Assimilation (whether voluntary, as with the Choctaw, or forced) became a consistent policy through American administrations. During the 19th century, the ideology of manifest destiny became integral to the American nationalist movement. Expansion of European-American populations to the west after the American Revolution resulted in increasing pressure on Native American lands, warfare between the groups, and rising tensions. In 1830, the U.S. Congress passed the Indian Removal Act, authorizing the government to relocate Native Americans from their homelands within established states to lands west of the Mississippi River, accommodating European-American expansion. This resulted in the ethnic cleansing of many tribes, with the brutal, forced marches coming to be known as The Trail of Tears.\n\nAs American expansion reached into the West, settler and miner migrants came into increasing conflict with the Great Basin, Great Plains, and other Western tribes. These were complex nomadic cultures based on (introduced) horse culture and seasonal bison hunting. They carried out resistance against United States incursion in the decades after the end of the Civil War and the completion of the Transcontinental Railroad in a series of Indian Wars, which were frequent up until the 1890s and continued into the 20th century. Over time, the United States forced a series of treaties and land cessions by the tribes and established reservations for them in many western states. U.S. Indian agents encouraged Native Americans to adopt European-style farming and similar pursuits, but European-American agricultural technology of the time was inadequate for the often dry reservation lands, leading to mass starvation. In 1924, Native Americans who were not already U.S. citizens were granted citizenship by Congress.\n\nContemporary Native Americans have a unique relationship with the United States because they may be members of nations, tribes, or bands with sovereignty and treaty rights. Cultural activism since the late 1960s has increased political participation and led to an expansion of efforts to teach and preserve indigenous languages for younger generations and to establish a greater cultural infrastructure: Native Americans have founded independent newspapers and online media, recently including First Nations Experience, the first Native American television channel; established Native American studies programs, tribal schools, and universities, and museums and language programs; and have increasingly been published as authors in numerous genres.\n\nThe terms used to refer to Native Americans have at times been controversial. The ways Native Americans refer to themselves vary by region and generation, with many older Native Americans self-identifying as \"Indians\" or \"American Indians\", while younger Native Americans often identify as \"Indigenous\" or \"Aboriginal\". The term \"Native American\" has not traditionally included Native Hawaiians or certain Alaskan Natives, such as Aleut, Yup'ik, or Inuit peoples. By comparison, the indigenous peoples of Canada are generally known as First Nations.\n\nIt is not definitively known how or when the Native Americans first settled the Americas and the present-day United States. The prevailing theory proposes that people migrated from Eurasia across Beringia, a land bridge that connected Siberia to present-day Alaska during the Ice Age, and then spread southward throughout the Americas over the subsequent generations. Genetic evidence suggests at least three waves of migrants arrived from Asia, with the first occurring at least 15 thousand years ago. These migrations may have begun as early as 30,000 years ago and continued through to about 10,000 years ago, when the land bridge became submerged by the rising sea level caused by the ending of the last glacial period. These early inhabitants, called Paleoamericans, soon diversified into many hundreds of culturally distinct nations and tribes.\n\nThe pre-Columbian era incorporates all period subdivisions in the history and prehistory of the Americas before the appearance of significant European influences on the American continents, spanning the time of the original settlement in the Upper Paleolithic period to European colonization during the Early Modern period. While technically referring to the era before Christopher Columbus' voyages of 1492 to 1504, in practice the term usually includes the history of American indigenous cultures until they were conquered or significantly influenced by Europeans, even if this happened decades or even centuries after Columbus' initial landing.\n\nNative American cultures are not normally included in characterizations of advanced stone age cultures as \"Neolithic,\" which is a category that more often includes only the cultures in Eurasia, Africa, and other regions. The archaeological periods used are the classifications of archaeological periods and cultures established in Gordon Willey and Philip Phillips' 1958 book \"Method and Theory in American Archaeology\". They divided the archaeological record in the Americas into five phases; see Archaeology of the Americas.\n\nNumerous Paleoindian cultures occupied North America, with some arrayed around the Great Plains and Great Lakes of the modern United States and Canada, as well as adjacent areas to the West and Southwest. According to the oral histories of many of the indigenous peoples of the Americas, they have been living on this continent since their genesis, described by a wide range of traditional creation stories. Other tribes have stories that recount migrations across long tracts of land and a great river, believed to be the Mississippi River. Genetic and linguistic data connect the indigenous people of this continent with ancient northeast Asians. Archeological and linguistic data has enabled scholars to discover some of the migrations within the Americas.\n\nThe Clovis culture, a megafauna hunting culture, is primarily identified by the use of fluted spear points. Artifacts from this culture were first excavated in 1932 near Clovis, New Mexico. The Clovis culture ranged over much of North America and also appeared in South America. The culture is identified by the distinctive Clovis point, a flaked flint spear-point with a notched flute, by which it was inserted into a shaft. Dating of Clovis materials has been by association with animal bones and by the use of carbon dating methods. Recent reexaminations of Clovis materials using improved carbon-dating methods produced results of 11,050 and 10,800 radiocarbon years B.P. (roughly 9100 to 8850 BCE).\nThe Folsom Tradition was characterized by the use of Folsom points as projectile tips and activities known from kill sites, where slaughter and butchering of bison took place. Folsom tools were left behind between 9000 BCE and 8000 BCE.\n\nNa-Dené-speaking peoples entered North America starting around 8000 BCE, reaching the Pacific Northwest by 5000 BCE, and from there migrating along the Pacific Coast and into the interior. Linguists, anthropologists, and archaeologists believe their ancestors comprised a separate migration into North America, later than the first Paleo-Indians. They migrated into Alaska and northern Canada, south along the Pacific Coast, into the interior of Canada, and south to the Great Plains and the American Southwest. Na-Dené-speaking peoples were the earliest ancestors of the Athabascan-speaking peoples, including the present-day and historical Navajo and Apache. They constructed large multi-family dwellings in their villages, which were used seasonally. People did not live there year-round, but for the summer to hunt and fish, and to gather food supplies for the winter.\n\nSince the 1990s, archeologists have explored and dated eleven Middle Archaic sites in present-day Louisiana and Florida at which early cultures built complexes with multiple earthwork mounds; they were societies of hunter-gatherers rather than the settled agriculturalists believed necessary according to the theory of Neolithic Revolution to sustain such large villages over long periods. The prime example is Watson Brake in northern Louisiana, whose 11-mound complex is dated to 3500 BCE, making it the oldest, dated site in North America for such complex construction. It is nearly 2,000 years older than the Poverty Point site. Construction of the mounds went on for 500 years until the site was abandoned about 2800 BCE, probably due to changing environmental conditions.\n\nThe Oshara Tradition people lived from 700-1000 B.C CE. They were part of the Southwestern Archaic Tradition centered in north-central New Mexico, the San Juan Basin, the Rio Grande Valley, southern Colorado, and southeastern Utah.\n\nPoverty Point culture is a Late Archaic archaeological culture that inhabited the area of the lower Mississippi Valley and surrounding Gulf Coast. The culture thrived from 2200 BCE to 700 BCE, during the Late Archaic period. Evidence of this culture has been found at more than 100 sites, from the major complex at Poverty Point, Louisiana (a UNESCO World Heritage Site) across a range to the Jaketown Site near Belzoni, Mississippi.\n\nThe Formative, Classic and post-Classic stages are sometimes incorporated together as the Post-archaic period, which runs from 1000 BCE onward. Sites & cultures include: Adena, Old Copper, Oasisamerica, Woodland, Fort Ancient, Hopewell tradition and Mississippian cultures.\n\nThe Woodland period of North American pre-Columbian cultures refers to the time period from roughly 1000 BCE to 1000 CE in the eastern part of North America. The Eastern Woodlands cultural region covers what is now eastern Canada south of the Subarctic region, the Eastern United States, along to the Gulf of Mexico. The Hopewell tradition describes the common aspects of the culture that flourished along rivers in the northeastern and midwestern United States from 100 BCE to 500 CE, in the Middle Woodland period. The Hopewell tradition was not a single culture or society, but a widely dispersed set of related populations. They were connected by a common network of trade routes, This period is considered a developmental stage without any massive changes in a short period, but instead having a continuous development in stone and bone tools, leather working, textile manufacture, tool production, cultivation, and shelter construction.\n\nThe indigenous peoples of the Pacific Northwest Coast were of many nations and tribal affiliations, each with distinctive cultural and political identities, but they shared certain beliefs, traditions, and practices, such as the centrality of salmon as a resource and spiritual symbol. Their gift-giving feast, potlatch, is a highly complex event where people gather in order to commemorate special events. These events include the raising of a Totem pole or the appointment or election of a new chief. The most famous artistic feature of the culture is the Totem pole, with carvings of animals and other characters to commemorate cultural beliefs, legends, and notable events.\nThe Mississippian culture was a mound-building Native American civilization archeologists date from approximately 800 CE to 1600 CE, varying regionally. It was composed of a series of urban settlements and satellite villages (suburbs) linked together by a loose trading network, the largest city being Cahokia, believed to be a major religious center. The civilization flourished from the southern shores of the Great Lakes at Western New York and Western Pennsylvania in what is now the Eastern Midwest, extending south-southwest into the lower Mississippi Valley and wrapping easterly around the southern foot of the Appalachians barrier range into what is now the Southeastern United States.\n\nNumerous pre-Columbian societies were sedentary, such as the Pueblo peoples, Mandan, Hidatsa and others, and some established large settlements, even cities, such as Cahokia, in what is now Illinois. The Iroquois League of Nations or \"People of the Long House\" was a politically advanced, democratic society, which is thought by some historians to have influenced the United States Constitution, with the Senate passing a resolution to this effect in 1988. Other historians have contested this interpretation and believe the impact was minimal, or did not exist, pointing to numerous differences between the two systems and the ample precedents for the constitution in European political thought.\n\nAfter 1492, European exploration and colonization of the Americas revolutionized how the Old and New Worlds perceived themselves. Many of the first major contacts were in Florida and the Gulf coast by Spanish explorers.\n\nFrom the 16th through the 19th centuries, the population of Indians sharply declined. Most mainstream scholars believe that, among the various contributing factors, epidemic disease was the overwhelming cause of the population decline of the Native Americans because of their lack of immunity to new diseases brought from Europe. It is difficult to estimate the number of pre-Columbian Native Americans who were living in what is today the United States of America. Estimates range from a low of 2.1 million to a high of 18 million (Dobyns 1983). By 1800, the Native population of the present-day United States had declined to approximately 600,000, and only 250,000 Native Americans remained in the 1890s. Chicken pox and measles, endemic but rarely fatal among Europeans (long after being introduced from Asia), often proved deadly to Native Americans. In the 100 years following the arrival of the Spanish to the Americas, large disease epidemics depopulated large parts of the eastern United States in the 16th century.\n\nThere are a number of documented cases where diseases were deliberately spread among Native Americans as a form of biological warfare. The most well-known example occurred in 1763, when Sir Jeffery Amherst, Commander-in-Chief of the Forces of the British Army, wrote praising the use of smallpox-infected blankets to \"extirpate\" the Indian race. Blankets infected with smallpox were given to Native Americans besieging Fort Pitt. The effectiveness of the attempt is unclear.\n\nIn 1634, Fr. Andrew White of the Society of Jesus established a mission in what is now the state of Maryland, and the purpose of the mission, stated through an interpreter to the chief of an Indian tribe there, was \"to extend civilization and instruction to his ignorant race, and show them the way to heaven\". Fr. Andrew's diaries report that by 1640, a community had been founded which they named St. Mary's, and the Indians were sending their children there \"to be educated among the English\". This included the daughter of the Piscataway Indian chief Tayac, which exemplifies not only a school for Indians, but either a school for girls, or an early co-ed school. The same records report that in 1677, \"a school for humanities was opened by our Society in the centre of [Maryland], directed by two of the Fathers; and the native youth, applying themselves assiduously to study, made good progress. Maryland and the recently established school sent two boys to St. Omer who yielded in abilities to few Europeans, when competing for the honor of being first in their class. So that not gold, nor silver, nor the other products of the earth alone, but men also are gathered from thence to bring those regions, which foreigners have unjustly called ferocious, to a higher state of virtue and cultivation.\"\n\nThrough the mid-17th century the Beaver Wars were fought over the fur trade between the Iroquois and the Hurons, the northern Algonquians, and their French allies. During the war the Iroquois destroyed several large tribal confederacies, including the Huron, Neutral, Erie, Susquehannock, and Shawnee, and became dominant in the region and enlarged their territory.\n\nIn 1727, the Sisters of the Order of Saint Ursula founded Ursuline Academy in New Orleans, which is currently the oldest continuously operating school for girls and the oldest Catholic school in the United States. From the time of its foundation, it offered the first classes for Native American girls, and would later offer classes for female African-American slaves and free women of color.\nBetween 1754 and 1763, many Native American tribes were involved in the French and Indian War/Seven Years' War. Those involved in the fur trade tended to ally with French forces against British colonial militias. The British had made fewer allies, but it was joined by some tribes that wanted to prove assimilation and loyalty in support of treaties to preserve their territories. They were often disappointed when such treaties were later overturned. The tribes had their own purposes, using their alliances with the European powers to battle traditional Native enemies. Some Iroquois who were loyal to the British, and helped them fight in the American Revolution, fled north into Canada.\n\nAfter European explorers reached the West Coast in the 1770s, smallpox rapidly killed at least 30% of Northwest Coast Native Americans. For the next eighty to one hundred years, smallpox and other diseases devastated native populations in the region. Puget Sound area populations, once estimated as high as 37,000 people, were reduced to only 9,000 survivors by the time settlers arrived en masse in the mid-19th century.\n\nSmallpox epidemics in 1780–82 and 1837–38 brought devastation and drastic depopulation among the Plains Indians. By 1832, the federal government established a smallpox vaccination program for Native Americans (\"The Indian Vaccination Act of 1832\"). It was the first federal program created to address a health problem of Native Americans.\n\nWith the meeting of two worlds, animals, insects, and plants were carried from one to the other, both deliberately and by chance, in what is called the Columbian Exchange. In the 16th century, Spaniards and other Europeans brought horses to Mexico. Some of the horses escaped and began to breed and increase their numbers in the wild. As Native Americans adopted use of the animals, they began to change their cultures in substantial ways, especially by extending their nomadic ranges for hunting. The reintroduction of the horse to North America had a profound impact on Native American culture of the Great Plains.\n\nKing Philip's War, also called Metacom's War or Metacom's Rebellion, was the last major armed conflict between Native American inhabitants of present-day southern New England and English colonists and their Native American allies from 1675 to 1676. It continued in northern New England (primarily on the Maine frontier) even after King Philip was killed, until a treaty was signed at Casco Bay in April 1678.\n\nSome European philosophers considered Native American societies to be truly \"natural\" and representative of a golden age known to them only in folk history.\n\nDuring the American Revolution, the newly proclaimed United States competed with the British for the allegiance of Native American nations east of the Mississippi River. Most Native Americans who joined the struggle sided with the British, based both on their trading relationships and hopes that colonial defeat would result in a halt to further colonial expansion onto Native American land. The first native community to sign a treaty with the new United States Government was the Lenape.\n\nIn 1779 the Sullivan Expedition was carried out during the American Revolutionary War against the British and the four allied nations of the Iroquois. George Washington gave orders that made it clear he wanted the Iroquois threat completely eliminated:\n\nThe British made peace with the Americans in the Treaty of Paris (1783), through which they ceded vast Native American territories to the United States without informing or consulting with the Native Americans.\n\nThe United States was eager to expand, develop farming and settlements in new areas, and satisfy land hunger of settlers from New England and new immigrants. The national government initially sought to purchase Native American land by treaties. The states and settlers were frequently at odds with this policy.\n\nUnited States policy toward Native Americans continued to evolve after the American Revolution. George Washington and Henry Knox believed that Native Americans were equals but that their society was inferior. Washington formulated a policy to encourage the \"civilizing\" process. Washington had a six-point plan for civilization which included:\n\nIn the late 18th century, reformers starting with Washington and Knox, supported educating native children and adults, in efforts to \"civilize\" or otherwise assimilate Native Americans to the larger society (as opposed to relegating them to reservations). The Civilization Fund Act of 1819 promoted this civilization policy by providing funding to societies (mostly religious) who worked on Native American improvement.\n\nThe population of California Indians was reduced by 90% during the 19th century—from more than 200,000 in the early 19th century to approximately 15,000 at the end of the century, mostly due to disease. Epidemics swept through California Indian Country, such as the 1833 malaria epidemic. The population went into decline as a result of the Spanish authorities forcing Native Californians to live in the missions where they contracted diseases from which they had little immunity. Dr. Cook estimates that 15,250 or 45% of the population decrease in the Missions was caused by disease. Two epidemics of measles, one in 1806 and the other in 1828, caused many deaths. The mortality rates were so high that the missions were constantly dependent upon new conversions.[21] During the California Gold Rush, many natives were killed by incoming settlers as well as by militia units financed and organized by the California government. Some scholars contend that the state financing of these militias, as well as the US government's role in other massacres in California, such as the Bloody Island and Yontoket Massacres, in which up to 400 or more natives were killed in each massacre, constitutes a campaign of genocide against the native people of California.\n\nAs American expansion continued, Native Americans resisted settlers' encroachment in several regions of the new nation (and in unorganized territories), from the Northwest to the Southeast, and then in the West, as settlers encountered the Native American tribes of the Great Plains. East of the Mississippi River, an intertribal army led by Tecumseh, a Shawnee chief, fought a number of engagements in the Northwest during the period 1811–12, known as Tecumseh's War. During the War of 1812, Tecumseh's forces allied themselves with the British. After Tecumseh's death, the British ceased to aid the Native Americans south and west of Upper Canada and American expansion proceeded with little resistance. Conflicts in the Southeast include the Creek War and Seminole Wars, both before and after the Indian Removals of most members of the Five Civilized Tribes.\n\nIn the 1830s, President Andrew Jackson signed the Indian Removal Act of 1830, a policy of relocating Indians from their homelands to Indian Territory and reservations in surrounding areas to open their lands for non-native settlements. \n\nThis resulted in the Trail of Tears.\nIn July 1845, the New York newspaper editor John L. O'Sullivan coined the phrase, \"Manifest Destiny\", as the \"design of Providence\" supporting the territorial expansion of the United States. Manifest Destiny had serious consequences for Native Americans, since continental expansion for the United States took place at the cost of their occupied land. A justification for the policy of conquest and subjugation of the indigenous people emanated from the stereotyped perceptions of all Native Americans as \"merciless Indian savages\" (as described in the United States Declaration of Independence). The Indian Appropriations Act of 1851 set the precedent for modern-day Native American reservations through allocating funds to move western tribes onto reservations since there were no more lands available for relocation.\n\nNative American nations on the plains in the west continued armed conflicts with the U.S. throughout the 19th century, through what were called generally Indian Wars. Notable conflicts in this period include the Dakota War, Great Sioux War, Snake War, Colorado War, and Texas-Indian Wars. Expressing the frontier anti-Indian sentiment, Theodore Roosevelt believed the Indians were destined to vanish under the pressure of white civilization, stating in an 1886 lecture:\n\nOne of the last and most notable events during the Indian wars was the Wounded Knee Massacre in 1890. In the years leading up to it the U.S. government had continued to seize Lakota lands. A Ghost Dance ritual on the Northern Lakota reservation at Wounded Knee, South Dakota, led to the U.S. Army's attempt to subdue the Lakota. The dance was part of a religious movement founded by the Northern Paiute spiritual leader Wovoka that told of the return of the Messiah to relieve the suffering of Native Americans and promised that if they would live righteous lives and perform the Ghost Dance properly, the European American colonists would vanish, the bison would return, and the living and the dead would be reunited in an Edenic world. On December 29 at Wounded Knee, gunfire erupted, and U.S. soldiers killed up to 300 Indians, mostly old men, women, and children.\n\nNative Americans served in both the Union and Confederate military during the American Civil War. At the outbreak of the war, for example, the minority party of the Cherokees gave its allegiance to the Confederacy, while originally the majority party went for the North. Native Americans fought knowing they might jeopardize their independence, unique cultures, and ancestral lands if they ended up on the losing side of the Civil War. 28,693 Native Americans served in the Union and Confederate armies during the Civil War, participating in battles such as Pea Ridge, Second Manassas, Antietam, Spotsylvania, Cold Harbor, and in Federal assaults on Petersburg. A few Native American tribes, such as the Creek and the Choctaw, were slaveholders and found a political and economic commonality with the Confederacy. The Choctaw owned over 2,000 slaves.\n\nIn the 19th century, the incessant westward expansion of the United States incrementally compelled large numbers of Native Americans to resettle further west, often by force, almost always reluctantly. Native Americans believed this forced relocation illegal, given the Treaty of Hopewell of 1785. Under President Andrew Jackson, United States Congress passed the Indian Removal Act of 1830, which authorized the President to conduct treaties to exchange Native American land east of the Mississippi River for lands west of the river.\n\nAs many as 100,000 Native Americans relocated to the West as a result of this Indian removal policy. In theory, relocation was supposed to be voluntary and many Native Americans did remain in the East. In practice, great pressure was put on Native American leaders to sign removal treaties. The most egregious violation, the Trail of Tears, was the removal of the Cherokee by President Jackson to Indian Territory. The 1864 deportation of the Navajos by the U.S. government occurred when 8,000 Navajos were forced to an internment camp in Bosque Redondo, where, under armed guards, more than 3,500 Navajo and Mescalero Apache men, women, and children died from starvation and disease.\n\nIn 1817, the Cherokee became the first Native Americans recognized as U.S. citizens. Under Article 8 of the 1817 Cherokee treaty, \"Upwards of 300 Cherokees (Heads of Families) in the honest simplicity of their souls, made an election to become American citizens\".\n\nFactors establishing citizenship included:\n\nAfter the American Civil War, the Civil Rights Act of 1866 states, \"that all persons born in the United States, and not subject to any foreign power, excluding Indians not taxed, are hereby declared to be citizens of the United States\".\n\nIn 1871, Congress added a rider to the Indian Appropriations Act, signed into law by President Ulysses S. Grant, ending United States recognition of additional Native American tribes or independent nations, and prohibiting additional treaties.\n\nAfter the Indian wars in the late 19th century, the government established Native American boarding schools, initially run primarily by or affiliated with Christian missionaries. At this time, American society thought that Native American children needed to be acculturated to the general society. The boarding school experience was a total immersion in modern American society, but it could prove traumatic to children, who were forbidden to speak their native languages. They were taught Christianity and not allowed to practice their native religions, and in numerous other ways forced to abandon their Native American identities.\n\nBefore the 1930s, schools on the reservations provided no schooling beyond the sixth grade. To obtain more, boarding school was usually necessary. Small reservations with a few hundred people usually sent their children to nearby public schools. The \"Indian New Deal\" of the 1930s closed many of the boarding schools, and downplayed the assimilationist goals. The Indian Division of the Civilian Conservation Corps operated large-scale construction projects on the reservations, building thousands of new schools and community buildings. Under the leadership of John Collier the Bureau of Indian Affairs (BIA) brought in progressive educators to reshape Indian education. The BIA by 1938 taught 30,000 students in 377 boarding and day schools, or 40% of all Indian children in school. The Navajo largely opposed schooling of any sort, but the other tribes accepted the system. There were now high schools on larger reservations, educating not only teenagers but also an adult audience. There were no Indian facilities for higher education. They deemphasized textbooks, emphasized self-esteem, and started teaching Indian history. They promoted traditional arts and crafts of the sort that could be conducted on the reservations, such as making jewelry. The New Deal reformers met significant resistance from parents and teachers, and had mixed results. World War II brought younger Indians in contact with the broader society through military service and work in the munitions industries. The role of schooling was changed to focus on vocational education for jobs in urban America.\n\nSince the rise of self-determination for Native Americans, they have generally emphasized education of their children at schools near where they live. In addition, many federally recognized tribes have taken over operations of such schools and added programs of language retention and revival to strengthen their cultures. Beginning in the 1970s, tribes have also founded colleges at their reservations, controlled, and operated by Native Americans, to educate their young for jobs as well as to pass on their cultures.\n\nOn August 29, 1911, Ishi, generally considered to have been the last Native American to live most of his life without contact with European-American culture, was discovered near Oroville, California.\n\nIn 1919, the United States under President Woodrow Wilson granted citizenship to all Native Americans who had served in World War I. Nearly 10,000 men had enlisted and served, a high number in relation to their population. Despite this, in many areas Native Americans faced local resistance when they tried to vote and were discriminated against with barriers to voter registration.\n\nOn June 2, 1924, U.S. President Calvin Coolidge signed the Indian Citizenship Act, which made all Native Americans born in the United States and its territories American citizens. Prior to passage of the act, nearly two-thirds of Native Americans were already U.S. citizens, through marriage, military service or accepting land allotments. The Act extended citizenship to \"all non-citizen Indians born within the territorial limits of the United States\".\n\nCharles Curtis, a Congressman and longtime US Senator from Kansas, was of Kaw, Osage, Potawatomi, and European ancestry. After serving as a United States Representative and being repeatedly re-elected as United States Senator from Kansas, Curtis served as Senate Minority Whip for 10 years and as Senate Majority Leader for five years. He was very influential in the Senate. In 1928 he ran as the vice-presidential candidate with Herbert Hoover for president, and served from 1929 to 1933. He was the first person with significant Native American ancestry and the first person with acknowledged non-European ancestry to be elected to either of the highest offices in the land.\n\nAmerican Indians today in the United States have all the rights guaranteed in the U.S. Constitution, can vote in elections, and run for political office. Controversies remain over how much the federal government has jurisdiction over tribal affairs, sovereignty, and cultural practices.\n\nMid-century, the Indian termination policy and the Indian Relocation Act of 1956 marked a new direction for assimilating Native Americans into urban life.\n\nThe census counted 332,000 Indians in 1930 and 334,000 in 1940, including those on and off reservations in the 48 states. Total spending on Indians averaged $38 million a year in the late 1920s, dropping to a low of $23 million in 1933, and returning to $38 million in 1940.\n\nSome 44,000 Native Americans served in the United States military during World War II: at the time, one-third of all able-bodied Indian men from eighteen to fifty years of age. Described as the first large-scale exodus of indigenous peoples from the reservations since the removals of the 19th century, the men's service with the U.S. military in the international conflict was a turning point in Native American history. The overwhelming majority of Native Americans welcomed the opportunity to serve; they had a voluntary enlistment rate that was 40% higher than those drafted.\n\nTheir fellow soldiers often held them in high esteem, in part since the legend of the tough Native American warrior had become a part of the fabric of American historical legend. White servicemen sometimes showed a lighthearted respect toward Native American comrades by calling them \"chief\". The resulting increase in contact with the world outside of the reservation system brought profound changes to Native American culture. \"The war\", said the U.S. Indian Commissioner in 1945, \"caused the greatest disruption of Native life since the beginning of the reservation era\", affecting the habits, views, and economic well-being of tribal members. The most significant of these changes was the opportunity—as a result of wartime labor shortages—to find well-paying work in cities, and many people relocated to urban areas, particularly on the West Coast with the buildup of the defense industry.\n\nThere were also losses as a result of the war. For instance, a total of 1,200 Pueblo men served in World War II; only about half came home alive. In addition, many more Navajo served as code talkers for the military in the Pacific. The code they made, although cryptologically very simple, was never cracked by the Japanese.\n\nMilitary service and urban residency contributed to the rise of American Indian activism, particularly after the 1960s and the occupation of Alcatraz Island (1969–1971) by a student Indian group from San Francisco. In the same period, the American Indian Movement (AIM) was founded in Minneapolis, and chapters were established throughout the country, where American Indians combined spiritual and political activism. Political protests gained national media attention and the sympathy of the American public.\n\nThrough the mid-1970s, conflicts between governments and Native Americans occasionally erupted into violence. A notable late 20th-century event was the Wounded Knee incident on the Pine Ridge Indian Reservation. Upset with tribal government and the failures of the federal government to enforce treaty rights, about 300 Oglala Lakota and AIM activists took control of Wounded Knee on February 27, 1973.\n\nIndian activists from around the country joined them at Pine Ridge, and the occupation became a symbol of rising American Indian identity and power. Federal law enforcement officials and the national guard cordoned off the town, and the two sides had a standoff for 71 days. During much gunfire, one United States Marshal was wounded and paralyzed. In late April, a Cherokee and local Lakota man were killed by gunfire; the Lakota elders ended the occupation to ensure no more lives were lost.\n\nIn June 1975, two FBI agents seeking to make an armed robbery arrest at Pine Ridge Reservation were wounded in a firefight, and killed at close range. The AIM activist Leonard Peltier was sentenced in 1976 to two consecutive terms of life in prison in the FBI deaths.\n\nIn 1968, the government enacted the Indian Civil Rights Act. This gave tribal members most of the protections against abuses by tribal governments that the Bill of Rights accords to all U.S. citizens with respect to the federal government. In 1975, the U.S. government passed the Indian Self-Determination and Education Assistance Act, marking the culmination of fifteen years of policy changes. It resulted from American Indian activism, the Civil Rights Movement, and community development aspects of President Lyndon Johnson's social programs of the 1960s. The Act recognized the right and need of Native Americans for self-determination. It marked the U.S. government's turn away from the 1950s policy of termination of the relationship between tribes and the government. The U.S. government encouraged Native Americans' efforts at self-government and determining their futures. Tribes have developed organizations to administer their own social, welfare and housing programs, for instance. Tribal self-determination has created tension with respect to the federal government's historic trust obligation to care for Indians; however, the Bureau of Indian Affairs has never lived up to that responsibility.\n\nNavajo Community College, now called Diné College, the first tribal college, was founded in Tsaile, Arizona, in 1968 and accredited in 1979. Tensions immediately arose between two philosophies: one that the tribal colleges should have the same criteria, curriculum and procedures for educational quality as mainstream colleges, the other that the faculty and curriculum should be closely adapted to the particular historical culture of the tribe. There was a great deal of turnover, exacerbated by very tight budgets. In 1994, the U.S. Congress passed legislation recognizing the tribal colleges as land-grant colleges, which provided opportunities for large-scale funding. Thirty-two tribal colleges in the United States belong to the American Indian Higher Education Consortium. By the early 21st century, tribal nations had also established numerous language revival programs in their schools.\n\nIn addition, Native American activism has led major universities across the country to establish Native American studies programs and departments, increasing awareness of the strengths of Indian cultures, providing opportunities for academics, and deepening research on history and cultures in the United States. Native Americans have entered academia; journalism and media; politics at local, state and federal levels; and public service, for instance, influencing medical research and policy to identify issues related to American Indians.\n\nIn 2009, an \"apology to Native Peoples of the United States\" was included in the defense appropriations act. It states that the U.S. \"apologizes on behalf of the people of the United States to all Native Peoples for the many instances of violence, maltreatment, and neglect inflicted on Native Peoples by citizens of the United States\".\n\nIn 2013, jurisdiction over persons who were not tribal members under the Violence Against Women Act was extended to Indian Country. This closed a gap which prevented arrest or prosecution by tribal police or courts of abusive partners of tribal members who were not native or from another tribe.\n\nMigration to urban areas continued to grow with 70% of Native Americans living in urban areas in 2012, up from 45% in 1970 and 8% in 1940. Urban areas with significant Native American populations include Minneapolis, Denver, Albuquerque, Phoenix, Tucson, Chicago, Oklahoma City, Houston, New York City, Los Angeles, and Rapid City. Many lived in poverty. Racism, unemployment, drugs and gangs were common problems which Indian social service organizations such as the Little Earth housing complex in Minneapolis attempted to address. Grassroots efforts to support urban Indigenous populations have also taken place, as in the case of Bringing the Circle Together in Los Angeles.\n\nThe 2010 Census showed that the U.S. population on April 1, 2010, was 308.7 million. Out of the total U.S. population, 2.9 million people, or 0.9 percent, reported American Indian or Alaska Native alone. In addition, 2.3 million people, or another 0.7 percent, reported American Indian or Alaska Native in combination with one or more other races. Together, these two groups totaled 5.2 million people. Thus, 1.7 percent of all people in the United States identified as American Indian or Alaska Native, either alone or in combination with one or more other races.\n\nThe definition of American Indian or Alaska Native used in the 2010 census:\n\nAccording to Office of Management and Budget, \"American Indian or Alaska Native\" refers to a person having origins in any of the original peoples of North and South America (including Central America) and who maintains tribal affiliation or community attachment.\n\nThe 2010 census permitted respondents to self-identify as being of one or more races. Self-identification dates from the census of 1960; prior to that the race of the respondent was determined by opinion of the census taker. The option to select more than one race was introduced in 2000. If American Indian or Alaska Native was selected, the form requested the individual provide the name of the \"enrolled or principal tribe\". \n\nThe census counted 248,000 Indians in 1890, 332,000 in 1930 and 334,000 in 1940, including those on and off reservations in the 48 states. Total spending on Indians averaged $38 million a year in the late 1920s, dropping to a low of $23 million in 1933, and returning to $38 million in 1940.\n78% of Native Americans live outside a reservation. Full-blood individuals are more likely to live on a reservation than mixed-blood individuals. The Navajo, with 286,000 full-blood individuals, is the largest tribe if only full-blood individuals are counted; the Navajo are the tribe with the highest proportion of full-blood individuals, 86.3%. The Cherokee have a different history; it is the largest tribe with 819,000 individuals, and it has 284,000 full-blood individuals.\n\nAs of 2012, 70% of American Indians live in urban areas, up from 45% in 1970 and 8% in 1940. Urban areas with significant Native American populations include Minneapolis, Denver, Phoenix, Tucson, Chicago, Oklahoma City, Houston, New York City, and Los Angeles. Many live in poverty. Racism, unemployment, drugs and gangs are common problems which Indian social service organizations such as the Little Earth housing complex in Minneapolis attempt to address.\n\nAccording to 2003 United States Census Bureau estimates, a little over one third of the 2,786,652 Native Americans in the United States live in three states: California at 413,382, Arizona at 294,137 and Oklahoma at 279,559.\n\nIn 2010, the U.S. Census Bureau estimated that about 0.8% of the U.S. population was of American Indian or Alaska Native descent. This population is unevenly distributed across the country. Below, all fifty states, as well as the District of Columbia and Puerto Rico, are listed by the proportion of residents citing American Indian or Alaska Native ancestry, based on the 2010 U.S. Census.\nIn 2006, the U.S. Census Bureau estimated that about less than 1.0% of the U.S. population was of Native Hawaiian or Pacific Islander descent. This population is unevenly distributed across twenty-six states. Below, are the twenty-six states that had at least 0.1%. They are listed by the proportion of residents citing Native Hawaiian or Pacific Islander ancestry, based on 2006 estimates:\n\nBelow are numbers for U.S. citizens self-identifying to selected tribal grouping, according to the 2000 U.S. census.\n\nThere are 573 federally recognized tribal governments in the United States. These tribes possess the right to form their own governments, to enforce laws (both civil and criminal) within their lands, to tax, to establish requirements for membership, to license and regulate activities, to zone, and to exclude persons from tribal territories. Limitations on tribal powers of self-government include the same limitations applicable to states; for example, neither tribes nor states have the power to make war, engage in foreign relations, or coin money (this includes paper currency).\n\nMany Native Americans and advocates of Native American rights point out that the U.S. federal government's claim to recognize the \"sovereignty\" of Native American peoples falls short, given that the United States wishes to govern Native American peoples and treat them as subject to U.S. law. Such advocates contend that full respect for Native American sovereignty would require the U.S. government to deal with Native American peoples in the same manner as any other sovereign nation, handling matters related to relations with Native Americans through the Secretary of State, rather than the Bureau of Indian Affairs. The Bureau of Indian Affairs reports on its website that its \"responsibility is the administration and management of of land held in trust by the United States for American Indians, Indian tribes, and Alaska Natives\". Many Native Americans and advocates of Native American rights believe that it is condescending for such lands to be considered \"held in trust\" and regulated in any fashion by other than their own tribes, whether the U.S. or Canadian governments, or any other non-Native American authority.\n\nAs of 2000, the largest groups in the United States by population were Navajo, Cherokee, Choctaw, Sioux, Chippewa, Apache, Blackfeet, Iroquois, and Pueblo. In 2000, eight of ten Americans with Native American ancestry were of mixed ancestry. It is estimated that by 2100 that figure will rise to nine out of ten.\n\nIn addition, there are a number of tribes that are recognized by individual states, but not by the federal government. The rights and benefits associated with state recognition vary from state to state.\n\nSome tribal groups have been unable to document the cultural continuity required for federal recognition. The Muwekma Ohlone of the San Francisco bay area are pursuing litigation in the federal court system to establish recognition. Many of the smaller eastern tribes, long considered remnants of extinct peoples, have been trying to gain official recognition of their tribal status. Several tribes in Virginia and North Carolina have gained state recognition. Federal recognition confers some benefits, including the right to label arts and crafts as Native American and permission to apply for grants that are specifically reserved for Native Americans. But gaining federal recognition as a tribe is extremely difficult; to be established as a tribal group, members have to submit extensive genealogical proof of tribal descent and continuity of the tribe as a culture.\nIn July 2000, the Washington State Republican Party adopted a resolution recommending that the federal and legislative branches of the U.S. government terminate tribal governments. In 2007, a group of Democratic Party congressmen and congresswomen introduced a bill in the U.S. House of Representatives to \"terminate\" the Cherokee Nation. This was related to their voting to exclude Cherokee Freedmen as members of the tribe unless they had a Cherokee ancestor on the Dawes Rolls, although all Cherokee Freedmen and their descendants had been members since 1866.\n\nAs of 2004, various Native Americans are wary of attempts by others to gain control of their reservation lands for natural resources, such as coal and uranium in the West.\n\nIn the state of Virginia, Native Americans face a unique problem. Until 2017 Virginia previously had no federally recognized tribes but the state had recognized eight. This is related historically to the greater impact of disease and warfare on the Virginia Indian populations, as well as their intermarriage with Europeans and Africans. Some people confused the ancestry with culture, but groups of Virginia Indians maintained their cultural continuity. Most of their early reservations were ended under the pressure of early European settlement.\n\nSome historians also note the problems of Virginia Indians in establishing documented continuity of identity, due to the work of Walter Ashby Plecker (1912–1946). As registrar of the state's Bureau of Vital Statistics, he applied his own interpretation of the one-drop rule, enacted in law in 1924 as the state's Racial Integrity Act. It recognized only two races: \"white\" and \"colored\".\n\nPlecker, a segregationist, believed that the state's Native Americans had been \"mongrelized\" by intermarriage with African Americans; to him, ancestry determined identity, rather than culture. He thought that some people of partial black ancestry were trying to \"pass\" as Native Americans. Plecker thought that anyone with any African heritage had to be classified as colored, regardless of appearance, amount of European or Native American ancestry, and cultural/community identification. Plecker pressured local governments into reclassifying all Native Americans in the state as \"colored\", and gave them lists of family surnames to examine for reclassification based on his interpretation of data and the law. This led to the state's destruction of accurate records related to families and communities who identified as Native American (as in church records and daily life). By his actions, sometimes different members of the same family were split by being classified as \"white\" or \"colored\". He did not allow people to enter their primary identification as Native American in state records. In 2009, the Senate Indian Affairs Committee endorsed a bill that would grant federal recognition to tribes in Virginia.\n\nTo achieve federal recognition and its benefits, tribes must prove continuous existence since 1900. The federal government has maintained this requirement, in part because through participation on councils and committees, federally recognized tribes have been adamant about groups' satisfying the same requirements as they did.\n\nNative American struggles amid poverty to maintain life on the reservation or in larger society have resulted in a variety of health issues, some related to nutrition and health practices. The community suffers a vulnerability to and disproportionately high rate of alcoholism.\n\nIn a study conducted in 2006–2007, non-Native Americans admitted they rarely encountered Native Americans in their daily lives. While sympathetic toward Native Americans and expressing regret over the past, most people had only a vague understanding of the problems facing Native Americans today. For their part, Native Americans told researchers that they believed they continued to face prejudice, mistreatment, and inequality in the broader society.\n\nFederal contractors and subcontractors, such as businesses and educational institutions, are legally required to adopt equal opportunity employment and affirmative action measures intended to prevent discrimination against employees or applicants for employment on the basis of \"color, religion, sex, or national origin\". For this purpose, a Native American is defined as \"A person having origins in any of the original peoples of North and South America (including Central America), and who maintains a tribal affiliation or community attachment\". However, self-reporting is permitted: \"Educational institutions and other recipients should allow students and staff to self-identify their race and ethnicity unless self-identification is not practicable or feasible.\"\n\nSelf-reporting opens the door to \"box checking\" by people who, despite not having a substantial relationship to Native American culture, innocently or fraudulently check the box for Native American.\n\nAmerican Indian activists in the United States and Canada have criticized the use of Native American mascots in sports, as perpetuating stereotypes.\n\nThere has been a steady decline in the number of secondary school and college teams using such names, images, and mascots. Some tribal team names have been approved by the tribe in question, such as the Seminole Tribe of Florida's approving use of their name for the teams of Florida State University.\n\nAmong professional teams, only the NBA's Golden State Warriors discontinued use of Native American-themed logos in 1971. Controversy has remained regarding teams such as the NFL's Washington Redskins, whose name is considered to be a racial slur, and MLB's Cleveland Indians, whose usage of a caricature called Chief Wahoo has also faced protest.\n\nNative Americans have been depicted by American artists in various ways at different periods. A number of 19th- and 20th-century United States and Canadian painters, often motivated by a desire to document and preserve Native culture, specialized in Native American subjects. Among the most prominent of these were Elbridge Ayer Burbank, George Catlin, Seth Eastman, Paul Kane, W. Langdon Kihn, Charles Bird King, Joseph Henry Sharp, and John Mix Stanley.\n\nIn the 20th century, early portrayals of Native Americans in movies and television roles were first performed by European Americans dressed in mock traditional attire. Examples included \"The Last of the Mohicans\" (1920), \"Hawkeye and the Last of the Mohicans\" (1957), and \"F Troop\" (1965–67). In later decades, Native American actors such as Jay Silverheels in \"The Lone Ranger\" television series (1949–57) came to prominence. Roles of Native Americans were limited and not reflective of Native American culture. By the 1970s some Native American film roles began to show more complexity, such as those in \"Little Big Man\" (1970), \"Billy Jack\" (1971), and \"The Outlaw Josey Wales\" (1976), which depicted Native Americans in minor supporting roles.\n\nFor years, Native people on U.S. television were relegated to secondary, subordinate roles. During the years of the series \"Bonanza\" (1959–1973), no major or secondary Native characters appeared on a consistent basis. The series \"The Lone Ranger\" (1949–1957), \"Cheyenne\" (1955–1963), and \"Law of the Plainsman\" (1959–1963) had Native characters who were essentially aides to the central white characters. This continued in such series as \"How the West Was Won\". These programs resembled the \"sympathetic\" yet contradictory film \"Dances With Wolves\" of 1990, in which, according to Ella Shohat and Robert Stam, the narrative choice was to relate the Lakota story as told through a Euro-American voice, for wider impact among a general audience.\nLike the 1992 remake of \"The Last of the Mohicans\" and \"\" (1993), \"Dances with Wolves\" employed a number of Native American actors, and made an effort to portray Indigenous languages.\n\nIn 2009 \"We Shall Remain\" (2009), a television documentary by Ric Burns and part of the \"American Experience\" series, presented a five-episode series \"from a Native American perspective\". It represented \"an unprecedented collaboration between Native and non-Native filmmakers and involves Native advisors and scholars at all levels of the project\". The five episodes explore the impact of King Philip's War on the northeastern tribes, the \"Native American confederacy\" of Tecumseh's War, the U.S.-forced relocation of Southeastern tribes known as the Trail of Tears, the pursuit and capture of Geronimo and the Apache Wars, and concludes with the Wounded Knee incident, participation by the American Indian Movement, and the increasing resurgence of modern Native cultures since.\n\nNative Americans are often known as Indians or American Indians. The term \"Native American\" was introduced in the United States in preference to the older term \"Indian\" to distinguish the indigenous peoples of the Americas from the people of India, and to avoid negative stereotypes associated with the term \"Indian\". Many indigenous Americans, however, prefer the term \"American Indian\" and many tribes include the word Indian in their formal title.\n\nCriticism of the neologism \"Native American\" comes from diverse sources. Russell Means, an American Indian activist, opposed the term \"Native American\" because he believed it was imposed by the government without the consent of American Indians. He has also argued that the use of the word \"Indian\" derives not from a confusion with India but from a Spanish expression \"en Dios\" meaning \"in God\" (and a near-homophone of the Spanish word for \"Indians\", \"indios\").\n\nA 1995 U.S. Census Bureau survey found that more Native Americans in the United States preferred \"American Indian\" to \"Native American\". Most American Indians are comfortable with \"Indian\", \"American Indian\", and \"Native American\", and the terms are often used interchangeably. The traditional term is reflected in the name chosen for the National Museum of the American Indian, which opened in 2004 on the Mall in Washington, D.C.\n\nGambling has become a leading industry. Casinos operated by many Native American governments in the United States are creating a stream of gambling revenue that some communities are beginning to leverage to build diversified economies. Although many Native American tribes have casinos, the impact of Native American gaming is widely debated. Some tribes, such as the Winnemem Wintu of Redding, California, feel that casinos and their proceeds destroy culture from the inside out. These tribes refuse to participate in the gambling industry.\n\nNumerous tribes around the country have entered the financial services market including the Otoe-Missouria, Tunica-Biloxi, and the Rosebud Sioux. Because of the challenges involved in starting a financial services business from scratch, many tribes hire outside consultants and vendors to help them launch these businesses and manage the regulatory issues involved.\nSimilar to the tribal sovereignty debates that occurred when tribes first entered the gaming industry, the tribes, states, and federal government are currently in disagreement regarding who possesses the authority to regulate these e-commerce business entities.\n\nProsecution of serious crime, historically endemic on reservations, was required by the 1885 Major Crimes Act, 18 U.S.C. §§1153, 3242, and court decisions to be investigated by the federal government, usually the Federal Bureau of Investigation, and prosecuted by United States Attorneys of the United States federal judicial district in which the reservation lies.\n\nA December 13, 2009 \"New York Times\" article about growing gang violence on the Pine Ridge Indian Reservation estimated that there were 39 gangs with 5,000 members on that reservation alone. Navajo country recently reported 225 gangs in its territory.\n\nAs of 2012, a high incidence of rape continued to impact Native American women and Alaskan native women. According to the Department of Justice, 1 in 3 Native women have suffered rape or attempted rape, more than twice the national rate. About 46 percent of Native American women have been raped, beaten, or stalked by an intimate partner, according to a 2010 study by the Centers for Disease Control. According to Professor N. Bruce Duthu, \"More than 80 percent of Indian victims identify their attacker as non-Indian\".\n\nToday, other than tribes successfully running casinos, many tribes struggle, as they are often located on reservations isolated from the main economic centers of the country. The estimated 2.1 million Native Americans are the most impoverished of all ethnic groups. According to the 2000 Census, an estimated 400,000 Native Americans reside on reservation land. While some tribes have had success with gaming, only 40% of the 562 federally recognized tribes operate casinos. According to a 2007 survey by the U.S. Small Business Administration, only 1% of Native Americans own and operate a business.\n\nSocial statistics highlight the challenges faced by Native American communities: highest teen suicide rate of all minorities at 18.5 per 100,000, highest rate of teen pregnancy, highest high school drop-out rate at 54%, lowest per capita income, and unemployment rates between 50% and 90%.\n\nThe barriers to economic development on Native American reservations have been identified by Joseph Kalt and Stephen Cornell of the Harvard Project on American Indian Economic Development at Harvard University, in their report: \"What Can Tribes Do? Strategies and Institutions in American Indian Economic Development\" (2008), are summarized as follows:\n\nA major barrier to development is the lack of entrepreneurial knowledge and experience within Indian reservations. \"A general lack of education and experience about business is a significant challenge to prospective entrepreneurs\", was the report on Native American entrepreneurship by the Northwest Area Foundation in 2004. \"Native American communities that lack entrepreneurial traditions and recent experiences typically do not provide the support that entrepreneurs need to thrive. Consequently, experiential entrepreneurship education needs to be embedded into school curricula and after-school and other community activities. This would allow students to learn the essential elements of entrepreneurship from a young age and encourage them to apply these elements throughout life\". \"Rez Biz\" magazine addresses these issues.\n\nHistorical trauma is described as collective emotional and psychological damage throughout a person's lifetime and across multiple generations. Examples of historical trauma can be seen through the Wounded Knee Massacre of 1890, where over 200 unarmed Lakota were killed, and the Dawes Allotment Act of 1887, when American Indians lost four-fifths of their land.\n\nAmerican Indian youth have higher rates of substance and alcohol abuse deaths than the general population. Many American Indians can trace the beginning of their substance and alcohol abuse to a traumatic event related to their offender's own substance abuse. A person's substance abuse can be described as a defense mechanism against the user's emotions and trauma. For American Indians alcoholism is a symptom of trauma passed from generation to generation and influenced by oppressive behaviors and policies by the dominant Euro-American society. Boarding schools were made to \"Kill the Indian, Save the man\". Shame among American Indians can be attributed to the hundreds of years of discrimination.\n\nAmerican Indians do not view mind, body, and soul as separate from each other or themselves as the Western worldview does. American Indians believe all is connected and related to each other. American Indian psychologists have been asked to use mental health practices that cultivate American Indian values rather than using conventional ways of counseling. The Wellbriety Movement creates a space for American Indians to learn how to reconnect with their culture by using culturally specific principles to become and remain sober. Some examples are burning sage, cedar, and sweetgrass as a means to cleanse physical and spiritual spaces, verbally saying prayers and singing in one's own tribal language, and participating in tribal drum groups and ceremonies as part of meetings and gatherings.\n\nThe culture of Pre-Columbian North America is usually defined by the concept of the culture area, namely a geographical region where shared cultural traits occur. The northwest culture area, for example shared common traits such as salmon fishing, woodworking, large villages or towns and a hierarchical social structure. \n\nThough cultural features, language, clothing, and customs vary enormously from one tribe to another, there are certain elements which are encountered frequently and shared by many tribes. Early European American scholars described the Native Americans as having a society dominated by clans.\n\nEuropean colonization of the Americas had a major impact on Native American culture through what is known as the Columbian exchange. The Columbian exchange, also known as the Columbian interchange, was the widespread transfer of plants, animals, culture, human populations, technology, and ideas between the Americas and the Old World in the 15th and 16th centuries, following Christopher Columbus's 1492 voyage. The Columbian exchange generally had a destructive impact on Native American culture through disease, and a 'clash of cultures', whereby European values of private property, the family, and labor, led to conflict, appropriation of traditional communal lands and slavery. \n\nThe impact of the Columbian exchange was not entirely negative however. For example, the re-introduction of the horse to North America allowed the Plains Indian to revolutionize their way of life by making hunting, trading, and warfare far more effective, and to greatly improve their ability to transport possessions and move their settlements. \n\nThe Great Plains tribes were still hunting the bison when they first encountered the Europeans. The Spanish reintroduction of the horse to North America in the 17th century and Native Americans' learning to use them greatly altered the Native Americans' culture, including changing the way in which they hunted large game. Horses became such a valuable, central element of Native lives that they were counted as a measure of wealth.\n\nThe Na-Dené, Algic, and Uto-Aztecan families are the largest in terms of number of languages. Uto-Aztecan has the most speakers (1.95 million) if the languages in Mexico are considered (mostly due to 1.5 million speakers of Nahuatl); Na-Dené comes in second with approximately 200,000 speakers (nearly 180,000 of these are speakers of Navajo), and Algic in third with about 180,000 speakers (mainly Cree and Ojibwe). Na-Dené and Algic have the widest geographic distributions: Algic currently spans from northeastern Canada across much of the continent down to northeastern Mexico (due to later migrations of the Kickapoo) with two outliers in California (Yurok and Wiyot); Na-Dené spans from Alaska and western Canada through Washington, Oregon, and California to the U.S. Southwest and northern Mexico (with one outlier in the Plains). Several families consist of only 2 or 3 languages. Demonstrating genetic relationships has proved difficult due to the great linguistic diversity present in North America. Two large (super-) family proposals, Penutian and Hokan, look particularly promising. However, even after decades of research, a large number of families remain.\n\nA number of English words have been derived from Native American languages.\n\nTo counteract a shift to English, some Native American tribes have initiated language immersion schools for children, where a native Indian language is the medium of instruction. For example, the Cherokee Nation initiated a 10-year language preservation plan that involved raising new fluent speakers of the Cherokee language from childhood on up through school immersion programs as well as a collaborative community effort to continue to use the language at home. This plan was part of an ambitious goal that, in 50 years, will result in 80% or more of the Cherokee people being fluent in the language. The Cherokee Preservation Foundation has invested $3 million in opening schools, training teachers, and developing curricula for language education, as well as initiating community gatherings where the language can be actively used. Formed in 2006, the Kituwah Preservation & Education Program (KPEP) on the Qualla Boundary focuses on language immersion programs for children from birth to fifth grade, developing cultural resources for the general public and community language programs to foster the Cherokee language among adults.\n\nThere is also a Cherokee language immersion school in Tahlequah, Oklahoma, that educates students from pre-school through eighth grade. Because Oklahoma's official language is English, Cherokee immersion students are hindered when taking state-mandated tests because they have little competence in English. The Department of Education of Oklahoma said that in 2012 state tests: 11% of the school's sixth-graders showed proficiency in math, and 25% showed proficiency in reading; 31% of the seventh-graders showed proficiency in math, and 87% showed proficiency in reading; 50% of the eighth-graders showed proficiency in math, and 78% showed proficiency in reading. The Oklahoma Department of Education listed the charter school as a Targeted Intervention school, meaning the school was identified as a low-performing school but has not so that it was a Priority School. Ultimately, the school made a C, or a 2.33 grade point average on the state's A-F report card system. The report card shows the school getting an F in mathematics achievement and mathematics growth, a C in social studies achievement, a D in reading achievement, and an A in reading growth and student attendance. \"The C we made is tremendous,\" said school principal Holly Davis, \"[t]here is no English instruction in our school's younger grades, and we gave them this test in English.\" She said she had anticipated the low grade because it was the school's first year as a state-funded charter school, and many students had difficulty with English. Eighth graders who graduate from the Tahlequah immersion school are fluent speakers of the language, and they usually go on to attend Sequoyah High School where classes are taught in both English and Cherokee.\n\nAn early crop the Native Americans grew was squash. Other early crops included cotton, sunflower, pumpkins, tobacco, goosefoot, knotgrass, and sump weed.\n\nAgriculture in the southwest started around 4,000 years ago when traders brought cultigens from Mexico. Due to the varying climate, some ingenuity was needed for agriculture to be successful. The climate in the southwest ranged from cool, moist mountains regions, to dry, sandy soil in the desert. Some innovations of the time included irrigation to bring water into the dry regions and the selection of seed based on the traits of the growing plants that bore them.\n\nIn the southwest, they grew beans that were self-supported, much like the way they are grown today. In the east, however, they were planted next to corn in order for the vines to be able to \"climb\" the cornstalks.\n\nThe most important crop the Native Americans raised was maize. It was first started in Mesoamerica and spread north. About 2,000 years ago it reached eastern America. This crop was important to the Native Americans because it was part of their everyday diet; it could be stored in underground pits during the winter, and no part of it was wasted. The husk was made into art crafts, and the cob was used as fuel for fires.\n\nBy 800 CE the Native Americans had established three main crops—beans, squash, and corn—called the three sisters.\n\nThe agriculture gender roles of the Native Americans varied from region to region. In the southwest area, men prepared the soil with hoes. The women were in charge of planting, weeding, and harvesting the crops. In most other regions, the women were in charge of doing everything, including clearing the land. Clearing the land was an immense chore since the Native Americans rotated fields frequently. There is a tradition that Squanto showed the Pilgrims in New England how to put fish in fields to act like a fertilizer, but the truth of this story is debated.\n\nNative Americans did plant beans next to corn; the beans would replace the nitrogen which the corn took from the ground, as well as using corn stalks for support for climbing. Native Americans used controlled fires to burn weeds and clear fields; this would put nutrients back into the ground. If this did not work, they would simply abandon the field to let it be fallow, and find a new spot for cultivation.\n\nEuropeans in the eastern part of the continent observed that Native Americans cleared large areas for cropland. Their fields in New England sometimes covered hundreds of acres. Colonists in Virginia noted thousands of acres under cultivation by Native Americans.\n\nNative Americans commonly used tools such as the hoe, maul, and dibber. The hoe was the main tool used to till the land and prepare it for planting; then it was used for weeding. The first versions were made out of wood and stone. When the settlers brought iron, Native Americans switched to iron hoes and hatchets. The dibber was a digging stick, used to plant the seed. Once the plants were harvested, women prepared the produce for eating. They used the maul to grind the corn into mash. It was cooked and eaten that way or baked as corn bread.\n\nTraditional Native American ceremonies are still practiced by many tribes and bands, and the older theological belief systems are still held by many of the native people. These spiritualities may accompany adherence to another faith, or can represent a person's primary religious identity. While much Native American spiritualism exists in a tribal-cultural continuum, and as such cannot be easily separated from tribal identity itself, certain other more clearly defined movements have arisen among \"traditional\" Native American practitioners, these being identifiable as \"religions\" in the prototypical sense familiar in the industrialized Western world.\n\nTraditional practices of some tribes include the use of sacred herbs such as tobacco, sweetgrass or sage. Many Plains tribes have sweatlodge ceremonies, though the specifics of the ceremony vary among tribes. Fasting, singing and prayer in the ancient languages of their people, and sometimes drumming are also common.\n\nThe Midewiwin Lodge is a traditional medicine society inspired by the oral traditions and prophesies of the Ojibwa (Chippewa) and related tribes.\n\nAnother significant religious body among Native peoples is known as the Native American Church. It is a syncretistic church incorporating elements of Native spiritual practice from a number of different tribes as well as symbolic elements from Christianity. Its main rite is the peyote ceremony. Prior to 1890, traditional religious beliefs included Wakan Tanka. In the American Southwest, especially New Mexico, a syncretism between the Catholicism brought by Spanish missionaries and the native religion is common; the religious drums, chants, and dances of the Pueblo people are regularly part of Masses at Santa Fe's Saint Francis Cathedral. Native American-Catholic syncretism is also found elsewhere in the United States. (e.g., the National Kateri Tekakwitha Shrine in Fonda, New York, and the National Shrine of the North American Martyrs in Auriesville, New York).\n\nThe eagle feather law (Title 50 Part 22 of the Code of Federal Regulations) stipulates that only individuals of certifiable Native American ancestry enrolled in a federally recognized tribe are legally authorized to obtain eagle feathers for religious or spiritual use. The law does not allow Native Americans to give eagle feathers to non-Native Americans.\n\nGender roles are differentiated in many Native American tribes. Many Natives have historically defied colonial expectations of sexuality and gender, and continue to do so in contemporary life.\n\nWhether a particular tribe is predominantly matrilineal or patrilineal, often both sexes have some degree of decision-making power within the tribe. Many Nations, such as the Haudenosaunee Five Nations and the Southeast Muskogean tribes, have matrilineal or Clan Mother systems, in which property and hereditary leadership are controlled by and passed through the maternal lines. In these Nations, the children are considered to belong to the mother's clan. In Cherokee culture, women own the family property. When traditional young women marry, their husbands may join them in their mother's household.\n\nMatrilineal structures enable young women to have assistance in childbirth and rearing, and protect them in case of conflicts between the couple. If a couple separates or the man dies, the woman has her family to assist her. In matrilineal cultures the mother's brothers are usually the leading male figures in her children's lives; fathers have no standing in their wife and children's clan, as they still belong to their own mother's clan. Hereditary clan chief positions pass through the mother's line and chiefs have historically been selected on recommendation of women elders, who also could disapprove of a chief.\n\nIn the patrilineal tribes, such as the Omaha, Osage, Ponca, and Lakota, hereditary leadership passes through the male line, and children are considered to belong to the father and his clan. In patrilineal tribes, if a woman marries a non-Native, she is no longer considered part of the tribe, and her children are considered to share the ethnicity and culture of their father.\n\nIn patriarchal tribes, gender roles tend to be rigid. Men have historically hunted, traded and made war while, as life-givers, women have primary responsibility for the survival and welfare of the families (and future of the tribe). Women usually gather and cultivate plants, use plants and herbs to treat illnesses, care for the young and the elderly, make all the clothing and instruments, and process and cure meat and skins from the game. Some mothers use cradleboards to carry an infant while working or traveling. In matriarchal and egalitarian nations, the gender roles are usually not so clear-cut, and are even less so in the modern era.\n\nAt least several dozen tribes allowed polygyny to sisters, with procedural and economic limits.\n\nLakota, Dakota, and Nakota girls are encouraged to learn to ride, hunt and fight. Though fighting in war has mostly been left to the boys and men, occasionally women have fought as well - both in battles and in defense of the home - especially if the tribe was severely threatened.\n\nNative American leisure time led to competitive individual and team sports. Jim Thorpe, Joe Hipp, Notah Begay III, Chris Wondolowski, Jacoby Ellsbury, Joba Chamberlain, Kyle Lohse, Sam Bradford, Jack Brisco, Tommy Morrison, Billy Mills, Angel Goodrich, Shoni Schimmel, and Kyrie Irving are well known professional athletes.\nNative American ball sports, sometimes referred to as lacrosse, stickball, or baggataway, were often used to settle disputes, rather than going to war, as a civil way to settle potential conflict. The Choctaw called it \"isitoboli\" (\"Little Brother of War\"); the Onondaga name was \"dehuntshigwa'es\" (\"men hit a rounded object\"). There are three basic versions, classified as Great Lakes, Iroquoian, and Southern.\n\nThe game is played with one or two rackets or sticks and one ball. The object of the game is to land the ball in the opposing team's goal (either a single post or net) to score and to prevent the opposing team from scoring on your goal. The game involves as few as 20 or as many as 300 players with no height or weight restrictions and no protective gear. The goals could be from around apart to about ; in lacrosse the field is . A Jesuit priest referenced stickball in 1729, and George Catlin painted the subject.\n\nCurrently in the WNBA, there are 2 women who are of Native ancestry and enrolled in federally recognized tribes.\n\nAngel Goodrich, Cherokee, was selected in the third round of the WNBA draft (29th pick overall) by the Tulsa Shock. At the time she was the highest-drafted Native American player in the history of the WNBA. During the 2013–14 off-season, she played for Chevakata Vologda in the Russian Premier League.[10] In 2014, she completed her second and final season for the Shock. In 2015, she was picked up on waivers by the Seattle Storm\n\nShoni Schimmel, Confederated Tribes of the Umatilla Indian Reservation, is an American professional basketball player. She was an All-American college player at the University of Louisville and a first round draft pick of the WNBA's Atlanta Dream. She also earned recognition as the 2014 WNBA All-Star Game Most Valuable Player on July 19, 2014, in Phoenix, Arizona.[7]\n\nChunkey was a game that consisted of a stone-shaped disk that was about 1–2 inches in diameter. The disk was thrown down a corridor so that it could roll past the players at great speed. The disk would roll down the corridor, and players would throw wooden shafts at the moving disk. The object of the game was to strike the disk or prevent your opponents from hitting it.\nJim Thorpe, a Sauk and Fox Native American, was an all-round athlete playing football and baseball in the early 20th century. Future President Dwight Eisenhower injured his knee while trying to tackle the young Thorpe. In a 1961 speech, Eisenhower recalled Thorpe: \"Here and there, there are some people who are supremely endowed. My memory goes back to Jim Thorpe. He never practiced in his life, and he could do anything better than any other football player I ever saw.\"\n\nIn the 1912 Olympics, Thorpe could run the 100-yard dash in 10 seconds flat, the 220 in 21.8 seconds, the 440 in 51.8 seconds, the 880 in 1:57, the mile in 4:35, the 120-yard high hurdles in 15 seconds, and the 220-yard low hurdles in 24 seconds. He could long jump 23 ft 6 in and high-jump 6 ft 5 in. He could pole vault , put the shot , throw the javelin , and throw the discus . Thorpe entered the U.S. Olympic trials for the pentathlon and the decathlon.\n\nLouis Tewanima, Hopi people, was an American two-time Olympic distance runner and silver medalist in the 10,000 meter run in 1912. He ran for the Carlisle Indian School where he was a teammate of Jim Thorpe. His silver medal in 1912 remained the best U.S. achievement in this event until another Indian, Billy Mills, won the gold medal in 1964. Tewanima also competed at the 1908 Olympics, where he finished in ninth place in the marathon.[1]\n\nEllison Brown, of the Narragansett people from Rhode Island, better known as \"Tarzan\" Brown, won two Boston Marathons (1936, 1939) and competed on the United States Olympic team in the 1936 Olympic Games in Berlin, Germany, but did not finish due to injury. He qualified for the 1940 Olympic Games in Helsinki, Finland, but the games were canceled due to the outbreak of World War II.\n\nBilly Mills, a Lakota and USMC officer, won the gold medal in the 10,000 meter run at the 1964 Tokyo Olympics. He was the only American ever to win the Olympic gold in this event. An unknown before the Olympics, Mills finished second in the U.S. Olympic trials.\n\nBilly Kidd, part Abenaki from Vermont, became the first American male to medal in alpine skiing in the Olympics, taking silver at age 20 in the slalom in the 1964 Winter Olympics at Innsbruck, Austria. Six years later at the 1970 World Championships, Kidd won the gold medal in the combined event and took the bronze medal in the slalom.\n\nAshton Locklear (Lumbee), an uneven bars specialist was an alternate for the 2016 Summer Olympics U.S. gymnastics team, the Final Five. In 2016, Kyrie Irving (Sioux) also helped Team USA win the gold medal at the 2016 Summer Olympics. With the win, he became just the fourth member of Team USA to capture the NBA championship and an Olympic gold medal in the same year, joining LeBron James, Michael Jordan, and Scottie Pippen.\n\nTraditional Native American music is almost entirely monophonic, but there are notable exceptions. Native American music often includes drumming or the playing of rattles or other percussion instruments but little other instrumentation. Flutes and whistles made of wood, cane, or bone are also played, generally by individuals, but in former times also by large ensembles (as noted by Spanish conquistador de Soto). The tuning of modern flutes is typically pentatonic.\n\nPerformers with Native American parentage have occasionally appeared in American popular music such as Rita Coolidge, Wayne Newton, Gene Clark, Buffy Sainte-Marie, Blackfoot, Tori Amos, Redbone (members are also of Mexican descent), and CocoRosie. Some, such as John Trudell, have used music to comment on life in Native America. Other musicians such as R. Carlos Nakai, Joanne Shenandoah and Robert \"Tree\" Cody integrate traditional sounds with modern sounds in instrumental recordings, whereas the music by artist Charles Littleleaf is derived from ancestral heritage as well as nature. A variety of small and medium-sized recording companies offer an abundance of recent music by Native American performers young and old, ranging from pow-wow drum music to hard-driving rock-and-roll and rap. In the International world of ballet dancing Maria Tallchief was considered America's first major prima ballerina, and was the first person of Native American descent to hold the rank. along with her sister Marjorie Tallchief both became star ballerinas.\n\nThe most widely practiced public musical form among Native Americans in the United States is that of the pow-wow. At pow-wows, such as the annual Gathering of Nations in Albuquerque, New Mexico, members of drum groups sit in a circle around a large drum. Drum groups play in unison while they sing in a native language and dancers in colorful regalia dance clockwise around the drum groups in the center. Familiar pow-wow songs include honor songs, intertribal songs, crow-hops, sneak-up songs, grass-dances, two-steps, welcome songs, going-home songs, and war songs. Most indigenous communities in the United States also maintain traditional songs and ceremonies, some of which are shared and practiced exclusively within the community.\n\nThe Iroquois, living around the Great Lakes and extending east and north, used strings or belts called \"wampum\" that served a dual function: the knots and beaded designs mnemonically chronicled tribal stories and legends, and further served as a medium of exchange and a unit of measure. The keepers of the articles were seen as tribal dignitaries.\n\nPueblo peoples crafted impressive items associated with their religious ceremonies. \"Kachina\" dancers wore elaborately painted and decorated masks as they ritually impersonated various ancestral spirits. \nPueblo people are particularly noted for their traditional high-quality pottery, often with geometric designs and floral, animal and bird motifs. Sculpture was not highly developed, but carved stone and wood fetishes were made for religious use. Superior weaving, embroidered decorations, and rich dyes characterized the textile arts. Both turquoise and shell jewelry were created, as were formalized pictorial arts. \n\nNavajo spirituality focused on the maintenance of a harmonious relationship with the spirit world, often achieved by ceremonial acts, usually incorporating sandpainting. For the Navajo the sand painting is not merely a representational object, but a dynamic spiritual entity with a life of its own, which helped the patient at the centre of the ceremony re-establish a connection with the life force. These vivid, intricate, and colorful sand creations were erased at the end of the healing ceremony. \n\nThe Native American arts and crafts industry brings in more than a billion in gross sales annually.\n\nNative American art comprises a major category in the world art collection. Native American contributions include pottery, paintings, jewellery, weavings, sculpture, basketry, and carvings. Franklin Gritts was a Cherokee artist who taught students from many tribes at Haskell Institute (now Haskell Indian Nations University) in the 1940s, the \"Golden Age\" of Native American painters. The integrity of certain Native American artworks is protected by the Indian Arts and Crafts Act of 1990, that prohibits representation of art as Native American when it is not the product of an enrolled Native American artist. Attorney Gail Sheffield and others claim that this law has had \"the unintended consequence of sanctioning discrimination against Native Americans whose tribal affiliation was not officially recognized\". Native artists such as Jeanne Rorex Bridges (Cherokee) who are not enrolled run the risk of fines or imprisonment if they continue to sell their art while affirming their Indian heritage.\n\nThe Inuit, or Eskimo, prepared and buried large amounts of dried meat and fish. Pacific Northwest tribes crafted seafaring dugouts long for fishing. Farmers in the Eastern Woodlands tended fields of maize with hoes and digging sticks, while their neighbors in the Southeast grew tobacco as well as food crops. On the Plains, some tribes engaged in agriculture but also planned buffalo hunts in which herds were driven over bluffs.\n\nDwellers of the Southwest deserts hunted small animals and gathered acorns to grind into flour with which they baked wafer-thin bread on top of heated stones. Some groups on the region's mesas developed irrigation techniques, and filled storehouses with grain as protection against the area's frequent droughts.\n\nIn the early years, as these native peoples encountered European explorers and settlers and engaged in trade, they exchanged food, crafts, and furs for blankets, iron and steel implements, horses, trinkets, firearms, and alcoholic beverages.\n\nInterracial relations between Native Americans, Europeans, and Africans is a complex issue that has been mostly neglected with \"few in-depth studies on interracial relationships\". Some of the first documented cases of European/Native American intermarriage and contact were recorded in Post-Columbian Mexico. One case is that of Gonzalo Guerrero, a European from Spain, who was shipwrecked along the Yucatan Peninsula, and fathered three Mestizo children with a Mayan noblewoman. Another is the case of Hernán Cortés and his mistress La Malinche, who gave birth to another of the first multi-racial people in the Americas.\n\nEuropean impact was immediate, widespread, and profound already during the early years of colonization and nationhood. Europeans living among Native Americans were often called \"white indians\". They \"lived in native communities for years, learned native languages fluently, attended native councils, and often fought alongside their native companions\".\n\nEarly contact was often charged with tension and emotion, but also had moments of friendship, cooperation, and intimacy. Marriages took place in English, Spanish, and French colonies between Native Americans and Europeans though Native American women were also the victims of rape. Given the preponderance of men among the colonists in the early years, generally European men tried to turn to Native American women for sexual relationships either through marriage, informal relationships, or rape.\nThere was fear on both sides, as the different peoples realized how different their societies were. The whites regarded the Indians as \"savage\" because they were not Christian. They were suspicious of cultures which they did not understand. The Native American author, Andrew J. Blackbird, wrote in his \"History of the Ottawa and Chippewa Indians of Michigan\" (1897), that white settlers introduced some immoralities into Native American tribes. Many Indians suffered because the Europeans introduced alcohol and the whiskey trade resulted in alcoholism among the people, who were alcohol-intolerant.\n\nBlackbird wrote:\n\nThe U.S. government had two purposes when making land agreements with Native Americans: to open it up more land for white settlement, and to ease tensions between whites and Native Americans by forcing the Native Americans to use the land in the same way as did the whites—for subsistence farms. The government used a variety of strategies to achieve these goals; many treaties required Native Americans to become farmers in order to keep their land. Government officials often did not translate the documents which Native Americans were forced to sign, and native chiefs often had little or no idea what they were signing.\nFor a Native American man to marry a white woman, he had to get consent of her parents, as long as \"he can prove to support her as a white woman in a good home\". In the early 19th century, the Shawnee Tecumseh and blonde hair, blue-eyed Rebbecca Galloway had an interracial affair. In the late 19th century, three European-American middle-class women teachers at Hampton Institute married Native American men whom they had met as students.\n\nAs European-American women started working independently at missions and Indian schools in the western states, there were more opportunities for their meeting and developing relationships with Native American men. For instance, Charles Eastman, a man of European and Lakota descent whose father sent both his sons to Dartmouth College, got his medical degree at Boston University and returned to the West to practice. He married Elaine Goodale, whom he met in South Dakota. He was the grandson of Seth Eastman, a military officer from Maine, and a chief's daughter. Goodale was a young European-American teacher from Massachusetts and a reformer, who was appointed as the U.S. superintendent of Native American education for the reservations in the Dakota Territory. They had six children together.\n\nThe majority of Native American tribes did practice some form of slavery before the European introduction of African slavery into North America, but none exploited slave labor on a large scale. Most Native American tribes did not barter captives in the pre-colonial era, although they sometimes exchanged enslaved individuals with other tribes in peace gestures or in exchange for their own members. When Europeans arrived as colonists in North America, Native Americans changed their practice of slavery dramatically. Native Americans began selling war captives to Europeans rather than integrating them into their own societies as they had done before. As the demand for labor in the West Indies grew with the cultivation of sugar cane, Europeans enslaved Native Americans for the Thirteen Colonies, and some were exported to the \"sugar islands\". The British settlers, especially those in the southern colonies, purchased or captured Native Americans to use as forced labor in cultivating tobacco, rice, and indigo. Accurate records of the numbers enslaved do not exist because vital statistics and census reports were at best infrequent.. Scholars estimate tens to hundreds of thousands of Native Americans may have been enslaved by the Europeans, being sold by Native Americans themselves or Europeans. \nSlaves became a caste of people who were foreign to the English (Native Americans, Africans and their descendants) and non-Christians. The Virginia General Assembly defined some terms of slavery in 1705:\n\nThe slave trade of Native Americans lasted only until around 1750. It gave rise to a series of devastating wars among the tribes, including the Yamasee War. The Indian Wars of the early 18th century, combined with the increasing importation of African slaves, effectively ended the Native American slave trade by 1750. Colonists found that Native American slaves could easily escape, as they knew the country. The wars cost the lives of numerous colonial slave traders and disrupted their early societies. The remaining Native American groups banded together to face the Europeans from a position of strength. Many surviving Native American peoples of the southeast strengthened their loose coalitions of language groups and joined confederacies such as the Choctaw, the Creek, and the Catawba for protection. Even after the Indian Slave Trade ended in 1750 the enslavement of Native Americans continued in the west, and also in the Southern states mostly through kidnappings.\n\nNative American women were at risk for rape whether they were enslaved or not; during the early colonial years, settlers were disproportionately male. They turned to Native women for sexual relationships. Both Native American and African enslaved women suffered rape and sexual harassment by male slaveholders and other white men.\n\nAfrican and Native Americans have interacted for centuries. The earliest record of Native American and African contact occurred in April 1502, when Spanish colonists transported the first Africans to Hispaniola to serve as slaves.\nSometimes Native Americans resented the presence of African Americans. The \"Catawaba tribe in 1752 showed great anger and bitter resentment when an African American came among them as a trader\". To gain favor with Europeans, the Cherokee exhibited the strongest color prejudice of all Native Americans. Because of European fears of a unified revolt of Native Americans and African Americans, the colonists tried to encourage hostility between the ethnic groups: \"Whites sought to convince Native Americans that African Americans worked against their best interests.\" In 1751, South Carolina law stated:\n\nIn addition, in 1758 the governor of South Carolina James Glen wrote:\n\nEuropeans considered both races inferior and made efforts to make both Native Americans and Africans enemies. Native Americans were rewarded if they returned escaped slaves, and African Americans were rewarded for fighting in the late 19th-century Indian Wars.\n\n\"Native Americans, during the transitional period of Africans becoming the primary race enslaved, were enslaved at the same time and shared a common experience of enslavement. They worked together, lived together in communal quarters, produced collective recipes for food, shared herbal remedies, myths and legends, and in the end they intermarried.\" Because of a shortage of men due to warfare, many tribes encouraged marriage between the two groups, to create stronger, healthier children from the unions.\n\nIn the 18th century, many Native American women married freed or runaway African men due to a decrease in the population of men in Native American villages. Records show that many Native American women bought African men but, unknown to the European sellers, the women freed and married the men into their tribe. When African men married or had children by a Native American woman, their children were born free, because the mother was free (according to the principle of \"partus sequitur ventrem\", which the colonists incorporated into law).\n\nWhile numerous tribes used captive enemies as servants and slaves, they also often adopted younger captives into their tribes to replace members who had died. In the Southeast, a few Native American tribes began to adopt a slavery system similar to that of the American colonists, buying African American slaves, especially the Cherokee, Choctaw, and Creek. Though less than 3% of Native Americans owned slaves, divisions grew among the Native Americans over slavery. Among the Cherokee, records show that slave holders in the tribe were largely the children of European men that had shown their children the economics of slavery. As European colonists took slaves into frontier areas, there were more opportunities for relationships between African and Native American peoples.\n\nIn the 2010 Census, nearly 3 million people indicated that their race was Native American (including Alaska Native). Of these, more than 27% specifically indicated \"Cherokee\" as their ethnic origin. Many of the First Families of Virginia claim descent from Pocahontas or some other \"Indian princess\". This phenomenon has been dubbed the \"Cherokee Syndrome\". Across the US, numerous individuals cultivate an opportunistic ethnic identity as Native American, sometimes through Cherokee heritage groups or Indian Wedding Blessings.\n\nMany tribes, especially those in the Eastern United States, are primarily made up of individuals with an unambiguous Native American identity, despite being predominantly of European ancestry. More than 75% of those enrolled in the Cherokee Nation have less than one-quarter Cherokee blood, and the current Principal Chief of the Cherokee Nation, Bill John Baker, is 1/32 Cherokee, amounting to about 3%.\n\nHistorically, numerous Native Americans assimilated into colonial and later American society, e.g. through adopting English and converting to Christianity. In many cases, this process occurred through forced assimilation of children sent off to special boarding schools far from their families. Those who could pass for white had the advantage of white privilege Today, after generations of racial whitening through hypergamy, many Native Americans are visually indistinguishable from White Americans, unlike mestizos in the United States, who may in fact have little or no non-indigenous ancestry.\n\nNative Americans are more likely than any other racial group to practice racial exogamy, resulting in an ever-declining proportion of indigenous blood among those who claim a Native American identity. Some tribes will even resort to disenrollment of tribal members unable to provide scientific \"proof\" of Native ancestry, usually through a Certificate of Degree of Indian Blood. Disenrollment has become a contentious issue in Native American reservation politics.\n\nIntertribal mixing was common among many Native American tribes prior to European contact, as they would adopt captives taken in warfare. Individuals often had ancestry from more than one tribe, particularly after tribes lost so many members from disease in the colonial era and after. Bands or entire tribes occasionally split or merged to form more viable groups in reaction to the pressures of climate, disease and warfare.\n\nA number of tribes traditionally adopted captives into their group to replace members who had been captured or killed in battle. Such captives were from rival tribes and later were taken from raids on European settlements. Some tribes also sheltered or adopted white traders and runaway slaves, and others owned slaves of their own. Tribes with long trading histories with Europeans show a higher rate of European admixture, reflecting years of intermarriage between Native American women and European men, often seen as advantageous to both sides. A number of paths to genetic and ethnic diversity among Native Americans have occurred.\n\nIn recent years, genetic genealogists have been able to determine the proportion of Native American ancestry carried by the African-American population. The literary and history scholar Henry Louis Gates, Jr., had experts on his TV programs who discussed African-American ancestry. They stated that 5% of African Americans have at least 12.5% Native American ancestry, or the equivalent to one great-grandparent, which may represent more than one distant ancestor. A greater percentage could have a smaller proportion of Indian ancestry, but their conclusions show that popular estimates of Native American admixture may have been too high. More recent genetic testing research of 2015, have found varied ancestries which show different tendencies by region and sex of ancestors. Though DNA testing is limited these studies found that on average, African Americans have 73.2-82.1% West African, 16.7%-29% European, and 0.8–2% Native American genetic ancestry, with large variation between individuals.\n\nDNA testing is not sufficient to qualify a person for specific tribal membership, as it cannot distinguish among Native American tribes; however some tribes such as the Meskwaki Nation require a DNA test in order to enroll in the tribe.\n\nNative American identity has historically been based on culture, not just biology, as many American Indian peoples adopted captives from their enemies and assimilated them into their tribes. The Indigenous Peoples Council on Biocolonialism (IPCB) notes that:\n\n\"Native American markers\" are not found solely among Native Americans. While they occur more frequently among Native Americans, they are also found in people in other parts of the world.\n\nGeneticists state:\n\nNot all Native Americans have been tested; especially with the large number of deaths due to disease such as smallpox, it is unlikely that Native Americans only have the genetic markers they have identified [so far], even when their maternal or paternal bloodline does not include a [known] non-Native American.\n\nTo receive tribal services, a Native American must be a certified (or enrolled) member of a federally recognized tribal organization. Each tribal government makes its own rules for eligibility of citizens or tribal members. Among tribes, qualification for enrollment may be based upon a required percentage of Native American \"blood\" (or the \"blood quantum\") of an individual seeking recognition, or documented descent from an ancestor on the Dawes Rolls or other registers. But, the federal government has its own standards related to who qualifies for services available to certified Native Americans. For instance, federal scholarships for Native Americans require the student both to be enrolled in a federally recognized tribe \"and\" to be of at least one-quarter Native American descent (equivalent to one grandparent), attested to by a Certificate of Degree of Indian Blood (CDIB) card issued by the federal government.\n\nSome tribes have begun requiring genealogical DNA testing of individuals' applying for membership, but this is usually related to an individual's proving parentage or direct descent from a certified member. Requirements for tribal membership vary widely by tribe. The Cherokee require documented direct genealogical descent from a Native American listed on the early 1906 Dawes Rolls. Tribal rules regarding recognition of members who have heritage from multiple tribes are equally diverse and complex.\n\nTribal membership conflicts have led to a number of legal disputes, court cases, and the formation of activist groups. One example of this are the Cherokee Freedmen. Today, they include descendants of African Americans once enslaved by the Cherokees, who were granted, by federal treaty, citizenship in the historic Cherokee Nation as freedmen after the Civil War. The modern Cherokee Nation, in the early 1980s, passed a law to require that all members must prove descent from a Cherokee Native American (not Cherokee Freedmen) listed on the Dawes Rolls, resulting in the exclusion of some individuals and families who had been active in Cherokee culture for years.\n\nSince the census of 2000, people may identify as being of more than one race. Since the 1960s, the number of people claiming Native American ancestry has grown significantly and by the 2000 census, the number had more than doubled. Sociologists attribute this dramatic change to \"ethnic shifting\" or \"ethnic shopping\"; they believe that it reflects a willingness of people to question their birth identities and adopt new ethnicities which they find more compatible.\n\nThe author Jack Hitt writes:\n\nThe journalist Mary Annette Pember notes that identifying with Native American culture may be a result of a person's increased interest in genealogy, the romanticization of the lifestyle, and a family tradition of Native American ancestors in the distant past. There are different issues if a person wants to pursue enrollment as a member of a tribe. Different tribes have different requirements for tribal membership; in some cases persons are reluctant to enroll, seeing it as a method of control initiated by the federal government; and there are individuals who are 100% Native American but, because of their mixed tribal heritage, do not qualify to belong to any individual tribe. Pember concludes:\n\nThe genetic history of indigenous peoples of the Americas primarily focuses on human Y-chromosome DNA haplogroups and human mitochondrial DNA haplogroups. \"Y-DNA\" is passed solely along the patrilineal line, from father to son, while \"mtDNA\" is passed down the matrilineal line, from mother to offspring of both sexes. Neither recombines, and thus Y-DNA and mtDNA change only by chance mutation at each generation with no intermixture between parents' genetic material. Autosomal \"atDNA\" markers are also used, but differ from mtDNA or Y-DNA in that they overlap significantly. Autosomal DNA is generally used to measure the average continent-of-ancestry genetic admixture in the entire human genome and related isolated populations.\n\nThe genetic pattern indicates Indigenous Americans experienced two very distinctive genetic episodes; first with the initial-peopling of the Americas, and secondly with European colonization of the Americas. The former is the determinant factor for the number of gene lineages, zygosity mutations and founding haplotypes present in today's Indigenous Amerindian populations.\n\nHuman settlement of the New World occurred in stages from the Bering sea coast line, with an initial 15,000 to 20,000-year layover on Beringia for the small founding population. The micro-satellite diversity and distributions of the Y lineage specific to South America indicates that certain Amerindian populations have been isolated since the initial colonization of the region. The Na-Dené, Inuit and Indigenous Alaskan populations exhibit haplogroup Q-M242 (Y-DNA) mutations, however, that are distinct from other indigenous Amerindians, and that have various mtDNA and atDNA mutations. This suggests that the paleo-Indian migrants into the northern extremes of North America and Greenland were descended from a later, independent migrant population.\n\nGovernment\n\nOrganizations and media\n\nAcademic collections and other resources\n"}
{"id": "21218", "url": "https://en.wikipedia.org/wiki?curid=21218", "title": "Nights into Dreams", "text": "Nights into Dreams\n\nDevelopment began after the release of \"Sonic & Knuckles\" in 1994, although the concept originated during the development of \"Sonic the Hedgehog 2\" two years prior. Development was led by Sonic Team veterans Yuji Naka, Naoto Ohshima, and Takashi Iizuka. Naka began the project with the main idea revolving around flight, and Ohshima designed the character Nights to resemble an angel that could fly like a bird. Ohshima designed Nights as an androgynous character. The team conducted research on dreaming and REM sleep, and was influenced by the works and theories of psychoanalysts Carl Jung and Sigmund Freud. An analogue controller, the Saturn 3D controller, was designed alongside the game and included with some retail copies.\n\n\"Nights into Dreams\" received acclaim for its graphics, gameplay, soundtrack, and atmosphere. It has appeared on several lists of the greatest games of all time. An abbreviated Christmas-themed version, \"Christmas Nights\", was released in December 1996. The game was ported to the PlayStation 2 in 2008 in Japan and a high-definition version was released worldwide for PlayStation 3, Xbox 360, and Windows in 2012. A sequel, \"\", was released for the Wii in 2007.\n\n\"Nights into Dreams\" is split into seven levels, referred to as \"Dreams\". The levels are distributed between the two teenage characters: three are unique to Claris, three to Elliot, and each play through an identical final seventh level, \"Twin Seeds\". Initially, only Claris' \"Spring Valley\" and Elliot's \"Splash Garden\" levels are available, and successful completion of one of these unlocks the next level in that character's path. Previously completed stages may be revisited to improve the player's high scores; a grade between A and F is given to the player upon completion, but a \"C\" grade (or better) in all the selected character's levels must be achieved to unlock the relevant \"Twin Seeds\" stage for that character. Points are accumulated depending on how fast the player completes a level, and extra points are awarded when the player flies through rings.\n\nEach level is split up into four \"Mares\" set in Nightopia and a boss fight which takes place in Nightmare. In each level, players initially control Claris or Elliot, who immediately have their Ideyas (spherical objects that contain emotions) of hope, wisdom, intelligence and purity stolen from them by Wizeman's minions, leaving behind only their Ideya of courage. The goal of each Mare is to recover one of the stolen Ideya by collecting 20 blue chips and delivering them to the cage holding the Ideyas, which overloads and releases the orb it holds. If the player walks around the landscape for too long, they are pursued by a sentient alarm clock which awakens the character and end the level if it comes into contact with the player. The majority of the gameplay centres on flying sequences, which are triggered by walking into the Ideya Palace near the start of each level so that the character merges with the imprisoned Nights. Once the flying sequence is initiated, the time limit begins.\n\nIn the flying sections, the player controls Nights' flight along a predetermined route through each Mare, resembling that of a 2D platformer. The player has only a limited period of time available before Nights falls to the ground and transforms back into Claris or Elliot, and each collision with an enemy subtracts five seconds from the time remaining. The player's time is replenished each time they return an Ideya to the Ideya Palace. While flying, Nights can use a \"Drill Dash\" to travel faster, as well as defeat certain reverie enemies scattered throughout the level. Grabbing onto certain enemies causes Nights to spin around, which launches both Nights and the enemy in the direction the boost was initiated. Various acrobatic manoeuvres can be performed, including the \"Paraloop\", whereby flying around in a complete circle and connecting the trail of stars left in Nights' wake causes any items within the loop to be attracted towards Nights. The game features a combo system known as \"Linking\", whereby actions such as collecting items and flying through rings are worth more points when performed in quick succession. Power-ups may be gained by flying through several predetermined rings, indicated by a bonus barrel. The power-ups include a speed boost, point multiplier and an air pocket.\n\nThe player receives a grade based on their score at the end of each Mare, and an overall grade for the level after clearing all four Mares. Nights is then transported to Nightmare for a boss fight against one of Wizeman's \"Level Two\" Nightmarens. Each boss fight has a time limit, and the game ends if the player runs out of time during the battle. Upon winning the boss fight, the player is awarded a score multiplier based on how quickly the boss was defeated, which is then applied to the score earned in the Nightopia section to produce the player's final score for that Dream. The game also features a multiplayer mode, which allows two players to battle each other by using a splitscreen. One player controls Nights, whereas the other controls Reala. The winner is determined by the first player to defeat the other, which is accomplished by hitting or paralooping the other player three times.\n\nThe game features an artificial life system known as \"A-Life\", which involves entities called Nightopians and keeps track of their moods. It is possible to have them mate with other Nightopians, which creates hybrids known as \"Superpians\". The more the game is played, the more inhabitants appear, and environmental features and aesthetics change. The A-Life system features an evolving music engine, allowing tempo, pitch, and melody to alter depending on the state of Nightopians within the level. The feature runs from the Sega Saturn's internal clock, which alters features in the A-Life system depending on the time.\n\nEvery night, all human dreams are played out in Nightopia and Nightmare, the two parts of the dream world. In Nightopia, distinct aspects of dreamers' personalities are represented by luminous coloured spheres known as \"Ideya\". The evil ruler of Nightmare, Wizeman the Wicked, is stealing this dream energy from sleeping visitors in order to gather power and take control of Nightopia and eventually the real world. To achieve this, he creates five beings called \"Nightmaren\": jester-like, flight-capable beings, which include Jackle, Clawz, Gulpo, Gillwing and Puffy as well as many minor maren. He also creates two \"Level One\" Nightmaren: Nights and Reala. However, Nights rebels against Wizeman's plans, and is punished by being imprisoned inside an Ideya palace, a container for dreamers' Ideya.\n\nOne day, Elliot Edwards and Claris Sinclair, two teenagers from the city of Twin Seeds, go through failures. Elliot is a basketball player who enjoys a game with his friends. He is challenged by a group of older school students and suffers a humiliating defeat on the court. Claris is a talented singer and her ambition is to perform on stage. She auditions for a part in the events commemorating the centenary of the city of Twin Seeds. Standing in front of the judges, she is overcome by stage fright and does not perform well, which causes her to lose all hopes of getting the role. When they go to sleep that night, both Elliot and Claris suffer nightmares that replay the events. They escape into Nightopia and find that they both possess the rare Red Ideya of Courage, the only type that Wizeman cannot steal.\n\nOnce in Nightopia, they discover and release Nights, who tells them about dreams and Wizeman and his plans; the three begin a journey to stop Wizeman and restore peace to Nightopia. When they defeat Wizeman and Reala, peace is returned to Nightopia and the world of Nightmare is suppressed. The next day, back in Twin Seeds, a centenary ceremony begins. Elliot is seen walking through the parade until he has a vision of Nights looking at him through a billboard. Realizing that Claris is performing in a hall, Elliot runs through the crowd and sees Claris on stage in front of a large audience, singing well. The two look at each other, and are transitioned to a spring valley in Nightopia, which leaves ambiguity as to whether what they achieved was real or just a dream.\n\nThe concept for \"Nights\" originated during the development of \"Sonic the Hedgehog 2\" in 1992, but development did not begin until after the release of \"Sonic & Knuckles\" in late 1994. Programming began in April 1995 and total development spanned six months. The development team consisted of staff who worked on previous \"Sonic the Hedgehog\" games: Yuji Naka was lead programmer and producer, while Naoto Ohshima and Takashi Iizuka were director and lead designer, respectively. Naka and Ohshima felt they had spent enough time with the \"Sonic\" franchise and were eager to work on new concepts. According to Naka, the initial development team consisted of seven people, and grew to 20 as programmers arrived.\n\n\"Sonic the Hedgehog\" creator and project director Ohshima created the character of Nights based on his inspirations from travelling around Europe and western Asia. He came to the conclusion that the character should resemble an angel and fly like a bird. Naka originally intended to make \"Nights into Dreams\" a slow-paced game, but as development progressed the gameplay pace gradually increased, in similar vein to \"Sonic\" games. The initial concept envisioned the flying character in a rendered 2D sprite art, with side-scrolling features similar to \"Sonic the Hedgehog\". The team were hesitant to switch the game from 2D to 3D, as Naka was sceptical that appealing characters could be created with polygons, in contrast to traditional pixel sprites, which Sonic Team's designers found \"more expressive\". According to Izuka, the game design and story took two years to finalise. The game's difficulty was designed with the intent that young and inexperienced players would be able to complete the game, while more experienced players would be compelled by the replay value.\n\nThe game was developed using Silicon Graphics workstations for graphical designs and Sega Saturn emulators running on Hewlett-Packard machines for programming. There were problems during early stages of development because of a lack of games to use as reference; the team had to redesign the Spring Valley level numerous times and build \"everything from scratch\". The team used the Sega Graphics Library operating system, said by many developers to make programming for the Saturn dramatically easier, only sparingly, instead creating the game almost entirely with custom libraries. Because the Sonic Team offices did not include soundproof studios, team members recorded sound effects at night. According to Naka, every phrase in the game has a meaning; for example, \"abayo\" is Japanese slang for \"goodbye\". The team felt that the global market would be less resistant to a game featuring full 3D CGI cut scenes than 2D anime. Norihiro Nishiyama, the designer of the in-game movies, felt that the 3D cutscenes were a good method to show the different concepts of dreaming and waking up. Naka said that the movies incorporate realism to make it more difficult for the player to disambiguate the boundary between dreams and reality.\n\nThe development took longer than expected because of the team's inexperience with Saturn hardware and uncertainty about using the full 560 megabyte space on the CD-ROM. The team initially thought that the game would consume around 100 megabytes of data, and at one point considered releasing it on two separate discs. Iizuka said that the most difficult part of development was finding a way of handling the \"contradiction\" of using 2D sidescroller controls in a fully 3D game. Naka limited the game's flying mechanic to \"invisible 2D tracks\" because early beta testing revealed that the game was too difficult to play in full 3D. The standard Saturn gamepad was found to be insufficient to control Nights in flight, so the team developed the Saturn analog controller to be used with the game. It took about six months to develop, and the team went through a large number of ideas for alternate controllers, including one shaped like a Nights doll.\n\nIizuka said that the game was inspired by anime and Cirque du Soleil's \"Mystère\" theatrical performance. The development team researched dream sequences and REM sleep, including the works of the psychoanalysts Carl Jung, Sigmund Freud and Friedrich Holtz. Iizuka analysed Jung's theories of dream archetypes and spent a considerable amount of time studying dreams and theories associated with them. Naka said that the main protagonist, Nights, is reflective of Jung's analytical \"shadow\" theory, whereas the two central characters, Claris and Elliot, were inspired by Jung's animus and anima.\n\n\"Nights into Dreams\" was introduced alongside an optional gamepad, the Saturn 3D controller, included with some copies of the game. It features an analogue stick and analogue triggers designed specifically for the game to make movement easier. Sonic Team noted the successful twinning of the Nintendo 64 controller with \"Super Mario 64\" (1996), and realised that the default Saturn controller was better suited to arcade games than \"Nights into Dreams\". During development, director Steven Spielberg visited the Sonic Team studio and became the first person outside the team to play the game. Naka asked him to use an experimental version of the Saturn 3D controller, and it was jokingly referred to as the \"Spielberg controller\" throughout development. The game was marketed with a budget of $10 million, which included television and print advertisements in the United States. In the US, the game was advertised with the slogan \"Prepare to fly.\"\n\n, or Christmas NiGHTS into Dreams..., is a Christmas-themed two-level game of \"Nights into Dreams\" that was released in December 1996. It was introduced in Japan as part of a Christmas Sega Saturn bundle; elsewhere it was given away with the purchase of Saturn games such as \"Daytona USA Championship Circuit Edition\" (1996) or issues of \"Sega Saturn Magazine\" and \"Next Generation Magazine\". In the United Kingdom, \"Christmas Nights\" was not included with the \"Sega Saturn Magazine\" until December 1997. In a 2007 interview, Iizuka stated that \"Christmas Nights\" was created to increase Saturn console sales. Development began in July 1996 and took three to four months, according to Naka.\n\n\"Christmas Nights\" follows Elliot and Claris during the holiday season following their adventures with Nights. Realizing that the Christmas Star is missing from the Twin Seeds Christmas tree, the pair travel to Nightopia to find it, where they reunite with Nights and retrieve the Christmas Star from Gillwing's lair.\n\n\"Christmas Nights\" contains the full version of Claris' Spring Valley dream level from \"Nights into Dreams\", playable as both Claris and Elliot. The Saturn's internal clock changes elements according to the date and time: December activates \"Christmas Nights\" mode, replacing item boxes with Christmas presents, greenery with snow and gumdrops, rings with wreaths, and Ideya captures with Christmas trees; Nightopians wear elf costumes, and the music is replaced with a rendition of \"Jingle Bells\" and an a cappella version of the \"Nights\" theme song. During the \"Winter Nights\" period, the Spring Valley weather changes according to the hour. Other changes apply on New Year's Day; on April Fool's Day, Reala replaces Nights as the playable character.\n\nThe game features several unlockable bonuses, such as being able to play the game's soundtrack, observe the status of the A-life system, experiment with the game's music mixer, time attack one Mare, or play as Sega's mascot Sonic the Hedgehog in the minigame \"Sonic the Hedgehog: Into Dreams.\" Sonic may only play through Spring Valley on foot, and must defeat the boss: an inflatable Dr. Robotnik. The music is a remixed version of \"Final Fever\", the final boss battle music from the Japanese and European version of \"Sonic CD\" (1993). In the HD version of \"Nights\", the \"Christmas Nights\" content is playable after the game has been cleared once.\n\nA game with the working title \"\"Air Nights\"\" was prototyped for the Saturn and began development for the Dreamcast, and in an August 1999 interview Yuji Naka confirmed that a sequel was in development; by December 2000, however, the project had been cancelled. Naka expressed reluctance to develop a sequel, but subsequently noted that he was interested in using \"Nights into Dreams\" as a licence \"to reinforce Sega's identity\". Aside from a handheld electronic game released by Tiger Electronics and small minigames featured in several Sega games, no full sequel was released for a Sega console.\n\nOn 1 April 2007, a sequel called \"\" was officially announced for the Wii. The game was first previewed on Portuguese publication \"Maxi Consolas\", after the release of short reveals from the \"Official Nintendo Magazine\" and \"Game Reactor\". The sequel is a Wii exclusive, making use of the Wii Remote. The gameplay involves the use of various masks, and features a multiplayer mode for two players in addition to Nintendo Wi-Fi Connection online functions. The game was developed by Sega Studio USA, with Iizuka, one of the designers of the original game, serving as producer. It was released in Japan and the United States in December 2007, and in Europe and Australia on 18 January 2008.\n\nIn 2010, Iizuka said that he would be interested in making a third \"Nights into Dreams\" game, should the management of Sega decide to commission one.\n\n\"Nights into Dreams\" has an average score of 89 percent at GameRankings, based on an aggregate of nine reviews. In Japan, \"Nights into Dreams\" was the bestselling Saturn game and the 21st-bestselling game of 1996.\n\nThe graphics and flight mechanics were the most praised aspects. Tom Guise from \"Computer and Video Games\" heralded the game's flight system and freedom as captivating and stated that \"Nights into Dreams\" is the \"perfect evolution\" of a \"Sonic\" game. Scary Larry of \"GamePro\" said flying using the analogue joystick \"is a breeze\" and that the gameplay is fun, enjoyable, and impressive. He gave it a 4.5 out of 5 for graphics and a 5 out of 5 in every other category (sound, control, and FunFactor). \"Entertainment Weekly\" said its \"graceful acrobatic stunts\" offer \"a more compelling sensation of soaring than most flight simulators\". \"Edge\" praised the game's analogue controller and called the levels \"well-designed and graphically unrivaled\", but the reviewer expressed disappointment in the limited level count compared to \"Super Mario 64\", and suggested that \"Nights\" seemed to prioritise technical achievements and Saturn selling points over gameplay with as clear a focus as \"Sonic\". Martin Robinson from Eurogamer opined that the flight mechanics were a \"giddy thrill\". Colin Ferris from Game Revolution praised the graphics and speed of the game as breathtaking and awe-inspiring, concluding that it offered the best qualities of the fifth-generation machines. \"GameFan\" praised the combination of \"lush graphics, amazing music, and totally unique gameplay\". \"Next Generation\" criticised the speed, saying that the only disappointing aspect was the way \"it all rushes by so fast\". However, the magazine praised the two-player mode and the innovative method of grading the player once they completed a level. \"Electronic Gaming Monthly\"s four reviewers were impressed with both the technical aspects and style of the graphics, and said the levels are great fun to explore, though they expressed disappointment that the game was not genuinely 3D and said it did not manage to surpass \"Super Mario 64\".\n\nLevi Buchanan from IGN believed that the console \"was not built to handle \"Nights\"\" due to the game occasionally clipping and warping, though he admitted that the graphics were \"pretty darn good\". A reviewer from \"Mean Machines Sega\" praised the game's vibrant colours and detailed textures, and described its animation as being \"fluid as water\". The reviewer also noted occasional pop-in and glitching during the game. Rad Automatic from the British \"Sega Saturn Magazine\" praised the visuals and colour scheme as rich in both texture and detail, while suggesting that \"Nights into Dreams\" is \"one of the most captivating games the Saturn has witnessed yet.\" \"Next Generation\" similarly commended the game's visuals, stating that they were \"beyond a doubt\" the most fluid and satisfying for any game on any system. Upon release, the Japanese \"Sega Saturn Magazine\" opined that the game would have a significant impact on the video game industry, particularly that in the action game genre. The reviewer also stated that the game felt better through the use of the analogue pad, in contrast to the conventional controller, and also praised the light and smooth feeling the analogue pad portrayed during gameplay.\n\nReviewers also praised the game's soundtrack and audio effects. Paul Davies from \"Computer and Video Games\" cited the game as having \"the best music ever\"; in the same review, Tom Guise attributed the music to creating a hypnotically magical atmosphere. Ferris stated that the music and sound effects were that of a dream world, and asserted that they were fitting for a game like \"Nights into Dreams\". IGN's Buchanan praised the game's soundtrack, stating that each stage's soundtrack is \"quite good\" and that the sound effects \"fit in perfectly with the dream universe\".\n\nIn \"Electronic Gaming Monthly\"s \"Best of '96\" awards, \"Nights Into Dreams\" was a runner-up for Flying Game of the Year (behind \"Pilotwings 64\"), Nights was a runner-up for Coolest Mascot (behind Mario), and the Saturn analog controller, which the magazine called the \"Nights Controller\", won Best Peripheral.\n\nSince its release, \"Nights into Dreams\" has appeared on several best game of all time lists. In a January 2000 poll by \"Computer and Video Games\", readers placed the game 15th on their \"100 Greatest Games\" list, directly behind \"Super Mario 64\". IGN ranked the game as the 94th best game of all time in their \"Top 100 Games\" list in 2007, and in 2008, Levi Buchanan ranked it fourth in his list of the top 10 Sega Saturn games. \"Next Generation\" ranked the game 25th in its list of the \"100 Greatest Games of All Time\" in their September 1996 issue (i.e. one month before they actually reviewed the game, and roughly two months before it saw release outside Japan). 1UP ranked the game third in its \"Top Ten Cult Classics\" list. In 2014, GamesRadar listed \"Nights into Dreams\" as the best Sega Saturn game of all time, stating that the game \"tapped into a new kind of platform gameplay for its era\".\n\nSega released a remake of \"Nights into Dreams\" for the PlayStation 2 exclusively in Japan on 21 February 2008. It includes 16:9 wide screen support, an illustration gallery and features the ability to play the game in classic Saturn graphics. The game was also featured in a bundle named the Nightopia Dream Pack, which includes a reprint of a picture book that was released in Japan alongside the original Saturn game. A \"Nights into Dreams\" handheld electronic game was released by Tiger Electronics in 1997, and a port of it was later released for Tiger's unsuccessful R-Zone console.\n\nA high definition remaster of the PlayStation 2 version was released for PlayStation Network on 2 October 2012 and for Xbox Live Arcade on 5 October 2012. A Windows version was released via Steam on 17 December 2012, with online score leaderboards and the option to play with enhanced graphics or with the original Saturn graphics. The HD version also includes \"Christmas Nights,\" but the two-player mode and \"Sonic the Hedgehog\" level were removed.\n\nClaris and Eliot make a cameo appearance in Sonic Team's \"Burning Rangers\" (1998), with both Claris and Eliot sending the Rangers emails thanking them for their help. \"Nights into Dreams\"-themed pinball areas feature in \"Sonic Adventure\" (1998) and \"Sonic Pinball Party\" (2003), with soundtrack being featured in the latter game. The PlayStation 2 games \"\" (2003) and \"Sega SuperStars\" (2004) both feature minigames based on \"Nights into Dreams\", in which Nights is controlled using the player's body. Nights is also an unlockable character in \"Sonic Riders\" (2006) and \"\" (2008).\n\nA minigame version of \"Nights into Dreams\" is playable through using the Nintendo GameCube – Game Boy Advance link cable connectivity with \"Phantasy Star Online Episode I & II\" (2000) and \"Billy Hatcher and the Giant Egg\" (2003). Following a successful fan campaign by a \"Nights into Dreams\" fansite, the character Nights was integrated into \"Sonic & Sega All-Stars Racing\" (2010) as a traffic guard. Nights and Reala also appear as playable characters in \"Sega Superstars Tennis\" (2008) and \"Sonic & All-Stars Racing Transformed\" (2012), the latter of which also features a \"Nights into Dreams\"-themed racetrack. The limited Deadly Six edition of \"Sonic Lost World\" (2013) features a \"Nights into Dreams\"-inspired stage, \"Nightmare Zone\", as downloadable content.\n\nIn February 1998, Archie Comics adapted \"Nights into Dreams\" into a three-issue comic book miniseries to test whether a Nights comic would sell well in North America. The first miniseries was loosely based on the game, with Nights identified as male despite the character's androgynous design. The company later released a second three-issue miniseries, continuing the story of the first, but the series did not gain enough sales to warrant an ongoing series. It was later added to a list of guest franchises featured in Archie Comics' \"Worlds Unite\" crossover between its \"Sonic the Hedgehog\" and \"Mega Man\" comics.\n\nCitations\nBibliography\n"}
{"id": "21220", "url": "https://en.wikipedia.org/wiki?curid=21220", "title": "Negligence per se", "text": "Negligence per se\n\nNegligence \"per se\" is a doctrine in US law whereby an act is considered negligent because it violates a statute (or regulation). The doctrine is effectively a form of strict liability.\n\nIn order to prove negligence \"per se\", the plaintiff usually must show that:\n\nIn some jurisdictions, negligence \"per se\" creates merely a rebuttable presumption of negligence.\n\nA typical example is one in which a contractor violates a building code when constructing a house. The house then collapses, injuring somebody. The violation of the building code establishes negligence \"per se\" and the contractor will be found liable, so long as the contractor's breach of the code was the cause (proximate cause and actual cause) of the injury.\n\nA famous early case in negligence \"per se\" is \"Gorris v. Scott\" (1874), a Court of Exchequer case that established that the harm in question must be of the kind that the statute was intended to prevent. \"Gorris\" involved a shipment of sheep that was washed overboard but would not have been washed overboard had the shipowner complied with the regulations established pursuant to the Contagious Diseases (Animals) Act 1869, which required that livestock be transported in pens to segregate potentially-infected animal populations from uninfected ones. Chief Baron Fitzroy Kelly held that as the statute was intended to prevent the spread of disease, rather than the loss of livestock in transit, the plaintiff could not claim negligence \"per se\".\n\nA subsequent New York Court of Appeals case, \"Martin v. Herzog\" (1920), penned by Judge Benjamin N. Cardozo, first presented the notion that negligence \"per se\" could be absolute evidence of negligence in certain cases.\n\nNegligence \"per se\" involves the concept of strict liability. Within the law of negligence there has been a move away from strict liability (as typified by \"Re Polemis\") to a standard of reasonable care (as seen in \"Donoghue v Stevenson\", \"The Wagon Mound (No. 1)\", and \"Hughes v Lord Advocate\"). This is true not just for breach of the common law, but also for breach of statutory duty. The criminal law case of \"Sweet v Parsley\" (which required \"mens rea\" to be read into a criminal statue) follows this trend. In this light, \"negligence \"per se\"\" may be criticised as running counter to the general tendency.\n\n\n"}
{"id": "21221", "url": "https://en.wikipedia.org/wiki?curid=21221", "title": "Neuromyotonia", "text": "Neuromyotonia\n\nNeuromyotonia (NMT) is a form of peripheral nerve hyperexcitability that causes spontaneous muscular activity resulting from repetitive motor unit action potentials of peripheral origin. The prevalence of NMT is unknown but 100–200 cases have been reported so far.\n\nNMT is a diverse disorder. As a result of muscular hyperactivity, patients may present with muscle cramps, stiffness, myotonia-like symptoms (slow relaxation), associated walking difficulties, hyperhidrosis (excessive sweating), myokymia (quivering of a muscle), fasciculations (muscle twitching), fatigue, exercise intolerance, myoclonic jerks and other related symptoms. The symptoms (especially the stiffness and fasciculations) are most prominent in the calves, legs, trunk, and sometimes the face and neck, but can also affect other body parts. NMT symptoms may fluctuate in severity and frequency. Symptoms range from mere inconvenience to debilitating. At least a third of people also experience sensory symptoms.\n\nThe three causes of NMT are:\n\nThe acquired form is the most common, accounting for up to 80 percent of all cases and is suspected to be autoimmune-mediated, which is usually caused by antibodies against the neuromuscular junction.\n\nThe exact cause is unknown. However, autoreactive antibodies can be detected in a variety of peripheral (e.g. myasthenia gravis, Lambert-Eaton myasthenic syndrome) and central nervous system (e.g. paraneoplastic cerebellar degeneration, paraneoplastic limbic encephalitis) disorders. Their causative role has been established in some of these diseases but not all. Neuromyotonia is considered to be one of these with accumulating evidence for autoimmune origin over the last few years. Autoimmune neuromyotonia is typically caused by antibodies that bind to potassium channels on the motor nerve resulting in continuous/hyper-excitability. Onset is typically seen between the ages of 15–60, with most experiencing symptoms before the age of 40. Some neuromyotonia cases do not only improve after plasma exchange but they may also have antibodies in their serum samples against voltage-gated potassium channels. Moreover, these antibodies have been demonstrated to reduce potassium channel function in neuronal cell lines.\n\nDiagnosis is clinical and initially consists of ruling out more common conditions, disorders, and diseases, and usually begins at the general practitioner level. A doctor may conduct a basic neurological exam, including coordination, strength, reflexes, sensation, etc. A doctor may also run a series of tests that include blood work and MRIs.\n\nFrom there, a patient is likely to be referred to a neurologist or a neuromuscular specialist. The neurologist or specialist may run a series of more specialized tests, including needle electromyography EMG/ and nerve conduction studies (NCS) (these are the most important tests), chest CT (to rule out paraneoplastic) and specific blood work looking for voltage-gated potassium channel antibodies, acetylcholine receptor antibody, and serum immunofixation, TSH, ANA ESR, EEG etc. Neuromyotonia is characterized electromyographically by doublet, triplet or multiplet single unit discharges that have a high, irregular intraburst frequency. Fibrillation potentials and fasciculations are often also present with electromyography.\n\nBecause the condition is so rare, it can often be years before a correct diagnosis is made.\n\nNMT is not fatal and many of the symptoms can be controlled. However, because NMT mimics some symptoms of motor neuron disease (ALS) and other more severe diseases, which may be fatal, there can often be significant anxiety until a diagnosis is made. In some rare cases, acquired neuromyotonia has been misdiagnosed as amyotrophic lateral sclerosis (ALS) particularly if fasciculations may be evident in the absence of other clinical features of ALS. However, fasciculations are rarely the first sign of ALS as the hallmark sign is weakness. Similarly, multiple sclerosis has been the initial misdiagnosis in some NMT patients. In order to get an accurate diagnosis see a trained neuromuscular specialist.\nThere are three main types of NMT:\n\nNeuromyotonia is a type of peripheral nerve hyperexcitability. Peripheral nerve hyperexcitability is an umbrella diagnosis that includes (in order of severity of symptoms from least severe to most severe) benign fasciculation syndrome, cramp fasciculation syndrome, neuromyotonia and morvan's syndrome. Some doctors will only give the diagnosis of peripheral nerve hyperexcitability as the differences between the three are largely a matter of the severity of the symptoms and can be subjective. However, some objective EMG criteria have been established to help distinguish between the three.\n\nMoreover, the generic use of the term \"peripheral nerve hyperexcitability syndromes\" to describe the aforementioned conditions is recommended and endorsed by several prominent researchers and practitioners in the field.\n\nThere is no known cure for neuromyotonia, but the condition is treatable. Anticonvulsants, including phenytoin and carbamazepine, usually provide significant relief from the stiffness, muscle spasms, and pain associated with neuromyotonia. Plasma exchange and IVIg treatment may provide short-term relief for patients with some forms of the acquired disorder. It is speculated that the plasma exchange causes an interference with the function of the voltage-dependent potassium channels, one of the underlying issues of hyper-excitability in autoimmune neuromyotonia. Botox injections also provide short-term relief. Immunosuppressants such as Prednisone may provide long term relief for patients with some forms of the acquired disorder.\n\nThe long-term prognosis is uncertain, and has mostly to do with the underlying cause; i.e. autoimmune, paraneoplastic, etc. However, in recent years increased understanding of the basic mechanisms of NMT and autoimmunity has led to the development of novel treatment strategies. NMT disorders are now amenable to treatment and their prognoses are good. Many patients respond well to treatment, which usually provide significant relief of symptoms. Some cases of spontaneous remission have been noted, including Isaac's original two patients when followed up 14 years later.\n\nWhile NMT symptoms may fluctuate, they generally don't deteriorate into anything more serious, and with the correct treatment the symptoms are manageable.\n\nA very small proportion of cases with NMT may develop central nervous system findings in their clinical course, causing a disorder called Morvan's syndrome, and they may also have antibodies against potassium channels in their serum samples. Sleep disorder is only one of a variety of clinical conditions observed in Morvan's syndrome cases ranging from confusion and memory loss to hallucinations and delusions. However, this is a separate disorder.\n\nSome studies have linked NMT with certain types of cancers, mostly lung and thymus, suggesting that NMT may be paraneoplastic in some cases. In these cases, the underlying cancer will determine prognosis. However, most examples of NMT are autoimmune and not associated with cancer.\n\n"}
{"id": "21224", "url": "https://en.wikipedia.org/wiki?curid=21224", "title": "Napoleon (disambiguation)", "text": "Napoleon (disambiguation)\n\nNapoleon (1769–1821) also known as Napoleon Bonaparte or Napoleon I, was a French military leader and emperor.\n\nNapoleon, Napoléon, or Napoleón, or Napoleone may also refer to:\n\n\n\n\n\n\n\n\n"}
{"id": "21226", "url": "https://en.wikipedia.org/wiki?curid=21226", "title": "Neurology", "text": "Neurology\n\nNeurology (from , \"string, nerve\" and the suffix -logia, \"study of\") is a branch of medicine dealing with disorders of the nervous system. Neurology deals with the diagnosis and treatment of all categories of conditions and disease involving the central and peripheral nervous systems (and their subdivisions, the autonomic and somatic nervous systems), including their coverings, blood vessels, and all effector tissue, such as muscle. Neurological practice relies heavily on the field of neuroscience, which is the scientific study of the nervous system.\n\nA neurologist is a physician specializing in neurology and trained to investigate, or diagnose and treat neurological disorders. Neurologists may also be involved in clinical research, clinical trials, and basic or translational research. While neurology is a nonsurgical specialty, its corresponding surgical specialty is neurosurgery.\n\nSignificant overlap occurs between the fields of neurology and psychiatry, with the boundary between the two disciplines and the conditions they treat being somewhat nebulous.\n\nA large number of neurological disorders have been described as listed. These can affect the central nervous system (brain and spinal cord), the peripheral nervous system, the autonomic nervous system, and the muscular system.\n\nThe academic discipline began between the 16th and 19th centuries with the work and research of many neurologists such as Thomas Willis, Robert Whytt, Matthew Baillie, Charles Bell, Moritz Heinrich Romberg, Duchenne de Boulogne, William A. Hammond, Jean-Martin Charcot, and John Hughlings Jackson.\n\n Many neurologists also have additional training or interest in one area of neurology, such as stroke, epilepsy, neuromuscular, sleep medicine, pain management, or movement disorders.\n\nIn the United States and Canada, neurologists are physicians having completed postgraduate training in neurology after graduation from medical school. Neurologists complete, on average, about 8 years of medical college education and clinical training, which includes obtaining a four-year undergraduate degree, a medical degree (DO or MD), which comprises an additional four years of study, then completing one year of basic clinical training and four years of residency. The four-year residency consists of one year of internal medicine internship training followed by three years of training in neurology.\n\nSome neurologists receive additional subspecialty training focusing on a particular area of the field. These training programs are called fellowships, and are one to two years in duration. Subspecialties include brain injury medicine, clinical neurophysiology, epilepsy, hospice and palliative medicine, neurodevelopmental disabilities, neuromuscular medicine, pain medicine, sleep medicine, neurocritical care, vascular neurology (stroke), behavioral neurology, child neurology, headache, multiple sclerosis, neuroimaging, neurorehabilitation.\n\nIn Germany, a compulsory year of psychiatry must be done to complete a residency of neurology.\n\nIn the United Kingdom and Ireland, neurology is a subspecialty of general (internal) medicine. After five to nine years of medical school and a year as a preregistration house officer (or two years on the Foundation Programme), a neurologist must pass the examination for Membership of the Royal College of Physicians (or the Irish equivalent) before completing two years of core medical training and then entering specialist training in neurology. A generation ago, some neurologists would have also spent a couple of years working in psychiatric units and obtain a diploma in psychological medicine. However, this requirement has become uncommon, and, now that a basic psychiatric qualification takes three years to obtain, the requirement is no longer practical. A period of research is essential, and obtaining a higher degree aids career progression. Many found it was eased after an attachment to the Institute of Neurology at Queen Square, London. Some neurologists enter the field of rehabilitation medicine (known as physiatry in the US) to specialise in neurological rehabilitation, which may include stroke medicine, as well as brain injuries.\n\nDuring a neurological examination, the neurologist reviews the patient's health history with special attention to the current condition. The patient then takes a neurological exam. Typically, the exam tests mental status, function of the cranial nerves (including vision), strength, coordination, reflexes, and sensation. This information helps the neurologist determine whether the problem exists in the nervous system and the clinical localization. Localization of the pathology is the key process by which neurologists develop their differential diagnosis. Further tests may be needed to confirm a diagnosis and ultimately guide therapy and appropriate management.\n\nNeurologists examine patients who are referred to them by other physicians in both the inpatient and outpatient settings. Neurologists begin their interactions with patients by taking a comprehensive medical history, and then performing a physical examination focusing on evaluating the nervous system. Components of the neurological examination include assessment of the patient's cognitive function, cranial nerves, motor strength, sensation, reflexes, coordination, and gait.\n\nIn some instances, neurologists may order additional diagnostic tests as part of the evaluation. Commonly employed tests in neurology include imaging studies such as computed axial tomography (CAT) scans, magnetic resonance imaging (MRI), and ultrasound of major blood vessels of the head and neck. Neurophysiologic studies, including electroencephalography (EEG), needle electromyography (EMG), nerve conduction studies (NCSs) and evoked potentials are also commonly ordered. Neurologists frequently perform lumbar punctures to assess characteristics of a patient's cerebrospinal fluid. Advances in genetic testing have made genetic testing an important tool in the classification of inherited neuromuscular disease and diagnosis of many other neurogenetic diseases. The role of genetic influences on the development of acquired neurologic diseases is an active area of research.\n\nSome of the commonly encountered conditions treated by neurologists include headaches, radiculopathy, neuropathy, stroke, dementia, seizures and epilepsy, Alzheimer's disease, attention deficit/hyperactivity disorder, Parkinson's disease, Tourette's syndrome, multiple sclerosis, head trauma, sleep disorders, neuromuscular diseases, and various infections and tumors of the nervous system. Neurologists are also asked to evaluate unresponsive patients on life support to confirm brain death.\n\nTreatment options vary depending on the neurological problem. They can include referring the patient to a physiotherapist, prescribing medications, or recommending a surgical procedure.\n\nSome neurologists specialize in certain parts of the nervous system or in specific procedures. For example, clinical neurophysiologists specialize in the use of EEG and intraoperative monitoring to diagnose certain neurological disorders. Other neurologists specialize in the use of electrodiagnostic medicine studies – needle EMG and NCSs. In the US, physicians do not typically specialize in all the aspects of clinical neurophysiology – i.e. sleep, EEG, EMG, and NCSs. The American Board of Clinical Neurophysiology certifies US physicians in general clinical neurophysiology, epilepsy, and intraoperative monitoring. The American Board of Electrodiagnostic Medicine certifies US physicians in electrodiagnostic medicine and certifies technologists in nerve-conduction studies. Sleep medicine is a subspecialty field in the US under several medical specialties including anesthesiology, internal medicine, family medicine, and neurology. Neurosurgery is a distinct specialty that involves a different training path, and emphasizes the surgical treatment of neurological disorders.\n\nAlso, many nonmedical doctors, those with doctoral degrees(usually PhDs) in subjects such as biology and chemistry, study and research the nervous system. Working in laboratories in universities, hospitals, and private companies, these neuroscientists perform clinical and laboratory experiments and tests to learn more about the nervous system and find cures or new treatments for diseases and disorders.\n\nA great deal of overlap occurs between neuroscience and neurology. Many neurologists work in academic training hospitals, where they conduct research as neuroscientists in addition to treating patients and teaching neurology to medical students.\n\nNeurologists are responsible for the diagnosis, treatment, and management of all the conditions mentioned above. When surgical or endovascular intervention is required, the neurologist may refer the patient to a neurosurgeon or an interventional neuroradiologist. In some countries, additional legal responsibilities of a neurologist may include making a finding of brain death when it is suspected that a patient has died. Neurologists frequently care for people with hereditary (genetic) diseases when the major manifestations are neurological, as is frequently the case. Lumbar punctures are frequently performed by neurologists. Some neurologists may develop an interest in particular subfields, such as stroke, dementia, movement disorders, neurointensive care, headaches, epilepsy, sleep disorders, chronic pain management, multiple sclerosis, or neuromuscular diseases.\n\nSome overlap also occurs with other specialties, varying from country to country and even within a local geographic area. Acute head trauma is most often treated by neurosurgeons, whereas sequelae of head trauma may be treated by neurologists or specialists in rehabilitation medicine. Although stroke cases have been traditionally managed by internal medicine or hospitalists, the emergence of vascular neurology and interventional neuroradiology has created a demand for stroke specialists. The establishment of Joint Commission-certified stroke centers has increased the role of neurologists in stroke care in many primary, as well as tertiary, hospitals. Some cases of nervous system infectious diseases are treated by infectious disease specialists. Most cases of headache are diagnosed and treated primarily by general practitioners, at least the less severe cases. Likewise, most cases of sciatica are treated by general practitioners, though they may be referred to neurologists or surgeons (neurosurgeons or orthopedic surgeons). Sleep disorders are also treated by pulmonologists and psychiatrists. Cerebral palsy is initially treated by pediatricians, but care may be transferred to an adult neurologist after the patient reaches a certain age. Physical medicine and rehabilitation physicians also in the US diagnosis and treat patients with neuromuscular diseases through the use of electrodiagnostic studies (needle EMG and nerve-conduction studies) and other diagnostic tools. In the United Kingdom and other countries, many of the conditions encountered by older patients such as movement disorders, including Parkinson's disease, stroke, dementia, or gait disorders, are managed predominantly by specialists in geriatric medicine.\n\nClinical neuropsychologists are often called upon to evaluate brain-behavior relationships for the purpose of assisting with differential diagnosis, planning rehabilitation strategies, documenting cognitive strengths and weaknesses, and measuring change over time (e.g., for identifying abnormal aging or tracking the progression of a dementia).\n\nIn some countries, e.g. US and Germany, neurologists may subspecialize in clinical neurophysiology, the field responsible for EEG and intraoperative monitoring, or in electrodiagnostic medicine nerve conduction studies, EMG, and evoked potentials. In other countries, this is an autonomous specialty (e.g., United Kingdom, Sweden, Spain).\n\nAlthough mental illnesses are believed by many to be neurological disorders affecting the central nervous system, traditionally they are classified separately, and treated by psychiatrists. In a 2002 review article in the \"American Journal of Psychiatry\", Professor Joseph B. Martin, Dean of Harvard Medical School and a neurologist by training, wrote, \"the separation of the two categories is arbitrary, often influenced by beliefs rather than proven scientific observations. And the fact that the brain and mind are one makes the separation artificial anyway\".\n\nNeurological disorders often have psychiatric manifestations, such as poststroke depression, depression and dementia associated with Parkinson's disease, mood and cognitive dysfunctions in Alzheimer's disease, and Huntington disease, to name a few. Hence, the sharp distinction between neurology and psychiatry is not always on a biological basis. The dominance of psychoanalytic theory in the first three-quarters of the 20th century has since then been largely replaced by a focus on pharmacology. Despite the shift to a medical model, brain science has not advanced to the point where scientists or clinicians can point to readily discernible pathologic lesions or genetic abnormalities that in and of themselves serve as reliable or predictive biomarkers of a given mental disorder.\n\nThe emerging field of neurological enhancement highlights the potential of therapies to improve such things as workplace efficacy, attention in school, and overall happiness in personal lives. However, this field has also given rise to questions about neuroethics and the psychopharmacology of lifestyle drugs.\n\n\n"}
{"id": "21227", "url": "https://en.wikipedia.org/wiki?curid=21227", "title": "Nu", "text": "Nu\n\nNu or NU may refer to:\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21228", "url": "https://en.wikipedia.org/wiki?curid=21228", "title": "Niue", "text": "Niue\n\nNiue ( ; ) is an island country in the South Pacific Ocean, northeast of New Zealand, east of Tonga, south of Samoa, and west of the Cook Islands. Niue's land area is about and its population, predominantly Polynesian, was about 1,600 in 2016. The island is commonly referred to as \"The Rock\", which comes from the traditional name \"Rock of Polynesia\". Niue is one of the world's largest coral islands. The terrain of the island has two noticeable levels. The higher level is made up of a limestone cliff running along the coast, with a plateau in the centre of the island reaching approximately 60 metres (200 feet) high above sea level. The lower level is a coastal terrace approximately 0.5 km (0.3 miles) wide and about 25–27 metres (80–90 feet) high, which slopes down and meets the sea in small cliffs. A coral reef surrounds the island, with the only major break in the reef being in the central western coast, close to the capital, Alofi. A notable feature are the many limestone caves near the coast.\n\nNiue is a self-governing state in free association with New Zealand; and New Zealand conducts most diplomatic relations—though not all—on its behalf. Niueans are citizens of New Zealand, and Queen Elizabeth II is head of state in her capacity as Queen of New Zealand. Between 90% and 95% of Niuean people live in New Zealand, along with about 70% of the speakers of the Niuean language. Niue is a bilingual country, with 30% of the population speaking both Niuean and English, though the percentage of monolingual English-speaking people is only 11%, while 46% are monolingual Niuean speakers.\n\nNiue is not a member of the United Nations (UN), but UN organisations have accepted its status as a freely-associated state as equivalent to independence for the purposes of international law. As such, Niue is a member of some UN specialised agencies (such as UNESCO, and the WHO), and is invited, alongside the other non-UN member state, the Cook Islands, to attend United Nations conferences open to \"all states\". Niue is subdivided into 14 \"villages\" (municipalities). Each village has a village council that elects its chairman. The villages are at the same time electoral districts; each village sends an assemblyman to the Parliament of Niue. A small and democratic nation, Niueans hold legislative elections every 3 years.\n\nThe \"Niue Integrated Strategic Plan\" (NISP), adopted in 2003, is the national development plan, setting national priorities for development in areas such as financial sustainability. Since the late 20th century Niue has become a leader in green growth; the European Union is helping the nation convert to renewable energy. In January 2004, Niue was hit by Cyclone Heta, which caused extensive damage to the island, including wiping out most of South Alofi. The disaster set the island back about two years from its planned timeline to implement the NISP, since national efforts concentrated on recovery.\n\nPolynesians from Samoa settled Niue around 900 AD. Further settlers arrived from Tonga in the 16th century.\n\nUntil the beginning of the 18th century, Niue appears to have had no national government or national leader; chiefs and heads of families exercised authority over segments of the population. Around 1700 the concept and practice of kingship appears to have originated through contact with the Tongans who settled around the 1600s. A succession of \"patu-iki\" (kings) ruled, beginning with Puni-mata. Tui-toga, who reigned from 1875 to 1887, was the first Christian king.\n\nThe first Europeans to sight Niue sailed under Captain James Cook in 1774. Cook made three attempts to land, but the inhabitants refused to grant permission to do so. He named the island \"Savage Island\" because, as legend has it, the natives who \"greeted\" him were painted in what appeared to be blood. The substance on their teeth was hulahula, a native red fe'i banana. For the next couple of centuries, Niue was known as Savage Island until its original name, Niuē, which translates as \"behold the coconut\", regained use.\n\nThe next notable European visitors represented the London Missionary Society; they arrived on the \"Messenger of Peace\". After many years of trying to land a European missionary, a Niuean named Nukai Peniamina went with his friend, Niumaga, to Samoa and trained as a pastor at the Malua Theological College. Peniamina returned in 1846 on the \"John Williams\" as a missionary with the help of Toimata Fakafitifonua. He was finally allowed to land in Uluvehi Mutalau after a number of attempts in other villages had failed. The chiefs of Mutalau village allowed him to land and assigned over 60 warriors to protect him day and night at the fort in Fupiu.\nIn July 1849 Captain John Erskine visited the island in HMS \"Havannah\".\n\nChristianity was first taught to the Mutalau people before it spread to all the villages. Originally other major villages opposed the introduction of Christianity and had sought to kill Peniamina. The people from the village of Hakupu, although the last village to receive Christianity, came and asked for a \"word of God\"; hence, their village was renamed \"Ha Kupu Atua\" meaning \"any word of God\", or \"Hakupu\" for short.\n\nIn 1889 the chiefs and rulers of Niue, in a letter to Queen Victoria, asked her \"to stretch out towards us your mighty hand, that Niue may hide herself in it and be safe\". After expressing anxiety lest some other nation should take possession of the island, the letter continued: \"We leave it with you to do as seems best to you. If you send the flag of Britain that is well; or if you send a Commissioner to reside among us, that will be well\". The British did not initially take up the offer. In 1900 a petition by the Cook Islanders asking for annexation included Niue \"if possible\". In a document dated 19 October 1901, the \"King\" and Chiefs of Niue consented to \"Queen Victoria taking possession of this island\". A despatch to the Secretary of State for the Colonies from the Governor of New Zealand referred to the views expressed by the Chiefs in favour of \"annexation\" and to this document as \"the deed of cession\". A British Protectorate was declared, but it remained short-lived. Niue was brought within the boundaries of New Zealand on 11 June 1901 by the same Order and Proclamation as the Cook Islands. The Order limited the islands to which it related by reference to an area in the Pacific described by co-ordinates, and Niue, at 19.02 S., 169.55 W, lies within that area.\n\nThe New Zealand Parliament restored self-government in Niue with the 1974 constitution, following a referendum in 1974 in which Niueans had three options: independence, self-government or continuation as a New Zealand territory. The majority selected self-government, and Niue's written constitution\nwas promulgated as supreme law. Robert Rex, ethnically part European, part native, was elected by the Niue Assembly as the first premier, a position he held until his death 18 years later. Rex became the first Niuean to receive a knighthood – in 1984.\n\nIn January 2004 Cyclone Heta hit Niue, killing two people and causing extensive damage to the entire island, including wiping out most of the south of the capital, Alofi.\n\nThe Niue Constitution Act of 1974 vests executive authority in Her Majesty the Queen in Right of New Zealand and in the Governor-General of New Zealand. The Constitution specifies that everyday practice involves the exercise of sovereignty by the Niue Cabinet of Ministers, composed of the Premier and of three other ministers. The Premier and ministers are members of the Niue Legislative Assembly, the nation's parliament.\n\nThe Assembly consists of 20 members, 14 of them elected by the electors of each village constituency, and six by all registered voters in all constituencies. Electors must be New Zealand citizens, resident for at least three months, and candidates must be electors and resident for 12 months. Everyone born in Niue must register on the electoral roll.\n\nNiue has no political parties; all Assembly members are independents. The last and only Niuean political party to have ever existed, the Niue People's Party (1987–2003), won once (in 2002) before disbanding the following year.\n\nThe Legislative Assembly elects the Speaker as its first official in the first sitting of the Assembly following an election. The Speaker calls for nominations for Premier; the candidate with the most votes from the 20 members is elected. The Premier selects three other members to form the Cabinet of Ministers, the executive arm of government. The other two organs of government, following the Westminster model, are the Legislative Assembly and the judiciary. General elections take place every three years, most recently on 6 May 2017.\n\nThe judiciary, independent of the executive and the legislature, includes a High Court and a Court of Appeal, with appeals to the Judicial Committee of the Privy Council in London.\n\nNiue has been self-governing in free association with New Zealand since 3 September 1974, when the people endorsed the Constitution in a plebiscite. Niue is fully responsible for its internal affairs. Niue's position concerning its external relations is less clear cut. Section 6 of the Niue Constitution Act provides that: \"Nothing in this Act or in the Constitution shall affect the responsibilities of Her Majesty the Queen in right of New Zealand for the external affairs and defence of Niue.\" Section 8 elaborates but still leaves the position unclear: Effect shall be given to the provisions of sections 6 and 7 [concerning external affairs and defence and economic and administrative assistance respectively] of this Act, and to any other aspect of the relationship between New Zealand and Niue which may from time to time call for positive co-operation between New Zealand and Niue after consultation between the Prime Minister of New Zealand and the Premier of Niue, and in accordance with the policies of their respective Governments; and, if it appears desirable that any provision be made in the law of Niue to carry out these policies, that provision may be made in the manner prescribed in the Constitution, but not otherwise.\"\n\nNiue has a representative mission in Wellington, New Zealand. It is a member of the Pacific Islands Forum and a number of regional and international agencies. It is not a member of the United Nations, but is a state party to the United Nations Convention on the Law of the Sea, the United Nations Framework Convention on Climate Change, the Ottawa Treaty and the Treaty of Rarotonga. The country is a member state of UNESCO since 26 October 1993.\n\nTraditionally, Niue's foreign relations and defence have been regarded as the responsibility of New Zealand. However, in recent years Niue has begun to follow its own foreign relations, independent of New Zealand, in some spheres. It established diplomatic relations with the People's Republic of China on 12 December 2007. The joint communique signed by Niue and China is different in its treatment of the Taiwan question from that agreed by New Zealand and China. New Zealand \"acknowledged\" China's position on Taiwan but has never expressly agreed with it, but Niue \"recognises that there is only one China in the world, the Government of the People's Republic of China is the sole legal government representing the whole of China and Taiwan is an inalienable part of the territory of China.\" Niue established diplomatic relations with India on 30 August 2012. On 10 June 2014 the Government of Niue announced that Niue had established diplomatic relations with Turkey. The Honourable Minister of Infrastructure Dalton Tagelagi formalised the agreement at the Pacific Small Island States Foreign Ministers meeting in Istanbul, Turkey. The Memorandum of Understanding with Turkey is part of increasing Niue's foreign relationship with countries including the People's Republic of China, India, Australia, Thailand, Samoa, Cook Islands and Singapore.\n\nThe people of Niue have fought as part of the New Zealand military. In World War I, Niue sent about 200 soldiers as part of the Māori Battalion in the New Zealand forces.\n\nNiue is not a republic but its full name was listed as \"the Republic of Niue\" for a number of years on the ISO list of country names (ISO-3166-1). In its newsletter of 14 July 2011, the ISO acknowledged that this was a mistake and the words \"the Republic of\" were deleted from the ISO list of country names.\n\nNiue is a raised coral atoll in the southern Pacific Ocean, east of Tonga. The geographic co-ordinates are . There are three outlying coral reefs within the Exclusive Economic Zone, with no land area:\n\nBesides these, Albert Meyer Reef, (20°53′S, 172°19′W, almost long and wide, least depth , southwest) is not officially claimed by Niue, and the existence of Haymet Rocks (26°S, 160°W, ESE) is in doubt.\n\nNiue is one of the world's largest coral islands. The terrain consists of steep limestone cliffs along the coast with a central plateau rising to about above sea level. A coral reef surrounds the island, with the only major break in the reef being in the central western coast, close to the capital, Alofi. A notable feature is the number of limestone caves near the coast.\n\nThe island is roughly oval in shape (with a diameter of about ), with two large bays indenting the western coast, Alofi Bay in the centre and Avatele Bay in the south. Between these is the promontory of Halagigie Point. A small peninsula, TePā Point (Blowhole Point), is close to the settlement of Avatele in the southwest. Most of the population resides close to the west coast, around the capital, and in the northwest.\n\nSome of the soils are geochemically very unusual. They are extremely weathered tropical soils, with high levels of iron and aluminium oxides (oxisol) and mercury, and they contain high levels of natural radioactivity. There is almost no uranium, but the radionucleides Th-230 and Pa-231 head the decay chains. This is the same distribution of elements as found naturally on very deep seabeds, but the geochemical evidence suggests that the origin of these elements is extreme weathering of coral and brief sea submergence 120,000 years ago. Endothermal upwelling, by which mild volcanic heat draws deep seawater up through the porous coral, may also contribute.\n\nNo adverse health effects from the radioactivity or the other trace elements have been demonstrated, and calculations show that the level of radioactivity is probably much too low to be detected in the population. These unusual soils are very rich in phosphate, but it is not accessible to plants, being in the very insoluble form of iron phosphate, or crandallite. It is thought that similar radioactive soils may exist on Lifou and Mare near New Caledonia, and Rennell in the Solomon Islands, but no other locations are known.\n\nAccording to the World Health Organization, residents are evidently very susceptible to skin cancer. In 2002 Niue reported skin cancer deaths at a rate of 2,482 per 100,000 people – far higher than any other country.\n\nNiue is separated from New Zealand by the International Date Line. The time difference is 23 hours during the Southern Hemisphere winter and 24 hours when New Zealand uses Daylight Saving Time.\n\nThe island has a tropical climate, with most rainfall occurring between November and April.\n\nA leader in green growth, Niue is also focusing on solar power provision, with help from the European Union. However, Niue currently deals with one of the highest rates of greenhouse gas production per capita in the world. Niue aims to become 80% renewable by 2025. The Niue Island Organic Farmers Association is currently paving way to a Multilateral Environmental Agreement (MEA) committed to making Niue the world's first fully organic nation by 2020. \n\nIn July 2009 a solar panel system was installed, injecting about 50 kW into the Niue national power grid. This is nominally 6% of the average 833 kW electricity production. The solar panels are at Niue High School (20 kW), Niue Power Corporation office (1.7 kW) and the Niue Foou Hospital (30 kW). The EU-funded grid-connected PV systems are supplied under the REP-5 programme and were installed recently by the Niue Power Corporation on the roofs of the high school and the power station office and on ground-mounted support structures in front of the hospital. They will be monitored and maintained by the NPC. In 2014 two additional solar power installations were added to the Niue national power grid, one funded under PALM5 of Japan is located outside of the Tuila power station – so far only this has battery storage, the other under European Union funding is located opposite the Niue International Airport Terminal.\n\nNiue's economy is small. Its gross domestic product (GDP) was NZ$17 million in 2003, or US$10 million at purchasing power parity. Niue uses the New Zealand dollar.\n\nThe Niue Integrated Strategic Plan (NISP) is the national development plan, setting national priorities for development. Cyclone Heta set the island back about two years from its planned timeline to implement the NISP, since national efforts concentrated on recovery efforts. In 2008, Niue had yet to fully recover. After Heta the government made a major commitment to rehabilitate and develop the private sector. The government allocated $1 million for the private sector, and spent it on helping businesses devastated by the cyclone, and on construction of the Fonuakula Industrial Park. This industrial park is now completed and some businesses are already operating from there. The Fonuakula Industrial Park is managed by the Niue Chamber of Commerce, a not-for-profit organisation providing advisory services to businesses.\n\nJoint ventures\n\nThe government and the Reef Group from New Zealand started two joint ventures in 2003 and 2004 to develop fisheries and a 120-hectare noni juice operation. Noni fruit comes from \"Morinda citrifolia\" a small tree with edible fruit. Niue Fish Processors Ltd (NFP) is a joint venture company processing fresh fish, mainly tuna (yellow fin, big eye and albacore), for export to overseas markets. NFP operates out of a state-of-the-art fish plant in Amanau Alofi South, completed and opened in October 2004.\n\nTrade\n\nNiue is negotiating free trade agreements with other Pacific countries, PICTA Trade in Services (PICTA TIS), Economic Partnership Agreements with the European Union, and PACERPlus with Australia and New Zealand. The Office of the Chief Trade Adviser (OCTA) has been set up to assist Niue and other Pacific countries in the negotiation of the PACERPlus.\n\nMining\n\nIn August 2005, an Australian mining company, Yamarna Goldfields, suggested that Niue might have the world's largest deposit of uranium. By early September these hopes were seen as overoptimistic, and in late October the company cancelled its plans, announcing that exploratory drilling had identified nothing of commercial value. The Australian Securities and Investments Commission filed charges in January 2007 against two directors of the company, now called Mining Projects Group Ltd, alleging that their conduct had been deceptive and that they engaged in insider trading. This case was settled out of court in July 2008, both sides withdrawing their claims.\n\nRemittances from expatriates were a major source of foreign exchange in the 1970s and early 1980s. Continuous migration to New Zealand has shifted most members of nuclear and extended families there, removing the need to send remittances back home. In the late 1990s, PFTAC conducted studies on the balance of payments, which confirmed that Niueans are receiving few remittances but are sending more money overseas.\n\nForeign aid\n\nForeign aid has been Niue's principal source of income. Although most aid comes from New Zealand, this is currently being phased out with reductions of NZ$250,000 each year. The country will need to rely more upon its own economy. The government generates some revenue, mainly from income tax, import tax and the lease of phone lines. \n\nOffshore banking\n\nThe government briefly considered offshore banking. Under pressure from the US Treasury, Niue agreed to end its support for schemes designed to minimise tax in countries like New Zealand. Niue provides automated Companies Registration, administered by the New Zealand Ministry of Economic Development. The Niue Legislative Assembly passed the Niue Consumption Tax Act in the first week of February 2009, and the 12.5% tax on goods and services was expected to take effect on 1 April 2009. Income tax has been lowered, and import tax may be reset to zero except for \"sin\" items like tobacco, alcohol and soft drinks. Tax on secondary income has been lowered from 35% to 10%, with the stated goal of fostering increased labour productivity.\n\nInternet\n\nIn 1997, the Internet Assigned Numbers Authority (IANA), under contract with the US Department of Commerce, assigned the Internet Users Society-Niue (IUS-N), a private nonprofit, as manager of the .nu top-level domain on the Internet. IUS-N's charitable purpose was – and continues to be – to use revenue from the registration of .nu domain names to fund low-cost or free Internet services for the people of Niue. In a letter to ICANN in 2007, IUS-N's independent auditors reported IUS-N had invested US$3 million for Internet services in Niue between 1999 and 2005 from .nu domain name registration revenue during that period. In 1999, IUS-N and the Government of Niue signed an agreement whereby the Government recognised that IUS-N managed the .nu ccTLD under IANA's authority and IUS-N committed to provide free Internet services to government departments as well as to Niue's private citizens. A newly elected government later disputed that agreement and attempted to assert a claim on the domain name, including a requirement for IUS-N to make direct payments of compensation to the Government. In 2005, a Government-appointed Commission of Inquiry into the dispute released its report, which found no merit in the government's claims; the government subsequently dismissed the claims in 2007. Starting in 2003, IUS-N began installing WiFi connections throughout the capital village of Alofi and in several nearby villages and schools, and has been expanding WiFi coverage into the outer villages since then, making Niue the first WiFi Nation. To assure security for Government departments, IUS-N provides the government with a secure DSL connection to IUS-N's satellite Internet link, at no cost.\n\nAgriculture is very important to the lifestyle of Niueans and the economy, and around 204 square kilometres of the land area are available for agriculture. Subsistence agriculture is very much part of Niue's culture, where nearly all the households have plantations of taro. Taro is a staple food, and the pink taro now dominant in the taro markets in New Zealand and Australia is an intellectual property of Niue. This is one of the naturally occurring taro varieties on Niue, and has a strong resistance to pests. The Niue taro is known in Samoa as \"talo Niue\" and in international markets as pink taro. Niue exports taro to New Zealand.\nTapioca or cassava, yams and kumara also grow very well, as do different varieties of bananas. Coconut meat, passionfruit and limes dominated exports in the 1970s, but in 2008 vanilla, noni and taro were the main export crops.\n\nMost families grow their own food crops for subsistence and sell their surplus at the Niue Makete in Alofi, or export to their families in New Zealand.\nCoconut crab, or uga, is also part of the food chain; it lives in the forest and coastal areas.\n\nIn 2003, the government made a commitment to develop and expand vanilla production with the support of NZAID. Vanilla has grown wild on Niue for a long time. Despite the setback caused by the devastating Cyclone Heta in early 2004, work on vanilla production continues. The expansion plan started with the employment of the unemployed or underemployed labour force to help clear land, plant supporting trees and plant vanilla vines. The approach to accessing land includes planning to have each household plant a small plot of around half to to be cleared and planted with vanilla vines. There are a lot of planting materials for supporting trees to meet demand for the expansion of vanilla plantations, but a severe shortage of vanilla vines for planting stock. There are the existing vanilla vines, but cutting them for planting stock will reduce or stop the vanilla from producing beans. At the moment, the focus is in the areas of harvesting and marketing.\n\nThe last agricultural census was in 1989.\n\nTourism is one of the three priority economic sectors (the other two are fisheries and agriculture) for economic development. In 2006, estimated visitor expenditure reached $1.6 million making tourism a major industry for Niue. Niue will continue to receive direct support from the government and overseas donor agencies. The only airport is Niue International Airport. Air New Zealand is the sole airline, flying twice a week from Auckland. In the early 1990s Niue International Airport was served by a local airline, Niue Airlines, but it closed in 1992.\n\nThere is a tourism development strategy to increase the number of rooms available to tourists at a sustainable level. Niue is trying to attract foreign investors to invest in the tourism industry by offering import and company tax concessions as incentives. New Zealand businessman Earl Hagaman, founder of Scenic Hotel Group, was awarded a contract in 2014 to manage the Matavai Resort in Niue after he made a $101,000 political donation to the National Party, which at that time led a minority government in New Zealand. The resort is subsidized by New Zealand, which wants to bolster tourism there. In 2015 NZ announced $7.5m in additional funding for expansion of the resort. The selection of the Matavai contractor was made by the Niue Tourism Property Trust, whose trustees are appointed by NZ Foreign Affairs minister Murray McCully. Prime Minister John Key said he did not handle campaign donations, and that Niue premier Toke Talagi has long pursued tourism as a growth strategy. McCully denied any link between the donation, the foreign aid and the contractor selection.\n\nThe sailing season begins in May. Alofi Bay has many mooring buoys and yacht crews can lodge at Niue Backpackers. The anchorage in Niue is one of the least protected in the South Pacific, so much that cruise ship tenders are often unable to risk landing passengers due to weather or sea conditions, as well as the associated risk of having them stranded ashore. Other challenges of the anchorage are a primarily coral bottom and many deep spots. Mooring buoys are attached to seine floats that support the mooring lines away from seabed obstructions.\n\nOn 27 October 2016, Niue officially declared that all its national debt was paid off. The Government plans to spend money saved from servicing loans on increasing pensions, and offering incentives to lure expatriates back home. However, Niue isn't entirely independent. New Zealand pays $14 million in aid each year and Niue still depends on New Zealand. Premier Toke Talagi said Niue managed to pay off US$4 million of debt and had \"no interest\" in borrowing again, particularly from huge powers such as China.\n\nThe first computers were Apple machines brought in by the University of the South Pacific Extension Centre around the early 1980s. The Treasury Department computerised its general ledger in 1986 using NEC personal computers that were IBM PC XT compatible. The Census of Households and Population in 1986 was the first to be processed using a personal computer with the assistance of David Marshall, FAO Adviser on Agricultural Statistics, advising UNFPA Demographer Dr Lawrence Lewis and Niue Government Statistician Bill Vakaafi Motufoou to switch from using manual tabulation cards. In 1987 Statistics Niue got its new personal computer NEC PC AT use for processing the 1986 census data; personnel were sent on training in Japan and New Zealand to use the new computer. The first Computer Policy was developed and adopted in 1988. \n\nIn 2003, Niue became the first country in the world to provide state-funded wireless internet to all inhabitants.\n\nIn August 2008 it has been reported that all school students have what is known as the OLPC XO-1, a specialised laptop by the One Laptop per Child project designed for children in the developing world. Niue was also a location of tests for the OpenBTS project, which aims to deliver low-cost GSM base stations built with open source software.\nIn July 2011, Telecom Niue launched pre-paid mobile services (Voice/EDGE – 2.5G) as Rokcell Mobile based on the commercial GSM product of vendor Lemko. Three BTS sites will cover the nation. International roaming is not currently available. The fibre optic cable ring is now completed around the island (FTTC), Internet/ADSL services were rolled out towards the end of 2011.\n\nIn January 2015 Telecom Niue completed the laying of the fibre optic cable around Niue connecting all the 14 villages, making land line phones and ADSL internet connection available to households.\n\nThe following demographic statistics are from the CIA World Factbook.\n\n\n\n\n\n\nNiue is the birthplace of New Zealand artist and writer John Pule. Author of \"The Shark That Ate the Sun\", he also paints tapa cloth inspired designs on canvas. In 2005, he co-wrote \"Hiapo: Past and Present in Niuean Barkcloth\", a study of a traditional Niuean artform, with Australian writer and anthropologist Nicholas Thomas.\n\nTaoga Niue is a new Government Department responsible for the preservation of culture, tradition and heritage. Recognising its importance, the Government has added Taoga Niue as the sixth pillar of the Niue Integrated Strategic Plan (NISP).\n\nNiue has two broadcast outlets, Television Niue and Radio Sunshine, managed and operated by the Broadcasting Corporation of Niue, and one newspaper, the \"Niue Star\".\n\nDespite being a small country, a number of sports are popular. Rugby union is the most popular sport, played by both men and women; Niue were the 2008 FORU Oceania Cup champions. Netball is played only by women. There is a nine-hole golf course at Fonuakula. There is a lawn bowling green under construction. Association Football is a popular sport, as evidenced by the Niue Soccer Tournament, though the Niue national football team has played only two matches. Rugby league is also a popular sport. Niue Rugby League have only started making strides within the international arena since their first ever test match against Vanuatu, going down 22–20 in 2013. On 4 October 2014, the Niue rugby league team record their first ever international test match win defeating the Philippines 36–22. In May 2015, Niue Rugby League recorded their second international test match win against the South African Rugby League side, 48–4. Niue now sit 31st in the Rugby League World Rankings.\n\n\n"}
{"id": "21230", "url": "https://en.wikipedia.org/wiki?curid=21230", "title": "New England (disambiguation)", "text": "New England (disambiguation)\n\nNew England is a region of north-eastern United States, comprising Connecticut, Maine, Massachusetts, Rhode Island, New Hampshire, and Vermont.\n\nNew England may also refer to:\n\n\n\n\n\n\n\n\n"}
