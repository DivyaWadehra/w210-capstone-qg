{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import ast\n",
    "import string\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# encoding =utf8\n",
    "#import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Article Pre-Processing\n",
    "To move our data through the pipeline and ensure that it is suitable for our app, we will format it like the SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_data = {\"data\": [], \"version\" : 1.0}\n",
    "\n",
    "num_files = 2 # update this as we know how many files we have\n",
    "\n",
    "for i in range(num_files): \n",
    "    \n",
    "    input_file = './AA_wiki_00_01/wiki_0' + str(i)\n",
    "    output_file = './AA_wiki_00_01/wiki_squad_0' + str(i) + '.json'\n",
    "    \n",
    "    with open(input_file) as f:\n",
    "        # each line represents a different wikipedia article\n",
    "        # we will ignore the id and url for now, not needed\n",
    "        for line in f:\n",
    "            line_dict = ast.literal_eval(line)\n",
    "            title = line_dict['title']\n",
    "            text = line_dict['text'].split(\"\\n\\n\",1)[1] # title is duplicated within text as well\n",
    "\n",
    "            # Break text up into paragraphs\n",
    "            paras = text.split(\"\\n\\n\")\n",
    "\n",
    "            context = [{'context': para, 'qas' : []} for para in paras]\n",
    "\n",
    "\n",
    "            wikipedia_data['data'].append({'title' : title, 'paragraphs' : context})\n",
    "\n",
    "    with open(output_file, 'w') as outfile:  \n",
    "        json.dump(wikipedia_data, outfile)\n",
    "    \n",
    "#print(wikipedia_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sents_and_weights(d,paragraphs):\n",
    "        \"\"\"Clean sentences, remove digit, punctuation, upper case to lower\n",
    "        Args: paragraphs = list of dicts with contexts\n",
    "        Return: sentences_processed: dict of cleaned sentences. key = number of sentence; value = list of stemmed words.\n",
    "        \"\"\"\n",
    "        labeled_sentences = {}\n",
    "        stemmed_sentences = {}\n",
    "        \n",
    "        # this needs to be a default dict so it returns 0 if word not found \n",
    "        word_dict = defaultdict(int)\n",
    "        word_distr = defaultdict(int)\n",
    "\n",
    "        # initialize for stemming\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokenize = nltk.word_tokenize\n",
    "\n",
    "        def stem_and_add(wd):\n",
    "            word_dict[wd] += 1\n",
    "            word_dict['total_count'] += 1\n",
    "            return stemmer.stem(wd)\n",
    "        \n",
    "        # need to go through each 'paragraph' (context) to gather info about sentences\n",
    "        for i,context in enumerate(paragraphs):\n",
    "\n",
    "            # split paragraph into sentences, make sure to keep digits, etc. together\n",
    "            sentences = context['context'].split('. ') #last one still has a period at the end\n",
    "\n",
    "            for j,sentence in enumerate(sentences):\n",
    "                # save list of unprocessed sentences for later\n",
    "                labeled_sentences[(d,i,j)] = sentence\n",
    "                            \n",
    "                # Remove all digits\n",
    "                sentence = ''.join([x for x in sentence if not x.isdigit()])\n",
    "                # Remove all punctuation (OK to include periods since it's been split)\n",
    "                sentence = ''.join([x for x in sentence if x not in string.punctuation])\n",
    "                \n",
    "                # Lowercase everything\n",
    "                sentence = sentence.strip()\n",
    "                sentence = sentence.lower()\n",
    "                \n",
    "                # Split into words & rejoin (remove extra spaces)\n",
    "                sentence = ' '.join(sentence.split())\n",
    "                \n",
    "                tokenized_stemmed_sent = [stem_and_add(word) for word in nltk.tokenize.word_tokenize(sentence) \n",
    "                                          if not word in stop_words]\n",
    "                \n",
    "                # keep track of tokenized for calculating sentence weight in next step\n",
    "                stemmed_sentences[(d,i,j)] = tokenized_stemmed_sent\n",
    "                \n",
    "                # update our word dictionary to be relative frequencies rather than absolute values\n",
    "                for word, ct in word_dict.items():\n",
    "                    # but keep our total count, we may want that later (not sure)\n",
    "                    if not word == 'total_count':\n",
    "                        word_distr[word] = word_dict[word] / word_dict['total_count']\n",
    "                \n",
    "        #print(\"article length:\",word_dict['total_count'])\n",
    "        #print(\"word dict:\",word_distr)\n",
    "\n",
    "        return labeled_sentences,stemmed_sentences,word_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sent_weight(word_dist, stemmed_sents):\n",
    "        \"\"\"Compute weight with respect to sentences\n",
    "        Args:\n",
    "                word_distribution: dict with word: weight\n",
    "                stemmed_sents: list with \n",
    "        Return:\n",
    "                sentence_weight: dict of weight of each sentence. key = sentence #, value = weight\n",
    "        \"\"\"\n",
    "        sentences_weight = {}\n",
    "        # Iterate through each word in each sentence, if word distribution and sentence id are in dictionary, \n",
    "        # add to existing word distribution. Else, sentence weight for given sentence equals current word distribution\n",
    "          \n",
    "        for key, words in stemmed_sents.items():\n",
    "            #print(words)\n",
    "            # Sentence weight equals sum of word distributions divided by length of cleaned sentence\n",
    "            if len(words) == 0:\n",
    "                weight = 0\n",
    "            else:\n",
    "                weight = sum([word_dist[word] for word in words]) / len(words)\n",
    "            \n",
    "            sentences_weight[key] = weight\n",
    "            \n",
    "        sentences_weight = sorted(sentences_weight.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        #print('sentence weight: ',sentences_weight)\n",
    "\n",
    "        return sentences_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topically_important_sentence(sentences_weight, labeled_sentences):\n",
    "        \"\"\"Select topically import sentences\n",
    "        Args:\n",
    "                sentence_weight: list of tuples, (sentence_num, sentence_weight) computed in sentence_weight\n",
    "                paragraph: set of sentences\n",
    "        Return:\n",
    "                sentences_selected: dict, topically important sentences selected\n",
    "        \"\"\"\n",
    "        final_sentences = {}\n",
    "        \n",
    "        total_sentences = len(sentences_weight)\n",
    "        # how many sentences to retain\n",
    "        num_sentences_selected = math.ceil(float(0.05) * total_sentences)\n",
    "        #print('num sentences for this passage:',num_sentences_selected)\n",
    "        \n",
    "        # key of selected sentences (# order of sentence in paragraph)\n",
    "        #sentences_selected_key = []\n",
    "        \n",
    "        # dictionary of all sentences \n",
    "        sentences_dict = {}\n",
    "        flag = 0\n",
    "        \n",
    "        # select num_sentences_selected # of sentences from list of sentence weights\n",
    "        selected_keys = [k for k,v in sentence_weight[0:num_sentences_selected]]\n",
    "        \n",
    "        #print(\"selected sentence(s):\",selected_keys)\n",
    "\n",
    "\n",
    "        for sent_key in selected_keys:\n",
    "            pre_processed_sentence = labeled_sentences[sent_key]\n",
    "            \n",
    "            processed_sentence = pre_processed_sentence.lower() #lowercase\n",
    "            processed_sentence = processed_sentence.replace('[[','')\n",
    "            processed_sentence = processed_sentence.replace(']]','')\n",
    "            processed_sentence = processed_sentence.replace(']','')\n",
    "            processed_sentence = processed_sentence.replace('[','')\n",
    "            processed_sentence = re.sub('(?<!\\d)([.,!?()])(?<!\\d)', r' \\1 ', processed_sentence)\n",
    "            processed_sentence = re.sub(r'\\(','-lrb- ',processed_sentence) # replace left parens, add space after\n",
    "            processed_sentence = re.sub(r'\\)',' -rrb-',processed_sentence) # replace left parens, add space after\n",
    "            processed_sentence = re.sub(r'\\([^)]*\\)', '',processed_sentence) #replace brackets in links\n",
    "            processed_sentence = re.sub('(?<=\\s)\\\"','`` ',processed_sentence) # replace first double quotes with ``\n",
    "            processed_sentence = re.sub(r'\\\"', \" ''\", processed_sentence) # replace second double quote with two single quotes ''\n",
    "\n",
    "            #print(processed_sentence)\n",
    "\n",
    "            final_sentences[sent_key] = processed_sentence\n",
    "            \n",
    "        return final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./AA_wiki_00_01/wiki_squad_00.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "\n",
    "# small file just for testing!! \n",
    "#with open('./sentence_selection/AA_wiki_00_01/wiki_squad_00_jb.json') as json_file:  \n",
    "    #data = json.load(json_file)    \n",
    "    \n",
    "    \n",
    "#type(data)\n",
    "data = pd.DataFrame.from_dict(data)\n",
    "df = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences_onmt = open('./test_sents_qg', \"w\")\n",
    "sentences_labeled = open('./test_sents_labeled', \"w\")\n",
    "\n",
    "\n",
    "# for each article in the file\n",
    "for row,value in df.iteritems():\n",
    "    # here is where we clean and stem words, build word distribution\n",
    "    #print(value['title'])\n",
    "    labeled_sentences, stemmed_sentences, word_distribution = sents_and_weights(row,value['paragraphs'])\n",
    "    \n",
    "    # use this word distribution to get weights for each sentence and calculate most important sentences \n",
    "    sentence_weight = calc_sent_weight(word_distribution,stemmed_sentences)\n",
    "        \n",
    "    # pull out most important sentences\n",
    "    # and keep track of where they came from: (doc #, context #, sentence #)\n",
    "    chosen_sentences = topically_important_sentence(sentence_weight,labeled_sentences)\n",
    "    \n",
    "    for sents in chosen_sentences.items():\n",
    "        \n",
    "        #save selected sentences directly to file, for onmt model\n",
    "        sentences_onmt.write(str(sents[1])+'\\n')\n",
    "        # keep track of their locations, though\n",
    "        sentences_labeled.write(str(sents)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
