{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Don't forget to activate the appropriate environment!!\n",
    "https://ipython.readthedocs.io/en/stable/install/kernel_install.html#kernels-for-different-environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "#import pandas as pd\n",
    "import ast\n",
    "import string\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# encoding =utf8\n",
    "#import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "File structure:\n",
    "\n",
    "w210-literacy/\n",
    "├── GenerationQ/\n",
    "│   ├── flask/\n",
    "│   ├── model/\n",
    "|       ├── data/\n",
    "|       ├── onmt/\n",
    "|       ├── trained/\n",
    "|       ├── emb_to_torch.sh\n",
    "|       ├── embeddings_to_torch.py\n",
    "|       ├── fwd.sh\n",
    "|       ├── preproc.sh\n",
    "|       ├── preprocess.py\n",
    "|       ├── test.py\n",
    "|       ├── test.sh\n",
    "|       ├── train.sh\n",
    "|       ├── train.py\n",
    "|       └── train.sh\n",
    "│   └── README.md\n",
    "├── DrQA/\n",
    "│   ├── TODO\n",
    "│   └── TODO\n",
    "├── data/\n",
    "│   ├── wikipedia_dump/\n",
    "│   └── wikipedia_squad/\n",
    "└── Wikipedia Article Sentence Selection.ipynb [this file!!]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Article Pre-Processing\n",
    "To move our data through the pipeline and ensure that it is suitable for our app, we will format it like the SQuAD dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONLY RUN THIS FOR TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikipedia_data = {\"data\": [], \"version\" : 1.0}\n",
    "\n",
    "num_files = 1 # each folder has 100 articles\n",
    "end_letter = 1 # because we only have folders up through 9\n",
    "\n",
    "# at this point, just for one letter of the alphabet. Would need to add another loop.\n",
    "wiki_input_file_dir = './A'\n",
    "#os.mkdir('./output')\n",
    "wiki_output_file_dir = './output/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONLY RUN THIS FOR FULL SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_files = 10 # each folder has 100 articles\n",
    "end_letter = 1 # because we only have folders up through 9\n",
    "\n",
    "wiki_input_file_dir = '../bucket-w210/wikipedia/json_out/A'\n",
    "#os.mkdir('./wikipedia_squad')\n",
    "wiki_output_file_dir = './wikipedia_squad/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACK TO NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# go through each file, add to mega-file of articles\n",
    "for c in string.ascii_uppercase[:end_letter]:\n",
    "    \n",
    "    output_dir = wiki_output_file_dir + c\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    # right now, it looks like all folders have exactly 100 files\n",
    "    for i in range(num_files): \n",
    "        \n",
    "        wikipedia_data = {\"data\": [], \"version\" : 1.0}\n",
    "\n",
    "        input_file = wiki_input_file_dir + c + '/wiki_' + '%02d' % i\n",
    "        #print(input_file)\n",
    "        output_file = output_dir + '/wiki_squad_' + '%02d' % i + '.json'\n",
    "\n",
    "        with open(input_file) as f:\n",
    "            # each line represents a different wikipedia article\n",
    "            # we will ignore the id and url for now, not needed\n",
    "            for line in f:\n",
    "                line_dict = ast.literal_eval(line)\n",
    "                title = line_dict['title']\n",
    "                #print(title)\n",
    "                text = line_dict['text'].split(\"\\n\\n\",1)[1] # title is duplicated within text as well\n",
    "\n",
    "                # arbitrary length, should eliminate articles like \"disambiguation\" articles, etc. \n",
    "                if len(text) > 1000: \n",
    "                    # Break text up into paragraphs\n",
    "                    paras = text.split(\"\\n\\n\")\n",
    "\n",
    "                    context = [{'context': para, 'qas' : []} for para in paras]\n",
    "\n",
    "                    wikipedia_data['data'].append({'title' : title, 'paragraphs' : context})\n",
    "        \n",
    "        with open(output_file, 'w') as outfile:  \n",
    "            json.dump(wikipedia_data, outfile)\n",
    "    \n",
    "#print(wikipedia_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sents_and_weights(d,paragraphs):\n",
    "        \"\"\"Clean sentences, remove digit, punctuation, upper case to lower\n",
    "        Args: paragraphs = list of dicts with contexts\n",
    "        Return: sentences_processed: dict of cleaned sentences. key = number of sentence; value = list of stemmed words.\n",
    "        \"\"\"\n",
    "        labeled_sentences = {}\n",
    "        stemmed_sentences = {}\n",
    "        \n",
    "        # this needs to be a default dict so it returns 0 if word not found \n",
    "        word_dict = defaultdict(int)\n",
    "        word_distr = defaultdict(int)\n",
    "\n",
    "        # initialize for stemming\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokenize = nltk.word_tokenize\n",
    "\n",
    "        def stem_and_add(wd):\n",
    "            word_dict[wd] += 1\n",
    "            word_dict['total_count'] += 1\n",
    "            return stemmer.stem(wd)\n",
    "        \n",
    "        # need to go through each 'paragraph' (context) to gather info about sentences\n",
    "        for i,context in enumerate(paragraphs):\n",
    "\n",
    "            # split paragraph into sentences, make sure to keep digits, etc. together\n",
    "            sentences = context['context'].split('. ') #last one still has a period at the end\n",
    "\n",
    "            for j,sentence in enumerate(sentences):\n",
    "                # save list of unprocessed sentences for later\n",
    "                labeled_sentences[(d,i,j)] = sentence\n",
    "                            \n",
    "                # Remove all digits\n",
    "                sentence = ''.join([x for x in sentence if not x.isdigit()])\n",
    "                # Remove all punctuation (OK to include periods since it's been split)\n",
    "                sentence = ''.join([x for x in sentence if x not in string.punctuation])\n",
    "                \n",
    "                # Lowercase everything\n",
    "                sentence = sentence.strip()\n",
    "                sentence = sentence.lower()\n",
    "                \n",
    "                # Split into words & rejoin (remove extra spaces)\n",
    "                sentence = ' '.join(sentence.split())\n",
    "                \n",
    "                tokenized_stemmed_sent = [stem_and_add(word) for word in nltk.tokenize.word_tokenize(sentence) \n",
    "                                          if not word in stop_words]\n",
    "                \n",
    "                # keep track of tokenized for calculating sentence weight in next step\n",
    "                stemmed_sentences[(d,i,j)] = tokenized_stemmed_sent\n",
    "                \n",
    "                # update our word dictionary to be relative frequencies rather than absolute values\n",
    "                for word, ct in word_dict.items():\n",
    "                    # but keep our total count, we may want that later (not sure)\n",
    "                    if not word == 'total_count':\n",
    "                        word_distr[word] = word_dict[word] / word_dict['total_count']\n",
    "                \n",
    "        #print(\"article length:\",word_dict['total_count'])\n",
    "        #print(\"word dict:\",word_distr)\n",
    "\n",
    "        return labeled_sentences,stemmed_sentences,word_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_sent_weight(word_dist, stemmed_sents):\n",
    "        \"\"\"Compute weight with respect to sentences\n",
    "        Args:\n",
    "                word_distribution: dict with word: weight\n",
    "                stemmed_sents: list with \n",
    "        Return:\n",
    "                sentence_weight: dict of weight of each sentence. key = sentence #, value = weight\n",
    "        \"\"\"\n",
    "        sentences_weight = {}\n",
    "        # Iterate through each word in each sentence, if word distribution and sentence id are in dictionary, \n",
    "        # add to existing word distribution. Else, sentence weight for given sentence equals current word distribution\n",
    "          \n",
    "        for key, words in stemmed_sents.items():\n",
    "            #print(words)\n",
    "            # Sentence weight equals sum of word distributions divided by length of cleaned sentence\n",
    "            if len(words) == 0:\n",
    "                weight = 0\n",
    "            else:\n",
    "                weight = sum([word_dist[word] for word in words]) / len(words)\n",
    "            \n",
    "            sentences_weight[key] = weight\n",
    "            \n",
    "        sentences_weight = sorted(sentences_weight.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        #print('sentence weight: ',sentences_weight)\n",
    "\n",
    "        return sentences_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topically_important_sentence(sentences_weight, labeled_sentences):\n",
    "        \"\"\"Select topically import sentences\n",
    "        Args:\n",
    "                sentence_weight: list of tuples, (sentence_num, sentence_weight) computed in sentence_weight\n",
    "                paragraph: set of sentences\n",
    "        Return:\n",
    "                sentences_selected: dict, topically important sentences selected\n",
    "        \"\"\"\n",
    "        final_sentences = {}\n",
    "        \n",
    "        total_sentences = len(sentences_weight)\n",
    "        # how many sentences to retain\n",
    "        num_sentences_selected = math.ceil(float(0.05) * total_sentences)\n",
    "        #print('num sentences for this passage:',num_sentences_selected)\n",
    "        \n",
    "        # key of selected sentences (# order of sentence in paragraph)\n",
    "        #sentences_selected_key = []\n",
    "        \n",
    "        # dictionary of all sentences \n",
    "        sentences_dict = {}\n",
    "        flag = 0\n",
    "        \n",
    "        # select num_sentences_selected # of sentences from list of sentence weights\n",
    "        #print(sentence_weight[0])\n",
    "        selected_keys = [k for k,v in sentence_weight[0:num_sentences_selected]]\n",
    "        \n",
    "        #print(\"selected sentence(s):\",selected_keys)\n",
    "\n",
    "\n",
    "        for sent_key in selected_keys:\n",
    "            pre_processed_sentence = labeled_sentences[sent_key]\n",
    "            \n",
    "            processed_sentence = pre_processed_sentence.lower() #lowercase\n",
    "            processed_sentence = processed_sentence.replace('[[','')\n",
    "            processed_sentence = processed_sentence.replace(']]','')\n",
    "            processed_sentence = processed_sentence.replace(']','')\n",
    "            processed_sentence = processed_sentence.replace('[','')\n",
    "            processed_sentence = re.sub('(?<!\\d)([.,!?()])(?<!\\d)', r' \\1 ', processed_sentence)\n",
    "            processed_sentence = re.sub(r'\\(','-lrb- ',processed_sentence) # replace left parens, add space after\n",
    "            processed_sentence = re.sub(r'\\)',' -rrb-',processed_sentence) # replace left parens, add space after\n",
    "            processed_sentence = re.sub(r'\\([^)]*\\)', '',processed_sentence) #replace brackets in links\n",
    "            processed_sentence = re.sub('(?<=\\s)\\\"','`` ',processed_sentence) # replace first double quotes with ``\n",
    "            processed_sentence = re.sub(r'\\\"', \" ''\", processed_sentence) # replace second double quote with two single quotes ''\n",
    "\n",
    "            #print(processed_sentence)\n",
    "\n",
    "            final_sentences[sent_key] = processed_sentence\n",
    "            \n",
    "        return final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in string.ascii_uppercase[:end_letter]:\n",
    "    \n",
    "    output_dir = wiki_output_file_dir + c\n",
    "\n",
    "    for i in range(num_files): \n",
    "    \n",
    "        output_file = output_dir + '/wiki_squad_' + '%02d' % i + '.json'\n",
    "\n",
    "        with open(output_file) as json_file:  \n",
    "            data = json.load(json_file)\n",
    "\n",
    "        data = pd.DataFrame.from_dict(data)\n",
    "        df = data['data']\n",
    "\n",
    "        sentences_onmt = open(output_dir + '/test_sents_qg_' + '%02d' % i, \"w\")\n",
    "        sentences_labeled = open(output_dir + '/test_sents_labeled_' + '%02d' % i, \"w\")\n",
    "\n",
    "        # for each article in the file\n",
    "        for row,value in df.iteritems():\n",
    "            # here is where we clean and stem words, build word distribution\n",
    "            #print(value['title'])\n",
    "            labeled_sentences, stemmed_sentences, word_distribution = sents_and_weights(row,value['paragraphs'])\n",
    "\n",
    "            #print(labeled_sentences)\n",
    "\n",
    "            # use this word distribution to get weights for each sentence and calculate most important sentences \n",
    "            sentence_weight = calc_sent_weight(word_distribution,stemmed_sentences)\n",
    "            \n",
    "\n",
    "            # pull out most important sentences\n",
    "            # and keep track of where they came from: (doc #, context #, sentence #)\n",
    "            chosen_sentences = topically_important_sentence(sentence_weight,labeled_sentences)\n",
    "\n",
    "            for sents in chosen_sentences.items():\n",
    "\n",
    "                #save selected sentences directly to file, for onmt model\n",
    "                sentences_onmt.write(str(sents[1])+'\\n')\n",
    "                # keep track of their locations, though\n",
    "                sentences_labeled.write(str(sents)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference: https://github.com/drewserles/GenerationQ. Assuming training has already been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating questions for file #0\n",
      "./wikipedia_squad/AA/test_sents_qg_00\n",
      "/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n",
      "/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py:542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(a, requires_grad=False)\n",
      "PRED AVG SCORE: -0.6408, PRED PPL: 1.8980\n",
      "generating questions for file #1\n",
      "./wikipedia_squad/AA/test_sents_qg_01\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py\", line 229, in translate\n",
      "    for batch in data_iter:\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 201, in process\n",
      "    tensor = self.numericalize(padded, device=device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 321, in numericalize\n",
      "    arr = self.postprocessing(arr, None)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/inputters/text_dataset.py\", line 235, in make_src\n",
      "    src_vocab_size = max([t.max() for t in data]) + 1\n",
      "RuntimeError: invalid argument 1: tensor must have one dimension at /opt/conda/conda-bld/pytorch_1549617926868/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:590\n",
      "generating questions for file #2\n",
      "./wikipedia_squad/AA/test_sents_qg_02\n",
      "/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n",
      "/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py:542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(a, requires_grad=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py\", line 229, in translate\n",
      "    for batch in data_iter:\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 201, in process\n",
      "    tensor = self.numericalize(padded, device=device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 321, in numericalize\n",
      "    arr = self.postprocessing(arr, None)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/inputters/text_dataset.py\", line 235, in make_src\n",
      "    src_vocab_size = max([t.max() for t in data]) + 1\n",
      "RuntimeError: invalid argument 1: tensor must have one dimension at /opt/conda/conda-bld/pytorch_1549617926868/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:590\n",
      "generating questions for file #3\n",
      "./wikipedia_squad/AA/test_sents_qg_03\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py\", line 229, in translate\n",
      "    for batch in data_iter:\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 201, in process\n",
      "    tensor = self.numericalize(padded, device=device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 321, in numericalize\n",
      "    arr = self.postprocessing(arr, None)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/inputters/text_dataset.py\", line 235, in make_src\n",
      "    src_vocab_size = max([t.max() for t in data]) + 1\n",
      "RuntimeError: invalid argument 1: tensor must have one dimension at /opt/conda/conda-bld/pytorch_1549617926868/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:590\n",
      "generating questions for file #4\n",
      "./wikipedia_squad/AA/test_sents_qg_04\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py\", line 229, in translate\n",
      "    for batch in data_iter:\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 201, in process\n",
      "    tensor = self.numericalize(padded, device=device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 321, in numericalize\n",
      "    arr = self.postprocessing(arr, None)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/inputters/text_dataset.py\", line 235, in make_src\n",
      "    src_vocab_size = max([t.max() for t in data]) + 1\n",
      "RuntimeError: invalid argument 1: tensor must have one dimension at /opt/conda/conda-bld/pytorch_1549617926868/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:590\n",
      "generating questions for file #5\n",
      "./wikipedia_squad/AA/test_sents_qg_05\n",
      "/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n",
      "/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py:542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(a, requires_grad=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py\", line 229, in translate\n",
      "    for batch in data_iter:\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 201, in process\n",
      "    tensor = self.numericalize(padded, device=device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 321, in numericalize\n",
      "    arr = self.postprocessing(arr, None)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/inputters/text_dataset.py\", line 235, in make_src\n",
      "    src_vocab_size = max([t.max() for t in data]) + 1\n",
      "RuntimeError: invalid argument 1: tensor must have one dimension at /opt/conda/conda-bld/pytorch_1549617926868/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:590\n",
      "generating questions for file #6\n",
      "./wikipedia_squad/AA/test_sents_qg_06\n",
      "/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n",
      "/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py:542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(a, requires_grad=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py\", line 229, in translate\n",
      "    for batch in data_iter:\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 201, in process\n",
      "    tensor = self.numericalize(padded, device=device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 321, in numericalize\n",
      "    arr = self.postprocessing(arr, None)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/inputters/text_dataset.py\", line 235, in make_src\n",
      "    src_vocab_size = max([t.max() for t in data]) + 1\n",
      "RuntimeError: invalid argument 1: tensor must have one dimension at /opt/conda/conda-bld/pytorch_1549617926868/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:590\n",
      "generating questions for file #7\n",
      "./wikipedia_squad/AA/test_sents_qg_07\n",
      "/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n",
      "/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py:542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(a, requires_grad=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py\", line 229, in translate\n",
      "    for batch in data_iter:\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 201, in process\n",
      "    tensor = self.numericalize(padded, device=device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 321, in numericalize\n",
      "    arr = self.postprocessing(arr, None)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/inputters/text_dataset.py\", line 235, in make_src\n",
      "    src_vocab_size = max([t.max() for t in data]) + 1\n",
      "RuntimeError: invalid argument 1: tensor must have one dimension at /opt/conda/conda-bld/pytorch_1549617926868/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:590\n",
      "generating questions for file #8\n",
      "./wikipedia_squad/AA/test_sents_qg_08\n",
      "/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n",
      "/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py:542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(a, requires_grad=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py\", line 229, in translate\n",
      "    for batch in data_iter:\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 201, in process\n",
      "    tensor = self.numericalize(padded, device=device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 321, in numericalize\n",
      "    arr = self.postprocessing(arr, None)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/inputters/text_dataset.py\", line 235, in make_src\n",
      "    src_vocab_size = max([t.max() for t in data]) + 1\n",
      "RuntimeError: invalid argument 1: tensor must have one dimension at /opt/conda/conda-bld/pytorch_1549617926868/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:590\n",
      "generating questions for file #9\n",
      "./wikipedia_squad/AA/test_sents_qg_09\n",
      "/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n",
      "/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py:542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(a, requires_grad=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 36, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/test.py\", line 24, in main\n",
      "    attn_debug=opt.attn_debug)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/translate/translator.py\", line 229, in translate\n",
      "    for batch in data_iter:\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 201, in process\n",
      "    tensor = self.numericalize(padded, device=device)\n",
      "  File \"/home/julia_buffinton/anaconda3/envs/pytorch-env/lib/python2.7/site-packages/torchtext/data/field.py\", line 321, in numericalize\n",
      "    arr = self.postprocessing(arr, None)\n",
      "  File \"/home/julia_buffinton/GenerationQ/model/onmt/inputters/text_dataset.py\", line 235, in make_src\n",
      "    src_vocab_size = max([t.max() for t in data]) + 1\n",
      "RuntimeError: invalid argument 1: tensor must have one dimension at /opt/conda/conda-bld/pytorch_1549617926868/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:590\n"
     ]
    }
   ],
   "source": [
    "# update as needed, depending on which model has lowest perplexity\n",
    "generationq_dir = '~/GenerationQ/model'\n",
    "\n",
    "for c in string.ascii_uppercase[:end_letter]:\n",
    "    \n",
    "    output_dir = wiki_output_file_dir + c\n",
    "\n",
    "    for i in range(num_files): \n",
    "    \n",
    "        wiki_squad_file = output_dir + '/wiki_squad_' + '%02d' % i + '.json'\n",
    "        sentences_onmt = output_dir + '/test_sents_qg_' + '%02d' % i\n",
    "        sentences_labeled = output_dir + '/test_sents_labeled_' + '%02d' % i\n",
    "        \n",
    "        question_output = output_dir + '/test_questions_' + '%02d' % i + '.txt'\n",
    "    \n",
    "        print \"generating questions for file #\" + str(i)\n",
    "        print sentences_onmt\n",
    "    \n",
    "        !python $generationq_dir/test.py -model $generationq_dir/trained/600rnn_step_14000.pt -src $sentences_onmt -output $question_output \\\n",
    "        -replace_unk -beam_size 3 -gpu 0 -batch_size 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Label Questions, add back to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each letter of the alphabet\n",
    "for c in string.ascii_uppercase[:end_letter]:\n",
    "\n",
    "    output_dir = wiki_output_file_dir + c\n",
    "\n",
    "    # for each file in there (total: 100)\n",
    "    for i in range(num_files): \n",
    "        \n",
    "        sentences_labeled = output_dir + '/test_sents_labeled_' + '%02d' % i\n",
    "\n",
    "        wiki_squad_file = output_dir + '/wiki_squad_' + '%02d' % i + '.json'\n",
    "        \n",
    "        #question_output = output_dir + '/test_sents_qg_' + '%02d' % i # THIS IS JUST FOR TESTING!!! \n",
    "        question_output = output_dir + '/test_questions_' + '%02d' % i + '.txt' # This is final!!\n",
    "        question_output_qs = output_dir + '/test_questions_q_' + '%02d' % i + '.txt'\n",
    "        \n",
    "        with open(question_output) as f:\n",
    "            pred_questions = f.read().splitlines()    \n",
    "            pred_questions = pred_questions[::2] # start with the 1st line\n",
    "            \n",
    "        with open(question_output_qs, 'w') as f:\n",
    "            for item in pred_questions:\n",
    "                f.write(\"%s\" % item) # removed newline character after this?\n",
    "            \n",
    "        with open(wiki_squad_file) as json_file:  \n",
    "            data = json.load(json_file)\n",
    "\n",
    "        #print(type(data))\n",
    "        #data = pd.DataFrame.from_dict(data)\n",
    "        wiki_df = data['data']\n",
    "        #print(type(wiki_df))\n",
    "\n",
    "        with open(question_output_qs) as f1:\n",
    "\n",
    "            with open(sentences_labeled) as f2:\n",
    "                for line_q,line_s in zip(f1,f2):\n",
    "                    \n",
    "                    line_tuple = eval(line_s)\n",
    "                    nums = line_tuple[0]\n",
    "                    #print(nums)\n",
    "                    #sent = line_tuple[1]\n",
    "\n",
    "                    doc = wiki_df[nums[0]] # pull the whole document\n",
    "                    context = doc[\"paragraphs\"][nums[1]]\n",
    "\n",
    "                    context['qas'].append({\"question\": line_q, \"answers\": [], \"id\": str(nums)})\n",
    "            \n",
    "        with open(wiki_squad_file, 'w') as outfile:  \n",
    "            json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_file = './wikipedia_squad/AA/wiki_squad_00.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/06/2019 04:10:17 PM: [ CUDA enabled (GPU -1) ]\n",
      "04/06/2019 04:10:17 PM: [ Initializing pipeline... ]\n",
      "04/06/2019 04:10:17 PM: [ Initializing document ranker... ]\n",
      "04/06/2019 04:10:17 PM: [ Loading /home/julia_buffinton/DrQA/data/wikipedia/docs-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz ]\n",
      "04/06/2019 04:12:13 PM: [ Initializing document reader... ]\n",
      "04/06/2019 04:12:13 PM: [ Loading model /home/julia_buffinton/DrQA/data/reader/multitask.mdl ]\n",
      "04/06/2019 04:12:49 PM: [ Initializing tokenizers and document retrievers... ]\n",
      "04/06/2019 04:12:51 PM: [ Loading queries from ./wikipedia_squad/AA/wiki_squad_00.json ]\n",
      "Traceback (most recent call last):\n",
      "  File \"../../DrQA/scripts/pipeline/predict.py\", line 109, in <module>\n",
      "    queries.append(data['question'])\n",
      "KeyError: 'question'\n"
     ]
    }
   ],
   "source": [
    "!python ../../DrQA/scripts/pipeline/predict.py $sample_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add answers back to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
