#!/usr/bin/env python
"""
    Choose sentence from paragraph
"""
import nltk
import os
import math
import string
import operator
from collections import defaultdict, OrderedDict

def clean_sentences(paragraph):
        """Clean sentences, remove digit, punctuation, upper case to lower
        Args: sentences: sentences to be cleaned
        Return: sentences_processed: dict of cleaned sentences
        """
        flag = 0
        sentence_processed = {}

        # Remove all punctuation except periods
        punc = set(string.punctuation)
        punc.remove('.')
        # Remove all digits
        paragraph = ''.join([x for x in paragraph if not x.isdigit()])
        paragraph = ''.join([x for x in paragraph if x not in punc])
        # Lowercase everything
        paragraph = ''.join([x.lower() for x in paragraph])
        # Split into words
        paragraph = ' '.join(paragraph.split())

        stop_words = nltk.corpus.stopwords.words('english')
        stemmer = nltk.stem.PorterStemmer()
        tokenize = nltk.word_tokenize

        for sentence in paragraph.split('.'):
            sentence = sentence.strip()
            sentence = [stemmer.stem(word) for word in tokenize(
                sentence) if not word in stop_words]
            if sentence:
                sentence_processed[flag] = sentence
                flag += 1
        print('sentence_processed: ' + str(sentence_processed))
        return sentence_processed

def word_distribution(sentence_processed):
        """Compute word probabilistic distribution which is calculated by \
        term frequency divided by total word count"""
        word_distr = defaultdict(int)
        word_count = 0.0
        # For each word in each sentence, count number of times each word appears as well as total words in sentence
        for k in sentence_processed:
            for word in sentence_processed[k]:
                word_distr[word] += 1
                word_count += 1

        for word in word_distr:
            word_distr[word] = word_distr[word] / word_count

        return word_distr

def sentence_weight(word_distribution, sentence_processed):
        """Compute weight with respect to sentences
        Args:
                word_distribution: probabilistic distribution of terms in document
                sentence_processed: dict of processed sentences generated by clean_sentences
        Return:
                sentence_weight: dict of weight of each sentence
        """
        sentence_weight = {}
        # Iterate through each word in each sentence, if word distribution and sentence id are in dictionary, 
        # add to existing word distribution. Else, sentence weight for given sentence equals current word distribution
        for sentence_id in sentence_processed:
            for word in sentence_processed[sentence_id]:

                if word_distribution[word] and sentence_id in sentence_weight:
                    sentence_weight[sentence_id] += word_distribution[word]
                else:
                    sentence_weight[sentence_id] = word_distribution[word]
        # Sentence weight equals sum of word distributions divided by length of cleaned sentence
            sentence_weight[sentence_id] = sentence_weight[
                sentence_id] / float(len(sentence_processed[sentence_id]))

        sentence_weight = sorted(sentence_weight.items(
        ), key=operator.itemgetter(1), reverse=True)
        print('sentence_weight: ' + str(sentence_weight))
        return sentence_weight

def topically_important_sentence(sentence_weight, paragraph):
        """Select topically import sentences
        Args:
                sentence_weight: dict, weight of sentences computed in sentence_weight
                sentences: set of sentences
        Return:
                sentences_selected: dict, topically important sentences selected
        """
        sentence_length = len(sentence_weight)
        # how many sentences to retain
        num_sentences_selected = math.ceil(float(0.05) * sentence_length)
        num_sentences_selected = int(num_sentences_selected)
        # key of selected sentences
        sentences_selected_key = []
        # dictionary of all sentences
        sentences_dict = {}
        flag = 0
        # select num_sentences_selected # of sentences from list of sentence weights
        for k, v in sentence_weight[0:num_sentences_selected]:
            sentences_selected_key.append(k)
        # Iterate through sentences in raw text and assign a id number
        print('paragraph: ' + str(paragraph))
        for sentence in paragraph.split('.'):
            if sentence:
                sentences_dict[flag] = sentence
                flag += 1
        print('sentences_dict: ' + str(sentences_dict))
        sentences_selected = OrderedDict()
        print('sentences_selected_key: ' + str(sentences_selected_key))
        print('sentences_selected: ' + str(sentences_selected))
        for key in sentences_selected_key:
            sentences_selected[key] = sentences_dict[key]

        return sentences_selected
